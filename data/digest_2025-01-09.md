## AI Submissions for Thu Jan 09 2025 {{ 'date': '2025-01-09T17:11:34.397Z' }}

### WorstFit: Unveiling Hidden Transformers in Windows ANSI

#### [Submission URL](https://blog.orange.tw/posts/2025-01-worstfit-unveiling-hidden-transformers-in-windows-ansi/) | 332 points | by [notmine1337](https://news.ycombinator.com/user?id=notmine1337) | [108 comments](https://news.ycombinator.com/item?id=42647101)

In a groundbreaking presentation at Black Hat Europe 2024, researchers from DEVCORE revealed a significant security vulnerability in Windows stemming from the internal charset conversion feature known as Best-Fit. This feature, primarily used for handling different character encodings, has been transformed into a multi-faceted attack surface, enabling threats like Path Traversal, Argument Injection, and Remote Code Execution (RCE) across various prominent applications.

The authors, including co-researcher splitline, conducted an intricate exploration of Windows encoding history—from the era of ANSI and code pages to the adoption of Unicode—shedding light on how these encoding methods can intersect to create potential vulnerabilities. They elaborated on how this issue arises from a mix of compiler behavior, runtime mishaps in C/C++, and frequent developer oversights, making it a complex challenge for correction within the open-source community.

Throughout their presentation, the researchers outlined how attackers can exploit this newfound vulnerability using real-world scenarios, such as circumventing seemingly secure PHP code to execute commands like launching Windows Calculator. The findings highlight the urgent need for better mitigation strategies to address these critical flaws in the encoding process.

For those eager to dive deeper, the detailed slides and updates from the research can be found on the project's [official website](https://worst.fit/). With this revelation, the lingering risks associated with character encoding and their implications are now in the spotlight, prompting developers and security professionals alike to reassess their approaches to encoding and system security.

In this discussion on Hacker News following the DEVCORE presentation about the Windows security vulnerability, participants delve into several aspects of encoding, legacy behavior, and coding practices associated with Windows APIs.

1. **Character Encoding Challenges**: Several commenters pointed out the historical complexity of character encoding in Windows, particularly how legacy features like Best-Fit mapping exacerbate security vulnerabilities. The shift from ANSI to UTF-8 and the integration of Unicode have introduced various path and command injection vulnerabilities, which are not well-handled in current applications.

2. **Developer Oversight**: Many users noted the frequent oversight by developers who do not account for the idiosyncrasies in Windows' character handling. This leads to gaps in security, particularly in applications written for the Win32 API that handle command-line inputs or paths.

3. **Coding Practices**: The conversation highlighted the importance of adopting robust coding practices and standards, especially when dealing with character strings. Suggestions included the use of modern API functions that are less prone to these legacy issues and better support for UTF-8 and other formats.

4. **Legacy Behavior**: Commenters discussed the need to address the backward compatibility that Windows APIs maintain, which often leads to persistent vulnerabilities. Users emphasized the importance of not relying on outdated APIs and urged an update to contemporary standards.

5. **Recommendations and Solutions**: Several suggestions were made on potential mitigation strategies, including avoiding non-standard library functions and instead using thoroughly vetted libraries designed for modern character encoding practices.

6. **Community Insights**: The community shared insights from their own experiences with these vulnerabilities, showing a mix of frustration with Microsoft's handling of the issues and hope for improvements in future iterations of Windows APIs.

Overall, the discussion reflected a strong concern regarding the security implications of character encoding practices in Windows, coupled with a collective desire for better coding standards and practices within the developer community.

### Show HN: TabPFN v2 – A SOTA foundation model for small tabular data

#### [Submission URL](https://www.nature.com/articles/s41586-024-08328-6/link) | 129 points | by [onasta](https://news.ycombinator.com/user?id=onasta) | [33 comments](https://news.ycombinator.com/item?id=42647343)

In a groundbreaking development in computational science, researchers have introduced the Tabular Prior-data Fitted Network (TabPFN), a novel tabular foundation model designed to enhance predictions from small to medium-sized datasets. With an impressive ability to outperform traditional methods—like gradient-boosted decision trees—by a wide margin, TabPFN can achieve superior classification results in just 2.8 seconds, compared to the four hours typically required for tuning baseline models.

TabPFN's strength lies in its use of in-context learning, a technique originally successful in large language models, which empowers it to learn from a vast array of algorithms, including those with no easy closed-form solutions. This model not only excels at filling in missing values but also offers fine-tuning and generative capabilities, thereby supporting various scientific fields from biomedicine to economics. By addressing the challenges of tabular data—like diverse column types and inherent data imbalances—TabPFN showcases the potential to revolutionize data analysis and accelerate scientific discovery across multiple disciplines.

In the Hacker News discussion surrounding the Tabular Prior-data Fitted Network (TabPFN), participants expressed enthusiasm for its impressive performance in handling tabular data with small to medium-sized datasets. Users shared their experiences comparing TabPFN with traditional models like SVM and described its ease of use and quick training time, significantly outperforming conventional methods which often require extensive tuning.

Some commenters highlighted TabPFN's capabilities in dealing with missing values and its potential for generative tasks, pointing out how it utilizes in-context learning to adjust to data characteristics effectively. They noted specific use cases, like benchmarking against datasets, and appreciated its proficiency in cross-validation results.

Others discussed the technical nuances of the model, including its ability to efficiently process diverse features and column types, and its flexibility in training on synthetic datasets. References to related work and extensions of TabPFN were shared, motivating discussions on its applicability in various fields, including economics and biomedicine.

Overall, the sentiment leaned towards excitement about the potential of TabPFN to revolutionize data analysis, reinforcing confidence in its capabilities to improve predictive performance while significantly reducing processing time compared to existing solutions.

### rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking

#### [Submission URL](https://arxiv.org/abs/2501.04519) | 29 points | by [roboboffin](https://news.ycombinator.com/user?id=roboboffin) | [6 comments](https://news.ycombinator.com/item?id=42641817)

In a groundbreaking new paper titled "rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking," a team of researchers led by Xinyu Guan explores the capabilities of small language models (SLMs) in mathematical problem-solving. Published on arXiv, this work claims that these models can match or even exceed the performance of larger counterparts, such as OpenAI's o1, by employing innovative techniques like Monte Carlo Tree Search (MCTS) combined with a self-evolving structure.

Key innovations include a unique data synthesis method that enhances the training process by generating comprehensive reasoning trajectories, a refined training approach for a process preference model that avoids simplistic scoring, and a self-evolution strategy that iteratively enhances the models' reasoning abilities. The results are impressive: the models showcased substantial performance improvements on benchmarks, with the Qwen2.5-Math-7B model increasing its math reasoning accuracy from 58.8% to 90%, and the Phi3-mini-3.8B from 41.4% to 86.4%. Notably, in the USA Math Olympiad (AIME), their system solved an average of 53.3% of the problems, putting it among the top 20% of high school math students.

This research not only shines a light on the potential of smaller models to compete in complex reasoning tasks but also provides a robust framework for future advancements in AI-driven mathematics. For those interested in diving deeper, the paper and associated resources are accessible online.

The discussion surrounding the "rStar-Math" paper reveals a mix of reactions and insights from the Hacker News community. 

1. **Performance and Implementation Concerns**: A user noted the paper's mention of Monte Carlo Tree Search (MCTS) and expressed skepticism about the efficiency of its implementation, particularly regarding branching structures and their potential for GPU parallelization.

2. **Advanced Reasoning Techniques**: Another commenter highlighted the inclusion of self-reflection in the training data, suggesting that this might lead to enhanced reasoning capabilities akin to advanced cognitive processes. They expressed excitement about the potential of this self-reflection methodology.

3. **Surprise at the Paper’s Popularity**: One user remarked on their surprise that the paper had received significant attention, pointing out the impressive abstract and the broader implications of the findings.

4. **Technical Issues**: A separate user briefly remarked on a technical issue related to a link (resulting in a 404 error) and commented on the abstract code's utility.

Overall, the dialogue reflects a blend of enthusiasm for the paper's innovations, technical scrutiny regarding implementation, and curiosity about its implications for AI-driven mathematics and reasoning.

### The Complete Text of "All Watched over by Machines of Loving Grace"

#### [Submission URL](https://blog.jgc.org/2024/12/the-complete-text-of-all-watched-over.html) | 62 points | by [MilnerRoute](https://news.ycombinator.com/user?id=MilnerRoute) | [30 comments](https://news.ycombinator.com/item?id=42646932)

In a delightful throwback to a cherished piece of literary history, a recent post has resurfaced Richard Brautigan's iconic poem, "All Watched Over by Machines of Loving Grace." This 1967 work has long resonated within tech circles, yet a complete PDF of the original publication has been elusive—until now! The contributor, recognizing the poem's relevance and the copyright's stipulation allowing free reproduction, has made a scan of the entire collection available online. The response from readers has been warm, with one noting the poem's unexpected beauty. It's a poetic reminder of the intersection between technology and art, sure to inspire both nostalgia and reflection among fans old and new.

The Hacker News discussion surrounding Richard Brautigan's poem, "All Watched Over by Machines of Loving Grace," is rich with reflections on its themes and historical context. Users share personal anecdotes about the poem's emotional impact and beauty, with some recalling their past experiences with similar literary works. Several commenters discuss the broader connection between technology, art, and the counter-culture movements of the 1960s, referencing figures like Gary Snyder and the Beatniks.

Some delve into the philosophical implications of the poem, with discussions on the relationships between humanity, machines, and nature—echoing a sense of dystopia. Others highlight the relevance of the work in today's rapidly advancing technological landscape, where the topics of AI and existential questions arise.

Participants also note the difficulty in accessing the original publication and express appreciation for its now-available full PDF. The conversation touches on copyright discussions and the legacy of avant-garde poetry, with mentions of other literary and artistic movements that resonate with Brautigan's vision.

Overall, the discussion is a nostalgic and thought-provoking exploration of the intersection of literature, technology, and the nuanced perspectives on modernity, inviting both admiration and critical analysis.

### Zuckerberg Approved AI Training on Pirated Books, Filings Say

#### [Submission URL](https://news.bloomberglaw.com/litigation/zuckerberg-approved-ai-training-on-pirated-books-filings-says) | 56 points | by [gnabgib](https://news.ycombinator.com/user?id=gnabgib) | [45 comments](https://news.ycombinator.com/item?id=42651007)

In a developing legal battle, Meta CEO Mark Zuckerberg is facing serious allegations related to copyright infringement tied to the company’s AI model, LLaMA. Court filings reveal that Zuckerberg greenlit the use of a pirated dataset of books for training the AI. A disturbing highlight from the unsealed documents is the admission from a Meta employee who allegedly stripped away copyright information from the LibGen repository—a notorious archive of copyrighted books—to cover up extensive copyright violations. This case has drawn the attention of a group of authors, including notable figures like comedian Sarah Silverman, who are suing Meta for these alleged transgressions. The outcome of this suit could have significant ramifications for intellectual property rights in the AI landscape.

In a vigorous discussion surrounding the recent allegations against Meta CEO Mark Zuckerberg regarding copyright infringement in training their AI model LLaMA, commenters are divided on various aspects. Some users emphasize the seriousness of the claims, particularly the alleged use of a pirated dataset of copyrighted books, with one noting that thousands of printed copyrighted works could be involved. Others debate the implications of copyright law as it relates to training AI models, with references to past legal decisions on fair use and copyright infringement.

The conversation also reflects concerns about the responsibilities of corporations, particularly in how they handle copyrighted material. Several participants express skepticism about the ethical implications of using such datasets without proper attribution or rights clearance. The notion of stripping copyright information from datasets is condemned by many, who view it as a clear violation of intellectual property rights.

Furthermore, users speculate on the potential legal repercussions for Meta if convicted, including substantial fines or even imprisonment for those involved. The broader implications for the AI landscape and intellectual property rights are highlighted, indicating that the outcome of this case could set significant precedents. Overall, the comments amplify a sense of unease about the intersection of AI development and copyright law, as well as the accountability of tech leaders.

