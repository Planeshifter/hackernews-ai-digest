## AI Submissions for Fri Dec 08 2023 {{ 'date': '2023-12-08T17:10:30.750Z' }}

### Cyborg cockroach could be the future of earthquake search and rescue

#### [Submission URL](https://www.nature.com/articles/d41586-023-03801-0) | 28 points | by [sohkamyung](https://news.ycombinator.com/user?id=sohkamyung) | [22 comments](https://news.ycombinator.com/item?id=38568062)

Researchers at Nanyang Technological University in Singapore are developing cyborg insects, specifically Madagascar hissing cockroaches, to aid in search and rescue missions after natural disasters like earthquakes. These cyborg cockroaches can be remotely controlled through implanted electrodes in their nervous systems and are equipped with sensors and transmitters to locate survivors and communicate with rescue workers. The project is part of the emerging field of biohybrid robotics, where engineers combine biological materials with synthetic materials to create functional robots. While there are still challenges to overcome, harnessing the natural capabilities of living organisms shows promise for advancing robotics.

The discussion on this submission covers a range of opinions and perspectives. Some comments express skepticism about the ethics and practicality of using electronic implants in insects, suggesting that it may not be ethical to control animals for human purposes. Others discuss the potential benefits of using cyborg insects in extreme environments for search and rescue missions. The efficiency and capabilities of cockroaches compared to small robots are also debated, with some suggesting that cockroaches are more adaptable and resilient. Additionally, there are discussions about the feasibility of controlling animals in general, with references to recent studies on implanting electronic devices in fish for navigation. Some comments express disgust or aversion to the idea, while others find it interesting from a scientific perspective. Overall, the discussion encompasses a variety of viewpoints on the topic.

### QuIP#: 2-bit Quantization for LLMs

#### [Submission URL](https://cornell-relaxml.github.io/quip-sharp/) | 191 points | by [jasondavies](https://news.ycombinator.com/user?id=jasondavies) | [45 comments](https://news.ycombinator.com/item?id=38576351)

Researchers have developed a compression method called QuIP#, which combines lattice codebooks with incoherence processing to create state-of-the-art 2 bit quantized language models (LLMs). LLMs are known for their impressive performance but are also very large, requiring significant memory resources. QuIP# addresses this issue by reducing the size of LLMs without sacrificing performance. By quantizing LLMs from 16 bit to 2 bit precision, the size of the models can be reduced by 8x, making them more manageable on GPUs. QuIP# achieves near-native performance at 2 bits, outperforming other baselines. The researchers provide a full codebase and infrastructure for users to quantize and deploy their own models using QuIP#. Overall, QuIP# presents a promising solution to the challenges posed by the size of LLMs.

The discussion on this submission covers various topics related to the paper. 

- Some users discuss the improvements in paragraph quality and the challenge of understanding network precision and quantization.
- Others mention the importance of quantization, especially for models like Mistral MoE, and how it works for smaller models.
- There is a discussion on pixel statistics and binary space compression in RGBA space.
- Some users ask questions about quantization, including its relationship to weight matrix flattening and its implementation on CPUs and GPUs.
- LM Studio is mentioned, but it is noted that running it on a MacBook requires a GPU server.
- There is a discussion on quantized LLMs, including the code and implementation details.
- Users discuss the testing and deployment of quantized models.
- Some users suggest looking into different quantization formats, such as EXL2 and OmniQuant.
- There is a request to test multi-level cell LLM quantization.
- A user provides details about the concept and application of higher-order functions, such as tetration.
- There is a clarification on the relevance of QuIP# in the discussion.
- Users discuss the challenges and feasibility of 1-bit quantization for functional programming and its potential usefulness in certain tasks.
- A user mentions a paper from 2017 that successfully utilized 1-bit quantization.

### Show HN: We've built an Open Source plugin to make using AI easier for VFX

#### [Submission URL](https://DeepMake.com/download/) | 28 points | by [bryanlyon](https://news.ycombinator.com/user?id=bryanlyon) | [7 comments](https://news.ycombinator.com/item?id=38574934)

Today on Hacker News, the top submission is an article titled "How to Become a Better Programmer by Having Fun."

In this insightful blog post, the author shares their firsthand experience on how to improve programming skills while enjoying the process. They emphasize the importance of finding joy in coding and how it can enhance productivity and learning.

The article suggests various methods to make programming more enjoyable. These include participating in coding challenges, exploring side projects, and engaging in open-source contributions. By actively seeking out these opportunities and embracing the playful aspect of coding, the author argues that programmers can become more motivated and efficient in their work.

Additionally, the author emphasizes the importance of continuous learning and experimenting. They encourage programmers to explore new technologies, attend conferences, and join coding communities to foster growth. By constantly pushing boundaries and trying new things, they assert that programmers can expand their skills and broaden their perspectives.

Overall, this article serves as a reminder that programming can be both productive and fun. By incorporating joy, experimentation, and continuous learning into their practice, programmers can become better at their craft while enjoying the process. So, embrace the playful side of coding and let the fun enhance your programming journey.

The discussion on this submission covers various topics related to the content of the article. Here is a summary of the comments:

- User "nopol10" states that the article sounds good and asks if there are any capabilities for downloading the content.
- User "brynlyn" responds to "nopol10" and mentions that they are working on releasing regular content products on their YouTube channel.
- User "Zetobal" comments that the information product website doesn't exist.
- User "Jenn_With_Two_N" asks if there is a method to contact the author directly.
- User "brynlyn" provides a link to the contact page, where the user can get in touch.
- User "gmbllnd" expresses the opinion that maintaining training data sources and licensed content makes sense for serious content creation.
- User "brynlyn" responds to "gmbllnd" and mentions that they are not training models, but rather bringing existing AI models and VFX workflow to individual artists.

Please note that the abbreviations and lack of context in the comments make it hard to determine the exact topic of conversation.

### Gaussian Head Avatar: Ultra High-Fidelity Head Avatar via Dynamic Gaussians

#### [Submission URL](https://yuelangx.github.io/gaussianheadavatar/) | 171 points | by [phil9l](https://news.ycombinator.com/user?id=phil9l) | [41 comments](https://news.ycombinator.com/item?id=38567074)

Researchers from Tsinghua University and NNKosmos Technology have developed a new method called "Gaussian Head Avatar" for creating high-fidelity 3D head avatars. The method combines controllable 3D Gaussians with a fully learned deformation field to capture complex expressions, resulting in fine-grained dynamic details and expression accuracy. To ensure stability and convergence during training, the researchers devised a geometry-guided initialization strategy based on implicit SDF and Deep Marching Tetrahedra. The experiments showed that their approach outperformed other state-of-the-art methods, achieving ultra high-fidelity rendering quality even under exaggerated expressions. The Gaussian Head Avatar rendered images at a resolution of 2K and demonstrated impressive cross-identity reenactment results with details like beards and teeth. The research paper provides further details on the methodology, and a demo video is available for reference.

The discussion surrounding the submission "Gaussian Head Avatar: High-Fidelity 3D Head Avatar from a Single Image" on Hacker News covers a range of topics. Some users discuss the potential applications of this technology, such as in gaming and virtual meetings, while others mention its resemblance to concepts found in science fiction, such as identity cloning. There is also mention of other related research papers and discussions on the technical aspects of Gaussian splitting. Additionally, there are comments discussing the potential impact of high-quality avatars on virtual reality and the challenges of distinguishing real photos from fictional ones. Other topics touched upon include security concerns and the trustworthiness of online meetings.

### Google shares sink following reports that some of their AI demo was faked

#### [Submission URL](https://www.cnbc.com/video/2023/12/08/google-shares-sink-following-reports-that-some-of-their-ai-demo-was-faked.html) | 74 points | by [donsupreme](https://news.ycombinator.com/user?id=donsupreme) | [30 comments](https://news.ycombinator.com/item?id=38574940)

Google shares tumbled after reports emerged that parts of its AI demo, known as Gemini, were manipulated to make the technology appear more advanced than it actually is. This revelation raised questions about Google's credibility in the AI space and the company's commitment to transparency. The story generated interest on CNBC's "Halftime Committee" segment, with experts discussing the potential impact of the scandal on Alphabet, Google's parent company. This incident serves as a reminder of the importance of ethical practices and honest representation in the development and demonstration of AI technologies.

Discussion:

- User "chtmst" comments that they didn't bother reading the announcement on Hacker News, as no one had pointed out anything interesting about it yet. They also mention that it's typical for big companies to manipulate product announcements to create hype.

- User "asd88" responds, saying that flashy product announcements are a mechanism used by leadership to build confidence and generate external hype.

- User "rdtsc" adds a humorous comment, guessing that the year is ending and people love holidays instead of discussing the topic.

- User "ralph84" brings up the accountability of Sundar Pichai, suggesting that he may ultimately be responsible for Google's attempts to commercialize its AI research.

- User "stllwtht" replies, stating that Google's AI launch has not yet affected the business significantly and that McKinsey has shown potential in achieving many accomplishments.

- User "mchl" shares their experience of working at McKinsey in 2004, mentioning that the consulting industry is highly competitive and that McKinsey has a strong presence.

- User "wg0" adds that Amazon is also taking baby steps in the AI space, mentioning their gaming service, Luma.

- User "Halen77" criticizes Sundar Pichai, stating that he is absolutely clueless about AI and lacks the charisma to support moving things forward, causing major embarrassment.

- User "pxys" comments that investors should look at the balance sheets and Hacker News comments to understand Google's stock price, suggesting that Sundar Pichai may not be willing to innovate and share pricing, similar to how Microsoft's stock skyrocketed after Steve Ballmer was replaced.

- User "tns" responds, comparing Sundar Pichai's CEO journey with Steve Ballmer's at Microsoft, suggesting that Sundar is willing to innovate and share pricing, unlike Ballmer.

- User "cwlks" adds that Microsoft's stock today is $373, showing a 10x growth compared to Google's 4x growth, which is not comparable.

- User "thclnr" argues that stock price doesn't signal growth due to external reasons like mobile growth or internet products, mentioning that Satya Nadella, Microsoft's CEO, launched major innovative cloud services that delivered growing revenue year after year.

- User "Crash0v3rid3" comments on the terrible comparison of stock splits between the companies' CEOs.

- User "pxys" adds that stock prices don't count splits either.

- User "ralph84" argues that Google underperformed in big tech and mentions other companies like Apple, Microsoft, Amazon, Nvidia, Tesla, TSMC, Broadcom, Adobe, AMD, and Netflix.

- User "pxys" disagrees, stating that Google is performing compared to other companies.

- User "ltsfplp" believes that big tech companies should hold a higher standard due to their potential impact on the world.

- User "hrphrpyj" adds that resources and available levels are limiting factors in holding high standards, mentioning leadership's willingness to hold a high standard.

- User "pdhl" responds, saying that leadership is working on it and may be revenue-driven.

- User "pylr" comments that leadership should be held accountable.

- User "wg0" mentions that marketing pages pitch products on native grounds, multiple models, multiple inputs, multiple models stitched together, and dropping flat computer systems, implying that it's revolutionary but should have a fine print somewhere.

- User "amf12" brings attention to the significant 15% drop in Google's stock in the past week.

- User "est31" mentions a larger 9% drop in AI marketing in January, commenting on the occurrence of marketing flips in the AI product industry.

- User "sprnt" states that it's relevant to expect shares to drop after a major product announcement.

- User "tsx" starts a separate thread, asking what is happening at Google in terms of the simplicity, efficiency, and engineering qualities of their software, mentioning that Google Chat is slow, complicated, and buggy.

- User "mxlmb" responds, suggesting that MBA recruitment departments are increasing their power, and leaders are nested technologists who truly value innovation and building quality products.

- User "ChrisArchitect" shares a link to a discussion on the topic.

- User "hppytgr" adds a comment below, which is not visible in the provided summary.

### 5Ghoul: Unleashing Chaos on 5G Edge Devices

#### [Submission URL](https://asset-group.github.io/disclosures/5ghoul/) | 134 points | by [rho138](https://news.ycombinator.com/user?id=rho138) | [24 comments](https://news.ycombinator.com/item?id=38567149)

The Singapore University of Technology and Design is making waves in the world of technology and design. The students, researchers, and faculties there are constantly pushing boundaries and making groundbreaking contributions to various fields. From developing innovative technologies to designing cutting-edge systems, they are leaving no stone unturned.

Their expertise extends across a range of domains, including people, research, publications, code, disclosures, testbeds, service, information systems, and technology. Each division brings its unique perspective, contributing to the university's reputation as a hub of innovation.

In terms of research, the Singapore University of Technology and Design is at the forefront. Their research projects cover a wide range of topics, from artificial intelligence and robotics to sustainable development and urban planning. With a multidisciplinary approach, their research aims to address real-world problems and deliver practical solutions.

The university's publications showcase the innovative ideas and breakthroughs achieved by their researchers. These publications serve as a valuable resource for scholars, industry professionals, and enthusiasts alike. Whether it's a journal article or a conference paper, the publications highlight the expertise and knowledge generated at the Singapore University of Technology and Design.

Code is at the heart of technological advancements, and the university recognizes its significance. By sharing their code, the researchers at the Singapore University of Technology and Design enable others to build upon their work, fostering collaboration and accelerating progress. Open-source projects and code snippets are just a few examples of their commitment to advancing technology.

Disclosures play a crucial role in ensuring transparency and trust. The university understands this and actively shares information about their inventions, patents, and intellectual property. By doing so, they encourage collaboration, licensing, and potentially even commercialization of their innovations.

Testbeds provide a real-world environment for researchers and students to validate their ideas and prototypes. The Singapore University of Technology and Design offers state-of-the-art testbeds, enabling hands-on experimentation and validation. These testbeds facilitate the development of robust and reliable solutions, ready to tackle real-world challenges.

Service is ingrained in the university's DNA. They actively engage with industry partners, government agencies, and the community to offer their expertise and resources. From consultancy services to collaborative projects, the Singapore University of Technology and Design aims to make a positive impact and drive meaningful change.

Information systems play a vital role in today's interconnected world. The university's expertise in this field allows them to develop efficient and secure systems. By leveraging cutting-edge technologies and innovative approaches, they contribute to the advancement of information systems, ensuring a seamless and secure digital experience.

The Singapore University of Technology and Design's commitment to technology and design is evident in all their endeavors. Their interdisciplinary approach, collaborative mindset, and focus on practical solutions make them a force to be reckoned with. As they continue to push boundaries and explore new frontiers, their impact on the world of technology and design will only continue to grow.

The discussion on this submission revolves around various aspects of technology and design, including software vulnerabilities, proprietary data, and communication protocols. Here are some key points from the discussion:

- One commenter points out that critical vulnerabilities in modern mobile devices often go unnoticed for a long time until they are patched.
- The disclosure of sensitive data and crash bugs in certain services is discussed, with some expressing concerns about the safety of customer data.
- A debate ensues regarding vulnerability branding and the need for CVE numbers to address specific vulnerabilities.
- The disclosure of sensitive data, particularly how it affects the confidentiality of LTE devices and exposes the International Mobile Subscriber Identity (IMSI), is mentioned.
- The presence of proprietary data and its impact on firmware and bootloaders is questioned, with concerns raised about the potential for malware.
- The impact of communication protocols on network security and privacy is discussed, with references to past vulnerabilities in protocols such as SMTP and Signaling System 7 (SS7).
- The limitations of current network protocols and the potential hindrance to innovation are also mentioned.
- Lastly, the discussion touches on the challenges of hardware and software integration, particularly in the context of computer systems in cars.

Overall, the discussion delves into the complexities and vulnerabilities in technology and design, highlighting the need for continuous improvement and innovation.

### Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts (2017)

#### [Submission URL](https://arxiv.org/abs/1701.06538) | 57 points | by [georgehill](https://news.ycombinator.com/user?id=georgehill) | [9 comments](https://news.ycombinator.com/item?id=38572284)

Researchers at Google have developed a new type of neural network layer called the Sparsely-Gated Mixture-of-Experts (MoE) layer, which allows for the creation of outrageously large neural networks. The MoE layer consists of thousands of feed-forward sub-networks and a trainable gating network that selects which experts to use for each example. This approach allows for greater model capacity without a proportional increase in computation. The researchers applied the MoE layer to language modeling and machine translation tasks, achieving significant improvements in results compared to state-of-the-art models at a lower computational cost. The model architectures they developed included a MoE layer with up to 137 billion parameters.

The discussion around the submission centers on the topic of outrageously large neural networks and the use of the Sparsely-Gated Mixture-of-Experts (MoE) layer. Some commenters point out that previous state-of-the-art models had significantly fewer parameters, ranging from 2 million to 151 million, while the MoE layer enables models with up to 137 billion parameters. They also mention the potential importance of scaling up model capacity appropriately for effective results. 

One commenter raises concerns about the tendency of some companies and practitioners to focus on model size and hyperparameters rather than the actual quality of the model and the importance of properly contextualizing research in the larger machine learning community. They highlight the strong evidence supporting the efficacy of other models and techniques like CNNs and Transformers.

Another commenter highlights the difficulties and the high costs associated with training and exploring generative models. They mention the challenges of reviewing, rejecting, and finding convincing results with models many times larger than previously explored, as well as the need for proper exploration of domain differences and scaling.

In addition to the discussion about the size and potential limitations of outrageously large models, there are references to previous discussions on similar topics dating back to 2016 and 2017. Some commenters provide their perspectives on the feasibility and cost considerations of implementing such large models, with one commenter noting that a 137 billion parameter model would cost around $120K to train. Other commenters mention their experiences running smaller models, with one suggesting that a 30 billion parameter model can run on a decent laptop, while another notes the potential cost savings of quantization techniques.

Overall, the discussion revolves around the implications, feasibility, and potential drawbacks of outrageously large neural networks and the Sparsely-Gated Mixture-of-Experts (MoE) layer.

### The Onyx Programming Language

#### [Submission URL](https://onyxlang.io/) | 108 points | by [PaulHoule](https://news.ycombinator.com/user?id=PaulHoule) | [68 comments](https://news.ycombinator.com/item?id=38569608)

Introducing Onyx, a data-oriented, expressive, and modern programming language! Onyx aims to provide a streamlined and efficient programming experience with its sleek syntax and powerful features. The language is procedural but also allows for functional-inspired syntax using the pipe operator. With strict type-checking, Onyx ensures code integrity while also providing type inference capabilities for a more comfortable coding experience. 

One of the notable highlights of Onyx is its fast compilation. Built entirely in C, the Onyx compiler boasts incredibly rapid compilation times. In fact, the compiler was used to build the web-server for the Onyx website, and it completed the task in just 47 milliseconds. 

Onyx compiles solely to WebAssembly, making it compatible with various platforms and allowing developers to run their code using a built-in WebAssembly runtime or a WebAssembly runner like Wasmer or Wasmtime. The language also features built-in support for linking to native C-libraries using C-FFI. 

If you're interested in joining the Onyx community, you can find them on Discord, where you can chat about language features, discuss any problems you're facing, and showcase your projects. Onyx is an open-source project, so contributions from the community are always welcome and encouraged. To stay up to date with the latest news and updates, you can visit the Onyx GitHub repository or check out their news section on the website. 

Recent releases of Onyx have brought exciting features like MacOS support, an automated installer, switch expressions, WASIX support, and an enhanced networking layer. The team is continuously working on improving package management and the overall user experience of the language. 

So, if you're looking for a modern and efficient programming language, it might be worth giving Onyx a try!

The discussion on Hacker News about the submission on Onyx, a new programming language, includes various perspectives and opinions.

Some users express skepticism towards new programming languages, stating that most languages fall short in solving fundamental problems. They argue that creating a general-purpose programming language is a challenging task and many projects end up being disappointing.

Others discuss the benefits of Onyx's focus on WebAssembly (WASM), highlighting the ability to run code on various platforms and link to native C-libraries. They appreciate Onyx's fast compilation and mention recent additions like MacOS support and an enhanced networking layer.

There is also a discussion about the trade-offs of type inference in Onyx. While some appreciate type inference as it simplifies coding and reduces verbosity, others argue that explicit typing provides clarity and helps with code comprehension.

The discussion touches on the challenge of creating new programming languages and the complexity of balancing syntax, type systems, and runtimes. Some users mention existing languages that have attempted to solve these challenges, such as Solidity for Ethereum development and OpenSCAD for 3D modeling.

Overall, the discussion reflects different perspectives on programming languages, ranging from skepticism about the need for new languages to debates about type inference and the trade-offs of language design.

### The industries AI is disrupting are not lucrative

#### [Submission URL](https://www.theintrinsicperspective.com/p/excuse-me-but-the-industries-ai-is) | 68 points | by [snewman](https://news.ycombinator.com/user?id=snewman) | [86 comments](https://news.ycombinator.com/item?id=38575199)

In a recent article from The Intrinsic Perspective, the author takes a critical look at the current state of AI and its potential impact on industries. The article highlights Google's recently unveiled AI model, Gemini, which was showcased in a video demonstrating its abilities to interact with a questioner in real-time. However, the author argues that this video was staged, with pre-recorded frames sent to Gemini for a response. This leads to the larger point that the AI industry relies heavily on hype and large investments, but the industries they are disrupting are not necessarily lucrative. The article questions the audience for the GPT Store, a platform for AI apps, and suggests that the use cases mentioned, such as writing essays or digital art, may not generate significant profits. The author concludes by stating that while AI models like Gemini may be impressive in their capabilities, the industries they are disrupting may not offer substantial returns on investment.

The discussion on Hacker News revolves around various aspects of the article. Here are some key points raised by the commenters:

1. The first commenter agrees with the article that many people do not fully realize the impact of language models (LLMs) on businesses. They highlight how LLMs can handle classification and structuring tasks that would otherwise require thousands of human hours.

2. Another commenter elaborates on their experience using LLMs for helpdesk support and points out that while the approach may not always work perfectly, it can enhance productivity for support agents.

3. Some commenters express agreement with the article's critique of the hype around AI and its potential impact on industries. They argue that AI models like ChatGPT may not completely replace current systems and that the current interface of ChatGPT is marketed as a replacement for Google, which is hard to achieve.

4. The discussion also touches upon the potential disruption caused by LLMs in various industries. Examples mentioned include government contractors and junior analysts in the market research industry.

5. There is a debate on the accuracy and reliability of LLMs in tasks such as classification and combating spam. Some commenters highlight the limitations and false outputs of LLMs, while others discuss approaches and solutions to improve their performance.

6. One commenter emphasizes the psychological mechanism of stochastic parroting, where LLMs mimic and respond randomly like a parrot. They argue that LLMs cannot fully replace human judgement and experience.

7. The discussion also includes concerns about the AI industry being in a bubble and the potential negative effects if it bursts. Commenters express skepticism about the potential long-term impact of AI and its underlying technology.

8. Lastly, there are arguments about the role of software-based technologies in creating and bursting bubbles. Some commenters question the feasibility of preventing bubbles and whether technological advancements can deliver substantial promises.

### "vi – How do I exit Vim?" on stackoverflow viewed +3M times

#### [Submission URL](https://stackoverflow.com/questions/11828270/how-do-i-exit-vim) | 13 points | by [virskyfan](https://news.ycombinator.com/user?id=virskyfan) | [13 comments](https://news.ycombinator.com/item?id=38576082)

The top submission on Hacker News is a request for users to take a short survey to help improve Stack Overflow. The survey aims to gather feedback on various aspects of the platform. 

In other news, Stack Overflow has introduced a new feature called Collectives™, which allows users to find centralized and trusted content related to the technologies they use most. It also enables collaboration within specific technology communities. 

Additionally, Stack Overflow has launched Teams, a platform where users can ask and answer questions related to their work in a structured and easily searchable manner. 

Users can also get early access to new features through the Labs section of Stack Overflow. 

In terms of specific questions on the platform, one submission asks how to exit Vim, a notoriously "sticky" text editor. The question receives numerous responses, with suggestions including pressing the Escape key and typing ":q", using the command ":x" to save and quit, or using the command ":wq" to write and quit. The thread also provides other useful commands and tips for using Vim effectively. 

Overall, these top stories highlight Stack Overflow's efforts to improve user experience and provide valuable resources for developers and technology enthusiasts.

The discussion around the top submission involves users expressing their frustration with the question classification system on Stack Overflow. One user mentioned that they tried to search for an answer to a CS-related question but instead received search results unrelated to their query. They suggested that Stack Overflow should improve the search functionality. Another user responded, encouraging the original poster to click on the link provided in the comment to discuss their confusion and provide relevant information. 

In another discussion thread, a user asked a question about how to exit Vim, a famous text editor. One user replied with a simple command to remove Vim, while another user jokingly commented that they have been using Vim for 10 years and still don't know how to quit. 

A separate user commented that they often get distracted while customizing their Vimrc file and asked for tips on how to quickly quit Vim. Another user responded, mentioning studies that show Vim is harder to quit than other text editors. 

In another comment, a user mentioned that they appreciate the defaults of Vim and find it frustrating when they accidentally exit the program. 

There was also a comment mentioning a blog article from 2017 that reached 1 million views on Stack Overflow. 

Lastly, a user shared their frustration with accidentally quitting the virtual machine and having to restart it. Another user suggested using a command that kills all processes associated with the virtual machine. A further comment mentioned that switching to Busybox, a minimal Unix-like operating system, can sometimes solve common issues with running virtual machines.

### Google launched a new AI, and has already admitted at least one demo wasn't real

#### [Submission URL](https://www.theverge.com/2023/12/7/23992737/google-gemini-misrepresentation-ai-accusation) | 75 points | by [ronron4693](https://news.ycombinator.com/user?id=ronron4693) | [30 comments](https://news.ycombinator.com/item?id=38564359)

Google recently launched Gemini, its latest suite of AI models, but it has already faced criticism for a demonstration video that appears to be edited and not fully representative of the AI's capabilities. In the video, Gemini is shown responding quickly and accurately to prompts, but a disclaimer in the video description reveals that latency was reduced and outputs were shortened. According to a Bloomberg op-ed, Google admitted that the video used still image frames and text prompts rather than real-time spoken prompts. This is not the first time Google has faced scrutiny over video demos, as its Duplex demo was also questioned for lack of ambient noise and authenticity. The op-ed suggests that Google is "showboating" to distract from the fact that Gemini still lags behind OpenAI's GPT. Google, however, maintains that the video is real and serves to inspire developers. The op-ed concludes that Google should focus on letting journalists and developers experience the AI's capabilities directly rather than relying on edited videos.

The discussion on this submission includes various opinions and perspectives. Some commenters criticize Google for the edited demonstration video of Gemini, arguing that it misrepresents the AI's capabilities. They compare it to previous instances of Google facing scrutiny over video demos. Others express skepticism about the reliability and intelligence of AI models, stating that they cannot accurately predict real-world scenarios. There is also debate about the potential benefits and drawbacks of self-driving cars and personalized advertising. Some commenters suggest that personalized ads are a problem, while others believe they are a solution. There is a discussion about the Verge's coverage of Google and the relevance of personalized data. Additionally, there are comments questioning the authenticity of the video and the expectations set by Google's announcements. Overall, the discussion covers a range of topics related to Google's AI models and the ethical implications of AI technology.

