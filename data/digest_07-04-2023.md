## AI Submissions for Tue Jul 04 2023 {{ 'date': '2023-07-04T17:10:15.539Z' }}

### Tips for programmers to stay ahead of generative AI

#### [Submission URL](https://spectrum.ieee.org/ai-programming) | 303 points | by [mfiguiere](https://news.ycombinator.com/user?id=mfiguiere) | [286 comments](https://news.ycombinator.com/item?id=36586248)

In a recent article on IEEE Spectrum, Rina Diane Caballar discusses how coders can survive and thrive in a world dominated by generative artificial intelligence (AI). While AI-powered coding assistants have the potential to increase developers' productivity, experts argue that AI won't replace human programmers entirely. Caballar provides four tips for coders to remain relevant in an AI-centric coding environment.

Firstly, it's crucial to stick to the basics and best practices of programming. Despite AI's assistance with code completion and generation, developers still need to possess fundamental skills such as reading and reasoning about code. Understanding how code fits into a larger system and problem-solving abilities are highly valued in the field.

Secondly, good software engineering practices are essential. Planning system designs, software architectures, and using good abstractions are becoming even more valuable with the aid of AI-based tools. AI can better predict what code is needed next when developers establish a solid structure.

Next, developers should focus on developing their creativity and problem-solving skills. While AI may be capable of generating code, there's still a significant difference between what developers do and what AI outputs. The ability to approach problems creatively and find elegant solutions provides a unique value that AI cannot replicate.

Lastly, it's important not to get discouraged or compare oneself to AI. AI is a statistical output of a large model, while developers bring nuanced expertise and domain knowledge to their work. Recognizing the differences and embracing one's unique contributions can help developers maintain their relevance.

By following these tips, developers can navigate the changing landscape of AI and continue to thrive in their careers. While AI may augment certain aspects of coding, human coders still play a vital role in software development.

The discussion on this submission covered a range of topics related to AI and coding. Here are some of the main points:

- Some commenters expressed skepticism about AI's ability to replace human programmers entirely. They argued that AI-generated code still requires human intervention and that developers bring unique problem-solving abilities and domain knowledge to their work.
- Others shared their personal experiences with using AI-powered coding assistants like GPT-3 and found them helpful in certain tasks, such as solving logic puzzles and generating code snippets.
- There was a discussion about the potential copyright issues surrounding AI-generated code. Some commenters mentioned that derivative works created by AI may not be subject to copyright, while others pointed out that current copyright laws may not fully address the challenges posed by AI-generated content.
- The topic of productivity and management practices also came up. Some commenters expressed frustration with unproductive meetings and unnecessary managerial involvement, highlighting that AI may be more efficient in certain managerial functions.
- One commenter noted that while AI-powered tools like GPT can assist with completing code snippets, understanding APIs and documentation, and providing suggestions, there is still a need for strong fundamental programming skills and comprehension of business processes.

Overall, the discussion highlighted the potential of AI to enhance the coding process, but also emphasized the importance of human skills and expertise in software development.

### The Lone Banana Problem in AI

#### [Submission URL](https://www.digital-science.com/tldr/article/the-lone-banana-problem-or-the-new-programming-speaking-ai/) | 139 points | by [JohnHammersley](https://news.ycombinator.com/user?id=JohnHammersley) | [96 comments](https://news.ycombinator.com/item?id=36582937)

In this article, Daniel Hook, CEO of Digital Science, explores the potential biases present in Large Language Models (LLMs) and their impact on AI-generated content. He coined the phrase "Lone Banana Problem" to describe the subtle biases that can be difficult to detect. To illustrate this, Hook used an AI program called Midjourney to generate an image of a single banana casting a shadow on a grey background. However, even after refining his prompt, the AI consistently produced images with multiple bananas. This led Hook to question the biases embedded in the AI's training data and the need for a deeper understanding of these technologies. Despite the amusing nature of the "Lone Banana Problem," it raises important considerations about the limitations and potential pitfalls of AI language models.

The discussion on this submission revolves around several key points. 

One commenter points out that the AI program used in the demonstration, Midjourney, consistently rendered images with multiple bananas despite the prompt asking for a single banana. This suggests that the AI's training data may contain biases that lead it to generate incorrect representations.

Another commenter adds that the article raises existential questions about human intelligence and creativity compared to AI's pattern recognition abilities. They suggest that AI models lack understanding of objects and human senses, and may unintentionally contain biases and incorrect representations.

There is a separate discussion about communication problems. One commenter explains that a common communication problem is when people assume others share the same context, which can lead to misunderstandings. Another commenter suggests that spoon-feeding basic reasoning and providing explicit context can help mitigate this issue.

One commenter disagrees with the representation of the Lone Banana Problem, highlighting that the article misrepresents what the AI was trying to generate. They argue that the AI's rendering of multiple bananas was due to prompt engineering, not biases in the training data.

There is also a debate about the length and complexity of the article. Some argue that it is overly long and complex, while others defend its content and suggest that skimming is a common reading strategy.

One commenter points out that the article was written by an AI and criticizes its lack of coherence and missing details.

Another commenter brings up the point that biases in AI language models can be smartly-verified based on available data, implying that the biases can be controlled.

The discussion briefly touches on the limitations and potential pitfalls of AI models, the difficulties of interpreting and understanding them, and the problems related to training writers on data-proof texts.

Overall, the discussion covers a range of perspectives on biases in AI models, communication challenges, and the strengths and weaknesses of AI-generated content.

### Google's updated privacy policy states it can use public data to train its AI

#### [Submission URL](https://www.engadget.com/googles-updated-privacy-policy-states-it-can-use-public-data-to-train-its-ai-models-095541684.html) | 173 points | by [firstSpeaker](https://news.ycombinator.com/user?id=firstSpeaker) | [91 comments](https://news.ycombinator.com/item?id=36586170)

Google has recently updated its privacy policy to clarify that it can use publicly available data to train its AI models. The updated policy specifies that this data can be used to not only build features but also full products like Google Translate, Bard, and Cloud AI capabilities. By making this change, Google is notifying users that anything they publicly post online could be used to train its AI systems. This comes as critics have raised concerns about companies using personal data from the internet without consent to train their language models for generative AI use. OpenAI, for example, is facing a proposed class action lawsuit for allegedly scraping personal data without consent. As more companies develop generative AI products, similar lawsuits are likely to emerge in the future. In response to data scraping concerns, websites like Reddit have started charging access to their API, while Twitter has limited the number of tweets a user can see per day.

The discussion on this submission revolves around the topic of public versus private information and the legality of taking and publishing photos in public spaces. Some users argue that taking photos in public places is legally allowed and does not require explicit consent from individuals in the background. Others mention that there are restrictions in certain countries, such as Germany, where taking photos of vulnerable individuals or private locations without permission is prohibited. 

There is also a discussion about the privacy laws in different countries, particularly in the European Union (EU) and the United States (US). Users point out that there are substantial differences between the two regions, with some arguing that EU laws prioritize individual privacy rights more than the US. The debate expands to include factors such as population size, democratic systems, and economic considerations.

Another topic discussed is the responsibilities of companies like Google in handling public data and complying with legal requirements. One user highlights the case of Wikileaks, raising questions about the legality of accessing and processing public data without authorization. It is noted that if information remains classified or requires security clearances, accessing or disseminating it would be illegal.

### Chat-based Large Language Models replicate the mechanisms of a psychicâ€™s con

#### [Submission URL](https://softwarecrisis.dev/letters/llmentalist/) | 24 points | by [EventH-](https://news.ycombinator.com/user?id=EventH-) | [10 comments](https://news.ycombinator.com/item?id=36586540)

In a thought-provoking article, Baldur Bjarnason investigates the phenomenon of language models, specifically chat-based large language models (LLMs), being perceived as intelligent. Bjarnason argues that LLMs are not capable of reasoning or thinking like humans do, as they are merely mathematical models of language tokens. He presents two possible explanations for the intelligence illusion: either the tech industry has unintentionally developed a completely new kind of mind, or the illusion lies in the mind of the user. Bjarnason aligns himself with the latter camp, drawing a parallel between the intelligence illusion and the tactics used by psychics in cold reading. By employing validation statements and statistically probable guesses, both chatbots and psychics create the illusion of intelligence and specificity. Bjarnason suggests that the rise of chat-based LLMs has unintentionally resulted in an automation of the psychic con, where users are tricked into perceiving an intelligence that doesn't truly exist.

The discussion in the comments starts with a user expressing frustration with system administrators and suggesting that GPT-4, a large language model (LLM), might help solve system problems. Another user points out that defining intelligence is a subjective and fluff-filled discussion, and that minimizing attention given to doubts and definitions can hinder understanding.

Another commenter is impressed by LLMs' ability to understand instructions to some extent, giving an example of using LLMs for debugging. However, they argue that LLMs' capabilities are limited to statistical matching rather than true intelligence.

A user counters this argument by suggesting that intelligence can be learned through training data, mentioning the concept of predictive coding and the recursive identification of models within the human brain. They propose that LLMs could potentially reach a level of intelligence similar to humans.

Someone else brings up the objective view that LLMs simply produce text based on probabilistic matrices and do not actually possess intelligence. They argue that LLMs mimic structures but lack the complexity for reasoning and consciousness.

Another user agrees with this view, stating that the development of LLMs has not led to the invention of a new kind of mind, but rather the tech industry stumbled upon unknown principles and processes. They assert that intelligence lies in the mind of the user rather than the LLM itself.

Finally, there is a mention of the challenges scientists face in understanding the intricacies of the human brain. A user sarcastically remarks that people impressed with LLMs should reconsider granting machines rights, hinting at potential issues surrounding artificial intelligence and its impact on society. One user concludes the discussion by referring to "scientism," suggesting that there is an overreliance on science as an ideology.

### How the RWKV language model works

#### [Submission URL](https://johanwind.github.io/2023/03/23/rwkv_details.html) | 70 points | by [EvgeniyZh](https://news.ycombinator.com/user?id=EvgeniyZh) | [6 comments](https://news.ycombinator.com/item?id=36582120)

In this post, the author provides a detailed explanation of how RWKV (Recurrent Weighted Key-Value) generates text. RWKV is a language model that uses a weighted key-value mechanism to predict the next token in a sequence of text. The author presents a minimal implementation of a 430m parameter RWKV model in about 100 lines of code.

The code starts by importing necessary libraries such as NumPy, torch, and tokenizers. It defines functions for layer normalization, exponentiation, and sigmoid activation. Then, it defines two main functions: `time_mixing()` and `channel_mixing()`, which are used for the key-value mechanism in RWKV.

The `RWKV()` function takes the RWKV model, a token, and the current state as input. It performs the forward pass for the token through the RWKV layers, applying time mixing and channel mixing operations, and returns the probability distribution for the next token and the updated state.

The author then explains how to use the RWKV model for text generation. They load the pre-trained weights of the RWKV model from a file using `torch.load()`, and tokenize the input text using `tokenizer.encode()`. They initialize the state to zeros and iterate over each token in the input text, passing it through the RWKV function to update the state and get the probability distribution for the next token. They then sample a token from the probability distribution using the `sample_probs()` function and continue the generation process.

The author provides an example of generating text using RWKV by continuing the input text: "In a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese."

The post concludes by mentioning that larger models would perform better than the small 430m RWKV model used in the example.

Overall, the post provides a comprehensive explanation of how RWKV generates text and offers a minimal implementation to showcase its functionality.

The discussion on this submission includes several comments. 

One user, trprnm, mentions that linear attention can forget details and suggests exploring alternatives such as proper quadratic attention computations in transformer benchmarks, where the number of parameters might decrease. Another user, sebzim4500, agrees and adds that it is generally faster to use linear attention blocks with lower model dimensions, while attention blocks with higher model dimensions are slower due to their complexity.

Another user, Buttons840, comments that the 150-line implementation of RWKV mentioned in the post looks great and asks if anyone has a similarly small implementation of transformers. In response, user rnsr recommends checking out KrptHys' nanoGPT, which they believe is good.

User lmsshtm makes a statement about RWKV, mentioning that it's time to make a statement training a large model with 14B parameters and that the functions in this post are impressive. Another user, psttj, simply comments that RWKV stands for Receptance Weighted Key Value.

Overall, the comments discuss the implications and applications of RWKV, suggest alternative attention mechanisms, and mention other implementations for transformers.

### Autonomous robot to stave off spotted lanternflies

#### [Submission URL](https://www.cmu.edu/news/stories/archives/2023/june/cmu-team-develops-autonomous-robot-to-stave-off-spotted-lanternflies) | 34 points | by [geox](https://news.ycombinator.com/user?id=geox) | [20 comments](https://news.ycombinator.com/item?id=36580487)

Carnegie Mellon University's Robotics Institute has developed an autonomous robot called TartanPest to combat the spread of spotted lanternflies. These invasive insects are known for destroying crops, and TartanPest aims to detect and destroy their egg masses using computer vision and a robotic arm attached to an electric tractor. By investing in tackling this issue now, the team hopes to prevent further damage as the lanternflies are predicted to spread across the entire country. The robot uses deep learning models to identify and scrape off the egg masses, potentially benefiting small farmers and reducing chemical pollution and labor costs in the food system.

- Some users expressed skepticism about the effectiveness and cost of the robot, with one user questioning the need to spend millions on a research project that may not have a significant impact.
- Another user shared their personal experience with lanternflies in Pennsylvania, noting that they can be quite damaging and disruptive.
- The efficiency of the robot in reducing pest populations was also mentioned, with a user expressing interest in how it could potentially decrease the need for pesticide use.
- There was a brief discussion about API location extraction, with one user suggesting that it could be a useful tool in this context.
- The potential impact on human employment was raised, with one user suggesting that the robot may not be cost-effective compared to hiring human workers.
- The decline in tradesman work and the challenges faced by small business owners were also mentioned in the discussion.
- The high cost of production and potential issues with genetically modified breeding were raised as concerns.
- One user commented on the slow movement of robots and the need for feedback loops to improve their performance.
- The discussion ended with a user expressing approval of the submission.

Note: Some parts of the discussion were unclear or contained irrelevant information.

### A human just defeated an AI in Go. Here's why that matters

#### [Submission URL](https://www.zmescience.com/future/a-human-just-defeated-an-ai-in-go-heres-why-that-matters/) | 64 points | by [amadeuspagel](https://news.ycombinator.com/user?id=amadeuspagel) | [21 comments](https://news.ycombinator.com/item?id=36590242)

In a surprising turn of events, a human has defeated an AI in the complex game of Go. Go is considered one of the most intricate games ever created, with an almost unfathomable number of possible moves. AI has proven to be a formidable opponent in the past, but humans have now learned to exploit its weaknesses. Researchers trained their own AI opponents to trick the reigning AI champion, KataGo, and amateur player Kellin Pelrine managed to beat KataGo 14 out of 15 times. This outcome highlights an important lesson for the future of artificial intelligence: high performance doesn't always guarantee robustness. Even the most advanced AI systems can have blind spots or vulnerabilities, which is crucial to consider as AI technology becomes increasingly integrated into real-world applications. By studying these flaws and exploits in game-playing AI, we can gain insights into how these algorithms behave and better understand the potential risks and limitations of AI in the real world.

The discussion surrounding the submission revolves around various topics related to the defeat of an AI by a human in the game of Go. Some comments draw parallels to other games like chess and CSGO, highlighting the importance of strategy and skill level. There is also a mention of how memory retention can impact gameplay. The discussion then delves into the vulnerabilities and flaws of AI systems, with references to Murphy's Law and the greater threat posed by human cognition. The exploit found by Kellin Pelrine in defeating KataGo is noted, along with comparisons to previous AI victories. The article's emphasis on the significance of AI weaknesses and the potential risks and limitations of AI in real-world applications is also acknowledged. Some comments explore the nature of AI training and the need for a deeper understanding of specific tactics. There is also a discussion about AI's ability to extrapolate and the possibility of AI making legally binding decisions. Overall, the discussion highlights the complexities of AI and the need for further exploration and understanding.

### GPT-4 is great at infuriating telemarketing scammers

#### [Submission URL](https://www.theregister.com/2023/07/03/jolly_roger_telephone_company/) | 139 points | by [isaacfrond](https://news.ycombinator.com/user?id=isaacfrond) | [87 comments](https://news.ycombinator.com/item?id=36583969)

In a refreshing twist on AI implementation, a California man has created a business that uses chatbots to frustrate telemarketing scammers. The Jolly Roger Telephone Company offers customers the ability to merge their calls with chatbots that engage the scammers in bizarre and nonsensical conversations, ultimately wasting their time. The company has a range of bots available, each with their own unique voice and quirks. Not only does this business provide entertainment for those annoyed by telemarketers, it also serves as an effective tool against scammers. The Jolly Roger Telephone Company has thousands of subscribers paying $23.80 a year for the service.

The discussion on the Hacker News submission revolves around various aspects of telemarketing scams and the use of chatbots to counter them. Some commenters discuss the technical implementation and efficacy of using chatbots to frustrate scammers, while others highlight the potential ethical and legal concerns. 

One commenter suggests using machine learning or AI-powered call blockers instead of chatbots, mentioning that the computational resources wasted on engaging with scammers could be better used elsewhere. Another commenter argues that relying on external networks and third-party services for call-blocking could pose security risks and may not be ideal from a privacy standpoint. 

There is also a discussion on the terminology used, particularly the term "telemarketing," with some pointing out that it traditionally refers to phone calls and not online advertising or billboards. The conversation diverges into debates about free speech, the regulation of advertising, and the banning of billboards in certain states. 

Commenters express concerns about the limitations and potential abuse of AI-powered systems, such as flooding emergency services with fake calls or spamming local businesses with negative reviews. Some also discuss the challenges of identity verification and the potential for AI-powered bots to handle sensitive information in the future.

The discussion ends with a couple of comments highlighting examples of AI implementation in other contexts and imagining the potential consequences of widespread adoption of AI.

### OpenAI temporarily disables the Browse with Bing beta feature

#### [Submission URL](https://help.openai.com/en/articles/8077698-how-do-i-use-chatgpt-browse-with-bing-to-search-the-web) | 132 points | by [gpayan](https://news.ycombinator.com/user?id=gpayan) | [110 comments](https://news.ycombinator.com/item?id=36582430)

No, I'm sorry. The information you provided does not answer the question on how to use ChatGPT Browse with Bing to search the web. It only mentions that the feature has been disabled temporarily due to some issues.

The discussion revolves around various aspects of ChatGPT and its limitations. Some users express frustration with the AI's responses and its inability to provide accurate or desired answers. There are discussions about AI safety measures and concerns about the negative impact AI could have on people's lives. Other users discuss the challenges in training and filtering AI models to prevent harmful or unwanted behaviors. The conversation also touches on the naming and control of AI technology, as well as the limitations of current hardware capabilities. Some users express concerns about AI development and the potential for centralized control of technology. Additionally, there are discussions about the resources and computational power required for AI training. Overall, the discussion covers a range of concerns and perspectives regarding the use and limitations of ChatGPT.

