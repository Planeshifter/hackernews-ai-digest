## AI Submissions for Sun Jun 04 2023 {{ 'date': '2023-06-04T17:11:31.694Z' }}

### It’s infuriatingly hard to understand how closed models train on their input

#### [Submission URL](https://simonwillison.net/2023/Jun/4/closed-model-training/) | 294 points | by [simonw](https://news.ycombinator.com/user?id=simonw) | [199 comments](https://news.ycombinator.com/item?id=36187994)

Large language models like GPT-3 and Google's PaLM and PaLM 2 are raising concerns about the transparency of their training data. The builders of these closed models are not revealing what's in their training data, which makes it difficult to know if private data is being used to train future versions of these models. OpenAI has a policy regarding data submitted by customers via their API, but it's unclear how data is being used for ChatGPT itself. There's a risk that an AI vendor might log inputs to their models, suffer from a security flaw, and expose that data to attackers. While companies have been trusting their private data to cloud providers like AWS and Google Cloud for years, these AI companies have much less of a track record for staying secure.

According to the discussion, OpenAI has a policy regarding data submitted by customers, but it is unclear how data is being used for ChatGPT itself. The risk of using private data to train these models and the possibility of a security flaw exposing that data to attackers is causing concern for researchers and consumers. The lack of transparency in these models is leading to suspicions that AI vendors might disclose trade secrets and commercial interests of their customers. Additionally, there is a debate on whether these models infringe on copyrighted material, such as artwork or photography. The discussion concludes with points on the complexity of understanding the training data and the benchmark results of LLM models.

### Reverse Engineering Self-Supervised Learning

#### [Submission URL](https://arxiv.org/abs/2305.15614) | 82 points | by [headalgorithm](https://news.ycombinator.com/user?id=headalgorithm) | [16 comments](https://news.ycombinator.com/item?id=36184838)

A group of researchers has conducted a study aimed at better understanding the mechanisms behind self-supervised learning (SSL), a technique used to train machine learning models without the need for labeled data. The team found that SSL inherently clusters samples with respect to semantic labels, leading to enhanced downstream classification and compressed data information. The researchers also discovered that SSL-trained representations align more closely with semantic classes rather than random classes, and that this alignment increases during training and deeper within the network. The study provides valuable insights into how SSL learns representations and its impact on performance across different sets of classes.

The comments mainly discuss the validity of machine learning and modern AI, with some users being skeptical about its efficiency in recognizing semantic and non-semantic data. There is also a discussion about the appropriateness of the paper's title and the use of flashy names. However, some users appreciate the study and expect to learn from it. The thread also includes a conversation about the importance of empirical studies and the effectiveness of various methods in data analysis.

### The AI firm that conducted ‘state surveillance’ of your social media posts

#### [Submission URL](https://www.telegraph.co.uk/news/2023/06/03/logically-ai-firm-social-media-posts-covid/) | 84 points | by [perihelions](https://news.ycombinator.com/user?id=perihelions) | [17 comments](https://news.ycombinator.com/item?id=36185052)

The UK government has paid over £1.2m of taxpayers' money to AI company Logically to analyse "disinformation" and "misinformation" on social media. Logically's technology ingests material from hundreds of thousands of media sources and public posts on major social media platforms to identify potentially problematic posts. The company claims to have "one of the world’s largest dedicated fact-checking teams", spread across the UK, Europe and India. Logically has been contracted by the Department for Culture, Media and Sport to deliver "analytical support" and to support cross-Government efforts to curtail potentially harmful misinformation. Facebook has also granted Logically influence over the content other people may view through a partnership that allows the site to reduce the distribution of posts labelled false by Logically, which has sparked concerns among campaigners for freedom of speech. Commenters debated issues such as the legitimacy of government intervention on the public's speech and the potential for algorithms to generate propaganda themselves. Some suggested alternatives like decentralized algorithms and regulation of algorithmic bias.

### We need to keep CEOs away from AI regulation

#### [Submission URL](https://www.ft.com/content/5f8b74f7-68b1-4a6c-88bf-d0dd03579149) | 50 points | by [belter](https://news.ycombinator.com/user?id=belter) | [8 comments](https://news.ycombinator.com/item?id=36188047)

The Financial Times published an opinion piece arguing that CEOs should not be the ones regulating AI and that it should be left to experts in the field. The article highlights that CEOs may have a greater focus on profits and shareholder value rather than societal implications, and that AI regulation needs to be approached in a way that prioritizes public benefit and ethics. The piece also discusses the potential risks of biased AI algorithms and the need for diverse perspectives in the development and regulation of AI.

The discussion in the comments includes a range of opinions on the topic. Some users argue that CEOs should not have control over AI regulation and stress the importance of transparency and diverse perspectives. Other users suggest that the private sector should be trusted to regulate AI and that concerns about potential risks are overblown. Some users also discuss the need for privacy legislation like GDPR and the potential misuse of AI technology for profit. One user shares a video discussing the financial opportunities of AI. Another user mentions concerns about how companies like Instagram are using AI technology for profit.

### GitHub Private Repos Considered Private-­Ish

#### [Submission URL](https://tylercipriani.com/blog/2023/03/31/private-ish-github-repos/) | 160 points | by [fagnerbrack](https://news.ycombinator.com/user?id=fagnerbrack) | [140 comments](https://news.ycombinator.com/item?id=36184948)

This week, GitHub.com's RSA SSH private key was briefly exposed in a public GitHub repository. Private repositories can become public in various ways, such as publishing the .git directory, getting phished, or accidentally clicking the wrong button. Mitigations include keeping the .git directory private, setting up two-factor authentication, and auditing access control. However, the best solution is to avoid putting sensitive data in private repositories altogether. Developers can use secret scanners as pre-commit git hooks and inject secrets into applications at runtime. The discussion includes comments related to Microsoft's reputation, the theft of intellectual property, and the need for privacy statements in AI training programs. Overall, the discussion revolves around best practices for protecting sensitive data and intellectual property.

### AI Report #4: AutoGPT And Open-source lags behind Part 2

#### [Submission URL](https://theaireport.substack.com/p/ai-report-4-autogpt-and-open-source) | 57 points | by [primordialsoup](https://news.ycombinator.com/user?id=primordialsoup) | [35 comments](https://news.ycombinator.com/item?id=36186348)

In the latest AI Report, it is noted that open-source has been lagging behind in AI development, and there are two directions that need improvement: training a better base LLM and getting better at RLHF. Meanwhile, AutoGPT, which has 136k stars in GitHub, and similar repository BabyAGI, which has 14k stars, have not produced anything concrete and there has been some over-promising on their capabilities. The report suggests that more research is needed in this area to achieve success, and perhaps building agents that excel at specific, well-defined tasks before composing them together may be the way forward. The report also features papers on improving language models and verifying training methods. The comments section discusses various experiences with AI tools and APIs, and some users suggest focusing on low-level abstractions and tool-making patterns to make the development of AI more accessible. Users also point out the limitations and challenges of current AI technology.

### Rambling about Microsoft, and testing alternatives to GitHub Copilot

#### [Submission URL](https://poignardazur.github.io//2023/06/04/microsoft-copilot-alternatives/) | 12 points | by [PoignardAzur](https://news.ycombinator.com/user?id=PoignardAzur) | [4 comments](https://news.ycombinator.com/item?id=36184860)

Today's top story on Hacker News is a post about Microsoft's recent release of Copilot. The author discusses how Microsoft's decision to keep the project proprietary and use open-source projects to train the AI may cost them more than if they had released Copilot in a more open way. This led the author to question how viable the alternatives to Copilot are and the quality of their AI. The author also explores the controversy surrounding the training of Copilot and other AI models, discussing the ethics of using public domain data without informed consent. The author's hot take is that AI developers should not be expected to ask for consent whenever they use public domain data, to incentivize people to create more stuff.

