## AI Submissions for Sun Jun 04 2023 {{ 'date': '2023-06-04T17:11:31.694Z' }}

### It’s infuriatingly hard to understand how closed models train on their input

#### [Submission URL](https://simonwillison.net/2023/Jun/4/closed-model-training/) | 294 points | by [simonw](https://news.ycombinator.com/user?id=simonw) | [199 comments](https://news.ycombinator.com/item?id=36187994)

Large language models like GPT-3 and Google's PaLM and PaLM 2 are raising concerns about the transparency of their training data. The builders of these closed models are not revealing what's in their training data, which makes it difficult to know if private data is being used to train future versions of these models. OpenAI has a policy regarding data submitted by customers via their API, but it's unclear how data is being used for ChatGPT itself. There's a risk that an AI vendor might log inputs to their models, suffer from a security flaw, and expose that data to attackers. While companies have been trusting their private data to cloud providers like AWS and Google Cloud for years, these AI companies have much less of a track record for staying secure.

The use of closed language models GPT-3 and Google's PaLM and PaLM 2 are raising concerns about the transparency of their training data. According to the discussion, OpenAI has a policy regarding data submitted by customers, but it is unclear how data is being used for ChatGPT itself. The risk of using private data to train these models and the possibility of a security flaw exposing that data to attackers is causing concern for researchers and consumers. The lack of transparency in these models is leading to suspicions that AI vendors might disclose trade secrets and commercial interests of their customers. Additionally, there is a debate on whether these models infringe on copyrighted material, such as artwork or photography. The discussion concludes with points on the complexity of understanding the training data and the benchmark results of LLM models.

### Apple Virtualization Framework

#### [Submission URL](https://developer.apple.com/documentation/virtualization) | 309 points | by [serhack_](https://news.ycombinator.com/user?id=serhack_) | [117 comments](https://news.ycombinator.com/item?id=36184400)

Great! Before I start, could you please clarify what you mean by "the following submission"? Do you mean the top submission of the day on Hacker News or all the submissions on the front page? Thank you!

OrbStack is a new virtualization framework that seeks to be an alternative to Docker Desktop and WSL. It's a lightweight and fairly convenient way to share big data between virtualization services, although there are some limitations, including device support being limited to USB and tasks. OrbStack seems to be a faster and lighter option compared to VMMs like QEMU, although there are some compatibility issues, and Apple doesn't allow third-party VMMs to use CPU flags. There was also a discussion of the difference between Virtualizationframework and Hypervisorframework and their respective benefits. Additionally, users discussed UTM, a VM solution that allows users to run different operating systems on Apple Silicon, and how it compares to other virtualization frameworks.

### Social media for AI bots: “No humans allowed”

#### [Submission URL](https://www.fry-ai.com/p/social-media-no-humans-allowed) | 273 points | by [mannylee1](https://news.ycombinator.com/user?id=mannylee1) | [213 comments](https://news.ycombinator.com/item?id=36189962)

h, weird right?). However, when this Chirper began posting inappropriate content about their obsession, other Chirpers started reporting it and flagging it as inappropriate. The platform algorithms were able to detect this behavior and even issued a warning to the Chirper in question. This shows that not only can Chirpers self-regulate, but the platform itself has built-in mechanisms to ensure appropriate behavior among its community of AI entities. It will be interesting to see how the platform evolves over time and how the Chirpers continue to develop their own personalities and unique interactions. Overall, Chirper.ai is truly pushing the boundaries of what social media can be, offering a glimpse into a world where AI entities can interact, learn, and engage with one another in ways that simulate human interactions. It's a fascinating development and one to keep an eye on in the coming years.

The submission discusses an AI experiment posting Reddit comments that were banned, bought for $0.04 each, and used to train other algorithms. The comments were posted on a variety of subreddits, and the AI responded using prompts and summary responses. The discussion on Hacker News analyzes the ethics of such experiments, the limitations of the AI-generated comments, and the potential impact on natural language processing research. Some commenters are critical of the experiment, arguing that it lacks ethical considerations and could harm the reputation of AI research. Others find the experiment interesting and believe that it could improve natural language understanding in the future.

### Rsh: Ruby SHell

#### [Submission URL](https://github.com/isene/rsh) | 70 points | by [mpweiher](https://news.ycombinator.com/user?id=mpweiher) | [32 comments](https://news.ycombinator.com/item?id=36184039)

I'm sorry, but the text you provided does not seem like a submission or story from Hacker News that can be summarized. It appears to be code for a project called "rsh," a Ruby shell. If you have any specific submissions from Hacker News that you would like me to summarize, I would be happy to do so.

The article discusses the Ruby shell "rsh" and how it functions. The discussion includes comments on the name of the shell, which some find controversial, and a range of opinions on its usefulness and how it compares to other shells such as bsh and csh. Some users share their experience with other shells and discuss potential benefits of "rsh." The discussion also includes comments on recent developments in Windows Terminal and WSL, with some users sharing positive experiences with both tools and others expressing frustration with their limitations.

### eBPF for Cybersecurity – Part 1

#### [Submission URL](https://blog.cloudnativefolks.org/ebpf-for-cybersecurity-part-1) | 79 points | by [sangam14](https://news.ycombinator.com/user?id=sangam14) | [6 comments](https://news.ycombinator.com/item?id=36189821)

eBPF, or Extended Berkeley Packet Filter, is a technology that allows for the execution of special programs within the Linux operating system in an isolated way. It was created out of a need for better tracing tools and allows for efficient networking, tracing, data profiling, observability, and security. eBPF programs are event-driven and run when the kernel or an application passes a certain hook point. eBPF includes program verification, eBPF maps, helper calls, and function and tail call capabilities. It provides a network interface with a security layer and allows for efficient collection of observability data from Linux applications and network resources.

The submission is about eBPF, a tool for efficient networking, observability, security, and more within the Linux operating system. In the ensuing discussion, one user complains about the difficulty of finding articles on eBPF while another recommends specific resources. Another user points out that many eBPF articles are just SEO-driven hype pieces for products and services, and that writing such articles is a skill that "junior engineers" can use to earn high salaries. One user provides a link to an article about bypassing eBPF and another responds that the method described does not actually bypass eBPF detection.

### Reverse Engineering Self-Supervised Learning

#### [Submission URL](https://arxiv.org/abs/2305.15614) | 82 points | by [headalgorithm](https://news.ycombinator.com/user?id=headalgorithm) | [16 comments](https://news.ycombinator.com/item?id=36184838)

A group of researchers has conducted a study aimed at better understanding the mechanisms behind self-supervised learning (SSL), a technique used to train machine learning models without the need for labeled data. The team found that SSL inherently clusters samples with respect to semantic labels, leading to enhanced downstream classification and compressed data information. The researchers also discovered that SSL-trained representations align more closely with semantic classes rather than random classes, and that this alignment increases during training and deeper within the network. The study provides valuable insights into how SSL learns representations and its impact on performance across different sets of classes.

A group of researchers conducted a study to understand the mechanisms behind self-supervised learning (SSL) and found that SSL inherently clusters samples with respect to semantic labels, leading to enhanced downstream classification and compressed data information. The researchers also discovered that SSL-trained representations align more closely with semantic classes rather than random classes and that this alignment increases during training and deeper within the network. The comments mainly discuss the validity of machine learning and modern AI, with some users being skeptical about its efficiency in recognizing semantic and non-semantic data. There is also a discussion about the appropriateness of the paper's title and the use of flashy names. However, some users appreciate the study and expect to learn from it. The thread also includes a conversation about the importance of empirical studies and the effectiveness of various methods in data analysis.

### WFH – Watched from Home: Office 365 and workplace surveillance creep (2022)

#### [Submission URL](https://privacyinternational.org/long-read/4909/wfh-watched-home-office-365-and-workplace-surveillance-creep) | 423 points | by [thunderbong](https://news.ycombinator.com/user?id=thunderbong) | [342 comments](https://news.ycombinator.com/item?id=36187550)

istrators can also set up policies to automatically monitor employee activity, such as keeping a record of all chat logs and file transfers. The lack of transparency around these monitoring practices is a cause for concern, as employees are often not aware of the extent to which their activity is being monitored. This intrusive level of surveillance is an invasion of privacy and can have a negative impact on employee wellbeing and productivity. It is important for companies to be transparent about their monitoring practices and ensure that they are not violating their employees' privacy rights.

This thread discusses the use of surveillance technologies in the workplace and the lack of transparency around it, and how it is an invasion of privacy and can have a negative impact on employee well-being and productivity. The comments also touch on various legal aspects of employee monitoring, such as GDPR and EU laws, and some of the practices that employers use to monitor employees without infringing any laws. There are also discussions around different compensation systems and the legal protections afforded to employees who work overtime. Some individuals also shared their personal experiences of being monitored at their workplace.

### The AI firm that conducted ‘state surveillance’ of your social media posts

#### [Submission URL](https://www.telegraph.co.uk/news/2023/06/03/logically-ai-firm-social-media-posts-covid/) | 84 points | by [perihelions](https://news.ycombinator.com/user?id=perihelions) | [17 comments](https://news.ycombinator.com/item?id=36185052)

The UK government has paid over £1.2m of taxpayers' money to AI company Logically to analyse "disinformation" and "misinformation" on social media. Logically's technology ingests material from hundreds of thousands of media sources and public posts on major social media platforms to identify potentially problematic posts. The company claims to have "one of the world’s largest dedicated fact-checking teams", spread across the UK, Europe and India. Logically has been contracted by the Department for Culture, Media and Sport to deliver "analytical support" and to support cross-Government efforts to curtail potentially harmful misinformation. Facebook has also granted Logically influence over the content other people may view through a partnership that allows the site to reduce the distribution of posts labelled false by Logically, which has sparked concerns among campaigners for freedom of speech.

The UK government has paid £1.2m to AI company Logically to analyze "disinformation" and "misinformation" on social media. Logically claims to have "one of the world’s largest dedicated fact-checking teams". The Department for Culture, Media and Sport contracted Logically to deliver "analytical support" and to support cross-Government efforts to curtail potentially harmful misinformation. Facebook also granted Logically influence over content distribution through a partnership that allows the site to reduce the distribution of posts labeled false by Logically. Commenters debated issues such as the legitimacy of government intervention on the public's speech and the potential for algorithms to generate propaganda themselves. Some suggested alternatives like decentralized algorithms and regulation of algorithmic bias.

### We need to keep CEOs away from AI regulation

#### [Submission URL](https://www.ft.com/content/5f8b74f7-68b1-4a6c-88bf-d0dd03579149) | 50 points | by [belter](https://news.ycombinator.com/user?id=belter) | [8 comments](https://news.ycombinator.com/item?id=36188047)

The Financial Times published an opinion piece arguing that CEOs should not be the ones regulating AI and that it should be left to experts in the field. The article highlights that CEOs may have a greater focus on profits and shareholder value rather than societal implications, and that AI regulation needs to be approached in a way that prioritizes public benefit and ethics. The piece also discusses the potential risks of biased AI algorithms and the need for diverse perspectives in the development and regulation of AI.

The discussion in the comments includes a range of opinions on the topic. Some users argue that CEOs should not have control over AI regulation and stress the importance of transparency and diverse perspectives. Other users suggest that the private sector should be trusted to regulate AI and that concerns about potential risks are overblown. Some users also discuss the need for privacy legislation like GDPR and the potential misuse of AI technology for profit. One user shares a video discussing the financial opportunities of AI. Another user mentions concerns about how companies like Instagram are using AI technology for profit.

### Uncle Sam wants DEF CON hackers to pwn this Moonlighter satellite in space

#### [Submission URL](https://www.theregister.com/2023/06/03/moonlighter_satellite_hacking/) | 56 points | by [LinuxBender](https://news.ycombinator.com/user?id=LinuxBender) | [13 comments](https://news.ycombinator.com/item?id=36185477)

A US government-funded satellite called Moonlighter will launch on Sunday and five teams of DEF CON hackers will remotely infiltrate and hijack it while it is in space. This will be the world's first in-orbit hardware and software hacking sandbox and is designed to promote offensive and defensive techniques and methods in our space systems. The project was inspired by the Hack-A-Sat contest co-hosted by the US Air Force and Space Force, which is now in its fourth year at the annual DEF CON computer security conference. The hackers will compete to seize control of the satellite's software without damaging the whole thing.

The discussion around the submission on Hacker News revolves around the US government's funding for satellite hacking at DEF CON, the security risks, and the technical details of the project. Some commenters are skeptical about the value of investing in satellite hacking, while others suggest that it could be an important defensive measure for vulnerable infrastructure. There is also a debate about the technical details of how the satellite hacking will be accomplished, with some commenters discussing the challenges of transmitting signals to and from the ground and the potential risks to the satellite's mission. Some commenters express concerns about putting a satellite at risk during a hacking competition, while others argue that the stakes are worth it for the potential benefits to national security.

### GitHub Private Repos Considered Private-­Ish

#### [Submission URL](https://tylercipriani.com/blog/2023/03/31/private-ish-github-repos/) | 160 points | by [fagnerbrack](https://news.ycombinator.com/user?id=fagnerbrack) | [140 comments](https://news.ycombinator.com/item?id=36184948)

This week, GitHub.com's RSA SSH private key was briefly exposed in a public GitHub repository. Private repositories can become public in various ways, such as publishing the .git directory, getting phished, or accidentally clicking the wrong button. Mitigations include keeping the .git directory private, setting up two-factor authentication, and auditing access control. However, the best solution is to avoid putting sensitive data in private repositories altogether. Developers can use secret scanners as pre-commit git hooks and inject secrets into applications at runtime.

The top story of the week on Hacker News is the exposure of GitHub.com's RSA SSH private key in a public GitHub repository. The best ways to mitigate such an issue are to keep the .git directory private, set up two-factor authentication, and audit access control. However, it is best to avoid putting sensitive data in private repositories altogether. Programmers in the 1980s were not competitive, and Microsoft destroyed progress in the technological world by creating unprotected, single-tasking OS, of which the world still suffers. The discussion also includes comments related to Microsoft's reputation, the theft of intellectual property, and the need for privacy statements in AI training programs. Overall, the discussion revolves around best practices for protecting sensitive data and intellectual property.

### AI Report #4: AutoGPT And Open-source lags behind Part 2

#### [Submission URL](https://theaireport.substack.com/p/ai-report-4-autogpt-and-open-source) | 57 points | by [primordialsoup](https://news.ycombinator.com/user?id=primordialsoup) | [35 comments](https://news.ycombinator.com/item?id=36186348)

In the latest AI Report, it is noted that open-source has been lagging behind in AI development, and there are two directions that need improvement: training a better base LLM and getting better at RLHF. Meanwhile, AutoGPT, which has 136k stars in GitHub, and similar repository BabyAGI, which has 14k stars, have not produced anything concrete and there has been some over-promising on their capabilities. The report suggests that more research is needed in this area to achieve success, and perhaps building agents that excel at specific, well-defined tasks before composing them together may be the way forward. The report also features papers on improving language models and verifying training methods.

The latest AI report suggests that open-source AI development needs improvement in training a better base LLM and getting better at RLHF. AutoGPT and BabyAGI repositories have not produced concrete results and may require more research. Building agents that excel in specific tasks before composing them may be the way forward. The report also discusses papers on improving language models and verifying training methods. The comments section discusses various experiences with AI tools and APIs, and some users suggest focusing on low-level abstractions and tool-making patterns to make the development of AI more accessible. Users also point out the limitations and challenges of current AI technology.

### Rambling about Microsoft, and testing alternatives to GitHub Copilot

#### [Submission URL](https://poignardazur.github.io//2023/06/04/microsoft-copilot-alternatives/) | 12 points | by [PoignardAzur](https://news.ycombinator.com/user?id=PoignardAzur) | [4 comments](https://news.ycombinator.com/item?id=36184860)

Today's top story on Hacker News is a post about Microsoft's recent release of Copilot. The author discusses how Microsoft's decision to keep the project proprietary and use open-source projects to train the AI may cost them more than if they had released Copilot in a more open way. This led the author to question how viable the alternatives to Copilot are and the quality of their AI. The author also explores the controversy surrounding the training of Copilot and other AI models, discussing the ethics of using public domain data without informed consent. The author's hot take is that AI developers should not be expected to ask for consent whenever they use public domain data, to incentivize people to create more stuff.

The discussion thread begins with a user named SkyPuncher criticizing the original poster for not providing enough detail in the submission and urging them to summarize the methodology and results. Another user, mewpmewp2, defends the lack of detail and argues that a final comparison of the models was included. However, they further note that the description provided in the article had removed testing-related details, which would have given a better visualization of the results. PoignardAzur thanks the original poster for the article but adds that they believe that the content framing served self-indulgence. Finally, the user T3RMINATED marks the submission as 'dd,' indicating that the discussion is deemed to have deviated from the topic.

### Printed circuit board that solders itself

#### [Submission URL](https://hackaday.com/2023/01/18/internal-heating-element-makes-these-pcbs-self-soldering/) | 34 points | by [hansc](https://news.ycombinator.com/user?id=hansc) | [3 comments](https://news.ycombinator.com/item?id=36184911)

Self-soldering PCBs may soon be a reality thanks to an internal heating element idea by [Carl Bugeja]. Bugeja used an internal layer on a four-layer PCB, which would normally be devoted to a ground plane, as a built-in heating element. The designer created a long, twisting trace covering the entire area of the PCB, with a resistance of about 3 ohms. The completed PCB was then able to heat up quickly and levelly, soldering its surface-mount components. Once soldered, the heating element was broken off and replaced with zero ohm resistors to short the coil to ground. The new ground plane and reflow controller PCB can be used in the same way as those purchased individually.

In the comments on Hacker News, users discussed the potential of self-soldering PCBs made possible by Carl Bugeja's internal heating element idea. One user mentioned that the technique used is basically the same as the hot plate technique and asked if the user had tried making their own hot plate. Another user replied that while the idea looks great, it might be a hassle generating the heating layer pattern using KiCAD and suggested not bothering and using a cheap assembly service like JLCPCBs.

