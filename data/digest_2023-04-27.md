## AI Submissions for Thu Apr 27 2023 {{ 'date': '2023-04-27T18:00:50.664Z' }}

### Hidet: A Deep Learning Compiler for Efficient Model Serving

#### [Submission URL](https://pytorch.org/blog/introducing-hidet/) | 108 points | by [ashvardanian](https://news.ycombinator.com/user?id=ashvardanian) | [14 comments](https://news.ycombinator.com/item?id=35737284)

Introducing Hidet: A Deep Learning Compiler for Efficient Model Serving by Team Hidet showcases the new Hidet deep learning compiler for PyTorch that simplifies the process of implementing high-performing deep learning operators on modern accelerators like NVIDIA GPUs. Hidet is easy to integrate into PyTorch and is an attractive option for PyTorch users who want to improve inference performance of their models. The blog post also provides a sample script to use Hidet to compile and optimize a pre-trained ResNet50 model from torchvision, and an example of how to implement a naive matrix multiplication using Hidet Script and integrate it as a PyTorch operator.

In the comments, users discuss the performance of Hidet compared to other compilers and frameworks like TensorRT, PyTorch Eager, and Triton. Some users highlight the benefits of Hidet Script, the domain-specific language that allows for high flexibility and expression of optimizations. Additionally, users bring up the relevance of benchmarks and the ability to create custom operators with Hidet Script. The discussion also includes technical issues and bugs that users have encountered with Hidet.

### Even Apple employees hate Siri and are skeptical of its future, new report says

#### [Submission URL](https://9to5mac.com/2023/04/27/apple-employees-siri-struggles/) | 395 points | by [carlycue](https://news.ycombinator.com/user?id=carlycue) | [411 comments](https://news.ycombinator.com/item?id=35730075)

A new report from The Information paints a daunting picture of the chaos and internal strife inside Apple's Siri and AI teams. According to more than three dozen former employees who spoke with the publication, "organizational dysfunction and a lack of ambition" have hindered Apple's efforts to improve Siri and its underlying technology, leading to the company falling further behind competitors like OpenAI, Microsoft, and Google. Furthermore, Apple lost three of its Siri engineers to Google, and the Siri team remains widely derided by current employees. Despite some efforts to improve the platform, this report suggests that there is much work to be done for Siri to catch up with its rivals.

The discussion on Hacker News focused on the flaws of Siri, including its speech recognition technology and its lack of understanding of some basic phrases. Some users also discussed the use of third-party keyboards and the limitations of adapting to different languages for personal assistants.

### Text-to-Audio Generation Using Instruction Tuned LLM and Latent Diffusion Model

#### [Submission URL](https://tango-web.github.io/) | 35 points | by [GaggiX](https://news.ycombinator.com/user?id=GaggiX) | [5 comments](https://news.ycombinator.com/item?id=35737151)

Researchers from the DeCLaRe Lab at the Singapore University of Technology and Design have developed a text-to-audio (TTA) generation AI called TANGO that uses an "instruction-tuned LLM" as a text encoder for better performance. TANGO outperforms the state-of-the-art AudioLDM on most metrics and stays comparable on the rest on AudioCaps test set, despite being trained on a smaller dataset. TANGO generates text-conditional sound effects, including human speech and music. While TANGO has limitations in terms of fine control of the generated audio, the team plans to improve it by training it on larger datasets. The code and model checkpoints have been released for reproducibility.

The discussion on this submission mostly involves appreciation for the technology and some additional insights on its capabilities. One user commends the researchers for their excellent work and also shares some resources featuring practical videos and high-level reviews. Another user expresses interest in the technology and suggests some additional tuning to improve its functionality. The user provides a link to an article on generating speech using machine learning. Another user comments on the state-of-the-art text-to-speech technology and shares a link to examples of speech generated synthetically through AI.

### The UIs ChatGPT Won't Replace

#### [Submission URL](https://exorva.com/blog/uis-chat-gpt-wont-replace) | 17 points | by [kmdupree](https://news.ycombinator.com/user?id=kmdupree) | [9 comments](https://news.ycombinator.com/item?id=35734660)

In a recent post on Hacker News, the founder of AI/LLM-powered app guide ChatGPT argues that traditional UIs won't be entirely replaced by chat-based experiences. The post examines tasks that rely on spatial metaphors, list items, and invariants, among other things, and demonstrates why chat-based UIs won't dominate the future. For example, busy people won't use ChatGPT to create calendar events, as they need to be able to see their schedule at a glance, while low-intent product exploration is better served by visual design patterns that instantly orient users. Ultimately, the author suggests that despite technological advancements, humans still work best with spatial and kinetic inputs.

The comments agree, with some suggesting certain tasks are better suited for GUIs or chat-based UIs. One commenter notes the importance of spatial understanding and memory, while another mentions that humans will always prefer human interfaces. Some comments also suggest that chat interfaces can be complementary to GUIs, and AI technology will allow easier access to a library of components for chat interfaces. Finally, one commenter mentions that time interaction feeling may become worse with chat-based interactions, and finding a balance between chat and GUI interfaces is important.

### Is Krita ready for HDR painting?

#### [Submission URL](https://notes.ericjiang.com/posts/1241) | 26 points | by [erjiang](https://news.ycombinator.com/user?id=erjiang) | [5 comments](https://news.ycombinator.com/item?id=35736913)

Krita, a digital painting software, has implemented support for high dynamic range (HDR) painting, allowing users to work with values above the traditional 0.0-1.0 range. However, there are areas within the software that still do not recognize these higher values, and some functions, such as LUT baking, are not yet possible. While it may be sufficient for general work and limited regions above 1, it may be difficult to work across a large dynamic range without proper exposure controls. Additionally, the software's target market may currently be too small to fully support HDR use.

Users on Hacker News talked about the importance of HDR painting and the limitations of current hardware, saying that cameras can capture more data than displays can render. Other users mentioned that HDR painting could be useful in creating works with a wider dynamic range and that similar workflows are used in 3D rendering software. One user also mentioned that games, TV, and movies are already using HDR rendering, but there are limitations due to the lack of HDR screens, which are not yet widely available. Overall, the discussion showed both excitement and caution about Krita's HDR support, with some saying that more exposure controls would be needed to work across a large dynamic range.

### Llama 1.3B Trained on 200B Tokens for Commercial Use

#### [Submission URL](https://huggingface.co/mosaicml/mpt-1b-redpajama-200b-dolly) | 23 points | by [vsroy](https://news.ycombinator.com/user?id=vsroy) | [7 comments](https://news.ycombinator.com/item?id=35737036)

The MPT-1b-RedPajama-200b-dolly is a powerful AI model with 1.3 billion parameters that has been fine-tuned on the Databricks Dolly instruction dataset. The model is a modification of a standard decoder-only transformer and features 24 layers, 16 attention heads, and width 2048. It has been pre-trained on a mix of datasets, with the majority being the RedPajama Common Crawl, and fine-tuned on the Databricks Dolly instruction dataset using the same hyperparameters found in their train_dolly.py script. The model uses ALiBi and QK LayerNorm and does not use biases. To use the model, one needs to pass `trust_remote_code=True` and use the MosaicML LLM codebase. The model was trained on the MosaicML Platform with sharded data parallelism using FSDP. The MPT-1b-RedPajama-200b-dolly is a valuable resource for instruction fine-tuning and natural language processing tasks.

The discussion in the comments primarily focuses on the number of parameters of the model and how they impact its performance. One user links to a paper on chinchilla scaling, which discusses optimizing the number of parameters for computational efficiency. Another user mentions being familiar with the RedPajama dataset.

### Lessons Learned Reproducing a Deep Reinforcement Learning Paper (2018)

#### [Submission URL](http://amid.fish/reproducing-deep-rl) | 48 points | by [tim_sw](https://news.ycombinator.com/user?id=tim_sw) | [10 comments](https://news.ycombinator.com/item?id=35732843)

Deep reinforcement learning is a fascinating field, but it can be more challenging than expected, as demonstrated by a recent project to reproduce a paper on training deep RL agents using feedback from humans. Debugging reinforcement learning involves lengthy iterations, and it's essential to be meticulous about the hypothesis-forming step to make the most of the scarce runs. It's also necessary to learn to recognize and follow through on confusion and be patient when getting stuck on problems for weeks at a time. Despite its challenges, the field holds much promise, as evidenced by recent work on training agents from human preference feedback.

In the comments, there is a discussion about the difficulties of reproducing research work and the need for patience and perseverance. One user shares their personal experience of creating a reinforcement learning agent to play a game and the challenges they faced. Another user recommends reading Sutton & Barto's book on reinforcement learning.

### Semantic Tokenizer for Enhanced Natural Language Processing

#### [Submission URL](https://arxiv.org/abs/2304.12404) | 68 points | by [PaulHoule](https://news.ycombinator.com/user?id=PaulHoule) | [28 comments](https://news.ycombinator.com/item?id=35729586)

A team of four researchers has published a paper titled Semantic Tokenizer for Enhanced Natural Language Processing on arXiv. The team presents a new tokenizer that uses semantics to drive vocabulary construction, with a trainer that uses stemming to enhance subword formation. The tokenizer is a drop-in replacement for the SentencePiece tokenizer and more than doubles the number of word forms represented in the vocabulary. The new tokenizer significantly improves NLP model convergence and improves the quality of word and sentence embeddings, with top performance seen in two Glue tasks using BERT-base, outperforming models more than 50 times in size.

Some comments noted that the paper was a significant improvement in transformer performance and highlighted how semantics can help processing multi-language texts. Others criticized the use of arXiv for class projects and questioned the significance of the paper's contribution. Additionally, some discussed the challenges of tokenization and the impact of vocabulary construction on natural language processing models.

### Palantir demos AI to fight wars but says it will be ethical

#### [Submission URL](https://www.vice.com/en/article/qjvb4x/palantir-demos-ai-to-fight-wars-but-says-it-will-be-totally-ethical-dont-worry-about-it) | 25 points | by [konart](https://news.ycombinator.com/user?id=konart) | [17 comments](https://news.ycombinator.com/item?id=35731534)

Palantir, co-founded by billionaire Peter Thiel, has demonstrated its Artificial Intelligence Platform (AIP) for military decision making. In Palantir's scenario, a military operator uses AI to monitor and respond to enemy activity, such as the recent amassing of military equipment near friendly forces. The operator asks a chatbot to show them more information, generates several plans of attack and organises the jamming of enemy communications. However, the author notes the dangers of automating warfare and abstracting it even further, suggesting the system is an illusion of safety and control for the Pentagon.

The comments discuss ethical concerns over the use of LLMs (lethal autonomous weapons). Some argue that the industry is ignoring these concerns, while others claim that the military will not deploy such systems until they are deemed safe and reliable. There are also some anecdotes about the long hours and intense work culture at Palantir.

### A Low Cost Approach to Improving Pedestrian Safety with Deep Learning

#### [Submission URL](https://nathanrooy.github.io/posts/2019-02-06/raspberry-pi-deep-learning-traffic-tracker/) | 62 points | by [djoldman](https://news.ycombinator.com/user?id=djoldman) | [58 comments](https://news.ycombinator.com/item?id=35727163)

A developer has created a cheap and accurate traffic counting system using TensorFlow and a Raspberry Pi Zero with an 8-megapixel infrared camera and rechargeable USB battery pack. The system uses a convolutional neural network with a secondary region proposal network to detect and localise objects within the frame, with lightweight temporal clustering to track them. The end result is a tool capable of separately counting vehicles, pedestrians and cyclists with high accuracy, potentially providing valuable data for urban planning and safety measures.

In the comments, there was a discussion on whether data on existing traffic patterns would be necessary for making biking more attractive in cities or if current infrastructure should be changed to support pedestrian traffic. Additionally, there was discussion on the correlation between frequent service and high passenger counts, as well as the challenges associated with increasing density and public transportation. There were also debates on making buses more efficient or switching to electric cars, as well as ideas for improving traffic congestion, such as increasing the use of roundabouts and encouraging the use of smaller cars.

