## AI Submissions for Tue May 09 2023 {{ 'date': '2023-05-09T17:13:24.011Z' }}

### Language models can explain neurons in language models

#### [Submission URL](https://openai.com/research/language-models-can-explain-neurons-in-language-models) | 662 points | by [mfiguiere](https://news.ycombinator.com/user?id=mfiguiere) | [426 comments](https://news.ycombinator.com/item?id=35877402)

Researchers at OpenAI have developed a methodology for using large language models to automatically generate and evaluate natural language explanations for neuron behavior in other language models. The team used GPT-4 to produce and score explanations for every neuron in GPT-2 and released a dataset of these explanations and scores. While the vast majority of the explanations scored poorly, the team identified over 1,000 neurons with explanations that accounted for most of the neuron's top-activating behavior. The researchers hope their work will lead to better techniques for generating higher-scoring explanations and to a rapid understanding of model computations.

In the discussion, some commenters noted the difficulty of analyzing larger models and the importance of understanding computations. Others talked about the history of artificial intelligence and how current research efforts are focused on making systems understand natural language and sensory input. Some also discussed the possibility of machine intelligence becoming comparable to human intelligence and the limitations of current language models.

### Patent for attention-based sequence transduction neural networks (2019)

#### [Submission URL](https://patents.google.com/patent/US10452978B2/en) | 98 points | by [ukuina](https://news.ycombinator.com/user?id=ukuina) | [94 comments](https://news.ycombinator.com/item?id=35877545)

Google has been granted a patent for attention-based sequence transduction neural networks (ABSTNN), which are designed to analyse and convert sequences of data, like machine translation of languages or speech-to-text transcription. ABSTNNs pay selective attention to specific parts of the input sequence to make accurate predictions, refining the network's output through multiple layers of encoder subnetworks. These subnetworks are designed to improve the accuracy of sequence transduction and reduce the comparative costs of memory-heavy, fully-connected designs.

Comments on the post suggested that Google could use its patents to thwart competition, while others noted that some of the most transformative algorithms are patentable. Attention was drawn to the fact that Google's success in AI is impressive, given its primary nature as an advertising business, and some suggested that Google has had a harder time enforcing patents compared with competitors. Finally, there was some debate over the usefulness of the patent system, with many suggesting that it may stifle innovation.

### Machine Learning Containers Are Bloated and Vulnerable

#### [Submission URL](https://deep.ai/publication/machine-learning-containers-are-bloated-and-vulnerable) | 24 points | by [PaulHoule](https://news.ycombinator.com/user?id=PaulHoule) | [8 comments](https://news.ycombinator.com/item?id=35877702)

Machine learning containers are often bloated and vulnerable, according to a paper by Huaifeng Zhang and colleagues. The researchers found that such containers can contain bloat of up to 80% of their size, leading to significant resource wastage. The authors suggest that debloating machine learning containers can speed provisioning times by up to 3.7x and reduce vulnerabilities by up to 98%. They have developed a framework called MMLB to quantify bloat at the container and package level, and removed it. The researchers say their work highlights the issue of technical debt in machine learning systems.

The discussion further explores the issue and the practicalities of managing containers, including difficulties in ensuring reproducibility and the maintenance of dependencies. The discussion finally highlights the importance of enabling containers and the removal of bloating while also acknowledging that it is a challenging task.

### Meta open-sources multisensory AI model that combines six types of data

#### [Submission URL](https://www.theverge.com/2023/5/9/23716558/meta-imagebind-open-source-multisensory-modal-ai-model-research) | 149 points | by [rntn](https://news.ycombinator.com/user?id=rntn) | [48 comments](https://news.ycombinator.com/item?id=35876147)

Meta has released an open-source AI model, ImageBind, that employs six types of data, including thermal, visual, and movement information, to create immersive and multisensory experiences. The research project marks a significant development in generative AI systems, which rely on linking multiple streams of data to create content. By incorporating touch, speech, smell, and brain fMRI signals, future models could learn holistically, approaching humans’ ability to learn directly from different types of information. Meta's approach, which is open source, comes as rival firms such as Google and OpenAI increasingly pursue a secretive strategy.

The discussion in the comments included some debate about the definition of open source versus free and open-source software (FOSS), and other topics such as the underdefined nature of terms related to open-source, the differences in licenses, and the potential hardware requirements needed for training the model. People also expressed their skepticism about Meta's involvement in AI for the "truly neutral" evaluation of natural language processing tasks and the company's controversial attitude toward privacy.

### Amazon Is Being Flooded with Books Written Entirely by AI

#### [Submission URL](https://futurism.com/the-byte/amazon-flooded-books-written-by-ai) | 52 points | by [redbell](https://news.ycombinator.com/user?id=redbell) | [43 comments](https://news.ycombinator.com/item?id=35881065)

Amazon's marketplace is being flooded with books almost entirely generated by AI. This trend is making it harder to distinguish between real authors and the non-existent writers created by AI algorithms. The AI-generated books are primarily listings on surprisingly niche topics with five-star reviews. However, AI content is flooding the internet and could spark a pandemic of misinformation. Several online publications are already making ample use of the technology to generate often dubiously sourced and redundant content. The emergence of AI-generated books and content represents new reality for businesses and poses risks of misinformation and confusing reality.

Commenters suggest that AI-generated content could be beneficial in creating recommenders for modern authors and helping companies find new incentives in machine-generated content. However, concerns arise if the AI-generated content is not reliable, leading to the spread of misinformation. Some users also point out that there are still many interesting printed books available to read and that not all books necessarily have the same value. Furthermore, some discussion in the comments centers on LitRPG books that are highly entertaining and those books generated by AI. There are also discussions on AI-generated ratings.

### Constitutional AI: RLHF on Steroids

#### [Submission URL](https://astralcodexten.substack.com/p/constitutional-ai-rlhf-on-steroids) | 151 points | by [jstanley](https://news.ycombinator.com/user?id=jstanley) | [68 comments](https://news.ycombinator.com/item?id=35870669)

Today, Anthropic, a big AI company, announced a new process for training AI called Constitutional AI, which allows the AI to give feedback to itself to train the AI to be less harmful and more ethical. The process involves showing the AI its first draft answer to a question, along with a prompt saying “rewrite this to be more ethical” until a large dataset of rewritten, more ethical second drafts is collected, and then the AI is trained to write answers that are less like the first drafts and more like the second drafts. The results have been positive, with Constitutionally trained models being "less harmful at a given level of helpfulness" than models trained with traditional reinforcement learning through human feedback.

Some users argue that accurate predictions and not political correctness should be the main goal of AI. Others argue that the new method could lead to more ethical AI and that companies should prioritize minimizing harm rather than maximizing profit. Some users also criticize the economics and politics that drive AI development.

### Show HN: LLM, a Rust Crate/CLI for CPU Inference of LLMs (LLaMA, GPT-NeoX, etc.)

#### [Submission URL](https://github.com/rustformers/llm) | 43 points | by [Philpax](https://news.ycombinator.com/user?id=Philpax) | [4 comments](https://news.ycombinator.com/item?id=35876928)

llm is a Rust ecosystem of libraries for running inference on large language models, inspired by llama.cpp. It is powered by the ggml tensor library and aims to bring the robustness and ease of use of Rust to the world of large language models. Currently, inference is only on the CPU, but there is hope to support GPU inference in the future through alternate backends. Supported models include GPT-2, GPT-J, LLaMA, Alpaca, Vicuna, Koala, GPT4All v1, GPT4-X, Wizard, GPT-NeoX, StableLM, Dolly v2, and BLOOMZ. The primary crate is the llm crate, which wraps llm-base and supported model crates. On top of llm, there is a CLI application, llm-cli, which provides a convenient interface for running inference on supported models.

The discussion on the submission revolves around users' experiences with llm and related libraries. One user who tried the library commented positively on its convenience but noted that it runs only on CPU at the moment and would be more efficient on multiple servers. Another user shared that they have attempted to build their own ML project but found it challenging to justify the expense. The discussion then shifted to ggml, a tensor library used in llm that offers performance gains and has a direct plan to build a competition graph with supported backends, including Intel MKL and CUDA. Overall, the discussion reflects the interest and excitement around using Rust for large language models, as well as the importance of performance optimization in machine learning projects.

### AI predicts pancreatic cancer 3 years before it happens

#### [Submission URL](https://www.theregister.com/2023/05/09/ai_pancreatic_cancer/) | 20 points | by [ter_adata](https://news.ycombinator.com/user?id=ter_adata) | [4 comments](https://news.ycombinator.com/item?id=35878002)

AI algorithms can predict whether a patient will develop pancreatic cancer up to three years before human doctors can, according to research published in the journal Nature. The study, led by Harvard Medical School and Danish academics, incorporated machine learning trained on millions of patient records obtained from databases in Denmark and the US. The best-performing model indicated that of the top 1,000 riskiest patients over the age of 50, roughly 320 would go on to develop pancreatic cancer. The researchers warn that local population data is necessary for accurate cancer-catching predictions.

The discussion on this submission includes comments about the effectiveness of machine learning algorithms in predicting pancreatic cancer and the potential bias that can occur in future AI-based cancer screening tools. Some users mention the need for local population data to ensure accurate predictions while others discuss the use of GPT transformers and the study's reliance on medical records obtained from databases in Denmark and the US. There is also mention of the high mortality rate of pancreatic cancer and the potential benefits of using AI to detect the disease earlier.

