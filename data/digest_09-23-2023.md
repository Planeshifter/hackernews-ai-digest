## AI Submissions for Sat Sep 23 2023 {{ 'date': '2023-09-23T17:09:56.788Z' }}

### The Cambridge Law Corpus: A corpus for legal AI research

#### [Submission URL](https://arxiv.org/abs/2309.12269) | 132 points | by [belter](https://news.ycombinator.com/user?id=belter) | [33 comments](https://news.ycombinator.com/item?id=37627129)

Researchers at Cambridge University have released the Cambridge Law Corpus (CLC), a corpus designed for legal artificial intelligence (AI) research. The CLC contains over 250,000 court cases from the UK, ranging from the 16th century to the present day. The corpus includes both raw text and metadata, and it also features annotations on case outcomes for 638 cases, provided by legal experts. The researchers used this annotated data to train and evaluate case outcome extraction models, including GPT-3, GPT-4, and RoBERTa, to establish benchmarks for future research. Due to the potentially sensitive nature of the material, the corpus will only be made available for research purposes under certain restrictions.

The discussion on Hacker News revolves around the release of the Cambridge Law Corpus (CLC) for legal AI research. Some users mention other similar legal resources, such as the Harvard Laws Library Innovation Lab's Caselaw Access Project, which provides a comprehensive collection of structured metadata for case law. The potential advantages and limitations of accessing and analyzing public legal information are also discussed. Privacy concerns are raised, with one user noting that while legal proceedings are generally public, sensitive information should still be handled carefully. The conversation then takes a turn towards constitutional rights, including the Fourth Amendment and the right to privacy. Finally, one user expresses gratitude for the submission and plans to submit related briefs.

### Computational Discovery on Jupyter

#### [Submission URL](https://computational-discovery-on-jupyter.github.io/Computational-Discovery-on-Jupyter/) | 95 points | by [__rito__](https://news.ycombinator.com/user?id=__rito__) | [4 comments](https://news.ycombinator.com/item?id=37626551)

Welcome to Computational Discovery on Jupyter! This Open Educational Resource aims to teach you Python programming through exploring interesting mathematics that goes beyond the standard curriculum.

So why use mathematics for learning programming? Well, math is the simplest and quickest way to learn Python. Plus, it offers a foundation for various other fields like biology, physics, chemistry, engineering, economics, psychology, and music.

The creators of this resource have listened to students' requests for more programming education and are incorporating it earlier in the curriculum. By offering intriguing mathematics, they hope to engage a wider audience and keep them interested in math for the long run.

What can you expect from this resource? They use a variety of mediums like videos, images, programs, programming activities, and even pencil-and-paper activities. You will also have access to the Jupyter Book and associated Jupyter Notebooks, and they encourage contributions from users to expand the material.

Overall, Computational Discovery on Jupyter is an exciting opportunity to learn Python while delving into fascinating mathematics. Get ready to explore and discover new concepts in the world of programming!

The discussion on this submission revolves around the relevance and importance of using Jupyter and Python in education. One user recommends the book "IPython Interactive Computation Visualization Cookbook" by Cyrille Rossant as a good resource for learning about IPython and Jupyter. Another user clarifies that the book was published between 2014 and 2018, and covers chapters 7-15, which are compatible with current versions of Python, NumPy, and Jupyter.

One user flags the discussion, expressing frustration that the comments are not directly related to Jupyter's development. They argue that discussions should stay on-topic and not devolve into unrelated topics. They also mention that it is important for students to actively engage and understand the existing knowledge of specialists.

Overall, the discussion seems to be mixed, with some users discussing relevant resources and others expressing frustration about the direction of the conversation.

### Neural-Symbolic Recursive Machine for Systematic Generalization

#### [Submission URL](https://arxiv.org/abs/2210.01603) | 65 points | by [beefman](https://news.ycombinator.com/user?id=beefman) | [3 comments](https://news.ycombinator.com/item?id=37627615)

A new research paper titled "Neural-Symbolic Recursive Machine for Systematic Generalization" proposes a machine learning model that aims to achieve systematic generalization, a human-like ability to learn compositional rules from limited data and apply them to unseen combinations in various domains. The authors introduce the Neural-Symbolic Recursive Machine (NSR), which utilizes a Grounded Symbol System (GSS) with combinatorial syntax and semantics. NSR consists of separate modules for neural perception, syntactic parsing, and semantic reasoning, which are jointly learned using a deduction-abduction algorithm. The researchers demonstrate that NSR achieves superior systematic generalization on three benchmarks from different domains: semantic parsing, string manipulation, and arithmetic reasoning. NSR outperforms state-of-the-art models on the arithmetic reasoning benchmark by approximately 23%. The authors attribute NSR's strong generalization to its symbolic representation and inductive biases, as well as its better transferability compared to existing neural-symbolic approaches. The paper provides insights into how NSR can effectively model various sequence-to-sequence tasks.

The discussion on this submission started with a comment pointing out that existing machine learning models often fail to achieve human-like systematic generalization. The commenter also mentions a recent paper published by the creators of ChatGPT, reinforcing the relevance of the topic.

Another commenter responds, expressing their belief that the Neural-Symbolic Recursive Machine (NSR) mentioned in the submission is similar to an existing model called MoE (Mixture of Experts). They suggest that the NSR implementation might be costly compared to the MoE approach, but it is not clear from the comment why this is the case.

### Auto-Regressive Next-Token Predictors Are Universal Learners

#### [Submission URL](https://arxiv.org/abs/2309.06979) | 91 points | by [dataminer](https://news.ycombinator.com/user?id=dataminer) | [12 comments](https://news.ycombinator.com/item?id=37619513)

Researchers have discovered that auto-regressive next-token predictors, such as language models, have the ability to solve complex tasks involving logical and mathematical reasoning. In a recent study, Eran Malach presented a theoretical framework for studying these predictors and showed that even simple models like linear and shallow Multi-Layer Perceptrons (MLPs) can approximate any function efficiently computed by a Turing machine. The researchers introduced a new complexity measure called "length complexity" and analyzed its relationship with other notions of complexity. They also conducted experiments that demonstrated the non-trivial performance of these predictors on text generation and arithmetic tasks. The study suggests that the power of language models may be attributed to the auto-regressive next-token training scheme, rather than a specific choice of architecture.

The discussion around the submission involves various perspectives on the power and implications of language models in solving complex tasks. Some commenters highlight the deterministic nature of linguistic determinism and its influence on thinking processes. Others point out that language skills do not necessarily equate to general intelligence and that there is a wide range of models and formalisms beyond Turing completeness that can approximate different types of thinking. 

There is also a conversation about the efficacy of language models in terms of practical applications, such as medical research and clinical note analysis. Some suggest that language models could be helpful in generating prompts for medical reports, while others caution that these models can encounter difficulties in generating coherent narratives.

There is interest in the concept of training models regressively and the challenges posed by difficult tasks, as well as the potential of intermediate steps in internal processing and the learning of step-by-step procedures. Some commenters express curiosity about the workings of language models and suggest that the focusing on regressive models in learning may enhance their performance.

One commenter provides a link to a related discussion on Hacker News with additional perspectives on the topic. Another commenter criticizes the practice of prompt engineering and suggests exploring alternative approaches.

### TinyML and Efficient Deep Learning Computing

#### [Submission URL](https://efficientml.ai/) | 232 points | by [samuel246](https://news.ycombinator.com/user?id=samuel246) | [34 comments](https://news.ycombinator.com/item?id=37620507)

Calling all aspiring AI enthusiasts! The course you've been waiting for is back: "Efficient AI Computing Techniques for Resource-Constrained Devices." This course will equip you with the skills to optimize large generative models like language models and diffusion models, making them more accessible and efficient on devices with limited computational resources.

The curriculum covers a wide range of topics, including model compression, pruning, quantization, neural architecture search, distributed training, data/model parallelism, gradient compression, and on-device fine-tuning. You'll also learn application-specific acceleration techniques for tasks like large language models, diffusion models, video recognition, and point cloud.

But that's not all! The course also touches on the fascinating field of quantum machine learning. Get ready to dive into the cutting-edge developments at the intersection of AI and quantum computing.

To ensure a hands-on experience, you'll have the opportunity to deploy large language models, like LLaMA 2, on your very own laptop. Plus, you'll have access to lecture recordings on YouTube and live streaming sessions on the course website every Tuesday and Thursday.

The course offers office hours and a dedicated Discord channel for lively discussions. To stay updated, sign up for their mailing list. Whether you have course-related questions or need assistance with personal matters, the instructors are ready to help.

Who are the ingenious minds behind this course? Meet Song Han, an Associate Professor at MIT EECS, and the TAs, Ji Lin and Han Cai, both brilliant PhD students in the same field.

So mark your calendars, because the course starts in Fall 2023. Get ready to unlock the potential of AI on resource-constrained devices. It's time to make efficient AI computing techniques your superpower!

The discussion on this submission covers a variety of topics related to efficient AI computing techniques and their impact on energy consumption, as well as other related discussions. Here are some highlights:

- One user recommends checking out the TinyML Talks, expressing interest in the upcoming schedule and the content on embedded systems.
- Another user shares their experience working on ML projects in Africa and the challenges they face in terms of limited resources.
- A discussion ensues about the energy consumption of machine learning and the potential environmental impact. One user mentions the energy consumption of big cloud companies that are planning to expand their ML infrastructure, while another user points out the energy efficiency concerns of AI development. The conversation touches upon the potential impact of banning certain algorithms, the energy efficiency of work-from-home setups, and the comparison with cryptocurrency mining infrastructure.
- Some users express their concerns about the energy consumption of bleeding-edge research work and its impact on the environment, while others argue that hindering research in the name of energy efficiency is not productive.
- The discussion also delves into the funding and resources allocated to bleeding-edge research work, mentioning government grants and industry funding.
- Users bring up the topic of efficient language models and the use of DeepSpeed and Huggingface Accelerate as potential solutions.
- There is a brief discussion about the quality of online lectures, sharing experiences with different platforms and the potential for AI to improve lecture recordings.
- One user mentions the absence of Google's highly efficient step-by-step distillation method in the course curriculum.
- A user raises a question about the course structure, wondering if it follows a semester-based or problem-based approach.

Overall, the discussion revolves around the energy consumption and efficiency of AI computing techniques, as well as related topics such as resources, online education, and quality of lectures.

### Show HN: ChatGPT for Med-School and Healthcare

#### [Submission URL](https://chat.radiantai.health/) | 52 points | by [doublemint2202](https://news.ycombinator.com/user?id=doublemint2202) | [55 comments](https://news.ycombinator.com/item?id=37620043)

Today's top story on Hacker News is about the rise of low-code and no-code development platforms, which have gained significant traction in recent years. These platforms allow developers, and even non-developers, to easily create applications without traditional coding skills. With drag-and-drop interfaces and pre-built components, anyone can bring their ideas to life without diving deep into complex code. While some argue that low-code platforms limit customization and hinder innovation, proponents believe they democratize software development and empower a wider audience to participate in creating digital solutions. As the low-code trend continues to evolve, it will be interesting to see how these platforms shape the future of software development.

The discussion on the submission includes a mix of different topics. Some users express concerns about the reliability and potential negative impacts of using AI language models like ChatGPT for medical diagnoses. Others discuss the potential applications of AI in medical records analysis and document scraping for medical research purposes. Some users also explore the challenges and limitations of low-code and no-code development platforms, while others provide suggestions for using search engines and web scraping tools to extract information from medical documents. Additionally, there are discussions about unusual deaths, an interesting Wikipedia page, and a humorous comment about making guacamole.

