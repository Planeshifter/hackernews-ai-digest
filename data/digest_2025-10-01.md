## AI Submissions for Wed Oct 01 2025 {{ 'date': '2025-10-01T17:16:05.164Z' }}

### Building the heap: racking 30 petabytes of hard drives for pretraining

#### [Submission URL](https://si.inc/posts/the-heap/) | 389 points | by [nee1r](https://news.ycombinator.com/user?id=nee1r) | [264 comments](https://news.ycombinator.com/item?id=45438496)

DIY beats the cloud (by a lot): 30 PB video store for ML at $1/TB/month

- The pitch: Training models on 90M hours of video dwarfs text LLM data needs, so storage—not compute—became the bottleneck. Instead of paying ~$12M/yr on AWS, a 5-person team racked their own disks in a downtown SF colo for ~$354k/yr all-in (~40x cheaper).
- Why on-prem works here: Pretraining data is commodity. They can tolerate corruption and don’t need “13 nines.” Losing 5% of samples is fine, so enterprise-grade redundancy is overkill.
- The build: ~30 PB using 2,400 used 12TB enterprise HDDs in 100 NetApp DS4246 JBODs, 10 head nodes, an Arista router, and 100 Gbps DIA from Zayo.
- Costs:
  - Recurring: Internet $7.5k/mo, power (incl. space/cooling) $10k/mo
  - One-time capex: $426.5k (mostly drives), depreciated over 3 years
  - Total: ~$29.5k/mo ($1/TB/mo)
- Cloud comparisons:
  - AWS: ~$1.13M/mo ($38/TB/mo; includes 10 PB/mo egress)
  - Cloudflare R2 (bulk): ~$270k/mo ($10/TB/mo), but they’ve hit metadata rate limits under heavy training loads
- Ops choice: Picked a pricier SF colo over cheaper Fremont to keep hands-on work close to the office and reduce productivity drag.
- Takeaway: For data-heavy AI, storage and egress dominate. If you can accept lower durability and manage some hardware, stacking used drives in a local colo can 10–40x your cost efficiency versus major clouds.

**Summary of Discussion:**

1. **Cost Efficiency & Cloud Comparisons:**
   - Participants highlight the significant cost savings of DIY storage (40x cheaper than AWS) but note that negotiating with cloud providers (e.g., AWS, Cloudflare) for bulk discounts could reduce the gap. Some argue cloud pricing remains prohibitive for large-scale AI training.

2. **Hardware & Colocation Choices:**
   - The team’s use of **used enterprise HDDs** and JBODs sparked debate on reliability vs. cost. Some suggest alternatives like Supermicro or Backblaze Storage Pods for higher density. The choice of a pricier SF colo over Fremont was defended for proximity and productivity benefits.

3. **Drive Reliability & Failure Rates:**
   - Concerns about drive failures were addressed with references to Backblaze’s annual reports (~1.36% failure rate). Discussions noted the "U-shaped" failure curve (higher early/late failures) and stressed the importance of diversified suppliers to mitigate risks.

4. **Networking & GPU Bottlenecks:**
   - Questions arose about 100 Gbps networking sufficiency for training. Replies clarified that preprocessing data minimizes bottlenecks, and GPUs are housed separately (likely in cloud/power-dense locations due to colo power limits).

5. **Hetzner & Alternative Providers:**
   - Hetzner’s storage solutions were debated—praised for cost but criticized for support and arbitrary data deletion policies. Some recommended local colos for better control and connectivity.

6. **Operational Insights:**
   - Anecdotes shared on managing drive failures (e.g., scripting RAID recovery) and the trade-offs of enterprise vs. consumer drives. Emphasis on testing (SMART, write verification) and accepting lower durability for cost savings.

**Key Takeaways:**
- **DIY storage** is viable for AI/ML teams willing to trade durability for cost, leveraging used hardware and colocation.
- **Cloud costs** remain high for bulk storage, but negotiation and alternative providers (e.g., Cloudflare R2) can help.
- **Drive management** requires proactive failure handling and supplier diversity.
- **Community experiences** with providers like Hetzner and Backblaze inform practical decisions, balancing cost, risk, and support.

### Unix philosophy and filesystem access makes Claude Code amazing

#### [Submission URL](https://www.alephic.com/writing/the-magic-of-claude-code) | 380 points | by [noahbrier](https://news.ycombinator.com/user?id=noahbrier) | [200 comments](https://news.ycombinator.com/item?id=45437893)

The Magic of Claude Code (Noah Brier) — why a terminal turns an LLM into an OS

- Brier describes how Claude Code went from “nice coding aid” to his day-to-day agentic operating system—especially for managing an Obsidian vault. He even runs it on a home server and SSHs in from his phone to read/write notes on the go.

- The unlock isn’t just code generation—it’s running in a terminal with native Unix tools. Simple, composable commands (pipes, grep, sed, etc.) align with how LLMs naturally chain tools, making them surprisingly effective operators.

- Filesystem access is the other breakthrough. Unlike browser chat UIs with no persistent memory and tight context windows, Claude Code can write to disk, keep running tallies, accumulate knowledge, and retain state across sessions.

- He contrasts it with Cursor/ChatGPT: not necessarily “better” at everything, but the combination of Unix + filesystem makes Claude Code feel qualitatively different and more reliable for building new workflows on top.

- Cites The Pragmatic Engineer’s deep dive and Boris Cherny’s “product overhang” idea: the model could already reason this way; products just hadn’t exposed the capability. Claude Code does, offering a blueprint for practical agentic workflows.

- Big picture: even without smarter models, better product surfaces (like terminals and filesystems) can unlock a lot of latent capability.

The Hacker News discussion around Noah Brier’s Claude Code submission highlights several key themes and critiques:

### Key Themes
1. **Practical Workflow Integration**  
   Users praised Claude Code’s ability to streamline debugging, log analysis, and scripting via Unix-like composability. Examples include diagnosing industrial device logs, automating Obsidian vault management, and generating scripts for repetitive tasks (e.g., refactoring code, translating text). The terminal-centric design and filesystem access were seen as transformative for agentic workflows.

2. **Comparison to Alternatives**  
   While Claude Code was contrasted favorably with tools like Cursor or ChatGPT for CLI-centric tasks, some noted it isn’t universally superior. AWS CLI and Terraform were cited as prior examples of terminal-driven tooling, emphasizing principles like least-privilege IAM policies.

3. **AI’s Role in Programming**  
   Debates emerged about whether LLMs like Claude can replace traditional programming workflows. Users observed that while humans naturally build tools incrementally, LLMs often brute-force solutions, leading to fragile code. Static analysis tools (linters, formatters) were deemed critical for catching errors LLMs might miss.

### Critiques and Challenges
- **Edge-Case Failures**: A user shared an anecdote where Claude insisted on using Bash for FreeBSD (which lacks native Bash), highlighting AI’s occasional rigidity or incorrect assumptions.
- **Skipped Validations**: Frustration arose when Claude Code skipped pre-commit checks (e.g., linters, tests) or generated code that "passed" tests without meaningful validation. One user joked that an AI skipping Rust tests “knows it’s Friday.”
- **Tooling Limitations**: Projects like `Mansnip` (STDIO wrappers) faced technical hurdles, such as Debian packaging issues, underscoring the gap between prototype and production-ready tools.

### Humor and Skepticism  
Comparisons to Clippy (“Clippy with Unix pipes”) and quips about AI “predicting weekend deployments” reflected mixed enthusiasm. While some lauded Claude’s efficiency, others questioned its reliability for complex tasks, emphasizing the need for human oversight.

### Bottom Line  
The discussion reinforced Brier’s thesis: terminal and filesystem access unlock latent LLM potential, but practical adoption requires balancing automation with robust error-checking and tooling maturity.

### The RAG Obituary: Killed by agents, buried by context windows

#### [Submission URL](https://www.nicolasbustamante.com/p/the-rag-obituary-killed-by-agents) | 244 points | by [nbstme](https://news.ycombinator.com/user?id=nbstme) | [160 comments](https://news.ycombinator.com/item?id=45439997)

The RAG Obituary: Why bigger context windows and agents may kill today’s RAG stack

- The pitch: Nicolas Bustamante (ex-Doctrine, now Fintool) argues that retrieval‑augmented generation is entering its twilight as context windows explode and agent-based systems mature. The costly machinery of chunking, embeddings, and rerankers is being outpaced by models that can “just read” far more of the source material and reason with it via agents.

- Why RAG took off: Early LLMs like GPT‑3.5/4 had tiny windows (4k–8k tokens), forcing systems to retrieve slivers of large documents (e.g., a 10‑K at ~51k tokens) and have the model synthesize answers from fragments.

- Where it breaks: He details the practical failures:
  - Chunking fractures meaning (splitting policies, tables, and narrative-context links), even with sophisticated, metadata-rich schemes.
  - Embeddings are blunt for domain nuance (e.g., conflating “revenue recognition” vs. “revenue growth”), returning boilerplate, duplicates, stale or irrelevant mentions.
  - Reranking and retrieval heuristics fight endless edge cases because the model never sees the whole document.

- His team’s best efforts still hit the wall: Fintool preserves hierarchy, keeps financial tables atomic, links notes and footnotes, tracks periods and sections—yet the core problem remains: you’re feeding fragments, not documents.

- The bet going forward: As context windows grow and agentic workflows mature, the need for chunking/embedding/rerank stacks diminishes. Instead of assembling context via retrieval, agents with large contexts can ingest full sections or entire filings and reason across them directly.

Bottom line: RAG solved an architectural constraint. If that constraint disappears, much of today’s retrieval stack becomes overhead. Bustamante’s contrarian take: the future of enterprise AI looks less like vector databases and rerankers—and more like long-context models orchestrated by agents.

**Summary of Hacker News Discussion:**

The discussion around the "RAG Obituary" submission reflects skepticism about the claim that RAG (Retrieval-Augmented Generation) is dying, with nuanced debates on its evolution versus obsolescence:

1. **Criticism of the Premise**:  
   Many argue that RAG’s core principles (retrieving external data to augment LLMs) remain relevant even as context windows grow. Some note that **grep-like keyword searches** (fast, precise) and **vector/semantic searches** serve different needs, and RAG’s hybrid approaches (e.g., blending BM25 with embeddings) address limitations of pure keyword or semantic methods.

2. **Practical Use Cases for RAG**:  
   Users highlight scenarios where RAG excels, such as searching across **millions of documents** or **distributed systems** where ingesting entire corpora into context windows is impractical. Others emphasize RAG’s role in **domain-specific tasks** (e.g., financial filings) where preserving document structure and semantic nuance matters.

3. **Semantic vs. Keyword Search**:  
   Debate centers on whether RAG inherently requires vector search. Some argue RAG is broader, encompassing any retrieval method (keyword, regex, hybrid), while others equate it with vector databases. Hybrid systems (e.g., BM25 + embeddings) are seen as evolving RAG, not replacing it.

4. **Agents vs. RAG**:  
   Skepticism arises about agents being a "replacement." Many see agentic workflows as **extensions of RAG** (e.g., iterative query refinement, dynamic context pulling) rather than a paradigm shift. The line between "RAG with rerankers" and "agents" blurs in practice.

5. **Technical Trade-offs**:  
   - **Cost/latency**: Large context windows (e.g., 2M tokens) are expensive and slow compared to optimized retrieval pipelines.  
   - **Rerankers**: While criticized for latency, lightweight rerankers (e.g., cross-encoders) are cheaper than LLM-based ranking.  
   - **Chunking**: Still necessary for granularity, even with larger contexts, to avoid overloading models with irrelevant data.

6. **Definitional Disputes**:  
   Critics accuse the original article of narrowly defining RAG as "vector databases," ignoring its broader utility. Some suggest the term is becoming diluted, with vendors rebranding existing techniques (e.g., semantic search) as "agentic."

**Key Takeaway**:  
The consensus leans toward RAG **evolving** (e.g., integrating agents, hybrid search) rather than dying. Larger context windows and agents may reduce reliance on *specific* RAG components (e.g., chunking), but retrieval-augmented workflows will persist in scalable, cost-sensitive, or domain-specific applications. The future likely involves **hybrid systems** combining the best of RAG, agents, and long-context models.

### OpenTSLM: Language models that understand time series

#### [Submission URL](https://www.opentslm.com/) | 261 points | by [rjakob](https://news.ycombinator.com/user?id=rjakob) | [77 comments](https://news.ycombinator.com/item?id=45440431)

OpenTSLM: making time a first-class modality for AI. The team proposes “Time-Series Language Models” (TSLMs) that treat temporal data as a native modality alongside text, aiming to directly reason, explain, and forecast over signals like heartbeats, sensor streams, prices, and logs in natural language. They claim order-of-magnitude gains in temporal reasoning while running on smaller, faster backbones.

Key points:
- What’s new: A foundation model class centered on time series as input/output, not just an add-on to LLMs. Targets reasoning, forecasting, and explanations over temporal data.
- Release status: White paper released Sep 30, 2025; Stanford repo on Oct 1, 2025. “Open core” base models trained on public data, plus “Frontier” proprietary models for enterprise.
- Why it matters: Real-world systems are driven by continuous signals; most current LLMs struggle with temporal structure and streaming. A robust TSLM could enable proactive healthcare, adaptive robotics, and resilient infrastructure.
- Compared to prior art: Lands amid growing “foundation models for time series” work (e.g., TimesFM, Chronos, PatchTST). The distinctive claim is native multimodality with time series + text and strong temporal reasoning on smaller models.
- What to watch: Benchmarks and datasets used, evaluation tasks (reasoning vs forecasting vs anomaly detection), sequence length and streaming latency, licensing of “open” models, and whether repos reproduce the reported gains.

Team includes researchers from ETH, Stanford, Harvard, Cambridge, TUM, CDTM, and major AI labs; they’re the paper’s original authors.

The discussion around OpenTSLM reveals a mix of enthusiasm and skepticism, focusing on technical feasibility, real-world applications, and limitations:

### **Key Debates & Insights**
1. **ECG Analysis & Edge Deployment**
   - A user questioned if OpenTSLM could reliably run on edge devices (e.g., heart monitors) given hardware constraints. The paper’s smaller 270M-parameter model requires 7GB RAM, still exceeding typical smartphone capabilities (6-8GB). Critics argue real-time deployment remains challenging without specialized hardware.

2. **Pattern Detection vs. Clinical Context**
   - While OpenTSLM claims advanced pattern recognition (85% accuracy with clinical context vs. 65% without), skeptics note this relies on curated templates and annotated datasets. Detecting subtle signals (e.g., heart arrhythmias) may still lag behind domain-specific algorithms validated in clinical trials.

3. **Financial Data Challenges**
   - Non-stationary signals (e.g., stock prices) pose unique hurdles compared to stationary medical data. Users highlight difficulties in detecting regime shifts or encrypted trading signals, questioning if TSLMs can adapt to rapidly changing, noisy financial environments.

4. **Causality & Interpretability**
   - Granger causality and causal discovery are flagged as underaddressed challenges. Some argue LLMs’ script-calling approach (e.g., invoking signal-processing libraries) risks being a “heavy lift” versus native temporal reasoning.

5. **Architecture & Technical Tradeoffs**
   - The model’s 1D convolutional cross-attention architecture is praised for capturing subtle patterns, but skeptics question if constrained architectures can generalize across domains (e.g., ECGs vs. stock data). Comparisons to vision-language models (e.g., Flamingo) suggest parallels in modality fusion.

6. **Release & Licensing Quirks**
   - A typo in the release date (“Sep 31, 2025”) sparked humor, with users noting the irony for a time-centric project. Concerns linger about the “open core” licensing and reproducibility of results from the Stanford repo.

### **Notable Perspectives**
- **Optimism**: Researchers praise OpenTSLM’s potential for proactive healthcare and infrastructure monitoring, citing its novel fusion of time-series and language modalities.
- **Skepticism**: Critics stress hardware limitations, domain specificity, and the gap between academic benchmarks and real-world deployment (e.g., financial data’s unpredictability).
- **Middle Ground**: Some suggest hybrid approaches, combining TSLMs with traditional statistical methods or domain-specific algorithms for reliability.

### **Conclusion**
While OpenTSLM introduces promising advances in temporal reasoning, its success hinges on overcoming hardware barriers, proving generalizability across non-stationary domains, and addressing causality challenges. The discussion underscores the tension between academic innovation and practical deployment constraints.

### High-resolution efficient image generation from WiFi Mapping

#### [Submission URL](https://arxiv.org/abs/2506.10605) | 135 points | by [oldfuture](https://news.ycombinator.com/user?id=oldfuture) | [35 comments](https://news.ycombinator.com/item?id=45434941)

LatentCSI: high‑res images from WiFi signals using a pretrained diffusion model

Researchers propose LatentCSI, a method that turns WiFi channel state information (CSI) into images by mapping CSI amplitudes directly into the latent space of a pretrained latent diffusion model (LDM). The diffusion model then denoises in latent space—optionally guided by a text prompt—before decoding to a high‑resolution image. This sidesteps pixel‑space generation and avoids training a heavy image generator or a separate image encoder.

Key points
- How it works: a lightweight neural net maps CSI amplitudes → LDM latent; the frozen LDM performs denoising with optional text guidance; the pretrained decoder produces the final image.
- Why it matters: leverages powerful vision priors in pretrained LDMs to get higher‑quality, controllable images from commodity WiFi data with far less compute.
- Results: on an in‑house wide‑band CSI dataset (off‑the‑shelf WiFi + cameras) and a subset of MM‑Fi, LatentCSI outperforms similarly lightweight baselines trained on images in both perceptual quality and efficiency, and supports text‑guided controllability.
- Efficiency angle: training focuses on a small mapper network; inference occurs in compact latent space, reducing cost versus pixel‑space GAN/diffusion approaches.
- Caveats: reconstructions rely heavily on the diffusion model’s priors and training alignment; risk of hallucinations and limited generalization across environments/devices; details like through‑wall capability aren’t claimed.
- Privacy/ethics: turning ambient WiFi into plausible images raises surveillance concerns despite potential benefits in robotics, smart homes, and sensing.

Paper: High-resolution efficient image generation from WiFi CSI using a pretrained latent diffusion model (arXiv:2506.10605, Ramesh & Nishio)

The discussion around generating high-resolution images from WiFi signals using a diffusion model (LatentCSI) reveals several key themes:

### **Skepticism & Technical Concerns**
1. **Accuracy & Hallucination**: Many question whether the model genuinely reconstructs images from WiFi data or relies on the diffusion model’s priors to "hallucinate" plausible details (e.g., clothing colors, object placement). Critics argue WiFi signals lack explicit visual data (e.g., color), making reliable inference doubtful.
2. **Overfitting**: Concerns that results are overfitted to limited training environments (specific rooms, angles, or hardware) and may not generalize to unseen scenarios.
3. **Bandwidth Limitations**: While higher WiFi bandwidth (e.g., 160MHz) provides more data points, critics doubt it suffices for high-resolution image generation, with one noting it’s akin to “predicting a 3D scene from 1992 input points.”

### **Ethical & Privacy Implications**
- **Surveillance Risks**: Users highlight dystopian implications, such as erosion of privacy via ambient WiFi becoming a surveillance tool. One commenter calls it a “scrubber of human privacy,” noting potential misuse in monitoring health or activities.
- **Ethical Dilemmas**: Concerns about deploying such technology without safeguards, especially given its potential to infer sensitive details.

### **Technical Counterpoints & Clarifications**
- **Efficiency & Novelty**: Supporters praise the method’s efficiency by mapping WiFi data to a pretrained latent diffusion model’s space, enabling faster training and text-guided generation. Authors clarify that the model focuses on small, environment-specific adaptations rather than full retraining.
- **Material Properties vs. Color**: Debates arise over whether WiFi signals (which interact with material dielectric properties) can correlate with visual colors. Some argue materials’ spectral responses don’t align with RGB colors, making accurate color inference unlikely.

### **Reproducibility & Practicality**
- **Hardware Challenges**: Discussions note the difficulty of extracting reliable CSI data from commodity hardware, citing tools like ESP32 or custom drivers for Intel NICs. Skeptics argue real-world deployment is far from trivial.
- **Dataset Transparency**: Critics request public datasets and reproducible setups, with one user sarcastically offering $1 to see the setup generate accurate images.

### **Author Responses**
- The authors (via a commenter) defend the approach, emphasizing its speed, efficiency, and potential for rapid adaptation to new environments. They acknowledge limitations in generalization but highlight applications in robotics or smart homes.

### **Miscellaneous Reactions**
- **Amazement vs. Cynicism**: Some find the results “insane” and revolutionary, while others dismiss it as “guesswork” or overhyped AI.
- **Cultural References**: A commenter poetically likens the tech to “Light Days,” evoking a future where quantum physics erases privacy.

### **Conclusion**
The discussion reflects a mix of fascination with the technical innovation and deep skepticism about its practicality, accuracy, and ethical implications. While the method is seen as a promising leap in wireless sensing, critics demand more rigorous validation, transparency, and ethical safeguards before accepting its real-world viability.

### Fossabot: AI code review for Dependabot/Renovate on breaking changes and impacts

#### [Submission URL](https://fossa.com/blog/fossabot-dependency-upgrade-ai-agent/) | 102 points | by [robszumski](https://news.ycombinator.com/user?id=robszumski) | [18 comments](https://news.ycombinator.com/item?id=45439721)

FOSSA launches fossabot, an AI agent that performs “strategic” dependency upgrades—researching release notes, assessing breaking changes in the context of your codebase, and even adapting application code—then delivers the work as a ready-to-merge PR. It targets the growing gap between fast-moving dependency trees and lagging flagship apps, arguing that typical updaters (Dependabot/Renovate) push small, reactive bumps while big, multi-hour upgrades sit in backlog.

What’s new
- Codebase-aware updates: Uses static analysis plus an AI reasoning layer to detect impacts based on how your app actually uses a library, not just semver.
- Handles complex upgrades: Claims it can tackle major-version or paradigm shifts (e.g., rewritten React libs) and patch-level behavior changes.
- Research at scale: Reads release notes, migration guides, and docs with “perfect memory,” citing references in PRs.
- Completed tasks, not alerts: Ships change summaries, risk assessments, and code modifications in a single PR; can ask for “last-mile” help when needed.
- Backed by an evaluation framework focused on accuracy, consistency, and correctness.

Positioning vs existing tools
- Says Dependabot/Renovate are configured “dumb” by necessity (patch-only) and don’t reason about app-specific risk.
- Example PR: A lodash patch bump with automated impact analysis across the repo and a merge recommendation.

Availability
- Public preview as a GitHub App, initially focused on JavaScript/TypeScript.
- Includes $15/month in free usage credits.
- Framed as part of FOSSA’s broader philosophy on automating updates and its EdgeBit acquisition.

Why HN will care
- If it works, it could turn the hardest part of dependency maintenance—impact analysis and code adaptation—into an automated workflow.
- Open questions: reliability on large/legacy codebases, false positives/negatives in impact detection, how it handles multi-repo monorepos, and whether teams will trust AI-authored refactors in critical paths.

**Summary of Hacker News Discussion:**

1. **Effectiveness of Static Analysis vs. Runtime Context**  
   - Users debated whether static analysis alone (as used by fossabot) suffices for dependency upgrades, especially in dynamically typed languages like Ruby.  
   - Example: Ruby 2.7→3.0 migrations involve runtime-breaking changes undetectable via static analysis.  
   - Skepticism arose about AI’s ability to handle codebases reliant on runtime behavior, deprecation warnings, or unsupported functions.  

2. **Codebase Variability Challenges**  
   - Commenters noted that dependency migrations vary widely based on codebase structure. Major version shifts (e.g., React rewrites) often require manual effort, even with AI.  
   - Assumptions that codebases are “similar” or “well-structured” might limit fossabot’s reliability in complex, legacy, or monorepo environments.  

3. **Naming Confusion & Competition**  
   - Some mistook “fossabot” for a Twitch/YouTube chat bot, highlighting potential branding issues.  
   - Questions arose about GitHub’s role: Why hasn’t GitHub improved Dependabot? Replies suggested GitHub prioritizes Copilot over dependency tooling, leaving room for third-party solutions.  

4. **Trust in AI-Generated Code**  
   - While praised for automating “time-sensitive” tasks, concerns lingered about trusting AI-authored code in critical paths. Users emphasized the need for transparency in risk assessments and code modifications.  

5. **Open-Source Inquiries**  
   - Users inquired about a FOSS version of fossabot, reflecting broader community interest in self-hostable or transparent tools.  

6. **Optimism from Maintainers**  
   - A Renovate maintainer expressed interest in testing fossabot, signaling openness to AI-driven advancements in dependency management.  

**Key Takeaways**:  
The discussion balances cautious optimism with technical skepticism. While fossabot’s automation of complex upgrades is seen as promising, its success hinges on handling real-world code variability, runtime nuances, and earning trust through reliability. Naming clarity and open-source availability could further influence adoption.

### Intelligent Kubernetes Load Balancing at Databricks

#### [Submission URL](https://www.databricks.com/blog/intelligent-kubernetes-load-balancing-databricks) | 125 points | by [ayf](https://news.ycombinator.com/user?id=ayf) | [24 comments](https://news.ycombinator.com/item?id=45434417)

Databricks: Ditching kube-proxy for client-side L7 load balancing with xDS

- The problem: Kubernetes’ default, L4 load balancing (via kube-proxy) picks a backend once per TCP connection. With gRPC/HTTP/2’s long-lived connections, that leads to uneven load, rising tail latencies, and wasted capacity. kube-proxy also lacks strategies like weighted routing, error-aware routing, and zone affinity.

- The insight: To make per-request routing decisions for persistent connections, you need L7 context and live topology—not DNS answers and per-connection hashing.

- The solution: Databricks built a proxyless, client-driven load balancer backed by a lightweight control plane that speaks xDS.
  - A custom Endpoint Discovery Service watches Kubernetes Services and EndpointSlices, enriching endpoints with zone, readiness, and shard metadata.
  - Clients (Armeria-based and API proxies) maintain a streaming xDS connection to receive live endpoint updates.
  - The client library does per-request balancing with strategies like power-of-two-choices, zone affinity, and error/latency-aware routing, plus fallback clusters.

- Why not headless services? DNS round-robin still suffers from caching and per-connection selection, lacks health/weight awareness, and can’t fix HTTP/2 connection skew.

- Why not Istio/sidecars? Powerful but operationally heavy: added latency and resource cost per pod, complex upgrades and debugging. Databricks already had a common client stack, making a lightweight, proxyless approach attractive.

- Results and trade-offs:
  - More even load, lower tail latency, and better capacity utilization by moving decisions to the client at L7.
  - Fewer critical-path dependencies on DNS or node-level proxies.
  - Requires broad client adoption and careful handling of control-plane availability, versioning, and cross-language support.

Takeaway: If you control the client stack and rely on gRPC at scale, a client-side, xDS-driven approach can outperform Kubernetes’ defaults and avoid the overhead of a full service mesh.

**Summary of Discussion:**

The discussion highlights various perspectives on client-side load balancing alternatives to Kubernetes' kube-proxy, with a focus on Databricks' xDS-driven approach. Key points include:

1. **Alternative Solutions & Comparisons:**
   - **Kuberesolver** (2018) and standardized **xDS resolvers** were noted as prior approaches, avoiding GRPC client patching. Some users favored Nginx’s native GRPC support over custom implementations.
   - **Consul** was mentioned as a viable alternative, leveraging Kubernetes integration for weighted routing and health checks.
   - **Rendezvous hashing (HRW)** was proposed, but critics argued it lacks support for dynamic factors like zone affinity or health checks, favoring client-side logic with real-time updates instead.

2. **Technical Challenges:**
   - **DNS caching issues** (e.g., 30s outages due to stale DNS entries) were a pain point, with Kuberesolver’s streaming updates seen as a fix.
   - **Scaling Kubernetes API watchers** raised concerns, though Databricks’ lightweight control plane mitigated this.
   - **Security and rate limits** when clients directly access Kubernetes API were noted as risks.

3. **Service Mesh Trade-offs:**
   - While **Istio/service meshes** offer powerful features, users criticized their operational complexity, latency, and debugging overhead. Databricks’ proxyless approach was praised for minimizing this while retaining xDS benefits.
   - Some shared frustrations with service meshes causing sidecar-related issues (e.g., certificate management, debugging "turtles all the way down" scenarios).

4. **Client-Side Advantages:**
   - **Per-request L7 decisions** (e.g., power-of-two-choices, zone affinity) were seen as superior to kube-proxy’s L4 limitations or DNS round-robin.
   - The approach’s **efficiency** (reduced connection overhead, better capacity utilization) and **simplicity** (no sidecars) resonated with users managing large-scale deployments.

5. **Protocol & Algorithm Insights:**
   - **gRPC’s move toward xDS** was highlighted as aligning with industry trends.
   - The **power-of-two-choices algorithm** sparked interest for its balance of simplicity and effectiveness in load distribution.

**Takeaway:** The community largely endorsed Databricks’ approach, emphasizing the value of client-side load balancing when controlling the client stack. Critiques centered on scalability and security trade-offs, but overall, the solution was seen as a pragmatic alternative to both kube-proxy and heavyweight service meshes.

### DARPA project for automated translation from C to Rust (2024)

#### [Submission URL](https://www.darpa.mil/news/2024/memory-safety-vulnerabilities) | 133 points | by [alhazraed](https://news.ycombinator.com/user?id=alhazraed) | [177 comments](https://news.ycombinator.com/item?id=45443368)

DARPA is launching TRACTOR (Translating All C to Rust), a program to substantially automate converting legacy C into safe, idiomatic Rust to wipe out memory-safety bugs at their source. The agency cites the dominance of memory-safety vulnerabilities and the limits of bug-finding tools, along with a cultural shift toward Rust and recent LLM breakthroughs, as the moment to try this at scale—especially given DoD’s deep C codebase. Program manager Dan Wallach says today’s LLMs can already do decent C→Rust translations “but not always”; TRACTOR aims to combine static/dynamic analysis with LLMs to match what a skilled Rust developer would produce and will run public competitions to benchmark progress. The effort aligns with calls from ONCD and CISA to move to memory-safe languages, with the promise of reducing a major class of security flaws across long-lived systems. Proposers Day was set for Aug 26, 2024 (registration by Aug 19).

**Summary of Discussion:**

The discussion revolves around DARPA's TRACTOR initiative to automate C-to-Rust translation, with mixed reactions and debates on feasibility, trade-offs, and alternatives:

1. **Existing Tools & Challenges**:  
   - Tools like **C2Rust** and **Metalift** are cited as early attempts at translation, but users note limitations (e.g., C2Rust can produce "buggy" code).  
   - Automated translation must preserve performance-critical behavior and avoid introducing errors, which remains difficult for complex codebases (e.g., JPEG 2000 decoders).  

2. **Memory Safety Claims**:  
   - Skepticism arises about whether Rust’s **static checks** (e.g., borrow checker) fully address memory safety versus **runtime checks** in alternatives like Fil-C (hypothetical language?).  
   - Fil-C is debated as a runtime-checked "memory-safe C," but some argue it sacrifices performance and doesn’t eliminate undefined behavior (UB) entirely.  

3. **Alternatives to Rust**:  
   - Users question why DARPA prioritizes Rust over languages like **Swift, Zig, or C++-with-changes**, citing diverse opinions in the ecosystem.  
   - Fil-C (or similar approaches) could theoretically avoid Rust’s steep learning curve while adding safety via instrumentation, but trade-offs in runtime overhead are noted.  

4. **Performance vs. Safety Trade-offs**:  
   - Projects requiring **low-level control** (OS kernels, game engines) may reject Rust due to runtime costs, favoring Fil-C or C-with-instrumentation.  
   - **Rust’s static checks** are praised for preventing vulnerabilities like use-after-free (UAF), but critics argue they’re insufficient for all memory-safety issues (e.g., logic errors).  

5. **Practicality of Rewriting**:  
   - Rewriting legacy systems in Rust is seen as labor-intensive, with risks of **regressions** and unclear benefits for performance-sensitive code.  
   - Some argue the effort would be better spent on **instrumentation tools** (e.g., ASAN) or gradual adoption of safer language subsets.  

6. **Cultural & Ecosystem Factors**:  
   - Fil-C adoption is deemed unlikely due to the momentum behind Rust and its growing community (“Rustaceans”).  
   - Debates highlight tensions between **“rewrite everything in Rust”** enthusiasm and pragmatic acceptance of incremental improvements to existing C/C++ codebases.  

**Key Takeaway**: While TRACTOR’s goals align with broader security priorities, the discussion underscores skepticism about fully automated translation, advocacy for alternative approaches, and unresolved debates over performance-safety trade-offs in memory-safe language adoption.

### Evaluating the impact of AI on the labor market: Current state of affairs

#### [Submission URL](https://budgetlab.yale.edu/research/evaluating-impact-ai-labor-market-current-state-affairs) | 139 points | by [Bender](https://news.ycombinator.com/user?id=Bender) | [175 comments](https://news.ycombinator.com/item?id=45442743)

AI and jobs: broad labor-market disruption hasn’t shown up yet

- Scope: 33 months after ChatGPT’s release, researchers compare how the U.S. occupational mix has shifted versus past tech waves (computers, internet), and test whether AI “exposure/automation/augmentation” metrics correlate with employment or unemployment changes.

- Main finding: No discernible economy‑wide job disruption so far. The occupational mix is changing slightly faster than in past episodes, but not by much—and much of the shift predates generative AI’s rollout.

- Context: In the late ’90s/early 2000s internet era, the occupational mix was ~7 percentage points different after six years; today’s change is only about 1 point higher on a comparable basis.

- Exposure ≠ outcomes (yet): Occupations rated as highly exposed to AI don’t show systematic differences in employment or unemployment trends so far.

- Industry view: Information, Financial Activities, and Professional/Business Services show somewhat larger mix shifts, but overall patterns still look limited and in line with recent pre‑AI trends.

- Why this might be: Large-scale workplace tech shifts typically unfold over decades; diffusion, reorganization, and workflow redesign take time.

- Caveats: The metric captures change, not cause; early shifts started before genAI; results aren’t predictive; better, more granular data are needed. The team plans monthly updates to track evolving impacts.

**Summary of Discussion:**

The discussion revolves around whether AI's impact on jobs mirrors historical technological disruptions, particularly the Industrial Revolution, and debates the current evidence of AI-driven labor market changes.

1. **Skepticism About Immediate AI Impact**:  
   - Participants note that while companies hype AI to justify layoffs and push productivity, real-world AI adoption remains in early stages, with limited movement beyond prototypes. Changes may unfold over decades, requiring workflow redesign and organizational shifts.

2. **Historical Parallels**:  
   - Comparisons to the Industrial Revolution highlight that technological advances initially worsened labor conditions (e.g., child labor, dangerous factories) but eventually increased productivity and labor participation. However, these gains often required labor movements to address exploitation.  
   - The Luddite analogy is debated: while 19th-century textile workers resisted mechanization that devalued their skills, modern AI resistance is seen as distinct due to differing economic contexts and ethical concerns (e.g., fast fashion’s environmental and labor costs).

3. **Labor Conditions and Transitions**:  
   - The shift from agrarian economies to factories involved harsh conditions, but displaced workers had few alternatives. Similarly, AI’s disruption might force transitions, but outcomes depend on societal responses (e.g., regulations, worker protections).  
   - Some argue that productivity gains historically benefited capital owners first, with labor improvements lagging until collective action (unions, laws) intervened.

4. **Economic Models and Time Lags**:  
   - Participants reference studies showing that technological adoption’s full effects take decades. For AI, measurable job impacts may not emerge until 2025–2030, aligning with historical patterns of slow diffusion.  
   - The "urban penalty" (lower life expectancy in cities pre-20th century) is noted as a cautionary example of how initial disruptions can have hidden costs.

5. **Ethical and Global Considerations**:  
   - Modern parallels include outsourcing and exploitative practices in developing nations, raising questions about whether AI could exacerbate inequality without systemic safeguards.

**Conclusion**: The discussion underscores cautious optimism tempered by historical lessons—AI’s potential for productivity gains is acknowledged, but participants stress the need for proactive policies and labor advocacy to mitigate adverse effects, mirroring past reforms.

### JetBrains will start training AI on your code on non-commercial license

#### [Submission URL](https://blog.jetbrains.com/blog/2025/09/30/detailed-data-sharing-for-better-ai/) | 77 points | by [Ianvdl](https://news.ycombinator.com/user?id=Ianvdl) | [37 comments](https://news.ycombinator.com/item?id=45440117)

JetBrains asks devs to share real IDE data to train better AI, makes non‑commercial users opt-in by default

JetBrains says today’s LLMs are trained on public code that doesn’t reflect complex, real-world workflows, so their AI stumbles on professional use cases. To fix that, it’s launching an expanded data-sharing program to feed models with actual IDE activity.

What’s changing
- Companies: Admins can enable company-wide data sharing. JetBrains is offering a limited number of free All Products Pack subscriptions to early adopters.
- Individuals (non‑commercial licenses): Data sharing is ON by default; you can turn it off in settings.
- Individuals (commercial licenses, trials, free community, EAP): No change; you can opt in via settings (subject to admin policy).

What data they want
- Two layers:
  - Existing anonymous telemetry (feature usage, time spent, clicks).
  - New, detailed code-related data: edit history, terminal usage, interactions with AI features, including code snippets, prompts, and AI responses.

Why JetBrains says it matters
- Train models on real workflows to reduce hallucinations and logic gaps.
- Smarter code completion and explanations; fewer false positives.
- Better security posture: detect and filter unsafe code.
- Lower cost for high-volume, low-intelligence tasks vs. using a general foundation model alone.

Privacy stance
- Data sharing is optional; admins control it for orgs.
- Claims of EU data protection compliance, restricted access, and no sensitive/personal information shared.
- More details available in their data collection and protection docs.

Extras
- JetBrains cites promising internal results using real data.
- Its code-completion LLM, Mellum, is open source on Hugging Face and Amazon Bedrock.

Why it’ll spark debate on HN
- Default-on for non‑commercial users and collection of code/terminal history will raise IP and privacy concerns.
- The value proposition—AI that truly understands professional workflows—may appeal to teams frustrated by generic LLM behavior.

The Hacker News discussion about JetBrains' opt-in-by-default data-sharing program highlights polarized views, with key themes including:

### **Privacy and Trust Concerns**
- **Opt-in-by-default for non-commercial users** sparks backlash, seen as exploitative ("paying with data instead of money"). Critics argue this undermines trust, especially for hobbyists or open-source contributors.
- **Data collection scope** (code snippets, terminal history, AI interactions) raises IP and privacy fears. Users question whether JetBrains has legal rights to use code from open-source projects for AI training.
- **Skepticism about transparency**: Some doubt JetBrains’ claims of EU compliance and anonymization, citing past grievances like UI changes forced without consent.

### **Defense of JetBrains’ Value**
- **Productivity advocates** praise JetBrains IDEs (e.g., IntelliJ, Rider) for stability, deep language support, and workflow efficiency compared to competitors like VS Code or Eclipse.
- **Business rationale**: Supporters argue the move aligns with improving AI tools for professional workflows, and opt-out options exist for those concerned.

### **Criticism of Recent Trends**
- **UI redesigns** are divisive: Some appreciate reclaiming screen space, while others label them "unnecessary churn" disrupting muscle memory.
- **Erosion of goodwill**: Long-time customers feel alienated by perceived profit-driven shifts, citing forced updates and opaque policies. Some threaten to cancel subscriptions.

### **Alternatives and Workarounds**
- Users suggest switching to **VS Code**, **Neovim**, or **Eclipse** for greater control. Others highlight challenges (e.g., C# development in Neovim lacking Razor Pages support).
- Technical workarounds: Blocking JetBrains telemetry via MDM tools or disabling data sharing immediately post-install.

### **Ethical and Licensing Debates**
- Questions about **open-source compliance**: Can JetBrains legally train models on code from projects with restrictive licenses (e.g., GPL)?
- **Opt-out friction**: Concerns that non-technical users might unknowingly contribute data, despite settings to disable sharing.

### **Overall Sentiment**
- **Mixed reactions**: While some see potential in AI trained on real workflows, distrust dominates. Critics view this as a slippery slope toward data exploitation, while supporters emphasize pragmatic trade-offs for better tools. The debate reflects broader tensions between innovation, user autonomy, and corporate control in AI development.

