## AI Submissions for Sun May 05 2024 {{ 'date': '2024-05-05T17:10:26.067Z' }}

### Deep Reinforcement Learning: Zero to Hero

#### [Submission URL](https://github.com/alessiodm/drl-zh) | 461 points | by [alessiodm](https://news.ycombinator.com/user?id=alessiodm) | [41 comments](https://news.ycombinator.com/item?id=40269489)

Today on Hacker News, a repository called "drl-zh" by alessiodm has been making waves. It offers a practical and hands-on course on deep reinforcement learning, guiding users through foundational algorithms like DQN, SAC, PPO, and more. By the end of the course, learners can train AI to play Atari games and even land on the Moon!

The setup for this course is straightforward, with a focus on ease of learning. It recommends using Miniconda for creating a virtual environment, installing Poetry for dependencies, and Visual Studio Code for coding. The course starts with an introductory notebook and progresses through various topics, providing solutions in case you get stuck.

For those interested in delving into deep reinforcement learning, "drl-zh" seems like a promising resource to level up your skills in this cutting-edge field.

### Swift's native Clocks are inefficient

#### [Submission URL](https://wadetregaskis.com/swifts-native-clocks-are-very-inefficient/) | 127 points | by [mpweiher](https://news.ycombinator.com/user?id=mpweiher) | [75 comments](https://news.ycombinator.com/item?id=40262897)

Today's top story on Hacker News delves into the intricacies of time tracking in Swift programming. The author highlights the inefficiencies of using Swift's native Clock protocol for frequent time checks and proposes using alternative methods like ContinuousClock and SuspendingClock for better performance. Surprisingly, the author discovers that mach_absolute_time, the lowest level time-tracking method, outperforms other APIs significantly.

Further, the author draws attention to the competitiveness of the Date class, even rivaling traditional C-level APIs in speed. This performance revelation leads to a deeper analysis of how Date functions efficiently, with minimal overhead, interspersed with insights on the evolution of Swift's Foundation framework.

The post concludes with a follow-up section acknowledging the positive reception on Hacker News and addressing queries regarding mach_absolute_time's restricted usage by Apple. It's a deep dive into the nuances of time tracking in Swift, shedding light on performance optimizations and insightful user interactions.

The discussion on the time tracking methods in Swift programming touches upon various aspects like mitigating Spectre vulnerability, the efficiency of Date and Clock APIs, implications of using mach_absolute_time, and privacy concerns related to APIs. Users delve into technical details such as the impact of Meltdown and Spectre vulnerabilities on different languages, the challenges in securing iOS and Android platforms, the performance comparisons between different time tracking methods, and the trade-offs between security and user convenience.

Additionally, there are discussions on the complexities of software development, privacy declarations in code, the design considerations in preventing application developers from making mistakes, and the optimization of code for efficient performance. Users also explore the nuances of measuring time accurately, misconceptions in optimization strategies, and the performance implications of design choices in code execution.

### A High-Level Technical Overview of Homomorphic Encryption

#### [Submission URL](https://www.jeremykun.com/2024/05/04/fhe-overview/) | 129 points | by [ggeorgovassilis](https://news.ycombinator.com/user?id=ggeorgovassilis) | [21 comments](https://news.ycombinator.com/item?id=40262626)

The author provides a high-level technical overview of Fully Homomorphic Encryption (FHE), explaining how it allows running programs directly on encrypted data without decryption. This cryptographic technique involves encrypting data in a way that enables performing computations on it without revealing the underlying information. The key concept is based on encryption schemes that are compatible with addition and multiplication operations.

FHE enables universal computation by expressing programs as boolean circuits, where XOR gates represent addition and AND gates represent multiplication. Despite its potential, there are challenges such as the slowness of simulating circuits due to cryptographic overhead, limitations on loop and transcendental function computations, and bandwidth concerns related to data size increase and key exchange.

The article explores the commonalities of FHE schemes and discusses approaches to address these limitations in subsequent sections. It offers insights into the intricacies of FHE, making the complex subject more accessible for software engineers interested in this cutting-edge cryptographic technology.

The comments on the Hacker News submission regarding Fully Homomorphic Encryption (FHE) cover various aspects of the topic:

- Users discuss the technical details and challenges of implementing FHE, such as the complexity of computations and the size of encrypted data.
- Some users mention other cryptographic concepts like functional encryption and partially homomorphic encryption, comparing them to FHE.
- There is a discussion about the potential applications and benefits of using FHE, despite the computational overhead it introduces.
- Users debate the dangers and implications of developing powerful AI systems that could potentially access encrypted computations.
- Finally, there is a conversation about the trustworthiness of computational frameworks and theoretical concerns surrounding encryption technologies. 

Overall, the discussion provides a broad overview of the technical, security, and ethical considerations related to FHE and its implications in the field of cryptography.

### Infini-Gram: Scaling unbounded n-gram language models to a trillion tokens

#### [Submission URL](https://arxiv.org/abs/2401.17377) | 128 points | by [nsagent](https://news.ycombinator.com/user?id=nsagent) | [50 comments](https://news.ycombinator.com/item?id=40266791)

The paper titled "Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion Tokens" by Jiacheng Liu and team explores the relevance of $n$-gram language models vis-a-vis neural large language models. By training at the same scale as neural LLMs (5 trillion tokens) and introducing $\infty$-gram LM with backoff, the authors showcase the potential of $n$-gram LMs in text analysis and enhancing neural LLMs. The novel infini-gram engine, powered by suffix arrays, enables efficient computation of $\infty$-grams with millisecond-level latency for next-token prediction. Their findings suggest that $\infty$-gram LM can aid in reducing neural LLM perplexity and reveal insights into deficiencies in neural LLM pretraining and Transformer positional embeddings. This research pushes the boundaries of language modeling and offers valuable insights for both text analysis and model improvement.

The discussion on Hacker News surrounding the submission about the paper "Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion Tokens" by Jiacheng Liu and team delved into various aspects of language models, artificial intelligence, and the implications of the research findings:

1. Discussion on the relevance and refinement of current mental models of large language models (LLMs), the incorporation of attention mechanisms in models, the limitations in neural LLMs, and the potential benefits of n-gram language models in improving neural LLMs.

2. Conversation on human stochastic processes and learning, the comparison between toddlers' learning processes and the challenges faced by artificial neural networks in understanding concepts, and the utility of n-grams in basic arithmetic and language learning.

3. Exploration of creating small group personalities for internal dialogue generation, the concept of stream consciousness in internal decisions, and the role of neural LLMs in extrapolating experiences and synthesizing coherent thoughts.

4. Debate on the extrapolation of experiences in artificial intelligence, the limitations of statistical models in comparison to human intelligence, and the fundamental differences between AI capabilities and human cognition.

5. Analysis of sophisticated data structures like compressing suffix trees and the utilization of massive corpora for training language models.

6. Mention of the Infini-gram engine by Hugging Face and the expectations in large-scale language model training.

7. Critique on the perplexity results and the presentation of training data in machine learning models.

Overall, the discussion touched upon a wide range of topics, from the technical aspects of language models to the philosophical considerations of human intelligence and the challenges faced by artificial neural networks in emulating human cognitive abilities.

### Flying planes in Microsoft Flight Simulator with a JavaScript autopilot (2023)

#### [Submission URL](https://pomax.github.io/are-we-flying/) | 264 points | by [TheRealPomax](https://news.ycombinator.com/user?id=TheRealPomax) | [57 comments](https://news.ycombinator.com/item?id=40267164)

The submission on Hacker News titled "Are we flying" dives into the fascinating world of controlling a virtual airplane in Microsoft Flight Simulator 2020 using JavaScript. The author clarifies that they are not controlling real aircraft but rather manipulating a virtual plane within the game through an autopilot system written in JS. Leveraging SimConnect SDK, which allows interaction with the game, the author details their journey of creating a web page to control the plane, visualize flight data, and even develop an autopilot system with unique features.

The article outlines the process of setting up the necessary components, from SimConnect API servers to creating a web server, and utilizing web sockets for bidirectional communication. The project structure is divided into six parts, including preparation work, visualization of flight data, autopilot creation, defining flight paths, testing with different plane types, and the exciting aspect of letting JavaScript fly the plane autonomously.

Readers are invited to explore the detailed tutorial on the "Are we flying?" Github repository, where they can learn about coding an autopilot, plotting flight data, and experiencing the thrill of automated takeoffs and landings. The article promises a blend of technical challenges, innovative solutions, and the joy of witnessing JavaScript control a virtual aircraft in a stunning game environment.

The discussion on the Hacker News submission "Are we flying" covers a range of topics related to developing autopilot systems in Microsoft Flight Simulator 2020 using JavaScript. Some of the key points include:

1. Mention of Asobo Studio currently hiring an Autopilot Programmer.
2. A request for the tutorial to cover positioning, elevation data, and more.
3. A debate on the complexities of developing autopilots for different types of planes and the role of templates in autopilot systems.
4. Discussions on the importance of altimeter settings and navigation systems in flying, with references to Instrument Landing Systems and Ground Proximity Warning Systems.
5. Comparisons between JavaScript and Lua for executing code within Flight Simulator, with considerations for performance and language characteristics.
6.  References to NASA's X-Plane Connect API and its utility for research purposes.
7. Reference to a detailed X-Plane tutorial and discussions on PID controllers in control systems.

Overall, the discussion delves into technical aspects of flight simulation, programming challenges, and comparisons between different languages for implementing aviation control systems.

### Machine Unlearning in 2024

#### [Submission URL](https://ai.stanford.edu/~kzliu/blog/unlearning) | 304 points | by [ignoramous](https://news.ycombinator.com/user?id=ignoramous) | [84 comments](https://news.ycombinator.com/item?id=40264352)

In May 2024, Ken Liu delves into the compelling world of machine unlearning, a concept gaining traction as machine learning models expand in size and complexity. Unlearning involves removing undesired elements like private data, copyrighted material, and harmful content from trained models without starting from scratch. The article explores the history and motivations behind unlearning, spurred by regulations like the GDPR's right-to-be-forgotten. As models evolve to encompass a wide range of data and tasks, the need for unlearning has grown beyond privacy concerns to encompass issues of access revocation and model correction. The post provides insights into the challenges, techniques, and potential solutions in the realm of machine unlearning, offering a thought-provoking look at the future of AI ethics and model governance.

The discussion on the submission about machine unlearning on Hacker News raised several interesting points. Users discussed practicality in implementing unlearning methods, highlighting the need for real-world applications and legal acceptance. They also delved into the technical aspects of unlearning, such as the challenges in legal compliance and the accuracy of deleted data. Additionally, there were debates on the effectiveness of certain technologies like Markov chains and Deep Learning, with some questioning their utility in this context. The conversation also touched upon ethical considerations regarding the deletion of copyrighted content and the potential implications of using machine learning models for commercial purposes without proper authorization. Finally, there were discussions about the challenges of enforcing laws related to machine unlearning, such as handling copyright infringement and ensuring accountability.

### Automated integer hash function discovery

#### [Submission URL](https://github.com/skeeto/hash-prospector) | 262 points | by [danny00](https://news.ycombinator.com/user?id=danny00) | [39 comments](https://news.ycombinator.com/item?id=40261681)

It seems like the content you pasted is related to a GitHub repository called hash-prospector by the user skeeto. The repository contains a tool for automated integer hash function discovery, with the ability to generate various hash functions based on reversible operations and evaluate their avalanche behavior. The tool can create both 32-bit and 64-bit integer hash functions using random generation and optimization techniques like hill climbing and genetic algorithms.

One notable discovery from the tool is a 32-bit, two-round permutation function with low bias that rivals the performance of established hash functions like MurmurHash3. The tool also found several other hash functions with different rounds and bias levels, showcasing the variety of functions it can generate. These functions are printed out in C syntax for practical usage.

Overall, the hash-prospector tool seems to be a powerful resource for exploring and developing integer hash functions, offering a way to discover optimized functions for various applications.

Here is a summary of the discussion on the Hacker News comments regarding the submission about the GitHub repository hash-prospector by user skeeto:

1. Users skssn, jnn, and cyfx highlighted various projects and libraries related to hash functions, JSON parsing libraries, branchless UTF-8 decoders, and stack libraries.
2. User pplby discussed their interest in hash finding based on developing good hash functions and mentioned the SMHasher project developed by Frank Wojcik for testing and improving hash functions.
3. User throwaway_1237 requested an explanation of hash functions, which led to a detailed discussion about generating custom hash functions using genetic programming and reversible steps for cryptographic hash functions.
4. Users AgentOrange1234, msklnn, and lg touched upon the importance of hash tables, hash functions for data structures, and the efficiency of integer hash functions in mapping values efficiently.
5. Users dtngl, nfglch, and o11c explored topics like hardware support for hash functions, customizing perfect hash functions to avoid collisions, and the limitations of reversible permutations in hash functions.
6. User thmsmg mentioned the impact of using single constant multiplications on reducing the code size and slightly speeding up computation in hash functions.
7. Users rnnc, pbsd, rrbn, and others delved into discussing the comparison of integer hash functions with cryptographic primitives, the role of randomness in hash function design, and methods for finding suitable hash functions with cryptographic properties.

This discussion reflects a deep engagement with the intricacies of hash function development, data structures, cryptographic properties, and performance optimization within the Hacker News community.

### The long long tail of AI applications

#### [Submission URL](https://blog.waleson.com/2024/05/the-long-long-tail-of-ai-applications.html) | 14 points | by [jtwaleson](https://news.ycombinator.com/user?id=jtwaleson) | [3 comments](https://news.ycombinator.com/item?id=40268011)

The author explores the long tail of AI applications where foundational AI like GPT-4 and applied AI companies play a crucial role. They highlight the challenges faced in utilizing large language models (LLMs) effectively. Firstly, it's essential to ask the right questions to LLMs to receive accurate responses, a task that requires significant input from human intelligence. Secondly, LLMs have limited context by default, necessitating manual programming of agents to provide relevant information. Thirdly, LLMs are not AGI and require structured commands for tasks, making agent programming essential. Fourthly, LLMs struggle with specialized problems as they lack access to non-public or non-English information. Lastly, integrating AI into products demands substantial effort, from framing questions to providing context, which indicates a significant workload for applied AI companies in the foreseeable future.

The discussion revolves around the importance of structuring and designing workflows to accomplish tasks effectively in real-world applications. One user emphasizes the significance of understanding human intelligence and the differences between AI, AGI, and human productivity in this context. Another user highlights the vital role of structuring information and building good applications to ensure successful task completion, mentioning the complexities of real-world use cases and the challenges faced in integrating models into applications effectively.

### Google Fit dev APIs shutdown set, fate of Android, Wear OS apps go unannounced

#### [Submission URL](https://9to5google.com/2024/05/04/google-fit-api-shutdown/) | 57 points | by [thunderbong](https://news.ycombinator.com/user?id=thunderbong) | [19 comments](https://news.ycombinator.com/item?id=40262082)

Google Fit developers APIs are approaching their sunset, with a deadline set for June 30, 2025. The transition to Health Connect, dubbed the "Android Health platform," is on the agenda, but some functionalities like the Goals API face uncertainty. The future of Google Fit apps on Android, Wear OS, and iOS remains vague, as Google has not yet disclosed their fate. Users are pondering where to migrate their data as Google Fit loses ground to other health tracking solutions. Stay tuned for updates at Google I/O for more on the future of Android Health.

The discussion on the submission about Google Fit APIs approaching their sunset includes various perspectives. Some users appreciate the centralized approach of Google Health Connect, enabling seamless integration of diverse health tracking apps. They discuss scenarios where apps sync health data background services and the advantages of Google Health Connect's 30-day permission for accessing records. A user also shares their experience with using a Wi-Fi scale and the challenges faced with Bluetooth connectivity for syncing. Additionally, there's a mention of Google Fit storing cloud data on Google Health Connect.

Furthermore, the conversation touches on the potential motivations behind Google's API changes, suggesting possible strategic shifts in their offerings. Users debate the benefits of open-source software and the pitfalls of centralized development. They discuss Google's approach to discontinuing services like Fitbit and the potential implications for user experience design changes with Fitbit integration into Google Fit. There are also comments on the perceived quality decline in Fitbit's UI after its integration into Google Fit. Lastly, someone mentions Google's search quality decline as a sideline topic.

### SEQUOIA: Exact Llama2-70B on an RTX4090 with half-second per-token latency

#### [Submission URL](https://infini-ai-lab.github.io/Sequoia-Page/) | 127 points | by [zinccat](https://news.ycombinator.com/user?id=zinccat) | [60 comments](https://news.ycombinator.com/item?id=40261965)

The Sequoia project introduces a cutting-edge speculative decoding framework that revolutionizes the speed and efficiency of serving large language models on consumer GPUs without compromising accuracy. By leveraging a large speculation budget, Sequoia achieves remarkable results, such as serving a Llama2-70B model on an RTX-4090 with an impressive latency of only 0.57 seconds per token. This is a significant improvement compared to existing serving systems, making it 8 times faster than a highly optimized offloading serving system.

Sequoia's scalability and robustness make it stand out in the field of speculative decoding. Its ability to adjust the size and depth of speculation trees based on hardware platforms ensures optimal performance across different configurations. The framework's innovative approach, including dynamic programming algorithms and sampling without replacement techniques, sets it apart in terms of efficiency and adaptability.

Moreover, Sequoia's potential impact extends beyond current hardware capabilities, as it is expected to perform exceptionally well on future hardware generations with increased compute and bandwidth ratios. This forward-looking approach makes Sequoia a promising solution for hosting powerful language models like the 70B variant on various low-cost consumer GPUs, opening up new possibilities for AI-generated content applications.

In conclusion, Sequoia's groundbreaking advancements in speculative decoding set a new standard for speed, scalability, and hardware-awareness in serving large language models. The project's emphasis on adaptability and future-proofing makes it a key player in the evolving landscape of AI technologies.

The discussion on the submission revolves around the OpenAI's GPT-4 model and its implications. Some users express skepticism regarding the progress and efficiency of GPT-4, citing concerns about its capability to compete with newer models like GPT-5. Others discuss the speculation surrounding OpenAI's secretive projects and the potential impact on the industry. There are debates on the performance and efficiency of GPT-4 compared to models like Claude 3 and Gemini. Additionally, the conversation touches upon the financial investments in OpenAI, the challenges of releasing advanced AI models, and the strategic decisions made by companies in the AI space. Discussions also include technical aspects such as model training, scalability, hardware utilization, and the future development of AI technologies.

### Show HN: I built a movie-recommending app that eliminates the pain of choosing

#### [Submission URL](https://moodiemoovy.netlify.app) | 8 points | by [sdotdev](https://news.ycombinator.com/user?id=sdotdev) | [7 comments](https://news.ycombinator.com/item?id=40264797)

Today on Hacker News, a new project called MoodieMoovy caught the community's attention. MoodieMoovy offers a unique way to decide what movie to watch by choosing based on mood, day, and time of day instead of scrolling through endless options. Whether you're looking for a nostalgic Friday afternoon film or a random selection, MoodieMoovy uses TMDB and JustWatch to help you find the perfect movie for the moment. Check it out and say goodbye to endless browsing!

The discussion primarily revolves around the new project, MoodieMoovy, which aims to help users decide what movie to watch based on their mood, day, and time of day. Al_borland faced an issue with the display not working properly on their iPhone 13 in portrait mode, but the problem has been fixed. They also received recommendations for relaxing Sunday afternoon movies and foreign language films with English subtitles. Sdtdv, the creator of MoodieMoovy, expressed interest in finding better movie recommendations and suggested utilizing an AI to enhance the selection process. The discussion also touched on implementing search features and receiving feedback for sharing content. Overall, the community seems supportive of the project, with some members already finding it useful.

