## AI Submissions for Sat Jan 03 2026 {{ 'date': '2026-01-03T17:08:16.502Z' }}

### Scaling Latent Reasoning via Looped Language Models

#### [Submission URL](https://arxiv.org/abs/2510.25741) | 78 points | by [remexre](https://news.ycombinator.com/user?id=remexre) | [13 comments](https://news.ycombinator.com/item?id=46481849)

TL;DR: A new open-source family of “Looped Language Models” (LoopLM), called Ouro, bakes multi-step reasoning into pretraining by letting the model iterate in latent space with a learned, dynamic depth. Small models (1.4B/2.6B) reportedly match the reasoning performance of state-of-the-art models up to 12B params, trained on 7.7T tokens.

What’s new
- Pretraining for reasoning, not just post-training prompts: Instead of relying on chain-of-thought at inference, Ouro trains models to perform iterative computation internally during pretraining.
- Latent loops + learned depth: The model “thinks” via internal loops in its hidden states, with an entropy-regularized objective that encourages it to allocate just enough steps per input.
- Scales well at small sizes: Ouro 1.4B and 2.6B are claimed to match much larger 12B models across diverse benchmarks.

Why it matters
- Smaller, smarter models: If latent looping reliably boosts reasoning, you can get big-model reasoning on smaller footprints—promising for cost, latency, and edge deployments.
- Beyond verbose CoT: Internal loops could reduce dependence on long chain-of-thought outputs (fewer tokens, less leakage), while keeping or improving reasoning quality.
- Manipulation > memory: Authors argue gains come from better “knowledge manipulation” rather than just more parameters or data memorization.

How it works (at a glance)
- Iterative hidden-state updates: The network applies multiple internal reasoning steps before emitting tokens.
- Dynamic depth via entropy regularization: A training objective that nudges the model to adaptively decide how many internal steps to take.
- Massive pretraining: Trained on 7.7T tokens to make the looped computation robust and general.

Notable claims
- 1.4B/2.6B Ouro models match up to 12B SOTA LLMs on a wide range of reasoning benchmarks.
- Reasoning traces are more aligned with final answers than typical chain-of-thought outputs.
- Controlled experiments suggest improvements come from how the model uses knowledge, not just how much it stores.

Caveats and open questions
- Inference cost/latency: Latent loops don’t emit tokens, but they still add compute—what’s the real-world speed/cost trade-off?
- Generality and robustness: How widely do the gains hold across domains and languages not in the benchmark set?
- Practical integration: Tool use, retrieval, and guardrails with looped inference remain to be validated at scale.

Availability
- The authors say models are open-sourced and provide a project page; details and weights are reportedly available. Authors include Yoshua Bengio and collaborators.

**Hacker News Discussion Summary**

The discussion focused on the architectural mechanics of Ouro and the safety implications of its opaque reasoning process.

*   **comparisons to ODEs and Universal Transformers:** Commenters drew strong parallels between Ouro and "Universal Transformers" or Neural ODEs (Ordinary Differential Equations), effectively describing the model as a solver that iterates in latent space. There was a technical debate regarding "flow-matching" in language models; users clarified that while language inputs and outputs are discrete tokens, the internal operations (and thus the looping) occur in a continuous multi-dimensional vector space, allowing for smooth interpolation.
*   **The "Black Box" Safety Concern:** A significant portion of the thread debated the interpretability of "latent loops." Unlike standard Chain-of-Thought (CoT), which produces human-readable reasoning steps, Ouro's internal steps are abstract vector manipulations not mapped to the vocabulary. Users compared this to "Coconut" models (Continuous Chain of Thought), noting that while this method is computationally efficient, it poses a safety risk because the "thought process" is illegible to humans and harder to monitor or guardrail.
*   **Visualizing the Architecture:** Participants used pseudo-code to illustrate the difference between Ouro and standard LLMs. While traditional models pass data through a fixed stack of distinct layers (Layer 1 $\to$ Layer 2 $\to$ ...), Ouro was described as looping through the *same* layer structure iteratively. It was noted that this depth is dynamic: the model runs more loops for difficult tokens and fewer for easy ones before outputting a result.

### Recursive Language Models

#### [Submission URL](https://arxiv.org/abs/2512.24601) | 147 points | by [schmuhblaster](https://news.ycombinator.com/user?id=schmuhblaster) | [23 comments](https://news.ycombinator.com/item?id=46475395)

Recursive Language Models: pushing LLMs past context limits by letting them call themselves

- What’s new: Alex L. Zhang, Tim Kraska, and Omar Khattab propose “Recursive Language Models” (RLMs), an inference-time strategy where the LLM treats a long prompt as an external environment, programmatically scans/decomposes it, and recursively calls itself on relevant snippets.
- Why it matters: This aims to break fixed context windows without retraining. The authors report handling inputs up to two orders of magnitude longer than the model’s context and, even on shorter prompts, outperforming base LLMs and common long‑context scaffolds across four diverse tasks—at comparable or lower per‑query cost.
- How it works (high level): The model acts as a controller that decides what to read next, how to chunk, and when to recurse—an instance of “inference‑time scaling” where more compute and structure at inference improve quality.
- For builders: If validated, this could offer a simpler alternative to bespoke long‑context pipelines, with potential gains in quality and cost. Open questions include latency/compute trade‑offs, robustness of the controller loop, and failure modes on messy real‑world corpora.

Paper: “Recursive Language Models” (9 pages + 24pp appendix)
arXiv: 2512.24601 (cs.AI, cs.CL) — authors’ claims based on four long‑context tasks
PDF: https://arxiv.org/pdf/2512.24601

Here is a summary of the discussion:

**Is this just Agents/RAG by another name?**
A significant portion of the discussion focused on terminology and classification. Several users argued that "Recursive Language Models" is mostly a rebrand of existing "subagent" architectures or "agentic scaffolds" (like BabyAGI or workflows used in Cursor and Claude Code).
*   **Recursion vs. Depth:** Commenters noted that if the system only goes one level deep (Main -> Subagent), as some suggested the paper implies, calling it "recursive" is a stretch; it is effectively just a subagent workflow.
*   **Task vs. Context Decomposition:** User *wsbdnr* offered a nuanced distinction: while standard agentic workflows usually view multiple calls as *task* decomposition, this paper frames it as *context* decomposition—treating the text as an environment to be navigated.

**RAG vs. RLM**
Users debated how this differs from standard Retrieval-Augmented Generation (RAG).
*   **The "Auto-RAG" Shift:** *bob1029* and *NitpickLawyer* identified the key innovation: in standard RAG, a human developer hard-codes the retrieval logic (chunking, embedding, searching). In RLM, the LLM itself acts as the controller, dynamically deciding what to read, search, or "page in" from the text-as-environment.
*   **Environment:** The consensus was that RLM treats long prompts as an external environment for the model to interact with symbolically, rather than just a stream of tokens to digest.

**Implementation & Training**
*   **Inference, not Weights:** Users clarified for those misled by the title that this is purely an inference-time strategy (scaffolding/prompting) and does not involve training new model weights or differentiable architectures.
*   **Tooling Wishlist:** The discussion touched on the desire for major providers (OpenAI, Anthropic) to expose these types of "computation hooks" in their APIs, allowing developers to inspect or swap the context-management logic (like that used in Claude Code) rather than interacting with opaque black boxes.
