## AI Submissions for Wed Apr 30 2025 {{ 'date': '2025-04-30T17:14:17.044Z' }}

### Xiaomi MiMo Reasoning Model

#### [Submission URL](https://github.com/XiaomiMiMo/MiMo) | 466 points | by [thm](https://news.ycombinator.com/user?id=thm) | [179 comments](https://news.ycombinator.com/item?id=43842683)

In an intriguing development within the AI community, Xiaomi has unveiled MiMo, a series of cutting-edge language models explicitly designed to maximize reasoning capabilities. Dubbed MiMo-7B, these models demonstrate extraordinary potential, giving larger models like the 32B a run for their money. What sets MiMo-7B apart is its rigorous pre-training and post-training regimen, aiming to optimize reasoning tasks rather than just language processing.

Key to MiMo's success is a refined data preprocessing pipeline and a strategic approach to pre-training. By augmenting their datasets with extensive synthetic reasoning data and employing a three-stage data mixture, the MiMo-7B model is prepped on a whopping 25 trillion tokens. This not only enhances reasoning patterns but also boosts efficiency in processing.

Post-training doesn't take a back seat; Xiaomi's MiMo team incorporated 130,000 challenging mathematics and code problems into reinforcement learning (RL) scenarios. Using rule-based accuracy for validation and innovative fine-grained reward systems, they've overcome traditional RL challenges like sparse rewards.

For the tech-savvy and curious, MiMo models are accessible on platforms like HuggingFace and ModelScope, complete with the MiMo-7B-Base and MiMo-7B-RL versions. The RL model, despite being smaller, rivals well-known models like OpenAI's o1-mini in terms of performance, especially in mathematics and code tasks.

Evaluation metrics further affirm MiMo's prowess, topping popular benchmarks and showcasing its robust reasoning and code-solving capabilities. With this release, Xiaomi not only propels its technological footprint but also provides a significant contribution to the open-source AI community, offering valuable insights for future advances in reasoning-focused language models.

**Hacker News Discussion Summary:**

1. **Language Model Focus: English vs. Chinese**  
   - Users debated the underrepresentation of Chinese-focused models in Western discourse. While companies like Xiaomi and DeepSeek (via 01.AI) are developing Chinese-first models, challenges include:
     - **Data Availability:** Common Crawl data—integral to training LLMs—is heavily English-dominated (43%), making non-English models harder to scale. Chinese data is harder to collect due to restricted internet access (e.g., censorship, "closed gardens" like Baidu) and lower-quality search results compared to Google.
     - **Benchmarks and Resources:** Scientific research and benchmarks are often English-centric, complicating the evaluation of Chinese models. Some argue synthetic data generation may compensate for limited Chinese corpora.

2. **Technical Aspects of MiMo’s Approach**  
   - Arcuru’s cryptic comment highlighted MiMo’s RL process, which leverages rule-based validation and high-quality synthetic datasets (e.g., 130k math/code problems) to improve reasoning. Others questioned whether RL’s role was overstated, with one user noting that smaller models might struggle with complex, lengthy inputs.

3. **Tokenization and Language Efficiency**  
   - English’s Latin script was seen as advantageous due to efficient tokenization (fewer characters, more flexible combinations). This contrasts with Chinese’s logographic system, which may require more tokens for the same semantic content.

4. **Challenges for Non-Native Speakers**  
   - Users noted that Mandarin models can produce literal or awkward translations for non-native speakers, even when technically correct. This raises concerns about cultural and contextual nuance in localized models.

5. **Multilingual vs. Localized Models**  
   - While some advocated for English-first models to maximize global reach, others emphasized the importance of multilingual support to capture diverse linguistic contexts. However, market dynamics (e.g., China’s focus on domestic models) complicate cross-regional adoption.

**Key Takeaways**  
The discussion underscores the technical and infrastructural hurdles in developing non-English LLMs, particularly Mandarin. While synthetic data and RL offer solutions, geopolitical and cultural barriers (e.g., data accessibility, localized benchmarks) remain significant. Xiaomi’s MiMo models exemplify progress in reasoning-focused AI, but broader shifts in data strategy and evaluation frameworks are needed for truly global language models.

### DeepSeek-Prover-V2

#### [Submission URL](https://github.com/deepseek-ai/DeepSeek-Prover-V2) | 378 points | by [meetpateltech](https://news.ycombinator.com/user?id=meetpateltech) | [75 comments](https://news.ycombinator.com/item?id=43847432)

In a fascinating leap in formal mathematical reasoning, DeepSeek-AI introduces DeepSeek-Prover-V2, a cutting-edge large language model tailored for formal theorem proving in Lean 4. But what makes this even more compelling is the unique way it harnesses reinforcement learning to tackle subgoal decomposition, driven by its predecessor, DeepSeek-V3.

Here's how it works: By initiating a recursive theorem proving pipeline, the system begins this journey with a cold-start training phase, wherein DeepSeek-V3 deconstructs complex problems into manageable subgoals. This results in a thought chain synthesized from proofs of these subgoals, effectively bridging informal and formal reasoning into a cohesive model.

DeepSeek-Prover-V2 steps up the game by incorporating reinforcement learning with synthetic cold-start data, allowing it to deftly handle problems unsolved by its smaller 7B counterparts. This evolution is clearly reflected in its remarkable performance, conquering 49 out of 658 challenges from the PutnamBench and achieving an impressive 88.9% pass ratio on the MiniF2F-test.

Accompanying this advancement is the introduction of ProverBench, a benchmark rich with 325 problems encapsulating real-world challenges from AIME competitions and a robust selection of textbook exemplars. This repository of problems serves as a testbed for evaluating both high-school and undergraduate-level mathematical conundrums.

Available in two powerful configurations, the 7B and the formidable 671B models, DeepSeek-Prover-V2 invites enthusiasts and scholars to explore its capabilities via Hugging Face's Transformers. For the curious and the keen, detailed datasets and proofs, including those for MiniF2F, are downloadable, making DeepSeek-Prover-V2 not just a pioneering tool but a community resource for advancing mathematical reasoning.

The discussion around DeepSeek-Prover-V2 and its approach to theorem proving involves several key themes:

1. **Technical Analysis of the Model**:  
   Users highlight the method of breaking problems into subgoals using reinforcement learning, comparing it to techniques like dynamic programming and cold-start training. Some note its distinction from prior models like **Kimina**, particularly in handling intermediate steps and error feedback. The example Lean 4 code snippet shared by *mcshcks* demonstrates hands-on testing of the model’s theorem-proving capabilities.

2. **Challenges in AI Problem-Solving**:  
   Participants discuss limitations of current models, such as maintaining context in large projects (*criley2*, *jhrmnn*) and scaling AI tools for codebases. Concerns about "hallucination" and AI-generated "bills of nonsensical steps" (*otabdeveloper4*) contrast with praise for specialized agents that improve context management (*prtymcprt*).

3. **Future Directions**:  
   Speculation arises around domain-specific expert models and wrappers (*smnwrds*, *Arcuru*), akin to Mixture-of-Experts (MoE) architectures, to delegate tasks to specialized submodels. Tools like **ProverBench** are noted as valuable for benchmarking.

4. **Humorous and Off-Topic Threads**:  
   Lighthearted tangents include sarcastic remarks about LLMs writing "XX-page documents on making a better peanut butter sandwich" (*ghtysxfr*) and links to unrelated topics like robotics (*jrvarela56*) or YouTube tutorials (*mls*).

5. **Human vs. AI Problem-Solving**:  
   Debates emerge on whether AI’s subgoal decomposition mirrors human reasoning or introduces rigidity. Some users stress the importance of human intuition in interpreting errors and maintaining project coherence, while others acknowledge AI’s growing role in technical workflows.

Overall, the conversation blends technical scrutiny of DeepSeek-Prover-V2’s innovations, reflections on AI’s evolving capabilities, and playful asides, reflecting both enthusiasm and skepticism about AI-driven formal reasoning.

### JetBrains defends removal of negative reviews for unpopular AI Assistant

#### [Submission URL](https://devclass.com/2025/04/30/jetbrains-defends-removal-of-negative-reviews-for-unpopular-ai-assistant/) | 203 points | by [przemub](https://news.ycombinator.com/user?id=przemub) | [130 comments](https://news.ycombinator.com/item?id=43850377)

JetBrains, a company known for its popular development tools, recently found itself in hot water after it removed negative reviews for its AI Assistant from its plugin marketplace. The controversial decision came to light when users noticed their critical feedback had vanished. JetBrains argued these reviews either violated their policies or addressed issues that had been solved. Despite having over 22 million downloads since its July 2023 release, the AI Assistant maintains a meager 2.3 out of 5 rating. Developers report frustrations such as automatic installation, slow performance, limited third-party support, and restricted core features.

In response to backlash, a JetBrains employee admitted that deleting multiple reviews at once may have seemed suspicious without proper communication. Users on social media speculated this move aimed to artificially improve the plugin’s ratings. Some developers echoed this sentiment, highlighting persistent issues like latency, inconsistent user experience, and scant documentation. A particularly vocal Reddit user labeled it “the annoying self-healing/reinstalling phoenix of a plug-in.”

To address competition and enhance its offerings, JetBrains introduced a free tier this month alongside a new AI agent named Junie, which has garnered more favorable reviews, although users noted its steep cost. This strategic move comes amidst pressure from free tools like Microsoft’s Visual Studio Code, as JetBrains seeks to balance its premium business model with community demands.

The Hacker News discussion surrounding JetBrains' removal of negative reviews for its AI Assistant plugin highlights widespread criticism of the company's transparency and handling of user feedback. Key points include:

1. **Critique of Review Moderation**:  
   Users argued that deleting reviews, even for "resolved" issues, erodes trust and removes valuable feedback. Many saw JetBrains’ actions as dishonest, likening it to censoring criticism. Comments emphasized that vendors typically respond to negative reviews instead of removing them, suggesting JetBrains prioritized optics over accountability.

2. **Plugin Quality Concerns**:  
   The AI Assistant’s poor performance, latency, auto-installation, and lack of third-party/local LLM support were repeatedly criticized. Users cited specific frustrations, such as restricted features, inconsistent UX, and sparse documentation, questioning the plugin’s premium pricing.

3. **Debate on Platform Policies**:  
   Some defended JetBrains’ right to moderate reviews under their policies but condemned the lack of communication when doing so. Others argued that outdated reviews should remain unless explicitly irrelevant, with calls for transparency (e.g., marking resolved issues instead of deletion).

4. **Broader Distrust in JetBrains’ Approach**:  
   Commenters linked the incident to broader concerns about the company’s profit-driven decisions, like bundling paid plugins and prioritizing proprietary tools over community needs. Comparisons to free alternatives (e.g., VS Code) underscored dissatisfaction with JetBrains’ value proposition.

5. **Side Discourse on HN Moderation**:  
   A sub-thread debated Hacker News’ own moderation policies after a detailed negative review was initially removed. Users expressed frustration with opaque rules and perceived censorship, highlighting tensions between free speech and platform governance.

**Sentiment Summary**:  
The discussion reflects deep skepticism toward JetBrains’ handling of criticism and product quality. Users perceive a pattern of prioritizing commercial interests over user experience, with the review deletions seen as emblematic of a lack of transparency. The incident has amplified existing frustrations with the AI Assistant’s shortcomings and JetBrains’ broader strategic choices.

### OCaml's Wings for Machine Learning

#### [Submission URL](https://github.com/raven-ml/raven) | 105 points | by [musha68k](https://news.ycombinator.com/user?id=musha68k) | [66 comments](https://news.ycombinator.com/item?id=43844279)

In an exciting development for the OCaml community, "Raven" is aiming to revolutionize machine learning and data science by bringing a Python-like ease and efficiency to the OCaml ecosystem. With a comprehensive suite of libraries and tools, Raven is designed to incorporate machine learning capabilities seamlessly into OCaml, leveraging its robust type safety and performance. Though Raven is still in the pre-alpha stage, its core components like Ndarray and Hugin are nearly feature-complete and open to user feedback.

Raven is meticulously structured with sub-projects like Ndarray for high-performance numerical computing, Ndarray-CV for computer vision tasks, and Hugin for creating stunning visualizations. An interesting addition is Quill, a notebook-style platform that encourages data exploration and sharing, aiming to rival Jupyter notebooks.

For those accustomed to Python, Raven’s documentation includes a comparison of Raven’s offerings against Python’s renowned libraries, illustrating how Raven might become the OCaml equivalent to familiar Python tools like NumPy and Matplotlib.

While some elements like deep learning frameworks are still in development, the Raven team welcomes contributions and feedback from developers and data scientists alike, inviting them to help shape the future of machine learning in OCaml. Operating under the ISC License, Raven remains accessible for both personal and commercial use, encouraging wide adoption and collaboration.

The Hacker News discussion on the Raven project and OCaml’s ecosystem revolves around several key themes:

### 1. **Notebooks vs. Traditional Development**
   - Some users debate the practicality of Jupyter-style notebooks (like Raven’s Quill) for interactive development. Proponents highlight their value for teaching, rapid prototyping, and data exploration. Critics argue notebooks encourage messy, stateful code and prefer REPLs or IDEs for structured workflows. One user suggests notebooks should serve as "quick-and-dirty" frontends, with code later refactored into modules.

### 2. **Comparison with Python and Other Languages**
   - While Raven aims to bring Python-like usability to OCaml, participants note Python’s dominance in ML is partly due to its accessible syntax and vast ecosystem. Some praise OCaml’s type safety and performance but acknowledge its steep learning curve and smaller community compared to Haskell or Elixir. A few suggest F# as a "middle ground" for ML workflows due to its .NET integration and JIT compilation.

### 3. **OCaml’s Challenges**
   - **Syntax**: OCaml’s syntax is polarizing. Critics call it archaic ("looks like shit"), while defenders argue it’s elegant once mastered. Some compare it unfavorably to Rust or Python’s readability.
   - **Multicore Support**: OCaml’s delayed multicore implementation is seen as a historical weakness, though recent progress is noted. Users contrast it with Erlang/Elixir’s concurrency model and Haskell’s parallelism.
   - **Community Growth**: OCaml’s niche status is attributed to limited marketing, unfamiliarity with its module system, and competition from trendier languages (e.g., Rust, Scala). Some hope tools like Dune (build system) will improve adoption.

### 4. **Raven vs. Owl**
   - Users question how Raven differentiates itself from **[Owl](https://github.com/owlbarn/owl)**, an existing OCaml ML library. Links suggest Owl is being restructured, and Raven could emerge as a successor, focusing on developer experience. The discussion lacks clarity on this point, reflecting Raven’s early stage.

### 5. **Technical Strengths of OCaml**
   - OCaml’s module system, type inference, and performance are praised. Fans argue it’s underrated for systems programming and research, but admit it struggles in commercial settings compared to Python or Java.

### 6. **Miscellaneous**
   - A brief tangent critiques OCaml’s "American" origins and lack of corporate backing, though Python (Dutch creator) is cited as counter-evidence to nationality affecting success.

### Key Takeaway
The thread reflects cautious optimism about Raven’s potential to modernize OCaml’s ML ecosystem but underscores challenges in overcoming Python’s dominance and OCaml’s historical quirks. The community’s focus areas include improving tooling, documentation, and addressing syntax barriers to attract broader adoption.

### "AI-first" is the new Return To Office

#### [Submission URL](https://www.anildash.com//2025/04/19/ai-first-is-the-new-return-to-office/) | 323 points | by [LorenDB](https://news.ycombinator.com/user?id=LorenDB) | [201 comments](https://news.ycombinator.com/item?id=43845089)

Tech CEOs are jumping on the latest bandwagon: declaring their companies to be "AI-first." This trend, spearheaded by Shopify's Tobi Lütke, involves demanding that AI usage be incorporated into performance reviews—a move that feels forced and unnecessary. In a humorous self-analysis, the author notes that while AI tools can be helpful for those less skilled in certain areas, they may not add value for those already excelling at their jobs. So why this push from the tech elite?

The phenomenon is likened to a form of groupthink, where tech leaders echo each other's phrases to show alignment with the latest industry trends. This has been seen before in the insistence on returning to the office and other performative behaviors. CEOs seem to be more interested in maintaining appearances within their exclusive circles than genuinely considering what might benefit their employees or the business.

A practical approach would be to offer evaluated AI tools as optional resources rather than mandatory requisites, allowing employees to choose what genuinely enhances their productivity. However, such a sensible strategy wouldn't fit in with the attention-seeking behaviors of tech tycoons who prefer to impose AI mandates as a way to signal their participation in the latest tech craze.

In conclusion, perhaps it's time to adopt a more balanced attitude towards technology—one that welcomes innovation without succumbing to fad-driven group pressures. Recognizing the value in diverse sources of innovation, including academia and open-source communities, could foster a more constructive and inclusive tech culture.

**Summary of Discussion:**

The discussion revolves around critiques of Shopify CEO Tobi Lütke’s management style, dubbed the "Tobi Tornado," characterized by abrupt, disruptive changes and high-pressure demands. Commenters compare this to "Seagull Management," where leaders swoop in, impose decisions, and leave chaos in their wake. While some defend Tobi’s approach as effective for Shopify’s growth, others argue it fosters instability and employee burnout, likening it to Elon Musk’s controversial leadership at Tesla and SpaceX. Critics highlight the human cost of such methods, noting stress and disrespect for work-life balance.

Debates also address Shopify’s business practices, with users raising concerns about data privacy and legal compliance in Europe, accusing the company of "shady" tactics in handling customer information. Skepticism emerges about the sustainability of performative tech trends, such as mandatory AI integration, with calls for optional tools instead of top-down mandates. Broader themes include critiques of Silicon Valley’s obsession with hyper-productivity, the ethics of leadership styles prioritizing innovation over employee well-being, and the tension between founder-driven vision and organizational stability. The conversation underscores a desire for balanced, inclusive approaches to technology and management that value diverse perspectives beyond CEO-driven hype.

### Satya Nadella says as much as 30% of Microsoft code is written by software

#### [Submission URL](https://www.cnbc.com/2025/04/29/satya-nadella-says-as-much-as-30percent-of-microsoft-code-is-written-by-ai.html) | 48 points | by [shinryudbz](https://news.ycombinator.com/user?id=shinryudbz) | [52 comments](https://news.ycombinator.com/item?id=43841868)

At the recent LlamaCon AI developer event, held by Meta in Menlo Park, California, Microsoft CEO Satya Nadella announced a groundbreaking shift in software development: up to 30% of the code within Microsoft’s repositories is now penned by artificial intelligence. Speaking alongside Meta's CEO Mark Zuckerberg, Nadella highlighted the rapid integration of AI into coding processes, noting a continual increase in AI-written code at the tech giant.

In a striking revelation, Zuckerberg shared that Meta aims to create an AI model capable of developing up to half of its new AI models within the next year. This highlights a broader trend of AI's growing role in software development across the tech industry. While exact figures for Meta's AI-generated code remain unspecified, the ambition mirrors a substantial shift in how leading tech companies like Google and Duolingo are leveraging AI to enhance efficiency and innovation.

This movement echoes the transformative potential of AI beyond tech, with AI-generated content and solutions being increasingly preferred over human efforts in various fields such as customer service and creative sectors. As companies like Shopify and Duolingo pivot towards AI-first thinking, there is a clear indication that automated code generation may become a new industry standard. As AI continues to gain ground, the tech landscape is poised for significant evolutions, challenging companies to adapt swiftly to a future where machines could outpace human coders in both volume and capability.

The Hacker News discussion surrounding Microsoft's claim that 30% of its code is AI-generated reveals widespread skepticism and nuanced critiques. Key themes include:

1. **Skepticism of Metrics**:  
   Many users question the validity of the "30%" figure, suggesting it could be marketing hype. Some argue the metric (e.g., lines of code vs. critical logic) is misleading, as AI may handle boilerplate, tests, or IDE-generated code rather than meaningful engineering work. Comparisons are drawn to legacy tools like IntelliSense or project templates (Rails/Django), which have automated code for years.

2. **Code Quality Concerns**:  
   Commenters express doubt that AI-generated code improves quality, citing risks of bugs, licensing issues, or "code bloat." One user notes that AI often produces verbose, repetitive patterns (e.g., `actualLogicFoo` boilerplate) requiring human cleanup. Others joke that Microsoft’s software quality (e.g., Windows 11 glitches) might *decline* if AI contributes significantly.

3. **Impact on Developers**:  
   While some fear AI could displace junior roles by automating routine tasks, others argue it merely accelerates grunt work (e.g., writing tests, API contracts). A recurring point: AI may shift developer focus to *prompt engineering* and debugging rather than eliminating jobs outright.

4. **Historical Parallels**:  
   Users compare the trend to past hype cycles, like Clippy’s resurgence as "Copilot," and note that auto-generated code (e.g., via IDE wizards) isn’t novel. Some mock Microsoft’s branding efforts ("Windows AI") as repackaged tools.

5. **Off-Topic Humor**:  
   A few comments derail into jokes about Windows’ performance, Mordor-like corporate landscapes, and absurd interview anecdotes, reflecting broader cynicism toward tech giants’ narratives.

Overall, the discussion underscores a divide: while AI code generation is seen as a productivity tool for mundane tasks, its portrayal as a revolutionary shift is met with doubt, with many emphasizing that human oversight remains critical.

### Wikipedia says it will use AI, but not to replace human volunteers

#### [Submission URL](https://wikimediafoundation.org/news/2025/04/30/our-new-ai-strategy-puts-wikipedias-humans-first/) | 79 points | by [thm](https://news.ycombinator.com/user?id=thm) | [41 comments](https://news.ycombinator.com/item?id=43846052)

In a recent announcement, the Wikimedia Foundation made it clear that replacing Wikipedia's dedicated human editors with AI is not on the agenda. Instead, their new AI strategy aims to empower these passionate volunteers. Over its 25-year history, the human touch has been vital to Wikipedia’s success, as editors have contributed tirelessly to build the world’s largest encyclopedia. Wikimedia's plan is to harness AI to both streamline technical processes and eliminate repetitive tasks, freeing up volunteers to focus on enriching Wikipedia's content.

AI will assist moderators by automating mundane tasks to ensure the integrity of information and improve content discoverability to ease editor research. It will also facilitate the translation of content, enhancing Wikipedia’s global reach, and improve onboarding for new volunteers through AI-guided mentorship. Wikimedia emphasizes that this technology will be implemented with their core values—such as privacy, transparency, and multilingual inclusivity—at the forefront. The Foundation intends to utilize open-source AI tech, ensuring their continued responsibility to provide accessible, reliable knowledge.

This AI initiative promises to be a collaboration, rather than a replacement, reinforcing Wikipedia's mission to remain a pillar of free and open knowledge for everyone. You can explore the full details of Wikimedia’s AI strategy on Meta-Wiki, crafted under the guidance of Chris Albon, Director of Machine Learning, and Leila Zia, Head of Research at the Foundation.

The Hacker News discussion on Wikimedia's AI strategy highlights cautious optimism and notable concerns among commenters. Key points include:

1. **Human-AI Collaboration**:  
   - Many support AI assisting with repetitive tasks (e.g., vandalism detection, translation) but stress **human oversight** remains critical. Concerns arise about accountability if AI makes errors in moderation or content generation.

2. **Translation Challenges**:  
   - While AI could improve Wikipedia’s multilingual reach, users note **quality disparities** between languages (e.g., detailed German articles vs. sparse Cebuano content). Some argue machine translation risks losing cultural nuance and that human input is still essential.

3. **Mentorship and Onboarding**:  
   - Skepticism exists around AI-guided mentorship replacing human interaction. Suggestions include using AI to flag problematic edits while preserving community-driven guidance for new contributors.

4. **Content Quality Risks**:  
   - Fears of “**AI slop**” and **recursive loops** if AI-generated content pollutes Wikipedia’s repository. Critics warn this could erode reliability, especially if businesses exploit AI-generated text.

5. **Open-Source and Commercialization**:  
   - Trust in Wikimedia’s commitment to open-source tools is mixed. Some speculate hidden profit motives (e.g., selling AI tools to universities), though others defend its non-profit ethos.

6. **Existing Bots vs. AI**:  
   - Users contrast pre-AI automation (limited, rule-based bots) with LLMs’ potential to either worsen inaccuracies or creatively solve issues like citation sourcing.

Overall, the community acknowledges AI’s potential to ease workloads but emphasizes vigilance to preserve Wikipedia’s human-driven integrity and avoid over-reliance on imperfect systems.

