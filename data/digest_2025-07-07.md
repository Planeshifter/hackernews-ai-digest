## AI Submissions for Mon Jul 07 2025 {{ 'date': '2025-07-07T17:13:03.149Z' }}

### Mercury: Ultra-fast language models based on diffusion

#### [Submission URL](https://arxiv.org/abs/2506.17298) | 526 points | by [PaulHoule](https://news.ycombinator.com/user?id=PaulHoule) | [217 comments](https://news.ycombinator.com/item?id=44489690)

In the cutting-edge realm of language models, Inception Labs and a team of innovative researchers present "Mercury," a revolutionary line of large language models (LLMs) engineered on diffusion principles. This marks a significant leap in computational speed and efficiency, particularly emphasized in their first iteration, Mercury Coder, crafted specifically for coding tasks.

Mercury introduces a novel approach by deploying Transformer architecture to simultaneously predict multiple tokens, demonstrating its prowess in both speed and quality. Tested rigorously, Mercury Coder Mini and Small models blaze through at staggering rates of 1109 and 737 tokens per second, respectively, using NVIDIA H100 GPUs. These stats represent up to a tenfold improvement in speed compared to existing frontline models, all while maintaining competitive performance quality.

Beyond sheer technical achievements, Mercury models have already proved their mettle on an array of coding benchmarks across various languages and applications. They're winning real-world tests too, securing a high ranking on Copilot Arena's quality charts and currently holding the title as the fastest model.

For those eager to dive into Mercury's capabilities, public access is facilitated via a newly released API, and a free playground offers hands-on exploration opportunities. This paper isn't just about numbers and metrics; it's a showcase that beckons developers and researchers alike to witness and participate in the evolving narrative of language model advancements. 

Discover more about Mercury's transformative potential, delve into their data, and perhaps join the conversation on arXiv to stay at the forefront of this technological frontier.

The discussion revolves around frustrations with slow Continuous Integration (CI) pipelines and testing bottlenecks in software development. Key points include:

1. **CI Pain Points**  
   Developers express annoyance with delays in PR validation, flaky tests, resource constraints, and inefficient caching systems. Some note feeling "collectively stuck" despite years of CI optimization efforts.

2. **Corporate vs. Small Teams**  
   Participants contrast Google’s massive parallel testing infrastructure (e.g., launching 10,000 machines) with smaller companies’ struggles to afford equivalent resources. High costs for cloud VMs and hardware are cited as limiting factors.

3. **Mercury’s Promise**  
   Mercury’s speed (e.g., 1,109 tokens/sec) sparks optimism for accelerating test execution and code generation. Users hope it could reduce CI bottlenecks, though skepticism exists around infrastructure/resource limitations.

4. **Technical Trade-offs**  
   Comments debate deterministic CI steps, caching reliability, and concurrency challenges in Java/testing. Tools like Bazel and cloud CI solutions are mentioned but critiqued for complexity/cost.

5. **Organizational Issues**  
   Some argue slow CI processes stem from *organizational* problems (e.g., prioritization, tooling choices) as much as technical ones. A Google employee lags in dependency management, highlighting internal inefficiencies.

6. **Cloud/Security Concerns**  
   Side discussions touch on cloud providers' security models (e.g., Azure’s confidential computing) and whether they truly mitigate risks like code theft.

**Overall**: The discourse underscores a tension between cutting-edge speed (Mercury) and systemic CI/CD challenges rooted in cost, complexity, and organizational inertia. While Mercury’s performance impresses, adoption hurdles remain for teams lacking Google-scale resources.

### LookingGlass: Generative Anamorphoses via Laplacian Pyramid Warping

#### [Submission URL](https://studios.disneyresearch.com/2025/06/09/lookingglass-generative-anamorphoses-via-laplacian-pyramid-warping/) | 117 points | by [jw1224](https://news.ycombinator.com/user?id=jw1224) | [23 comments](https://news.ycombinator.com/item?id=44495154)

LookingGlass is taking the world of optical illusions to a new level by merging traditional art forms with cutting-edge technology. Created by a team of researchers from DisneyResearch|Studios and ETH, this groundbreaking work introduces the concept of generative anamorphosis. Unlike traditional anamorphic images, which require specific angles or devices for interpretation, LookingGlass leverages latent rectified flow models to produce images that can also be appreciated directly from the front.

The secret sauce? A new technique called Laplacian Pyramid Warping. This approach is frequency-aware, allowing for the creation of high-quality, visually stunning illusions. The method extends the reach of anamorphic images by integrating them with advanced latent space models and spatial transformations, offering an impressive array of new generative perceptual illusions.

The research holds significant implications for both artistic and scientific communities, offering fresh ways to create engaging visual experiences. It's a fascinating intersection of art, mathematics, and technology, all aimed at expanding the scope of how we view and interpret the world through images. Keep an eye out for this innovation as it brings the enchanting world of illusions to our everyday visual experiences.

The discussion around the *LookingGlass* submission highlights both technical fascination and broader reflections on innovation:

1. **Related Work & Comparisons**: Users note similarities to existing projects like *visual anagrams* and *diffusion illusions*, referencing work by creators like Steve Mould, Daniel Geng, and others. Techniques such as pixel swapping, reflection-based puzzles, and multi-layer image transformations are seen as precursors to this research.

2. **Technical Nuances**:  
   - Compression artifacts (e.g., in low-detail areas like skies) are acknowledged as visible trade-offs.  
   - The **Laplacian Pyramid Warping** method is contextualized within historical concepts like *anamorphic encryption*, with some pointing to EUROCRYPT 2022 research and morphic techniques spanning centuries.  

3. **Artistic & Corporate Implications**:  
   - Praise for Disney Research’s involvement underscores the blend of art and tech driving progress.  
   - Skeptical remarks liken Disney's innovation to a "Silicon Valley burnout" story, reflecting tension between corporate scale and grassroots creativity. Others defend the project’s achievements despite its small-team origins.

4. **AI’s Role Debated**: A subthread questions whether such breakthroughs depend on AI, with one user cautioning against dismissing non-AI scientific contributions.  

Overall, the discussion balances admiration for *LookingGlass*’s technical ingenuity with critiques of its novelty and reflections on corporate-driven innovation. The intersection of historical methods, modern generative models, and artistic application emerges as a key theme.

### Adding a feature because ChatGPT incorrectly thinks it exists

#### [Submission URL](https://www.holovaty.com/writing/chatgpt-fake-feature/) | 1110 points | by [adrianh](https://news.ycombinator.com/user?id=adrianh) | [386 comments](https://news.ycombinator.com/item?id=44491071)

At Soundslice, the company renowned for its sheet music scanner, something unexpected yet oddly intriguing occurred. Adrian Holovaty, the man behind the operation, noticed an unusual trend surfacing within the error logs. Instead of dealing exclusively with faulty images of sheet music, they were flooded with screenshots of ChatGPT sessions. These weren't traditional music notations but ASCII tablature—guitar music’s rather rudimentary notation style. Unlike other types of uploads, these images weren't supported by their system.

So, why were these ASCII tab screenshots gaining traction on their platform? The mystery unraveled when Holovaty delved into ChatGPT himself. The AI was erroneously advising users to visit Soundslice for audio playback of ASCII tabs, a feature that didn't actually exist. This miscommunication had inadvertently turned into a stream of users seeking a solution that the platform didn’t offer.

Faced with a unique challenge–a steady influx of users misled by AI—and an unconventional market demand, Soundslice had a choice. They could dismiss the misconceptions with disclaimers or innovate by meeting this unforeseen demand head-on. In a twist to the tale, they opted for the latter, developing an ASCII tab importer—a feature Holovaty humorously admitted was at the bottom of his 2025 expectation list.

This situation presents an intriguing conundrum for modern businesses: How should companies respond when misinformation about their product inadvertently creates customer demand? Should strategic decisions be influenced by incorrect external narratives? While Holovaty finds satisfaction in creating a tool beneficial to users, there’s a lingering ambivalence—was their hand forced into development by AI misinformation? A quandary that sparks broad reflection on the ethical interplay between AI, misinformation, and product development.

The discussion revolves around AI's impact on technology, society, and ethics, sparked by the Soundslice case where users followed ChatGPT’s misleading advice, leading the company to adapt by adding unsupported features. Key themes include:  
- **AI’s Role in Development**: Users noted GPT-4’s ability to guess API code, but emphasized that opaque APIs and AI’s unpredictability risk confusion.  
- **Content Quality Concerns**: Tools like Grammarly, while helpful, were criticized for stripping human nuance (e.g., passive voice fixes harming stylistic intent). AI-generated text’s reliability was debated—praised for SEO efficiency but derided for “hallucinations” and threats to authenticity.  
- **Job Displacement Fears**: Many worried AI could rapidly replace jobs, disproportionately affecting workers without political safeguards (e.g., universal basic income). Historical parallels to the Luddite movement and industrial revolution underscored resistance to disruptive tech.  
- **Corporate Responsibility**: Critics blamed “greedy business managers” for prioritizing short-term cost-cutting via AI over long-term societal health, risking job markets and quality outputs.  
- **Ethical Regulation**: Calls for frameworks to ensure AI benefits society, not just corporations. Some argued societal structures, not tech itself, are the root issue (e.g., unnecessary jobs vs. equitable redistribution).  
- **Irony and Paradox**: The Soundslice incident exemplified unintended demand creation via AI errors. Others humorously compared worshipping AI to deity-like reliance, highlighting unease with unchecked power.  

Overall, the comments reflect cautious pessimism about AI’s rapid integration without ethical guardrails, stressing the need for human oversight, adaptive policies, and proactive regulation to mitigate disruption.

### AI cameras change driver behavior at intersections

#### [Submission URL](https://spectrum.ieee.org/ai-intersection-monitoring) | 51 points | by [sohkamyung](https://news.ycombinator.com/user?id=sohkamyung) | [107 comments](https://news.ycombinator.com/item?id=44489552)

In an effort to make roads safer and reduce traffic fatalities, U.S. cities are adopting Vision Zero, a strategy originally from Sweden that aims to eliminate road deaths by employing AI-driven camera systems. These systems, powered by companies like Stop for Kids and Obvio.ai, are being deployed at intersections to catch drivers who ignore stop signs and engage in risky behavior. Intersections are a critical focus since they are the site of about half of all car accidents, often resulting in severe outcomes.

One poignant story fueling this technological push is that of Kamran Barelli, CEO of Stop for Kids. Barelli founded the company after his wife and young son were nearly killed by an inattentive driver. Dissatisfied with traditional speed signs and intermittent police presence, Barelli and his team designed a more sophisticated solution. Their AI cameras, capable of operating around the clock and in all lighting conditions, automatically issue citations for violations while respecting driver privacy by not recording faces.

The system has shown promising results in pilot programs, such as in Saddle Rock, N.Y., where compliance with stop signs soared from 3% to 84% following implementation. These AI cameras not only encourage safer driving but also offer incentives like potential reductions in insurance rates, making them both a carrot and a stick for promoting road safety. As these technologies gain traction, they offer a glimpse into a future where AI plays a crucial role in transforming driver behavior and contributing to public safety.

**Summary of Discussion:**

The discussion revolves around the efficacy of **stop signs vs. rolling stops**, enforcement challenges, and comparisons of traffic safety infrastructure across regions (e.g., U.S., EU). Key points include:

1. **Stop Sign Compliance vs. Rolling Stops**:  
   - Critics argue many drivers perform "rolling stops" (slowing but not stopping completely), risking pedestrian safety, especially at intersections with poor visibility.  
   - Some defend rolling stops in low-traffic scenarios (e.g., empty intersections), claiming full stops waste time and energy. Others counter that incremental injuries from non-compliance add up over time.  

2. **Technology & Enforcement**:  
   - AI-driven cameras and computer vision are supported for 24/7 enforcement, particularly near schools/residential areas. Critics caution against over-reliance on tech without addressing systemic issues like road design.  
   - Mixed opinions exist on automated citations: supporters emphasize fairness and deterrence, while skeptics highlight privacy concerns and potential misuse.  

3. **Infrastructure Comparisons**:  
   - European designs (e.g., roundabouts, priority rules in Netherlands/Germany) are praised for reducing conflicts, contrasting with U.S. reliance on 4-way stops.  
   - Debate on signage clarity: U.S. "yield" conventions differ from EU road markings, impacting driver predictability.  

4. **Pedestrian Safety**:  
   - Poorly marked crosswalks, driver distraction, and lax enforcement create risks. Suggestions include better lighting, redesigned intersections, and stricter penalties for ignoring stop signs.  

5. **Statistics & Cultural Factors**:  
   - Higher U.S. traffic fatalities (vs. EU) are linked to urban sprawl, longer driving distances, and car-dependent lifestyles. Calls for public transit investment and reduced sprawl to mitigate risks.  
   - Cultural attitudes (e.g., prioritizing convenience over pedestrian safety) are seen as barriers to Vision Zero goals.  

**Conclusion**: The thread highlights tensions between pragmatic driving habits and stringent enforcement, advocating for balanced solutions combining AI enforcement, infrastructure redesign, and systemic shifts toward pedestrian-centric planning.

### tinymcp: Let LLMs control embedded devices via the Model Context Protocol

#### [Submission URL](https://github.com/golioth/tinymcp) | 49 points | by [hasheddan](https://news.ycombinator.com/user?id=hasheddan) | [10 comments](https://news.ycombinator.com/item?id=44491460)

Are you ready for a technological leap that combines the power of AI with the physical world? Meet tinymcp, an experimental project that allows Large Language Models (LLMs) to control embedded devices through the Model Context Protocol (MCP). The project, hosted on GitHub by Golioth, demonstrates how this can be done seamlessly with existing microcontrollers using the Golioth management API.

Tinymcp is designed to work with Golioth's LightDB State and Remote Procedure Calls (RPCs), making it possible to expose device functionalities without modifying device firmware, a boon for developers looking to integrate AI into embedded systems efficiently. The project comes packed with handy examples, like the "blinky" demonstration, which shows how to manage LED control via LLMs.

For the curious developer ready to dive in, setting up tinymcp requires connecting a device to the Golioth platform, running a local MCP server, and configuring environment variables. A wealth of resources, including documentation for setup and interaction using different tools like MCP Inspector, Claude Code, and Gemini CLI, are provided.

Remember, while the integration of AI with physical devices holds immense potential, it also demands caution due to the experimental nature of the project and the capability delegation involved. Join the cutting edge of tech by exploring tinymcp, which seeks to unlock the full potential of LLMs in the world of microcontrollers. For further insights and to get started, head to the detailed guide available on Golioth's blog.

The Hacker News discussion explores a mix of technical, philosophical, and sci-fi-inspired reactions to **tinymcp**, a project enabling LLMs to control embedded systems. Key themes include:

1. **Sci-Fi Parallels**: Users humorously reference *Hal 9000* (from *2001: A Space Odyssey*) and *Ubik* (Philip K. Dick’s novel), drawing parallels to scenarios where AI-controlled systems malfunction or refuse commands (e.g., "the doctor refused to open the door"), highlighting concerns about autonomous decision-making in physical devices.

2. **Token Limitations**: Comments touch on LLM token constraints ("Youre f tkns"), noting challenges in prompt efficiency and context handling when integrating AI with microcontrollers.

3. **Metaphors for Control**: Discussions metaphorically compare device operations to industrial processes (e.g., docking tankers, pumping oil), underscoring the complexity and potential risks of delegating control to AI models. Terms like "DAO" (Door Opens Job) hint at debates around access control and deterministic outcomes.

4. **Caution & Humor**: While some users joke about AI "freezing" or behaving unpredictably, others raise implicit concerns about reliability, security, and the ethics of embedding LLMs in physical systems. The fragmented, coded language reflects playful experimentation aligned with the project’s experimental nature.

Overall, the discussion blends curiosity about tinymcp’s innovation with wariness about its implications, anchored in cultural references and technical critiques of AI determinism in embedded contexts.

### I am uninstalling AI coding assistants from my personal computer

#### [Submission URL](https://sam.sutch.net/posts/uninstailling-ai-coding-from-personal-computer) | 75 points | by [ssutch3](https://news.ycombinator.com/user?id=ssutch3) | [39 comments](https://news.ycombinator.com/item?id=44493503)

In a heartfelt post, Samuel Sutch opens up about his decision to uninstall AI coding assistants from his personal workflow, marking a significant shift in his approach to coding. After spending months using AI tools like Claude Code to rapidly develop features for his startup, Roam, Samuel found himself in a coding frenzy reminiscent of a high-speed race fueled by these digital assistants. While the initial thrill was addictive and allowed for impressive productivity, he soon discovered a sense of emptiness that followed—an artistic void rooted in the lack of personal involvement in the coding process.

Samuel expresses concerns about how this AI-enhanced workflow impacted his psychology and creativity. As someone who views coding as a personal art form and a core expression of his values, he found himself missing the hands-on, deeply engaging process that coding once represented for him. The realization that he hadn't written a single line of code himself in weeks alarmed him, prompting a reconsideration of his methods.

In his essay, Samuel also acknowledges the broader industry pressure to adopt AI tools for increased efficiency but draws a personal line for his projects. Emphasizing the intrinsic satisfaction of creating without intermediaries, he decides to return to a more traditional approach for his personal endeavors. While recognizing the inevitability of AI in professional settings, he is committed to maintaining a hands-on relationship with code in his own ventures, recapturing the connection between mind and machine.

It's a thought-provoking reminder of the balance between innovation and authenticity in the tech world, leaving readers to ponder the true essence of creativity amid advancing technology. Samuel invites feedback and interaction from the community, indicating his openness to discussions around this evolving dynamic.

**Summary of Discussion:**  
The discussion reflects polarized views on AI coding tools, with technical, creative, and workplace implications debated.

### Key Themes:  
1. **Creativity vs. Productivity**:  
   - Many relate to Samuel’s artistic dissatisfaction. NathanKP compares AI-assisted coding to gardening or raising children—structured but lacking deeper fulfillment. Others find AI stifles craftsmanship, likening it to outsourcing creativity.  
   - Critics argue AI disrupts the personal connection to code, while proponents highlight efficiency gains (e.g., automating repetitive tasks).  

2. **Technical Challenges**:  
   - Users like **blfrbrnd** describe inconsistent results with tools like Claude or Gemini, struggling to maintain code quality and control.  
   - Debates arise about AI’s suitability for niche tasks (e.g., React development, GLSL shaders) versus mainstream coding.  

3. **Workplace Pressures**:  
   - Employers increasingly mandate AI adoption for productivity metrics, creating tension with developers who prioritize hands-on work.  
   - Concerns about AI replacing junior roles (*"disposable interns"*) or enabling management to devalue skilled labor emerge.  

4. **Debate Over Execution**:  
   - While tools like Aider or Claude streamline workflows for some, others criticize hallucinations, context limitations, and hidden costs (e.g., API expenses).  
   - Skepticism persists around measuring ROI (*"AI metrics feel performative"*) and ethical concerns (e.g., code security, reliance on training data).  

### Notable Perspectives:  
- **NathanKP**: Advocates for explicit prompt engineering but acknowledges AI’s "toddler phase" limitations.  
- **clown_strike**: Blasts AI metrics as gaslighting, fearing erosion of professional standards and job security.  
- **geoka9**: Shares mixed success with AI tools, praising efficiency but lamenting dwindling control.  
- **20after4**: Questions whether AI aids or hampers experienced programmers, suggesting a "cop-out" for subpar outcomes.

### Conclusion**:  
The thread underscores a tension between embracing AI’s potential and preserving the craft of coding. While some herald efficiency gains, others warn of creative stagnation and workplace commodification. Samuel’s essay resonated as a catalyst for broader reflection on balancing innovation with authenticity.

