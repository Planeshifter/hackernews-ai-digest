## AI Submissions for Tue Dec 12 2023 {{ 'date': '2023-12-12T17:11:21.291Z' }}

### 'Biocomputer' combines lab-grown brain tissue with electronic hardware

#### [Submission URL](https://www.nature.com/articles/d41586-023-03975-7) | 67 points | by [pseudolus](https://news.ycombinator.com/user?id=pseudolus) | [51 comments](https://news.ycombinator.com/item?id=38611422)

Researchers have developed a hybrid biocomputer by combining laboratory-grown human brain tissue with electronic circuits. The system, called Brainoware, uses brain organoids made from stem cells that can differentiate into neurons. The researchers placed the organoids onto a plate containing thousands of electrodes to connect them to electric circuits. The organoids were then trained to perform tasks such as voice recognition, achieving an accuracy of 78%. The technology could potentially be integrated into artificial intelligence systems and used for modeling the brain in neuroscience research. It could also be utilized to study neurological disorders and test treatments. However, challenges remain in keeping the organoids alive and in scaling up the complexity of tasks they can perform.

The discussion on this submission covers various aspects of the topic at hand, with some users providing additional information and perspectives. 
One user expressed concerns about the ethical and philosophical implications of creating a biocomputer using lab-grown human brain tissue. They argued that there are potential risks and discomfort associated with creating conscious entities. Another user disagreed with this perspective, stating that neurons do not have the capacity to feel pain and therefore would not suffer.
There was also a discussion about whether AI systems can truly experience consciousness. Some users argued that consciousness is dependent on complex systems and sensory components, while others pointed out that AI algorithms do not have motivations or subjective experiences.
The topic of animal suffering and the moral implications of AI intelligence and consciousness were brought up. Some users discussed the significance of animal suffering and the brutality of nature, while others argued that anthropomorphizing AI carries risks.
Other users shared relevant information and links. One user provided a link to a paper on self-assembled genetic algorithms, while another shared a video about growing rat and human neurons connected to a computer.
Overall, the discussion revolved around the ethical implications, the nature of consciousness, and the potential applications of the hybrid biocomputer technology.

### Show HN: Open-source macOS AI copilot using vision and voice

#### [Submission URL](https://github.com/elfvingralf/macOSpilot-ai-assistant) | 419 points | by [ralfelfving](https://news.ycombinator.com/user?id=ralfelfving) | [153 comments](https://news.ycombinator.com/item?id=38611700)

Introducing macOSpilot: your personal macOS AI assistant. This open-source project uses voice and vision-powered AI to answer questions about any application on your Mac. With just a keyboard shortcut, you can trigger the assistant, ask your question, and receive an answer in context and in audio within seconds.
Here's how it works: macOSpilot takes a screenshot of your active window and sends it, along with a transcript of your question, to OpenAI's GPT Vision. The answer is then displayed in text and converted into audio using OpenAI's TTS (text-to-speech).
The best part is that it works with any application on macOS. No need to switch between windows or tabs. Simply press the keyboard shortcut, speak your question, and macOSpilot will provide the answer in an overlay window and in audio.
To get started, you'll need to install the NodeJS project and dependencies. Once configured with your OpenAI API key, you can run macOSpilot in the background. When you need to use it, just press the keyboard shortcut, speak your question, and macOSpilot will take care of the rest.
Check out the GitHub repository for more details on installation and usage. And for a video walk-through and explanation of how it works, head over to YouTube.

The discussion on Hacker News revolves around the macOSpilot project, which is an open-source AI assistant for macOS. Some users point out that the name "macOSpilot" is inconsistent with Apple's naming conventions and suggest using "cOSXpilot" or "cOSXpilot" instead. Others discuss the potential trade-offs of using voice commands versus text input for interacting with the assistant. One user shares a similar project they wrote for Linux using keyboard shortcuts. Another user raises concerns about the cost of using the OpenAI API and recommends being mindful of the spending limits. Some users appreciate the feature of taking screenshots to assist with specific inquiries about the interface of applications. The discussion also diverges into topics such as music production software, integrating AI assistants in different workflows, and the potential privacy implications of using AI assistants. There is also appreciation for the project and gratitude for sharing it with the community.

### AI’s big rift is like a religious schism

#### [Submission URL](https://www.programmablemutter.com/p/the-singularity-is-nigh-republished) | 294 points | by [anigbrowl](https://news.ycombinator.com/user?id=anigbrowl) | [512 comments](https://news.ycombinator.com/item?id=38616888)

In an article titled "The Singularity is Nigh!" republished from The Economist, the author explores the cult-like battles between two factions within the field of artificial intelligence (AI): E-Acc (engineers of AI) and the AI Doomers. The E-Acc sect believes in progress and embraces the potential of AI, while the AI Doomers are gripped by fear and foresee negative consequences of advancing technology. This religious schism has overshadowed practical discussions on the implications of AI. Both sects were influenced by science fiction, with some optimists predicting a future in which humans become immortal and merge with AI, while others fear the potential risks and existential threats AI might pose. The article delves into the rationalist movement, which seeks to improve human reasoning and mitigate AI risks. It also explores the shifting dynamics between Silicon Valley's profit-driven model and the concerns of rationalists. The backlash against the AI Doomers is transforming into a new religion of techno-optimism led by influential figures in Silicon Valley.

The discussion on this submission covers a range of topics related to AI and its complexity. Some users discuss the concept of the Singularity and its portrayal in science fiction, while others mention the impact of AI on different industries such as archaeology. There is also a debate about the simplification of complex systems and the potential limitations of human comprehension. Additionally, there is a discussion about the role of programming and the challenges of cognitive dissonance in the field of AI. One user brings up the idea of a "Programmer-t-Arms," implying a deep connection between software technology and human cognition. Overall, the discussion touches on various aspects of AI, from its potential implications to its relationship with other fields and its impact on society.

### Phi-2: The surprising power of small language models

#### [Submission URL](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/) | 242 points | by [birriel](https://news.ycombinator.com/user?id=birriel) | [93 comments](https://news.ycombinator.com/item?id=38614361)

Microsoft Research has released Phi-2, a 2.7 billion-parameter language model that demonstrates exceptional reasoning and language understanding capabilities. The model outperforms larger models on complex benchmarks and is available in the Azure AI Studio model catalog for researchers. Phi-2 achieved its superior performance by focusing on the quality of the training data, using "textbook-quality" data and carefully selected web data. By transferring knowledge from the previous model, Phi-1.5, to Phi-2, the team was able to accelerate training convergence and improve benchmark scores. Despite not undergoing reinforcement learning from human feedback, Phi-2 exhibited better behavior in terms of toxicity and bias compared to existing models.

The discussion around the submission primarily focuses on the comparison between Phi-2 and larger language models like GPT-3. Some users highlight the difference in the number of parameters and training data between the models. It is noted that Phi-2 has significantly fewer parameters but was still able to outperform larger models in certain benchmarks. The quality of the training data is emphasized as a crucial factor in Phi-2's performance, with the team using "textbook-quality" data and carefully selected web data. There is also a discussion about the legality and copyright aspects of training models with certain datasets. Some users raise concerns about the limited capabilities of models like Phi-2 and the importance of human feedback in learning. Other topics discussed include the potential challenges in collecting training data similar to how babies learn and the cost of training models like Phi-2. Some users suggest alternative methodologies like distilling or using synthetic datasets to train smaller models. The concept of feedback loops in language models and their impact on quality is also discussed. Finally, there is a brief discussion about the limitations of the Phi models and the potential benefits of specialized models versus generalized models.

### NASA says SpaceX’s next Starship flight could test refueling tech

#### [Submission URL](https://arstechnica.com/space/2023/12/nasa-wants-to-see-gas-stations-in-space-but-so-far-its-tanks-are-empty/) | 146 points | by [_Microft](https://news.ycombinator.com/user?id=_Microft) | [194 comments](https://news.ycombinator.com/item?id=38612585)

SpaceX and NASA could be taking a step towards orbital refueling on the next test flight of Starship. NASA has expressed interest in demonstrating orbital refueling technology, which could lead to propellant depots in space for rockets heading to destinations beyond Earth. In 2020, NASA entered agreements with four companies, including SpaceX, to prove capabilities in refueling and propellant depots. These capabilities involve cryogenic fluids such as liquid hydrogen, methane, and liquid oxygen, which must be maintained at extremely low temperatures. Extending the lifetime of these super-cold propellants requires new technologies, including automated couplers, flow meters, and advanced insulation. SpaceX appears to be on track to complete the tasks outlined in its agreement, paving the way for future demonstrations involving docked Starships in Earth orbit and eventually a crew landing on the Moon.

The discussion on Hacker News about the submission revolves around various aspects of orbital refueling, the challenges involved, and the feasibility of SpaceX's plans.

One user points out the cost implications of orbital refueling and expresses surprise that NASA would invest in such experiments if Starship doesn't reach orbit. Another user responds, explaining that the assumption is faulty since NASA's 2020 agreement with SpaceX includes the transfer of cryogenic propellants of liquid oxygen, which is already present on Starship.
There is a discussion about the challenges of cryogenic fuel transfer and the need for propellant management devices to ensure successful transfer. Users discuss techniques such as maintaining pressure, pumplng, and the use of flexible couplings. There are also discussions on the concept of ullage, propellant mixing, and the safety aspects of handling flammable propellants.
Some users bring up the potential difficulties of propellant transfer in space, including the challenges of gravity, stability during acceleration, and the movement of fuel in tanks. Potential solutions such as using smaller thrusters, rotating tanks, and propellant management devices are discussed.
One user mentions the challenge of quick and efficient transfer in space and the sensitivity of rocket combustion to changes in propellant flow. Another user references their experience in Kerbal Space Program (KSP) and compares real-life attempts to the game.
The discussion also touches on Starship's development and the reliability of the Super Heavy booster for successful missions. There is debate about the ability of the heat shields to hold up during re-entry and the potential need for smaller tiles and better thermal management.
Finally, users discuss the cost and feasibility of both the booster and the orbiter, with a comparison to the Space Shuttle program and the issues it faced. There is also a mention of the lack of wings on Starship and a light-hearted comparison to Siberian Flying Squirrels.

### Role-playing with AI will be a powerful tool for writers and educators

#### [Submission URL](https://resobscura.substack.com/p/roleplaying-with-ai-will-be-powerful-tool) | 148 points | by [benbreen](https://news.ycombinator.com/user?id=benbreen) | [92 comments](https://news.ycombinator.com/item?id=38612164)

In a recent article by Benjamin Breen on resobscura.substack.com, the author explores the use of AI as a tool for writers and educators. Breen discusses the potential of generative AI to simulate historical events and experiences, allowing users to actively engage and explore alternative versions of the world. He believes that rather than replacing human authors, AI can enhance experiential learning and provide a powerful tool for understanding different time periods. Breen shares his own experience using AI historical simulations in his world history class at UCSC, where students reported a greater understanding of the time period and the ability to make choices as historical actors. While accuracy remains a challenge for AI historical simulations, Breen emphasizes the importance of discussing and reflecting on historical inaccuracies. He also highlights the value of AI in creatively imagining another world, helping to kickstart historical imagination and empathy. Breen provides an example of a simulated acid trip in 1963, showcasing how AI can contribute to the "vibes-based" elements of writing history. Overall, Breen sees the potential for AI to be a valuable tool in the hands of writers and educators, allowing for immersive and experiential learning experiences.

The discussion on Hacker News surrounding the submission about the use of AI as a tool for writers and educators covers a variety of topics. 

Some users point out the inherent challenges of relying on AI systems for generating historical simulations, citing concerns about the accuracy of information and the potential for the AI to propagate inaccuracies. They highlight the need for systems to verify information and detect and correct mistakes.
Others mention the problem of widely inaccurate school textbooks and advocate for the use of AI in generating historical questions to encourage critical thinking and judgment.
There is a discussion about the limitations of AI language models like LLMs in understanding context and providing accurate responses. Some users express their distrust in search engines and AI-generated summaries, citing instances of misleading or incorrect answers.
The debate extends to the role of AI in education and the potential benefits and drawbacks of using AI as a teaching tool. Some argue that AI could expand the scope of learning materials and provide personalized instruction, while others express concerns about AI replacing human teachers and the limitations of AI in understanding complex subjects.

Overall, the discussion highlights the potential of AI as a tool for enhancing education but also raises concerns about accuracy, bias, and the limitations of AI systems.

### MemoryCache: Augmenting local AI with browser data

#### [Submission URL](https://future.mozilla.org/blog/introducing-memorycache/) | 443 points | by [NdMAND](https://news.ycombinator.com/user?id=NdMAND) | [96 comments](https://news.ycombinator.com/item?id=38614824)

Mozilla recently introduced MemoryCache, an early exploration project that aims to augment an on-device, personal model with local files saved from the browser, offering users a more personalized and tailored experience while prioritizing privacy and agency. The project currently consists of a Firefox extension that allows users to save web pages and notes to their local machine, a shell script that monitors changes in the saved files directory, and code to enable saving web pages as PDF for easier readability. Mozilla sees MemoryCache as a sandbox for experimenting with unique aspects of the brainstorming and idea generation process. The project is still in its early stages and can be followed on GitHub or through Mozilla's website to stay updated on its progress.

The discussion on this submission covers a range of topics related to local models, privacy, and browsing history. Some users point out that loading personal data remotely may not be ideal and suggest using local models instead. Others discuss the limitations of local models and the need for high-end hardware for training them. There is also a conversation about the implications of personal AI models and the potential for decentralized and private AI networks. Some users mention the importance of search history and bookmark indexing for personalized experiences, while others suggest alternative solutions like WorldBrain and Rewindai. The discussion ends with a comment acknowledging the experimental nature of the project and the funding it requires.

### Arena Group fires CEO in wake of Sports Illustrated AI articles scandal

#### [Submission URL](https://www.theguardian.com/technology/2023/dec/12/arena-group-ceo-ross-levinsohn-fired-sports-illustrated-ai-articles) | 74 points | by [dpflan](https://news.ycombinator.com/user?id=dpflan) | [40 comments](https://news.ycombinator.com/item?id=38614585)

The CEO of Arena Group, the publisher of Sports Illustrated, has been fired in the wake of an AI-generated articles scandal. Ross Levinsohn's termination comes after it was revealed that Sports Illustrated had published articles written by fake authors with AI-generated headshots and biographies. The board of Arena Group stated that the decision was made to "improve the operational efficiency and revenue of the company." The AI scandal was exposed by a report from the science and technology news publication Futurism, which uncovered multiple fake profiles and articles. The Arena Group denied the allegations and claimed that the articles were sourced commercial content from a third-party advertising company, AdVon Commerce. The company has since ended its partnership with AdVon and removed its content from Arena websites.

The discussion on this submission revolves around various aspects of the CEO's firing and the use of AI-generated articles.
One commenter points out that the CEO is a majority shareholder of the Arena Group's parent company, and it makes sense that he left considering the scandal surrounding the AI-generated articles.
Another commenter finds it interesting that the founder of a well-known energy drink company serves as the CEO of Arena Group, indicating that they are surprised by the CEO's background.
Another commenter mentions that they have tried the 5-Hour Energy Drink, and while they find it to be effective, they also mention some negative side effects.
There is a discussion on whether the AI-generated articles were misleading or not. Some argue that the articles were technically correct, while others call them misleading and suggest that the AI executives are responsible for the scandal.
One commenter suggests that the use of AI-generated articles is a cost-cutting measure to increase operational efficiency.
There are comments expressing skepticism about the use of AI-generated articles, with some suggesting that it is unethical and fraudulent.
Another commenter mentions that AI-generated content is widely used for advertising purposes.
One commenter suggests that leadership publications should learn from this incident and be more cautious in publishing AI-generated articles.
There is a discussion about the CEO's history and previous controversies, as well as his tenure at various companies.
A commenter raises doubts about the claims made in the article and suggests that the AI connection may not be fully verified.
One commenter mentions that reliance on AI can lead to job losses and negatively impact the business.
There are comments debating whether the CEO deserved to be fired and discussing the definition of "damaging behavior."
Several comments express concern about the erosion of trust and the impact of AI on journalism.
One commenter argues that innovation requires taking risks.
Overall, the discussion touches on the implications of AI-generated articles, the ethical concerns surrounding their use, and the consequences for those involved in the scandal.

### FTC wants Microsoft's relationship with OpenAI under the microscope

#### [Submission URL](https://www.theregister.com/2023/12/11/microsoft_openai_investment_ftc/) | 245 points | by [magoghm](https://news.ycombinator.com/user?id=magoghm) | [53 comments](https://news.ycombinator.com/item?id=38607540)

The US Federal Trade Commission (FTC) is reportedly considering investigating Microsoft's investment in OpenAI to determine if any antitrust rules have been broken. Despite investing over $10 billion in OpenAI, Microsoft does not have control of the company, as it remains a non-profit organization. Microsoft's Chief Communications Officer emphasized that the company does not own any portion of OpenAI but is entitled to a share of profit distributions. The FTC's potential investigation follows the recent departure and return of OpenAI's boss, which led Microsoft to appoint a non-voting observer to OpenAI's board. Additionally, the UK's Competition and Markets Authority (CMA) has launched a consultation to explore Microsoft's relationship with ChatGPT developer and determine if it could skew competition. The CMA will initiate an official inspection if necessary. The FTC has already been investigating OpenAI's ChatGPT over privacy and reputational concerns and has previously attempted to undo Microsoft's $69 billion merger deal with Activision Blizzard.

The discussion on Hacker News revolves around several key points:

1. Ownership Structure: There is discussion about the ownership structure of OpenAI and Microsoft's investment. Some users point out that Microsoft is a minority owner and does not have control over OpenAI, while others argue that the financial arrangements between the two companies imply some level of ownership.
2. Antitrust Concerns: The potential investigation by the US Federal Trade Commission (FTC) is seen as a response to the perceived antitrust implications of Microsoft's investment in OpenAI. Users speculate on the motives behind the FTC investigation and discuss the impact it may have on both companies.
3. Comparison to Salesforce: The discussion brings up Salesforce's similar offer to hire employees from Microsoft, indicating that such actions are not uncommon in the tech industry. However, it is noted that Salesforce did not face the same scrutiny as Microsoft in their case.
4. OpenAI's Collaboration and Competition: Questions are raised about the relationship between OpenAI and Microsoft, wondering if OpenAI can exist independently from Microsoft and collaborate with other tech companies in a meaningful way. The practical differences between the privacy and user experience approaches of OpenAI and Microsoft are also discussed.
5. Microsoft's Track Record: Some users bring up Microsoft's track record in terms of antitrust issues, mentioning past cases and suggesting that the FTC may be scrutinizing the company due to its history.
6. FTC's Role and Progress: The role of the FTC in safeguarding competition and protecting consumer interests is debated. Some users express skepticism about the FTC's effectiveness in addressing anticompetitive practices, while others see the potential investigation as a sign of progress under the leadership of Lina Khan.

Overall, the discussion touches on topics such as market competition, ownership structures, regulatory oversight, and the potential impact of the FTC investigation on Microsoft and OpenAI.

### AI made from living human brain cells performs speech recognition

#### [Submission URL](https://www.newscientist.com/article/2407768-ai-made-from-living-human-brain-cells-performs-speech-recognition/) | 18 points | by [moneil971](https://news.ycombinator.com/user?id=moneil971) | [5 comments](https://news.ycombinator.com/item?id=38615584)

Scientists at Indiana University Bloomington have successfully used brain organoids to perform basic speech recognition tasks. Brain organoids are clusters of nerve cells grown from stem cells, which mimic the structure and function of the human brain on a smaller scale. In this study, the organoids were trained to recognize the voice of a specific individual from a set of audio clips, achieving an accuracy of 70 to 80% after two days of training. This research is part of a broader effort to explore the potential of biocomputing using living nerve cells, which could offer advantages such as reduced energy consumption and improved information processing compared to conventional silicon chips. However, there are still significant challenges to overcome, including the limited lifespan of organoids and the need for further improvements in performance.

The discussion on this submission begins with user "thrn" expressing skepticism and pushing back against the idea of using brain organoids for speech recognition tasks. They suggest that progress in this field is slow and doubts the practicality of the research.

User "jrschrdr" agrees with "thrn" and emphasizes the need for caution, mentioning the potential ethical considerations. Another user, "Grimblewald," responds to this comment, dismissing the concern and suggesting that newborns in schools don't feel pain.

User "mdkkrs" chimes in, expressing excitement and inquiring about the possibility of using quantum computing and energy conversion systems for extended manipulation and vision sensors.

Overall, the discussion involves skepticism about the practicality and speed of progress in brain organoid research, concerns about ethical implications, and questions about the potential for advanced computing systems.

