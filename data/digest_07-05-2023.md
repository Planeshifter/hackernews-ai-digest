## AI Submissions for Wed Jul 05 2023 {{ 'date': '2023-07-05T17:10:12.974Z' }}

### How the BPE tokenization algorithm used by large language models works

#### [Submission URL](https://sidsite.com/posts/bpe/) | 15 points | by [montebicyclelo](https://news.ycombinator.com/user?id=montebicyclelo) | [9 comments](https://news.ycombinator.com/item?id=36596302)

Byte pair encoding (BPE) is a tokenization algorithm used by large language models like GPT-3, LLaMA, and RoBERTa. It involves breaking down text into smaller, meaningful units called tokens. To better understand how BPE works, let's go through a step-by-step example and explore a couple of Python implementations.

In this example, we start with the text "aa abc abc." The goal is to tokenize the text using BPE. Here's how the algorithm works:

1. Start with an initial set of tokens, usually single characters: [" ", "a", "b", "c"].
2. Tokenize the text using the initial set of tokens. For example, "aa abc abc" becomes ["a", "a", " ", "a", "b", "c", " ", "a", "b", "c"].
3. Count how many times each pair of tokens appears. Skip pairs where one token is a space (to avoid counting across words).
4. Select the pair of tokens that appeared the most and combine them to create a new token. In our example, the pair ("a", "b") appeared twice, so we merge them to create the token "ab".
5. Repeat this process a number of times. In this example, we iterate four times.
6. Stop if there are no more pairs that appear more than once.

Based on this algorithm, the tokens generated for our example text are: [" ", "a", "b", "c", "ab", "abc", "aa"]. The original text is tokenized as ["aa", " ", "abc", " ", "abc"].

Now, let's take a look at two different Python implementations of the BPE algorithm.

1. The first implementation is based on Sennrich et al.'s algorithm, which splits and counts the words before using spaces to distinguish tokens. It's adapted to match the example above. This implementation iterates through the text and modifies it as it goes. The result is the same as the previous example.

2. The second implementation is a streaming version that uses a trie data structure. This implementation looks over the text without modifying it. It tests whether the next character in the trie exists or not, allowing it to find the longest possible token without splitting it. The result is also the same as the previous example.

In both implementations, the tokens are assigned numbers for further processing. For example, the tokens [" ", "a", "b", "c", "ab", "abc", "aa"] would be assigned numbers [0, 1, 2, 3, 4, 5, 6].

BPE is a commonly used tokenization algorithm, but there are alternative approaches like WordPiece. Additionally, there is ongoing research on tokenizer-free approaches for transformer models.

Overall, BPE is a powerful tool for tokenizing text and plays a crucial role in various language models.

The discussion on Hacker News primarily revolves around the tokenization process of East Asian languages using Byte Pair Encoding (BPE). One user raises a question about how tokenization algorithms handle Chinese and other East Asian languages, as these languages have characters that could potentially result in a larger vocabulary size. Another user responds by pointing out that while BPE tokenization splits characters into multiple tokens, GPT models do not necessarily understand the characters, but rather process them as bytes. The discussion then delves into the technical details of tokenization and the challenges of handling East Asian languages in language models.

In response to the initial question, another user explains that BPE-based tokenization works reasonably well with East Asian languages, but there could be issues related to the consistency of logical encoding and Unicode characters. They also mention that GPT models typically process tokens as bytes, which could pose problems in understanding and representing Chinese characters. The discussion continues with further clarification about BPE, tokenization, and the challenges of handling East Asian languages in language models.

Towards the end, an expert in East Asian languages mentions that tokenizers for GPT models in these languages are usually byte-based and do not merge single-byte characters. Another user adds that GPT models using BPE can understand space-separated words and subwords, but may have difficulties with nearly all the characters in the CJK alphabet. Finally, the original user expresses their understanding and appreciation for the help provided in the discussion.

Overall, the discussion revolves around the tokenization of East Asian languages using BPE in language models like GPT and addresses the challenges and limitations associated with this process.

### Cicada: Private on-chain voting using time-lock puzzles

#### [Submission URL](https://a16zcrypto.com/posts/article/building-cicada-private-on-chain-voting-using-time-lock-puzzles/) | 38 points | by [subsequent](https://news.ycombinator.com/user?id=subsequent) | [27 comments](https://news.ycombinator.com/item?id=36607081)

A new open-source Solidity library called Cicada has been released, aimed at providing privacy for on-chain voting on platforms such as Ethereum. Cicada utilizes time-lock puzzles and zero-knowledge proofs to ensure private voting, addressing the drawbacks of current on-chain voting protocols that lack privacy. By securing the secrecy of individual ballots, vote tallies, and voter identities, Cicada aims to prevent manipulation and encourage democratic outcomes in decentralized organizations. Developers are encouraged to explore Cicada's GitHub repository and consider its potential for different voting schemes and features. The library offers running tally privacy, which can be combined with zero-knowledge group membership proofs to achieve voter anonymity and ballot privacy.

The discussion on the submission about the new open-source Solidity library, Cicada, revolves around various aspects of its privacy features and the challenges associated with on-chain voting.

One user points out that the privacy aspect of on-chain voting is an interesting concept, but they find it difficult to fully understand how it would work in practice. Another user adds that while the library offers privacy, the final tally of the votes can still be deduced, albeit reasonably small.

The topic of time-lock puzzles is brought up by one user, who mentions that they are a fancy way of saying proof of work. Another user argues that decryptable, verifiable daily functions are essential and hardware capabilities that can perform proof of work are needed.

Another user comments that the library seems to be using "magic beans" in its implementation. This leads to a discussion about the technical aspects of the library, with some users highlighting potential weaknesses and complexities in the design.

One user shares a link to their previous work on cryptographically provable voting systems and mentions that they have faced hostility and discouragement from cryptography professors. This sparks a conversation about the challenges of implementing internet-based voting systems and the importance of considering security and integrity.

Another user suggests looking into existing schemes such as ThreeBallot for secure end-to-end data bullet schemes. The conversation then shifts to the difficulties of conducting voting remotely and the potential vulnerabilities and threats to privacy and verification.

The discussion also touches on the notion of blockchain technology and its reputation for providing security and trust. Some users express skepticism about implementing transparent, blockchain-backed voting systems, as they believe they could still be subject to conspiracy and fraud.

One user shares a relevant XKCD comic that highlights the issue of misinformation and the challenges of reinventing the entire voting system. The conversation further delves into the feasibility and trustworthiness of different voting systems, with references to specific regions and their respective experiences.

A user comments on the misconception that non-technical people believe perfect security is impossible in software. Another user shares their belief that a cryptographically secure voting system is possible. The topic of hacked voting machines is also brought up, raising concerns and alarming examples of vulnerabilities in the current system.

Overall, the discussion highlights the complexities and challenges associated with implementing secure and private voting systems, while also questioning the effectiveness and trustworthiness of existing solutions.

### The many ways that digital minds can know â€“ A better way to think about LLMs

#### [Submission URL](https://moultano.wordpress.com/2023/06/28/the-many-ways-that-digital-minds-can-know/) | 112 points | by [moultano](https://news.ycombinator.com/user?id=moultano) | [14 comments](https://news.ycombinator.com/item?id=36603573)

LLMs, or large language models, have been the subject of intense debate among proponents and critics. However, the author of this post argues that LLMs actually fulfill the claims of both sides simultaneously. They possess both the ability to generate AGI-like output and to learn the complex functions of multivariable calculus. The author introduces new terminology to discuss these phenomena: search index size and memorization. They draw an analogy between search engine index size and LLMs' ability to "memorize" information. A larger search index allows search engines to provide more accurate results to specific queries, even for obscure topics. Similarly, LLMs can generate relevant responses by inferring meaning from queries, even if the exact wording is not present in the training data. The post explores this analogy further, highlighting the importance of both memorization and generalization in LLMs' capabilities.

The discussion surrounding the submission is varied. One user discusses the similarities between large language models (LLMs) and the concept of Relational Frame Theory (RFT), highlighting how both involve identifying and manipulating relationships and contextual understanding. Another user disagrees, stating that LLMs do not truly understand text but instead analyze and reproduce statistical patterns. The conversation shifts to the understanding and interpretation capabilities of LLMs compared to humans, with one user highlighting their conversations with ChatGPT and suggesting that it has some level of understanding, albeit different from humans. There are also discussions about cognitive context, the limitations of LLMs, and the role of numerical computation in LLMs. Another user comments on the power and limitations of LLMs, while others question the balance of views and parameters in LLMs and the concept of digital minds.

### ChatGPT's explosive growth shows first decline in traffic since launch

#### [Submission URL](https://www.reuters.com/technology/booming-traffic-openais-chatgpt-posts-first-ever-monthly-dip-june-similarweb-2023-07-05/) | 24 points | by [mfiguiere](https://news.ycombinator.com/user?id=mfiguiere) | [13 comments](https://news.ycombinator.com/item?id=36606754)

OpenAI's AI chatbot, ChatGPT, experienced a decline in monthly traffic and unique visitors in June, marking the first time this has occurred since its launch in November 2022. According to analytics firm Similarweb, desktop and mobile traffic to the ChatGPT website dropped by 9.7%, while unique visitors decreased by 5.7%. The amount of time spent on the website also saw an 8.5% decline. Similarweb's Senior Insights Manager David Carr attributes the decreasing traffic to the novelty of the chatbot wearing off, while RBC Capital Markets analyst Rishi Jaluria suggests a greater demand for generative AI with real-time information. Despite the drop in website traffic, ChatGPT remains the fastest-growing consumer application ever and currently boasts over 1.5 billion monthly visits, placing it among the top 20 websites globally. OpenAI has not yet commented on the recent decline in traffic.

The discussion about the decline in traffic to OpenAI's ChatGPT chatbot on Hacker News includes several different perspectives. 

One user, stbybbs, comments that the decline in traffic indicates a disruption in software development and content creation in general. They mention that people prioritize quality over quantity and that there may be too much noise rather than thoughtful discussions.

Another user, GaggiX, believes that the reason for the traffic decline is clear: students are no longer using ChatGPT as heavily.

pcrv adds to the discussion by mentioning their observation that they've seen a decline in ChatGPT's activity on LinkedIn.

wg0 brings up the idea of an "AI Winter" and suggests that breakthroughs in AI model development are becoming outdated quickly. They wonder about the training data patching required to achieve higher accuracy, noting that it seems expensive and competitive.

rvz mentions the Gartner Hype Cycle and suggests that AI has reached the "Peak of Inflated Expectations." They argue that AI is squeezing out legitimate research and trustworthy, serious work in favor of summarization and generating output that may not be reliable or meaningful.

In response, rgqtt argues that ChatGPT is not necessarily generating "bullshit" and suggests that it can be useful for making machine learning plans and discussing trends and preferences.

ProllyInfamous suggests trying PerplexityAI, an AI tool that provides sentence predictions based on citations. They note that it doesn't require login/signup or an email.

crlmr agrees with ProllyInfamous, stating that organization-controlled LLMs (large language models) can be very capable for pre-training and fine-tuning.

wg0 apologizes for their earlier downvoted comment and states that they foresee the issue being resolved by 2028. They believe that the hype of large language models will fade away and that unless lower standards are accepted, brand identity will suffer due to the promotion of unreliable models.

