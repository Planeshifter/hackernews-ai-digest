## AI Submissions for Tue Jan 02 2024 {{ 'date': '2024-01-02T17:09:27.205Z' }}

### Images altered to trick machine vision can influence humans too

#### [Submission URL](https://deepmind.google/discover/blog/images-altered-to-trick-machine-vision-can-influence-humans-too/) | 75 points | by [xnx](https://news.ycombinator.com/user?id=xnx) | [75 comments](https://news.ycombinator.com/item?id=38846764)

A new study published in Nature Communications reveals that digital images altered to deceive computer vision systems can also influence human perception. The researchers conducted a series of experiments and found that human judgments were systematically influenced by adversarial perturbations. Adversarial images are those that have been subtly altered to mislead AI models into misclassifying their contents. The study highlights the need for further research to understand the impact of adversarial images on both humans and AI systems. While human vision is not as susceptible to these perturbations as machine vision, the study suggests that they can still bias human perception towards the decisions made by machines. These findings have implications for AI safety and security research, and could potentially contribute to the development of more robust computer vision models.

The discussion on this submission covers various aspects of the study and its implications. Some users discuss the specific details of the experiments, such as the types of images used and the participants involved. Others highlight the potential implications of adversarial images for AI systems and question the reliability of machine vision. There is also a conversation about the methodology of the study, including the sample size and the validity of the findings. Some users express disappointment with the research methodology, suggesting that it is wasteful and does not provide significant insights. Overall, the discussion touches on different angles of the study and raises questions about its significance and validity.

### Distributed Inference and Fine-Tuning of Large Language Models over the Internet

#### [Submission URL](https://browse.arxiv.org/html/2312.08361v1) | 100 points | by [cyanf](https://news.ycombinator.com/user?id=cyanf) | [20 comments](https://news.ycombinator.com/item?id=38843069)

Researchers from HSE University, Yandex, Neiro.ai, the University of Washington, and Hugging Face have developed a cost-efficient method for running large language models (LLMs) by pooling together idle compute resources over the Internet. LLMs with over 50 billion parameters have become increasingly useful in natural language processing (NLP) tasks, but they require high-end hardware that is inaccessible to most researchers. The team investigated methods for distributed inference and fine-tuning of LLMs, comparing local and distributed strategies. They found that a large enough model can run efficiently on geodistributed devices in a consumer-grade network. The team developed fault-tolerant inference algorithms and load-balancing protocols to address the challenges of unreliable devices and unequal hardware, enabling efficient pooling of resources for LLMs. They showcased their algorithms in Petals, a decentralized system that runs LLMs over the Internet up to 10 times faster than offloading for interactive generation. The performance of the system was evaluated in simulated conditions and a real-world setup spanning two continents. This work provides a more cost-effective way of running large language models and opens up access to these models for researchers with limited resources.

The discussion on this submission covers a range of topics related to the development and implementation of decentralized technologies, concerns about the implications of large language models, and debates around the use of Proof of Work in cryptocurrencies.

One user points out that decentralized and distributed technologies are important for the future of AI and suggests practical measures to ensure security. Another user comments that this development makes it harder to shut down AI, while another argues that it makes fighting closed corporations supported by AI more difficult.

A user shares a link to the project they are working on, which is related to the topic of distributed networks and GPUs. Another user expresses surprise at how quickly this technology has progressed and recommends various protocol projects to explore.

The conversation then shifts towards a discussion about distributed network GPUs, with remarks about the availability and affordability of GPUs in the market. Some users argue for the use of cryptocurrencies as a means of payment for GPU usage, while others question the necessity of cryotocurrencies in this context.

There is also a mention of the potential waste of resources in GPU hosting and the need for participants in distributed networks to make choices based on energy efficiency.

The conversation takes a tangent to discuss the environmental impact of Bitcoin and the unnecessary resource consumption compared to other cryptocurrencies.

Lastly, a user mentions the Gridcoin BOINC project as a relevant example of decentralized computing.

Overall, the discussion touches upon various related topics, including decentralized technologies, the implications of large language models, and debates about different approaches to cryptocurrency.

### Microsoft Copilot iOS App

#### [Submission URL](https://apps.apple.com/at/app/microsoft-copilot/id6472538445) | 69 points | by [franze](https://news.ycombinator.com/user?id=franze) | [45 comments](https://news.ycombinator.com/item?id=38841302)

Introducing Copilot: The AI-powered chat assistant designed to enhance your productivity. Powered by the latest OpenAI models, GPT-4 and DALL·E 3, Copilot provides fast, complex, and accurate responses, as well as the ability to create stunning visualizations from simple text descriptions. Whether you're drafting emails, writing stories or scripts, summarizing complex texts, translating content, creating personalized travel plans, or updating resumes, Copilot is the versatile AI assistant that can help you get things done faster. But that's not all - Copilot also features Image Creator, which can transform your design process by quickly generating high-quality visualizations based on text prompts. Whether you're exploring new styles and ideas, curating social media content, developing brand motifs, designing logos, creating custom backgrounds, building portfolios, illustrating books, or visualizing film and video storyboards, Copilot combines the power of GPT-4 with the imagination of DALL·E 3 to elevate your design workflow and inspire your creativity. Experience the future of AI interaction - download Copilot for free today!

The discussion on this submission covers a few different topics:

1. Comparing GPT-4 and Microsoft's AI products: One commenter asks for a detailed analysis comparing GPT-4 with Microsoft's AI products, but no responses with specific comparisons are provided.
2. Issues with Apple's Private Relay and Microsoft sign-in: A discussion starts about concerns with Apple's Private Relay service not working properly in certain regions and Microsoft sign-in problems.
3. Various comments on different apps: Some users mention different apps they have installed, including weather apps, Bing AI apps, and Twitch chat on iOS.
4. Testing the complexity of the AI: One user mentions testing the complexity of the AI and compares it to ChatGPT.
5. Issues with text input and background processes: Some users mention issues with text input not capitalizing sentences correctly and background processes related to Microsoft Edge and Bing.
6. OpenAI's pricing: A user speculates that OpenAI may drop their pricing for GPT-4.
7. ChatGPT's limitations: Users discuss limitations of ChatGPT, such as daily limits and the AI's ability to approximate Hitler's rhetoric.
8. Facebook and Microsoft products: Users express dissatisfaction with Facebook Messenger and Microsoft products, including complaints about cancellation of services and the building of personal profiles.
9. Paying for AI apps: A discussion emerges about people not wanting to pay for AI apps and the value they expect from free software.
10. Privacy concerns: Some commenters express concerns about the privacy implications of AI systems collecting data and Microsoft's practices compared to Apple's.
11. Starting with Copilot: One commenter expresses excitement about trying Copilot.

This summary provides an overview of the main topics discussed, but please note that the comments have been condensed and may not reflect the full context of each discussion.

### The Infinite/Eternal Jukebox has been revived

#### [Submission URL](https://jukebox.davi.gq/jukebox_search.html) | 150 points | by [Pikamander2](https://news.ycombinator.com/user?id=Pikamander2) | [30 comments](https://news.ycombinator.com/item?id=38842703)

Today's Top Stories on Hacker News:

1. "Search for a song" - Ever wished you could easily find a song by just typing in a few lyrics? Well, Paul Lamere's creation at Music Hack Day @ MIT does just that. This innovative tool, hosted by daviirodrig and powered by Spotify, allows users to search for songs by typing in lyrics or even humming the tune. Say goodbye to those frustrating moments of not knowing the name of that catchy tune!

2. "Search for a track" - Are you tired of scrolling through endless playlists trying to find that one track you heard on the radio? Look no further! The "Search for a track" feature developed by Paul Lamere during Music Hack Day @ MIT has got you covered. Simply enter a few details - like the artist, genre, or even the mood you're in - and let this tool find the perfect track for your mood. Whether you're feeling energetic or just need a little relaxation, this innovative search engine will have you covered.

3. "Pick one of these favorites" - Need inspiration for your next listen? Discover new tracks based on popular favorites with this nifty feature developed by Paul Lamere during Music Hack Day @ MIT. It's like having your own personal music curator! Let this tool suggest tracks based on your taste or explore new genres you may have never considered before. With a wide range of favorites to choose from, you'll never run out of new songs to enjoy.

Stay tuned for more exciting updates on the latest innovations in the world of music technology!

The discussion on Hacker News revolves around the various features and functionalities of the Infinite Jukebox, a music streaming tool developed by Paul Lamere during Music Hack Day @ MIT. Some users share their experiences using the tool, while others discuss different aspects and potential improvements.

One user mentions that the tool is effective for discovering new songs based on popular favorites. Another user suggests using the tool to find songs with repetitive sections, while someone else shares their surprise with the regional trend and traditional jukebox fork providing a similar music experience.

Several users discuss technical issues and offer suggestions for improvement. Some users mention using ad blockers, such as uBlock Origin and Privacy Badger, to block scripts and enhance the user experience. Others mention experiencing delays or issues with music circle loading, and some speculate on the dependency of Autocanonizer on Echo Nest, a music analysis API.

There are also comments about compatibility with different platforms. Users mention testing the tool on iOS and noting that it works with content blockers, while others report issues with mobile loading.

Overall, the discussion revolves around users' experiences, technical issues, and potential enhancements for the Infinite Jukebox tool.

### Improving Text Embeddings with Large Language Models

#### [Submission URL](https://arxiv.org/abs/2401.00368) | 41 points | by [cmcollier](https://news.ycombinator.com/user?id=cmcollier) | [6 comments](https://news.ycombinator.com/item?id=38845508)

Researchers Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei have introduced a new method for improving text embeddings using large language models (LLMs). Unlike existing approaches that rely on multi-stage pre-training and fine-tuning with labeled data, their method uses synthetic data generated by proprietary LLMs followed by fine-tuning on open-source decoder-only LLMs. The experiments show that their method achieves strong performance on competitive text embedding benchmarks without using any labeled data. Additionally, when fine-tuned with a combination of synthetic and labeled data, their model sets new state-of-the-art results on the BEIR and MTEB benchmarks. The paper is available for download in PDF format.

The discussion on this submission revolves around the use of synthetic data and large language models (LLMs) for improving text embeddings. One user finds the idea of using synthetic data helpful for generating useful embeddings, especially when fine-tuning with the E5 model. Another user expresses surprise that LLMs can be used for text embeddings. They are then informed that LLMs are commonly used for embedding models, and larger LLMs tend to yield better results.

Another user discusses the comparison between different sizes of LLMs, mentioning that while a larger LLM may be more effective, it also has slower training times. Lastly, a user expresses confusion about LLMs and text embeddings, to which another user explains that LLMs can effectively generate embeddings for semantically similar content while minimizing distances between dissimilar content.

### The I in LLM stands for intelligence

#### [Submission URL](https://daniel.haxx.se/blog/2024/01/02/the-i-in-llm-stands-for-intelligence/) | 71 points | by [nameequalsmain](https://news.ycombinator.com/user?id=nameequalsmain) | [7 comments](https://news.ycombinator.com/item?id=38840907)

Today, I want to talk about the significant effect that AI has had on bug bounties, specifically in the context of the curl factory. Bug bounties involve offering real money rewards to hackers who uncover security issues. While bug bounties have been successful in finding legitimate vulnerabilities in the past, they have also attracted individuals who submit low-quality or even fake reports in hopes of earning a quick buck. Initially, these "rubbish reports" were easy to identify and discard, causing minimal disruption. However, with the advancement of AI technology, these fake reports have become more sophisticated, making it harder and more time-consuming for developers to distinguish between valid and fake reports.

The issue lies in the fact that every security report requires human attention and assessment. The more convincing a fake report appears, the more time and energy developers waste on investigating it before realizing it's garbage. This not only diverts precious resources from productive work but also poses a risk of allowing genuine bug fixes or feature development to be delayed. Additionally, dealing with rubbish reports is mentally draining.

While it's possible that AI could be trained to identify and report legitimate security issues, the current trend seems to be using AI language models to generate fake vulnerability reports. Some users simply input curl code into these models and submit the resulting output as a security report. The challenge for developers is that the generated reports often contain elements of truth mixed with hallucinations, making it difficult to quickly dismiss them. Language barriers and the use of AI or translation tools further complicate the assessment process.

To illustrate the issue, the author provides two examples. The first example involved a user submitting a report claiming that code changes for a severe vulnerability, CVE-2023-38545, had been publicly disclosed online. However, closer examination revealed that the report was a product of an AI tool called Bard and had no connection to reality.

The second example was more challenging to detect. A user filed a report titled "Buffer Overflow Vulnerability in WebSocket Handling," which initially seemed legitimate. The report contained details and even suggested a fix. However, as it turned out, the WebSocket code was not part of the bug bounty program, and upon further investigation, the report exhibited signs of being generated by an AI tool.

These examples highlight the growing problem of AI-generated rubbish reports in bug bounty programs. As AI technology continues to improve, developers and security teams will need to develop strategies to distinguish between genuine vulnerabilities and fake reports. Ultimately, the goal is to ensure that bug bounties remain effective in identifying and addressing security issues while minimizing distractions and wasted resources caused by rubbish reports.

The discussion around this submission involves various perspectives on the impact of AI on bug bounties and the validity of reports. Some users argue that AI could eventually lead to increased productivity gains, while others express concerns about the potential for false positives and the need for human verification. One user points out the resource-intensive nature of testing and validation, emphasizing the imperfections of the current system. Another user highlights the disparity in access to high-quality AI models and the potential for lower-quality models to generate misleading reports. One user humorously mentions the resemblance to science fiction where AI models could replace humans. Another user mentions the expectations of HackerOne, a bug bounty platform, regarding reporter communication and functionality testing. One user shares a related blog post on the topic and another user sarcastically suggests that AI stands for "Absent Intelligence."

