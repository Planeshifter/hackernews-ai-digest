## AI Submissions for Thu Jul 04 2024 {{ 'date': '2024-07-04T17:11:47.159Z' }}

### Finding near-duplicates with Jaccard similarity and MinHash

#### [Submission URL](https://blog.nelhage.com/post/fuzzy-dedup/) | 226 points | by [brianyu8](https://news.ycombinator.com/user?id=brianyu8) | [33 comments](https://news.ycombinator.com/item?id=40872438)

In a recent blog post, the author delves into the topic of approximate deduplication using Jaccard similarity and the MinHash approximation technique. The post explores the concept of defining similarity between documents and the challenges of approximate deduplication at scale.

The Jaccard index, a measure widely used in text processing, compares sets by calculating the ratio of their overlap to the size of their union. The author explains how this index intuitively captures the similarity between sets based on their elements.

To scale up the process of finding approximate duplicates, the post discusses the use of locality-sensitive hashing techniques for Jaccard similarity. By creating MinHash signatures for documents, which are small, fixed-size representations of the original sets, similar documents can be grouped efficiently.

By employing random sampling techniques and permutations, MinHash signatures offer a way to estimate Jaccard similarity between documents without examining the entire sets. This approach provides a practical method for identifying approximate duplicates within a large collection of documents.

1. **ryntwlf** shared a link to a blog post discussing a GPU-accelerated version of a fuzzy deduplication algorithm. They also provided links to documentation and Python scripts related to fuzzy deduplication.

2. **tpchr** highlighted the importance of metrics like Jaccard similarity, Tanimoto coefficients, F1 scores, and Dice coefficients in comparing fuzzy sets. They discussed the complexity of expressing intersection, union, and other concepts related to fuzzy sets.

3. **BiteCode_dev** mentioned their experience implementing a duplicate detection system in Python using a French government database and suggested using the `datasketch` library for memory-efficient implementations. They also mentioned a specialized version of `datasketch` called `rensa`.

4. **RobinL** and **BiteCode_dev** engaged in a conversation about different approaches to duplicate detection, specifically mentioning the Fellegi-Sunter model and the importance of different types of information in unsupervised learning.

5. **-db** provided some historical context about the early days of duplicate detection techniques used by Google and Jeffery Ullman's work on MinHash.

6. **pkeenan11** shared details about implementing a MinHash system to find interesting patterns in inverted matrices, enabling hash-based comparisons and clustering.

7. **crnwl** mentioned working on a MinHash-based system for visualizing Jaccard Similarity calculations and exploring multiple strings.

8. **gpdrtt** discussed the probabilistic nature of MinHash algorithms and the faster calculation of Jaccard results using MinHash.

9. **vvzkstrl** brought up the use of hashing and vector search engines like Tanimoto/Jaccard in duplicate detection strategies for large datasets.

10. **dlftnk** discussed their implementation of a MinHash-like system in BigQuery to calculate cosine similarities above a certain threshold, using techniques like n-grams and global arrays.

11. **gpdrtt** questioned the efficiency of MinHash in calculating distances in a 600,000-item matrix.

12. **drfr** mentioned tackling document clustering and duplicate detection as machine learning problems using pre-trained models and vector embeddings.

### Diffusion Forcing: Next-Token Prediction Meets Full-Sequence Diffusion

#### [Submission URL](https://boyuan.space/diffusion-forcing/) | 206 points | by [magoghm](https://news.ycombinator.com/user?id=magoghm) | [12 comments](https://news.ycombinator.com/item?id=40871783)

The paper "Diffusion Forcing: Next-token Prediction Meets Full-Sequence Diffusion" introduces a novel training paradigm that combines the strengths of full-sequence diffusion models and next-token prediction models. By training a diffusion model to denoise tokens with varying noise levels, this approach, called Diffusion Forcing, allows for flexible and compositional generation while offering sequence-level guidance.

Diffusion Forcing can stabilize auto-regressive rollout, guide over long horizons, and support planning with causal uncertainty by utilizing different noise levels across a sequence during sampling. The method showcases stable and consistent video prediction results in datasets like DMLab and Minecraft, outperforming traditional teacher forcing and causal full-sequence diffusion models.

Moreover, Diffusion Forcing enables the generation of long videos beyond the training horizon without the need for a sliding window approach, demonstrating its stabilization effect. It also extends to diffusion planning, offering a decision-making framework with causal relationships and the ability to model long-horizon tasks like real robot arm manipulations.

Overall, Diffusion Forcing presents a versatile approach that optimizes a variational lower bound on token likelihoods, leading to marked performance gains in decision-making, planning tasks, and video prediction applications.

- **vssns** noted the notable merging of sequence masking and key training of LLMs diffusion models to manage uncertainty levels, treating pixel uncertainty levels as diffusion model noise levels controlled by a sort of embedding. They mentioned interesting aspects like solving controlling robot arm moving tasks and raised questions about tasks around model architecture deserving significant exploration.
- **brvr** appreciated the elegant modeling of uncertainty in planning and search tasks, highlighting the potential for task length forcing and generalizing paths in the context of current state consequences. They also pointed out the need for understanding the missing components in the codebase.
- **IanCal** mentioned a lack of discussion on the linked codebase and expressed interest in exploring its contents further.
- **lk-stnly** discussed research tools leveraging existing text generating LLM diffusion techniques for pre-training and fine-tuning tasks, drawing comparisons to models like GPT Phi 3 and mentioning interest in going deeper into generation levels.
- **trprnm** brought up the applicability of diffusion in robotics, with **krsn** providing a related link to recent research on diffusion in robotics.
- **jmsmmns** appreciated the work presented in a clear and concise manner but sought clarification on the specific problems being addressed by the generative model, with **ctnfrmfr** expressing a lack of understanding regarding concepts like Teacher Forcing.
- **blvscff** raised a concern about missing training time in adding token noise during training, while **mrhc** highlighted the characteristic of Diffusion Forcing resembling a blend of teacher forcing and diffusion models.
- Lastly, **y1zhou** gave a positive flag to indicate agreement or approval of the submission.

### Insights from over 10,000 comments on "Ask HN: Who Is Hiring" using GPT-4o

#### [Submission URL](https://tamerc.com/posts/ask-hn-who-is-hiring/) | 396 points | by [comcuoglu](https://news.ycombinator.com/user?id=comcuoglu) | [151 comments](https://news.ycombinator.com/item?id=40877136)

In a blog post, the author explores the job market and trends by structuring 10,000 comments from Hacker News using GPT-4o and LangChain technology. The author sought to understand the current job landscape, especially in NYC, where they aspire to move. By analyzing job postings, they found insights on remote work, visa sponsorship, experience level distribution, job locations in the US, popular databases, and in-demand JavaScript frameworks. The process involved scraping comments, classifying them, and visualizing data, providing a quick understanding of the job market using LLMs and data science methods.

The discussion on the Hacker News submission mainly focused on technical aspects and critiques of the LangChain technology used in the blog post. Users discussed the challenges with temperature settings in generating structured JSON outputs, the limitations of LangChain in handling certain tasks, and the potential drawbacks of using GPT-4o for complex problems. Some users shared their own experiences with similar AI technologies and highlighted the importance of standardization in data values for machine learning models. Additionally, there were comments about the implications of using AI for job applications and the potential impact on the job market. Overall, the conversation touched on the capabilities and limitations of AI models like LLMs in understanding and generating text.

### Japan introduces enormous humanoid robot to maintain train lines

#### [Submission URL](https://www.theguardian.com/world/article/2024/jul/04/japan-train-robot-maintain-railway-lines) | 208 points | by [thunderbong](https://news.ycombinator.com/user?id=thunderbong) | [87 comments](https://news.ycombinator.com/item?id=40877648)

West Japan Railway has unveiled a futuristic solution to maintain train lines in Japan - a towering 12-metre high humanoid robot mounted on a truck. With eyes resembling Coke bottles and a head reminiscent of Wall-E, this massive machine is equipped with large arms that can wield blades or paint brushes for tasks like trimming tree branches and painting. Operated remotely by a driver in a cockpit, the robot's impressive 40-foot vertical reach allows it to carry heavy loads and perform various maintenance operations, aiming to address worker shortages and enhance safety in the railway industry. This innovative approach could set a new standard for dealing with labor challenges in Japan and beyond.

The discussion around the submission of the humanoid robot for maintaining train lines in Japan on Hacker News covers various aspects. Users commented on the design resembling famous fictional robots like Gundam and Patlabor, with some referencing pop culture such as Wall-E and Johnny 5. There was a debate on the efficiency of using such robots compared to human workers, with concerns about job displacement and the complexity of fully integrating them into tasks like tree trimming. The conversation also touched on the potential cultural and political implications of relying on robots for infrastructure maintenance, as well as the possibility of using virtual reality and advanced camera systems for control and operation. Some users expressed concerns about job shortages and immigration impacts in Japan due to the increased use of robots.

### "Superintelligence" 10 years later

#### [Submission URL](https://www.humanityredefined.com/p/superintelligence10-years-later) | 86 points | by [evilcat1337](https://news.ycombinator.com/user?id=evilcat1337) | [109 comments](https://news.ycombinator.com/item?id=40872799)

In a nostalgic reflection on the tech scene of 2014, Conrad Gray revisits the release of Nick Bostrom's groundbreaking book, "Superintelligence." The book sparked conversations about AI risks and the emergence of machine intelligence. Elon Musk and other influential figures endorsed it, with Musk famously likening AI to "summoning a demon." Despite critics dismissing the existential threats, the discussion around AI safety has now entered the mainstream, fueled by recent advancements such as ChatGPT. The public's exposure to cutting-edge AI technology has raised awareness and sparked debates about the implications of superintelligence. A compelling reflection on the evolution of AI discourse in the past decade.

The discussion on the submission about the article "Superintelligenceâ€”10 years later" delves into various aspects related to artificial intelligence and corporate intelligence. There are debates about maximizing profits, the role of corporations in society, the dangers of AI, and the concepts of superintelligence. The commenters discuss topics such as the implications of superintelligence, the capabilities of AI compared to human understanding, philosophical discussions on omnipotence, the nature of intelligence, and the intersection of philosophy and technology. Overall, the discussion spans from practical considerations of AI to abstract philosophical reflections on the nature of intelligence and its implications for society.

### AI washing: Silicon Valley's big new lie

#### [Submission URL](https://www.computerworld.com/article/2511301/ai-washing-silicon-valleys-big-new-lie.html) | 32 points | by [sharpshadow](https://news.ycombinator.com/user?id=sharpshadow) | [10 comments](https://news.ycombinator.com/item?id=40876638)

Today's top story on Hacker News delves into the concept of AI washing, a misleading marketing practice that exaggerates the role of artificial intelligence in products or services being promoted. The piece by Mike Elgan sheds light on how companies often overstate the capabilities of AI, presenting a facade of autonomous systems while relying heavily on human intervention behind the scenes.

The article highlights examples like Amazon's high-tech stores and self-driving cars, revealing that despite the AI hype, there are significant human efforts involved in making these technologies function effectively. For instance, Amazon's cashier-less stores required around 1,000 human employees to ensure smooth operations, despite the initial impression of a completely automated checkout experience.

The piece also touches on the reasons behind AI washing, attributing it to the belief among tech leaders that AI can solve complex problems autonomously. However, the reality often falls short of these lofty claims, leading companies to downplay the human involvement necessary to support and operate their AI-driven solutions.

In a tech landscape where AI promises are abundant, this insightful commentary serves as a reminder to look beyond the AI hype and understand the nuanced interplay between artificial intelligence and human intervention in modern tech innovations.

- **ddgrd** comments on the notion of experts in VR, blockchain, and AI, suggesting that simply mentioning these terms doesn't make someone an expert, and that true expertise requires more than just superficial knowledge.

- **joe_the_user** reflects on the complexity of AI and its limitations in directly transforming things, pointing out the tendency in the industry to focus more on generating content rather than improving the quality of existing content.

- **chrsjj** criticizes Amazon for labeling its technology as AI in a deceptive manner, with a discussion ensuing about the actual application of AI in Amazon's stores and the involvement of human employees despite the AI facade. There's also a mention of Amazon's staff in India remotely managing cameras in stores and rejecting the notion of fully autonomous operations.

- **superb_dev** and **chrsjj** delve into the discussion of Amazon's AI washing practices, highlighting that the sales pitch doesn't match the reality of human involvement in the technology.

- **lfw** comments on the significant human workforce required in Amazon's stores, particularly in India, and questions the true level of automation versus human intervention in the operation of these stores.

Overall, the discussion touches upon the misrepresentation of AI in marketing, the importance of genuine expertise in technology, and the realities of human involvement behind the scenes of seemingly autonomous systems.

