## AI Submissions for Thu Oct 16 2025 {{ 'date': '2025-10-16T17:16:52.000Z' }}

### DoorDash and Waymo launch autonomous delivery service in Phoenix

#### [Submission URL](https://about.doordash.com/en-us/news/waymo) | 288 points | by [ChrisArchitect](https://news.ycombinator.com/user?id=ChrisArchitect) | [650 comments](https://news.ycombinator.com/item?id=45605501)

DoorDash x Waymo: autonomous delivery pilot in Phoenix + $10 Waymo ride perk for DashPass

- What’s new: DoorDash is testing fully autonomous deliveries with Waymo in Metro Phoenix now, aiming for broader commercial ops later this year. Early rollout starts with DashMart orders; some customers may be matched with a driverless Waymo vehicle via DoorDash’s Autonomous Delivery Platform (which orchestrates Dashers, robots, drones, and AVs).

- Member promo: DashPass members in LA, SF, and Phoenix get $10 off one Waymo ride per month through Dec 31, 2025. A new promo code is issued at the start of each month. Valid on weekday rides booked between 2 a.m. and 2 p.m.; terms apply.

- Why it matters: 
  - Signals DoorDash’s push toward a multimodal, automated last mile (and follows its Oct 9 partnership to bring Serve Robotics’ delivery robots onto the platform).
  - Phoenix remains a key AV testbed; this ties robo‑taxis directly into mainstream delivery commerce.
  - If it scales, it could change delivery unit economics and labor mix; for now it’s limited in geography, merchants (DashMart), and scope.

- Fine print: The AV delivery is a test; timelines and expansion are subject to change (forward‑looking statements). Promo is time‑windowed and limited to one discounted ride per month.

The discussion revolves around the challenges faced by small restaurants in cities with high minimum wages, such as Seattle and Denver, and broader economic implications:

1. **Impact of High Minimum Wages**:  
   - Critics argue that elevated minimum wages strain small restaurants, forcing price hikes and reducing customer traffic. Some claim this favors large chains (e.g., McDonald’s) with better labor efficiency, squeezing out independent eateries.  
   - Counterarguments assert businesses unable to pay living wages “shouldn’t exist,” emphasizing ethical labor practices over profitability.  

2. **Commercial Rent and Urban Costs**:  
   - Many highlight **skyrocketing commercial rents** and zoning restrictions as critical issues, arguing these costs outweigh wage pressures. Corporate landlords and real estate speculation are blamed for displacing small businesses.  
   - Suggestions include a **Land Value Tax** to deter rent-seeking and zoning reforms to increase housing density, lowering operational costs.  

3. **Drone Delivery and Automation**:  
   - A tangent proposes drone delivery as a cost-saving model (à la Ryanair), but skeptics note logistical hurdles (e.g., air traffic management for millions of packages).  

4. **Systemic Solutions**:  
   - Ideas like **Universal Basic Income (UBI)** and worker-owned cooperatives emerge as alternatives to wage mandates, aiming to reduce reliance on low-wage labor.  
   - Others blame urban desirability and immigration for inflating housing/rental markets, exacerbating small-business struggles.  

5. **Geographic Examples**:  
   - Seattle’s housing shortage and construction costs are dissected, with mixed views on whether zoning reforms (e.g., allowing multi-family units) have helped.  
   - Australia is cited as a counterpoint, where high wages coexist with thriving small restaurants, suggesting other factors (e.g., rent control) might be at play.  

**Key Tensions**: The debate reflects ideological divides—pro-labor vs. pro-business perspectives, with systemic critiques of capitalism (e.g., corporate consolidation, rentier economies) underpinning many arguments. The DoorDash-Waymo pilot, while not directly addressed, symbolizes the automation trend that could further disrupt labor dynamics in delivery services.

### Codex Is Live in Zed

#### [Submission URL](https://zed.dev/blog/codex-is-live-in-zed) | 258 points | by [meetpateltech](https://news.ycombinator.com/user?id=meetpateltech) | [56 comments](https://news.ycombinator.com/item?id=45606698)

Zed adds OpenAI Codex via ACP, open-sources adapter

- The Zed IDE now supports OpenAI’s Codex out of the box through the Agent Client Protocol (ACP), joining existing integrations like Claude Code and Google’s Gemini CLI. You can pick Codex from the New Thread menu.
- Privacy and billing: Zed doesn’t proxy requests or charge for external agents—your prompts/code go directly to OpenAI, and you pay OpenAI directly.
- The codex-acp adapter is open-sourced, so Codex via ACP can be used outside Zed as well.
- Implementation notes: Codex runs terminal commands inside the agent and streams output to the client, unlike other agents that ask the client to run commands. This surfaces tradeoffs:
  - PTY mode (client-run): interactive, colorful output, but can deadlock agents (e.g., git rebase --continue opening an editor).
  - Non-PTY (agent-run): fewer colors/less interactivity, but fewer “stuck” states.
- ACP momentum: Originally built with the Gemini CLI team, ACP is now being adopted across editors (Neovim, Emacs, JetBrains). Zed plans to focus on evolving the protocol with the community rather than building more adapters.

Why it matters: ACP is quickly becoming a cross-editor standard for AI coding agents, and Zed’s privacy-first, open-source adapter approach makes it easier for developers to choose their agent without switching tools.

**Summary of Hacker News Discussion on Zed's OpenAI Codex Integration via ACP:**

1. **Performance and Feature Comparisons**  
   - Users noted Zed's speed but highlighted **pain points with Python completions** and file navigation compared to JetBrains IDEs (e.g., PyCharm).  
   - **C# support via OmniSharp** was criticized as slow for larger projects, prompting discussions about alternatives like Rider or VS Code.  
   - Missing **Jupyter notebook support** was raised as a barrier to adoption, though some suggested plugin possibilities.  

2. **AI Integration and Quality**  
   - **Codex via CLI** was perceived as slower than Claude and Gemini, with mixed feedback on its utility.  
   - **In-line AI suggestions** faced criticism:  
     - Users compared Zed unfavorably to **Cursor** and JetBrains AI, citing weak renaming/module refactoring support.  
     - Some argued that AI should complement—not replace—traditional LSP-driven features like semantic search.  
   - Skepticism emerged about relying on AI for deterministic tasks (e.g., code renaming), with calls to prioritize accuracy over novelty.  

3. **Pricing and Privacy**  
   - Confusion arose over Zed’s **$10/month subscription vs. $5 AI credits model**. Clarifications highlighted separate billing for AI providers (OpenAI, Claude).  
   - Privacy practices (direct API calls to OpenAI, no proxying) were praised, though debates surfaced about AI-generated comment detection.  

4. **Community and ACP Adoption**  
   - Enthusiasm for **ACP becoming a cross-editor standard** (Neovim, Emacs, JetBrains) but calls for Zed to focus on **improving core IDE features** rather than building more adapters.  
   - GitButler comparisons sparked interest in collaborative workflows, though users questioned its required workflow changes.  

5. **User Workflow Preferences**  
   - Some advocated disabling AI entirely, favoring **traditional snippets, regex, and multi-cursor edits** for reliability.  
   - Others expressed frustration with Zed’s **learning curve for keyboard shortcuts** and context-aware features.  

6. **Criticisms and Requests**  
   - Requests for **Windows optimizations** and clearer release cycles (LTS vs. rapid updates).  
   - Mixed reactions to Zed’s UI/UX: praise for its minimalist design but complaints about **"logger-like" file exploration** and diff-view limitations.  

**Conclusion**: The discussion reflects **cautious optimism** for ACP’s potential and Zed’s privacy-first approach, tempered by critiques of its current AI implementation and niche IDE shortcomings. Users emphasized balancing innovation with refining core functionality (e.g., LSP performance, language support) to compete with established tools.

### Gemini 3.0 spotted in the wild through A/B testing

#### [Submission URL](https://ricklamers.io/posts/gemini-3-spotted-in-the-wild/) | 401 points | by [ricklamers](https://news.ycombinator.com/user?id=ricklamers) | [255 comments](https://news.ycombinator.com/item?id=45607758)

Rumored Gemini 3.0 surfaces in Google AI Studio A/B test, excels at SVG generation

- What happened: A user reports catching an A/B test in Google AI Studio that appears to expose “Gemini 3.0.” Using a simple prompt to “Create an SVG image of an Xbox 360 controller,” the model produced a notably high‑fidelity SVG—better than current frontier models in their experience.

- Why it matters: SVG/structured drawing has emerged as a surprisingly strong proxy for overall model capability (popularized by Simon Willison’s “pelican riding a bicycle” test). Strong SVG suggests better spatial reasoning, precision, and code/format adherence—skills that often correlate with coding performance, a key expectation for Gemini 3.0.

- Details:
  - Prompt: “Create an SVG image of an Xbox 360 controller. Output it in a Markdown multi-line code block.”
  - Reported model ID: ecpt50a2y6mpgkcn (not clearly indicative of version).
  - Performance deltas vs Gemini 2.5 Pro: ~+24s time-to-first-token; ~40% longer output (including apparent reasoning tokens).
  - Author speculates the A/B was likely Gemini 3.0 Pro vs 2.5 Pro; a 3.0 Flash vs 2.5 Pro matchup seems less likely.

- Caveats:
  - N=1 anecdote; A/B access appears sporadic.
  - Model ID isn’t definitive; Google hasn’t announced details.
  - Longer latency/output doesn’t necessarily imply heavy test-time compute—just different generation behavior.

Bottom line: If accurate, early signs point to Gemini 3.0 making a visible leap in structured/code-like generation, bolstering hopes for improved coding performance ahead of any official release.

**Summary of Discussion:**

The Hacker News discussion revolves around the rumored Gemini 3.0's SVG generation capabilities and expands into broader debates about AI model performance, creativity, and practical applications. Key points include:

1. **Model Comparisons and Strengths**:  
   - Users compare Gemini 2.5 Pro, Claude Opus, GPT-5, and DeepSeek in creative writing and reasoning tasks.  
   - Gemini is praised for technical tasks (e.g., summarizing papers, HTML/CSS) but criticized for weaker creative writing and poetry compared to Claude or GPT-5.  
   - Some note Gemini’s ability to handle large token contexts, making it useful for technical documentation and structured outputs like SVG generation.

2. **Creativity vs. Determinism**:  
   - Adjusting parameters (temperature, top_p, top_k) significantly impacts creativity. Lower settings yield predictable outputs, while higher values produce more poetic or nonsensical text.  
   - Users joke about AI-generated "slackborn" poetry, referencing historical examples like Racter (1980s procedural poetry) and literary figures like Paul Celan.  

3. **Practical Applications**:  
   - AI’s role in creative workflows is debated. Some use models for brainstorming RPG campaigns or drafting content, though outputs often lack polish.  
   - For collaborative storytelling (e.g., D&D), AI-generated characters/worlds are seen as useful starting points but require human refinement.  

4. **Technical Limitations**:  
   - Concerns arise about AI’s tendency to "spiral" into incoherence, especially in creative tasks.  
   - Users critique Gemini’s latency and occasional inconsistency in code generation compared to competitors.  

5. **Philosophical Reflections**:  
   - Debates emerge about whether "good writing" hinges on human intent or reader perception, with some arguing AI’s role is to augment, not replace, creativity.  

**Bottom Line**: While excitement exists for Gemini 3.0’s potential in structured tasks like SVG/code generation, the discussion underscores ongoing challenges in balancing AI creativity, reliability, and practical utility. Historical parallels and technical parameter debates highlight the community’s nuanced views on progress in AI capabilities.

### Claude Skills

#### [Submission URL](https://www.anthropic.com/news/skills) | 743 points | by [meetpateltech](https://news.ycombinator.com/user?id=meetpateltech) | [394 comments](https://news.ycombinator.com/item?id=45607117)

Anthropic launches Agent Skills: on-demand “skill packs” that make Claude better at specific jobs across apps, Claude Code, and the API.

- What it is: Skills are folders with instructions, scripts, and resources (SKILL.md + assets). Claude scans available skills and auto-loads only what’s needed, stacking multiple skills when relevant.
- Why it matters: Moves beyond ad‑hoc prompting to reusable, governed workflows—more predictable outputs on tasks like Excel, PowerPoint, Word, and fillable PDFs. Think “packaged expertise” you can share across teams.
- Under the hood: Skills can run executable code inside the Code Execution Tool beta sandbox. They’re composable, portable, and efficient (minimal loading). You can see which skills were invoked as Claude works.
- For users: Available in Claude apps for Pro, Max, Team, and Enterprise. A “skill-creator” guides you through building skills—no manual file editing. Admins must enable Skills org‑wide for Team/Enterprise.
- For developers: Add skills to Messages API requests; manage versions via the new /v1/skills endpoint and the Claude Console. Install via the anthropics/skills marketplace or manually at ~/.claude/skills. Supported in the Claude Agent SDK.
- Early adopters: Box, Notion, and Canva highlight faster, more consistent outputs; finance teams report multi-spreadsheet reviews and reporting dropping from a day to about an hour.
- What’s next: Simplified creation and enterprise-wide deployment. Caveat: Skills execute code—use trusted sources and be mindful of data security.

Docs, console, and example skills are available to get started.

The discussion around Anthropic's Claude Skills reveals several key themes and debates:

**1. Comparisons to Model Control Protocol (MCP):**  
Many commenters contrast Claude Skills with the open-source MCP framework. Skills are seen as more proprietary and client-focused, while MCP emphasizes a server-client architecture with discoverable prompts. Some view Skills as a streamlined alternative to MCP's complexity, praising their folder-based structure (`SKILL.md` + assets) over JSON configurations. However, critics argue Skills lack MCP's standardized tooling ecosystem.

**2. Technical Implementation:**  
- Skills leverage a **directory pattern** similar to `AGENTSmd`/VSCode's agent system, allowing dynamic context-aware loading.  
- The **code execution sandbox** (via Code Interpreter) enables practical workflows like PDF processing or spreadsheet automation.  
- Users highlight efficient context management, with Skills loading only relevant instructions to avoid token bloat.  

**3. Enterprise Adoption & Use Cases:**  
- Early adopters like finance teams report **90% time savings** (e.g., multi-spreadsheet analysis reduced from a day to an hour).  
- Enterprise admins appreciate governance features—skills can be centrally managed via API endpoints and require org-wide enablement.  

**4. Security Concerns:**  
Warnings emerge about **code execution risks**, echoing Anthropic's caveat. Users stress the need to vet skill sources, as malicious skills could exploit sandbox access.

**5. Community Reactions:**  
- **Positives**: Praise for simplified prompt engineering, reusable workflows, and Claude Console integration. Examples like [PDF processing skills](https://github.com/anthropic/skills/tree/main/pdf-document-skill) demonstrate tangible utility.  
- **Criticisms**: Some see Skills as "hyped framework appendices" that merely inject text prompts, questioning if they meaningfully differ from existing prompt-chaining techniques.  

**6. Ecosystem Integration:**  
- Skills complement tools like **Cursor Rules** and **Linear** for task-specific context retrieval.  
- Developers note parallels with ChatGPT's "Projects" but highlight Claude's tighter focus on task-specific skill stacking.  

**7. Future Directions:**  
Debates arise about whether Skills will evolve toward **sub-agent orchestration** or remain focused on prompt templating. Simon Willison's [analysis](https://simonwillison.net/2025/Oct/16/claude-skills/) suggests Skills could mature into a standardized "package manager" for LLM capabilities.  

In summary, Claude Skills are viewed as a pragmatic step toward modular, enterprise-safe AI workflows, albeit with lingering questions about differentiation from existing paradigms like MCP and the long-term vision for AI agent ecosystems.

### Show HN: Inkeep (YC W23) – Agent Builder to create agents in code or visually

#### [Submission URL](https://github.com/inkeep/agents) | 74 points | by [engomez](https://news.ycombinator.com/user?id=engomez) | [49 comments](https://news.ycombinator.com/item?id=45604700)

Inkeep Agents: no‑code + TypeScript AI agents with 2‑way sync

What it is
- A source-available framework for building AI assistants and multi‑agent workflows via either a no‑code visual builder or a TypeScript SDK—both stay in sync via CLI push/pull.

Why it’s interesting
- 2‑way sync between visual and code editing lets non‑technical and engineering teams collaborate without drift.
- Built for real-time chat assistants and workflow automation (support, docs, CRMs, ticket triage, content ops).
- Multi‑agent architecture with MCP tools and credential management.

How it works
- Components: manage API (configs), run API (runtime + state + OTEL traces), visual builder UI, TS SDK (@inkeep/agents-sdk), CLI, and embeddable chat UI library.
- Streams are compatible with Vercel AI SDK primitives (useChat, AI Elements); deploy via Vercel or Docker.
- Observability via a Traces UI and OpenTelemetry.

Licensing and openness
- Elastic License 2.0 with supplemental terms (fair‑code, source‑available). LLM‑agnostic, standard protocols, self‑host friendly. Managed cloud “coming soon.”

Links
- GitHub: github.com/inkeep/agents
- Docs: docs.inkeep.com

Takeaway
- A pragmatic option if you want Assistants/Agents you can design visually, refine in code, and run with production‑grade tracing and deployment—without locking into a single LLM or vendor runtime.

The discussion around Inkeep Agents on Hacker News highlights both enthusiasm for its collaboration-focused features and questions about its differentiation from existing tools:

### Key Themes:
1. **Two-Way Sync & Collaboration**:  
   - Praised for enabling **visual no-code + code workflows** that reduce drift between non-technical and engineering teams. Users note this addresses a gap in tools like *LangChain* and *n8n*, which lack bidirectional code export/import.  
   - Compared to legacy systems (e.g., *LabVIEW*, *SAS Enterprise Miner*), with some debate on whether visual programming aids or complicates debugging and adoption.

2. **Differentiation & Use Cases**:  
   - Questions about how it stands apart from *OpenAI Assistants* (more code-focused) and *Dify* (limited code flexibility). The team clarifies Inkeep offers **LLM-agnosticism** and production-grade observability.  
   - Highlighted for **multi-agent workflows** suited for conversational AI (e.g., customer support bots) over deterministic pipelines.

3. **Technical Feedback**:  
   - Interest in **integration** with services like AWS Bedrock and OpenRouter.  
   - Concerns about **self-hosting complexity** (Docker setup issues resolved via an overlooked `.env` file) and debugging capabilities.  
   - Query on **conflict resolution** (currently manual merging; better tooling in development).

4. **Adoption & Pricing**:  
   - Skepticism about targeting VC-backed B2B SaaS startups, with speculation about pricing models. The team shares a waitlist for their upcoming managed cloud service.  
   - Acknowledgment of potential enterprise value for automating internal processes (e.g., procurement workflows).

### Team Responses:  
- Inkeep’s developers actively engaged, addressing questions about integrations, self-hosting, and roadmap plans.  
- Emphasized focus on **modularity** and compatibility with tools like Vercel AI SDK, aiming to avoid vendor lock-in.

### Conclusion:  
While seen as promising for bridging code/no-code divides, real-world adoption may hinge on easing debugging, refining conflict resolution, and proving value against established workflow tools.

### A stateful browser agent using self-healing DOM maps

#### [Submission URL](https://100x.bot/a/a-stateful-browser-agent-using-self-healing-dom-maps) | 117 points | by [shardullavekar](https://news.ycombinator.com/user?id=shardullavekar) | [57 comments](https://news.ycombinator.com/item?id=45604451)

I’m ready to summarize—could you share the Hacker News thread link (or item ID) or the article URL/text for the submission you want covered?

Options I can include:
- 2–3 paragraph summary plus key takeaways and “Why it matters”
- Notable HN comments and consensus/controversy
- Quick stats (points, comments, ranking) and related links

Tell me your preferred length (brief ~120 words, standard ~250, or deep dive ~500) and tone (neutral, punchy, or technical).

**Hacker News Discussion Summary: "Self-Healing CSS & AI-Driven Web Testing"**  
*(Key Themes: AI in Testing, Accessibility, DOM Stability, Tooling Challenges)*  

**Core Debate**: The thread explores challenges in maintaining reliable CSS selectors for dynamic web elements, especially when integrating AI-driven testing tools. Users discuss whether "self-healing" CSS/selectors (via AI or browser-generated logic) can address brittleness caused by frequent frontend changes, or if prioritizing semantic HTML/ARIA roles and WCAG compliance is a more sustainable approach.  

**Notable Points**:  
1. **AI & Testing Workflows**: Tools like Playwright and experimental projects (Agent4, Chrome DevTools MCP) aim to generate stable selectors or "memorize" workflows. Critics argue AI-generated scripts risk fragility if they rely on arbitrary class names instead of semantic markup.  
2. **Accessibility vs. Convenience**: Some emphasize that proper ARIA labels and semantic elements inherently create reliable selectors while aiding screen readers. Others counter that legacy/commercial sites often lack these, forcing reliance on brittle class-based targeting.  
3. **Tooling Innovations**:  
   - **Playwright** and **MCP** (Memory Control Protocol) are highlighted for enabling browser-agent communication, though setup complexity is noted.  
   - Open-source efforts like [promptware/readweb](https://github.com/promptware/readweb) use preset HTML detectors to extract content programmatically.  

**Controversies**:  
- **Security**: Bypassing CAPTCHAs/Turnstile via AI raises ethical concerns.  
- **Maintenance**: Frequent site UX changes disrupt even "stable" selectors, creating endless upkeep for browser agents.  
- **AI Reliability**: Users report frustration with AI coding tools (e.g., Cursor, Claude) producing broken code after minor updates, highlighting a need for "checkpointing" workflows.  

**Why It Matters**: As AI adoption grows in testing and automation, balancing dynamic adaptability with semantic rigor becomes critical. The discussion underscores a tension between quick fixes (AI-generated selectors) and long-term sustainability (accessibility-first design).  

**Stats/Context**:  
- Mentions of **WCAG compliance**, **Playwright**, and **Chrome DevTools MCP** recurred as focal points.  
- A subthread on [YouTube](https://youtu.be/nfOVgz_omlU) explores logging/feedback loops for LLMs in debugging.  

**Tone**: Technical with pragmatic undertones, reflecting both optimism about tooling advances and skepticism toward overreliance on AI.

### New coding models and integrations

#### [Submission URL](https://ollama.com/blog/coding-models) | 215 points | by [meetpateltech](https://news.ycombinator.com/user?id=meetpateltech) | [65 comments](https://news.ycombinator.com/item?id=45601834)

Ollama adds big-code models and one-click IDE integrations

- New models: GLM-4.6 and Qwen3-Coder-480B are now on Ollama Cloud; Qwen3-Coder-30B is updated for faster, more reliable tool-calling in Ollama’s new engine.
- Local if you dare: Qwen3-Coder-480B can run on your machine if you’ve got more than 300GB of VRAM; otherwise use the cloud variants.
- Plug into your editor: Native hooks for VS Code (Copilot Chat → Provider: Ollama), Zed (now on Windows; LLM providers → Ollama, host http://localhost:11434), and Droid (factory.ai CLI with simple model switching). Docs also cover Cline, Roo Code, and more.
- Quickstart: ollama run glm-4.6:cloud or ollama run qwen3-coder:480b-cloud; or ollama pull those models to make them selectable inside your IDE.
- Cloud API: Create an API key and hit ollama.com/api/chat with Bearer auth to use cloud models directly from your apps.
- Why it matters: You get access to giant, code-specialized LLMs without managing GPUs, plus smoother agent/tool use in popular editors.

**Summary of Discussion:**

1. **Model Performance & Preferences:**
   - Users report positive experiences with **GLM-4.6** for complex reasoning and coding tasks, though some note limitations compared to **Claude** and **Codex**. 
   - **Qwen3-Coder-30B** is praised for backend code generation but criticized for impractical local hardware requirements (300GB+ VRAM). 
   - Mixed opinions on **Claude**: While powerful, users express frustration with usage limits (daily/weekly caps) and pricing tiers ($20-$100/month). Some have switched to GLM-4.6 or ChatGPT due to cost.

2. **Cost Concerns:**
   - Debates over subscription models: $6/month for Claude is deemed reasonable, but higher tiers ($100+) face backlash. 
   - Alternatives like **OpenRouter** (cheaper credits) and **GLM-4.6 via cloud** are explored to bypass Claude’s restrictions.

3. **Hardware Challenges:**
   - Running large models locally (e.g., Qwen3-Coder-480B) requires extreme hardware (e.g., NVIDIA GH200 GPUs, Mac Studio with 512GB RAM), sparking skepticism about accessibility. 
   - Local inference on laptops is deemed impractical due to slow performance; cloud solutions are preferred for practicality.

4. **Ollama’s Direction & Sustainability:**
   - Criticism of Ollama’s shift toward cloud reliance and opaque model support. Users advocate for broader local compatibility (e.g., via KoboldCPP).
   - Concerns about Ollama’s business model: Reliance on VC funding, potential acquisitions, or subscription fatigue. Some suggest open-source sustainability or community funding.
   - Defenders highlight Ollama’s convenience as a wrapper for `llama.cpp` and willingness to pay for bandwidth/development.

5. **Political & Ethical Tangents:**
   - A subthread critiques Chinese AI integration with military research, countered by examples of U.S. tech companies (Meta, Microsoft) implicated in conflicts (Myanmar, Israel-Palestine).

**Key Takeaways:**  
Users value Ollama’s new models and IDE integrations but question hardware feasibility and long-term viability. While GLM-4.6 gains traction, Claude’s limits and cost drive exploration of alternatives. Debates highlight tensions between local/cloud workflows and sustainability of AI tooling ecosystems.

### SWE-Grep and SWE-Grep-Mini: RL for Fast Multi-Turn Context Retrieval

#### [Submission URL](https://cognition.ai/blog/swe-grep) | 93 points | by [meetpateltech](https://news.ycombinator.com/user?id=meetpateltech) | [27 comments](https://news.ycombinator.com/item?id=45607822)

Cognition unveils SWE-grep and SWE-grep-mini: RL-trained, parallel “Fast Context” retrievers for codebases, now rolling into Windsurf

- The problem: In tools like Windsurf and Devin, more than 60% of the first turn often goes to context retrieval. Traditional approaches either rely on embedding/RAG (fast but often imprecise for multi-hop code tracing) or “agentic” CLI exploration (flexible but slow, chatty, and prone to context pollution).

- The pitch: SWE-grep and SWE-grep-mini are small, agentic retrieval models trained via RL to match the retrieval quality of frontier coding models while running an order of magnitude faster. They power a new Windsurf subagent called Fast Context.

- How it works:
  - Trained to run limited serial turns (about 4), each with highly parallel tool calls (e.g., 8-way grep/glob/read), minimizing latency while exploring multiple code paths at once.
  - Co-designed with a tight toolset and fast execution (indexing, multi-threading, restricted commands) plus fast inference.
  - Retrieval is verifiable: instead of summarizing, the subagent returns file paths and line ranges, enabling a clean, deterministic reward signal for RL and avoiding misleading summaries.

- Why a subagent: It conserves the main agent’s context budget and reduces “context poisoning,” handing over only the relevant lines/files so the smarter model can focus on reasoning and edits.

- Where to try:
  - Rolling out progressively in Windsurf; it triggers automatically when code search is needed (or force it with Cmd+Enter in Cascade).
  - Public demo playground with side-by-side runs against stock Claude Code and Cursor CLI: https://playground.cognition.ai/
  - Caveat: The comparison is a demo, not a rigorous benchmark; all agents are hosted in identical Modal containers with stdin/stdout piping to resemble local use. Authors recommend trying in your own setup.

- Why it matters: If retrieval is the latency bottleneck for coding agents, a fast, parallel, RL-tuned retriever that returns precise code spans could make “smart” agents feel much snappier on large codebases—without drowning them in irrelevant tokens.

Here's a concise summary of the Hacker News discussion about Cognition's SWE-grep tools and Windsurf integration:

**Key Reactions & Discussions:**
1. **Positive Feedback**:  
   - Users praised the engineering effort ("highly impressive") and real-world utility, noting Windsurf's responsiveness compared to traditional LLM coding tools.  
   - SWE-grep's parallel processing (8-way grep/glob) and deterministic context retrieval were highlighted as innovative solutions to LLM latency issues.

2. **Technical Questions**:  
   - Clarification sought on how SWE-grep balances speed/precision vs embeddings/RAG.  
   - Interest in how "Fast Context" subagents avoid "context poisoning" by returning clean file/line references instead of summaries.  

3. **Performance Observations**:  
   - Users shared speed comparisons: Claude Code (0.1s) vs Cursor CLI (19s) in demo tests.  
   - Discussion emphasized that retrieval speed ("60% of first-turn latency") critically impacts coding agent usability.  

4. **Skepticism & Caveats**:  
   - Some questioned demo validity, urging independent benchmarks due to concerns about cached results or artificial constraints.  
   - Noted the tradeoff between "agentic search" flexibility and SWE-grep's restricted-but-faster command set.  

5. **Broader Implications**:  
   - Debate about whether dedicated retrieval subagents (like Fast Context) represent a paradigm shift vs incremental optimization.  
   - Recognition that efficient context engineering ("Read Files" capability) is becoming as crucial as generation quality ("Generate Diffs") for coding agents.  

**Miscellaneous**:  
- Public demo availability drew interest, though some reported temporary access issues.  
- Users requested deeper technical details about RL training and verifiable reward signals.  
- Comparisons made to IDE tools (RubyMine) and speculation about future "workflow memory" optimizations.

### Nvidia DGX Spark and Apple Mac Studio = 4x Faster LLM Inference with EXO 1.0

#### [Submission URL](https://blog.exolabs.net/nvidia-dgx-spark/) | 57 points | by [edelsohn](https://news.ycombinator.com/user?id=edelsohn) | [19 comments](https://news.ycombinator.com/item?id=45611912)

NVIDIA DGX Spark + Mac Studio: EXO splits LLM inference to cut latency, boost throughput

- The pitch: EXO 1.0 pairs two very different boxes for one request—DGX Spark handles the compute‑heavy prefill (time‑to‑first‑token), while an Apple Mac Studio M3 Ultra handles the memory‑bound decode (tokens/sec). Result: up to 4x faster inference in the right conditions.

- Why this works:
  - Prefill is compute‑bound. DGX Spark (~100 TFLOPs FP16, 128 GB coherent CPU‑GPU memory at 273 GB/s) excels here.
  - Decode is memory‑bandwidth‑bound. M3 Ultra (512 GB unified memory at 819 GB/s, ~26 TFLOPs FP16 GPU) excels here.
  - EXO streams each layer’s KV cache over the network as it’s produced, overlapping communication with compute to hide transfer costs.

- Key technical idea: layer‑by‑layer KV streaming over 10 GbE. If per‑layer compute time exceeds KV transfer time, the network cost is hidden. Rule of thumb from their derivation:
  - Need s > (P/B)·q / K, where s is prompt length, P/B is compute-to-bandwidth ratio, q is KV quantization bits, and K depends on attention type.
  - With P/B ≈ 10,000 (100 TFLOPs over 10 Gbps), 8‑bit KV:
    - K=16 (GQA, e.g., Llama‑3 70B): s > ~5k tokens
    - K=8 (Llama‑3 8B): s > ~10k tokens
    - K=2 (MHA, Llama‑2 7B): s > ~40k tokens
  - Takeaway: bigger contexts and GQA models benefit sooner; faster links would lower thresholds.

- Benchmark (Llama‑3.1 8B, FP16, 8,192‑token prompt, 32‑token output):
  - Mac Studio only: 6.42 s (baseline)
  - DGX Spark only: 4.34 s (1.9× faster)
  - DGX Spark + Mac Studio (EXO): 2.32 s (2.8× faster)
  - Interpreting the split: DGX slashes TTFT (prefill), Mac Studio maximizes TPS (decode).

- Why it matters: A pragmatic, heterogenous inference design that exploits what each device does best—compute vs memory bandwidth—without requiring giant GPUs. It also shows how model architecture (GQA vs MHA), KV precision, context length, and network speed jointly determine whether hybrid inference pays off.

**Summary of Discussion:**

1. **Prefill vs. Decode Importance:**  
   - Users debate the practical balance between prefill (compute-heavy) and decode (memory-bound). Some argue that prefill dominates in scenarios with short outputs (e.g., simple queries), while decode matters more for tasks requiring long token generation.  
   - Medium-sized prompts (~1k tokens) on smaller models (8B-20B) reportedly suffer slowdowns on Apple Silicon (e.g., M1), emphasizing prefill’s role in latency.

2. **Hardware Limitations & Model Compatibility:**  
   - Concerns arise about DGX Spark’s 128GB memory cap limiting larger models. A reply notes upcoming EXO 1.0 support for models like DeepSeek-R1 (up to 128GB) and plans to open-source the framework.  
   - Questions about MoE (Mixture of Experts) model compatibility and layer distribution efficiency remain unresolved.

3. **Networking & Cost Concerns:**  
   - USB-C/Thunderbolt networking between DGX Spark and Mac Studio is discussed, with users confirming USB4/Thunderbolt 3 compatibility for high bandwidth.  
   - The $30k+ cost of DGX hardware sparks jokes about affordability, though some suggest alternatives like RTX Pro 6000 GPUs for cost-conscious setups.

4. **Project Availability & Use Cases:**  
   - Some express disappointment that EXO is currently private, though the team hints at future open-sourcing.  
   - Potential applications beyond LLMs (e.g., Stable Diffusion) are noted, with DGX Spark’s compute benefits highlighted for other AI workloads.

**Key Takeaways:**  
The discussion reflects enthusiasm for hybrid inference architectures but highlights practical hurdles like hardware costs, model compatibility, and real-world prompt dynamics. Community interest in open-sourcing EXO and leveraging Apple’s upcoming M5 hardware (claimed 35x TTFT gains) suggests ongoing optimization trends in edge AI.

### TaxCalcBench: Evaluating Frontier Models on the Tax Calculation Task

#### [Submission URL](https://arxiv.org/abs/2507.16126) | 68 points | by [handfuloflight](https://news.ycombinator.com/user?id=handfuloflight) | [23 comments](https://news.ycombinator.com/item?id=45601230)

TaxCalcBench: Can AI file your taxes? Not yet. A new benchmark tests frontier LLMs on calculating US personal income tax returns when given all the needed facts. Even on a simplified set, state-of-the-art models correctly compute fewer than one-third of federal returns. Common failure modes: misusing tax tables, arithmetic mistakes, and misjudging eligibility. The authors conclude that LLMs need additional tooling/infrastructure to be reliable for tax prep—plain language modeling isn’t enough for rule-heavy, tabular, multi-step calculations. (arXiv:2507.16126)

**Summary of Hacker News Discussion:**

The discussion revolves around the challenges and limitations of using LLMs (like Claude, GPT, etc.) for tax preparation, as highlighted in the TaxCalcBench study. Key points include:

1. **Mixed Practical Experiences**:  
   - Some users shared attempts to automate personal finance/tax tasks with LLMs (e.g., Claude Code), achieving partial success in exploratory data analysis but facing issues like misclassified transactions. Manual verification was still required, underscoring the need for better tooling.  
   - Skepticism persisted about LLMs’ reliability for direct financial decisions without rigorous testing or safeguards.

2. **Common Failure Modes**:  
   - Errors in arithmetic, tax table usage, and eligibility judgments were noted as persistent issues. Even advanced models like ChatGPT produced wildly incorrect tax numbers despite claiming confidence.  
   - Participants emphasized that tax calculation involves narrow legal definitions and multi-step logic, which LLMs struggle to navigate without structured data (e.g., CSV/TSV) or domain-specific scaffolding.

3. **Potential Solutions**:  
   - Suggestions included integrating LLMs with calculators, task-specific guardrails, or retrieval-augmented generation (RAG) to access tax code references.  
   - The authors clarified that their benchmark tested models’ ability to interpret tax forms (e.g., IRS Form 1040) and code, linking to open-source tax tools like Iris for context-aware solutions.

4. **Liability & Trust Concerns**:  
   - Users questioned liability if AI-made errors occurred, highlighting risks in real-world deployment. Many argued humans already struggle with tax complexity, and AI’s current limitations make it unsuitable for unsupervised use.  

5. **Community Reactions**:  
   - Surprise was expressed at the IRS’s GitHub repository of tax questions in XML, hinting at potential structured data sources for future LLM training.  
   - A leaderboard for model performance on TaxCalcBench was shared, with speculation about restrictions in benchmark simplicity (e.g., scores might improve with more constrained scenarios).  

**Conclusion**: While LLMs show promise in automating tax tasks, their current limitations in accuracy, legal nuance, and calculation reliability necessitate hybrid approaches combining AI with traditional software tools and human oversight. The discussion reflects cautious optimism tempered by practical and ethical concerns.

### Coral NPU: A full-stack platform for Edge AI

#### [Submission URL](https://research.google/blog/coral-npu-a-full-stack-platform-for-edge-ai/) | 25 points | by [LER0ever](https://news.ycombinator.com/user?id=LER0ever) | [7 comments](https://news.ycombinator.com/item?id=45600470)

Google unveils Coral NPU, an open-source, full-stack platform to bring always-on, private AI to low-power edge devices like wearables, hearables, AR glasses, and smartwatches. Co-designed with Google Research and DeepMind, it targets three pain points: performance on tiny power budgets, software fragmentation across diverse hardware, and on-device privacy.

What it is
- A reference NPU architecture and toolchain that flips the usual chip priorities: it centers the ML matrix engine and treats scalar compute as supporting cast.
- Built on RISC-V–compliant architectural IP, aimed at ultra‑low power, always‑on ambient sensing and assistive AI.

Key architecture pieces
- Scalar core: lightweight, C-programmable RISC‑V front end using a run-to-completion model for minimal power.
- Vector unit: RVV v1.0 SIMD co-processor for bulk data ops.
- Matrix unit: quantized outer‑product MAC engine purpose-built for neural nets (under development; code coming to GitHub later this year).
- Claimed baseline: ~512 GOPS at just a few milliwatts.

Unified developer experience
- Works with modern compilers (IREE, TensorFlow Lite Micro), MLIR, and supports frameworks like TensorFlow, JAX, and PyTorch.
- Provides a full toolchain: TFLM compiler, general MLIR pipeline, C compiler, custom kernels, and a simulator, aiming to smooth over the “fragmentation tax.”

Why it matters
- If it delivers, Coral NPU could become a common, low-power, RISC‑V–based target for edge AI, standardizing tooling and improving battery life while keeping personal data on-device.
- It’s positioned as a configurable IP block for SoC designers, not just a single chip—potentially accelerating custom silicon for wearables and other constrained devices.

What’s available now (and what’s not)
- Docs and developer tools are out so teams can start prototyping.
- The matrix execution unit—the heart of the accelerator—isn’t released yet; GitHub drop is promised later this year.

**Summary of Hacker News Discussion on Google Coral NPU:**

1. **Comparisons with Existing Hardware:**  
   - Users compared Coral NPU to Synaptics’ SL2610 chip (1 TOPS NPU + ARM cores) and Google’s original Coral TPU (2018-2020, 4 TOPS at ~10mW). The Coral TPU’s lack of hardware-accelerated video encoding (H264/HEVC) was noted as a limitation, unlike competitors like Rockchip’s RK3588 with VPU support.  
   - The new Coral NPU’s focus on ultra-low power and RISC-V/RVV architecture was seen as a step forward, but its matrix unit (key for ML acceleration) is still unreleased.

2. **Concerns About Google’s Track Record:**  
   - Skepticism arose about Google’s long-term commitment, given the original Coral TPU’s apparent phase-out. Users questioned whether Coral NPU might face similar abandonment.  

3. **Technical Discussions:**  
   - Praise for RISC-V/RVV standardization, which avoids proprietary systolic arrays and aligns with developer familiarity.  
   - MLIR-based toolchains and compiler support (IREE, TensorFlow Lite Micro) were highlighted, though some doubted their practicality, citing fragmented ML tooling historically.  

4. **Competitive Landscape:**  
   - Comments noted the complexity of hardware ecosystems, with EDA tools and open-source silicon IP (e.g., GitHub-based projects) competing for developer attention.  

5. **Availability and Limitations:**  
   - Initial tools/docs are released, but the critical matrix unit remains pending (GitHub code promised late 2024).  
   - No built-in video encoding hardware, which may limit use cases compared to alternatives.  

**Key Takeaways:**  
The community sees potential in Coral NPU’s low-power, privacy-focused edge AI vision but remains cautious due to Google’s history and unresolved technical gaps (e.g., missing matrix unit, video encoding). The success hinges on execution, long-term support, and how it stacks against established players in custom silicon.

### Tor browser removing various Firefox AI features

#### [Submission URL](https://blog.torproject.org/new-alpha-release-tor-browser-150a4/) | 333 points | by [HelloUsername](https://news.ycombinator.com/user?id=HelloUsername) | [217 comments](https://news.ycombinator.com/item?id=45605842)

Tor Browser 15.0a4 (last alpha before 15.0 stable): privacy-first cleanup, ESR security fixes

What’s new
- Security base: Rebased to Firefox 140.4.0esr, OpenSSL 3.5.4, NoScript 13.2.1.
- AI removals: Strips Mozilla’s new AI features (e.g., chatbot sidebar) as unauditable/privacy-risky.
- Branding purge: Removes Firefox/Mozilla-specific branding and features (Firefox Home, new History Sidebar replaced with legacy panel).
- Censorship circumvention: “meek-azure” renamed to “meek” with migration handling; Snowflake bridge lines updated.
- UX/polish: Improved dark-theme styling for Tor UI; show full URL protocol on desktop; letterboxing style aligned with Firefox 140 and plays nicely with vertical tabs.
- Fonts/layout: Linux bundles Noto Color Emoji; switches CJK rendering from Noto to Jigmo for better Chinese/Japanese/Korean glyph coverage.
- Security levels: WebAssembly control moved to NoScript—keeps it disabled in Safer/Safest for web content without breaking the built-in PDF reader.
- Fixes: Opensearch respects FPI; RFP contrast/pen-hiding tweaks; PDF images restored; various bugfixes and migrations.

Release status
- RC: Aiming to promote 15.0 to stable in the last week of October.
- QA focus next week; testing tracked in tor-browser#43984 (Desktop) and #43985 (Android).

Reminder: Alpha is for testing only—use stable if you need strong anonymity.

**Summary of Hacker News Discussion on Tor Browser 15.0a4 Release:**

1. **Privacy Concerns with AI Integration:**  
   - Users criticize Mozilla and mainstream browsers (e.g., Firefox, Chrome) for integrating AI features like chatbots and sidebars, citing privacy risks and lack of auditability.  
   - **Waterfox** (a Firefox fork) is praised for removing Mozilla’s AI features and account sync, though some confusion exists about its current capabilities.  
   - Tor’s decision to strip AI features aligns with its privacy-first ethos, avoiding potential exploits by adversaries targeting at-risk users.

2. **Alternative Browsers & Tools:**  
   - **Orion** (privacy-focused, WebKit-based) and **Kagi** (OSS, zero telemetry) gain mentions as alternatives.  
   - Users suggest configuring local AI providers via extensions (e.g., **PageAssist** with Ollama) to avoid cloud dependencies.  
   - **Zen Browser** (Firefox fork) and **Vivaldi** (Chromium-based) are noted for balancing features and privacy.

3. **Criticism of Mozilla’s Priorities:**  
   - Frustration mounts over Firefox’s focus on AI instead of addressing long-standing user requests (e.g., tab group syncing, JPEG XL support, window management).  
   - Some users switch to Chromium-based browsers for better workflow tools, though others stick with Firefox for philosophical reasons.

4. **Technical Feedback on Tor’s Update:**  
   - Positive reception for rebasing to Firefox 140 ESR, OpenSSL updates, and NoScript integration for WebAssembly control.  
   - Appreciation for UI improvements (dark theme, URL protocol visibility) and censorship circumvention tweaks (meek-azure rename).  
   - Reminder that the alpha is for testing; stable versions remain recommended for anonymity.

5. **Miscellaneous:**  
   - Avast Secure Browser is criticized as a "privacy nightmare," contrasting Tor’s efforts.  
   - Side discussions on AI tools like Claude for technical/research assistance and debates over browser sidebar utility vs. bloat.

**Key Takeaway:** The discussion underscores a tension between innovation (AI features) and privacy/functionality, with Tor’s update praised for prioritizing security while users seek alternatives to mainstream browsers’ increasingly complex ecosystems.

### Who's Submitting AI-Tainted Filings in Court?

#### [Submission URL](https://cyberlaw.stanford.edu/whos-submitting-ai-tainted-filings-in-court/) | 79 points | by [cratermoon](https://news.ycombinator.com/user?id=cratermoon) | [59 comments](https://news.ycombinator.com/item?id=45600263)

Who’s submitting AI-tainted court filings? A Stanford researcher dug into a global database of “AI hallucination” incidents and analyzed 114 U.S. cases from June 2023 (post–Mata v. Avianca) through Oct 7, 2025. Key takeaways:

- It’s mostly small shops: 90% of implicated firms were solo practitioners or small firms.
- Plaintiffs more than defendants: 56% of cases were attributed to plaintiff’s counsel, 31% to defense, 13% were “other” (e.g., bankruptcy, family, probate, tax, agency, habeas, disciplinary).
- ChatGPT shows up a lot: Among matters where a tool was specified, about half cited some version of ChatGPT.
- Government lawyers were rare in the sample; there were no U.S. prosecutor cases in the time window.
- The problem persists despite court standing orders on AI, ethics opinions, and CLEs warning about AI use in legal practice.

Method notes and caveats:
- Source: Damien Charlotin’s AI Hallucination Cases database; author restricted to U.S., excluded pro se matters, and verified parties and firm affiliations via orders and dockets.
- Firm-size bands followed NALP conventions with solos split out; some classifications required best guesses from websites and directories.
- The dataset is moving fast; the author notes it was outdated within days as new matters were added. Treat results as indicative, not comprehensive.

Why it matters:
- The skew toward solo and small firms suggests resource and workflow gaps—less access to vetted research tools, fewer review layers, and greater temptation to lean on general-purpose LLMs.
- Courts are increasingly scrutinizing filings, and lawyers face reputational and potential sanctions risk if they don’t verify citations.
- For tool builders: audit trails, authoritative citation verification, and guardrails against fabricated authorities remain must-haves for legal workflows.

**Summary of Hacker News Discussion:**

1. **Surprise at Lawyers’ Oversight**:  
   Commenters expressed disbelief that legal professionals often fail to rigorously verify AI-generated content, relying on unreliable sources like Google, blogs, or general LLMs (e.g., ChatGPT) instead of authoritative databases like Westlaw. This aligns with the study’s finding that hallucinations stem from workflow gaps in smaller firms.

2. **Small Firms & Resource Constraints**:  
   Many agreed that solo/small firms lack resources (e.g., vetting tools, peer review) to catch errors, increasing hallucination risks. However, critiques emerged about the study’s methodology:  
   - Without comparing against the broader lawyer population (e.g., small firms dominate the legal market), claims of overrepresentation might be misleading.  
   - Statistical context was noted: In some jurisdictions, 50%+ of legal work is handled by firms with ≤50 lawyers, complicating causality between firm size and error rates.

3. **AI Limitations & Verification Challenges**:  
   - LLMs like ChatGPT are seen as “early-stage” tools requiring heavy human oversight. Skepticism exists about their ability to collate data accurately without guardrails (e.g., citation verification).  
   - Technical debates arose about AI’s role in solving NP-hard problems (e.g., verifying legal citations), though these were tangential to the core issue.

4. **Professional Standards & Accountability**:  
   - Concerns were raised about lower-performing law graduates or overworked lawyers cutting corners. Some argued the bar exam and professional certifications inadequately enforce rigor.  
   - Anecdotes highlighted real-world consequences: One user described a lawyer using Claude (an AI) who faced judicial criticism for flawed filings.

5. **Critique of Government Sources**:  
   Even government-published legal guidance was noted to sometimes contain subtle errors, exacerbating reliance on flawed sources. This underscores the need for authoritative, up-to-date materials in legal workflows.

**Key Takeaways**:  
While the study’s findings resonated (small firms, ChatGPT prevalence), commenters emphasized methodological caveats and broader systemic issues (e.g., resource disparities, professional accountability). Calls for AI tools with audit trails, better citation checks, and ethical guardrails were echoed, alongside skepticism about current AI reliability in high-stakes legal contexts.

