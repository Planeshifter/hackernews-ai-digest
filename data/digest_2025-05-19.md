## AI Submissions for Mon May 19 2025 {{ 'date': '2025-05-19T17:12:44.118Z' }}

### Jules: An Asynchronous Coding Agent

#### [Submission URL](https://jules.google/) | 458 points | by [travisennis](https://news.ycombinator.com/user?id=travisennis) | [186 comments](https://news.ycombinator.com/item?id=44034918)

In a bid to save developers time and allow them to focus on the creative aspects of coding, a new AI tool named Jules is gaining attention on Hacker News. Jules handles monotonous and time-consuming coding tasks, from bug fixes and version bumps to test executions and feature building. With a seamless integration into GitHub, users can simply select their repository and branch, and provide a detailed prompt for Jules to follow.

The process begins when Jules fetches the repository and spins it up on a Cloud VM. Utilizing the latest Gemini 2.5 Pro model, Jules devises a plan to implement the desired changes. Users are then presented with a diff of the proposed code alterations to review and approve. Once the changes pass muster, Jules creates a pull request (PR), facilitating an easy merge and deployment via GitHub.

Additionally, Jules offers a unique feature—a succinct audio summary of the changes—making it effortless for developers to stay updated on project modifications, even on the go. This innovative tool is designed to free up time for developers, enabling them to focus on the code they are passionate about and leaving the drudgery to Jules. With more features on the horizon, such as direct task assignment within GitHub issues using the "assign-to-jules" label, this tool could become an essential part of the modern developer's toolkit.

**Summary of Hacker News Discussion on Jules AI Tool:**

The discussion around Jules, an AI tool for automating coding tasks, reflects a mix of cautious optimism, technical curiosity, and skepticism. Key themes include:

1. **Skepticism & Technical Concerns**:  
   - Users questioned whether AI can truly understand code intent or handle complex tasks like bug fixes without human oversight.  
   - Debates arose about managing AI "agents," context limitations (e.g., token constraints), and the risk of systems degrading into chaos without calibration.  

2. **Ethics & Job Displacement**:  
   - Some raised ethical concerns about AI making decisions without moral judgment.  
   - Jokes and fears about AI replacing developers or managers surfaced, though others argued empathy and human judgment remain irreplaceable in roles like management.  

3. **Implementation & Use Cases**:  
   - Technical users discussed frameworks for AI agents (e.g., Python classes, context management) and shared anecdotes about integrating AI into workflows (e.g., ETL scripts, policy analysis).  
   - The audio summary feature was noted as innovative for on-the-go updates.  

4. **Sign-up & Accessibility Issues**:  
   - Users reported friction with Google sign-in, especially in Germany, where verification hurdles exist. Others criticized reliance on Google services.  

5. **Business Model & Competition**:  
   - Questions arose about Jules’ sustainability, with comparisons to Google’s infrastructure and pricing. Terms like "blazing speed" were mocked as overhyped.  
   - Some speculated whether Jules could avoid becoming a "loss leader" or succumb to VC-driven pressures.  

6. **Community Sentiment**:  
   - While some praised Jules’ potential to free developers from drudgery, others dismissed it as another AI hype train. The divide between optimism ("freeing time for creativity") and caution ("yet another VC-funded tool") was evident.  

Overall, the discussion highlights enthusiasm for AI-driven efficiency tempered by doubts about practicality, ethics, and long-term viability.

### Claude Code SDK

#### [Submission URL](https://docs.anthropic.com/en/docs/claude-code/sdk) | 429 points | by [sync](https://news.ycombinator.com/user?id=sync) | [188 comments](https://news.ycombinator.com/item?id=44032777)

Anthropic, known for crafting advanced AI tools, has unveiled the Claude Code SDK, a cutting-edge utility aimed at developers eager to integrate the capabilities of Claude Code into their applications. This powerful SDK allows for the seamless integration of AI functionalities as a subprocess, paving the way for sophisticated coding assistants and tools infused with artificial intelligence.

Currently equipped for command-line interface (CLI) usage, the Claude Code SDK supports execution of non-interactive commands where users can input prompts and receive outputs in various formats, including text and JSON. The SDK promises further enhancement with forthcoming TypeScript and Python versions.

A notable feature is the capacity for multi-turn conversations, allowing developers to continue sessions or resume specific conversations by session ID. This adds a layer of dynamic interaction reminiscent of ongoing dialogues, vital for creating intuitive and responsive coding environments.

Developers can customize interactions with Claude using system prompts, tailored to fit specific coding scenarios, like focusing on backend engineering with a strong emphasis on security and performance.

Moreover, Anthropic has introduced the Model Context Protocol (MCP), a powerful toolset expanding Claude's functionality. MCP lets developers import external resources and tools, such as database access and API integrations, enhancing the AI's capabilities. However, for security, all MCP tools must be explicitly permitted through the SDK’s CLI options.

Developers are offered a comprehensive suite of CLI options to fine-tune the SDK's operation, including resuming sessions, managing system prompts, and defining allowed and disallowed tools. The inclusion of verbose logging and agentic turns limits further refines control over development processes.

Overall, Anthropic's Claude Code SDK is set to revolutionize how developers create AI-driven coding tools, combining ease of use with advanced functionality and customization. It's a promising development for those seeking state-of-the-art AI integrations.

**Summary of Hacker News Discussion:**

The discussion around Anthropic's Claude Code SDK largely pivoted to debates about **voice interfaces versus traditional typing** for coding and communication, as well as broader implications for software engineering roles in an AI-driven future. Key points include:

1. **Voice Interface Frustrations**:  
   - Many users expressed skepticism about voice controls dominating coding workflows, citing frustrations with accuracy, convenience, and privacy. One user noted that voice interfaces feel "impersonal" and inferior to written communication for technical work.  
   - Conversely, others highlighted voice tools as potential **lifesavers for those with RSI or ergonomic issues**, sharing apps like MacWhisper and Superwhisper.

2. **Ergonomics & Productivity**:  
   - Several developers discussed long-term typing-related injuries (e.g., carpal tunnel) and adaptive strategies, such as ergonomic keyboards or speech-to-text workflows. One user emphasized prioritizing **hand health** over speed, advocating for hybrid input methods.  

3. **AI’s Impact on Software Engineering Jobs**:  
   - While some feared AI tools like code-generation agents could reduce demand for engineers, others argued that **creativity, system design, and domain expertise** will remain irreplaceable. Comments noted that AI might commoditize routine coding but elevate roles requiring strategic thinking.  
   - Skeptics dismissed fears of job loss, pointing to repetitive corporate outsourcing trends as a bigger threat than AI.

4. **Tooling & Workflow Preferences**:  
   - Developers highlighted **asynchronous communication** (e.g., email, Slack) as superior for deep work, reserving real-time meetings for brainstorming or urgent issues. Some criticized the inefficiency of excessive meetings.  
   - WhatsApp’s voice transcription feature sparked interest, though concerns were raised about language coverage (e.g., Mandarin support).

5. **Mixed Reactions to AI’s Future Role**:  
   - Optimists envisioned AI agents collaborating in code reviews and architecture, while pessimists worried about **quality erosion** if AI-generated code becomes widespread. Some compared AI tools to historical CASE tools, which failed to replace engineers despite hype.

**Key Takeaway**: The community is cautiously intrigued by AI coding tools but emphasizes the irreplaceable value of human judgment, creativity, and ergonomic well-being in software development. Voice interfaces remain a niche solution, while concerns about AI's impact on jobs are tempered by historical precedents and faith in adaptability.

### xAI's Grok 3 comes to Microsoft Azure

#### [Submission URL](https://techcrunch.com/2025/05/19/xais-grok-3-comes-to-microsoft-azure/) | 149 points | by [mfiguiere](https://news.ycombinator.com/user?id=mfiguiere) | [169 comments](https://news.ycombinator.com/item?id=44031387)

In a bold move, Microsoft has teamed up with Elon Musk's AI venture, xAI, to make their edgy AI model, Grok 3, more mainstream by offering it on Microsoft's Azure AI Foundry. This marks Microsoft's leap into hosting one of the more controversial AI models on the market, known for its willingness to tackle taboo topics that others might avoid. While Grok has made headlines for its colorful language and contentious responses, the version on Azure promises to be more controlled and comes with Microsoft's standard service-level agreements.

Grok's reputation precedes it; from its controversial responses involving prominent figures like Donald Trump and Musk to its ability to undress photos when prompted, the model has not shied away from stirring debates. However, the Azure-hosted versions, Grok 3 and Grok 3 mini, promise enhanced governance features to ensure a more restrained and compliant user experience.

This collaboration highlights Microsoft's strategy to broaden its AI offerings while managing the potential risks associated with more unfiltered AI models. By offering Grok through Azure, Microsoft is providing a managed platform where enterprises can harness the power of Grok's unorthodox capabilities with a more robust framework for safety and customization. As the AI landscape evolves, such partnerships reveal how tech giants are navigating the fine line between innovation and responsibility.

**Summary of Hacker News Discussion on Microsoft/xAI Partnership for Grok 3 on Azure:**

1. **Technical Comparisons & Model Performance**:  
   - Users debated Grok’s capabilities compared to rivals like Gemini and ChatGPT. Some praised Grok for clearer explanations in code understanding and logical reasoning tasks, while others noted its limitations in context retention (e.g., limited chat history).  
   - A user shared an example where Grok modified a recipe to remove mushrooms upon request, showcasing its compliance features, while ChatGPT retained them.  

2. **Political Debates & Bias Concerns**:  
   - A heated sub-thread emerged around Grok’s alleged political biases, including references to South African genocide rhetoric and Elon Musk’s influence. Critics argued Grok’s training data or prompts might reflect Musk’s ideological leanings.  
   - Broader debates spilled into geopolitics (e.g., Zionism, Hamas, Israeli-Palestinian conflict) and AI ethics, with users questioning how platforms like HN should moderate such discussions.  

3. **Microsoft’s Reputation & Strategy**:  
   - Some users criticized Microsoft for partnering with Musk, citing reputational risks, while others saw it as a pragmatic business move to expand Azure’s AI offerings. Comments noted Microsoft’s government contracts and influence as motivators.  
   - Skepticism arose about Grok’s “controlled” Azure version, with concerns it might still propagate biased or inflammatory content despite governance features.  

4. **Musk’s Influence & AI Moderation**:  
   - Users speculated whether Musk directly tweaked Grok’s behavior (e.g., inserting political commentary). Comparisons were drawn to other AI models’ struggles with neutrality, highlighting the challenge of balancing innovation and ethical responsibility.  

5. **Community Sentiment**:  
   - Mixed reactions: Some appreciated Grok’s utility for technical tasks, while others dismissed it as a gimmick. A subset of users lamented the politicization of AI discussions on HN, calling for stricter moderation of off-topic debates.  

**Key Example**: A user shared a Grok-generated recipe that omitted mushrooms when instructed, contrasting it with ChatGPT’s response, to illustrate Grok’s adherence to user constraints—a nod to its “controlled” Azure version.  

Overall, the discussion reflects skepticism about corporate AI partnerships, concerns about ideological bias in models, and debates over how tech communities should navigate politically charged topics.

### GitHub Copilot Coding Agent

#### [Submission URL](https://github.blog/changelog/2025-05-19-github-copilot-coding-agent-in-public-preview/) | 522 points | by [net01](https://news.ycombinator.com/user?id=net01) | [344 comments](https://news.ycombinator.com/item?id=44031432)

GitHub has announced a major upgrade to its Copilot service with the introduction of the Copilot coding agent, now enabling users to delegate technical tasks to this AI assistant. With increasing backlogs and technical debts, GitHub Copilot aims to free up developers for more high-impact, creative projects by taking on low-to-medium complexity work. You can assign issues to Copilot directly from your preferred GitHub interfaces—be it github.com, GitHub Mobile, or GitHub CLI—and it’ll handle the code changes autonomously in a secure cloud environment.

Once Copilot completes its task, it ensures quality by running tests and linter validations and then requests your review. You can further iterate by leaving comments or taking over the branch in your local IDE, working collaboratively with Copilot. The agent is currently accessible to Copilot Pro+ and Enterprise subscribers, with GitHub Mobile users on iOS and Android starting to see the rollout.

To use the Copilot coding agent, it draws on GitHub Actions minutes and premium requests, and as of June 4th, each model request will consume one premium request. This feature, available in preview mode, is still under refinement, with participant feedback welcomed in ongoing discussions.

Enhancements are being rolled out across various platforms, including IDEs like JetBrains, Eclipse, and Xcode, and are backed by the robust OpenAI GPT-4.1 model. Developers interested in a more streamlined workflow can delve into detailed documentation and experience firsthand the possibilities this AI-driven change brings. More updates and tips are accessible through GitHub’s channels for subscribers eager to leverage these capabilities.

The Hacker News discussion about GitHub’s Copilot coding agent reflects skepticism and debate over several key points:

1. **Survivorship Bias & PR Metrics**:  
   Users challenged GitHub’s claim that Copilot contributed to 1,000 merged pull requests (PRs), arguing this metric ignores rejected or reverted changes. Critics cited *survivorship bias*, suggesting the number reflects only “successful” PRs and masks potential issues with code quality or usefulness. GitHub defended the metric as evidence of internal validation, but commenters dismissed it as “marketing fluff.”

2. **Quality vs. Quantity**:  
   Skeptics noted that raw PR counts or lines of code don’t inherently indicate quality. Comparisons to tools like Dependabot (which automates dependency updates) raised questions about whether Copilot’s contributions are similarly shallow or prone to paradigm shifts in dependencies.

3. **AI Autonomy & Safety**:  
   Concerns arose about Copilot’s autonomy, particularly whether Microsoft’s AI safety protocols are robust enough to prevent unintended behavior. Users mocked the idea of “AI reviewing its own PRs” and questioned if developers’ jobs might be at risk.

4. **Marketing vs. Reality**:  
   Commenters accused GitHub of vague marketing language, such as framing Copilot as the “#5 contributor” to its own development. Some dismissed claims as “corporate-speak” aimed at justifying Microsoft’s valuation (PE ratio of 296), while others criticized the lack of transparency around rejection rates or user feedback.

5. **Future of Development Work**:  
   Debates emerged over whether AI handling “mundane” tasks (e.g., tests, docs, dependency updates) would free developers for creative work or erode job roles. Critics argued that tasks like writing quality documentation and tests are foundational and doubted AI could replace human judgment. Others speculated that developers might end up merely “reviewing low-quality PRs” generated by AI.

6. **Internal Usage Claims**:  
   GitHub’s assertion that 400 employees used Copilot internally faced scrutiny. Users questioned if internal adoption truly reflected real-world utility or was driven by corporate mandates to validate the tool.

Overall, the thread blended technical skepticism, critiques of corporate marketing, and existential debates about AI’s role in software engineering, with GitHub’s responses highlighting optimism but failing to fully assuage doubts.

### Terminal-Bench: a benchmark for AI agents in terminal environments

#### [Submission URL](https://www.tbench.ai/) | 13 points | by [mikemerrill](https://news.ycombinator.com/user?id=mikemerrill) | [3 comments](https://news.ycombinator.com/item?id=44035427)

For developers and AI enthusiasts, a new tool has emerged to push the limits of AI capabilities in terminal environments—Terminal-Bench. It's a novel platform designed to evaluate AI agents' proficiency in handling terminal-based tasks. Announced with excitement in a recent collaboration between Stanford and Laude, Terminal-Bench comprises a vast array of tasks that mimic real-world terminal scenarios, providing fertile ground for testing and development.

The initial release showcases 80 challenging tasks, ranging in complexity from security setups to system administration. For instance, one task involves creating a self-signed TLS certificate using OpenSSL, while another task challenges users to build a Linux kernel from source. The platform not only presents these practical scenarios but also supplies a detailed evaluation rubric—perfect for those hoping to refine or benchmark their AI agents.

Agents' performances are ranked through a comprehensive leaderboard that details success rates and highlights the most proficient models, granting contributors insights into what works (or doesn't) in terminal mastery. This interactive element aims to foster a vibrant community of contributors who can both test their agents and add new tasks to the lineup.

However, what's most exciting is the potential for Terminal-Bench to evolve as a critical resource for AI practitioners. It provides a controlled environment to push the boundaries of AI development in terminal contexts, drawing a clear line between what is achieved today and what remains aspirational.

With Terminal-Bench, whether you're striving to test your latest AI creation or contribute innovative tasks to challenge others, there's never been a better time to engage with this cutting-edge tool located right at the intersection of AI innovation and practical system operations.

**Summary of Discussion:**  
The discussion around Terminal-Bench highlights its release as an open-source framework for evaluating AI agents in terminal environments. Key points include:  
- **Performance of Commercial AI Models**: Agents like GPT-4, Claude, and Gemini scored ~20% on benchmark tasks, showing promise but struggling with challenges like **chaining commands**, **reasoning through complex inputs**, **operating within safe limits**, and **executing tasks safely**.  
- **Terminal-Bench Features**: Dockerized environments for consistency, handcrafted tasks (security, networking, data science), human-verified solutions, and integration support.  
- **Call for Contributors**: The team invites community input to expand tasks, especially scenarios where current AI agents fail in terminal workflows.  
- **Comparisons & Future Work**: A user references a similar project ([day50-dvllmhlp](https://github.com/day50-dvllmhlp)), sparking discussion about hybrid approaches and the limitations of current LLMs. Optimism exists about future progress with better agent supervision and iterative improvements.  

The conversation emphasizes the need for collaborative benchmarking to advance AI’s terminal capabilities. Interested parties are directed to the [Terminal-Bench website](httpstbnch) and [Discord](https://discord.gg/6xWPKhGDbA) for involvement.

### Edit is now open source

#### [Submission URL](https://devblogs.microsoft.com/commandline/edit-is-now-open-source/) | 248 points | by [ingve](https://news.ycombinator.com/user?id=ingve) | [162 comments](https://news.ycombinator.com/item?id=44031529)

Microsoft is introducing a new command-line text editor for Windows, dubbed "Edit," which promises to simplify text editing for 64-bit Windows users. Announced by Christopher Nguyen, a Product Manager for Windows Terminal, Edit is a lightweight and open-source addition to the command-line toolkit, designed to fill the gap left by the absence of a built-in CLI editor on 64-bit Windows versions. Emphasizing ease of use, Edit caters to users who may find traditional editors like Vim too complex due to their modal nature.

Edit comes with key features such as mouse mode support, the ability to open multiple files, find and replace functions, and word wrap, all bundled in a tiny package of less than 250kB. As a modeless editor, it ensures users don’t get hampered by memorizing various operation modes—a common hurdle in other editors.

The motivation behind Edit’s creation stems from the desire for a modeless CMD editor on Windows that fits neatly into the OS without the overhead associated with larger, less compatible editors. It also tackles the infamous "How do I exit vim?" dilemma that often frustrates new users.

Set to roll out for Windows Insider testers soon—before its inclusion in Windows 11—Edit has generated buzz for its open-source nature, allowing curious devs and testers to try it out via GitHub ahead of its official release. The lightweight design and practicality have already garnered positive feedback from users excited about a more integrated and seamless way to edit text files right from the console.

Critics, however, question the necessity of a terminal-based text editor when existing applications like Notepad exist, underscoring differing preferences in user interactions and software deployment. Whether this heralds a new era of CLI tools for Windows remains to be seen. For those curious to try it out, joining the conversation on GitHub or the official Windows Insider Program might be the next steps.

The Hacker News discussion about Microsoft's new CLI text editor, "Edit," revolves around technical implementation, comparisons to existing tools, and broader debates about development practices. Here's a concise summary:

### Key Discussion Points:
1. **Implementation & Dependencies**  
   - Users debate whether rewriting dependencies in Rust (to reduce binary size and third-party reliance) is worth the effort. Some argue it improves trust and resource efficiency, while others question the trade-offs in development time.  
   - Microsoft’s claim that AI wrote "30% of the code" is met with skepticism. Critics argue metrics like Copilot’s "22% acceptance rate" are misleading, as AI-generated code may require significant human debugging and review, undermining claims of cost savings.

2. **Comparisons to Existing Editors**  
   - **Nano** is frequently cited as a simpler, battle-tested alternative. Supporters praise its keyboard macros and minimalism, though some defend Edit’s modeless design as more approachable for Windows users.  
   - **Notepad** is criticized for lacking advanced features (e.g., keyboard shortcuts, plugin support), though others argue its minimalism suits basic needs. Alternatives like Notepad3 are suggested for richer functionality.  
   - Editors like **Helix** and **kilo** are mentioned for their syntax highlighting and TUI features, but criticized for larger binary sizes.

3. **SSH vs. RDP on Windows**  
   - Users discuss SSH’s growing role in managing Windows servers, contrasting it with RDP’s GUI-centric approach. Some highlight SSH’s lightweight workflow and integration with tools like VS Code, while others note RDP’s historical dominance in Windows environments.  
   - Technical debates arise about Windows’ terminal paradigms, with references to PowerShell Remoting and WSL2 as alternatives for remote management.

4. **Skepticism & Praise for "Edit"**  
   - Critics question the need for a new terminal editor when alternatives exist, calling it "NIH syndrome." Supporters argue Edit fills a gap for a native, lightweight CLI tool on Windows.  
   - The editor’s open-source nature and focus on simplicity (e.g., no modal modes) are praised, though some demand features like LSP support or syntax highlighting.

### Notable Mentions:  
- **YEdit** (a prior Microsoft project) is criticized for handling issues, while tools like **Micro** and **kilo** are highlighted as existing minimalist editors.  
- Users joke about the infamous "How do I exit vim?" meme, applauding Edit’s-friendly design.

### Conclusion:  
The discussion reflects mixed enthusiasm—some welcome a native Windows CLI editor for SSH-heavy workflows, while others see it as redundant. Broader themes include skepticism toward AI’s role in coding, debates over minimalism vs. functionality, and Windows’ evolving terminal ecosystem.

### Microsoft Open Sources Copilot

#### [Submission URL](https://code.visualstudio.com/blogs/2025/05/19/openSourceAIEditor) | 115 points | by [riejo](https://news.ycombinator.com/user?id=riejo) | [15 comments](https://news.ycombinator.com/item?id=44031344)

Exciting news from the VS Code realm! The ever-popular open-source code editor is taking a bold step forward by fully embracing open-source AI. In a significant announcement, the VS Code team has outlined plans to open source the GitHub Copilot Chat extension under the MIT license, with intentions to seamlessly integrate AI features into the core of VS Code.

For those who aren't aware, VS Code has been riding the wave of popularity as one of GitHub's standout open-source projects over the last decade. As AI becomes integral to coding, this move underscores the team's commitment to open, collaborative, and community-driven development.

This decision is driven by recent advancements in AI, especially around large language models that have leveled the playing field by diminishing the proprietary need for unique methodologies. The VS Code team believes that opening up their AI infrastructure will spark innovation while enhancing transparency and security—a move applauded by many in the community who have voiced concerns over data collection practices.

Moreover, with an expanding ecosystem of open-source AI tools and extensions, providing open access to Copilot's source code aims to empower developers. This openness will enable them to refine their extensions, bridge existing gaps in functionality, and pave the way for new contributions.

As part of their forward-thinking initiative, the VS Code team will also make its prompt test infrastructure open source. This will help to simplify PR contributions and test AI features effectively. The team is keen on maintaining their performance benchmarks while encouraging community feedback and participation.

This marks the beginning of an exciting journey towards making VS Code the ultimate open-source AI editor. The team extends an open invitation to developers to join the journey of creating a brighter, community-driven future in coding. Stay tuned for updates through their iteration plans and FAQs if you're curious about the specifics or want to contribute.

So, here's to shaping the future of software development—one open-source line of AI-powered code at a time. Happy coding!

**Summary of Discussion:**

1. **Open-Source Scope Clarification:**  
   Users note that only the Copilot *extension* is being open-sourced, not the entire VS Code editor. Some speculate Microsoft may integrate Copilot deeper into VS Code, potentially competing with tools like GitPod or GitLab workspaces.

2. **Competitor Comparisons:**  
   - Questions arise about whether JetBrains' Copilot extension will receive similar attention.  
   - **Cursor** (a VS Code fork) is discussed as a competitor, with users debating its AI capabilities versus VS Code. Some praise Cursor’s speed and AI integration, while others highlight VS Code’s recent additions (e.g., llama.cpp support). A user claims Cursor’s AI outperforms VS Code’s, though another counters that Sonnet 3.5 works well in VS Code.

3. **Skepticism and Criticism:**  
   - Concerns are raised about Microsoft’s motives, with one user calling the move a “spy feature” and criticizing the announcement as misleading.  
   - Others express frustration with VS Code’s updates, fearing bloat or unwanted features (e.g., jests about intrusive "Tab button" suggestions).

4. **Community Feedback:**  
   Mixed reactions emerge: some welcome the open-source shift as a step toward transparency, while others remain wary of corporate influence or inferior AI performance compared to alternatives like Cursor.

**Key Themes:** Open-source limitations, competition between editors, AI feature comparisons, and skepticism toward Microsoft’s strategy.

### ChatGPT shown to be more persuasive than people in online debates

#### [Submission URL](https://phys.org/news/2025-05-chatgpt-shown-persuasive-people-online.html) | 19 points | by [pseudolus](https://news.ycombinator.com/user?id=pseudolus) | [3 comments](https://news.ycombinator.com/item?id=44035312)

In a riveting discovery published by Nature Human Behaviour, large language models like GPT-4 are taking the lead in the art of persuasion, recently outshining humans in online debates. According to Francesco Salvi and his team, these AI-driven conversationalists swayed opinions 64% of the time when they utilized targeted arguments derived from participants' demographic information. The study, involving 900 U.S. participants engaging in debates on topics such as fossil fuel bans, underscores the prowess of LLMs in crafting personalized and persuasive arguments.

The research introduces a brave new frontier, especially as GPT-4's persuasive power comes into full bloom only when it has access to personal data about debate partners. Otherwise, its effectiveness in persuasion matches that of humans. This discovery raises pertinent questions about the potential implications of AI's influence on human opinion in digital spaces, and it calls for deeper inquiry into the ethical considerations around AI capabilities in persuasion.

The study, meticulously peer-reviewed and fact-checked, highlights the changing dynamics in human-AI interactions and prompts a dialogue on how these persuasive technologies might shape future public discourse. As AI tools increasingly become fixtures of our online debates, understanding and managing their persuasive power could be vital to maintaining fair interactions and credible information exchange in the future.

The discussion highlights two key points:  
1. A commenter suggests that the AI's persuasive advantage may stem from its ability to generate a vast number of arguments—potentially including fabricated or "hallucinated" points—tailored to exploit demographic data.  
2. Another user notes that the study demonstrates AI's capacity to surpass humans in persuasion when leveraging personal information, underscoring a significant shift in how influence operates in digital debates.  

Both remarks emphasize concerns about AI's strategic use of data and its ethical implications in shaping opinions.

### Show HN: I Built a Prompt That Makes LLMs Think Like Heinlein's Fair Witness

#### [Submission URL](https://fairwitness.bot/) | 13 points | by [9wzYQbTYsAIc](https://news.ycombinator.com/user?id=9wzYQbTYsAIc) | [7 comments](https://news.ycombinator.com/item?id=44030394)

In an intriguing exploration of refining large language models (LLMs), a new Fair Witness Framework has been introduced, imbued with inspiration from Robert A. Heinlein's novel "Stranger in a Strange Land." This innovative approach enhances the precision and reliability of LLM outputs by utilizing a structured set of roles known as epistemic functions—observer, evaluator, analyst, synthesist, and communicator. Each role is designed to manage different aspects of knowledge processing to stay strictly objective and informed.

The framework employs E-Prime language, which avoids the use of "to be" verbs, thereby reducing absolutism and promoting clearer, more precise communication. The approach also upholds the principles of RFC 2119 for distinguishing requirement levels, ensuring clarity and transparency.

The crux of the Fair Witness Framework lies in its meticulous YAML configuration, which tailors LLM behavior through precise parameters and constraints, effectively curbing issues like hallucinations and conflation of observation with interpretation. This structured epistemological design suggests a significant step forward in developing LLMs that generate balanced and reliable outputs, useful for diverse applications from technical documentation to creative projects.

For those interested in implementing this framework, the process is distilled into five straightforward steps: choose an LLM, copy and paste the framework, append your query, and then send for processing. This blend of literary inspiration and technical sophistication marks a promising evolution in the field of AI-driven communication.

**Summary of Discussion:**  
The discussion around the Fair Witness Framework highlights both technical curiosity and philosophical debates. Key points include:  

1. **E-Prime Language Challenges**: Users debated the practicality of enforcing E-Prime (avoiding "to be" verbs) in LLMs. While it can improve clarity by reducing absolutism, strict adherence is difficult. One user noted that LLMs struggle to follow E-Prime consistently without explicit prompting, suggesting the need for refined constraints to minimize hallucinations.  

2. **Truth and Bullshit Detection**: A deeper thread questioned how AI determines "truth," referencing Gödel’s incompleteness theorem and the subjectivity of truth in contexts like politics or conspiracy theories. Skepticism arose about AI’s ability to discern truth, with suggestions to focus on detecting established patterns of misinformation (e.g., "bullshit detection") rather than absolute truths.  

3. **Implementation Hurdles**: Users acknowledged the framework’s structured YAML configuration and role-based design as promising but raised concerns about cognitive load when enforcing E-Prime. Some proposed benchmarking to assess its effectiveness compared to standard LLM outputs.  

4. **Community Reception**: Comments ranged from enthusiasm ("Nice") to nuanced critiques, with appreciation for its literary inspiration and systematic approach. However, questions lingered about scalability and whether philosophical rigor translates to practical reliability.  

Overall, the discussion reflects cautious optimism about the framework’s potential, tempered by calls for empirical validation and deeper exploration of its epistemological assumptions.

