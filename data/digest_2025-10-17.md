## AI Submissions for Fri Oct 17 2025 {{ 'date': '2025-10-17T17:12:44.799Z' }}

### Andrej Karpathy – It will take a decade to work through the issues with agents

#### [Submission URL](https://www.dwarkesh.com/p/andrej-karpathy) | 990 points | by [ctoth](https://news.ycombinator.com/user?id=ctoth) | [875 comments](https://news.ycombinator.com/item?id=45619329)

Andrej Karpathy on Dwarkesh: “AGI is still a decade away” and why this is the decade of agents, not the year

Key takeaways
- Decade of agents: Karpathy pushes back on hype that “this is the year of agents.” To be genuinely useful as intern/employee-level assistants, agents still need major upgrades: reliable computer/tool use, strong multimodality, persistent memory, and continual learning.
- Timelines: “The problems are tractable, but they’re still difficult.” Based on ~15 years in AI, he pegs the path to capable agents and AGI at roughly a decade—not 1 year, not 50.
- Cognitive gaps in LLMs: Today’s models are impressive but lack stable memory, on-the-fly learning, and robust reasoning—shortfalls that limit real-world autonomy.
- RL is “terrible” (but best available): Reinforcement learning remains sample-inefficient and finicky, yet alternatives are even less practical for aligning complex behaviors.
- Model collapse: Heavy training on model-generated data can degrade learning dynamics, a key reason LLMs won’t simply scale into human-like learners without fresh, high-signal data.
- Macro impact: He expects AGI’s economic effects to blend into the historical ~2% GDP growth trend rather than trigger a sharp step change overnight.
- Self-driving lessons: The slog to autonomy underscores how edge cases, reliability, and real-world integration dominate late-stage timelines.
- Education’s future: Anticipates agent-tutors and personalized curricula once memory, adaptation, and tool use mature.

Why it matters
- A sober, experience-based forecast from a leading practitioner: rapid progress, but many non-glamorous engineering and data hurdles remain before agents can truly “work.”
- Signals near-term priorities for labs and startups: memory, continual learning, tool use, multimodality, and high-quality data curation.

Listen/watch: YouTube, Apple Podcasts, Spotify (episode posted Oct 17, 2025).

**Summary of Discussion:**

The discussion delves into philosophical and technical debates sparked by Karpathy’s claims about AGI timelines and agent limitations. Key themes include:

1. **LLMs vs. World Models**:  
   - Participants debate whether LLMs truly “understand” the world or merely compress text data. Some reference Rich Sutton and Yann LeCun’s arguments that LLMs lack **grounded world models** (e.g., simulating physics, causality), unlike biological systems (e.g., squirrel brains) that evolve through direct environmental interaction.  
   - Skepticism arises about equating LLMs with AGI, as they lack **persistent memory**, continual learning, and embodied experiences.

2. **Intelligence Metrics**:  
   - Critics argue Sutton’s “squirrel” analogy for AGI is semantically vague and lacks quantitative benchmarks. Others counter that LLMs’ text compression still reflects a form of intelligence, albeit narrow compared to humans’ multimodal, feedback-driven abstraction.

3. **Consciousness and Reductionism**:  
   - A subthread explores whether AI can achieve consciousness. Some analogize human cognition to emergent properties of cells (e.g., photoreceptors processing light), while others stress that LLMs lack **self-preservation, adaptation, or sensory grounding** — key traits of living systems.  
   - Debates touch on reductionism vs. emergent phenomena, with references to Kantian philosophy and the “map vs. territory” problem.

4. **Aesthetics and Training Data**:  
   - The discussion whimsically pivots to why humans find the night sky beautiful. Some attribute this to evolutionary training (e.g., associating clear skies with survival), while others argue beauty transcends utility. Critics note LLMs might mimic such associations but lack genuine subjective experience.

5. **Technical Hurdles**:  
   - Participants align with Karpathy’s emphasis on **memory, tool use, and multimodal integration** as critical gaps. Reinforcement learning’s inefficiency and the risk of “model collapse” from synthetic data are cited as barriers to agent autonomy.

**Why It Matters**:  
The conversation underscores the interdisciplinary challenges of AGI, blending technical critiques (e.g., world modeling, data quality) with philosophical questions about intelligence and consciousness. It reinforces Karpathy’s argument that agents require foundational advances beyond scaling LLMs, while highlighting divergent views on how to define and measure progress toward human-like AI.

### Claude Skills are awesome, maybe a bigger deal than MCP

#### [Submission URL](https://simonwillison.net/2025/Oct/16/claude-skills/) | 633 points | by [weinzierl](https://news.ycombinator.com/user?id=weinzierl) | [330 comments](https://news.ycombinator.com/item?id=45619537)

Headline: Claude Skills: simple, load-on-demand “packages” that turn Claude into a practical general agent

- What’s new: Anthropic introduced Claude Skills—folders containing a Markdown “how-to” plus optional scripts and resources. Claude scans each skill’s YAML frontmatter at session start (just a few dozen tokens per skill) and only loads full details when relevant. Anthropic published an official anthropics/skills repo; their document-creation features (.pdf, .docx, .xlsx, .pptx) are implemented as skills.

- Why it’s interesting: The model gains specialized abilities (e.g., Excel work, brand guidelines) without bloating prompts. The pattern is conceptually simple and easy to iterate on—more like shareable “playbooks” than complex plugins.

- Real-world test: Willison tried the slack-gif-creator skill with Sonnet 4.5. The Python script imports the skill’s core library, generates an animated GIF, and validates Slack constraints (e.g., under 2MB). The first result was ugly, but the workflow shows how quickly skills can be refined.

- Big caveat: Skills depend on a sandboxed coding environment with filesystem and command execution. That’s powerful—and raises familiar safety concerns (prompt injection, isolation). Still, it unlocks a lot compared to MCP/plugins.

- Takeaway: Skills feel like a pragmatic path to “agents” (tools-in-a-loop). Claude Code looks less like just a coding tool and more like general computer automation. The simplicity is the point—and could make Skills a bigger deal than MCP.

The discussion revolves around the challenges and evolving role of documentation in software development, particularly with AI tools like Claude Skills. Key points include:

1. **Context & Maintenance Challenges**:  
   - Documentation often becomes outdated quickly due to changing contexts ("context window" problem).  
   - Writing docs in advance risks wasted effort if assumptions prove wrong, emphasizing the need for *testable*, *current-context* documentation.  
   - Stable systems (e.g., libraries, dependencies) are exceptions where long-term docs add value.

2. **AI’s Role**:  
   - **Potential benefits**: AI can reduce documentation costs, auto-generate explanations, and verify consistency (e.g., flagging outdated docs in PRs).  
   - **Risks**: Over-reliance on AI may erode human understanding, create misaligned incentives (e.g., docs for AI vs. humans), and introduce errors if unchecked.  
   - **Debate**: Some argue AI-generated docs (verified by humans) are more reliable than traditional ones, while others stress the irreplaceable value of human context and reasoning.

3. **Human Factors**:  
   - **Principal-agent problem**: Developers may write docs for AI consumption rather than human clarity, risking disconnects in team discipline.  
   - **Job security**: Poor documentation can make developers replaceable, incentivizing "knowledge hoarding."  
   - **Organizational trust**: High-friction, low-trust environments hinder documentation quality, whereas small teams with shared missions prioritize it.

4. **Practical Solutions**:  
   - Integrate docs into code review processes.  
   - Use tests as living documentation.  
   - Focus on self-explanatory code with minimal, high-value comments explaining *why* (not just *what*).  

**Takeaway**: Documentation’s future lies in balancing AI efficiency with human context, ensuring clarity, testability, and alignment with real-world use cases.

### Asking AI to build scrapers should be easy right?

#### [Submission URL](https://www.skyvern.com/blog/asking-ai-to-build-scrapers-should-be-easy-right/) | 125 points | by [suchintan](https://news.ycombinator.com/user?id=suchintan) | [66 comments](https://news.ycombinator.com/item?id=45620653)

Skyvern teaches itself to code: 2.7x cheaper, 2.3x faster browser automations

- What’s new: Skyvern can now generate and maintain its own Playwright code from prompts, using reasoning models to turn one “explore” run into a deterministic, reusable script. It falls back to the agent only when something novel happens.
- Why it matters: Removes an LLM from the hot path on every run, making automations faster, cheaper, and more reliable—especially on messy, change-prone sites.
- How it works:
  - Explore mode: the agent navigates a flow, learns coupled interactions (e.g., linked radio buttons), records a trajectory, and captures intent metadata (the “why,” not just the “what”).
  - Replay mode: compiles those learnings into Playwright for deterministic execution; uses targeted fallbacks to handle outages, DOM shifts, or unexpected branches.
- Real-world example: Registering for an EIN via Delaware/IRS forms. Naive scripts break on linked choices and nighttime outages; Skyvern infers the branching logic at runtime, encodes it, and recovers gracefully when the portal changes or is down.
- Why reasoning models matter: They boost accuracy to production levels and let the agent write engineer-like scripts that reflect its exploration and intent.
- Availability: Open source and Cloud options.

Bottom line: This is a record→compile→repair loop for web automation—self-healing scrapers with auditable Playwright code and LLMs only where they add value.

The Hacker News discussion on Skyvern's browser automation tool highlights several key themes:

1. **Efficiency & Practicality**:  
   - Skyvern's ability to generate/maintain Playwright scripts via exploration and deterministic replay is praised for reducing LLM dependency, lowering costs, and improving reliability. Users note its potential for handling dynamic, real-world sites (e.g., IRS forms) gracefully.

2. **Debates on LLM Limitations**:  
   - Skepticism arises about LLMs’ ability to handle formal logic and deterministic programming. Critics argue LLMs rely on pattern recognition rather than true reasoning, struggling with abstraction and context shifts. Some suggest pairing LLMs with symbolic logic systems for robustness.

3. **Comparisons to Human Learning**:  
   - A philosophical debate emerges: while some liken Skyvern’s exploration to a child learning through trial/error, others emphasize LLMs lack embodied experience or intentionality, limiting their “learning” to statistical text manipulation.

4. **Technical Challenges**:  
   - Users discuss hurdles like JavaScript-heavy sites (e.g., Cloudflare-protected pages) and advocate for reverse-engineering APIs instead of browser automation. Concerns about brittle scrapers persist despite self-healing claims.

5. **Legal & Ethical Concerns**:  
   - Automating sensitive tasks (e.g., IRS forms) raises red flags about legality and risk, though proponents argue public web forms are fair game.

6. **Skepticism vs. Optimism**:  
   - While some herald Skyvern as a breakthrough for reducing maintenance, others remain wary of fully replacing human expertise, stressing the need for hybrid systems combining LLMs with traditional code.

Overall, the discussion reflects cautious optimism about Skyvern’s approach but underscores broader challenges in AI-driven automation, particularly around reliability, adaptability, and the inherent limitations of LLMs.

### AI has a cargo cult problem

#### [Submission URL](https://www.ft.com/content/f2025ac7-a71f-464f-a3a6-1e39c98612c7) | 172 points | by [cs702](https://news.ycombinator.com/user?id=cs702) | [128 comments](https://news.ycombinator.com/item?id=45618350)

AI has a cargo cult problem (Financial Times)

The FT piece argues that much of today’s AI hype mimics the outward trappings of progress—flashy demos, leaderboard wins, and ritualized “best practices”—without reliably delivering durable value. Think cargo cult science: copying the form of intelligence (prompts, agents, benchmark badges) instead of verifying substance.

What the article is getting at
- Surface over substance: Teams replicate viral techniques (chain-of-thought, agents, RAG “sprinkles”) and cherry-picked demos that don’t survive real workloads, edge cases, or cost/latency constraints.
- Benchmark theater: Synthetic leaderboards and narrow tasks are optimized to a fault, while real-world reliability, safety, and maintenance are under-measured.
- Misaligned incentives: Marketing-driven releases, demo-first roadmaps, and “GPU burn” as status symbols encourage ritual rather than rigor.
- Enterprise AI theater: Pilots abound, but few ship robustly; success criteria are vague, and post-deployment telemetry is thin.

Why it matters
- Wasted spend and trust erosion: Organizations burn time and money on brittle solutions, souring stakeholders on genuinely useful AI.
- Policy and investment misfires: Decisions based on demos and vanity metrics can skew capital allocation and regulation.
- Slower real progress: Overfitting to leaderboards crowds out hard engineering on data quality, evaluation, and reliability.

What to do instead
- Define success up front: Tie tasks to ground truth and business metrics (quality, latency, cost, reliability). Compare against strong non-AI baselines.
- Make evals adversarial and reproducible: Pre-register test sets, include edge cases and distribution shifts, run ablations, report error bars.
- Measure total cost of ownership: Track token spend, infra, human oversight, drift management, and failure recovery.
- Prefer the simplest thing that works: Smaller/cheaper models, retrieval and data fixes before bigger models, explicit guards and fallbacks.
- Operate it like a system: Observability, feedback loops, red-teaming, human-in-the-loop where stakes are high.

A fair counterpoint
- Not all progress is theater: There are real gains in coding assistance, summarization, and search—especially when teams do the unglamorous work on data, UX, and ops.
- Demos can be useful probes—provided they’re followed by rigorous evaluation and iteration.

Bottom line
The piece is a call to swap ritual for rigor: value real-world metrics over demo vibes, and build AI systems that stand up to adversarial tests, costs, and time—not just conference stages.

**Summary of Discussion:**

The discussion revolves around skepticism toward AI's current hype cycle, mirroring the FT article's concerns about prioritizing form over substance. Key points include:

1. **Skepticism & Overhyped Claims**:  
   - Many users express fatigue with AI/LLM hype, noting inflated promises versus real-world utility. Critics argue that while AI tools (e.g., ChatGPT) offer productivity gains, they often fail in reliability, accuracy, and ethical alignment. Examples include hallucinations in web searches and nonsensical code generation.  
   - Concerns are raised about companies prioritizing investor-driven metrics (e.g., user growth, token spend) over genuine problem-solving, likening AI adoption to a "cargo cult" of superficial practices.

2. **Data Privacy & Exploitation**:  
   - Debates emerge over whether AI companies exploit user data, with accusations of unethical scraping (e.g., training on personal data without consent). Some counter that enterprise contracts often include data clauses, though skepticism remains about compliance and transparency.

3. **Mixed Practical Experiences**:  
   - Developers share positive anecdotes, such as using AI to automate scripting tasks (e.g., AWS CLI/SDK integration), saving significant time despite occasional errors. Others highlight frustrations with AI-generated inaccuracies requiring manual correction, questioning overall efficiency gains.  
   - A recurring theme is the difficulty in quantifying AI's value: while some users report saving hours on research or coding, others argue these benefits are overstated or context-dependent.

4. **Comparisons to Academic & Industry Failures**:  
   - Users draw parallels between AI's reliability issues and broader systemic problems, such as the replication crisis in academia. Critics note that AI’s error rates (e.g., 40% inaccuracies) might be no better—or worse—than human errors in fields like medical or legal research.

5. **Market Dynamics & Ethics**:  
   - Discussions touch on the concentration of power among major AI players (OpenAI, Anthropic) and their subsidized models, raising concerns about long-term sustainability and monopolistic practices. Some predict a "bubble" akin to the dot-com crash if hype outpaces real utility.  

**Conclusion**:  
The thread reflects a tension between acknowledging AI's potential (e.g., coding assistance, summarization) and critiquing its overhyped, often unproven applications. Calls for rigorous evaluation, transparency, and ethical practices align with the FT article’s push for substance over spectacle. However, personal experiences vary widely, underscoring the technology’s uneven adoption and impact.

### OpenAI Needs $400B In The Next 12 Months

#### [Submission URL](https://www.wheresyoured.at/openai400bn/) | 254 points | by [chilipepperhott](https://news.ycombinator.com/user?id=chilipepperhott) | [235 comments](https://news.ycombinator.com/item?id=45619544)

OpenAI Needs $400B In The Next 12 Months (Edward Zitron)

The take: Zitron argues OpenAI’s newly signaled buildout—spanning Broadcom custom chips, AMD MI450s, NVIDIA’s next-gen “Vera Rubin,” and multiple “Stargate” data centers—implies roughly 33 GW of capacity and would require capital on the order of hundreds of billions, with ~$400B needed in the next year to make promised 2026–2029 timelines remotely plausible.

Key points
- Cost math escalates: He updates to ~$50B per gigawatt of data center capacity (including GPUs, networking, buildings, power infrastructure), up from earlier ~$32.5B estimates. Back-of-the-envelope: ~333k Blackwell GPUs per GW at ~$60k each is ~$20B before networking/power/buildings.
- Time and supply-chain constraints: GW-scale sites typically take ~2–2.5 years; even if money existed, transformers, electrical-grade steel, grid interconnects, and specialized labor are bottlenecks.
- 2026 milestones he deems unrealistic:
  - Broadcom/OpenAI inference chip taped out and enough units manufactured to fill a 1 GW site in H2’26.
  - 1 GW of AMD Instinct MI450s deployed in H2’26.
  - 1 GW of NVIDIA Vera Rubin systems deployed in H2’26.
  For any of this, he says, construction and power procurement should already be well underway.
- Site reality checks: Some locations remain unnamed; a Lordstown, OH facility cited in local reporting is “not a full-blown data center,” undermining the scale implied by announcements.
- Skepticism on financing/optics: He frames vendor and partner statements as market-pleasing optics rather than executable plans, pointing to NVIDIA’s rising accounts receivable as a potential red flag about customer financing.

Why it matters
- If these roadmaps are overstated, the repercussions could hit chipmakers, data-center builders, utilities, and investors banking on uninterrupted AI capex hypergrowth.
- The piece challenges mainstream coverage for treating multi-GW pledges as straightforwardly achievable.

Caveats
- Highly opinionated and combative in tone; many figures rest on author assumptions (e.g., $50B/GW) and partial public disclosures.

What to watch
- Concrete site announcements, grid interconnect agreements, and transformer orders.
- Actual tapeouts/volume production for Broadcom’s OpenAI chip, AMD MI450, and NVIDIA Vera Rubin.
- Capital raises, JV structures, or vendor financing that could bridge the funding gap—or any walk-backs and delays.

**Summary of Hacker News Discussion on OpenAI's $400B Funding Needs and Growth Claims:**

The discussion revolves around skepticism and debate over OpenAI's reported growth metrics, infrastructure challenges, and the broader utility of AI tools like ChatGPT. Key points include:

1. **Productivity vs. Usage**:  
   - Users question whether high adoption (e.g., 100M monthly users) equates to **real productivity gains**. Critics argue frequent usage (e.g., 26 messages/day) doesn’t inherently prove value, comparing it to social media or "sugar"—habitual but not necessarily transformative.  
   - Others counter that sustained user engagement (e.g., weekly usage) signals practical utility, especially in professional contexts like coding or research.  

2. **Growth Metrics Scrutiny**:  
   - OpenAI’s reported **$700M-$800M weekly active users** are met with doubt. Comparisons are drawn to social media platforms (Instagram, TikTok) that boast large user bases but varied utility.  
   - Questions arise about monetization: Can free users convert to paid ($20/month) at scale? Some note even a 5% conversion rate would be surprisingly high.  

3. **Infrastructure and Competition**:  
   - Concerns about the **energy and financial costs** of scaling AI infrastructure (e.g., GPUs, data centers) are highlighted, with parallels drawn to crypto/NFT bubbles.  
   - Competition from alternatives like **Grok, Mistral, and Gemini** is noted, with users citing tool-switching for cost or quality reasons.  

4. **Societal Impact and Priorities**:  
   - Critics contrast AI’s “luxury” focus with **unmet basic needs** (clean water, housing), calling tech investment myopic. Others defend AI’s potential to democratize access to advanced tools globally.  

5. **Financial Realities**:  
   - Skepticism about **investor enthusiasm** driving unrealistic valuations, with comparisons to past tech hype cycles. Questions linger about return on investment (ROI) for AI’s massive capex.  

6. **Tone and Sentiment**:  
   - The debate is polarized: Some dismiss growth claims as marketing hype, while others defend AI’s transformative potential. A recurring theme is the difficulty of measuring “usefulness” objectively.  

**Key Takeaway**: The discussion underscores deep divides over AI’s practical value, financial sustainability, and prioritization in a world with pressing systemic challenges. While critics demand proof of ROI and societal benefit, proponents emphasize AI’s evolving utility and long-term potential.

