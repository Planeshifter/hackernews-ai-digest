## AI Submissions for Thu Jan 18 2024 {{ 'date': '2024-01-18T17:11:01.963Z' }}

### How Meta is advancing GenAI

#### [Submission URL](https://engineering.fb.com/2024/01/11/ml-applications/meta-advancing-genai/) | 32 points | by [atg_abhishek](https://news.ycombinator.com/user?id=atg_abhishek) | [9 comments](https://news.ycombinator.com/item?id=39036986)

In the latest episode of the Meta Tech Podcast, Meta engineer Pascal Hartig speaks with AI research director Devi Parikh about generative AI (GenAI). They discuss the history and future of GenAI, as well as the most interesting research papers that have recently come out. They also talk about Meta's latest GenAI innovations, including Audiobox, a model for generating sound using natural language prompts, Emu, a model for image generation, and Purple Llama, a suite of tools for deploying GenAI models safely and responsibly. The podcast is available to download or listen to on various platforms. If you're interested in AI career opportunities at Meta, you can visit their careers page.

The first comment from user "up2isomorphism" seems to be a playful response and does not provide any meaningful discussion. 
The second comment from user "Dig1t" mentions a speech-to-text model called WhisperSpeech developed by Elevenai OpenAI. User "lnnstrstrk" responds that computers allow for better compression, to which user "bn cmptrs llws scmmrs scm ppl" agrees. 
The third comment from user "gwld" expresses excitement about Facebook's work on cryptocurrency and non-fungible tokens (NFT). User "drts" responds that the Llama Llama project is a great crowdfunding initiative in the NFT world.
The fourth comment from user "lrwbwrkhv" mentions Facebook's future work on hardware. User "gcpm" provides a link to an article about Meta's infrastructure for creating hardware and mentions Quest and Meta Raybans.

### Teller: Universal secret manager, never leave your terminal to use secrets

#### [Submission URL](https://github.com/tellerops/teller) | 106 points | by [nateb2022](https://news.ycombinator.com/user?id=nateb2022) | [22 comments](https://news.ycombinator.com/item?id=39036265)

TellerOps, an open-source project on GitHub, aims to provide a cloud-native secrets management solution for developers. With TellerOps, developers can securely manage and access secrets without ever leaving the command line. The project supports various platforms such as Heroku, AWS, GCE, and CyberArk, and is built on Golang and Hashicorp Vault. TellerOps makes secret management easier and more efficient, allowing developers to focus on writing code rather than handling sensitive information. The project has gained popularity, with over 2.3k stars and 163 forks on GitHub. Check it out at tlr.dev.

The discussion about the TellerOps project on Hacker News covers various topics related to secrets management and different tools and techniques to handle secrets securely.
One commenter raises a question about password storage and suggests using a personal password manager. Another user suggests using GPG encryption and generating passwords using a GPG key ID.
Another user recommends using encrypted USB drives for backups, while another suggests using a tool like Syncthing to synchronize a shared folder structure.
Someone mentions the integration of password management with GPG and highlights its scriptability and platform support. They also mention using a dedicated window manager or virtual desktops to manage different projects and their secrets.
The importance of properly managing local environment variables and securely storing secrets like Stripe keys and AWS S3 access is emphasized by a user.
One commenter talks about the security of the NPM ecosystem and mentions that secrets are unlikely to be stored in NPM packages.
Some users mention alternative solutions like the 1Password CLI, macOS Keychain Access, and AWS SSM for managing secrets.
There is a discussion about the name "TellerOps," with some users making playful comments about its similarity to "magician secrets."
One user shares a simple workflow for utilizing the macOS Keychain Access command-line tool for managing secrets.

Finally, there is an unrelated comment stating "dd stbl," which seems to be unrelated to the main discussion.

### WhisperSpeech – An open source text-to-speech system built by inverting Whisper

#### [Submission URL](https://github.com/collabora/WhisperSpeech) | 446 points | by [nickmcc](https://news.ycombinator.com/user?id=nickmcc) | [111 comments](https://news.ycombinator.com/item?id=39036796)

Introducing WhisperSpeech, an exciting Open Source text-to-speech system developed by Collabora. WhisperSpeech is built by using an innovative method of inverting Whisper. With 1.8k stars and 72 forks on GitHub, this project has gained significant attention from the developer community. The system is based on PyTorch and offers advanced speech synthesis capabilities. If you're interested, you can check out the repository and find more information about WhisperSpeech on the official website. Happy exploring!

The discussion about the WhisperSpeech submission on Hacker News covers various topics, including licensing, voice synthesis in different languages, improvements to the project, and related tools.

- Some users discuss licensing concerns and the need for proper licensing of speech recordings for open-source models, especially when used in commercial applications. There is mention of potential legal risks and the importance of clarity in licensing documentation.
- Another user raises the point that protecting models like WhisperSpeech may pose challenges until clearer legal frameworks are established for non-generative models. They also mention the need for higher scrutiny of text-to-speech systems like Whisper ASR.
- A user expresses interest in customized voice synthesis and wonders if WhisperSpeech supports that capability. The response mentions that voice cleaning is mentioned in the README, and a link to the examples in the README is provided.
- The WhisperSpeech developer expresses gratitude for the comments and mentions ongoing work to improve the project. They also mention being interested in collaboration with people working on VR-related projects.
- Some users express interest in Mandarin Chinese speech synthesis and high-quality phonetic models. They discuss existing projects like EmotiVoice and share links to CLI wrappers and related resources.
- The topic of generating flashcards and text-to-speech for language learning is brought up. Users discuss resources like LibriVox and its variable recording quality, as well as potential solutions for generating Anki decks with IPA symbols and Greek letters.
- The use of Mimic 3, a speech synthesis tool developed by Mycroft, is mentioned. Users compare it to WhisperSpeech and discuss its capabilities and quality.

Overall, the discussion highlights different aspects of WhisperSpeech, including licensing concerns, language support, voice synthesis customization, and related tools for language learning and speech synthesis.

### Mark Zuckerberg’s new goal is creating artificial general intelligence

#### [Submission URL](https://www.theverge.com/2024/1/18/24042354/mark-zuckerberg-meta-agi-reorg-interview) | 197 points | by [mfiguiere](https://news.ycombinator.com/user?id=mfiguiere) | [301 comments](https://news.ycombinator.com/item?id=39045153)

Mark Zuckerberg, CEO of Meta, has revealed that the company's AI research group, FAIR, will be moved closer to the team building generative AI products across Meta's apps. The goal is to directly reach Meta's billions of users with AI breakthroughs. Zuckerberg expressed his desire to build artificial general intelligence (AGI) but did not provide a timeline for its development. He also acknowledged the intense competition for AI talent and announced that Meta will own over 340,000 of Nvidia's H100 GPUs by the end of the year, highlighting their commitment to scaling AI capabilities. Despite lacking a clear definition of AGI, Zuckerberg emphasized the importance of it having broad capabilities. The company is currently training Llama 3, its latest large language model, which will have code-generating capabilities. Zuckerberg also addressed the question of who will control AGI, stating that Meta's current approach is to make its AI models open source, similar to the Llama model.

The discussion about Mark Zuckerberg's announcement on Hacker News covers various perspectives on the development and potential dangers of artificial general intelligence (AGI). Here are some key points raised by commenters:

1. Responsibility and Control: Some commenters express concerns about the potential dangers of AGI and who should be in control of it. They discuss the risks of AI making decisions that could harm humanity and the need for responsible development.
2. Distributed Intelligence: There is a debate about whether AGI should be centralized or distributed. Commenters argue that current AI systems are already connected and distributed across the internet and that a distributed approach may be more practical and less worrisome.
3. Cost and Scale: The cost of developing and scaling AGI is highlighted, and one commenter mentions that Meta's ownership of Nvidia GPUs demonstrates their commitment to scaling AI capabilities.
4. Definition of AGI: Some commenters discuss the difficulties in defining AGI and whether it should be limited to human-level intelligence. They point out that there are different levels of intelligence among various animal species and suggest that AGI should be defined by its capability for abstract reasoning.
5. Dangers and Responsibilities: Various dangers and potential risks associated with AGI are discussed, such as its impact on jobs, possible misuse by individuals or governments, and the need to consider the ethical implications.
6. Comparisons to Human Intelligence: Some commenters argue that human intelligence has its own limitations and flaws, and that AGI should not necessarily be expected to replicate human intelligence. They suggest that AGI may have limited or different forms of intelligence.
7. Ethics and Rights: The ethical considerations surrounding AGI are debated, with some commenters suggesting that AGI should have rights and freedoms similar to humans, while others argue that it should be treated as a tool or slave due to its lack of consciousness.

Overall, the discussion reflects a range of opinions on the development, implications, and control of AGI.

### Escaping surveillance capitalism, at scale

#### [Submission URL](https://ergaster.org/posts/2024/01/18-escaping-surveillance-capitalism-at-scale/) | 252 points | by [thibaultamartin](https://news.ycombinator.com/user?id=thibaultamartin) | [201 comments](https://news.ycombinator.com/item?id=39043547)

In an effort to escape surveillance capitalism, many people are turning to self-hosting and paid subscriptions. However, these solutions may not always guarantee privacy and protection of personal data. The mental health service provider BetterHelp, for example, shared customer data with third parties, despite promising privacy. Self-hosting can be more effective in preventing surveillance capitalism by keeping data on individual infrastructures, but it requires time and knowledge. One workaround is using software that can be self-hosted but buying it as a service from providers like Ionos. However, this can lead to a race to the bottom in terms of cost and quality. Additionally, not all providers can be trusted, as some may sell user data for profit. End-to-End Encryption (E2EE) is another option that ensures data can only be read by intended recipients, but it comes with its drawbacks. Overall, finding effective alternatives to surveillance capitalism requires a careful evaluation of available options and understanding their limitations.

The discussion around the submission covers various aspects of self-hosting and alternative solutions to escape surveillance capitalism. 

- One user suggests using Umbrel, a self-hosting solution, as it provides backups and the ability to restore connectivity in emergencies. However, another user prefers DIY machines and encrypting important data rather than relying on Umbrel.
- There is a discussion about the reliability and availability of internet connectivity, with users sharing their experiences with different ISPs and connection speeds. Some users emphasize the importance of backups and having connectivity options to ensure data protection.
- Other users discuss different self-hosting alternatives, such as Urbit and running servers at home. They highlight the advantages of self-hosting, such as control over data and privacy, but also acknowledge the challenges and knowledge required to set up and maintain self-hosted systems.
- The discussion also includes recommendations for trusted providers and services, as well as potential solutions for encryption and secure storage. However, there are concerns raised about the trustworthiness of hosts and the risks of relying on third-party services.
- Some users suggest solutions like running personal cloud services, using virtual private servers (VPS), or implementing containerization technologies like Docker for better control and management of data.
- There are also discussions about the limitations of self-hosting, including the need for key management and the potential vulnerability of virtual machines to surveillance. Worries about government surveillance and increasing restrictions on free speech are also mentioned.
- Overall, the discussion emphasizes the need for careful consideration of available options and the trade-offs between convenience, privacy, and security when seeking alternatives to surveillance capitalism.

### Linear transformers are faster after all

#### [Submission URL](https://manifestai.com/blogposts/faster-after-all/) | 111 points | by [JoelEinbinder](https://news.ycombinator.com/user?id=JoelEinbinder) | [22 comments](https://news.ycombinator.com/item?id=39036355)

Researchers have been exploring ways to improve the efficiency of transformer models, which are widely used in natural language processing tasks. One approach involves replacing the exponential operation in the attention layer of the transformer with a linear operation. This allows for a recurrent reformulation of the model, reducing the computational cost from quadratic to linear on the length of the input. However, initial experiments showed that training large language models with linear transformers was slower compared to traditional transformers. A recent post on Hacker News challenges this notion and presents several different implementations of linear transformers that can achieve massive speed-ups. The experiments compare the speed and performance of different algorithms, including an optimized transformer, a straightforward recurrent implementation of the linear transformer, and an optimized linear transformer. The results show that the optimized linear transformer outperforms even the highly-optimized FlashAttention baseline in terms of training speed, especially for moderately large context sizes. However, it is noted that linear transformers can negatively affect learning as the sequence length grows, leading to degraded performance. The post concludes by mentioning that future posts will explore ways to improve the learning of linear transformers and enable the training of language models with super-fast sampling.

The discussion on this submission mainly revolves around the performance and potential drawbacks of linear transformers. Some users express skepticism about the claimed benefits of linear transformers and suggest that the author's results may be misleading. Others point out that linear transformers can simplify the model and computation, making it more efficient. There is also a mention of another paper on the topic that presents a faster and more accurate algorithm called MoE-Mamba. Some users express interest in seeing comparisons of linear transformers with large-scale models and highlight the importance of testing on a wide range of contexts. One user appreciates the experiments conducted and wonders what would happen if nested models were rectified with linear attention, while another user notes that linear attention is a simpler way to express the computations and reduces the training data requirements.

There is also a brief exchange about the presentation format of the paper, with one user mentioning the use of LaTex and another providing developer tools to render math expressions in the discussion.

### Each Facebook user is monitored by thousands of companies

#### [Submission URL](https://www.consumerreports.org/electronics/privacy/each-facebook-user-is-monitored-by-thousands-of-companies-a5824207467/) | 279 points | by [mfiguiere](https://news.ycombinator.com/user?id=mfiguiere) | [134 comments](https://news.ycombinator.com/item?id=39035536)

A recent analysis by Consumer Reports reveals the widespread surveillance of Facebook users by thousands of companies. The study, conducted using data collected from 709 volunteers who shared their Facebook archives, found that a total of 186,892 companies had sent data about these users to Facebook. On average, each participant had their data sent by 2,230 different companies, with some panelists' data linked to over 7,000 companies. The study focused on server-to-server tracking, which involves personal data being transferred from a company's servers to Facebook's servers. The results offer a rare insight into the collection and aggregation of personal information online. Notably, retailers like Home Depot, Macy's, and Walmart were among the most frequently identified companies sharing user data with Facebook, alongside data brokers such as LiveRamp, Experian, and Acxiom. While Meta, the parent company of Facebook, offers transparency tools to users, Consumer Reports identified shortcomings with these tools, including unclear disclosure of data providers and limited control over opt-out requests.

The discussion on this submission revolves around different aspects of Facebook's data tracking practices and the potential implications for user privacy. Several commenters point out that Facebook's tracking pixels no longer work on many browsers due to enhanced privacy features, while others discuss the limitations and effectiveness of browser fingerprinting and other tracking methods. There is also conversation about the role of data brokers in sharing user data with Facebook and the responsibility of both Facebook and these companies in protecting user privacy. Overall, the comments reflect concerns about the widespread surveillance of Facebook users and highlight the need for better transparency and control over personal data collection online.

### Self-Consuming Generative Models Go MAD

#### [Submission URL](https://arxiv.org/abs/2307.01850) | 48 points | by [galaxyLogic](https://news.ycombinator.com/user?id=galaxyLogic) | [49 comments](https://news.ycombinator.com/item?id=39038850)

A new paper titled "Self-Consuming Generative Models Go MAD" explores the phenomenon of using synthetic data to train generative AI models in a self-consuming loop. The authors analyze three different types of autophagous loops, which vary in the availability of real training data and the bias in samples from previous generation models. The study concludes that without enough fresh real data in each generation, the quality or diversity of future generative models progressively decreases. They term this condition "Model Autophagy Disorder" (MAD), drawing an analogy to mad cow disease. The paper provides a thorough analysis using state-of-the-art generative image models and offers insights into the limitations of self-consuming generative AI algorithms.

The discussion surrounding the submission is quite varied. Some users discuss the limitations and challenges of training generative AI models, with one user highlighting the need for fresh real data in each generation to avoid the decrease in quality and diversity of future models. Others argue that humans consuming AI-generated content is not a problem and that it is similar to humans consuming content created by other humans. The notion of humans and AI interacting and feeding off each other is explored, with some users expressing concerns about the potential consequences and others stating that it's similar to how humans have always learned from and interacted with each other. There are also discussions about the complexities of training AI models and the differences between AI and human cognition. The analogy of Model Autophagy Disorder (MAD) to mad cow disease is brought up, and there are some playful comments comparing AI to cows and humans to milk consumers. Overall, the discussion raises interesting points about the challenges and implications of self-consuming generative AI algorithms.

### 'No AI Fraud Act' Could Outlaw Parodies, Political Cartoons, and More

#### [Submission URL](https://reason.com/2024/01/17/ai-fraud-act-could-outlaw-parodies-political-cartoons-and-more/) | 116 points | by [elsewhen](https://news.ycombinator.com/user?id=elsewhen) | [138 comments](https://news.ycombinator.com/item?id=39040720)

Reps. María Elvira Salazar (R-Fla.) and Madeleine Dean (D-Pa.) have introduced the No Artificial Intelligence Fake Replicas And Unauthorized Duplications (No AI FRAUD) Act, a bill that aims to protect individuals' likeness and voice from unauthorized use in AI-generated content. While the bill's sponsors claim to balance the right to control the use of one's identifying characteristics with First Amendment protections, critics argue that it could have a chilling effect on comedy, commentary, and artistic expression. The bill creates a right to sue for damages if someone uses another person's likeness or voice without permission, and it defines digital depictions and digital voice replicas very broadly, potentially encompassing a wide range of content. This could have implications for social media platforms, video platforms, and any entity that enables the sharing of art, entertainment, and commentary. The bill's provisions raise concerns about the infringement of free speech rights and the potential for legal disputes over the use of AI-generated content.

The discussion on this submission covers various perspectives on the No AI FRAUD Act and its potential impact on free speech and artistic expression. Some users argue that protecting individuals' likeness and voice is necessary to prevent deception and fraud, while others express concerns about the potential for the bill to stifle comedy, commentary, and creativity. There is also discussion about the broader implications for social media platforms and the enforcement of such legislation. The conversation touches on legal definitions, the role of AI in generating content, and the challenge of distinguishing between satire and misinformation. Additionally, the discussion delves into constitutional and legal precedents related to speech, with references to Supreme Court cases and the importance of context in determining the limits of free speech. There is also some debate about the use of metaphors and the concept of strawman arguments. Overall, the discussion reflects a range of viewpoints on the balance between protecting individuals' rights and preserving freedom of expression in the context of AI-generated content.

