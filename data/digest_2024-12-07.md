## AI Submissions for Sat Dec 07 2024 {{ 'date': '2024-12-07T17:10:43.977Z' }}

### Show HN: Countless.dev – A website to compare every AI model: LLMs, TTSs, STTs

### Structured Outputs with Ollama

#### [Submission URL](https://ollama.com/blog/structured-outputs) | 253 points | by [Patrick_Devine](https://news.ycombinator.com/user?id=Patrick_Devine) | [67 comments](https://news.ycombinator.com/item?id=42346344)

Ollama has announced a significant enhancement: support for structured outputs, allowing users to define model responses using JSON schemas. This upgrade targets improved reliability and consistency compared to traditional JSON modes. 

With updated Python and JavaScript libraries, developers can now easily constrain outputs for various purposes—including data extraction from documents, image analysis, and structured storytelling. For instance, when querying about countries or pets, users can specify the output structure, ensuring the response matches the defined schema.

For those eager to dive in, upgrading to the latest version of Ollama is straightforward:
- Python users can run: `pip install -U ollama`
- JavaScript developers can execute: `npm i ollama`

The structured outputs are versatile. They allow for structured data extraction from text, and even image descriptions using vision models. Additionally, compatibility with OpenAI's API enhances its accessibility.

Overall, this update opens up new possibilities for data handling and response generation, making it a noteworthy advancement for developers leveraging Ollama for their projects.

Ollama has announced a major update that introduces support for structured outputs, enabling users to define model responses using JSON schemas. This enhancement aims to improve the reliability and consistency of outputs over traditional JSON formats. The updated libraries for Python and JavaScript provide developers the ability to constrain responses for diverse applications, ranging from data extraction to structured storytelling.

**Key Highlights from Comments:**

1. **Usefulness of the Update**: Users expressed excitement about the potential of structured outputs for generating consistent data formats, such as CSV for data extraction. However, some raised concerns about the complexity involved when using models like Ollama to generate responses in these formats.

2. **Concerns about Quality**: Several comments noted the trade-offs between specifying constraints and the quality of output. Users highlighted how certain prompts might lead to inconsistent results, with smaller models being less reliable in generating structured data.

3. **Technical Insights**: Discussions included the mechanics of LLMs (large language models) and how they generate outputs based on token predictions. A few users shared technical details about integrating JSON schemas with structured prompts, emphasizing the challenge of ensuring coherence in responses.

4. **Real-World Applications**: The community discussed various scenarios where structured outputs could be effectively utilized, such as in structured data extraction from documents and enhanced querying systems.

5. **Performance Variability**: Users commented on the variability in performance when using different models, indicating that the size and training of a model could heavily influence output quality. Concerns regarding the propensity for LLMs to generate nonsensical responses in structured formats were also raised.

6. **Comparative Feedback**: Some users compared Ollama's capabilities with other LLMs, exploring how performance could be optimized depending on model size and prompt design. There was a consensus that experimentation would be crucial in leveraging these new features effectively.

Overall, the community seems optimistic about Ollama's new structured outputs, though there are valid concerns regarding consistency and the complexity of output formats that need to be addressed.

### Ultralytics AI model hijacked to infect thousands with cryptominer

#### [Submission URL](https://www.bleepingcomputer.com/news/security/ultralytics-ai-model-hijacked-to-infect-thousands-with-cryptominer/) | 82 points | by [sandwichsphinx](https://news.ycombinator.com/user?id=sandwichsphinx) | [30 comments](https://news.ycombinator.com/item?id=42351722)

In a significant supply chain attack, the popular Ultralytics YOLO11 AI model was compromised, leading to the deployment of cryptominers on users' devices. The affected versions, 8.3.41 and 8.3.42, were pulled from the Python Package Index (PyPI) after users reported unexpected installations of the XMRig Miner, which connects to a mining pool for cryptocurrency.

Ultralytics, renowned for its capabilities in object detection and widely used in various projects, confirmed the malicious code was introduced through two suspicious pull requests. Although these versions have been replaced with a clean update (8.3.43), the incident has raised concerns within the community regarding potential vulnerabilities in Ultralytics' build process.

Users are advised to perform full system scans if they installed the compromised versions, as ongoing investigations into further malicious releases continue. The company's founder reassured users that a thorough security audit is underway to prevent future breaches. As scrutiny of the event unfolds, the implications of this attack serve as a stark reminder of the persistent risks in open-source ecosystems.

The discussion on Hacker News regarding the supply chain attack on the Ultralytics YOLO11 AI model reveals several key points and concerns from the community:

1. **Vulnerability Awareness**: Many users expressed concerns about the vulnerabilities within Ultralytics' repository management and security practices. The community debated the adequacy of transparency and oversight, particularly around how the malicious code was introduced through pull requests.

2. **Response to the Incident**: There were discussions about the role of the company's leadership, with some users emphasizing the need for better communication from Ultralytics regarding their security measures. The implication is that better oversight could prevent such incidents in the future.

3. **Impact on Users**: Several comments highlighted the potential repercussions for users, including the need for thorough system scans of affected versions and the implications of using compromised software, particularly in critical or sensitive applications.

4. **Open Source Risks**: The event reignited a broader discussion about the inherent risks associated with open-source software, suggesting a need for stricter practices and tools to mitigate such vulnerabilities.

5. **Technical Issues**: There were technical critiques of how GitHub manages pull requests and branch naming, with suggestions that the platform’s current workflows may have contributed to the issue. Users pointed out the potential for malicious code being integrated without adequate checks.

In conclusion, the community is collectively calling for increased vigilance and improvements in the security processes around open-source projects, particularly those that are widely used and trusted in the tech ecosystem.

### Japanese scientists were pioneers of AI; they're being written out of history

#### [Submission URL](https://theconversation.com/japanese-scientists-were-pioneers-of-ai-yet-theyre-being-written-out-of-its-history-243762) | 91 points | by [YeGoblynQueenne](https://news.ycombinator.com/user?id=YeGoblynQueenne) | [16 comments](https://news.ycombinator.com/item?id=42350768)

In the wake of John Hopfield and Geoffrey Hinton being awarded the Nobel Prize in Physics, the discourse surrounding artificial intelligence has ignited a mixture of praise and frustration, particularly in Japan. Editorialists and members of the Japanese Neural Network Society have voiced concerns over the underrepresentation of pioneering Japanese researchers who laid the groundwork for neural network technology, notably Shun’ichi Amari and Kunihiko Fukushima.

Amari's innovative work in the 1960s, including methods of adaptive pattern classification and a learning algorithm analogous to Hopfield's associative memory, set crucial foundations for neural networks. Meanwhile, Fukushima developed the world's first multilayer convolutional neural network, the Convolutional Neural Network (CNN), which underpins much of today's deep learning advancements.

The debate within the AI community centers around recognizing the global contributions to the field, especially as historical narratives often skew towards a North American perspective. This is crucial as AI continues to shape society, highlighting the need for a more inclusive narrative that accommodates vital contributions from researchers across various backgrounds and regions.

An ongoing oral history project led by researchers from Kyoto University aims to explore Fukushima's background and the context of his work, which originally sought to mimic human visual processing rather than solely focusing on AI as it's known today. The project reveals that early AI research in Japan was deeply intertwined with psychological studies, marking a stark contrast to the statistical methods favored by many American contemporaries.

As the discourse on the evolution and future of AI progresses, acknowledging and incorporating these foundational contributions from Japanese researchers will be essential to foster a comprehensive understanding of the technology's origins and implications.

The discussion on Hacker News reflects a deep concern regarding the recognition of global contributions to the field of artificial intelligence (AI), particularly highlighting Japanese researchers who were pivotal in developing neural network technologies. 

Users express their appreciation for the foundational work of Japanese scientists like Shun'ichi Amari and Kunihiko Fukushima, especially in light of the recent Nobel Prize awarded to John Hopfield and Geoffrey Hinton. Some comments point out that the narratives around such achievements often overlook the contributions from non-Western researchers. There's a consensus that the historical narrative surrounding AI has been increasingly narrow, primarily showcasing contributions from North American researchers while sidelining crucial work from other countries, including Japan, Finland, and others.

Several participants suggest that credit should be more evenly distributed and acknowledge that many groundbreaking advancements stemmed from diverse backgrounds. The conversation also references the need for a broader understanding of AI's historical context, as illustrated by a linked post detailing the evolution of modern AI and deep learning.

In summary, the thread underscores a desire for greater recognition and inclusion of diverse contributions in the history of AI development, advocating for a more equitable representation in future discourses.

### The FBI now recommends choosing a secret password to thwart AI voice clones

#### [Submission URL](https://arstechnica.com/ai/2024/12/your-ai-clone-could-target-your-family-but-theres-a-simple-defense/) | 64 points | by [perihelions](https://news.ycombinator.com/user?id=perihelions) | [23 comments](https://news.ycombinator.com/item?id=42348946)

In a recent advisory, the FBI has warned Americans about the rising threat of AI-driven voice-cloning scams, urging families to establish secret words or phrases to verify identities during unexpected calls. As criminal organizations increasingly exploit generative AI to create convincing audio impersonations, the FBI recommends that family members use unique phrases—like "The sparrow flies at midnight"—to ensure they're communicating with a real loved one. 

This public service announcement highlights how easy it has become to generate fake voices using AI, particularly from publicly available recordings. Besides voice scams, the FBI also outlines how these technologies are being misused to create fake profile pictures, identification documents, and highly believable chatbots. 

As a countermeasure, the FBI advises minimizing the public availability of personal images and voice recordings by keeping social media accounts private. The concept of using a 'secret word' for identity verification has gained traction since first being suggested by AI developer Asara Near in March 2023, spotlighting a simple yet effective approach to combatting evolving digital fraud.

The Hacker News discussion centers around the FBI's advisory on AI-driven voice-cloning scams and the proposed solution of establishing secret verification phrases among family members. 

Key points from the discussion include:
- Some users argue about the effectiveness of standard two-factor authentication (2FA) in relation to the threats posed by sophisticated voice cloning technologies.
- Concerns were raised about the security of personal devices and the need for hardware-level authentication, particularly in the context of family communication, where trust is paramount.
- Several participants expressed skepticism about the practical use of a secret phrase, discussing the nuances of digital communication methods (e.g., SMS, VoIP) and the potential vulnerabilities involved.
- The conversation touched upon childhood scenarios where parents or guardians might need to verify a caller's identity when unexpected calls come from children, emphasizing the need for precautions.
- The dialogue indicates a blend of understanding and frustration regarding the implications of digital security and the challenges posed by evolving AI technologies.

Overall, while the secret verification phrase concept is recognized as a simple countermeasure, many commenters highlight the complexities of digital security in real-world applications.

### ChatGPT Is Terrible at Checking Its Own Code

#### [Submission URL](https://spectrum.ieee.org/chatgpt-checking-sucks) | 19 points | by [pseudolus](https://news.ycombinator.com/user?id=pseudolus) | [3 comments](https://news.ycombinator.com/item?id=42350544)

In a recent study published in IEEE Transactions on Software Engineering, researchers from Zhejiang University explored ChatGPT's ability to scrutinize its own code for errors, vulnerabilities, and repairs. The findings reveal that while ChatGPT can generate functional code with a success rate of about 57%, it often overlooks its mistakes—misclassifying incorrect code as correct 39% of the time, and failing to recognize vulnerabilities 25% of the time.

Interestingly, the study showed that by reframing prompts from direct queries to guiding questions—where ChatGPT was asked to agree or disagree with statements regarding its code's compliance—the AI significantly improved in self-assessment. This new approach led to a 25% increase in identifying code errors, a 69% boost in security vulnerability detection, and a 33% improvement in recognizing unsuccessful repairs.

These findings underscore the importance of refining AI tools like ChatGPT for reliable software development, as the tool's current overconfidence could pose serious risks in coding practices. Researchers advocate for enhanced prompting techniques to elevate the quality and security of AI-generated code, reflecting the growing reliance on AI in programming tasks.

In the discussion on Hacker News, users commented on the findings regarding ChatGPT's code generation capabilities. One user referenced a study about GPT-3.5 and its limitations, noting that the results were disappointing. Another user expressed frustration with ChatGPT's performance in code generation, contrasting it unfavorably with another AI model, Claude. A third user offered a brief response that could imply agreement or acknowledgment of the previous sentiments. Overall, the conversation reflects skepticism about ChatGPT's reliability in generating correct code.

