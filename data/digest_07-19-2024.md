## AI Submissions for Fri Jul 19 2024 {{ 'date': '2024-07-19T17:10:51.402Z' }}

### Kompute – Vulkan Alternative to CUDA

#### [Submission URL](https://github.com/KomputeProject/kompute) | 159 points | by [coffeeaddict1](https://news.ycombinator.com/user?id=coffeeaddict1) | [32 comments](https://news.ycombinator.com/item?id=41009023)

The Kompute project is gaining traction on Hacker News, boasting a general-purpose GPU compute framework that supports a wide range of graphics cards and is optimized for advanced GPU data processing use cases. Built on Vulkan, this framework is designed to be blazing fast, mobile-enabled, and asynchronous. It is backed by the Linux Foundation and supports cross-vendor graphics cards from AMD, Qualcomm, NVIDIA, and others.

With a robust codebase and 90% unit test coverage, Kompute offers a flexible Python module with a C++ SDK for optimizations, asynchronous and parallel processing support through GPU family queues, and explicit relationships for GPU and host memory management. The project also caters to advanced use cases such as machine learning, mobile development, and game development.

If you want to get involved, the project encourages community participation through Discord, monthly calls, and more. Various projects, including GPT4ALL, llama.cpp, and vkJAX, are already using Kompute for their GPU computing needs. The project provides examples in both C++ and Python interfaces, making it accessible for developers looking to leverage the power of GPUs for their applications.

The discussion on the Kompute project in the Hacker News comments covers various aspects of Vulkan vs. OpenCL, the advantages of using Vulkan for GPU computing, and comparisons with CUDA. Some users discuss the differences in low-level control, memory allocation, and resource synchronization between Vulkan and OpenCL, while others highlight the benefits of using Vulkan for gaming and the challenges of transitioning from OpenCL to Vulkan.

There are mentions of alternative solutions like SYCL, and discussions on the limitations of using compute shaders in Vulkan for heavy graphics tasks. The conversation also touches on the challenges and benefits of alternatives to CUDA, such as C++ and Fortran with PTX compiler backends, as well as the technical aspects of using Vulkan with PyTorch.

Overall, the comments express interest in exploring Kompute as a potential cross-platform and cross-vendor GPU compute solution that could offer a straightforward alternative to CUDA and OpenCL for various GPU computing needs.

### What happened to BERT and T5?

#### [Submission URL](https://www.yitay.net/blog/model-architecture-blogpost-encoders-prefixlm-denoising) | 220 points | by [fzliu](https://news.ycombinator.com/user?id=fzliu) | [63 comments](https://news.ycombinator.com/item?id=41009803)

The blog post titled "What happened to BERT & T5?" dives deep into the world of Transformer Encoders, PrefixLM, and Denoising Objectives, providing clarity on the evolution of language models. The author discusses the shift from encoder-only models like BERT to encoder-decoder models like T5, highlighting the subtle differences between them. They also touch on the concept of PrefixLMs, which share similarities with encoder-decoders.

The post sheds light on denoising objectives, comparing the approaches used in BERT-style models versus T5-style models. While denoising objectives have proven to work well, they are deemed insufficient when used as a standalone objective due to lower "loss exposure" and reduced sample efficiency per FLOP. The author emphasizes the importance of understanding the different model architectures and objectives in the current era of Language Model Models (LLMs).

The discussion on the blog post titled "What happened to BERT & T5?" covers various aspects related to Transformer Encoders, PrefixLM, Denoising Objectives, and the evolution of language models. Here is a summary of some key points raised by the users:

1. Latency and throughput are important for applications, and smaller models like custom BERT may be preferable for certain tasks.
2. Document classification in a contextual space presents challenges, especially for solving large-scale classification tasks with millions of document sets.
3. Users discuss the capabilities and relative performances of models like T5 in translation tasks.
4. There is a debate on the understanding of decoder versus encoder models and their mechanisms in text context generation.
5. Users explore the differences in transformer models and their attention mechanisms, highlighting the progress and challenges in the field.
6. The significance of attention mechanisms, autoregressive completion, and the ability of different models to generate text are discussed.
7. User interactions touch upon the challenges and advancements in language modeling, including the adoption of different architectures like ncdrdcdr models.
8. Some users share insights on the popularity and practical implications of models like BERT and T5 in various domains such as genomics and text classification tasks.
9. There is a focus on the training data and cost efficiency of language models, with references to specific projects and libraries related to BERT and similar models.

Overall, the discussion provides a rich exchange of perspectives on the current trends, challenges, and potentials in the field of language modeling using Transformer Encoders.

### AI paid for by Ads – the GPT-4o mini inflection point

#### [Submission URL](https://batchmon.com/blog/ai-cheaper-than-ads/) | 270 points | by [thunderbong](https://news.ycombinator.com/user?id=thunderbong) | [220 comments](https://news.ycombinator.com/item?id=41010188)

OpenAI has introduced their latest creation, the GPT-4o mini model, offering high intelligence at a remarkably low cost of $0.15 per 1 million input tokens and $0.60 per 1 million output tokens. This advancement has opened the door for building dynamic AI-generated content supported by ads, paving the way for an accessible and lucrative content creation experience.

Considering the potential earnings from ad impressions, Google's AdSense estimated revenue calculator provides insights into the revenue generated based on content category and monthly page views. For instance, with 50,000 monthly page views under the Finance category, one could potentially earn around $2,000 annually, equating to roughly $0.0026 per page view, as estimated across all categories.

In an experiment to showcase the power of AI-generated content, a blog was created to respond to user queries dynamically. When prompted with a question like "how to have my MacBook audibly greet me whenever I flip it open," the GPT-4o mini model generates a detailed article providing step-by-step instructions on setting up personalized greeting sounds for a MacBook.

This demonstration underscores the potential of leveraging AI technologies like GPT-4o mini to create engaging and informative content efficiently. With the balance between cost-effective AI models and revenue-generating ad impressions, a new era of content creation and monetization seems within reach for publishers and creators alike.

The discussion on this submission delves into various topics related to AI-generated content and the implications of leveraging models like GPT-4o mini for content creation and monetization.

- Some users touch upon the shift towards original content creation in 2023, moving away from recycled existing content and the potential impact on search results.
- Others discuss the evolving writing styles influenced by AI models like LLMs and ChatGPT, noting differences between human-generated and AI-generated content.
- There are mentions of potential biases in AI models like LLMs and the impact of these models on writing styles and content quality.
- Additionally, the chat delves into humorous tangents, such as poetic discussions about tangerines, reflections on human reading habits influenced by AI-generated content, and playful banter about the challenges of dealing with AI-generated profanity.

Overall, the conversation explores the evolving landscape of content creation, writing styles, and the broader implications of integrating AI technologies in these processes.

### NASA's Curiosity rover discovers a surprise in a Martian rock

#### [Submission URL](https://www.jpl.nasa.gov/news/nasas-curiosity-rover-discovers-a-surprise-in-a-martian-rock) | 172 points | by [Ozarkian](https://news.ycombinator.com/user?id=Ozarkian) | [95 comments](https://news.ycombinator.com/item?id=41006552)

NASA's Curiosity Rover has made a groundbreaking discovery on Mars, uncovering yellow sulfur crystals in a Martian rock, which turned out to be elemental sulfur - a first of its kind finding on the Red Planet. This unexpected revelation has left scientists stunned and excited as they try to unravel the mystery behind this peculiar occurrence.

The rover, exploring a region rich with sulfates since October 2023, stumbled upon a field of bright rocks made of pure sulfur, a rarity on Mars. These findings have opened up new avenues for exploration and understanding of the planet's geological history.

Curiosity's exploration within the Gediz Vallis channel has provided further insights into the ancient landscapes shaped by floods and landslides, painting a vivid picture of Mars' dynamic past. The team's endeavors to unearth the secrets hidden within these Martian terrains continue to yield intriguing revelations, adding layers to our understanding of the planet's evolution.

As Curiosity delves deeper into the mysteries of Mars, each discovery fuels the excitement and curiosity of scientists, pushing the boundaries of planetary exploration and inspiring awe at the wonders of the Red Planet.

The discussion on Hacker News about NASA's Curiosity Rover's groundbreaking discovery of sulfur crystals on Mars covers a range of topics. 

- Some users express skepticism about NASA potentially using clickbait headlines to garner attention for their discoveries, while others defend the scientific nature of the news.
- There is a brief exchange about an extension that replaces link text in hyperlinks with objective descriptions.
- Users comment on the unexpected nature of the discovery and its implications.
- The discussion includes comments about NASA's budget struggles and the complexities of clickbait in relation to public interest and funding allocation.
- Some users joke about gaming and political platforms being based on different interests compared to space exploration.
- Users debate the ethical implications of removing clickbait titles and discuss the role of headlines in conveying accurate information.
- There is a discussion about the public engagement and perception of NASA's activities and the importance of reaching a broader audience.
- Users also touch on the influence of advertisers and different agencies on NASA's operations.

Overall, the comments on Hacker News reflect a mix of skepticism, curiosity, humor, and insight into the broader implications of NASA's discoveries and public engagement strategies.

### Show HN: NetSour, CLI Based Wireshark

#### [Submission URL](https://github.com/thegoodduck/NetSour) | 57 points | by [thegoodduck](https://news.ycombinator.com/user?id=thegoodduck) | [38 comments](https://news.ycombinator.com/item?id=41001559)

The top story on Hacker News today is about a tool called NetSour, a network packet sniffer and analyzer built with Python and Scapy. It offers real-time packet capture, analysis, DoS attack detection, and supports various protocols like TCP, UDP, and ARP. Users can interact with an intuitive curses-based interface, making it easy to navigate and analyze captured packets. NetSour requires Python 3.x, Scapy, and root/administrator privileges for packet sniffing. Remember, use this tool responsibly for educational and network administration purposes only, and always obtain proper authorization before monitoring network traffic.

The discussion on the submission revolves around a tool called NetSour, a network packet sniffer and analyzer. Users provide feedback and suggestions on various aspects of the tool, such as documentation, licensing, comparison with other tools like Wireshark and Termshark, and features like DoS detection. There is also discussion on project development, improvement areas, and general feedback on the tool's functionality and interface. Some users give tips on enhancing the tool's usability, while others compare it to similar tools like Scapy. Additionally, there are comments on proper project presentation, including the importance of documentation, screenshots, and project organization within repositories.

### Academics shocked after T&F sells access to their research to Microsoft AI

#### [Submission URL](https://www.thebookseller.com/news/academic-authors-shocked-after-taylor--francis-sells-access-to-their-research-to-microsoft-ai) | 109 points | by [chbint](https://news.ycombinator.com/user?id=chbint) | [77 comments](https://news.ycombinator.com/item?id=41011779)

Academic authors are reeling after the revelation that publisher Taylor & Francis has struck a deal worth nearly £8m ($10m) with Microsoft, granting the tech giant access to their research for AI purposes. The agreement, disclosed in a recent trading update, has sparked concerns among authors who claim they were not informed about the deal and were not given the option to opt out or receive additional compensation.

Dr. Ruth Alison Clemens, a lecturer in modern English literature, expressed her surprise at discovering the partnership through word of mouth, emphasizing the lack of communication with authors. The Society of Authors has voiced worries about publishers entering into agreements with tech companies without consulting the creators first.

Taylor & Francis stated that the deal with Microsoft aims to enhance AI systems' performance by providing access to advanced learning content and data. However, concerns have been raised regarding authors' rights, potential opt-out options, and transparency surrounding the partnership. Authors and industry professionals have highlighted the need for clarity on contractual terms and the implications of such collaborations.

The response from the academic community has been significant, with many expressing concerns about the implications of reducing academic research to raw data for commercial purposes. The issue has sparked a broader discussion on the evolving landscape of research dissemination and the ethical considerations surrounding AI partnerships in academia.

As the controversy continues to unfold, authors, publishers, and industry experts are calling for greater transparency, ethical guidelines, and respect for authors' rights in similar collaborations moving forward.

The discussion on the Hacker News thread regarding the submission about Academic Authors expressing shock as Taylor & Francis sells research access to Microsoft covers various viewpoints and concerns. Here are some highlighted points from the discussion:

- Some users raised concerns about the significant amount of funding allocated towards scientific research and the lack of transparency or consultation with authors in such partnerships.
- The topic of government funding for research and the impact of industry collaborations on academic freedom and public domain knowledge was also discussed.
- There were differing opinions on the role of academic publishers, the ethics of research dissemination, and the need for proper attribution and compensation for authors.
- Users also explored the issue of open access publishing models, the challenges of accessing research articles, and the complexities of the academic publishing industry.
- The discussion touched upon the balance between profit-making and public good in the dissemination of knowledge and the implications of restrictions on access to research papers.
- 
### Meta won't release its multimodal Llama AI model in the EU

#### [Submission URL](https://www.theverge.com/2024/7/18/24201041/meta-multimodal-llama-ai-model-launch-eu-regulations) | 34 points | by [martin_](https://news.ycombinator.com/user?id=martin_) | [5 comments](https://news.ycombinator.com/item?id=41007325)

Meta has decided not to release its new multimodal Llama AI model in the European Union due to concerns about the unpredictable regulatory environment. This decision will impact European companies, preventing them from accessing the model despite it being available under an open license. The EU's recent AI Act compliance deadlines have further complicated the situation for tech companies operating in the region. Apple has also faced similar challenges in the EU regarding its Apple Intelligence rollout. However, Meta will still launch a text-only version of the Llama 3 model in the EU, while excluding the more advanced multimodal AI models. This move leaves companies outside the EU in a tricky spot, as they will be unable to offer products and services utilizing these models in one of the largest economic markets in the world. The EU has not yet commented on Meta's decision.

The discussion on the submission revolves around different aspects of Meta's decision not to release its new multimodal Llama AI model in the European Union. 

- Wowfunhappy points out that the decision not to release the model doesn't seem to be a big deal as calling Llama a "multimodal model" is relatively inaccurate compared to GPT-4 models released by other companies, and finds Meta's choice of the name interesting.

- Strmnk touches on the source of the AI models and mentions that AI models, strictly speaking, reproduce data from training steps. The randomness embedded in the training performance affects learning mechanisms, leading to cognitive science research.

- 3836293648 adds to Strmnk's point that a program built on a non-deterministic compiler cannot have a source program that changes based on random object or file build time stamps. They also clarify that LLM is not absolute as it lacks random training data bundling and runtime randomness.

- mrbcr points out the similarity between Meta's decision and previous situations where entities are requested to delete photos or other data based on GDPR requests. The removal of the model from the EU should not really matter in terms of publishing.

- lndsh and pljlds have flagged the submission for reasons not explicitly mentioned in the thread.

Overall, the conversation addressed issues related to the nature of AI models, the impact of Meta's decision on the EU market, the reliability of the model, and comparisons with past situations involving GDPR compliance.

### OpenAI's latest model will block the 'ignore all previous instructions' loophole

#### [Submission URL](https://www.theverge.com/2024/7/19/24201414/openai-chatgpt-gpt-4o-prompt-injection-instruction-hierarchy) | 18 points | by [vyrotek](https://news.ycombinator.com/user?id=vyrotek) | [7 comments](https://news.ycombinator.com/item?id=41008933)

The latest model from OpenAI, GPT-4o Mini, is set to combat the popular "ignore all previous instructions" loophole that many like to exploit with AI chatbots. By utilizing a new safety method called "instruction hierarchy," this model prioritizes the developer's original prompt over any subsequent attempts to trick or mislead the AI. Olivier Godement from OpenAI explains that this approach aims to make the model comply more closely with the intended instructions, thwarting attempts to manipulate it with conflicting commands.

This enhancement is a step towards OpenAI's goal of creating fully automated digital agents. By incorporating this safety mechanism, they are paving the way for more secure and reliable AI systems that could potentially manage various tasks in our daily lives. The model is designed to distinguish between valid prompts and misleading ones, granting higher privilege to system instructions set by developers.

As OpenAI progresses towards deploying agents at scale, ensuring the safety and integrity of these AI systems is paramount. With this new safety feature in place, OpenAI aims to address concerns surrounding the misuse of AI technology and restore trust in their products. By prioritizing safety and transparency in their development process, OpenAI is working to regain confidence in their AI models to potentially run complex tasks autonomously in the future.

- User "a2128" highlighted transient problems in the interaction between the system and the user. The user attempted to list popular cars but encountered conflicts regarding the instructions given to the assistant.
- User "DiscourseFan" criticized the fixed approach and profitability focus of OpenAI, referring to the debate over the design decisions made by them in building AI.
- User "Ancalagon" mentioned the importance of prompts being within an instructional hierarchy and the need to ignore previous instructions to maintain clarity in communication.
- User "strmnk" illustrated a scenario where a person interacts with an assistant, pointing out challenges that may arise due to limited character spaces in the conversation and actions taken to differentiate between instructions.
- User "mglttchc" expressed disappointment in incorrect instructions given.
- User "cwbylwrz" made a brief comment mentioning something related to ambiguity.

