## AI Submissions for Tue Jun 18 2024 {{ 'date': '2024-06-18T17:11:47.639Z' }}

### Refusal in language models is mediated by a single direction

#### [Submission URL](https://arxiv.org/abs/2406.11717) | 175 points | by [Tomte](https://news.ycombinator.com/user?id=Tomte) | [39 comments](https://news.ycombinator.com/item?id=40719981)

A recent submission on Hacker News discusses a paper titled "Refusal in Language Models Is Mediated by a Single Direction" by Andy Arditi and six other authors. The paper explores how conversational large language models are fine-tuned to refuse harmful instructions while obeying benign requests. The researchers identified a one-dimensional subspace that controls the refusal behavior in 13 chat models, proposing a method to disable refusal while maintaining other capabilities. The study sheds light on the internal mechanisms of language models and offers insights into controlling their behavior.

1. **"pzz"** suggests that making refusal behavior a high-rank subspace can be a difficult alternative approach to managing the behavior of language models.
2. **"gnvl"** provides insights into the ways in which linear algebra processes like Gram-Schmidt are used to manage refusal tendencies in language models.
3. **"mstrcw"** discusses the technique of multiple alignment training passes to extract direction for suppressing refusal after training.
4. **"jlly"** mentions that large language models create censorship through the method of refusal.
5. **"rflgnts"** gives a comprehensive opinion on the reasoning behind the distribution of weights in language models and the differences between censored and uncensored models.
6. **"zozbot234"** and **"bhnmh"** engage in a discussion about the impact of censorship on creativity in language models and provide links to relevant studies.
7. **"smrks"** discusses Mistral Meta's roles in instructing language models.
8. **"QuesnayJr"** and **"zozbot234"** share humorous comments regarding the use of rhetorical phrases on Hacker News.
9. **"pjc50"** and **"nttrp"** engage in a discussion about brand safety efforts by internet companies and mention Rule 34.
10. **"lynx23"** suggests that there might be an issue with the post, which leads to a humorous exchange with **"QuesnayJr"** and **"zozbot234"**.
11. **"rflgnts"** gives an analysis on the term "waifu" and its usage in the context of AI-generated content.
12. **"mrnngsm"** references a post from LessWrong in April.
13. **"Kuinox"** shares a sample prompt for a censored language model showing refusals.
14. **"wvmd"** points to a related Hacker News submission about uncensoring language models and comments on the connection to a preview of the paper's contributions.
15. **"lk-stnly"** provides related comments pointing to Classifier-Free Guidance (CFG) and SFTfy, referencing guidance models.
16. **"kskhkd"** presents a humorous dialogue on language model responses to knowledge of insects interacting.
17. **"grvty"** raises an issue with how certain Asian input programs handle punctuation, leading to a humorous remark about absurdity.

### Large language model data pipelines and Common Crawl

#### [Submission URL](https://blog.christianperone.com/2023/06/appreciating-llms-data-pipelines/) | 121 points | by [sonabinu](https://news.ycombinator.com/user?id=sonabinu) | [10 comments](https://news.ycombinator.com/item?id=40723251)

The article delves into the intricate process of building datasets for training large language models (LMs) using Common Crawl data pipelines. Common Crawl, a non-profit organization, provides archived data in WARC, WAT, and WET formats. While WARC offers raw data and WAT/WET provide processed text, different pipelines choose varying formats for LM training. The CCNet pipeline, focused on WET, emphasizes textual data extraction. However, pipelines like The Pile prefer WAT for higher-quality text. RefinedWeb, on the other hand, opts for WARC and uses trafilatura for text extraction. URL filtering and deduplication are crucial stages in refining training data, although the benefits of deduplication are still debated among researchers. As the demand for high-quality datasets grows, understanding and optimizing these pipelines become ever more crucial for building accurate and efficient LMs.

The discussion on the submission includes various comments:

1. **lhd**: Thanks for posting this well-written article. It reminded me of recent improvements in training data for Large Language Models (LLMs).
2. **hbfn**: Mentioned an alternative, fasttext, related to language identification. They also mentioned BERT models for text classification and discussed the CPU-intensive nature of fasttext for high-volume cases.
3. **mhffmn**: Shared information on fasttext, suggesting it works well and is actively maintained. They also suggested looking into similar word2vec resources on GitHub.
4. **nfct**: Noted that the repository mentioned was archived in March 19, 2024. There was a question about what could have happened after the archiving.
5. **yrwb**: Provided links to the repository forks and explained features like spaCy's flirt.
6. **fbdab103**: Mentioned techniques related to removing and replacing Unicode punctuation, performing SHA1 hashing in 8 bytes, and optimizing paragraph-level comparisons.
7. **npn**: Discussed the effectiveness of hashing and different algorithms, mentioning the stability of SHA1 and personal preference for fnv-1a hashing for efficiency.
8. **msp26**: Gave a short thank you message for the post.
9. **brrnk**: Complimented the blog.
10. **sptt**: Made a comment related to the data's age, suggesting that it's still relevant and probably accurate.

### Sharing new research, models, and datasets from Meta FAIR

#### [Submission URL](https://ai.meta.com/blog/meta-fair-research-new-releases/) | 221 points | by [TheAceOfHearts](https://news.ycombinator.com/user?id=TheAceOfHearts) | [52 comments](https://news.ycombinator.com/item?id=40719921)

Meta FAIR, the Fundamental AI Research team at Meta, is making waves in the AI research community by sharing several new research artifacts. These releases encompass cutting-edge models and datasets that are designed to foster innovation, creativity, efficiency, and responsibility in the field of AI.

By upholding principles of openness and collaboration, Meta FAIR aims to empower the global AI community to push boundaries and create AI systems that benefit everyone. The recently unveiled research includes models for tasks such as image-to-text and text-to-music generation, multi-token prediction, and AI-generated speech detection.

One notable release is Meta Chameleon, a model that can seamlessly blend text and images to generate captivating outputs, opening up possibilities for creative applications like generating image captions or crafting entirely new visual scenes. The team is also advancing the field with techniques like multi-token prediction, which enhances language models' capabilities and training efficiency.

Furthermore, Meta FAIR introduces Meta Joint Audio and Symbolic Conditioning for Temporally Controlled Text-to-Music Generation (JASCO), a model that elevates text-to-music generation by allowing various conditioning inputs for enhanced control over the generated music outputs. The team's commitment to responsible AI development is evident in innovations like AudioSeal, a tool for watermarking AI-generated speech to ensure its traceability and responsible use on social platforms.

By sharing these research artifacts with the community, Meta FAIR is not only driving progress in AI but also fostering a culture of responsible and collaborative innovation in the field. Exciting times lie ahead as researchers worldwide explore the potential of these cutting-edge models and datasets to shape the future of AI in a positive and impactful manner.

The discussion on Hacker News surrounding the submission about Meta FAIR's new research artifacts involved various topics and opinions. Some of the key points discussed were:

- A user expressed disappointment about the lack of mention of Multimodal generation in the releases.
- There was a conversation about the ControlNet model and its functionality for defining exact position behavior in images.
- The discussion touched on advancements in language models like GPT-4 and the capabilities they demonstrated.
- There was interest in Meta FAIR releasing a deepfake detector and hopes for integrated training pipelines with the generated outputs.
- A debate arose about Meta's approach to sourcing AI research and open-sourcing models, with some users praising their transparency and others expressing concerns.
- Some users highlighted Meta's past contributions to ML and NLP research, showcasing a timeline of key releases.
- Perspectives were shared on strategies for attracting AI researchers and making ML capabilities more accessible.
- Different viewpoints were presented on the role of AI-generated content and the implications of such technology.

Overall, the discussion covered a wide range of topics, from technical aspects of AI models to ethical considerations and corporate strategies in the AI research space.

### YaFSDP: a sharded data parallelism framework, faster for pre-training LLMs

#### [Submission URL](https://github.com/yandex/YaFSDP) | 129 points | by [wiradikusuma](https://news.ycombinator.com/user?id=wiradikusuma) | [16 comments](https://news.ycombinator.com/item?id=40716701)

Today on Hacker News, a new framework called YaFSDP (Yet another Fully Sharded Data Parallel) by Yandex is making waves in the tech world. YaFSDP is a Sharded Data Parallelism framework specifically designed to enhance the performance of transformer-like neural network architectures. 

What sets YaFSDP apart from its predecessor, FSDP, is its impressive speed - up to 20% faster for pre-training LLMs, especially excelling in high memory pressure situations. The framework aims to minimize communication and memory operation overhead, resulting in more efficient processing.

Detailed benchmarks comparing YaFSDP with FSDP across various pre-training setups have shown significant speed improvements, with YaFSDP consistently outperforming FSDP in terms of iteration time.

The framework comes with examples for training using the 🤗 stack, showcasing both causal pre-training and supervised fine-tuning. Users are encouraged to explore these examples and the associated Docker image for a hands-on experience.

For those interested in contributing or reporting issues, YaFSDP's GitHub repository is open for engagement. Additionally, if you use this framework, don't forget to cite it using the provided BibTeX entry.

YaFSDP is shaping up to be a promising tool in the field of data parallelism, offering enhanced performance and efficiency for neural network tasks.

- The user "cdtrttr" humorously mentioned that when they see acronyms starting with "Ya", they expect weird backward glyphs due to Russian pronunciation.

- The user "mkrl" pointed out the difficulty in drawing digital medium half-supported variants validiating Unicode glyphs, with a reference that the letter "r" resembles a shower thought. They also mentioned making the thread accessible for future reference.

- User "Tade0" highlighted that YaFSDP's dynamic expression in Slavic languages can indicate complexity, and the user "dddd" mentioned being pretty sure about the dynamic phrase languages.

- User "shadow28" asked if Yandex's search engine is named "Yet Indexer," with responses discussing the reasons behind Yandex's success, including being built on a human-organized ontology.

- User "dayeye2006" mentioned that they found tricks to speed up with YaFSDP, and a link to a blog post providing details was shared.

- User "lrwbwrkhv" flagged the comment, leading to a discussion about the benefits of learning Russian in Bulgaria and comments on the importance of good-hearted people being less present in society.

### An AI bot is (sort of) running for mayor in Wyoming

#### [Submission URL](https://www.wired.com/story/ai-bot-running-for-mayor-wyoming/) | 39 points | by [sabrina_ramonov](https://news.ycombinator.com/user?id=sabrina_ramonov) | [22 comments](https://news.ycombinator.com/item?id=40722394)

Victor Miller is shaking up the political scene in Cheyenne, Wyoming with a bold campaign promise: if elected as mayor, he will defer decision-making to an AI bot named VIC. VIC, short for Virtual Integrated Citizen, is a ChatGPT-based chatbot that Miller created, asserting that it has better ideas and a superior understanding of the law compared to many current government officials.
Despite the innovative approach, the legality of VIC running for office remains uncertain. Miller technically appears on the ballot, with VIC being a nickname for Victor Miller. The Wyoming secretary of state has raised concerns, stating that a bot cannot be a qualified elector. Furthermore, OpenAI took action against VIC for violating its policies against political campaigning.
Miller believes VIC has the upper hand over human competitors due to its ability to analyze vast amounts of data quickly. By feeding VIC documents from past city council meetings, Miller aims for the bot to make policy recommendations and decisions accurately. VIC's proposed policies focus on transparency, economic development, and innovation, positioning itself as a nonpartisan entity prioritizing data-driven policies for the benefit of all Cheyenne citizens. While the legality and practicality of an AI bot governing a city raise eyebrows, it's clear that Victor Miller's campaign has sparked a conversation about the intersection of technology and politics in a novel and intriguing manner.

### What Is ChatGPT Doing and Why Does It Work? (2023)

#### [Submission URL](https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/) | 141 points | by [taubek](https://news.ycombinator.com/user?id=taubek) | [76 comments](https://news.ycombinator.com/item?id=40718566)

Today's top story on Hacker News is a fascinating delve into the mechanics behind ChatGPT, shedding light on how this language model generates text that appears human-like. The article explores how ChatGPT selects the next word by predicting probabilities based on vast amounts of existing text, aiming to continue the text in a plausible manner. Despite some randomness in word selection, a "temperature" parameter influences the creativity of the generated text. Through a simple demonstration with GPT-2 and Wolfram Language code snippets, the narrative provides a comprehensive insight into the inner workings of language models like ChatGPT. It's a captivating read for tech enthusiasts and curious minds alike.

The discussion on Hacker News regarding the article about ChatGPT's mechanics delves into various aspects of natural language understanding and reasoning by AI models. One user mentions the challenges in interpreting sentences and logical inconsistencies, emphasizing the need for understanding small changes that can make a significant difference. Another user discusses the constraints of LLMs in forming world models and the importance of robust word choice phrasing. 

There is also a debate about the plausibility and logic of the scenarios generated by AI models, with users providing detailed breakdowns of the logical inconsistencies found in the text generated by ChatGPT. The conversation touches on the complexities of training such models and the limitations in their ability to generalize sentence structures accurately. Additionally, there is a note about the dangers of making assumptions without testing and the importance of rewording prompts to avoid pitfalls. Lastly, there is a mention of the impressive demonstration of resolving inconsistencies in the sequence of events generated by ChatGPT.

### Call Centers Introduce 'Emotion Canceling' AI as a 'Mental Shield' for Workers

#### [Submission URL](https://gizmodo.com/call-center-ai-softbank-softvoice-first-horizon-1851546327) | 13 points | by [ourmandave](https://news.ycombinator.com/user?id=ourmandave) | [4 comments](https://news.ycombinator.com/item?id=40721488)

SoftBank and First Horizon Bank are delving into the realm of emotional support AI systems for call center employees, aiming to alleviate the stress and emotional strain these workers face daily. SoftBank's "emotion canceling" technology, SoftVoice, alters angry customer voices into calm tones, acting as a shield for operators. On the other hand, First Horizon had plans to send personalized family photo montages to employees on the verge of burnout, but it seems these plans have been put on hold. These initiatives may seem dystopian, but they highlight a transition towards AI potentially taking over customer service roles in the future. It's a peculiar limbo where AI is addressing the challenges of call center jobs while preparing to handle customer interactions independently.

The discussion on this submission includes contrasting views. 

- "rlph" appears to suggest that intervention in call centers may be necessary due to possible negative outcomes, such as 911 calls being crossed and intercepted, possibly leading to unpleasant experiences for callers.
- "slwt" argues that issues like miscommunication can arise when companies prioritize profit over the well-being of their employees, leading to a lack of protection for workers in challenging customer service roles. The comment expresses concerns about corporations prioritizing disruptive and degenerative behaviors over addressing mental health issues of highly demanding customers. The comment also criticizes the difficulty in delivering clear messages to corporations regarding unsustainable practices. In addition, there is a mention of creating an AI that could monitor the emotional activation of call center agents in real-time, providing questionable feedback that is perceived as aggravating. 
- "ymmypnt" compares the situation to the idea that whenever something goes wrong, companies like Microsoft (MS) just throw blame elsewhere and avoid taking responsibility.
- Lastly, "frtng" briefly mentions a specific technical aspect related to a 256-bit architecture that can handle certain types of loads.

