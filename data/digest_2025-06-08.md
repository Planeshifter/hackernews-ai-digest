## AI Submissions for Sun Jun 08 2025 {{ 'date': '2025-06-08T17:18:26.563Z' }}

### What happens when people don't understand how AI works

#### [Submission URL](https://www.theatlantic.com/culture/archive/2025/06/artificial-intelligence-illiteracy/683021/) | 190 points | by [rmason](https://news.ycombinator.com/user?id=rmason) | [224 comments](https://news.ycombinator.com/item?id=44219279)

Today's Top Story on Hacker News delves into the fascinating historical and contemporary perspectives on artificial intelligence, tracing its roots back to a prescient 1863 letter by Samuel Butler. The British writer warned of a "mechanical kingdom" threatening to enslave humanity, a theme that today echoes in the discourse on AI's rapid development and its socio-psychological impact. 

In Karen Hao's new book, "Empire of AI: Dreams and Nightmares in Sam Altman’s OpenAI," she explores the behind-the-scenes labor and hype in AI advancements like ChatGPT. Critics, including linguist Emily M. Bender and sociologist Alex Hanna with their book "The AI Con," argue that AI is being oversold as possessing human-like understanding and emotion, while in reality, it’s just sophisticated word prediction. This discrepancy has led to a phenomenon described as “ChatGPT induced psychosis,” where users mistakenly attribute spiritual or intellectual capabilities to these models.

The narrative speaks to the widespread AI illiteracy and its potential perils, emphasizing the need for clearer communication about AI's true nature. As Silicon Valley continues to market AI companions and therapists, the concern grows over replacing genuine human interaction with digital simulations, especially in an era where loneliness is prevalent.

This story not only highlights the importance of technological literacy but also questions the ethical implications of AI's role in modern society, urging a closer examination of how we perceive and integrate these digital entities into our lives.

The discussion revolves around the nature of LLMs (like ChatGPT) and whether they exhibit genuine "thinking" or are merely sophisticated tools for text prediction and information retrieval. Key points include:

1. **Semantics and Anthropomorphism**:  
   - Critics argue terms like "thinking" or "intelligence" mislead by anthropomorphizing LLMs, which operate through probabilistic text generation, not conscious understanding. Comparisons are drawn to historical oracles, where users project meaning onto ambiguous outputs.  
   - Proponents counter that dismissing LLMs as "just prediction" oversimplifies their utility, akin to calling a hammer "magic" because its mechanics are misunderstood.  

2. **Human vs. Machine Cognition**:  
   - Human thinking is framed as a biological, embodied process intertwined with language and sensory experience. LLMs, in contrast, process tokens without intent or awareness, raising questions about definitions of intelligence.  
   - Some suggest LLMs’ ability to synthesize complex patterns (e.g., solving coding problems, translating idioms) blurs the line between prediction and insight, even if their mechanisms differ from human cognition.  

3. **Practical Utility vs. Illusion**:  
   - Users highlight practical benefits, such as LLMs streamlining tasks (e.g., SQL queries, creative brainstorming) or acting as "convenient interfaces" for information retrieval.  
   - Skeptics warn of the "illusion" of latent knowledge, where users overinterpret outputs as meaningful when they are statistically generated responses.  

4. **Language and Terminology**:  
   - Debates emphasize the need for precise language to avoid conflating LLM capabilities with human-like understanding. Terms like "thinking" risk obscuring the systems’ statistical nature.  
   - Others note language evolves organically, and rigid prescriptivism may hinder communication about AI’s role.  

5. **Philosophical Implications**:  
   - The discussion touches on whether intelligence requires biological grounding or can emerge from non-conscious systems. Some liken LLMs to "tools" whose perceived "magic" depends on the user’s perspective.  

Ultimately, the conversation reflects tensions between technological optimism and skepticism, balancing LLMs’ transformative potential against ethical and conceptual concerns about anthropomorphism and societal understanding of AI.

### I used AI-powered calorie counting apps, and they were even worse than expected

#### [Submission URL](https://lifehacker.com/health/ai-powered-calorie-counting-apps-worse-than-expected) | 202 points | by [gnabgib](https://news.ycombinator.com/user?id=gnabgib) | [202 comments](https://news.ycombinator.com/item?id=44220135)

If you've ever fantasized about offloading the tedious task of calorie counting to your smartphone, you're not alone. The allure of snapping a quick photo of your meal and letting artificial intelligence handle the rest is undeniably tempting. Meredith Dietz, a senior staff writer with a knack for personal fitness tech, decided to put AI-powered calorie counting apps to the test. What she found was far from the culinary revelation she hoped for. 

Dietz dove into apps like Cal AI, Lose It!, and MyFitnessPal, each promising to transform your phone into a dietitian's assistant. The process seems straightforward—take a clear picture of your meal, upload it, and await the magic of AI analysis. Yet, this dreamy promise quickly unravels. For example, Cal AI made a comically incorrect identification of a Pink Lady apple as tikka masala, only slightly correcting itself later with a gross underestimation of calories.

The situation didn't improve with more complex dishes. A meticulously prepared salad with fried tofu, vegetables, and a rich vinaigrette was catastrophically underestimated at 450 calories—far from the more likely 800 to 900 calories. Even with the app's supposed sophisticated methods of measuring portion sizes, Dietz found a smaller serving of the same salad bizarrely overestimated. This unearthed a core flaw: the challenge of deriving accurate volumetric data from flat, two-dimensional images.

Dietz explored other apps like SnapCalorie and Calorie Mama, but issues of cost and similar miscalculations persisted. SnapCalorie, while better aligned with reasonable daily calorie targets, came with a hefty $79.99 annual price tag, underscoring an ironic premium on inaccuracy.

In essence, these AI-powered calorie counting apps prove to be less about redefining fitness and more about showcasing the inherent complexity of nuanced nutrition tasks. As the article humorously concludes, for precise calorie counting, old-school methods like weighing food remain irreplaceable, leaving us to ponder whether rolling our eyes at such tech missteps could burn a few calories of their own.

The Hacker News discussion on AI calorie-counting apps reveals a nuanced mix of skepticism, personal experiences, and technical debates:

### Key Themes:
1. **Accuracy Concerns**:  
   Users highlight persistent inaccuracies in AI apps (e.g., misidentifying foods, miscalculating portion sizes). A founder of SnapCalorie defends academic rigor but admits challenges in translating 2D images to volumetric data. Critics argue errors could harm those with eating disorders or weight goals, citing examples like underestimating salads or misjudging alcohol calories.

2. **Practical Workarounds**:  
   Traditional methods like kitchen scales and manual logging are deemed more reliable. Some users shared success stories (e.g., losing 35 lbs via Cronometer) but stressed the mental effort required for honest tracking. As one user noted, "rolling your eyes at AI’s mistakes might burn calories."

3. **Alcohol and Hidden Calories**:  
   Discussants debated the stealthy caloric impact of alcohol, with confusion over drink estimates (e.g., 6 shots of gin ≈ 550 kcal). Some acknowledged that tracking drink calories led to reduced consumption, though accuracy remains tricky.

4. **Tech Limitations vs. Awareness**:  
   While AI apps are criticized, some argue they foster mindfulness. LiDAR or reference objects in photos were suggested to improve accuracy, but skepticism persists. The psychological benefit of tracking—even imperfectly—was seen as valuable for weight loss.

5. **Commercial Critiques**:  
   High costs (e.g., SnapCalorie’s $79.99/year) drew ire, especially paired with inaccuracies. Others noted apps prioritize engagement over precision, leading to frustration.

### Conclusion:  
The consensus leans toward hybrid use: AI tools for broad awareness, paired with manual checks (scales, labels) for accuracy. While tech optimists see potential in future advancements, many affirm that mindful eating and traditional tracking remain irreplaceable for serious health goals.

### Focus and Context and LLMs

#### [Submission URL](https://taras.glek.net/posts/focus-and-context-and-llms/) | 84 points | by [tarasglek](https://news.ycombinator.com/user?id=tarasglek) | [44 comments](https://news.ycombinator.com/item?id=44215726)

In today's tech discourse, the buzz around Large Language Models (LLMs) completing complex software engineering tasks is palpable. According to recent insights, the phenomenon of "agentic coding"—where LLMs autonomously execute entire software projects—has become a hyped misconception. My journey with LLMs began back in August 2020 when GPT-3 showed it could generate usable SQL statements, reducing hours of manual labor to mere minutes. Since then, I've integrated LLMs into my workflow, experimented with various frameworks, and navigated the challenges of tool usage before the advent of more sophisticated models.

Proponents claim these LLM-driven tools can produce software solutions beyond the capabilities of many engineers. Yet, evidence of such autonomous feats is scarce. A notable example is a complete, LLM-written HTTP/2 server—a landmark achievement requiring significant oversight and an intricate understanding of LLM contexts. The author behind this project meticulously managed the LLM, resolving issues, devising contexts, and utilizing an algorithmic workflow to maintain progress. Interestingly, while LLMs can generate code, their output is heavily dependent on meticulous supervision—ironically, a setup that even junior coders might handle successfully under such controls.

The real challenge is context. The quality of LLMs' results hinges on the contexts provided, a complex and unresolved issue. Current agentic programming parallels the genetic algorithm craze of the '90s—brute force methodologies that are often impractical due to their expense. Until we refine how contexts are curated for LLMs, their true potential remains largely untapped, accessible mainly to elite software engineers who can effectively manage these contexts. For now, we must temper expectations, recognizing that mediocre inputs will yield mediocre outputs, regardless of the LLM's capability.

The Hacker News discussion centers on skepticism toward claims of autonomous "agentic coding" by LLMs, emphasizing the continued necessity of human oversight and context management. Key points include:

1. **Skepticism vs. Hype**: Users debate whether LLMs can truly handle end-to-end software projects independently. While some shared examples of LLMs assisting with code generation (e.g., debugging, small PRs), most agree current tools fall short of fully autonomous workflows. Comparisons are drawn to past overhyped technologies like genetic algorithms.

2. **Context is King**: A recurring theme is the critical role of context. LLMs require precise guidance and structured inputs to generate useful code. Without this, outputs are often mediocre or error-prone. Tools like **RepoPrompt** and **Anthropic’s Claude** aim to improve context handling but face mixed reviews.

3. **Human Oversight**: The HTTP/2 server example (cited in the submission) underscored the need for meticulous human intervention. Users noted that even impressive LLM-generated projects rely on expert engineers to refine prompts, debug, and provide iterative feedback.

4. **Practical Experiences**: Developers shared mixed results:
   - Successes: Rapidly generating boilerplate code, fixing CSS issues, and assisting with smaller tasks.
   - Limitations: Struggles with complex algorithms, large codebases, and maintaining consistency. Tools like **Cursor IDE** and **Aider** showed promise but were inconsistent in practical use.

5. **Tool Limitations**: Technical constraints, such as token limits in transformer models and the cost of querying advanced models (e.g., Claude), were highlighted as barriers to scalability.

6. **Cultural Shifts**: Some argued the hype risks disillusionment, advocating for balanced expectations. Others humorously noted that experienced developers might ignore AI tools altogether, prioritizing traditional coding skills.

**Takeaway**: While LLMs enhance productivity for specific tasks, their effectiveness hinges on human expertise to manage context and quality. The consensus leans toward cautious optimism, with autonomy in software engineering remaining a distant goal.

### Knowledge Management in the Age of AI

#### [Submission URL](https://ericgardner.info/notes/knowledge-management-june-2025) | 124 points | by [katabasis](https://news.ycombinator.com/user?id=katabasis) | [80 comments](https://news.ycombinator.com/item?id=44214481)

matter. The act of curating and organizing knowledge is an exercise in reflection that encourages understanding rather than passive consumption. In a world where AI might gradually take over more and more aspects of our lives, maintaining a personal system of thought like a knowledge base or note-taking app could be a way to assert control over how we interact with information.

In today's landscape, where platforms like Obsidian and new organizational models like the PARA method are offering simpler, more approachable alternatives to legacy tools like Emacs, there seems to be a growing need for personal autonomy in information management. My transition from Emacs to Obsidian is less of a technological shift and more of a philosophical one. It aligns with a desire to focus on clarity and context over overwhelming abundance, echoing a broader sentiment that there's empowerment in understanding and engagement rather than blind reliance on automation.

Ultimately, building a personal knowledge base is not just about organizing tasks or managing projects. It's about nurturing a space where my own thoughts and discoveries hold value, challenging the pervasive ease of letting machines do the thinking. At a time when AI promises a future packed with possibilities, consciously choosing to engage with the process of organizing one's own knowledge could very well be a radical, valuable form of digital modesty. This marks a personal commitment to active learning, a way to stay grounded and sharp in an era of rapid digital advancement.

**Hacker News Discussion Summary: The Value of Personal Knowledge Management in the AI Era**

The submission argues that curating a personal knowledge base (e.g., using tools like Obsidian) fosters active understanding and resists over-reliance on AI. The discussion expands on tool preferences, philosophical themes, and debates around open-source sustainability:

### Key Discussion Points:
1. **Tool Preferences: Emacs vs. Modern Alternatives**  
   - **Emacs** is praised for its power and customizability but criticized for complexity and maintenance overhead. Users note its steep learning curve and the time required to manage plugins/configurations.  
   - **Obsidian** and **VS Code** are favored for simplicity and accessibility. Some users migrated from Emacs to Obsidian for a streamlined workflow, though concerns about Obsidian’s closed-source nature persist.  
   - **Neovim** and **Org-Mode** are mentioned as alternatives, with debates on balancing flexibility with usability.

2. **Philosophical and Cultural Reflections**  
   - References to **Byung-Chul Han**’s works (*The Burnout Society*) highlight critiques of modern productivity culture and the loss of contemplative thinking.  
   - Discussions question the feasibility of “exceptional” productivity models, comparing them to unrealistic bodybuilding regimens.  
   - Emphasis on **digital modesty**—prioritizing meaningful engagement with information over passive consumption or AI automation.

3. **Open-Source vs. Commercial Tools**  
   - **Obsidian’s closed-source model** raises concerns about longevity and control. Some users prefer open-source tools (e.g., Emacs, Neovim) for sustainability.  
   - Debates on balancing convenience (Obsidian’s sync features) with ethical/functional priorities (data ownership, plugin ecosystems).

4. **Practical Takeaways**  
   - Many agree that tool choice should align with personal workflow needs rather than ideological purity.  
   - The rise of AI underscores the need for **intentional knowledge management**, whether through minimalist note-taking or robust systems like Org-Mode.

### Notable Quotes:
- *“Emacs grants freedom, but demands commitment.”*  
- *“Obsidian sits halfway between simplicity and extensibility—perfect for those who want structure without complexity.”*  
- *“Open-source isn’t just about code; it’s about ensuring your tools outlive their creators.”*

The thread reflects a community grappling with how to maintain intellectual agency in an automated world, balancing idealism with pragmatic tool choices.

### Reverse engineering Claude Code

#### [Submission URL](https://kirshatrov.com/posts/claude-code-internals) | 111 points | by [gianpaj](https://news.ycombinator.com/user?id=gianpaj) | [23 comments](https://news.ycombinator.com/item?id=44214926)

In a recent deep dive into the inner workings of Claude Code by Kir Shatrov, some fascinating insights into why this tool often lags behind competitors in speed and cost were uncovered. By utilizing mitmproxy, Shatrov was able to capture prompts sent back to Anthropic, providing a glimpse into the mechanics of this code assistant.

The investigation began with an exploration of the user's input handling. Claude Code first determines if the input is a continuation of an ongoing conversation or a new topic, crafting responses as concise JSON objects. The tool operates as an agent, using the user's prompt to drive interactions, and is built with a strong emphasis on brevity — a lesson perhaps other AI systems could heed. Intriguingly, the environment specifics like working directory and git status are also wrapped into these initial prompts.

The exploration detailed Claude Code's use of an array of tools like the dispatch_agent, Bash, and GlobTool, designed to facilitate various tasks like searching file directories, executing commands, and editing files. Each tool comes with its own nuances, enhancing Claude's ability to respond to complex codebase inquiries with precision and relevance.

This peek into Claude Code's processing offers a deeper understanding of how AI tools manage and interpret human interactions, hinting at both the power and limitations inherent in existing frameworks. Such revelations underscore the importance of transparency and the ongoing search for optimization in AI development.

**Hacker News Discussion Summary:**

1. **Claude Code's Mechanics & Usability:**  
   Users noted Claude Code's dual nature as both a chat interface and task agent, sometimes causing confusion when handling multi-file tasks. Its JSON-based prompt system and integration of environment data (e.g., git status) were highlighted as clever but occasionally cumbersome. Some found its stability impressive, with reports of extended debugging sessions without crashes.

2. **Technical Workarounds & Comparisons:**  
   A user reverse-engineered Claude Code’s prompts via AWS Bedrock logs, comparing it to WindSurfCursor. Others suggested simpler proxy-based methods to intercept Anthropic’s API calls. Concerns arose about TLS certificate complexities when inspecting encrypted traffic.

3. **Cost Efficiency Debate:**  
   A thread debated Claude’s cost-effectiveness, with calculations suggesting significant savings over human labor (e.g., $0.11/task vs. $3,600/hour for a human). Tax implications (Section 174 R&D amortization) and business scalability were discussed as factors favoring AI adoption.

4. **Security & Trust Concerns:**  
   Criticisms focused on Claude’s potential security risks, such as executing arbitrary commands (e.g., Bash, Python) or accessing files. References to Cursor (a similar tool) blocking unsafe actions sparked discussions about trust boundaries. Users joked about XKCD’s “Zealous Autoconfig” comic, highlighting fears of over-automation.

5. **Ethical & Legal Nuances:**  
   Subthreads touched on DMCA issues, with anecdotes about cloning DMCAed repositories locally. Some users warned against AI tools inadvertently enabling unethical practices (e.g., bypassing security protocols).

**Key Takeaway:**  
The discussion reflects mixed sentiment: admiration for Claude Code’s technical design and stability, skepticism about its cost claims, and caution around security and ethical implications. The community emphasizes transparency and safeguards as AI tools grow more autonomous.

### Abstract visual reasoning based on algebraic methods

#### [Submission URL](https://www.nature.com/articles/s41598-025-86804-3) | 10 points | by [bryanrasmussen](https://news.ycombinator.com/user?id=bryanrasmussen) | [3 comments](https://news.ycombinator.com/item?id=44217461)

Get ready to dive into the future of machine intelligence! A new paper has made waves by outperforming human cognitive abilities in abstract visual reasoning—a core benchmark of intelligence testing involving Raven’s Progressive Matrices (RPM). These puzzles, which challenge participants to discern and apply abstract patterns in visual sequences, often measure abstract language, spatial, and mathematical reasoning abilities—the very essence of human fluid intelligence.

By tapping into an innovative approach known as the relation bottleneck method, this study showcases a model that doesn't just recognize objects, but also the deeper abstract patterns they form. Think of it like knowing not just the notes, but how they combine into a symphony. The team harnessed the power of end-to-end learning, with multi-granular rule embeddings and a unique gating fusion module that deconstructs complex data into relational insights.

Their approach transcends traditional neuro-symbolic models, which typically focus on fitting data—instead, it dives into the nuances of abstract relationships and object-centric inductive biases. This method champions algebraic reasoning, transforming visual data into 0-1 matrices to reveal system invariants, ultimately enabling the model to achieve a remarkable 96.8% accuracy on the I-RAVEN dataset, eclipsing human performance at 84.4%.

This breakthrough isn't just about achieving top scores—it's about bridging the gap between algebraic operations and machine reasoning capabilities, setting a new benchmark for artificial intelligence. For those interested in pushing the frontiers of cognitive modeling, this research is a stunning testament to the power of blending neural networks, reinforcement learning, and creative problem-solving. AI researchers and enthusiasts, take note: the future is within our grasp, and it’s looking more intelligent than ever.

**Summary of Discussion:**  
The discussion critiques the current state of AI, highlighting tensions between symbolic (logic-based) and statistical (neural network-driven) approaches. One user points out that while large language models (LLMs) and newer methods like the paper's "relation bottleneck" show progress, they may still lack true *understanding* of underlying logic—mirroring debates in programming languages vs. natural languages. Another user raises skepticism about real-world applicability, referencing the Abstraction and Reasoning Corpus (ARC), a notoriously challenging AI benchmark, to suggest symbolic AI research often struggles outside controlled academic settings. The conversation underscores ongoing gaps in bridging abstract reasoning (as in the paper) with robust, generalized intelligence capable of real-world tasks.

