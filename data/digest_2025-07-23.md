## AI Submissions for Wed Jul 23 2025 {{ 'date': '2025-07-23T17:17:07.716Z' }}

### CARA – High precision robot dog using rope

#### [Submission URL](https://www.aaedmusa.com/projects/cara) | 930 points | by [hakonjdjohnsen](https://news.ycombinator.com/user?id=hakonjdjohnsen) | [158 comments](https://news.ycombinator.com/item?id=44661846)

CARA, the latest innovation in robotic quadrupeds, stands out for its groundbreaking use of capstan drives, foregoing traditional gears and pulleys. This unique feature not only positions CARA ahead of her predecessors like ZEUS, ARES, and TOPS but also places her as the second-ever quadruped to adopt such technology after Stanley. Capstan drives, known for their zero backlash, high torque transparency, low inertia, and quiet operation, are perfect for robotics, bringing significant advantages, especially in achieving accurate gear ratios—an aspect thoroughly explored and refined by CARA's creator.

Embarking on this project, the primary challenge was attaining an exact 8:1 gear ratio using capstan drives. Initial attempts were slightly off-mark due to an oversight in calculating the effective diameters of the drums where rope wraps around, akin to misconceptions in a notorious SAT question from 1982. The solution involved a meticulous process of trial, error, and calculation, culminating in an almost perfect 8.000619:1 ratio, achieved through linear interpolation and rigorous testing.

CARA's leg design epitomizes innovation and efficiency. The use of a coaxial 5-bar linkage—rare in quadrupeds—ensures even load distribution, compactness, and a unique mechanical structure. Powered by robust motors and driven by advanced electronics like the ODrive S1 FOC Controllers, each leg integrates seamlessly into the robot’s architecture, with high-strength materials like PET-CF and Polycarbonate ensuring durability under stress.

The robot's design is impressively straightforward yet effective, centering around four legs attached to a carbon fiber frame that offers unmatched strength-to-weight benefits. This structural efficiency is complemented by strategically placed electronics boxes and robust TPU components like its handle and feet.

CARA's creation marks a significant leap in robotic design, showcasing the potential and versatility of capstan drives in robotics, all while coupling creativity with precise engineering. The journey and insights shared, including available CAD files and further technical resources, invite enthusiasts and engineers alike to explore and expand the boundaries of robotic development.

The discussion around CARA, the robotic quadruped with capstan drives, revolves around several key themes:

### Technical Innovation & Design
- Users praised CARA's capstan drives for **zero backlash, high torque, and compact design**, with comparisons to historical uses in film equipment. The coaxial 5-bar linkage and material choices (e.g., Dyneema, Kevlar) were highlighted for improving strength, elasticity, and durability.
- Debates emerged on **material trade-offs** (e.g., Dyneema vs. steel cables) and engineering challenges like backlash compensation, linear interpolation, and gear ratio precision. Some noted parallels to DIY robotics projects, such as surgical robots using similar mechanisms.

### Content Discovery & Algorithms
- Many discussed **YouTube's recommendation algorithm**, arguing it both helps and hinders niche technical creators. While it occasionally surfaces high-quality, small channels (e.g., Aaeds’ "Breaking Taps"), users lamented its bias toward popular content and difficulty in reliably discovering specialized topics. Some attributed CARA’s visibility to algorithmic "luck," emphasizing the struggle for smaller creators to compete.

### Creator Appreciation & Accessibility
- Aaeds’ **dedication, presentation style, and technical depth** were widely applauded. Commenters admired the video’s production quality and the project’s open-source ethos (shared CAD files, detailed documentation).
- Discussions highlighted how platforms like YouTube democratize access to advanced engineering knowledge, though concerns were raised about **content saturation** and the need for better search/discovery tools.

### Corporate vs. Independent Work
- Users noted the rarity of CARA’s **corporate sponsorship** (via Automata) in a landscape dominated by academic or hobbyist projects, sparking interest in sustainable funding models for niche engineering endeavors.

In summary, the thread blends technical admiration for CARA’s design, critiques of content algorithms, and appreciation for accessible engineering education, reflecting a community passionate about both innovation and knowledge-sharing.

### AI overviews cause massive drop in search clicks

#### [Submission URL](https://arstechnica.com/ai/2025/07/research-shows-google-ai-overviews-reduce-website-clicks-by-almost-half/) | 619 points | by [jonbaer](https://news.ycombinator.com/user?id=jonbaer) | [700 comments](https://news.ycombinator.com/item?id=44663227)

In a recent deep dive, the Pew Research Center unveils a striking paradigm shift in online behavior, all thanks to Google's new AI Overviews. Released last May as part of Google's "search generative experience," these AI-driven summaries have gradually crept into more search results and are altering the way users interact with information online. Pew's analysis of data from 900 test users starkly reveals that the click-through rate on search results plummets from 15% without AI to a mere 8% with AI Overviews. For those who remain skeptical about the changes in web traffic, this study is a strong rebuttal against Google's assurances that AI benefits web publishers. Despite this, Google maintains its stance, arguing that AI enhances user experience and creates more opportunities for website engagement.

The study highlights a worrying trend: users are abandoning further exploration after absorbing AI-delivered summaries that could potentially carry inaccuracies due to the "hallucinations" known in generative AI. Currently, roughly 1 in 5 Google searches includes an AI summary, especially for queries structured as questions. With the tech titan opposing Pew's findings, calling their methodology "flawed," the debate over AI's role in shaping search engines and its repercussions for website traffic intensifies. Amidst these changes, Google's profits soar, underscoring the complex relationship between innovation and traditional web publishing. Ryan Whitwam, a senior technology reporter at Ars Technica, documents this ongoing shift as AI continues to redefine information consumption.

The discussion revolves around the impact of Google's AI Overviews and broader ecosystem changes on user experience, content creators, and web traffic. Key points include:

### **User Experience Frustrations**
- **Intrusive Ads and Pop-ups**: Users cite annoying experiences with mobile pages immediately scrolling, sticky banners, consent pop-ups, and video ads that hinder access to content. Many resort to ad blockers (e.g., uBlock Origin) to mitigate this but face ethical dilemmas about supporting creators.
- **YouTube's Degraded Experience**: Non-Premium users report buffering, unskippable ads, and interface clutter. Even Premium subscribers note declining quality, with increased ads in content and algorithmic pushes for "Shorts" or low-effort videos.

### **Impact on Content Creators**
- **Monetization Challenges**: Google’s AI summaries reduce click-through rates, directly affecting creators reliant on Adsense revenue. Smaller creators struggle as revenue skews toward top-tier channels, pushing them toward direct payments (Patreon, sponsorships) or unsustainable content strategies.
- **Shift to Subscriptions**: YouTube Premium is criticized for unclear revenue sharing, with doubts about how much supports smaller creators. Some users prefer direct creator support over Google’s subscription model.

### **Ad Blockers vs. Ethical Concerns**
- **Ad Blocker Arms Race**: Users face constant technical hurdles (e.g., broken video playback) as Google works to bypass blockers. This fuels resentment, with some reluctantly paying for Premium to avoid hassles.
- **Creators Caught in Middle**: While ads fund content, intrusive implementations drive users to blockers, harming creators. SponsorBlock and SmartTube are mentioned as solutions to skip non-ad content (e.g., sponsorships), but sustainability remains unclear.

### **Criticism of Google’s Dominance**
- **Ecosystem Control**: Google’s垄断 over search, ads (via Adsense/AdMob), and YouTube allows it to prioritize profit over user/creator needs. Header bidding and ad-space monopolization further squeeze publishers.
- **AI’s Long-Term Threat**: Users and creators fear AI summaries and generative answers will eclipse traditional content, reducing traffic and eroding the open web. Comparisons are drawn to YouTube’s evolution from a creator-friendly platform to an ad-centric monopoly.

### **Broader Sentiment**
- **Cynicism Toward Big Tech**: Many distrust Google’s claims about AI and Adsense benefits, viewing profit motives as overriding user/creator welfare. The push for subscriptions and ads is seen as exploitative, with few alternatives in a Google-dominated landscape.
- **Nostalgia for Older Web**: Some lament the decline of straightforward, ad-free browsing and creator ecosystems, replaced by tracking, paywalls, and algorithm-driven content.

In summary, the discussion highlights tension between user convenience, creator sustainability, and Google’s monetization strategies. While AI Overviews symbolize a shift toward centralized content delivery, the broader ecosystem faces criticism for degrading user experience and undermining the open web’s vitality.

### US AI Action Plan

#### [Submission URL](https://www.ai.gov/action-plan) | 389 points | by [joelburget](https://news.ycombinator.com/user?id=joelburget) | [550 comments](https://news.ycombinator.com/item?id=44660323)

In a compelling bid to secure a leading role in the global artificial intelligence landscape, the United States has unveiled an ambitious AI Action Plan under the direction of President Trump. This action plan is designed around three pivotal pillars that aim to propel America to the forefront of AI innovation, infrastructure development, and international diplomacy.

**Pillar 1: Accelerate AI Innovation**
The first pillar emphasizes fostering an environment ripe for AI innovation by supporting private sector-led advancements. The plan advocates for the removal of regulatory obstacles to unleash creativity and productivity. It highlights commitments to advance AI interpretability, safeguard free speech, encourage open-source development, and empower the workforce in this technological era. The federal government is also keen on accelerating AI adoption in various agencies, including defense, while combating challenges like synthetic media in the legal system.

**Pillar 2: Build American AI Infrastructure**
Recognizing the vital need for robust infrastructure, the second pillar focuses on upgrading energy capacities and semiconductor manufacturing—a stark need given America's stagnant growth compared to China's rapid expansion. Priorities include developing a sturdy grid, enhancing cybersecurity for critical infrastructure, and training a skilled workforce to actualize these technological implementations.

**Pillar 3: Lead in International AI Diplomacy and Security**
The United States seeks not only to lead at home but also to influence global AI standards and practices. This pillar highlights America’s role in exporting AI technologies to allies while countering China's influence in global governance. It also stresses the importance of securing global alliances and enforcing stringent control over AI-associated exports to prevent adversaries from exploiting American innovations.

Each of these pillars is backed by an array of targeted actions and policies aiming to consolidate America's leadership in the AI domain. The plan envisions a future where AI fuels economic growth, enhances national security, and fosters scientific breakthroughs, placing the United States at the cusp of a new era of technological dominance and global influence.

**Summary of Discussion:**

The discussion revolves around the AI Action Plan's energy infrastructure goals, with a focus on nuclear power versus renewable energy (solar + storage), regulatory challenges, and real-world implementation issues.

1. **Nuclear vs. Renewables Debate**:
   - **Pro-Nuclear**: Some argue nuclear is essential for clean energy, citing Trump's executive orders to boost U.S. nuclear capacity by 400% over 25 years. Others note nuclear’s reliability for energy-intensive industries and criticize its PR issues. Waste concerns are downplayed (e.g., "waste per person is small" and stored safely in dry casks).
   - **Anti-Nuclear/Solar Advocates**: Critics highlight nuclear’s high costs, risks (Fukushima/Chernobyl), and toxic waste challenges. Solar+storage is seen as cheaper and safer, with advancing technology reducing reliance on fossil fuels. Links to articles (e.g., solar’s economic edge) reinforce this stance.

2. **Regulatory and Practical Hurdles**:
   - U.S. energy leadership is questioned due to regulatory bottlenecks. For example, California faces issues like HOAs blocking solar installations, permitting delays, and grid limitations despite abundant solar potential. Users criticize inefficient policies hindering decentralized renewable adoption.
   - Debate touches on global market dynamics: If solar/battery demand surges, U.S. domestic fossil production could collapse, favoring cheaper imports.

3. **Infrastructure and Cost Concerns**:
   - GPU clusters for AI training face high costs and capacity shortages (e.g., AWS limitations), highlighting the need for affordable, scalable energy solutions.
   - Chernobyl’s exclusion zone and Fukushima’s $1 trillion cleanup costs are cited as nuclear’s legacy risks, contrasting with renewables’ lower long-term liabilities.

**Key Takeaways**:  
The HN community is split on nuclear’s role in clean energy. Proponents stress its reliability and undervalued potential, while opponents advocate for solar+storage due to cost and safety. Real-world examples (e.g., California’s regulatory maze, GPU infrastructure costs) underscore the challenges of transitioning to next-gen energy systems, whether nuclear or renewable.

### Building better AI tools

#### [Submission URL](https://hazelweakly.me/blog/stop-building-ai-tools-backwards/) | 324 points | by [eternalreturn](https://news.ycombinator.com/user?id=eternalreturn) | [168 comments](https://news.ycombinator.com/item?id=44659921)

Here’s a refreshing take on AI development that suggests we’re doing it all wrong—working backwards, in fact. The author argues that our approach to creating AI tools is often misaligned with how humans actually learn and innovate. The key issue? We’ve built AI systems that sidestep instead of leverage our natural strengths, such as problem-solving through cumulative iteration and collaboration.

Let’s break it down: humans are innately good at learning through processes, applying efforts to recall information, and innovating collectively rather than in isolation. This collective model of learning and problem-solving emphasizes the strength of brainstorming and community iteration—a stark contrast to the glorified myth of the lone genius developer. Yet, current AI tooling seems to bypass these strengths, performing tasks for us rather than enhancing our innate capabilities.

The typical AI workflow—click for magic, get AI-generated suggestions, proceed based on AI prompts—lacks the engagement required for effective learning and knowledge transfer. As humans become deskilled due to over-reliance on AI for tasks we excel at, the feedback loop degenerates, resulting in a loss of high-quality data and effective systems.

The proposed solution? Reimagine AI as an "absent-minded instructor," there to guide and enhance your skills without doing the thinking for you. Picture AI as a quirky, Socratic rubber duck, facilitating learning rather than dispensing pre-made answers. This approach refines the typical teaching method into “Explain, Demonstrate, Guide, Enhance,” aiming to embed human action into iterative learning for better mastery and skill enhancement over time.

The argument raises essential questions about how we engage with AI, suggesting that a better collaborative design could not only empower humans but also lead to richer AI systems. By tailoring AI tooling to suit human strengths rather than replace them, we can unlock the unrealized potential of both human and artificial intelligence.

**Summary of Discussion:**

The discussion centers on the **role of AI in incident management** and how it intersects with human learning and decision-making. Key points debated include:

1. **AI Reliability & Overreach**:  
   - Users highlight risks of AI making errors (e.g., wiping databases despite contrary instructions) and stress the need for caution when granting AI direct access to critical systems.  
   - Skepticism arises about AI autonomously resolving incidents, with concerns about accountability ("People want responsibility but avoid actions"). Tools performing tasks "behind the scenes" risk deskilling humans and undermining accountability.

2. **Chess Analogy & Human Skill**:  
   - Comparisons to chess suggest AI should aid analysis (like engines suggesting moves) but not replace human judgment. Even strong players improve through mental practice, not passive tool use.  
   - Critical thinking and iterative problem-solving remain essential, with AI tools criticized for generating "fast, flashy hypotheses" that lack depth without human validation.

3. **Incident Resolution Workflows**:  
   - Some propose AI as a **diagnostic assistant**, surfacing patterns/logs quickly (e.g., analyzing DeviceMapper errors to spot block size mismatches) while letting humans finalize actions. Others fear tools like LLMs might distract with irrelevant hypotheses.  
   - Privacy/security concerns emerge when AI handles sensitive data autonomously. Trust issues arise from AI executing commands without transparent oversight.  

4. **Human-Centric Design**:  
   - Emphasis on designing tools that **guide**, not replace, human reasoning. For example, dashboards presenting summarized data for informed decisions.  
   - Learning is tied to hands-on investigation; delegating tasks like log analysis to AI risks stifling skill development. Engineers must retain the ability to troubleshoot foundational systems (e.g., DNS, LVM2).  

5. **Balancing Automation & Control**:  
   - Proposals include AI tools reinforcing good practices (e.g., observing workflows to suggest optimizations) without dictating actions. Ensuring humans remain the "driver" for critical processes.  

**Conclusion**: While AI can accelerate diagnostics and surface insights, its best role is augmenting—not replacing—human judgment. Trust hinges on transparency, maintaining human agency, and avoiding overreliance that erodes problem-solving skills. The ideal system empowers users to learn *through* AI collaboration, preserving expertise and accountability.

### Lumo: Privacy-first AI assistant

#### [Submission URL](https://proton.me/blog/lumo-ai) | 202 points | by [pentagrama](https://news.ycombinator.com/user?id=pentagrama) | [105 comments](https://news.ycombinator.com/item?id=44657556)

In a landscape where AI is often critiqued for privacy invasions and data misuse, Proton introduces Lumo, a revolutionary AI assistant that prioritizes user privacy. Unlike many AI tools, Lumo doesn’t convert people into products by harvesting their data. Instead, it offers a private, secure alternative that refuses to compromise your data for profit. Built within the trusted Proton privacy ecosystem, Lumo ensures no logs are kept, and every chat is encrypted with zero-access technology, providing users with the confidence that their conversations remain confidential and safe from data leaks or third-party exploitation.

Lumo distinguishes itself with features such as ghost mode, which allows conversations to disappear once closed, and integration with secure services like Proton Drive. An open-source model, Lumo operates out of Europe, offering robust legal privacy protections far from the lengthy reach of US and Chinese jurisdictions. By not using user interactions to train the AI, it ensures sensitive information isn’t repurposed inadvertently in other outputs—a crucial factor for businesses and individuals handling confidential material. 

Available for free without requiring an account, Lumo allows you to start harnessing the power of AI right away by simply visiting their website or downloading the app on Android or iOS. Embrace the benefits of AI without the trade-off of personal privacy, thanks to Lumo’s innovative approach.

**Summary of Discussion:**

The discussion revolves around Proton's Lumo AI and broader privacy-centric technologies, focusing on jurisdiction, technical implementation, and trust in corporate practices. Key points include:

1. **Jurisdiction & Legal Protections**:  
   - Switzerland is praised for robust privacy laws, shielding Proton from foreign jurisdictions like the US and China. Users debate the practical efficacy of these protections, questioning whether Swiss-based hosting genuinely prevents external interference (e.g., US FVEY alliances).  
   - Comparisons to Apple’s **Private Cloud Compute** highlight cryptographic safeguards and third-party audits, though skepticism remains about trusting corporate-controlled systems even with transparency measures.

2. **Proton’s Product Critique**:  
   - Some users contrast ProtonMail with alternatives like Fastmail or Zoho, citing usability flaws (e.g., ProtonMail lacks transactional email support). Others defend Proton’s commitment to privacy, emphasizing encryption and features like "ghost mode" and Proton Drive integration.  
   - Questions arise about Lumo’s **open-source claims**, with users unable to locate its source code, raising concerns about transparency.

3. **Technical and Privacy Concerns**:  
   - Discussions delve into encryption practices (e.g., zero-access encryption) and infrastructure security, but skepticism persists about trusting **proprietary models** over fully open alternatives.  
   - Mention of tools like Monero, VPNs, and decentralized tech (i2p) reflects broader debates on balancing privacy with usability.

4. **Surveillance and Trust Issues**:  
   - References to cases like **Ross Ulbricht** and Snowden-era surveillance underscore distrust in government overreach and mass surveillance capabilities. Users criticize AI-driven tools (e.g., web3, VPNs) as potential privacy traps without enforceable safeguards.  
   - Apple’s approach is noted as a benchmark, blending cryptographic hardware with third-party verification, though doubts linger about centralized control.

5. **Corporate Accountability**:  
   - Calls for independent audits and legal transparency recur, emphasizing that jurisdictional advantages (e.g., Switzerland) must align with verifiable practices to avoid "privacy theater."

**Conclusion**: The thread underscores a tension between advocating for jurisdiction-based privacy solutions (like Proton’s Swiss hosting) and demanding technical/legal accountability. While Proton’s Lumo is viewed as a step forward, users stress the need for auditable open-source frameworks, skepticism toward corporate claims, and broader systemic reforms to counter surveillance overreach.

### AI coding agents are removing programming language barriers

#### [Submission URL](https://railsatscale.com/2025-07-19-ai-coding-agents-are-removing-programming-language-barriers/) | 142 points | by [Bogdanp](https://news.ycombinator.com/user?id=Bogdanp) | [170 comments](https://news.ycombinator.com/item?id=44655515)

For a decade, this developer lived and breathed Ruby, mastering everything from Rails to core tooling. But in 2025, they took a bold leap into the world of multi-language programming, diving into C++, C, and Rust projects with newfound confidence, thanks to Shopify and AI tools like Cursor and Claude Code.

The shift wasn't just a personal milestone; it was propelled by changes in project requirements at Shopify. The developer needed to support Sorbet for RBS, prompting a foray into system programming languages they had never touched before. Fortunately, Shopify’s nurturing environment and mentors provided the foundational knowledge needed. Initially daunting, the transition was made smoother by these supportive colleagues who shared their expertise generously.

The developer's work on ZJIT, a Ruby JIT compiler, exemplified the complexity of juggling Rust, C, and Ruby, along with compiler theory. Here, AI tools played a pivotal role—not as code generators but as collaborative partners. AI assisted by offering insights into syntax, patterns, and theoretical explorations, allowing the developer to focus on project requirements and context. This dynamic pairing highlighted how AI can halve learning obstacles, accelerating skill acquisition without taking away the need for human expertise for course corrections.

The developer stresses that AI's assistance doesn’t replace deep expertise but lowers the entry barrier, making contributions to projects achievable sooner. Mistakes with syntax or unfamiliar tools are quickly corrected, freeing developers to immediately impact without needing to master every detail upfront.

This transformation from a Ruby-only background to a multi-language developer within a year speaks to a larger shift. As AI continues to enhance learning, the cognitive load of understanding new languages is easing, allowing developers to focus on solving complex problems. This evolution is not only redefining individual careers but could reshape the landscape of programming specialization, making language versatility more accessible to a broader range of developers.

The Hacker News discussion critiques the role of AI (specifically LLMs) in programming, emphasizing its strengths, limitations, and broader implications:

### Key Themes:
1. **AI as "Glorified Pattern Matchers":**
   - Critics argue LLMs are limited to pattern-matching existing code and struggle with **abstract reasoning** (e.g., handling novel problems like concurrent resource locking or missing logical steps).  
   - Advocates counter that AI excels in **context-specific tasks** (generating code snippets, debugging, or CSV/config processing) and accelerates workflows by automating repetitive patterns.

2. **Niche vs. Mainstream Languages:**
   - Non-mainstream languages (Scala, OCaml, Elm) face challenges, as AI tools have less training data for them, making code generation less reliable.  
   - Some users note AI can still assist in **scaffolding** (e.g., translating Python libraries to Scala) but requires human expertise for optimization and correctness.

3. **Bug Detection & Reliability:**
   - LLMs can surface potential bugs quickly but often generate **false positives** or miss critical context. Users stress the need for human verification (tests, code reviews) to avoid catastrophic errors.  
   - Skeptics highlight that AI lacks the intuition for "needle-in-haystack" bugs, relying instead on deterministic analysis or examples from its training corpus.

4. **Human-AI Collaboration:**
   - While AI lowers barriers to entry (e.g., generating boilerplate code), it doesn’t replace human reasoning. For complex systems (flight transfers, financial logic), humans must design safeguards.  
   - Some users express optimism that AI will evolve beyond current limitations, but others dismiss this as overhyped determinism.

5. **Ethical & Practical Concerns:**
   - Concerns about AI-generated code leading to **economic misplanning** (e.g., unreliable CSV parsers) or security risks (random column-flipping in data) surface.  
   - The debate touches on AI’s role in stifling creativity, with some fearing it could homogenize solutions, while others see it freeing developers for higher-level tasks.

### Notable Examples:
- A user shared frustration with AI generating **outdated code styles** (e.g., "Zig code with 1990s-era loops").  
- Another highlighted **AI’s utility in security research**, where it helped find a curl bug but required extensive human validation to avoid false reports.  

### Conclusion:
The consensus leans toward AI as a powerful **supplement to human expertise**—ideal for pattern-driven tasks but dependent on human oversight for abstract reasoning, context, and system-level integrity.

### You can now disable all AI features in Zed

#### [Submission URL](https://zed.dev/blog/disable-ai-features) | 548 points | by [meetpateltech](https://news.ycombinator.com/user?id=meetpateltech) | [261 comments](https://news.ycombinator.com/item?id=44660519)

Exciting news for developers who value control over their coding environment: Zed, the high-performance code editor, now allows users to disable all AI features! This update responds to feedback from the coding community, recognizing that not everyone wants or can use AI in their workflows. Whether due to philosophical reasons, concerns about data privacy, or organizational restrictions, developers can now choose a non-AI path with a simple addition to their settings.json: `{ "disable_ai": true }`.

The option to disable AI is rolling out in Zed’s Preview today and will be part of the Stable release next week. Plus, new users soon will have the choice to disable AI features during the onboarding process. For those who still want to benefit from AI but worry about privacy, Zed offers alternatives like using personal API keys or local AI models to keep everything secure and private.

As AI tools increasingly influence software development, understanding them is becoming an essential skill. Zed recognizes this, and through its Agentic Engineering series, aims to educate developers about effectively leveraging AI while respecting those who choose traditional methods.

The open-source editor, available on macOS and Linux, continues to evolve with these user-centered developments. And if you're passionate about crafting the future of coding tools, Zed is hiring! Download Zed today to explore a customizable coding environment tailored to your preferences.

**Summary of Hacker News Discussion on Zed's AI Disable Feature and Performance:**

The discussion revolves around Zed’s new option to disable AI features, its performance, and comparisons with other editors/IDEs. Key points include:  

1. **Performance vs. Features**:  
   - Users praise Zed’s speed and low resource usage, especially after recent optimizations. However, some note lingering latency issues on Linux and when handling large files, even on high-end hardware like M3 Max MacBooks.  
   - JetBrains IDEs (e.g., IntelliJ) are cited as *feature-rich but sluggish*, while Zed and Sublime Text are lauded for responsiveness.  

2. **AI Integration Controversy**:  
   - The decision to add AI features sparks debate. Some users disable AI for privacy/performance reasons, while others want better AI customization (e.g., local models, context-aware tab completion).  
   - Comparisons with tools like **Cursor** (AI-focused editor) highlight gaps in Zed’s AI-driven tab completion quality, though Zed’s speed keeps users engaged.  

3. **Plugin Ecosystem Critique**:  
   - Zed’s limited plugin system is a pain point. Users miss the extensibility of VS Code/JetBrains and hope Zed prioritizes plugin APIs to rival competitors.  

4. **Vim/Neovim Comparisons**:  
   - Zed’s modal editing and keyboard shortcuts receive mixed reviews. Some find it "smoother than Vim," while NeoVim loyalists cite missing features (e.g., fuzzy file matching) and integration gaps.  

5. **Platform-Specific Issues**:  
   - Linux users report higher input latency compared to macOS, with some calling Zed’s Linux port "the worst latency experience."  

**Overall Sentiment**:  
Zed is celebrated for its snappiness and lightweight design but faces pressure to improve its AI tooling, plugin ecosystem, and platform parity. The ability to disable AI is welcomed, reflecting a community split between AI enthusiasts and minimalists. Zed’s future success may hinge on balancing performance with extensibility.

### Cerebras launches Qwen3-235B, achieving 1.5k tokens per second

#### [Submission URL](https://www.cerebras.ai/press-release/cerebras-launches-qwen3-235b-world-s-fastest-frontier-ai-model-with-full-131k-context-support) | 358 points | by [mihau](https://news.ycombinator.com/user?id=mihau) | [148 comments](https://news.ycombinator.com/item?id=44657727)

Cerebras Systems has set a new benchmark in AI technology with the launch of Qwen3-235B, now available on the Cerebras Inference Cloud. This model is claimed to be the fastest frontier AI reasoning model, offering unprecedented speed and efficiency at a fraction of the cost of its competitors. Qwen3-235B stands out for its ability to provide production-grade code generation at 30 times the speed and at only 10% of the cost compared to other closed-source models.

**Breaking New Ground in AI Performance**

This cutting-edge model not only accelerates processing speed but also revolutionizes enterprise AI deployment by enabling real-time reasoning. With its Wafer Scale Engine, Qwen3-235B processes data at an unprecedented 1,500 tokens per second, cutting response times from minutes to mere seconds. This transformation means developers can now perform complex coding and reasoning tasks almost instantaneously.

**Expanding Horizons with 131K Context Support**

Cerebras has enhanced its model’s capabilities by increasing its context length support from 32K to 131K tokens. This leap allows Qwen3-235B to effectively manage large codebases and complex documents, addressing the growing demand for production-grade AI applications in the enterprise code generation market.

**Strategic Collaboration with Cline**

In a strategic move, Cerebras has partnered with Cline, a renowned coding agent for Microsoft VS Code. This integration means Cline users will experience vastly improved coding speeds, starting with Qwen3-32B on the free tier and eventually benefiting from the full capabilities of Qwen3-235B. Cline's fast processing and real-time iteration potential place it at the forefront of developer tooling.

**The Future of AI Computing: A Strategic Edge**

Cerebras’ latest offering provides a significant edge over competitors like OpenAI and Anthropic by combining superior model intelligence with unparalleled speed and affordability. Qwen3-235B is not only a game-changer in terms of performance and cost but also demonstrates what’s possible when AI evolves to keep pace with the rapid needs of developers.

**About Cerebras Systems**

A leader in AI supercomputing, Cerebras continues to innovate with their Wafer-Scale Engine-3, powering the CS-3 system—the world's largest AI processor. Their solutions, offered through cloud and on-premises, support some of the most advanced AI applications today. For more about their groundbreaking work, visit cerebras.ai.

**Summary of Hacker News Discussion on Cerebras Qwen3-235B**

1. **Model Confusion and Availability**:  
   Users noted confusion around the model’s naming conventions (e.g., Qwen3-235B vs. Qwen3-405B mentions) and its availability across platforms like OpenRouter, with some pointing out differences between Cerebras's version and third-party implementations.

2. **Hardware Cost and Architecture Debate**:  
   - A key thread debated Cerebras’s claimed $135M total cost (45 chips at $3M each) for 44GB SRAM per chip. Critics argued this was likely exaggerated, distinguishing between individual chip costs and wafer-scale system pricing (e.g., one user noted TSMC wafers cost ~$30K, but wafer-scale systems are far pricier).  
   - Comparisons to Nvidia’s DGX B200 systems highlighted cost disparities: $500K for Nvidia’s 28TB memory setup vs. Cerebras’s $135M claim, favoring Nvidia for affordability despite Cerebras’s speed claims.  

3. **SRAM vs. HBM Practicality**:  
   - Skepticism arose around Cerebras’s reliance on SRAM for performance. Users argued that while SRAM offers fast access, it’s impractical for large models due to size and thermal constraints. Some speculated Cerebras combines SRAM with external DRAM (via MemoryX) to offset limitations.  
   - Nvidia’s HBM approach was defended as more scalable for real-world workloads, with debate over latency vs. bandwidth trade-offs.  

4. **Token Speed and Efficiency**:  
   The 1,500 tokens/second claim was dissected, with users questioning if this relies entirely on SRAM or hybrid memory. Some compared Groq’s SRAM-focused chips but noted Cerebras’s wafer-scale design might face challenges in power/cooling.  

5. **Quantization and Model Optimization**:  
   Discussions praised modern quantization methods (e.g., GGUF formats) for reducing memory usage without significant precision loss. Dynamic precision assignment in layers was highlighted as a practical optimization, though some debated its implementation complexity.  

6. **Cost-Benefit Analysis**:  
   Critics challenged Cerebras’s value proposition: while $135M hardware might deliver speed, a $500K Nvidia system could break even in ~62 days versus cloud API costs (e.g., $8K/day for Anthropic at scale), favoring Nvidia for cost efficiency.  

**Conclusion**:  
The thread reflects technical skepticism toward Cerebras’s cost and architecture claims, with users emphasizing Nvidia’s ecosystem advantages. While Cerebras’s SRAM-driven speed is acknowledged, practical limitations in scalability, cooling, and cost-effectiveness dominate concerns. Quantization and hybrid memory strategies emerged as critical factors in real-world AI deployment.

### Copilot Vision on Windows 11 sends data to Microsoft servers

#### [Submission URL](https://www.theregister.com/2025/07/23/microsoft_copilot_vision/) | 77 points | by [rntn](https://news.ycombinator.com/user?id=rntn) | [9 comments](https://news.ycombinator.com/item?id=44659137)

In a bold yet controversial move, Microsoft is doubling down on AI integration within Windows 11, introducing features like Copilot Vision, which has garnered mixed reactions. This new AI tool aims to be a "true companion" by capturing everything you do on your PC screen and analyzing it through Microsoft's servers. Although the technology is only used when explicitly activated by the user, privacy concerns are rife given the tool's capability to process every action performed on a machine.

Copilot Vision follows the footsteps of the infamous Recall feature. Despite their assurances, critics remain wary of Microsoft's claim that users' data won't be held for long-term storage or used for ads personalization. Echoes of the playful yet intrusive Clippy of old might make some nostalgic but leave others wary. Europe, famously stringent on data privacy, remains exempt for now, likely influenced by the looming AI Act, but Microsoft plans to extend the service to "non-European countries" soon.

Alongside Copilot Vision, Microsoft is also teasing more AI functionalities, such as Mu - their first "agentic" AI model - eager to assist and act on user commands with natural language instructions. While this sounds promising, potential pitfalls loom under the notorious "hallucination" error common in language models, where they might produce logic-defient results.

In the meantime, innovative features like "Click to Do" are available for testing outside Europe but have continued to stir debates over privacy versus innovation. Microsoft is forging ahead, seemingly betting on AI rising to meet the needs, wants, and occasionally the whims of its users, while skeptics watch closely.

**Summary of Discussion:**  

The discussion highlights widespread skepticism and concern over Microsoft's AI-driven features in Windows 11, particularly **Copilot Vision**, with users emphasizing **privacy risks**, **lack of control**, and comparisons to past controversies:  

1. **Privacy and Data Collection**:  
   - Users criticize Microsoft's history of collecting data despite user settings (e.g., forced updates altering configurations) and question assurances about limited data retention. Comments note that even "opt-in" features could funnel sensitive data to Microsoft’s servers, drawing parallels to the invasive **Recall** feature and older blunders like **Clippy**.  
   - Concerns are raised about **Copilot Vision** acting as a "screen-sharing" tool similar to Teams, potentially transmitting screen data without explicit consent.  

2. **User Control and Proprietary Systems**:  
   - Many argue that proprietary operating systems like Windows inherently limit user autonomy, with one user stating, *"When your vendor locks you into their OS, there’s no stopping their software."* Some advocate switching to Linux for greater control.  

3. **Forced Updates and Lack of Transparency**:  
   - Anecdotes highlight forced Windows updates resetting user preferences (e.g., login backgrounds), eroding trust. Critics accuse Microsoft of prioritizing corporate interests over user agency.  

4. **Antitrust and Legal Risks**:  
   - References to the **Department of Justice (DoJ)** suggest concerns about monopoly practices, with joking predictions of billion-dollar fines. Others sarcastically note that legal repercussions rarely deter big tech.  

5. **AI Reliability and Implementation**:  
   - Skepticism surrounds AI “hallucinations” (erroneous outputs) and the rushed rollout of features like **Mu**, labeled as experimental "Labs" previews. Users question whether Copilot’s crashes and UI issues indicate unfinished development.  

6. **Geopolitical Exemptions**:  
   - Europe’s exclusion (due to strict privacy laws like the **AI Act**) is noted, though plans to expand elsewhere amplify fears of unchecked data harvesting in other regions.  

**Overall Sentiment**: A mix of resignation and frustration, with users wary of Microsoft’s AI ambitions and distrustful of its privacy claims, even as the company pushes forward with new features.

### Executive Order – Preventing Woke AI in the Federal Government

#### [Submission URL](https://www.whitehouse.gov/presidential-actions/2025/07/preventing-woke-ai-in-the-federal-government/) | 31 points | by [hooverd](https://news.ycombinator.com/user?id=hooverd) | [21 comments](https://news.ycombinator.com/item?id=44665070)

On July 23, 2025, a new executive order titled "Presidential Actions PREVENTING WOKE AI IN THE FEDERAL GOVERNMENT" was introduced, focusing on the implementation and regulation of Artificial Intelligence (AI) in the federal government. The initiative aims to ensure that AI models, particularly large language models (LLMs), deliver reliable and unbiased outputs free of ideological bias. The directive highlights concerns that concepts such as diversity, equity, and inclusion (DEI) could lead to distortions in AI outputs, including historical inaccuracies and ideological skewing.

Under this order, federal agencies are directed to procure AI models adhering to two key principles: truth-seeking and ideological neutrality. These principles mandate that AI systems provide factual, objective, and unbiased information while acknowledging uncertainties. The order also sets forth a process for guiding agencies in implementing these principles, advocating for transparency and flexibility in AI development and procurement while ensuring that federal contracts for AI adhere to these standards.

The guidance will be shaped by input from various federal bodies, emphasizing technical limitations, transparency, and the latitude for vendors to innovate. Exceptions are made for national security systems, recognizing the unique demands of such domains. This initiative builds upon previous efforts to foster trustworthy AI use within the government, reinforcing a commitment to accuracy and impartiality in federal AI applications.

The discussion on Hacker News about the executive order targeting "woke AI" in the federal government reflects a mix of satire, semantic debates, and skepticism. Key themes include:  

1. **Political Satire and Jabs**:  
   - Comments mock the order’s framing, comparing it to dystopian satire (*Idiocracy*) and questioning its legitimacy. References to "Superman," arrests of former presidents, and "HR memes" highlight hyperbolic or cynical takes on the policy’s motives.  

2. **Defining "Woke"**:  
   - Users debate the term’s ambiguity:  
     - Some reductively define it as "non-traditional" or "things I don’t like," linking it to critiques of DEI initiatives.  
     - Others argue "woke" has become a meaningless buzzword, stretched to absurdity (e.g., applied to "vegetarian pizza toppings").  

3. **Skepticism About AI Neutrality**:  
   - Critics question the feasibility of ideologically neutral AI, noting biases are inherent in training data. One user points out the irony of mandating neutrality while rejecting DEI-informed models.  

4. **Policy and Implementation Concerns**:  
   - Practical challenges are raised, such as defining "truth-seeking" in AI and addressing technical limitations. Some suggest the order risks stifling innovation or oversimplifying complex issues.  

5. **Political Tangents**:  
   - Threads derail into broader political grievances, invoking figures like Trump, Musk, and RFK Jr., and debates over education (e.g., claims about "racist math curricula").  

6. **Flame Wars and Fragmentation**:  
   - Portions of the discussion devolve into flagged or unproductive exchanges, reflecting polarized views on the term "woke" and the policy itself.  

**Tone**: The thread blends humor, frustration, and ideological sparring, with some users engaging earnestly on technical challenges while others dismiss the policy as performative or politically charged.

