## AI Submissions for Thu Jun 29 2023 {{ 'date': '2023-06-29T17:11:45.160Z' }}

### Building Boba AI: Lessons learnt in building an LLM-powered application

#### [Submission URL](https://martinfowler.com/articles/building-boba.html) | 160 points | by [nalgeon](https://news.ycombinator.com/user?id=nalgeon) | [62 comments](https://news.ycombinator.com/item?id=36523480)

Today's top story on Hacker News is about the lessons learned and patterns discovered while building an experimental AI co-pilot called "Boba." Boba is designed to assist with product strategy and generative ideation by leveraging a Large-Language Model (LLM) to generate ideas and help users navigate complex conversational flows.

The article outlines several patterns for building generative co-pilot applications, including Templated Prompt, Structured Response, Real-Time Progress, Select and Carry Context, Contextual Conversation, Out-Loud Thinking, Iterative Response, and Embedded External Knowledge. These patterns aim to enhance the user's interaction with the LLM, improve the quality of generated results, and integrate external knowledge that the LLM may not have.

Boba is described as an AI co-pilot that augments the early stages of strategy ideation and concept generation. It enables users to generate and evaluate ideas in partnership with AI, leveraging OpenAI's LLM to generate ideas and answer questions related to specific domains. The first prototype of Boba focuses on capabilities such as researching signals and trends, creative matrix concepting, scenario building, strategy ideation, concept generation, and storyboarding.

The article also mentions that Boba is a web application that serves as an interface between the user and the LLM (currently GPT 3.5). The goal of Boba is to simplify the interaction with the LLM for users who may not be familiar with effectively engaging with AI systems.

Overall, Boba aims to enhance the creative ideation process by leveraging AI capabilities and providing users with a collaborative co-pilot for generating innovative ideas and addressing customer needs. The patterns and lessons learned in building Boba offer valuable insights for anyone interested in building similar AI-powered applications.

The discussion surrounding the submission on Hacker News revolves around various topics related to AI co-pilots, generative models, and the challenges of building such applications. Here are some key points from the discussion:

1. One commenter emphasizes the importance of practical applications of AI co-pilots rather than just focusing on the novelty of the technology. They suggest that most people don't want to interact with AI directly through APIs and that integration with existing tools like Microsoft Bing, Google Docs, and Office 365 would be more valuable.

2. The length of the book mentioned in the submission is criticized as being too short, pointing out that a 43-page book on DnD rules is considered sufficient.

3. The discussion touches on the skepticism surrounding AI co-pilots and their potential capabilities. Some argue that many projects are simply using buzzwords while lacking substantial advancements in technology.

4. Some comments highlight the need for careful consideration when using large language models like LLMs in NLP projects. Issues like lack of explainability and the difficulty of solving complex problems are mentioned.

5. The challenges and limitations of working with LLMs and NLP are discussed, with commenters mentioning the tedious nature of solving problems, the need for manual review, and the black-box nature of AI prompts.

6. The importance of structured data and context in interactions with LLMs is emphasized. Several commenters mention the value of providing structured responses, capturing relevant information, and maintaining a coherent conversation with the AI.

7. Various perspectives are shared on the potential applications and exciting possibilities of AI co-pilots. Some commenters express interest in using AI for text generation and creative purposes, while others discuss its potential for aiding in project management and complex tasks.

Overall, the discussion highlights a mix of enthusiasm for AI co-pilots and a critical examination of their limitations, practicality, and potential for real-world applications. There is also a focus on the importance of integrating AI with existing tools and incorporating structured data for improved interactions and results.

### Tesla Fleet Telemetry

#### [Submission URL](https://github.com/teslamotors/fleet-telemetry) | 205 points | by [shekhar101](https://news.ycombinator.com/user?id=shekhar101) | [120 comments](https://news.ycombinator.com/item?id=36525940)

Tesla has released a decentralized framework called Fleet Telemetry, which allows Tesla customers to create a secure and direct connection between their Tesla devices and authorized third-party providers. Fleet Telemetry is a simple, scalable, and secure data exchange service for vehicles and other devices. It handles device connectivity, data transmission, and storage. Customers can configure telemetry records and receive acknowledgments, error responses, or rate limit notifications. Fleet Telemetry can be installed on Kubernetes with a Helm Chart or as a standalone binary. It requires setting up a publicly available endpoint and mutual TLS (mTLS) WebSocket connections for device communication. The service can be configured for different data backends and dispatchers. Tesla emphasizes the importance of security and privacy in Fleet Telemetry, allowing customers to have control over their data sharing.

The discussion on this submission revolves around the topics of privacy, data sharing, and the implications of Tesla's Fleet Telemetry framework. Some users express concerns about the potential misuse of customer data by third-party apps and the need for stronger privacy laws to protect consumers. Others argue that the benefits of data sharing and telemetry outweigh the risks, and that Tesla owners have control over their data. There are also discussions about the level of privacy protection provided by current laws and the potential for manipulation through targeted advertising.

### Gitlabâ€™s AI-assisted code suggestions

#### [Submission URL](https://about.gitlab.com/solutions/code-suggestions/) | 155 points | by [sh_tomer](https://news.ycombinator.com/user?id=sh_tomer) | [151 comments](https://news.ycombinator.com/item?id=36524201)

GitLab, a leading DevOps platform, has been named a Leader in the Gartner Magic Quadrant for DevOps Platforms. Their AI-assisted Code Suggestions feature helps teams create software faster and more efficiently by suggesting code while developers are working. With the ability to complete entire lines of code with a single keystroke, start functions quickly, generate boilerplate code, and even generate tests, GitLab's AI-powered suggestions streamline the coding process. The Code Suggestions feature prioritizes privacy, ensuring that proprietary source code is kept secure within GitLab's infrastructure and not used for training data. It uses open-source pre-trained models that are continuously fine-tuned with a customized dataset to support multiple programming languages. Currently available in 13 languages, GitLab plans to expand its IDE support and improve the user experience for code suggestions. Additionally, they are working on making Code Suggestions available for self-managed instances. GitLab's AI-powered capabilities are poised to advance the field of DevSecOps, allowing developers to work more effectively.

The discussion on Hacker News surrounding GitLab being named a Leader in the Gartner Magic Quadrant for DevOps Platforms and their AI-assisted Code Suggestions feature largely revolves around users sharing their experiences and opinions on GitLab's AI capabilities.

One user expresses their dissatisfaction, stating that GitLab's feature set is not very appealing as they believe GitLab is doing only half-working features that do not increase value. They suggest finishing and polishing existing features and maybe creating a policy on big feature counts. They provide examples of areas that need improvement such as security scanning, user interface/user experience, and project management. Another user agrees with this sentiment and mentions that they find GitLab CI lacking compared to Github Actions.

However, a GitLab team member jumps in to acknowledge the feedback and shares their excitement about the AI/ML features GitLab has planned. They mention that GitLab has started experimenting with machine learning in their product, focusing on AI workflows and DevSecOps platforms. They also provide links to blog posts and the GitLab website where users can learn more about these upcoming features.

Several users discuss the potential of GitLab's AI capabilities. One user cancels their Copilot subscription, a service from OpenAI, and states that they find GitLab's suggestions to be more productive. Another user shares their positive experience of using Copilot to write code comments, mentioning that it improved comment quality. They also mention that they enjoy paying the price for the increased productivity.

There are also discussions about the training data used in AI models. One user wonders if GitLab uses GitHub's code suggestions since they find GitHub's suggestions to be superior. Another user mentions the importance of privacy and how GitLab's AI features prioritize keeping proprietary source code secure within their infrastructure and not using it for training data.

Some users express their skepticism toward AI-powered coding tools. They argue that experience and a reliable typing system are more important than relying on AI tools. However, other users, especially those who identify themselves as junior programmers, share their positive experiences with Copilot and AI tools in general. They find them to be productive and helpful in improving their coding skills.

Overall, the discussion showcases a mix of opinions and experiences with GitLab's AI capabilities, with some users excited about the potential and others having concerns or preferring alternative tools.

### OpenOrca: open source dataset and instruct-tuned LLMs

#### [Submission URL](https://erichartford.com/openorca) | 225 points | by [npsomaratna](https://news.ycombinator.com/user?id=npsomaratna) | [54 comments](https://news.ycombinator.com/item?id=36515187)

OpenOrca is a new open-source dataset and series of instruct-tuned language models announced by Eric Hartford. Inspired by Microsoft's research paper on Orca, Hartford decided to replicate their efforts and create OpenOrca. The dataset consists of approximately 1 million FLANv2 augmentations with GPT-4 completions and around 3.5 million FLANv2 augmentations with GPT-3.5 completions. The team is currently performing full weights fine-tuning on the foundation of LLaMA-13b and aims to release OpenOrca-LLaMA-13b in mid-July 2023. They are also seeking GPU compute sponsors for training OpenOrca on various platforms. Hartford expresses his gratitude to the open-source AI/ML engineers who worked alongside him and the sponsors who have already contributed to the project.

The discussion surrounding the submission on Hacker News covers various topics:

1. Some users express their frustration with the limitations and constraints of AI language models, highlighting the challenges of training large models and the need for more powerful tools and contexts.

2. A user shares a fictional scenario related to content moderation and law enforcement, sparking a discussion about the potential implications and challenges of AI in these areas.

3. A user makes a reference to the movie "Free Willy," which leads to a discussion about the complexity of language models and their ability to handle different tasks.

4. Users discuss the potential performance of GPT-4 and GPT-3.5, and there are differing opinions on their capabilities and limitations.

5. The announcement of the OpenOrca project is acknowledged, with users expressing their support and acknowledging the contributors' efforts.

6. Users discuss the estimated model size and compute resources required for training different models, highlighting the cost and resource-intensive nature of large-scale training.

7. The topic of legal implications and the usage of AI language models is brought up, with users speculating about the potential legal challenges and the collection of personal data.

8. Recommended resources for learning about language models and deep learning are shared, promoting platforms like Hugging Face for tutorials and courses.

Overall, the discussion covers technical aspects of AI language models, their potential applications, and the challenges they pose, as well as the implications of their usage and the importance of responsible development and usage.

### Show HN: Build a discord/Slack bot to answer questions with your docs and GPT4

#### [Submission URL](https://www.windmill.dev/blog/knowledge-base-discord-bot) | 134 points | by [rubenfiszel](https://news.ycombinator.com/user?id=rubenfiszel) | [20 comments](https://news.ycombinator.com/item?id=36520410)

In today's blog post titled "Replace a SaaS," the author explores the idea of building your own minimal solution instead of relying on a dedicated SaaS. Specifically, they walk through the process of creating a bot that can answer questions about technical documentation using Windmill, OpenAI, and Supabase.

The post begins by introducing Supabase, a platform used to store embeddings. The author references a tutorial on storing OpenAI embeddings in Postgres with pgvector and explains how embeddings are representations of text or other information that capture relatedness or meaning. Using pgvector in PostgreSQL, it becomes possible to store and query vector embeddings, opening up powerful applications like semantic search and recommendation systems.

Next, the author outlines the steps to create a bot that can answer questions about technical documentation. They provide the necessary commands to run and explain that the main difference is the addition of a link column to store the documentation page for each embedding.

Moving on, the author discusses how to create a scheduled flow using Windmill to ingest data from the Windmill documentation. They explain that this flow periodically retrieves the documentation from the Windmill repository on GitHub, creates embeddings using OpenAI, and stores the embeddings in Supabase using pgvector. The flow consists of three steps: scraping the Windmill documentation from GitHub using the Oktokit API, creating the embeddings using OpenAI, and storing the embeddings in Supabase.

For the first step, the author mentions that there is already a community-contributed script available on the Windmill Hub to fetch content from GitHub. They explain that they modified the script to extract content only from markdown files, and provide the TypeScript code for reference.

Overall, this blog post offers a detailed guide on how to build a bot that answers questions about technical documentation using Windmill, OpenAI, and Supabase. By following the provided instructions, readers can create their own tailored solution instead of relying on a dedicated SaaS.

The discussion on Hacker News surrounding the blog post "Replace a SaaS" covers various topics related to building alternative solutions and tools. Here are some notable points from the discussion:

- One user points out the trade-off of building one's own solution instead of relying on generic SaaS subscriptions, suggesting that while it may save on costs, it can also require significant development time.
- Another user expresses interest in trying out the solution and asks about the associated charges.
- Several users share their own projects that are related to indexing, searching, and answering questions based on data.
- Discussion shifts towards content marketing and motivations to read the blog post.
- One user appreciates the mention of using Windmill for hosted plugin play solutions and mentions their own project involving Discord and Telegram integration.
- Another user mentions their fascination with the videos from Windmill, expressing a desire for more interfaces and interactions.
- Some users discuss the AI development tools they are using and their preferences for certain platforms.
- There are comments regarding SharePoint, DevOps, and Microsoft Teams.
- The author of the blog post offers a cloud version and receives congratulations with requests for more information.
- Some discussion arises about the importance of retaining the human element in support and the potential problems of automation.
- One user mentions their Discord server and its contribution to problem-solving and support.

Overall, the discussion covers topics related to alternative tools, AI development, content marketing, and the pros and cons of building one's own solutions.

### Valve is not willing to publish games with AI generated content anymore?

#### [Submission URL](https://old.reddit.com/r/aigamedev/comments/142j3yt/valve_is_not_willing_to_publish_games_with_ai/) | 611 points | by [Wouter33](https://news.ycombinator.com/user?id=Wouter33) | [380 comments](https://news.ycombinator.com/item?id=36522665)

Valve, the company behind the Steam gaming platform, has recently made it clear that they are not willing to publish games with AI-generated content. A developer shared their experience of trying to release a game with assets that were obviously AI-generated, only to have their submission rejected by Valve. The rejection message stated that the game contained art assets generated by AI that appeared to be relying on copyrighted material owned by third parties. Valve cited the unclear legal ownership of such AI-generated art as the reason for their decision. The developer then improved the assets by hand, but their resubmitted app was still rejected. This incident highlights the uncertainty around AI-generated content and the challenges developers may face in getting their games published. While some games on Steam do mention the use of AI, Valve, at least for now, seems wary and not willing to publish AI-generated content. The developer plans to try uploading their game to itch.io to see if they face similar issues there.

The discussion on this submission revolves around Valve's decision to not publish games with AI-generated content. Some users argue that Valve's rejection of games with AI-generated assets is justified due to potential copyright infringement, while others express frustration with the lack of clear guidelines and transparency from Valve. There are also discussions about the ethical implications of AI-generated content and the role of journalists in verifying information. Some users bring up the possibility of Valve's stance being influenced by legal concerns or demands from the public. The inconsistency in Valve's decisions and the potential impact on blockchain games are also mentioned in the discussion. Overall, there are mixed reactions and discussions about the legal, ethical, and practical aspects of AI-generated content in games.

### XGen-7B, a new 7B foundational model trained on up to 8K length for 1.5T tokens

#### [Submission URL](https://blog.salesforceairesearch.com/xgen/) | 260 points | by [bratao](https://news.ycombinator.com/user?id=bratao) | [92 comments](https://news.ycombinator.com/item?id=36514936)

Salesforce's team of researchers have trained a series of 7B Long-Range Language Models (LLMs) called XGen-7B that can handle input sequence lengths of up to 8K tokens. The models achieve comparable or better results than state-of-the-art open-source LLMs on standard NLP benchmarks. They also outperform 2K- and 4K-seq models on long sequence modeling tasks. XGen-7B performs well on both text and code tasks and has a training cost of $150K on 1T tokens. The codebase and checkpoint for XGen-7B are available on GitHub and Hugging Face, respectively. The researchers explain that the need for LLMs to effectively model long sequences is crucial for tasks such as summarizing text, writing code, and predicting protein sequences. Most open-source LLMs are trained with a maximum of 2K token sequence length, which limits their ability to handle long sequences. The XGen models were fine-tuned on public-domain instructional data, resulting in instruction-tuned counterparts. The researchers used a two-stage training strategy and JaxFormer, their in-house library, to train the XGen-7B models. They also explored "loss spikes" during training and made improvements to ensure stable training at larger model sizes. Overall, XGen-7B with 8K sequence length offers advancements in long sequence modeling.

The discussion on the Hacker News submission revolves around various aspects of the topic:

1. Some users discuss the benefits and drawbacks of larger language models. They mention that larger models like XGen-7B have better performance but require more resources, including RAM and VRAM.

2. There are discussions about the availability of other large language models like OpenLLaMA 13B and their comparison to XGen-7B. Users also highlight the importance of tokenizers in language models.

3. The cost and resources required for training large language models are discussed. Some users suggest that smaller models can also be effective and cost-efficient.

4. The potential applications and limitations of XGen-7B and language models in general are debated. Some users express concerns about the negative impact of commercial language models on the AI industry.

Overall, the discussion provides different perspectives on the advancements and challenges associated with large language models like XGen-7B.

### Artificial Intelligence Canâ€™t Work Without Our Data. We Should Be Paid for It

#### [Submission URL](https://www.politico.com/news/magazine/2023/06/29/ai-pay-americans-data-00103648) | 32 points | by [robg](https://news.ycombinator.com/user?id=robg) | [17 comments](https://news.ycombinator.com/item?id=36527284)

Today's article discusses the importance of data in powering artificial intelligence (AI) and proposes a solution to address the issue of who owns and benefits from this data. The authors highlight that AI companies are using the data created by individuals without their knowledge or consent, and it's time for people to be compensated for their contributions. They suggest implementing an "AI Dividend" where Big Tech companies would pay a small licensing fee for using public data to train their AI models. The fees would be distributed equally to all residents nationwide, similar to Alaska's Permanent Fund. The proposal exempts hobbyists and small businesses, with only companies with substantial revenue required to contribute. Additionally, AI companies would receive a license to use public data by agreeing to pay into the fund. The authors emphasize the importance of including everyone's data, regardless of writing skills or profession, as generative AI requires a wide variety of information. They also highlight that the proposal aims to incentivize the development of creative and valuable AI while protecting against misuse. Overall, the authors argue that AI companies should pay for the data they rely on, and individuals should be compensated for their contributions.

The discussion on this submission revolves around various aspects of data training for AI models and the proposed AI Dividend. 

One user comments that the proposal is similar to Andrew Yang's focus on testing the idea of a dividend reception for Universal Basic Income (UBI). The idea of compensating people for their contributions to AI is seen as interesting.

Another user argues that the cost of training AI models is quite significant, specifically mentioning that generating 1000 words with GPT-3.5 costs around $0.15 whereas using ChatGPT for conversation can range from $5 to $10.

There is a discussion about the effectiveness of different data training methods, with one user emphasizing the importance of effective data training.

The use of public data and the potential challenges of obtaining and processing data are discussed. One user suggests that scraping data might be blocked by bots or require advanced OCR techniques. The importance of embracing the future and considering available value is also mentioned.

The role of community platforms and the benefits of decentralized licensing are debated, with a comparison made to large portions of Wikipedia being written by a small group of people who then distribute the benefits.

There is a mention of the Vatican charging royalties for the Marble Sculptor and a discussion about the willingness of people to compare humans and AI, with some arguing that software should be treated differently.

The topic of payment for content emerges, with some expressing skepticism about AI software paying, while others argue that paying for content without explicit permission is not ethical.

The conversation concludes with a discussion about the value of knowledge and the potential threat to certain circles if private entities harvest and profit from it. The importance of knowledge sharing and the impact of individual contributions are also mentioned.

