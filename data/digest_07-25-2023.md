## AI Submissions for Tue Jul 25 2023 {{ 'date': '2023-07-25T17:10:15.182Z' }}

### Whom the gods would destroy, they first give real-time analytics (2013)

#### [Submission URL](https://mcfunley.com/whom-the-gods-would-destroy-they-first-give-real-time-analytics) | 157 points | by [sbdchd](https://news.ycombinator.com/user?id=sbdchd) | [64 comments](https://news.ycombinator.com/item?id=36870140)

A programmer at Etsy explains why real-time analytics may not be as useful as they seem. While engineers are inclined to see real-time data as beneficial, there are many ways in which it can lead to flawed decision-making. These include disregarding statistical significance testing, halting experiments as soon as significance is measured, and making decisions based on a short timeframe of data. The author argues that delayed analytics can actually be beneficial because it allows for more thorough analysis and prevents rash decision-making.

The discussion surrounding the submission revolves around the pros and cons of real-time analytics. Some commenters agree with the author, highlighting the potential pitfalls of relying too heavily on real-time data. They argue that real-time metrics can lead to flawed decision-making and the disregarding of statistical significance testing. Delayed analytics, on the other hand, allow for more thorough analysis and prevent rash decision-making. 

Other commenters, however, provide counterarguments. They suggest that real-time metrics can be valuable, especially in the early stages of development. Real-time metrics can help with faster product iterations and decision-making based on relevant trends. Additionally, they mention the importance of measuring conversion rates directly and the challenges in analyzing such data in real-time. 

The discussion also touches on related topics such as the politics and biases involved in metric selection, the difficulty of building commercial real-time analytics systems, and the practicality of implementing real-time analytics in different industries. 

Overall, there is a range of opinions regarding the usefulness and limitations of real-time analytics, with some expressing support for its benefits and others advocating for a more cautious approach.

### OpenAI shuts down its AI Classifier due to poor accuracy

#### [Submission URL](https://decrypt.co/149826/openai-quietly-shutters-its-ai-detection-tool) | 481 points | by [cbowal](https://news.ycombinator.com/user?id=cbowal) | [272 comments](https://news.ycombinator.com/item?id=36862850)

OpenAI has shut down its AI Classifier, a tool designed to detect whether a piece of content was created using generative AI tools such as its own ChatGPT. The tool was discontinued due to its low rate of accuracy. OpenAI acknowledged the importance of accurately detecting AI-written text, particularly in the education sector where there have been concerns about students using AI chatbots to write essays. The company stated that it is continuing to research more effective techniques for detecting AI-generated content and plans to develop new mechanisms to enable users to understand if audio or visual content is AI-generated.

The discussion on Hacker News revolves around various aspects of OpenAI's decision to shut down its AI Classifier tool and the challenges of detecting AI-generated content.

Some users express their support for OpenAI's decision, highlighting the difficulty of accurately detecting AI-written text. They mention that the tool was limited in its ability to differentiate between human-written and AI-generated content and that accurately detecting AI-generated content is a complex problem.

Others discuss the possible approaches to improving content detection, including using neural networks and backpropagation. Some users mention the need for more reliable and sophisticated indicators of content quality and truthfulness. They also discuss the limitations of current models like ChatGPT and the challenges in training AI to generate highly convincing and diverse content.

In response to concerns about students using AI chatbots to write essays, some users emphasize the importance of teaching critical thinking skills and the role of human discernment in evaluating content.

There are also discussions about the use of watermarks as a method of identifying AI-generated content. Some users highlight the potential limitations of watermarking, as it can be easily removed or rewritten by AI models.

The conversation delves into topics such as consensus reality, the involvement of cryptographic signing, and the control of information in a post-AI world. Some users debate the existence of consensual reality and the manipulation of truth by powerful entities.

There are also discussions on the capabilities of ChatGPT in conveying information and mimicking human writing styles. Some users highlight that ChatGPT has been explicitly trained on human writing and attempts to mimic distinct writing styles based on prompts. They note that the content generated by ChatGPT often sounds confident and authoritative, and it can combine various writing styles in its responses.

Overall, the discussion reflects the complexity and challenges associated with detecting AI-generated content and highlights the need for further research and development in this area.

### ONNX runtime: Cross-platform accelerated machine learning

#### [Submission URL](https://onnxruntime.ai/) | 146 points | by [valgaze](https://news.ycombinator.com/user?id=valgaze) | [34 comments](https://news.ycombinator.com/item?id=36863522)

The top story on Hacker News today is about ONNX Runtime, a cross-platform tool for accelerating machine learning processes. It includes built-in optimizations that can deliver up to 17 times faster inferencing and up to 1.4 times faster training. ONNX Runtime is designed to be easily integrated into existing technology stacks and supports a variety of frameworks, operating systems, and hardware platforms. The technology behind ONNX Runtime is already used in popular products like Office 365, Visual Studio, and Bing, where it delivers over a trillion inferences every day. The post also encourages readers to participate in a customer survey to help improve ONNX Runtime.

The discussion on the submission about ONNX Runtime covers a variety of topics related to the technology.

- One commenter mentions that Microsoft recently worked on deploying ONNX-based models to Azure and mentions the Llama 2 Azure project.
- Another commenter indicates that ONNX only provides information about the ONNX runtime working with MLDL0.
- A user named "Roark66" points out a few limitations of ONNX, such as the 2GB limit on serialized files and difficulties in partitioning existing large models.
- There is a back-and-forth discussion about the size limits of serialized files and memory representation, with suggestions for increasing the limit or finding alternatives.
- Commenters discuss various aspects of ONNX, including its support for different frameworks and the difference between training and inference.
- Some debate arises regarding the limitations and optimizations of ONNX Runtime for deployment, with comparisons to other frameworks and discussion of specific use cases.
- A user called "Zetobal" mentions the biggest problem with ONNX models being reshaping.
- Other users express excitement about different aspects of ONNX Runtime, such as its compatibility with other languages and the potential for different backends.
- Some users mention other related projects such as StableHLO, Tinygrad, and Triton Inference Server.
- The ease of installation and the speed of development in the ML community are discussed, with mentions of the ONNX project's progress in the past five years.
- There are mentions of running ONNX models in the browser and links to relevant blog posts.
- Several comments touch on the limitations and memory consumption of ONNX, as well as alternatives like Tinygrad, TVM, and OpenVino.

Overall, the discussion covers a range of perspectives and considerations related to ONNX Runtime and its usage in the machine learning community.

### Vectorization of Raster Manga by Primitive-Based Deep Reinforcement Learning

#### [Submission URL](https://github.com/SwordHolderSH/Mang2Vec) | 35 points | by [PaulHoule](https://news.ycombinator.com/user?id=PaulHoule) | [3 comments](https://news.ycombinator.com/item?id=36862376)

The Mang2Vec project is a PyTorch implementation of "Vectorization of Raster Manga by Deep Reinforcement Learning." It aims to convert raster manga images into vector graphics using deep reinforcement learning techniques. The project provides demos, installation instructions, and a quick start guide in its README file. If you're interested in this research or using the code for your own work, don't forget to cite the original paper. The project has received 43 stars on GitHub and has 2 forks.

The discussion on the submission begins with a user named "brnkwsk" noting that manga panels appear smaller below the poster size, which seems to be a visual effect. Another user named "PaulHoule" shares a link to the arXiv paper related to the project. Lastly, a user named "rowanG077" comments that the project is significant in terms of replicating pixel structure in Adobe Illustrator. However, it seems that the comment may have been cut off, as the ending is unclear.

### What we know about LLMs

#### [Submission URL](https://willthompson.name/what-we-know-about-llms-primer) | 345 points | by [wilhelm____](https://news.ycombinator.com/user?id=wilhelm____) | [158 comments](https://news.ycombinator.com/item?id=36860992)

In a recent article titled "What We Know About LLMs," author Will Thompson dives into the world of Large Language Models (LLMs) and explores what we currently know about them. LLMs, which are a type of deep learning architecture known as Transformers, have been garnering a lot of attention lately with their potential to create immeasurable wealth for society while also posing a threat to knowledge workers.

Thompson highlights the current AI fervor and how many companies, including the big tech giants, are investing heavily in LLMs. He also notes that LLMs have become the focus of research efforts and are being adopted by a large percentage of startups in Y Combinator's cohort.

To understand what LLMs are, Thompson explains that they are models that work with sequence data, such as text or images, and learn the contextual relationships between values within a sequence through a mechanism called attention. Unlike previous models like Recurrent Neural Networks (RNNs), Transformers can process the entire sequence at once, allowing for faster training times and larger model parameter sizes.

Thompson categorizes Transformers into three main types: "encoder only," "decoder only," and "encoder-decoder" architectures. Each type has its own strengths and is suited for different tasks, such as sentiment classification or language translation.

The article also reflects on what we've learned about LLMs so far. One key insight is that LLMs have the ability to generalize, meaning they can complete various tasks with only a few examples. Additionally, LLMs exhibit predictable scaling behavior, with larger models becoming more data-efficient and performing better on benchmarks.

Overall, the article provides a comprehensive overview of LLMs, shedding light on their potential and the current state of research in this field. With the AI industry buzzing with excitement, it's essential to understand the capabilities and implications of these powerful language models.

Discussion Summary:

- One user expresses skepticism about the hype surrounding LLMs and notes that some of the arguments advocating for them seem faulty.
- Another user shares their positive experience using Github Copilot, a tool that integrates LLMs into the coding process, significantly improving productivity.
- The discussion veers towards the topic of ORM (Object-Relational Mapping) tools and their impact on productivity, with different opinions expressed.
- Some users share examples of AI-generated content, including market copy and chat responses, highlighting both the potential and potential issues with LLMs.
- The conversation touches on the use of LLMs in various applications, such as office integration and text messaging systems.
- References are made to Clippy, a virtual assistant from Microsoft, and other AI-related stories and concepts.
- Some users discuss the relevance of Gartner's Hype Cycle in the context of LLMs and technology trends in general, with varying opinions on its usefulness and credibility.
- The discussion concludes with a user pointing out the importance of considering the ingestion and creation of large-scale language models, as well as the limitations and potential risks associated with them.

### JPMorgan warns that an AI bubble is brewing

#### [Submission URL](https://markets.businessinsider.com/news/stocks/stock-market-outlook-jpmorgan-bearish-ai-bubble-mega-cap-tech-2023-7) | 45 points | by [1vuio0pswjnm7](https://news.ycombinator.com/user?id=1vuio0pswjnm7) | [19 comments](https://news.ycombinator.com/item?id=36869983)

JPMorgan's Marko Kolanovic remains bearish on the stock market and warns of an AI bubble that is forming. He points out that stock concentration in the S&P 500 is at a 60-year high, with the top seven companies accounting for over 25% of the index. Kolanovic believes this concentration, along with other anecdotal evidence, indicates a bubble caused by the hype around artificial intelligence. While he recognizes the potential of AI technologies, he argues that they are not yet ready for mainstream adoption. Kolanovic also highlights three bearish catalysts that could trigger a significant market sell-off: the delayed impact of the global interest rate shock, erosion of consumer savings, and the troubled geopolitical landscape. He recommends investing in commodities, which he believes are undervalued and backed by strong fundamentals. Kolanovic is not alone in his bearish outlook, as other Wall Street strategists, like Morgan Stanley's Mike Wilson, also expect a market decline.

The discussion on this submission covers various topics related to AI, Apple stock, and financial markets. Here are some key points:

- One user mentions that Apple's stock price loss of $50 billion is negligible compared to its total market fund, which raises a question about the relevance of Apple stock in relation to the AI bubble.
- Another user suggests that Apple's stock pricing is related to self-driving electric cars.
- There is a mention of Waste Management being an investment opportunity related to AI.
- A user comments on the lack of trust in Business Insider and how financial strategies are not always beneficial for the audience.
- The concentration of large companies and the riskiness of investment in small, unestablished companies is discussed.
- The idea of a bubble is linked to various technologies like NFTs, cryptocurrencies, AR/VR, and the Metaverse. It is argued that the AI bubble is a result of FOMO and people losing their savings.
- The disappointment with AI's practical applications is mentioned, with a user giving an example of machine translation not being as effective as professional translators.
- A link to an archive of an article related to the discussion is shared.
- A user mentions the increasing severity of climate change and the need for people to study physics and work on solving the problem rather than focusing on AI.
- The potential improvement in productivity due to AI and its impact on various domains, including law and sustainable development, is discussed.

Overall, the discussion delves into different aspects of the AI bubble, the stock market, and the broader implications of these trends.

### Robo-Taxis are rolling, did you notice?

#### [Submission URL](https://cmte.ieee.org/futuredirections/2023/07/25/robo-taxi-are-rolling-did-you-notice/) | 32 points | by [kungfudoi](https://news.ycombinator.com/user?id=kungfudoi) | [23 comments](https://news.ycombinator.com/item?id=36858633)

Robo-Taxis are quietly making their way onto the streets, and they are set to revolutionize the transportation industry. In a recent blog post, Roberto Saracco highlights the progress of self-driving cars for public transportation. While the initial hype around robo-taxis soared a few years ago, the industry experienced a setback as investors lowered their expectations. However, companies like Baidu and Waymo have quietly been deploying robo-taxis and are now reaping the benefits. Baidu is operating robo-taxis in Beijing, while Waymo has doubled its service area in Phoenix and is preparing to launch in San Francisco. Uber is also planning to incorporate Waymo into its fleet, with the goal of having up to 20% of its rides managed by robo-taxis by 2025. With these developments, the industry is reaching the plateau of productivity on the Gartner Hype Cycle. The future of transportation is here, and it's autonomous.

Discussion Summary:

- One user points out that robo-taxis in European cities may face challenges due to narrow streets and complicated traffic conditions. They also mention that the regulatory environment in Europe is different from that in the US, with stricter regulations and a lack of a framework for the deployment of autonomous vehicles.
- Another user discusses the weather conditions in Europe and how they can affect the development and deployment of self-driving cars. They argue that the regulatory environment in Europe is not conducive to rapid progress.
- Some users express concerns about the safety of autonomous vehicles on regular roads, particularly when it comes to interacting with human drivers, pedestrians, and cyclists.
- A user brings up the issue of level crossings and barriers when it comes to autonomous trains, suggesting that there are still challenges to overcome in making public transportation fully autonomous.
- The topic of driverless transport is discussed, with one user mentioning that driverless trams and trains are already a common sight in certain locations. They argue that autonomous systems can be more reliable and safe compared to human-operated transportation.
- A user responds to the concern about autonomous trains and explains that modern train systems already have automated signaling, brake distance management, and driver enforcement of safety protocols. They argue that self-driving systems can enhance the capabilities of trains and improve safety.
- The conversation goes on to discuss fully automated transport systems, with one user mentioning their experience riding in fully automated transit systems around the world, specifically mentioning PRT (Personal Rapid Transit) as a type of fully automated transport system.

Overall, the discussion revolves around the challenges and potential benefits of robo-taxis and autonomous transportation systems. There are concerns about regulatory issues, weather conditions, and the interactions between autonomous vehicles and other road users. However, there is also recognition of the potential safety and efficiency advantages that autonomous transport can bring.

### AI is being used to create child sex abuse images and also to prevent them

#### [Submission URL](https://news.yahoo.com/ai-is-being-used-to-create-child-sex-abuse-images-its-also-being-used-to-prevent-them-192951595.html) | 20 points | by [yenniejun111](https://news.ycombinator.com/user?id=yenniejun111) | [8 comments](https://news.ycombinator.com/item?id=36870842)

Artificial intelligence (AI) technology has taken a dark turn as bad actors are using open-source forms of AI, like ChatGPT, to create sexual images of children. These AI-generated child sex abuse materials (CSAM) are becoming increasingly prevalent, with thousands of images being created and shared across the internet. Users on the dark web are sharing detailed instructions on how to create realistic AI images of children engaged in sexual acts. While the issue is still relatively small, experts are urging for proactive measures to prevent it from growing further. AI tools like DALL-E and Stable Diffusion allow users to generate lifelike images by describing what they want to see. While organizations like OpenAI, the creator of ChatGPT, are working to implement protections to prevent CSAM, there is debate surrounding the legality of these AI-generated images. Some believe they violate federal child protection laws, while others argue that since the children depicted are not real, they should not be considered illegal. The biggest challenge in combating this issue is the lack of visibility, as the images cannot be shown or shared to raise awareness. Thorn, a nonprofit organization founded by Ashton Kutcher and Demi Moore, is working on victim identification, stopping revictimization, and preventing abuse in the first place. It is essential for parents to be cautious in the digital age and avoid sharing explicit photos of their children as perpetrators can misuse them.

The discussion surrounding the submission mainly revolves around the following points:

1. The movie "Artifice Girl" is recommended as an interesting and relevant watch, as it touches on metaphysical topics related to consciousness and consent.

2. One commenter emphasizes the importance of educating children about the dangers of sharing explicit photos, as these can be misused by perpetrators.

3. A discussion arises about the correlation between consuming media and harmful effects on children. Some argue that there might be benefits in developing materials to redirect harmful tendencies, while others express skepticism and point out the limited evidence supporting this idea.

4. The issue of artificial child sexual abuse materials (CSAM) and the potential harm they can cause is brought up. One commenter argues that the production of AI-generated CSAM might not significantly impact actual victimization, as it is likely that those who would engage in such behavior already exist in the market for real CSAM.

5. A suggestion is made that disrupting the black market for CSAM may be an effective approach to combating the problem, as the primary motivation for producers is financial gain.

6. The legal implications of AI-generated CSAM are discussed, with some arguing that it should be treated the same as real child exploitation materials, while others believe that the lack of real victims makes it a different issue.

7. The need for surveillance and censorship measures to catch and prevent the creation and distribution of harmful AI-generated materials is mentioned, with the suggestion of stricter regulations to curb the production of such content.

Overall, the discussion highlights the complexity of the issue at hand, including the legal, ethical, and practical challenges in combating AI-generated child sexual abuse materials.

### Show HN: Prosona – Your co-pilot for intelligent responses in Slack

#### [Submission URL](https://www.prosona.ai) | 13 points | by [hgunasekara](https://news.ycombinator.com/user?id=hgunasekara) | [17 comments](https://news.ycombinator.com/item?id=36858102)

Introducing Prosona, the co-pilot for intelligent responses in Slack. Are you tired of spending valuable time on routine queries? Prosona is here to augment domain experts within your company, freeing up their time to focus on their core responsibilities. With its sophisticated search capabilities, Prosona provides the most relevant results in seconds, blending the best of vector and keyword search.

But that's not all. Prosona is a privacy-first system, ensuring that your internal knowledge remains secure and your secret sauce remains secret. It also promotes documentation, preserving vital knowledge even when an employee leaves. Plus, it increases accountability by allowing domain experts to oversee shared knowledge, stopping outdated or incorrect information from spreading.

And the best part? Prosona offers hassle-free integration, eliminating the complexities and risks associated with in-house AI tool development. Connect to all stores of knowledge seamlessly.

Don't just take our word for it. Adrian Sarstedt, CTO of Sphere, says that Prosona has saved him so much time on routine queries while keeping his unique tone intact. Nina Brown, Product Manager at CCV, and Brian Chan, Software Engineer at a startup, also vouch for its effectiveness.

So, why burden yourself with crafting routine replies when you can focus on what truly matters – meeting your OKRs, making an impact, or enjoying your downtime? Join the Prosona beta waitlist now and empower yourself with an intelligent co-pilot in Slack.

The discussion begins with smt88 pointing out the importance of protecting encrypted vector databases and questioning how Prosona achieves this. hgnskr responds by mentioning a similar approach taken by Cloak AI in encrypting vector embeddings. smt88 argues that without detailed information, it's essentially impossible to provide a service with zero-knowledge encryption. hgnskr agrees, stating that privacy and encryption are important considerations.

stitched2gethr jumps in to mention that Prosona shouldn't have access to colleague-level status data and suggests exploring AI settings and sharing information through screen sharing. hgnskr agrees and acknowledges that documentation is imperfect but emphasizes the importance of providing additional context and information to make informed decisions.

lrshd comments that people are lazy and asks how relying on systems like Prosona promotes learning and keeping up with job-related questions. hgnskr responds by saying that streamlining workplace communication allows people to focus on important tasks.

pavel_lishin asks if Prosona acts as a system that integrates knowledge stores and learns in order to generate personalized messages. hgnskr confirms that Prosona is designed to integrate and learn from various sources to provide personalized responses.

xn expresses a dystopian view and argues that trusting machines to handle private information is not ideal. They suggest that true privacy is unlikely unless humans truly control the technology. hgnskr responds by saying that Prosona aims to strike a balance by providing awareness and human oversight in message approvals.

brgnclndr simply mentions the product "Glean" as an integrated workflow solution that helps answer questions and compares it to Prosona.

Overall, the discussion revolves around the implementation and privacy concerns of Prosona, with some users expressing skepticism and others providing insights and suggestions for improvement.

### Retentive Network: A Successor to Transformer Implemented in PyTorch

#### [Submission URL](https://github.com/Jamie-Stirling/RetNet) | 11 points | by [panabee](https://news.ycombinator.com/user?id=panabee) | [3 comments](https://news.ycombinator.com/item?id=36857245)

RetNet is an implementation of "Retentive Network: A Successor to Transformer for Large Language Models" in PyTorch. The code prioritizes correctness and readability over optimization, and it aims to aid scientific and technological understanding and advancement. The features implemented include single-scale and multi-scale retention, multi-layer retentive network with FFN and LayerNorm, and a causal language model built on top of the retentive network. The contributors to this repository are not authors of the original paper, but they have implemented the ideas and formulations described in the paper. The repository welcomes contributions, and examples of basic usage can be found in the test scripts.

The discussion on Hacker News about the submission titled "RetNet: An implementation of 'Retentive Network: A Successor to Transformer for Large Language Models' in PyTorch" mainly consists of two comments.

The first comment by user "wstrnr" provides links to the original research paper, as well as to details about the implementation on various platforms like HuggingFace and TorchScale.

The second comment by user "pnb" seems to be incomplete and does not provide any meaningful information. However, there is a nested comment by user "ttctcyf" that mentions a related repository from Microsoft called "torchscale," suggesting that it might be worth exploring.

Overall, the discussion seems to be limited and lacks in-depth analysis or further engagement from the Hacker News community.

