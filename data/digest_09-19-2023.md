## AI Submissions for Tue Sep 19 2023 {{ 'date': '2023-09-19T17:10:35.468Z' }}

### Graph Neural Networks use graphs when they shouldn't

#### [Submission URL](https://arxiv.org/abs/2309.04332) | 126 points | by [Pseudomanifold](https://news.ycombinator.com/user?id=Pseudomanifold) | [20 comments](https://news.ycombinator.com/item?id=37571535)

Graph Neural Networks (GNNs) have become a popular approach for learning on graph data in various domains. However, a new paper titled "Graph Neural Networks Use Graphs When They Shouldn't" by Maya Bechler-Speicher and her colleagues challenges the assumption that GNNs always make accurate predictions based on graph structure. The researchers show that GNNs tend to overfit the graph structure, even when it is non-informative for the predictive task. They provide a theoretical explanation for this phenomenon and propose a graph-editing method to mitigate the overfitting. The paper concludes with empirical evidence that this method improves the accuracy of GNNs across multiple benchmarks. This research has implications for the use of GNNs in fields such as social networks, molecular biology, and medicine.

The discussion on the submission revolves around various aspects of Graph Neural Networks (GNNs) and their use in learning on graph data. Here are some key points raised by the commenters:

- One commenter mentions that GNNs tend to overfit the graph structure, even when it is non-informative for the predictive task at hand. They provide links to the research paper challenging this assumption.
- Another commenter suggests that attention layers and nested graph convolution layers can help GNNs learn graph structures effectively.
- There is a discussion on the use of graph editing and graph representation in mitigating overfitting in GNNs.
- Some commenters share their experiences with working on GNNs and highlight the importance of studying the behavior and dynamics of GNNs.
- The limitations and challenges of using GNNs in practical applications are also mentioned, such as computational complexity and the need for regularization techniques.
- It is pointed out that GNNs can have a problem of overfitting due to imbalanced class distribution and dependence on specific graph interactions.
- Several papers and research works related to GNNs are shared, covering topics like message passing, algorithmic reasoning, diffusion, sparsity, training tricks, expressive power, and over-squashing.

Overall, the discussion highlights the potential issues and solutions related to the use of GNNs in various domains.

### The physical process that powers a new type of generative AI

#### [Submission URL](https://www.quantamagazine.org/new-physics-inspired-generative-ai-exceeds-expectations-20230919/) | 96 points | by [digital55](https://news.ycombinator.com/user?id=digital55) | [14 comments](https://news.ycombinator.com/item?id=37570743)

Physicists at MIT have introduced a new method of generative AI called the Poisson flow generative model (PFGM). Rather than using black box algorithms like traditional neural networks, PFGM is based on the principles of diffusion and the distribution of charged particles. PFGM represents data with charged particles that create an electric field, and the model learns to estimate that electric field through the training process. This allows PFGM to generate high-quality images, similar to diffusion-based models, but at a much faster speed. The use of physical processes in AI models could open the door to harnessing other physical phenomena to improve neural networks.

The discussion on Hacker News surrounding the submission about the Poisson flow generative model (PFGM) involves various perspectives. 

One commenter points out that the concept of Boltzmann Machines is nothing new, and the use of black box algorithms in neural networks has been replaced by diffusion-based models. Another commenter adds that implementing Poisson flow generative models could be challenging due to memory constraints, but a breakthrough in GPU RAM manufacturing could potentially solve this issue. The discussion then diverges into a debate about the reliance on AI SaaS subscription services and the associated costs.

Another thread of the discussion touches on the idea that physical processes can be effectively modeled in neural networks, opening up possibilities for incorporating other physical phenomena. However, a commenter from the World Economic Forum mentions the challenges in accurately predicting service waiting times.

Some comments express interest in the differences between Poisson flow generative models and traditional diffusion models, while others discuss the potential benefits of utilizing physical processes in network modeling.

One commenter wonders why the article does not provide direct comparisons between Poisson flow generative models and other diffusion models, while another commenter provides a link to relevant research papers.

There is also appreciation for the elegance of incorporating principles from physics into AI models. However, someone points out that counting on quantum computing to solve such problems might be wishful thinking.

Lastly, there is a discussion about how swapping decoding techniques in NLP frequently leads to generating novelty in text generation tasks, but traditional methods still have their merits. The debate focuses on the trade-off between control and novelty in generating text.

### 64-bit bank balances ‘ought to be enough for anybody’?

#### [Submission URL](https://tigerbeetle.com/blog/2023-09-19-64-bit-bank-balances-ought-to-be-enough-for-anybody/) | 237 points | by [todsacerdoti](https://news.ycombinator.com/user?id=todsacerdoti) | [347 comments](https://news.ycombinator.com/item?id=37568856)

TigerBeetle, a systems programming company, has decided to use 128-bit integers to store financial amounts and balances, moving beyond the previous use of 64-bit integers. While some may argue that 64 bits is sufficient, TigerBeetle realized they needed to go beyond this limit to store all kinds of transactions adequately. By using binary encoding, computers can represent numbers, and larger numbers require more bits. Fractions and decimal numbers pose challenges for computers, as they cannot accurately express decimal numbers using binary floating point. TigerBeetle solves this problem by representing money as whole numbers, using a minimal integer factor defined by the user. They also avoid using negative numbers and instead keep separate positive integer amounts for debits and credits. 128-bit integers are necessary to represent values smaller than a cent and meet the precision and scale requirements of various applications. TigerBeetle's database, called TigerBeetle, can count not only money but anything that can be modeled using double-entry accounting, such as inventory items or API calls. The company also considers the future-proof aspect of their system, as long-running systems can accumulate high transaction volumes over time. Unexpected events like hyperinflation can also push a currency towards the upper limits of a 64-bit integer, making 128-bit integers a necessary choice. Overall, TigerBeetle's decision to switch to 128-bit integers ensures more robust and flexible financial storage capabilities.

The discussion on this submission revolves around the use of decimal points and rounding in financial software. Some commenters discuss the problems that arise when handling fractions and decimals in computer systems and emphasize the importance of accurately calculating taxes and sales transactions. Others mention the different regulations and rules in different jurisdictions regarding rounding and decimal precision. Some commenters express surprise at the number of people who overlook decimal precision and make mistakes in billing and financial calculations. There is also discussion about the use of integer arithmetic and the limitations of binary representation in computers. Overall, commenters emphasize the need for precise and accurate financial calculations and highlight the challenges and potential pitfalls in implementing billing and accounting software.

### Google DeepMind's AI successor predicts how 71M mutations cause disease

#### [Submission URL](https://endpts.com/google-deepminds-alphafold-successor-predicts-how-71m-mutations-cause-disease/) | 52 points | by [birriel](https://news.ycombinator.com/user?id=birriel) | [7 comments](https://news.ycombinator.com/item?id=37578616)

Google DeepMind has announced the development of a new AI system called AlphaMissense. This technology is the successor to AlphaFold, which was known for its ability to predict the structures of proteins. AlphaMissense, on the other hand, focuses on predicting the likelihood that genetic mutations, specifically 71 million missense mutations, will cause disease. Each missense mutation refers to a single-letter change in an amino acid that makes up a protein sequence. The announcement of AlphaMissense comes alongside the publication of a paper in the journal Science.

The discussion surrounding the announcement of Google DeepMind's AlphaMissense AI system on Hacker News includes several comments:

1. User "blvl" wrote a quick script that checked 23andme data and found various percentages of mutated genes associated with different conditions.
2. User "kgtsmn" thanked "blvl" for the information and added percentages of mutated genes classified as benign, pathogenic, and likely benign across different classifications.
   a. "kgtsmn" also mentioned the MTHFR C677T mutation and its association with reduced enzyme activity and elevated homocysteine levels in individuals with decreased activity in the AA genotype.
3. "blvl" responded, stating that the rabbit hole goes deep and mentioned the SIRT1 mutation's association with longevity traits.
4. User "pknmd" expressed their understanding of 23andme data and questioned the need for medical professionals due to the self-reported nature of the data. They also found it interesting to compare the percentages of pathogenic and non-pathogenic mutated genes.
5. User "pfd1986" shared a link to the published paper in the journal Science regarding AlphaMissense and an additional link to a non-paywalled article about it.
6. User "PBnFlash" speculated about the potential impact of powerful AI systems like AlphaMissense on healthcare and medical research.
7. User "7e" made a general comment about experts not making decisive guesses.

Overall, the discussion involved users sharing their findings, questioning the need for medical professionals in analyzing genetic data, and discussing the potential implications of AI systems like AlphaMissense in the field of healthcare and genetics.

### The Princeton researchers calling out ‘AI snake oil’

#### [Submission URL](https://www.semafor.com/article/09/15/2023/the-princeton-researchers-calling-out-ai-snake-oil) | 32 points | by [irtefa](https://news.ycombinator.com/user?id=irtefa) | [7 comments](https://news.ycombinator.com/item?id=37576259)

The Princeton researchers, Arvind Narayanan and Sayash Kapoor, behind the popular newsletter and upcoming book "AI Snake Oil" are on a mission to dispel hype and clarify the limits of AI. They focus on distinguishing between predictive AI and generative AI, with most of the snake oil concentrated in predictive AI. They highlight the lack of statistical validity in certain predictive AI applications, such as AI hiring tools. They also express concerns about the potential flood of disinformation from generative AI, but argue that addressing other AI-related harms, like non-consensual deepfakes, should take precedence. The researchers propose that AI companies publish regular transparency reports to shed light on potential harms and usage patterns. They also discuss the need for better controls on the open access archive arXiv.org to prevent misinterpretation of AI research studies.

The discussion on this submission seems to cover a range of topics related to AI and its limitations:

1. Some commenters discuss the distinction between predictive AI and generative AI, with the consensus that most of the "snake oil" is concentrated in predictive AI.
2. A link is shared regarding the challenges of replacing scientific reproducibility with machine learning approaches.
3. The potential risks of AI are debated, with one comment suggesting that the greatest risk comes from humans controlling the technology.
4. There is a discussion about the potential shortcomings of GPT-4 when it comes to professional benchmarks and generating the correct answers to wrong questions.
5. The capabilities of ChatGPT as a "bullshit generator" are mentioned, with some being impressed by its ability to generate seemingly plausible responses.
6. A suggestion is made to focus on addressing non-consensual deepfakes and the spread of misinformation as priorities rather than the harms of generative AI.
7. The idea of companies publishing transparency reports to shed light on potential harms and usage patterns of AI is proposed.
8. Concerns are raised about the need for better controls on the open access archive arXiv.org to prevent misinterpretation of AI research studies.
9. A link to an archive discussing the potential dystopian aspects of AI is shared.
10. A commenter expresses their amusement with the ongoing discussion and suggests not taking it too seriously.

The conversation also includes some meta-discussion, with one commenter requesting others not to engage in shallow dismissals and to provide constructive criticism. Another commenter flags a comment as snarky.

### Tackling the curse of dimensionality with physics-informed neural networks

#### [Submission URL](https://arxiv.org/abs/2307.12306) | 75 points | by [jhoho](https://news.ycombinator.com/user?id=jhoho) | [17 comments](https://news.ycombinator.com/item?id=37565140)

In a recent paper submitted to arXiv, researchers Zheyuan Hu, Khemraj Shukla, George Em Karniadakis, and Kenji Kawaguchi introduce a new method for tackling the curse of dimensionality with Physics-Informed Neural Networks (PINNs). The curse of dimensionality refers to the heavy computational burden that exponentially increases as the dimensionality of a problem increases. The authors propose a method called Stochastic Dimension Gradient Descent (SDGD) that decomposes the gradient of Partial Differential Equations (PDEs) into pieces corresponding to different dimensions and randomly samples a subset of these dimensional pieces in each iteration of training PINNs. The proposed method has been experimentally demonstrated to solve notoriously hard high-dimensional PDEs, such as the Hamilton-Jacobi-Bellman and Schrödinger equations, in thousands of dimensions very quickly on a single GPU. In fact, the researchers were able to solve nontrivial nonlinear PDEs in 100,000 dimensions in just 6 hours on a single GPU using SDGD with PINNs. This new method has the potential to scale up the solving of arbitrary high-dimensional PDEs using PINNs.

The discussion on this submission covers various topics related to the dimensions and complexity of problems, as well as the potential advantages of quantum computers.

One user notes that in machine learning, vectors are typically considered to have numerical properties, while in physics, vectors can represent multiple dimensions. Another comment clarifies that the confusion arises from how different disciplines define and describe dimensions in their specific contexts.

Another user mentions that solving the Schrödinger equation in thousands of dimensions is possible for non-quantum mechanical systems, but it becomes more challenging for quantum-hard problems. A response to this comment suggests trying to solve the quantum harmonic oscillator potential, which is analytically solvable. However, another user points out that the Schrödinger equation is a separable differential equation that implies a specific network structure, which may not apply in general cases.

The discussion then moves to the advantages of quantum computers in solving complex problems. One user mentions that classical computers can calculate mean field energies for thousands of interacting electrons, but quantum computers have an advantage when it comes to calculating exchange correlation energies for interacting electrons. Another user adds that the evaluation of transition probabilities and energy differences can also be advantageous in quantum computers.

Ultimately, the comments touch on various aspects related to the dimensions and complexity of problems, highlighting differences in approaches between machine learning and physics, and discussing the potential advantages of quantum computers in solving complex equations.

