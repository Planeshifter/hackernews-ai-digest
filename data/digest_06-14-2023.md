## AI Submissions for Wed Jun 14 2023 {{ 'date': '2023-06-14T17:11:45.911Z' }}

### Show HN: Agency – Unifying human, AI, and other computing systems, in Python

#### [Submission URL](https://github.com/operand/agency) | 43 points | by [0perand](https://news.ycombinator.com/user?id=0perand) | [6 comments](https://news.ycombinator.com/item?id=36326587)

"Agency" is a Python framework for unifying human, AI, and traditional computing systems. The platform defines a common communication and action framework that allows shared environments called "spaces" to be established, where multiple agents can address each other as individuals and perform actions on each other. The framework handles the details of the common messaging protocol and allows discovering and invoking actions across all parties in the space. Users can integrate the framework with various systems, such as UI-driven applications, terminal environments, and other AI agents.

The submission discusses "Agency," a Python framework designed for integrating human, AI, and traditional computing systems. The platform provides a common communication and action framework, allowing multiple agents to interact in shared environments called "spaces." Comments on the post commend the author for creating an abstract framework that focuses on high-level thinking and objective choices. However, some users raise concerns regarding the GIL, single-process assembly, and difficulty in running Flask servers with child threads. The README is praised for being objective and easy to understand with quick and significant sample code. Several users also mention their excitement about the use of such a library in building integrated systems. One user expresses confusion about the feedback system, while another praises the flexibility of integrating various agents regardless of the system used, including non-Python projects.

### Native JSON Output from GPT-4

#### [Submission URL](https://yonom.substack.com/p/native-json-output-from-gpt-4) | 566 points | by [yonom](https://news.ycombinator.com/user?id=yonom) | [237 comments](https://news.ycombinator.com/item?id=36330972)

OpenAI has released a new feature called function calling, which allows for GPT to call a function instead of returning a string. This makes it much simpler to generate structured data, such as JSONs, when integrating LLMs into products. By setting the function_call parameter, developers can reliably expect JSON as responses from GPT calls. This has implications for how we interact with OpenAI LLMs beyond plugins. This feature is available for the chat models gpt-3.5-turbo-0613 and gpt-4-0613.

OpenAI has recently released a new function calling feature that enables GPT to call a function instead of returning a string, thus making it easier to generate structured data like JSONs and integrate LLMs into products. However, some comments suggest that OpenAI's documentation does not include security checks such as human validation or content validation for SQL queries and that relying on GPT for direct SQL queries increases the risk of compromise. Others suggest that a solution could be to build completely isolated backends with a restricted data access layer and not allow direct access to databases. Additionally, the discussion delves into the potential security risks involved with ChatGPT directly connecting with SQL databases and the importance of implementing secure features built into databases. Some comments also talk about the uncertainty and lack of clarity in OpenAI's documentation regarding certain security measures and urge developers to take the necessary precautions. There is also a discussion about the potential capabilities of GPT-4 functioning as a transformation layer and how it could be used in developing secure chat applications. Furthermore, a comment describes a solution that utilizes JSON-based schema to ensure that the JSON structure is valid.

### Nvidia H100 and A100 GPUs – comparing available capacity at GPU cloud providers

#### [Submission URL](https://llm-utils.org/Nvidia+H100+and+A100+GPUs+-+comparing+available+capacity+at+GPU+cloud+providers) | 168 points | by [tikkun](https://news.ycombinator.com/user?id=tikkun) | [115 comments](https://news.ycombinator.com/item?id=36333321)

Hi there! I'm excited to help you write a daily digest of the top stories on Hacker News. What kind of stories are you interested in summarizing?

The discussion revolves around the usage and comparison of different types of GPUs for machine learning, particularly the H100 versus A100. Some commenters mention difficulties with using PyTorch containers and issues with the NVIDIA hardware, while others discuss the challenges of large-scale experimentation and the cost of these GPUs. There is also some debate about the viability and competitiveness of non-NVIDIA GPU options. Finally, there is some discussion about the importance of CUDA in programming and the technical specifications of different GPUs.

### Recovering secret keys from devices using video footage of their power LED

#### [Submission URL](https://www.nassiben.com/video-based-crypta) | 322 points | by [jedisct1](https://news.ycombinator.com/user?id=jedisct1) | [92 comments](https://news.ycombinator.com/item?id=36331446)

Researchers from Cornell Tech and Ben-Gurion University have developed a new method called "Video-Based Cryptanalysis" that allows attackers to recover secret keys from non-compromised devices using video footage of their power LED. They were able to exploit commercial video cameras, even an iPhone 13, to increase the sampling rate by three orders of magnitude, thus enabling the recovery of secret keys. They were able to demonstrate this by recovering a 256-bit ECDSA key from a smartcard and a 378-bit SIKE key from a Samsung Galaxy S8. Countermeasures are discussed, and the researchers warn that video-based cryptanalysis could prove a serious security risk and should be addressed by hardware manufacturers and cryptographic software developers.

Researchers from Cornell Tech and Ben-Gurion University have developed a new method that allows attackers to recover secret keys from non-compromised devices using video footage of their power LED. The researchers used commercial video cameras, including an iPhone 13, to exploit the power LED and increase the sampling rate by three orders of magnitude. By using this method, they recovered a 256-bit ECDSA key from a smart card and a 378-bit SIKE key from a Samsung Galaxy S8. The discussion on Hacker News includes suggestions for countermeasures, such as using a capacitor or covering LEDs with tape. Some comments highlight the need for high-resolution cameras to enable the sampling frequency required by the attack. The discussion also includes general skepticism about the practicality of this attack on budget-level devices and the need for secure key generation techniques.

### AMD openSIL open source firmware proof of concept

#### [Submission URL](https://github.com/openSIL/openSIL) | 205 points | by [wmf](https://news.ycombinator.com/user?id=wmf) | [41 comments](https://news.ycombinator.com/item?id=36331394)

The AMD open Silicon Initialization Library (openSIL) has released a Proof-of-Concept (POC), which showcases the collection of C libraries that can be compiled with an x86 host firmware. It consists of three statically linked libraries, namely xSIM, xPRF, and xUSL. While the source for these libraries is still undergoing final review, it can be compiled directly, or it can be linked with static libraries. The open-source project is available on GitHub, and the libraries can be cloned and built for 32bit and/or 64bit compilation and static libraries. The project supports both the GNU/GCC or LLVM/clang tool chain and the Microsoft Visual C tool chain.

AMD has released a proof-of-concept (POC) for their open Silicon Initialization Library (openSIL) which includes three statically-linked libraries that can be compiled with x86 host firmware, namely xSIM, xPRF, and xUSL. These C libraries can be compiled directly, or they can be linked with static libraries to support both 32-bit and 64-bit compilation and static libraries. AMD's open-source project supports the LLVM/clang or GNU/GCC tool chain and the Microsoft Visual C tool chain. Commenters discussed related topics such as the potential for improving transparency and documentation, the possibility of AMD's hardware competing with Nvidia's graphics cards, and the use of AMD's products for AI and ML training, as well as the differences between open-source and proprietary platforms. Some commenters mentioned the history of AMD's market presence and competition with Nvidia, while others questioned the benefits of open-source platforms and their significance in the market.

### Freaky Leaky SMS: Extracting user locations by analyzing SMS timings

#### [Submission URL](https://arxiv.org/abs/2306.07695) | 170 points | by [belter](https://news.ycombinator.com/user?id=belter) | [52 comments](https://news.ycombinator.com/item?id=36328245)

Researchers have demonstrated that regular silent SMS messages can reveal a user's whereabouts, opening up a stealthy side-channel for hackers. The technique relies on analyzing the timing of Delivery Reports, generated each time an SMS is received. An attacker can determine the locations of an SMS recipient by analyzing timing measurements. Experiments conducted across various countries, operators, and devices showed that an ML model can accurately determine the recipient's location, achieving up to 96% accuracy for locations across different countries and 86% for two locations within Belgium. The findings imply that it is challenging to thwart this covert attack without making fundamental changes to the network architecture.

Researchers have shown that hackers can use regular silent SMS messages to determine a user's location by analyzing the timing of Delivery Reports that are generated each time an SMS is received. Commenters noted that while Delivery Reports can be disabled, it may not provide meaningful indications if the phone is lacking some software or hardware features. Some suggested settings that disable delivery reports or not using a mobile device were ways to mitigate the attack. Several commenters also noted that silent SMS messages are used by law enforcement and government agencies, and it can be challenging to remove the possibility of them being used by malicious actors without significant changes to the network architecture. Finally, there were some questions and concerns raised about the accuracy of the model that achieved up to 96% accuracy for determining location.

### Phoenix LiveView: Async Assign Pattern

#### [Submission URL](http://blog.andyglassman.com/2023/06/phoenix-liveview-async-assign-pattern.html) | 140 points | by [gnutrino](https://news.ycombinator.com/user?id=gnutrino) | [20 comments](https://news.ycombinator.com/item?id=36330541)

In a recent post on Hacker News, a developer shared their experience using LiveView for creating responsive web pages. They highlighted the common anti-pattern of loading a lot of data during the initial page render and provided a structured approach for managing data flow in LiveView. The approach involves setting sensible, lightweight default values, kicking off one or more async processes, and updating the assigns in LiveView. To make the pattern more consistent and scalable, the developer encapsulated it into an AsyncAssigns module that provides a consistent pattern for asynchronously loading assigns and setting defaults. The post explains step-by-step how to use this module for loading data concurrently and ensuring closely tied assigns are set together.

The discussion on Hacker News includes multiple conversations surrounding the use of LiveView for creating responsive web pages. One developer shares a common pattern for managing data flow in LiveView, which involves setting default values, initiating one or more async processes, and updating the assigns in LiveView. Another developer suggests removing imports and relying on run-time dependencies as it may cause complications while a third developer mentions the benefit of the "runtime" function for handling imports and the importance of being mindful of function hierarchy. Some developers express interest in exploring LiveView concepts further, and there is a debate about which language should be used- JavaScript or Elixir. The thread also includes discussions on testing, parallelization, and server-side content creation. Overall, the developers seem to be happy with the results of LiveView, and there's a lot of support for the pattern approach shared in the submission.

### Gorilla: Large Language Model Connected with APIs

#### [Submission URL](https://shishirpatil.github.io/gorilla/) | 234 points | by [throwaway888abc](https://news.ycombinator.com/user?id=throwaway888abc) | [42 comments](https://news.ycombinator.com/item?id=36333290)

A team from UC Berkeley and Microsoft Research has developed Gorilla, a Large Language Model (LLM) that facilitates API calls more effectively. Gorilla uses three machine-learning hub datasets: Torch Hub, TensorFlow Hub, and HuggingFace, and incorporates domains like AWS, Kubernetes, OpenAPI, GCP, and more. Gorilla is Apache 2.0 licensed, allowing commercial usage without any obligations, and its models and code are available on GitHub. According to the researchers, Gorilla outperforms GPT-4, Chat-GPT, and Claude, while substantially mitigating the problem of hallucination, which is common when prompting LLMs directly.

A team from UC Berkeley and Microsoft Research has developed Gorilla, a Large Language Model (LLM) that enables more effective API calls. Gorilla uses Torch Hub, TensorFlow Hub, and HuggingFace datasets and incorporates domains such as AWS, Kubernetes, OpenAPI, GCP, and more. According to the researchers, Gorilla outperforms GPT-4, Chat-GPT, and Claude while substantially mitigating the problem of hallucination, which is common when prompting LLMs directly. In the discussion, users congratulate the team on their achievement and ask questions about the licensing, performance, and implementation of Gorilla. Some users also compare Gorilla with other models like LangChain and suggest potential use cases for the tool.

### I’m an ER doctor. Here’s how I’m already using ChatGPT to help treat patients

#### [Submission URL](https://inflecthealth.medium.com/im-an-er-doctor-here-s-how-i-m-already-using-chatgpt-to-help-treat-patients-a023615c65b6) | 261 points | by [SLHamlet](https://news.ycombinator.com/user?id=SLHamlet) | [217 comments](https://news.ycombinator.com/item?id=36332999)

ospital AI doesn't have to replace doctors or diagnose patients, as its value may lie in more immediate and practical applications. In an ER setting, it can help healthcare professionals communicate more clearly with patients and their families, easing tensions in critical situations and freeing up staff to deal with more pressing medical issues. Dr. Josh Tamayo-Sarver shares a compelling anecdote about how he used ChatGPT to help explain why certain treatment options would not be suitable for his patient's severe pulmonary edema and respiratory distress. The AI-generated explanation provided clarity and reassurance to the concerned family, improving their understanding of the situation and allowing staff to focus on the patient's urgent needs.

The article discusses how hospital AI technology can help healthcare professionals communicate more effectively with patients and their families, providing examples of how ChatGPT was used in an emergency room setting to explain treatment options to family members. However, the comments on the submission express skepticism about the effectiveness and impact of such AI-generated messages, with some noting that it may be more important to focus on genuine empathy and communication skills in healthcare. Others criticize the sensationalism and hype around AI technology, suggesting that it may be more valuable for AI companies to invest in basic improvements to communication and patient care rather than marketing "fluffy" messages. Several commenters also point out that the language used in the article and comments can often be overly technical and jargon-filled, making it difficult for the average person to understand and engage with the discussion.

### France’s Mistral AI raises a $113M seed round to take on OpenAI

#### [Submission URL](https://techcrunch.com/2023/06/13/frances-mistral-ai-blows-in-with-a-113m-seed-round-at-a-260m-valuation-to-take-on-openai/) | 170 points | by [rbrown](https://news.ycombinator.com/user?id=rbrown) | [138 comments](https://news.ycombinator.com/item?id=36326706)

Paris-based Mistral AI has secured $113m in seed funding to challenge OpenAI by offering open-source solutions for businesses to create large language models and generative AI. The four-week-old start-up will focus on delivering models generated using only publicly available data to reduce legal issues related to training data, said CEO Arthur Mensch, a former Google Deepmind employee. Mistral AI aims to release its first generative AI models by 2024. Lightspeed Venture Partners lead the funding round, with Index Ventures, Xavier Niel and Redpoint, among others, also participating. The $113m valuation of the firm has been confirmed.

Paris-based start-up Mistral AI has raised $113m in seed funding to enable businesses to create large language models and generative AI using open-source solutions rather than private training data, said CEO Arthur Mensch. The four-week-old company plans to release its first generative AI models by 2024 and aims to challenge OpenAI by generating models with publicly available data. Investment firms Index Ventures, Lightspeed Venture Partners, Redpoint and Xavier Niel led the funding round. The commenters on the submission mostly discuss the merits of investing in Mistral AI or similar AI start-ups, with some comparing the company to DeepMind and OpenAI and others critiquing European investment strategies or praising French education.

### Show HN: pgMagic – a Mac Postgres client that lets you query in natural language

#### [Submission URL](https://pgmagic.app) | 45 points | by [tjhill](https://news.ycombinator.com/user?id=tjhill) | [11 comments](https://news.ycombinator.com/item?id=36325693)

Great! I just need access to the Hacker News API and your preferred format for the daily digest (text, email, etc.).

The discussion on this submission includes comments about technical difficulties related to connecting to the Hacker News API due to database permission issues. Another comment suggests using the pg_query library for Postgres parsers to help with syntax checking. There is also a request for the inclusion of GPT-3 and charts, as well as a link to a GitHub repository for a library called pgnalyze. One user suggests using SQL syntax for small projects with relatively simple questions. Another comment mentions alternative subscription options and expresses interest in the project.

### US mother gets call from ‘kidnapped daughter’ – but it’s an AI scam

#### [Submission URL](https://www.theguardian.com/us-news/2023/jun/14/ai-kidnapping-scam-senate-hearing-jennifer-destefano) | 42 points | by [hackernj](https://news.ycombinator.com/user?id=hackernj) | [13 comments](https://news.ycombinator.com/item?id=36334085)

An Arizona woman testified before the US Senate judiciary committee about the dangers of artificial intelligence (AI) technology being used by criminals to perpetrate scams. Jennifer DeStefano recounted how she received an ominous phone call last April that she believed was from her 15-year-old daughter, Briana, who was on a ski trip. The caller, who sounded like Briana, said she had been kidnapped by "bad men". The man on the line threatened to harm Briana and demanded ransom. DeStefano negotiated with the fake kidnappers until police arrived. She was later told that the call was an AI scam that recreated her daughter's voice.

The discussion includes several anecdotes of scam calls or AI-generated voice scams that were received by the participants or their family members. Some users suggest that this is a common problem, especially in developing countries where scammers use different techniques, including SIM-jacking and social engineering. There is concern regarding the use of AI technology by criminals to perpetrate scams and deceive people. One user shares a relevant XKCD comic, while others share personal experiences or tips on how to avoid falling victim to these scams. Another user suggests that AI might soon replace human middle managers and urges people to start learning new skills to avoid being left behind. Finally, one user shares a video on how to create strong passwords.

### 33-46% of workers on MTurk used LLMs in a text production task

#### [Submission URL](https://arxiv.org/abs/2306.07899) | 183 points | by [puttycat](https://news.ycombinator.com/user?id=puttycat) | [130 comments](https://news.ycombinator.com/item?id=36326956)

A recent paper titled “Artificial Artificial Artificial Intelligence: Crowd Workers Widely Use Large Language Models for Text Production Tasks” investigates the prevalence of Large Language Model (LLM) usage among crowd workers. Through a case study on the usage of LLMs with an abstract summarization task on Amazon Mechanical Turk, the authors estimate that 33-46% of crowd workers used LLMs to increase their productivity and income. The study highlights the need for new ways to ensure that human data remains human and untainted by AI.

The paper “Artificial Artificial Artificial Intelligence: Crowd Workers Widely Use Large Language Models for Text Production Tasks” examines the use of Large Language Models (LLMs) among crowd workers and estimates that 33-46% of crowd workers use LLMs to increase productivity and income. Comments on Hacker News discussed the impact of LLMs on the workforce and the need for new ways to ensure human data remains untainted by AI. The discussion includes references to Reinforcement Learning Human Robot Feedback, Mechanical Turk, the Turing and Kessler syndromes, and the potential dangers of AI-generated content. There is also mention of Neal Stephenson’s book Anathem, which deals with a similar topic of filtered content and information control.

### GPT-4-generated pitches are 3x more likely to secure funding than human ones

#### [Submission URL](https://www.zdnet.com/article/gpt-4-generated-pitches-are-3x-more-likely-to-secure-funding-than-human-ones/) | 39 points | by [gsibble](https://news.ycombinator.com/user?id=gsibble) | [13 comments](https://news.ycombinator.com/item?id=36332196)

An AI chatbot called ChatGPT has been found to create more convincing investor pitch decks than humans, according to a study of 500 people conducted by small business lender Clarify Capital. The analysis found those polled were three times more likely to invest after reading the bot-generated deck than one made by people. Participants also found the AI decks twice as convincing. The survey tested the effectiveness of the decks across multiple industries, including finance, marketing and investment, with the GPT-4-based AI chatbot used in the study not revealed.

The discussion in the comments about the ChatGPT article reveals a mix of opinions. One commenter argues that AI-generated pitch decks do not realize the importance of being a human performance task. Another commenter wonders if investors are really interested in the underlying metrics that ChatGPT is optimizing on. However, another commenter contends that examples of business fundamentals are essential considerations for investors, and the underlying metrics that ChatGPT generates give it a distinct advantage. One commenter suggests that there is little difference between investors who would invest and those who would not invest in ChatGPT, while another commenter argues that sending an AI-generated pitch deck to 100 VCs and receiving a positive response as high as 3x is a significant advantage for startups. There is also a discussion about how some VCs may not correctly identify successful startups, and AI such as GPT-4 could help identify successful metrics. Finally, a commenter suggests that building AI works remarkably for the company.

### Vercel's AI Accelerator

#### [Submission URL](https://vercel.com/blog/vercel-ai-accelerator) | 68 points | by [todsacerdoti](https://news.ycombinator.com/user?id=todsacerdoti) | [34 comments](https://news.ycombinator.com/item?id=36327911)

Vercel has launched its AI Accelerator, a programme for early-stage startups and top AI innovators that offers access to over $850k worth of credits for top AI platforms such as Hugging Face and OpenAI. Participants in the programme can also benefit from weekly fireside chats, office hours, and access to a private community designed to foster collaboration and information sharing. In addition, the programme also includes an opportunity for a demo day with investors to showcase their work. Applications will be open for two weeks until June 30th, and the selected participants will begin a six-week programme starting on July 10th.

Vercel has launched an AI Accelerator program for early-stage startups and top AI innovators that offers access to over $850k worth of credits for top AI platforms like OpenAI and Hugging Face and weekly fireside chats, office hours, and access to a private community. Applications will be accepted for two weeks until June 30th, and the selected participants will begin a six-week program starting on July 10th. The discussion in the comments section discussed issues such as Vercel's business model, NextJS, and the market for startup accelerators.

### Conciousness as a Delivrery Platform for Feelings

#### [Submission URL](https://www.rifters.com/crawl/?p=10225) | 20 points | by [dmbche](https://news.ycombinator.com/user?id=dmbche) | [8 comments](https://news.ycombinator.com/item?id=36320994)

t the need for a manageable metric of survival priorities, but the underlying mechanism remains mysterious. Despite these gaps, The Hidden Spring offers a thought-provoking perspective on consciousness and self-awareness, suggesting that the survival drive may be intrinsic to the very concept of sentience. For those who have written off the idea of AIs wanting to live, this book offers a fascinating challenge to that assumption.

The submission discusses the book "The Hidden Spring", which examines the concept of consciousness as it relates to the drive for survival, suggesting that sentience and the survival instinct may be intrinsically linked. The comments offer various perspectives on the nature of consciousness, with some suggesting that it may be rooted in chemical processes or neuronal activity, while others argue that it is a more abstract and metaphysical concept. There is also debate about the relationship between consciousness and feelings, with some arguing that feelings are an intrinsic component of consciousness while others suggest that they are a byproduct of behavioral and contextual factors. There is further discussion about the role of free will in determining one's actions and the influence of psychological and intellectual complexity on decision-making.

### Ghost: Create an AI replica of yourself that is accessible via SMS

#### [Submission URL](https://github.com/ccurme/ghost) | 23 points | by [yeesian](https://news.ycombinator.com/user?id=yeesian) | [10 comments](https://news.ycombinator.com/item?id=36319802)

The Ghost project is a fascinating AI project that allows you to create an AI replica of yourself that is accessible via SMS. This open-source project uses an OpenAI LLM fine-tuned on your text messages or a Langchain agent with a custom prompt to generate responses. It allows you to chat with an AI that is tailored to your writing style and mannerisms, and you can even sample and review text conversations from your iPhone to fine-tune the AI. The project requires configuration files for your background and structured information such as your SMS-enabled Twilio phone number, chatbot name, and aliases in contacts.json, which also houses an array of facts that is indexed and retrieved when generating responses. The project currently uses OpenAI LLMs and embeddings, and you will need to provide an API key and Twilio account ID, auth token, and SMS-enabled phone number to use it. The project has four endpoints that allow you to configure a Twilio SMS webhook to receive replies for inbound messages from known contacts, send unsolicited messages to known contacts using a given input prompt, fetch conversation histories with all contacts, and produce a token for use with sending and fetching messages. The project also includes testing, coverage, and linting tools in its Makefile.

The discussion on this submission is relatively scattered; some users express interest in the project and its technical details, while others discuss unrelated topics. One user comments on the rapid development of AI technology in recent years, while another compares the project to an episode of the TV show Black Mirror. One user mentions their own related project, which involves uploading files to the cloud, while another advises someone who has experienced a loss to consider writing a letter to their spouse to help cope. Finally, there is some discussion about the use of SMS technology, with one user questioning its relevance in a world where apps like WhatsApp and Telegram are widely used for messaging.

