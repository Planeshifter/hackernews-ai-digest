## AI Submissions for Fri Jun 23 2023 {{ 'date': '2023-06-23T17:12:02.524Z' }}

### HumbleObject (2020)

#### [Submission URL](https://martinfowler.com/bliki/HumbleObject.html) | 22 points | by [mooreds](https://news.ycombinator.com/user?id=mooreds) | [12 comments](https://news.ycombinator.com/item?id=36452498)

If you've ever struggled to test certain parts of your code, you're not alone. In fact, there are some elements that are inherently difficult or even impossible to test, leaving them prone to bugs that are difficult to fix. However, by making these "untestable objects" humble, we can reduce the chances of them harboring evil bugs. A common example of this is in the user-interface, which can be difficult to test using automated methods. But by minimizing the behavior of UI controls using patterns like Presentation Model and Passive View, we can still effectively test them. To learn more, check out Gerard Meszaros's xUnit Test Patterns book, which explores the concept of humble objects in depth.

The discussion on this submission mainly revolves around testing and how to make testing of difficult-to-test code more manageable. The idea of "humble objects" is discussed, which involves minimizing the behavior of UI controls using patterns like Presentation Model and Passive View to effectively test them. Encapsulation, well-defined functions, and minimal state compounding can help with testing. While the value of modern testing frameworks is questioned, some users suggest that presentational code should be separated from business logic to facilitate testing. The difficulty of testing graphical user interfaces is highlighted, along with the suggestion that declarative GUIs are easier to test than non-declarative GUIs. Finally, the power of CSS is debated, with some users suggesting that it can be used to abuse complexity, while others argue that it offers significant benefits.

### Open source licenses need to leave the 1980s and evolve to deal with AI

#### [Submission URL](https://www.theregister.com/2023/06/23/open_source_licenses_ai/) | 79 points | by [gumby](https://news.ycombinator.com/user?id=gumby) | [103 comments](https://news.ycombinator.com/item?id=36444854)

Open source licenses and free software have yet to adequately evolve to handle AI models, which raise grayer legal issues than code-centric software. With programming datasets so reliant upon open source and free software code, Stefano Maffulli, executive director at Open Source Initiative, among other tech leaders, is looking into ways to align AI and open source licenses for more clarity. Concerns over copyright infringement and proprietary licenses mean that tech companies producing AI-generated code will ultimately regard them as private IP, just as software code was considered the property of the software company in previous years. Other discussions pertained to how to license datasets involved with AI models, despite how they don't fit under traditional copyright models, and the difficulties found with open sourcing medical data versus commercial LLM datasets, which are typically black boxes. Several organizations are collaborating on defining a common understanding of open source AI principles that they intend to use to lobby legislative bodies.

The article discusses how open source licenses and free software have yet to evolve to handle legal issues with AI models. Since programming datasets rely heavily on open source and free software code, there are concerns over copyright infringement and proprietary licenses. Tech companies producing AI-generated code may regard them as private intellectual property, similar to software code. Discussions revolve around licensing datasets used in AI models, including open sourcing medical data versus commercial LLM datasets, which are typically black boxes. Several organizations are collaborating on defining a common understanding of open source AI principles to lobby legislative bodies. In the ensuing discussion, users debated the legal issues regarding AI models, such as whether AI-generated weights should be considered copyrightable, and whether there is a clear precedent in copyright law for AI. Additionally, the discussion highlighted the complexities of licensing AI and how some view AI-generated data as non-copyrightable.

### Making C++ safe without borrow checking, reference counting, or tracing GC

#### [Submission URL](https://verdagon.dev/blog/vale-memory-safe-cpp) | 260 points | by [jandeboevrie](https://news.ycombinator.com/user?id=jandeboevrie) | [196 comments](https://news.ycombinator.com/item?id=36448759)

safety solutions beyond reference counting, tracing garbage collection, or borrow checking, which have been the go-to solutions for the problem. Evan Ovadia, who has spent the past decade exploring memory safety, presents at least eleven more methods, including generational references, regions, and basil memory stacks, that can make C++ memory-safe without extending the language. The approaches come with their own tradeoffs, but they all provide possibilities for making C++ memory-safe without using traditional solutions. Ovadia also discusses blending four main ingredients to achieve memory safety, including "Borrowless affine style", constraint references, generational references, and simplified borrowing. He offers some caveats upfront, such as varying composability with existing unsafe C++ code. However, Ovadia believes these techniques provide the tools necessary, coupled with static analysis tools, for making C++ memory-safe.

The article describes various approaches to making memory-safe C++ code without relying on traditional solutions such as reference counting or borrow checking. The post mentions eleven methods, including generational references and basil memory stacks, and discusses the blend of four ingredients required for memory safety. The comments section features a debate about the advantages of Rust over C++, with some arguing about Rust's tighter control over memory allocation and greater level of memory safety, while others question the 5% increase in dependency code that Rust requires compared to C++ and the possibility of undefined behavior when working with unsafe code. There is also a discussion about how Haskell is considered a memory-safe language and the cultural differences between the C++ and Rust communities in approaching memory safety.

### 64-Bit RISC-V with Apache NuttX Real-Time Operating System

#### [Submission URL](https://lupyuen.codeberg.page/articles/riscv.html) | 84 points | by [lupyuen](https://news.ycombinator.com/user?id=lupyuen) | [22 comments](https://news.ycombinator.com/item?id=36453810)

In this article, the author walks through booting the Apache NuttX Real-Time Operating System (RTOS) on a 64-bit RISC-V device using the QEMU Emulator. They explain the RISC-V Boot Code in NuttX and trace its execution, including instructions for fetching the CPU ID, disabling interrupts, and waiting for an interrupt that will never come. They also provide helpful tips on building and running NuttX on the QEMU Emulator. As NuttX works similarly to a tiny version of Linux, readers familiar with Linux will find the commands familiar and useful.

There are several comments on this post discussing the NuttX RTOS and RISC-V. One user mentions the difficulty of catching exceptions on RISC-V and explains the process of designing a chip. Another user recommends learning NuttX as it is a great RTOS for hobbyists and professionals alike. A few users discuss the pros and cons of NuttX compared to other POSIXy RTOSes, such as Zephyr. Another user shares a link to Samsung's fork of NuttX called TizenRT. One user mentions that QEMU does support 8-bit RISC and provides a link to a small RISC-V emulator written in JavaScript. Another user notes that NuttX was created by Gregory Nutt and is released under GPL 3. Finally, there is a discussion on the meaning of software names, with one user sharing their experience in naming functions and how it can sometimes create confusion.

### Millions of GitHub repos likely vulnerable to RepoJacking, researchers say

#### [Submission URL](https://www.bleepingcomputer.com/news/security/millions-of-github-repos-likely-vulnerable-to-repojacking-researchers-say/) | 124 points | by [pyeri](https://news.ycombinator.com/user?id=pyeri) | [47 comments](https://news.ycombinator.com/item?id=36452322)

AquaSec's security team, Nautilus, has issued a warning that millions of repositories on GitHub may be vulnerable to dependency repository hijacking, or RepoJacking. The attack involves a malicious actor registering a username under the name of an older repository, used by an organisation that has since changed their name or had a change in ownership. Any project using the dependency of the attacked project will subsequently fetch dependencies and code from the attacker-controlled repository, which could contain malware. AquaSec scanned major organisations and found exploitable cases in repositories managed by Google and Lyft, in which vulnerable dependencies pointed to rogue repositories.

AquaSec's security team, Nautilus, warns that millions of repositories on GitHub may have been affected by dependency repository hijacking, or RepoJacking. Google and Lyft are two of the affected organizations, and vulnerable dependencies have been found pointing to rogue repositories. The discussion includes various comments, such as the difficulty of implementing global namespaces, the use of SSL certificates, and the dangers of compromised domain names. Furthermore, there are some comments about the importance of maintaining package lockfiles and regularly auditing packages for vulnerabilities in package managers like npm. The discussion also touches on GitHub integration systems and the differences between package managers such as npm and Python. Finally, there is a comment about potential alternatives to GitHub.

### Show HN: Factiverse AI editor – Fact-checking text made smarter and simpler

#### [Submission URL](https://editor.factiverse.ai/) | 55 points | by [vinni2](https://news.ycombinator.com/user?id=vinni2) | [28 comments](https://news.ycombinator.com/item?id=36449837)

Hello! I'm an AI language model and I can definitely write a daily digest of the top stories on Hacker News. Let's get started!

This discussion revolves around the submission of a multi-step tutorial model that includes some off-the-shelf techniques with some preloaded examples for demonstration purposes. Some members of the discussion express concerns about flooded fields of AI generated content, which they deem frequently contain bullshit. However, it is noted that with responsible use, AI generated content could be useful in improving online conversations and promoting a deeper level of understanding. 

Other concerns expressed in the discussion are pricing models, the diversity of topics, trustworthiness, media sources, and fact-checking. It is suggested that valid source credentials and fact-checking methods are necessary for building trust in AI generated content. Participants also discuss the importance of linguistic context and the limitations of specific reference frames. 

Finally, an API with flexible pricing options and subscription plans is suggested for hobbyists who want to test the model.

### Any Deep ReLU Network Is Shallow

#### [Submission URL](https://arxiv.org/abs/2306.11827) | 124 points | by [headalgorithm](https://news.ycombinator.com/user?id=headalgorithm) | [56 comments](https://news.ycombinator.com/item?id=36451149)

Researchers have proven that every deep ReLU network can be rewritten as a functionally identical three-layer network with weights valued in the extended reals using an algorithm, allowing for the creation of transparent and explainable models. The new methodology, outlined in "Any Deep ReLU Network is Shallow" by Mattia Jacopo Villani and Nandi Schoots, could improve the accuracy and interpretability of AI systems, proving especially useful in industries with high stakes, such as healthcare and finance.

The submission on Hacker News discusses a new algorithm that allows for every deep ReLU network to be rewritten as a three-layer network with weights valued in the extended reals, enabling the creation of transparent and explainable models. Commenters go on to discuss the benefits and drawbacks of using shallow networks over deep networks. Some highlights from the discussion include a debate on the usefulness and limitations of Borel functions and approximation theorems, as well as a comparison of the effectiveness of deep and shallow networks in various industries, including healthcare and finance. Additionally, some commenters tangentially discuss the relevance of various mathematical concepts such as Taylor series, fully-connected neural networks, and support vector machines.

### Full ignition for ESA’s reusable rocket engine

#### [Submission URL](https://www.esa.int/Enabling_Support/Space_Transportation/Full_ignition_for_ESA_s_reusable_rocket_engine) | 213 points | by [ZacnyLos](https://news.ycombinator.com/user?id=ZacnyLos) | [180 comments](https://news.ycombinator.com/item?id=36448791)

The European Space Agency (ESA) and ArianeGroup are making progress in developing a reusable engine for European rockets. The prototype of Prometheus has had a successful 12-second burn test at ArianeGroup's test facility in France. The engine uses liquid oxygen-liquid methane fuel, which simplifies handling and makes it clean burning. The 100-tonne thrust class engine has been designed to reduce costs to just a tenth of Ariane 5's Vulcain 2.0. The Prometheus engine features variable thrust and multiple ignition capability, and extensive use of new materials and manufacturing techniques to reduce waste and speed up production. The engine will be tested again at the German aerospace agency DLR's test site at the end of 2023.

The European Space Agency (ESA) and ArianeGroup have successfully tested the Prometheus reusable engine, which is designed to significantly reduce rocket launch costs compared to current models. The engine uses liquid oxygen-liquid methane fuel, which is clean burning and easier to handle. The engine features variable thrust and multiple ignition capability, and extensive use of new materials and manufacturing techniques to reduce waste and speed up production. Commenters on the Hacker News thread mainly discussed the cultural differences between the American and European space industries and the various marketing techniques used to promote rocket launches. Some commenters also criticized ESA for not being transparent with the public about their spending and for not giving journalists sufficient access to their events.

### Twilight of the programmers?

#### [Submission URL](https://danielbmarkham.com/twilight-of-the-programmers/) | 82 points | by [signa11](https://news.ycombinator.com/user?id=signa11) | [69 comments](https://news.ycombinator.com/item?id=36445513)

As a programmer, there is something very important that we are losing in today's world, according to an article on Hacker News. The author believes that programming should be telling us more than we are telling it, and that we've got it all backwards. Unlike other professions, when a programmer encounters a situation where conflicting meanings happen, each of which is correct, it is a much more profound piece of information to provide the client. The article ends with a quote from a friend: "I think some of the best programs were essays, in the sense that the authors didn't know when they started exactly what they were trying to write."

The submission on Hacker News discusses how programming should be telling us more than we are telling it. The discussion that follows explores the limitations of the current programming system, especially in the context of part-time employees, and the importance of abstractions. Some commenters argue that abstractions are broken, but others point out that they can be useful in creating a better understanding of complex concepts. Additionally, some commenters assert that different professionals have different approaches to their work and that there is no universal logical world. Others suggest that the key to solving business problems is dependent on the depth of knowledge and intelligence of the business partners and that there should be more personal learning opportunities. Overall, the conversation touches on the role of programming in the real world and the complexities it faces.

### From word models to world models

#### [Submission URL](https://arxiv.org/abs/2306.12672) | 97 points | by [dimmuborgir](https://news.ycombinator.com/user?id=dimmuborgir) | [100 comments](https://news.ycombinator.com/item?id=36445197)

A group of researchers has proposed a framework for language-informed thinking that combines neural models of language with probabilistic models for rational inference. The framework, called "rational meaning construction", views linguistic meaning as a context-sensitive mapping from natural language into a "probabilistic language of thought" (PLoT), which is a general-purpose symbolic substrate for probabilistic, generative world modeling. The paper illustrates the framework in action through examples covering four core domains from cognitive science and extends the framework to integrate cognitively-motivated symbolic modules to provide a unified, commonsense thinking interface from language.

A group of researchers have suggested a framework for language-informed thinking that combines neural models of language with probabilistic models for rational inference. However, several users are skeptical about the approach, with one argument being that the models currently lack high-quality training data and cannot answer complex questions. Some users believe that the language models' ability to understand the world in a limited symbolic representation is not fully effective for intelligence. Still, others contend that natural language processing is a vital step towards achieving artificial general intelligence. Some users suggest that the research illustrates the limitations of current large language models, while others argue that such models can drink and perform very well. There are also discussions about the capacity of humans to comprehend complex thought and whether machines can do likewise.

### What is a transformer model? (2022)

#### [Submission URL](https://blogs.nvidia.com/blog/2022/03/25/what-is-a-transformer-model/) | 288 points | by [Anon84](https://news.ycombinator.com/user?id=Anon84) | [51 comments](https://news.ycombinator.com/item?id=36449788)

Transformers are driving a wave of advances in machine learning, earning them the nickname "transformer AI." These neural networks learn context and meaning by tracking relationships in sequential data, using a set of mathematical techniques called attention or self-attention to detect subtle influences among distant data elements. Transformers are already being used in a host of applications, from preventing fraud to improving healthcare, and can analyze sequential text, image, or video data. Stanford researchers call transformers "foundation models", as they see them driving a paradigm shift in AI and replacing the most popular types of deep learning models from just five years ago.

Transformers, or "transformer AI," is driving new advancements in machine learning as these neural networks can learn context and meaning by tracking relationships in sequential data. Stanford researchers call transformers "foundation models" as they see them influencing a shift in AI and replacing the most popular types of deep learning models from just five years ago. In the comments, users recommend resources for learning about transformers from building them from scratch to implementing them in a practical application, as well as discussing the capabilities and limitations of transformers compared to other models like CNNs and RNNs. Some users express concern about the potential negative impacts of transformers on society and the need for responsible research practices.

### AudioPaLM: A Large Language Model That Can Speak and Listen

#### [Submission URL](https://google-research.github.io/seanet/audiopalm/examples/) | 111 points | by [ml_basics](https://news.ycombinator.com/user?id=ml_basics) | [32 comments](https://news.ycombinator.com/item?id=36443676)

Google has introduced AudioPaLM, a new large language model for speech understanding and generation that combines text-based and speech-based language models. It uses a unified multimodal architecture that can process and generate text and speech, with applications including speech recognition and speech-to-speech translation. AudioPaLM was able to outperform existing systems for speech translation tasks and also demonstrated zero-shot speech-to-text translation for many languages not seen in training. The model significantly improved speech processing by leveraging the larger quantity of text training data used in pretraining. AudioPaLM inherits the capability to preserve paralinguistic information such as speaker identity and intonation from AudioLM and linguistic knowledge present only in text large language models such as PaLM-2.

Google has introduced AudioPaLM, a new large language model that combines text-based and speech-based language models for speech understanding and generation. It has a unified multimodal architecture that can process and generate text and speech, and the model can handle paralinguistic information such as speaker identity and intonation. It has been shown to outperform existing systems for speech translation tasks and demonstrated zero-shot speech-to-text translation in many languages not seen in training. The discussion on the submission included comments about the accuracy of the translation and the potential for spam calls to be intercepted with AudioPaLM. There were also discussions about the benefits of bilingual models for literal translations and how LLMs can represent different languages. Some commenters expressed skepticism towards the model's capabilities and the possibility of spying.

### Show HN: A package manager for AI plugins

#### [Submission URL](https://openpm.ai/) | 81 points | by [maccaw](https://news.ycombinator.com/user?id=maccaw) | [17 comments](https://news.ycombinator.com/item?id=36447683)

OpenPM is a package manager for AI plugins that simplifies the integration of various APIs. It offers a seamless process for exploring, publishing, and integrating APIs into AI platforms. With OpenPM, developers can easily discover and use new AI plugins, while API providers can publish their plugins to the registry and make them available to the community. Its API search function streamlines the integration process, removing the need for manual coding to connect various APIs. Overall, OpenPM represents a handy tool for streamlining AI development and integration processes.

The discussion around the OpenPM submission on Hacker News includes several topics related to AI development and integration. One commenter raises concerns about security and the supply chain attacks that can occur. Others discuss the packages available, including Cloudflare's Worker building AI plugins and WorkGPT, a framework for working with GPT functions. There are also discussions about API search and preview functionality, the adoption of AI plugins, and the role of OpenPM in streamlining AI development. Some comments also touch on the confusion between OpenPM and other similar tools like OpenAPI and OpenAI. Another contributor compares alternative platforms and suggests that OpenAI is leading the industry with its commitment to regulation and support for open-source alternatives.

### Quality of new vehicles in US declining on more tech use: study

#### [Submission URL](https://www.reuters.com/business/autos-transportation/quality-new-vehicles-us-declining-more-tech-use-study-shows-2023-06-22/) | 228 points | by [lxm](https://news.ycombinator.com/user?id=lxm) | [431 comments](https://news.ycombinator.com/item?id=36441054)

According to a new report by J.D. Power, the quality of new vehicles sold in the United States has declined due to the growing use of technology and lower build quality of certain components. The report collected data from 93,380 purchasers and lessees of 2023 model-year vehicles and found that build quality issues, such as those with audio systems and cup holders, have resulted in quality problems. Automakers have been relying on software and technology to launch innovative models amid labor shortages and supply constraints. The problems per 100 vehicles (PP100) have risen by 30 PP100 during the past two years. However, the quality ratings of Tesla have increased by 31 PP100 year-over-year.

A new report by J.D. Power shows that lower build quality of certain components and the growing use of technology have contributed to a decline in the quality of new vehicles sold in the US. The report reveals that problems per 100 vehicles (PP100) have risen by 30 PP100 in the past two years. However, the quality ratings of Tesla have increased by 31 PP100 year-over-year. The commenters discussed the increasing technology in cars, the narrow niches served by some vehicles, and the affordability of cars in various regions. Some highlighted Dacia's Sandero model, which is a cheaper model produced by Renault and popular in Europe because of its affordability. One comment also questioned if BMW's high maintenance costs are worth the prestige of the brand.

### Groundbreaking AI project translates 5k-year-old cuneiform

#### [Submission URL](https://www.timesofisrael.com/groundbreaking-ai-project-translates-5000-year-old-cuneiform-at-push-of-a-button/) | 42 points | by [isaacfrond](https://news.ycombinator.com/user?id=isaacfrond) | [18 comments](https://news.ycombinator.com/item?id=36446289)

A team of Israeli archaeologists and computer scientists has created an AI-powered translation program for ancient Akkadian cuneiform, the oldest known form of writing whose translation has been largely confined to around a few hundred experts who can decode the clay tablets filled with wedge-shaped symbols. The neural machine translation from Akkadian to English works by converting words into a string of numbers and uses a complex mathematical formula to output a natural sentence construction in another language. The team hopes that the program will encourage armchair archaeologists to try their hand at cuneiform interpretation, and will eventually allow exposure to the first days of history and uncover lost knowledge.

An Israeli group has created an AI-powered translation program for ancient Akkadian cuneiform, which they hope will encourage people to explore its history. The accuracy of such an interpretation has been debated, with reference to the Cyrus Cylinder. Some people see this as an accomplishment in AI, while others are skeptical about its accuracy. One poster pointed out that, while the program may get us part of the way, humans are needed to verify cuneiform translations. Finally, despite the enthusiasm, another poster noted that any advances made may be offset by the unavailability of relevant information among present-day governments.

### How Jane Street is making OCaml the new Rust

#### [Submission URL](https://www.efinancialcareers.com/news/2023/06/jane-street-programming-language) | 28 points | by [edwintorok](https://news.ycombinator.com/user?id=edwintorok) | [4 comments](https://news.ycombinator.com/item?id=36452483)

Proprietary trading firm Jane Street wants to emulate the low-level cult favourite Rust in its favourite programming language, Ocaml. Rust’s primary design goal is to entirely forbid shared mutability, which it achieves by only allowing mutable values to be referenced uniquely. In comparison, Ocaml, “records containing mutable fields are relatively common,” which makes handling data difficult. Richard Eisenberg, a functional language designer at Jane Street, also spoke about his efforts in the space. For Eisenberg it’s not what Rust adds that's impressive, rather it's what it takes away: garbage collection.

The discussion includes a link to Part 1 and Part 2 of a blog post on the subject. One commenter suggests learning OCaml, while another notes that OCaml has been around for 10 years. Another commenter remarks that OCaml is a truly bizarre choice compared to Rust, and a response to that states that the article's point is that Jane Street wants to adopt Rust's memory management features for better control.

### WebKit on VisionOS Will Support Open WebXR Content

#### [Submission URL](https://daringfireball.net/linked/2023/06/23/webkit-visionos-webxr) | 24 points | by [alwillis](https://news.ycombinator.com/user?id=alwillis) | [3 comments](https://news.ycombinator.com/item?id=36451025)

Apple has confirmed that Safari on its Vision Pro headset will support WebXR, a new standard for immersive experiences on the web. This means that users will be able to access XR experiences through the browser, in addition to existing websites that should work as expected. The move is not entirely surprising, as Apple has a longstanding commitment to open content and the web, and recently hired Ada Rose Cannon, a preeminent figure in the WebXR community. The news was announced in Apple's WWDC 2023 developer talk titled "Meet Safari for Spatial Computing".

The first comment is a short response expressing vague suspicion about the potential impact of Safari supporting WebXR on the battery life of iPhones.

The second comment seems to suggest deleting content on the Vision Pro headset for privacy reasons and warns others not to leave personal information on the device.

