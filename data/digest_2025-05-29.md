## AI Submissions for Thu May 29 2025 {{ 'date': '2025-05-29T17:12:48.133Z' }}

### Human coders are still better than LLMs

#### [Submission URL](https://antirez.com/news/153) | 561 points | by [longwave](https://news.ycombinator.com/user?id=longwave) | [665 comments](https://news.ycombinator.com/item?id=44127739)

In the ever-evolving world of AI and software development, human coders remain a step ahead of Large Language Models (LLMs) like Gemini 2.5 PRO, at least for now. A recent real-world case illustrates this dynamic beautifully. The problem at hand involved a complex bug fix within Vector Sets for Redis, tied to how corrupted data could disrupt node links in Redis' graph serialization approach.

After discovering that the vanilla solution slowed down loading times significantly, the coder turned to Gemini for advice on optimizing speed. However, responses from the AI were less insightful than hoped, suggesting only basic adjustments like ordering pointers for binary search.

The coder's creative thinking, something AI hasn't quite mastered, played a crucial role. They proposed an innovative solution involving a hash-based method to check for non-reciprocal links, which the AI could appreciate but not improve upon. Further refinement led to using an XOR-based method with a fixed accumulator to detect anomalies—a technique cautious of potential data collisions.

Seeing potential risks, the coder incorporated a hash function with a random seed to reduce collision chances further, achieving a level of robustness that could thwart even deliberate attacks.

This story reaffirms that while LLMs serve as valuable tools, providing suggestions and alternate perspectives, the intricate problem-solving and intuition of human developers remain unmatched. AI complements but does not yet replace our analytical prowess, particularly in complex or security-sensitive tasks.

**Summary of Discussion:**

The discussion explores the strengths and limitations of LLMs (like ChatGPT) in software development and problem-solving, with mixed perspectives:  

### **Key Criticisms of LLMs:**
1. **Unreliability for "Blank Page" Problems**:  
   - LLMs struggle with undefined or open-ended tasks (e.g., starting from scratch, complex design decisions). Users noted they often produce plausible-sounding but incorrect answers, requiring significant human verification.  
   - Example: Translating assembly code or solving novel issues often still demands human expertise.  

2. **Hallucinations and Inaccuracies**:  
   - LLMs sometimes invent nonexistent libraries (e.g., npm packages) or misinterpret technical terms, forcing developers to double-check outputs.  
   - One user described frustration with ChatGPT inventing a "PiicoDev_SlidePot" class that didn’t exist.  

3. **Search Engine Limitations**:  
   - LLMs are seen as inferior to traditional search engines (even older ones like 2005-era Google) for factual queries. Conversational interfaces lack the skepticism users apply to search results, and SEO spam/SEO-optimized content degrades reliability.  

4. **Prompt Engineering Challenges**:  
   - While detailed prompts improve results, LLMs may still make arbitrary design choices. Users emphasized that even "perfect" prompts don’t guarantee accuracy, requiring iterative refinement.  

---

### **LLM Strengths and Use Cases**:  
1. **Productivity Boost for Tedious Tasks**:  
   - Automating boilerplate code (e.g., mapping functions between data types), writing small scripts, or generating documentation saves time.  
   - Example: An LLM reliably converting `protoFooID` to `gomodelFooID` reduced manual work by ~50%.  

2. **ADHD and Workflow Support**:  
   - Developers with ADHD found LLMs helpful for overcoming "blank page paralysis" or hyperfocusing on minor details (e.g., variable naming).  

3. **Learning and Prototyping**:  
   - LLMs accelerate exploration of new APIs, libraries, or concepts, acting as a conversational guide.  

---

### **Broader Sentiment**:  
- **LLMs as Tools, Not Replacements**: Most agreed LLMs are valuable assistants but lack human intuition, creativity, and critical thinking for complex, security-sensitive, or undefined tasks.  
- **Hybrid Workflows**: Developers often combine LLMs with traditional methods (e.g., writing code with AI, then testing/debugging manually).  
- **Future Concerns**: Some worry about over-reliance on LLMs for junior developers, potentially hindering growth in problem-solving skills.  

**Conclusion**: While LLMs enhance productivity and reduce grunt work, their limitations necessitate human oversight, especially for high-stakes or novel challenges. The consensus is pragmatic—embrace LLMs for efficiency but remain vigilant about their shortcomings.

### Web Bench: a new way to compare AI browser agents

#### [Submission URL](https://blog.skyvern.com/web-bench-a-new-way-to-compare-ai-browser-agents/) | 31 points | by [suchintan](https://news.ycombinator.com/user?id=suchintan) | [9 comments](https://news.ycombinator.com/item?id=44126725)

In the rapidly evolving world of web browsing agents, a new benchmark has emerged to push the boundaries of what these digital assistants can achieve. Introducing Web Bench, a pioneering dataset designed to rigorously evaluate AI web agents across 5,750 tasks on 452 diverse websites. This innovation significantly expands on the existing WebVoyager benchmark, which fell short with just 643 tasks on 15 sites, focusing heavily on reading tasks like data extraction.

Web Bench raises the bar by incorporating a crucial distinction between READ and WRITE tasks—where the latter includes more complex actions such as logging into accounts, filling out forms, and downloading files. The review of Web Bench's results reveals that current agents struggle most with these write-heavy challenges, highlighting a major opportunity for growth in the field. Among the contenders, Skyvern 2.0 excels in handling these tasks, while Anthropic's CUA leads in read-heavy scenarios.

The development of Web Bench is a collaboration with Halluminate, sorting through the top 1,000 traffic-heavy websites and refining the list to eliminate paywalled or redundant domains. The dataset creation involved rigorous testing using consistent browser infrastructures to control variables, allowing for a fair comparison of agent performance.

Despite the advancements, agents displayed notable shortcomings in write-heavy tasks due to navigation and information extraction issues, often faltering on seemingly simple website interactions like solving captchas or finding clickable buttons. These findings underscore the nuanced challenges of creating truly adept web browsing agents and suggest parallels to challenges faced by AI in other domains, such as coding.

As the landscape of AI browsing agents continues to mature, Web Bench stands as a critical tool to challenge the current limits and inspire innovations that might finally conquer these digital terrains. The dataset and its insights are open-source, inviting further exploration and refinement by the global developer community.

**Summary of Discussion:**  
The Hacker News discussion highlights enthusiasm for the **Web Bench** benchmark's expansion over **WebVoyager**, particularly praising its broader scope (452 websites vs. 15) and real-world relevance. Users note that WebVoyager’s limited scale made it less practical, and expanding further (e.g., to 10,000 sites) could enhance benchmarking accuracy.  

**Key points raised:**  
1. **Technical Workflow Debate:** A user questions whether generic browser workflows (like Skyvern’s) are more effective than tools like Playwright for building web agents.  
2. **Benchmark Gaps:** Commenters emphasize that existing benchmarks lack complex end-to-end tests (e.g., logged-in dashboards, forms, 2FA flows), making Web Bench’s focus on these areas critical for real-world AI agent performance.  
3. **Community Appreciation:** Contributors thank the team for open-sourcing the dataset, with one noting its potential to advance AI QA testing.  
4. **Agent Performance:** Skyvern’s success in write-heavy tasks is acknowledged, while anticipation builds for future benchmarks involving Claude 4 or Anthropic’s CUA.  
5. **Humor & Critique:** A jest about “Nelly” scoring 0 on the benchmark sparks a redirect to the official repository, underscoring the community’s engagement.  

Overall, the discussion reflects optimism about Web Bench’s role in driving innovation, while stressing the need for even more comprehensive and nuanced testing frameworks.

### Untrusted chatbot AI between you & the internet is a disaster waiting to happen

#### [Submission URL](https://macwright.com/2025/05/29/putting-an-untrusted-chat-layer-is-a-disaster) | 105 points | by [panic](https://news.ycombinator.com/user?id=panic) | [49 comments](https://news.ycombinator.com/item?id=44129529)

Imagine a future where every digital interaction is mediated by a chatbot, every purchase suggestion, and every piece of news tailored by an unseen hand. This scenario is more than a mere thought experiment; it's rapidly approaching reality, warns Tom MacWright, as companies like the Browser Company pivot towards chatbot-centric platforms like Dia. 

MacWright likens this trend to hiring a butler for all your digital needs—a move that seems convenient but eventually makes you vulnerable to manipulation both economically and ideologically. He points to current practices by tech giants like Google, Amazon, and Microsoft who unabashedly promote their own products in search results and recommendations, thanks to negligible regulatory consequences in the US.

However, the economic skew is just one aspect of this potentially dystopian picture. The subtler threat is ideological manipulation. Historical instances, detailed in exposés like "Careless People," reveal how platforms have already tweaked algorithms to favor certain voices. AI might exacerbate this issue, operating with greater efficiency and subtlety, and ultimately, working not for the user, but for those who program it.

MacWright’s message is clear: unless care is taken, this “butler” will not only start deciding what’s on the menu—but may eventually decide what you can and can't consume.

The Hacker News discussion around Tom MacWright’s warnings about AI-driven digital intermediation highlights several key concerns and debates:

### **1. AI Recommendations and Economic Bias**  
- Users note that people are increasingly accepting AI-generated answers (e.g., ChatGPT) for decisions like retail purchases, raising fears of economic manipulation. For example, a user shared an anecdote about retail workers using ChatGPT to manage sales and coupon codes.  
- Comparisons are drawn between AI-generated content and SEO spam, with some arguing both prioritize profit over quality: *“What’s the difference between LLM garbage and SEO garbage?”*  

---

### **2. Ideological Manipulation and Trust**  
- Skepticism about AI’s neutrality persists, with users pointing to historical examples (e.g., Google favoring its own products) and warning that AI could amplify propaganda or deceptive information.  
- Trust in corporations like Google is eroding: *“I don’t trust LLMs… they’re attached to companies that sell data to the highest bidder.”* Others argue AI could replicate the “invisible hand” myth, masking corporate agendas.  

---

### **3. Technical and Market Challenges**  
- Technical debates focus on the feasibility of decentralized, privacy-focused LLMs (e.g., encrypted prompts), but users question whether vendors would allow easy switching due to integration costs and proprietary lock-in.  
- Some predict AI intermediaries like ChatGPT could replace Google Search, but critics argue AI-generated content is just repackaged SEO tactics, creating a *“hellscape”* of low-quality, recycled posts.  

---

### **4. Social Media and AI-Generated Content**  
- Experiments to detect AI-generated comments on platforms like HN and Reddit reveal challenges in distinguishing human vs. synthetic content. Users note AI bots could manipulate discussions subtly, mimicking real engagement.  
- Concerns about AI-driven social media echo chambers and propaganda networks are likened to cable TV’s curated narratives: *“The dystopian internet is here.”*  

---

### **5. Regulatory and Ethical Gaps**  
- Many call for regulation to force transparency in AI recommendations (e.g., disclosing paid promotions), but others doubt enforcement will materialize.  
- Decentralized solutions (e.g., peer-to-peer search engines) are proposed, though skeptics argue they’re impractical without mainstream adoption.  

---

### **Conclusion**  
The discussion reflects widespread anxiety about AI centralizing power, eroding trust, and degrading information quality. While some see potential in technical fixes or regulation, others fear a future where AI intermediaries control access to knowledge, replicating—or worsening—existing flaws like SEO spam and corporate bias. As one user starkly put it: *“The AI dystopia is already happening.”*

### Domain Adaptation of Base Models + ShadowdarkQA Bench

#### [Submission URL](https://gygaxtest.com/posts/continued_pretraining_for-rules/) | 17 points | by [pact_inference](https://news.ycombinator.com/user?id=pact_inference) | [6 comments](https://news.ycombinator.com/item?id=44126214)

in an ominous voice) artwork, and loads of essential content for comprehending the game's mechanics. To feed this data into our model, I took advantage of modern OCR (Optical Character Recognition) tools, which have made remarkable strides in recent years. Despite initial hesitance, the extraction of text from the rulebook was surprisingly efficient and painless, allowing me to collect a solid foundation of information for our model to digest.

Our endeavor here is akin to equipping our little LLM with a knowledge base somewhat analogous to the actual Game Masters who wield rulebooks as their guiding tomes. By providing these data inputs, we’ll allow the model to form a baseline understanding of Shadowdark RPG's rulesets before we dive deeper into fine-tuning processes aimed at assisting GMs or players. This progressive learning journey is crucial for our vision to unfold: evolving an LLM to eventually take on a more dynamic role, perhaps metamorphosing into the game master itself.

The overarching ambition here is not just to hasten the development of an autonomous LLM Game Master, but to savor each step, filling it with robust learning and understanding. This involves embracing every part of the development stack and dealing sincerely with the limitations at hand, such as constrained computational resources, which steers our course toward maintaining efficiency and practicality with smaller models and synthetic datasets. With mindful progression in data strategy and model refinement, our mission is to sculpt a truly capable in-game assistant that can transition into the richly layered role of an LLM-powered GM.

**Hacker News Discussion Summary: Training an LLM as a Shadowdark RPG Game Master**

The discussion revolves around technical challenges and strategies for training a language model (LLM) to act as a Game Master (GM) for the Shadowdark RPG, based on rulebook data extracted via OCR. Key points include:

1. **RAG vs. Instruction-Tuning**:  
   - A user questions whether the approach uses **Retrieval-Augmented Generation (RAG)**. The response highlights a focus on **instruction-tuning** instead, with tools to search rulebook content (via markdown) for Q&A and programmatic verification of answers. Domain-specific pretraining (e.g., using models like **Qwen-3-06B**) is proposed to improve rule comprehension.

2. **Handling Game Mechanics**:  
   - Ambiguity in terms like *"4d6"* (dice notation) and game-specific concepts is noted. Solutions include using **formal grammars (EBNF)** to structure rule descriptions and experimenting with synthetic datasets to translate existing fantasy narratives into structured blocks (e.g., character sheets).

3. **Model Training & Learning Rates**:  
   - A debate arises over **learning rate choices** (e.g., 5e-5 for pretraining vs. 5e-6 for fine-tuning) to avoid catastrophic forgetting. A typo correction clarifies that higher rates (5e-4) might be better for pretraining, while lower rates preserve knowledge during fine-tuning.

4. **Structured Storytelling**:  
   - Ideas for integrating narrative generation include combining LLMs with **formalized character schemas** and campaign structures. A referenced [paper](https://arxiv.org/abs/2503.22828) explores modifying story-generation methods to align with RPG campaign objects and reward experimentation.

5. **Tooling & Validation**:  
   - Emphasis is placed on programmatically verifying outputs (e.g., dice rolls, character sheet validity) and using markup-to-text tools to parse rulebooks efficiently.

**Takeaway**: The project balances practical constraints (compute limits) with ambitions to evolve the LLM from an assistant to a full GM, requiring nuanced handling of game rules, structured data, and careful model training strategies.

