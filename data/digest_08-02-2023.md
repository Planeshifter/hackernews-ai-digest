## AI Submissions for Wed Aug 02 2023 {{ 'date': '2023-08-02T17:11:16.296Z' }}

### Tidal Cycles â€“ Live coding music with Algorithmic patterns

#### [Submission URL](https://tidalcycles.org/) | 95 points | by [pabs3](https://news.ycombinator.com/user?id=pabs3) | [12 comments](https://news.ycombinator.com/item?id=36967413)

If you're into music and coding, Tidal Cycles is worth checking out. Tidal Cycles, also known as Tidal, is a free/open-source live coding environment for creating algorithmic patterns. Developed in Haskell, this powerful tool allows users to generate flexible and dynamic sequences of sounds, notes, parameters, and much more.

Tidal Cycles takes advantage of another open-source software called SuperCollider for synthesis and I/O. This combination opens up a world of possibilities for musicians and composers who want to experiment with algorithmic music.

One of the notable features of Tidal Cycles is its pattern-based approach to music creation. With Tidal, you can write code to create patterns, enabling you to explore polyphonic, polyrhythmic, and generative sequences of sounds. It's a flexible and expressive way to compose, improvise, and delve into the depths of algorithmic music.

But Tidal is not just a tool; it's also a thriving community of musicians who utilize the software for their compositions, improvisations, and explorations. The Tidal Blog offers insights from fellow community members, and you can even submit your own blog post to share your experiences and knowledge. If you're looking to connect and learn from other Tidal enthusiasts, this community is the place to be.

Whether you're a seasoned musician or a curious coder, Tidal Cycles offers an exciting platform to express your creativity through algorithmic music. Give it a try, and who knows, you might just discover a whole new world of sonic possibilities.

There are a few comments in the discussion about Tidal Cycles. One user suggests trying an alternative called Strudel, another mentions that they have been making music with Tidal for 10 years and shares some links to their work. Another user asks for a comparison between Tidal and other similar packages like Sonic Pi, Ruby FoxDot, and Python TidalHaskell in terms of workflow and style. A user named "jrmtg" responds, saying they are more interested in writing SuperCollider code and find visual programming languages less interesting. They mention that TidalCycles can depend on SuperCollider for MIDI and sample playback. Another user mentions that Tidal Cycles connects with Ableton MIDI, making composition a fun experience with declarative sequencing. Overall, the discussion includes some alternative suggestions, personal experiences, and comparisons with other music packages.

### Open-sourcing AudioCraft: Generative AI for audio

#### [Submission URL](https://ai.meta.com/blog/audiocraft-musicgen-audiogen-encodec-generative-ai-audio/) | 868 points | by [iyaja](https://news.ycombinator.com/user?id=iyaja) | [301 comments](https://news.ycombinator.com/item?id=36972347)

Meta, the parent company of Facebook, has open-sourced AudioCraft, a framework that generates high-quality audio and music from text-based user inputs. This technology allows professional musicians to explore new compositions without needing to play any instruments, indie game developers to add realistic sound effects on a budget, and small business owners to easily add soundtracks to their social media posts. AudioCraft consists of three models: MusicGen, AudioGen, and EnCodec. The pre-trained models and code are now available for research purposes, enabling researchers and practitioners to train their own models and advance the state of the art in generative audio.

The discussion on Hacker News revolves around the licensing issues related to the open-sourced AudioCraft framework. Users point out that the CC-BY-NC license used for the MusicGen models restricts commercial use, which could limit its practicality. Some argue that the definition of "noncommercial" in copyright law is subjective and varies, while others provide examples and legal references to support their interpretations. The conversation also touches on the potential challenges and benefits of generating commercial music using AudioCraft, as well as the nuances of noncommercial licensing.

### ChromeOS is splitting the browser from the OS, getting more Linux-y

#### [Submission URL](https://arstechnica.com/gadgets/2023/08/google-is-finally-separating-chrome-from-chromeos-for-easier-updates/) | 106 points | by [mfiguiere](https://news.ycombinator.com/user?id=mfiguiere) | [75 comments](https://news.ycombinator.com/item?id=36977107)

Google is preparing to split up ChromeOS and its Chrome browser in an upcoming release. Codenamed "Lacros," this project will separate ChromeOS's Linux OS from the Chrome browser, allowing for independent updates. ChromeOS will move from the homemade Freon graphics stack to Wayland, the normal desktop Linux graphics stack. On the browser side, ChromeOS will switch to the Chrome browser for Linux. The split is expected to make it easier to update ChromeOS and could extend the lifespan of older devices. Google has not officially confirmed the project, but the code suggests it is heading in that direction.

The discussion on this submission covers a range of topics and opinions. Here are some key points:

- One user suggests that Microsoft may release their own Chromebook-like devices running EdgeOS and Edge browser.
- Another user argues that Microsoft is targeting the education market with locked-down operating systems and software like Teams, but it may be difficult for them to compete with Chromebooks in that space.
- The topic of data privacy and advertising is brought up, with one user mentioning concerns about Google harvesting advertising data from students' Chromebooks.
- There is a discussion about the benefits of using Chromebooks in schools, such as centralized management and affordability, as well as the possibility of using Linux laptops running Firefox and LibreOffice.
- Some users question the necessity of laptops for kids in schools, suggesting that desktop computers or tablets may be more suitable.
- The reliability and cost-effectiveness of Chrome OS compared to Windows and macOS is debated.
- A disagreement arises regarding the importance of traditional subjects like clear speech, critical thinking, mathematics, geography, and history in the curriculum, with one user arguing that Chromebooks can't replace the value of these subjects.
- The availability and popularity of Chromebooks in Scandinavia are questioned, with some users suggesting that they are not widely used in schools there.
- One user finds it interesting that Microsoft is selling a Linux-based consumer device.
- The completion of Google's Project LaCros, which separates ChromeOS and Chrome browser, is discussed.
- There is a conversation about running different Linux distributions on Chromebooks and the limitations of virtual machines.
Overall, the discussion covers a wide range of perspectives on Chromebooks, their use in education, and the future of ChromeOS and Chrome browser.

### Cookbook: Finetuning Llama 2 in your own cloud environment, privately

#### [Submission URL](https://blog.skypilot.co/finetuning-llama2-operational-guide/) | 116 points | by [covi](https://news.ycombinator.com/user?id=covi) | [12 comments](https://news.ycombinator.com/item?id=36975245)

Yesterday, Meta released Llama 2, a pre-trained language model that can be fine-tuned on user data and used commercially. In this article, the authors provide a step-by-step recipe for finetuning Llama 2 on your own data using open-source tools. They emphasize the advantages of this approach, including full control over compute, data, and models, support for multiple cloud providers, high GPU availability, and reduced costs through the use of spot instances. The recipe includes instructions for obtaining access to the Llama-2 model, installing SkyPilot (the tool used for training), and configuring the training data and model identity. It also provides a command to start training on any cloud, with options for selecting cloud provider, GPU availability, and cost optimization. Overall, this guide offers a comprehensive and open-source approach to fine-tuning Llama 2 and using it in commercial settings.

The discussion about the submission mainly revolves around the cost and efficiency of using Llama 2 for fine-tuning and production inference. One user points out that the cost depends on the GPU type and the serving system's traffic patterns, recommending the use of higher-cost optimized GPUs. They also highlight the benefits of cost optimization and mention the difference in cost between Llama-2 and GPT models.

Another user raises a question about the running cost of Llama 2 on a 70B GPU, assuming maximum utilization. There is also a mention of the latest release of Vicuna-15.

The topic of fine-tuning is also discussed, with one user suggesting replacing the retrieval step with a knowledge organization step. However, another user points out the challenges of fine-tuning based on organizational data, as the underlying data can change significantly, leading to high maintenance costs.

The possibility of customizing the knowledge identity and the challenges of fine-tuning due to the chit-chat problem are discussed. A user suggests that fine-tuning cannot address the chit-chat problem effectively, and contextual solutions that provide relevant answers should be considered.

The advantage of combining methods for better performance is also mentioned, such as the combination of fine-tuning and retrieval steps.

A related thread about running Llama 2 locally and the potential use of Llama 2 for specific purposes like Apple Silicon is also mentioned.

Overall, the discussion revolves around cost optimization, challenges in fine-tuning, and the customization and limitations of Llama 2 for various use cases.

### Nvidia AI Image Generator Fits on a Floppy Disk and Takes 4 Minutes to Train

#### [Submission URL](https://decrypt.co/150861/nvidia-ai-image-generator-floppy-disk-4-minutes) | 20 points | by [magoghm](https://news.ycombinator.com/user?id=magoghm) | [3 comments](https://news.ycombinator.com/item?id=36974890)

Nvidia researchers have introduced a new text-to-image personalization method called Perfusion, which allows for significant creative flexibility in AI-generated art while maintaining the identity of specific concepts. Perfusion outperforms other AI art generators in terms of efficiency and offers the feature of combining multiple personalized concepts in a single image with natural interactions. The key idea behind Perfusion is "Key-Locking," which connects new concepts to more general categories during image generation, preventing overfitting and enabling the portrayal of personalized concepts while retaining their core identity. Perfusion's small size of just 100KB makes it more efficient and customizable compared to bulkier AI image generators.

The discussion on Hacker News focused on the technical aspects and potential implications of Nvidia's Perfusion text-to-image personalization method. One user expressed skepticism, stating that the key-locking approach of connecting new concepts to general categories seemed like a dishonest form of clickbait. They argued that Perfusion should not be called an art generator but recognized that it outperforms other AI image generators in terms of efficiency. Another commenter compared Perfusion to other existing models in the AI art generation landscape, such as Stable Diffusion and MidJourney, but did not fully understand the personalization method used in Perfusion. They acknowledged the small size of Perfusion and its potential for better performance and customization compared to larger models. Another user appreciated the idea of embedding the model in just 100KB.

