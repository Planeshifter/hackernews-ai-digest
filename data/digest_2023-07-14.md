## AI Submissions for Fri Jul 14 2023 {{ 'date': '2023-07-14T17:09:55.792Z' }}

### Tinygrad and rusticl and aco: why not?

#### [Submission URL](https://airlied.blogspot.com/2023/07/tinygrad-rusticl-aco-why-not.html) | 34 points | by [pantalaimon](https://news.ycombinator.com/user?id=pantalaimon) | [25 comments](https://news.ycombinator.com/item?id=36722158)

In a recent blog post, a developer shared their experience working with tinygrad, rusticl, and ACO. They started by running tinygrad on their Radeon 6700XT using rusticl with the LLVM backend and found that it could successfully run an LLM model. The developer then decided to experiment with the Mesa ACO compiler backend and compared the performance to LLVM. They found that ACO was about four times faster to compile but produced less optimized binaries. The benchmark results showed that the LLVM backend had better performance in terms of runtime and GFLOPS. The developer mentioned that they plan to investigate ROCm in the future but are currently dealing with a cold/flu.

The discussion on the submission revolves around various topics related to NixOS, tinygrad, ACO, MLIR, and Rusticl.

- Users discuss the pros and cons of using NixOS for machine learning environments, with some praising its declarative configuration and others highlighting potential challenges with managing dependencies and complexity.
- Regarding tinygrad, there is a debate about its validity and whether it is a worthwhile project. Some express skepticism and question its benchmarks, while others appreciate its simplicity and ease of installation.
- MLIR and Rusticl are also discussed. Some users comment on the increasing popularity of MLIR-based middle-layer frameworks. Rusticl's hidden positive points are mentioned, and there is interest in exploring distributions with better hardware support.
- The ACO (AMD Compiler) and Mesa OpenCL driver are mentioned, with discussions about their compatibility and performance on different hardware and Linux distributions. There are some questions about specific APUs and their OpenCL support.
Overall, the discussion includes a mix of technical insights, opinions, and experiences related to the various technologies and projects being discussed.

### Meta to release open-source commercial AI model

#### [Submission URL](https://www.zdnet.com/article/meta-to-release-open-source-commercial-ai-model-to-compete-with-openai-and-google/) | 169 points | by [maskil](https://news.ycombinator.com/user?id=maskil) | [157 comments](https://news.ycombinator.com/item?id=36724739)

Meta, formerly known as Facebook, is gearing up to release a commercial version of its open-source large language model (LLM), LLaMA. LLaMA can generate text, images, and code using artificial intelligence (AI). The commercial release of LLaMA will enable developers and businesses to build applications using the foundational model, leading to accelerated technological innovation across various sectors. Meta's LLaMA comes in different sizes, ranging from 7 billion parameters to 65 billion parameters, surpassing OpenAI's GPT-3.5, which has 175 billion parameters. OpenAI and Google are Meta's main competitors in the AI space, and with the release of LLaMA, Meta hopes to make significant advancements in the field while addressing concerns about transparency and security associated with closed or proprietary software.

### Pulling my site from Google over AI training

#### [Submission URL](https://tracydurnell.com/2023/07/11/pulling-my-site-from-google-over-ai-training/) | 46 points | by [headalgorithm](https://news.ycombinator.com/user?id=headalgorithm) | [90 comments](https://news.ycombinator.com/item?id=36727384)

Tracy Durnell, a writer and designer in Seattle, has decided to de-index her website from Google in protest against the company using the content posted on the internet to train their generative AI models. She was influenced by posts from Jeremy Keith and Vasilis van Gemert. Although Tracy admits that she doesn't know how much search traffic her website receives, she's willing to sacrifice it for her beliefs. Tracy plans to start by pulling her websites out of Google search and then work on adding her sites to directories. She has added a noindex meta tag to her WordPress header and created a robots.txt file to block bots that collect training data for AI models. Tracy's decision highlights the ethical concerns surrounding AI training with user-generated content.

The discussion surrounding Tracy Durnell's decision to de-index her website from Google in protest against the company's use of user-generated content for training AI models is varied. Some users argue that reproducing content without permission is plagiarism and a breach of copyright, while others point out that machine learning and human learning operate under different principles and assumptions. There is a debate about the subjective nature of plagiarism and copyright law, and the potential consequences of AI training. Some users argue that AI cannot perform tasks at the speed and volume of humans and will never fully replicate human capabilities, while others believe that AI will continue to evolve and may have significant consequences. There are discussions about alternative search engines, the effectiveness of blocking bots with a robots.txt file, and the role of directories and webrings in web search. Some users highlight the challenges of determining what is legal or illegal, while others argue that the legality of certain actions does not make them right or wrong.

### Meta introduces CM3leon, a more efficient image generation model

#### [Submission URL](https://ai.meta.com/blog/generative-ai-text-images-cm3leon/) | 28 points | by [envy2](https://news.ycombinator.com/user?id=envy2) | [3 comments](https://news.ycombinator.com/item?id=36723886)

CM3leon is a state-of-the-art generative AI model that can generate both text and images. It is the first multimodal model trained with a recipe adapted from text-only language models, using a combination of retrieval-augmented pre-training and multitask supervised fine-tuning. Despite being trained with five times less compute, CM3leon achieves state-of-the-art performance in text-to-image generation. One of the key advantages of CM3leon is its versatility. Unlike previous models that could only generate either text or images, CM3leon can generate sequences of text and images conditioned on other image and text content. This expands its functionality and makes it more powerful. CM3leon also excels in tasks such as image caption generation, visual question answering, text-based editing, and conditional image generation. It outperforms Google's text-to-image model and achieves an FID score of 4.88 on the widely used image generation benchmark, establishing a new state of the art. The model demonstrates an impressive ability to generate complex compositional objects and performs well across a variety of vision-language tasks.
