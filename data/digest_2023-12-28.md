## AI Submissions for Thu Dec 28 2023 {{ 'date': '2023-12-28T17:11:49.520Z' }}

### Cold-blooded software

#### [Submission URL](https://dubroy.com/blog/cold-blooded-software/) | 500 points | by [arbesman](https://news.ycombinator.com/user?id=arbesman) | [198 comments](https://news.ycombinator.com/item?id=38793206)

In his blog post, "Cold-blooded software," Patrick Dubroy draws an interesting analogy between cold-blooded animals and software projects. He recounts a lecture where he learned about the remarkable ability of painted turtle hatchlings to survive being frozen. Just as cold-blooded animals adapt their metabolism to the temperature around them, Dubroy suggests that software projects should adopt a similar approach. He argues that warm-blooded projects, which thrive on constant motion and activity, are vulnerable to failure when frozen, akin to being put on hold for several months. In contrast, cold-blooded projects, like the baby painted turtle, can be frozen and easily picked back up without missing a beat. Dubroy emphasizes the importance of using "boring" technology in cold-blooded projects. By relying on vendored dependencies and avoiding external services that may change or disappear, he believes that software projects can remain resilient and functional for years to come. Dubroy shares the example of his own software project, a static site generator written in Python, which has remained intact and functional for twelve years. He concludes by saying that he fully expects it to continue working for another twelve years. The analogy and insights provided by Dubroy offer a unique perspective on the resilience and longevity of software projects.

The discussion on this submission involves various points regarding the longevity and stability of software projects. One commenter points out that Express, a popular JavaScript framework, has been running for 13 years and has served a large number of requests. They express contentment with using Express and state that it is good software overall. Another commenter adds that Express version 5 is expected to be released soon. A different commenter brings up Python and its ability to build resilient software. They mention that while Python 2 has reached its end of life, a Java 10-year-old code can still run in modern environments. This leads to a discussion about the differences between Python and Java and their approaches to backward compatibility. 

In response to this discussion, another commenter mentions that the Airflow project faced issues when renaming an operator, highlighting the complexities of maintaining software over time. They argue that Java is more cautious about making breaking changes compared to Python. The conversation shifts to the topic of build tools, with Maven being praised for its ability to handle large-scale dependencies. Others express their preference for Gradle or using Python's specific versioning for machine learning libraries. The discussion then touches on the challenges of managing dependencies in Python, with a suggestion to use tools like Poetry or conda to create virtual environments that work effectively. 

Lastly, there is a discussion about Python 3.11 and its release notes, which mention potential breaking changes and the removal of deprecated APIs. Some commenters argue that breaking changes in Python are limited and mainly involve removing deprecated APIs or semantics changes. Others mention specific examples of breaking changes they have encountered in different Python versions. 

Overall, the discussion delves into the considerations, challenges, and differences in maintaining and upgrading software projects.

### NY Times copyright suit wants OpenAI to delete all GPT instances

#### [Submission URL](https://arstechnica.com/tech-policy/2023/12/ny-times-sues-open-ai-microsoft-over-copyright-infringement/) | 505 points | by [justinc8687](https://news.ycombinator.com/user?id=justinc8687) | [849 comments](https://news.ycombinator.com/item?id=38790255)

The New York Times has filed a lawsuit against OpenAI and Microsoft, claiming that their AI models infringe on the newspaper's copyright and devalue its content. The suit alleges that OpenAI-powered software can bypass The New York Times' paywall and generate misinformation that is mistakenly attributed to the publication. The Times argues that this undermines its relationship with readers and reduces its revenue from subscriptions, licensing, advertising, and affiliates. The lawsuit also highlights examples of OpenAI's models reproducing large sections of Times articles and providing inaccurate information. The Times is seeking damages and an injunction to prevent further unauthorized use of its content.

The discussion surrounding the New York Times lawsuit against OpenAI and Microsoft is diverse. Some users argue that scraping and summarizing news articles is legal and a common practice. They point out that search engines also heavily rely on indexing and summarizing content without facing copyright lawsuits. Others argue that the Times' concern is justified because OpenAI's models can generate misinformation that is falsely attributed to the publication, undermining its relationship with readers and revenue streams.

There is also debate about the legality of AI models producing derivative work. Some users argue that OpenAI's models are transformative and don't directly copy the New York Times' content verbatim. They compare the situation to CliffNotes or Wikipedia, which provide summaries and impact the market for original works but are not considered copyright infringement. Others believe that these models still have a substantial negative impact on the market for the original content and should be subject to copyright laws.

The discussion also covers broader topics such as the nature of copyright in the digital age and the changing roles of lawyers in understanding and interpreting copyright laws. Some users argue that copyright laws need to be updated to reflect the realities of the digital world, while others emphasize the importance of protecting intellectual property rights.

Overall, the discussion showcases varying opinions on the legality and ethics of OpenAI's AI models and their impact on the New York Times' content.

### Dark Visitors – A list of known AI agents on the internet

#### [Submission URL](https://darkvisitors.com) | 127 points | by [johneth](https://news.ycombinator.com/user?id=johneth) | [63 comments](https://news.ycombinator.com/item?id=38797487)

Have you ever wondered about the hidden world of autonomous chatbots and data scrapers on the web? Well, now you can get a glimpse into this mysterious ecosystem with Dark Visitors. This platform provides insight into the AI agents crawling across the internet and allows you to protect your website from unwanted access. One of the featured AI agents is ChatGPT-User, dispatched by OpenAI's powerful ChatGPT. This intelligent assistant provides answers to user prompts and often includes summaries of website content. It's a handy tool for quickly grasping the essence of a page.

Cohere-ai is another intriguing AI agent, likely used by Cohere's AI chat products. This agent retrieves content from the internet based on user prompts, assisting in providing accurate information.
Anthropic-ai, an unconfirmed agent, may be employed by Anthropic to download training data for its Large Language Models (LLMs), including the notable Claude. It's fascinating to see how AI technologies rely on these behind-the-scenes processes.
CCBot, employed by Common Crawl, is an open-source web crawler. It maintains a repository of web crawl data accessible to everyone. This data has become instrumental in training a wide range of LLMs, including OpenAI's GPT-3.
FacebookBot is another web crawler, used by Meta. This bot downloads training data for Meta's AI speech recognition technology. It highlights the vast array of AI applications across different platforms.
Google-Extended, operated by Google, is yet another web crawler. It gathers AI training content for Google's AI products like Bard and Vertex AI generative APIs. This emphasizes the importance of quality data for training advanced AI models.
GPTBot, a familiar web crawler to OpenAI, is responsible for downloading training data for their LLMs. These models power various AI products, including the popular ChatGPT. It's remarkable to see how AI companies utilize such tools to continually improve their offerings.
Lastly, Omgilibot, operated by Webz.io, maintains a repository of crawl data that it sells to other businesses. This data is often employed for training AI models, enabling companies to enhance their AI capabilities.
If you want to explore further, Dark Visitors provides information on additional AI agents and allows you to stay updated on new ones. By keeping an eye on these agents and updating your website's robots.txt accordingly, you can ensure better control over your website's access.

The discussion surrounding the submission revolves around various topics related to AI agents and web crawling. Here are the key points:

- One user points out that Cohere's AI chat products likely use their AI agent, Cohere-ai, to retrieve content from the internet based on user prompts.
- Another user raises a question about the specificity of AI agents and suggests that the term "AI Agent" should refer to step-by-step procedures, multiple generation rounds, using tools to interact, and creating an environment for Large Language Models (LLMs).
- A user humorously notes that the term "Dark Visitors" is common in the English translation of books about hacking, such as in the context of hacking in China.
- One user argues that Common Crawl should not be blocked as they are a charity doing great work in producing open data sets.
- The legality and ethics of blocking web crawlers are discussed, with some arguing that it is within website owners' rights to control access, while others believe in the importance of maintaining an open internet.
- The potential risks and benefits of web scraping for training AI models are debated. Some argue that the availability of data benefits everyone, while others express concerns about potential copyright infringement and industrial espionage.
- There are suggestions for resources and tools to manage and block web crawlers, including a link to a robots.txt builder.

Overall, the discussion explores the nuances, challenges, and implications of AI agents and their impact on web browsing and data access.

### Knowledge Graph Reasoning Based on Attention GCN

#### [Submission URL](https://arxiv.org/abs/2312.10049) | 50 points | by [PaulHoule](https://news.ycombinator.com/user?id=PaulHoule) | [10 comments](https://news.ycombinator.com/item?id=38794757)

A new technique to enhance Knowledge Graph Reasoning has been proposed in a paper titled "Knowledge Graph Reasoning Based on Attention GCN," authored by Meera Gupta and three other researchers. The authors have combined Graph Convolution Neural Network (GCN) with the Attention Mechanism to develop a detailed feature representation for each entity in the knowledge graph.

The Attention Mechanism is used to examine the relationships between entities and their neighboring nodes, enabling the generation of comprehensive implicit feature vectors. By integrating entity attributes and interactions, this approach outperforms traditional neural network models in tasks such as entity classification and link prediction. The authors believe that this methodology can provide important support for applications like search engines, question-answering systems, recommendation systems, and data integration tasks.

The paper, available for download as a PDF, explains the methodology and presents experimental results to validate the effectiveness of the proposed approach. This research contributes to the field of Information Retrieval and offers insights for improving knowledge graph reasoning using attention-based graph convolutional networks.

The comments on this submission cover a range of topics related to knowledge graph reasoning and neural networks. Some users discuss the importance of explainability in artificial intelligence and the challenges of dealing with large knowledge graphs. One user mentions the limitations of current language models and the need to consider the context of knowledge graphs when answering questions. Others discuss the trade-offs and computational challenges of using attention mechanisms in neural networks. There is also a discussion about the performance and efficiency of graph neural networks for knowledge graph reasoning.

### SageBrush: AI Painter – the simplest way to create with AI

#### [Submission URL](https://www.apposite.ai/sagebrush-ai-painter-the-simplest-way-to-create-with-ai.html) | 19 points | by [gdubs](https://news.ycombinator.com/user?id=gdubs) | [15 comments](https://news.ycombinator.com/item?id=38788419)

Introducing SageBrush, the ultimate AI creativity tool that transforms your sketches into stunning artworks. Whether you're a seasoned artist or just starting out, SageBrush puts you in control of your composition with a human touch. With a prompt editor that allows you to specify subject and style details, you can guide the AI to create the perfect image. Want to let your imagination run wild? Crank up the prompt strength and watch the AI work its magic. SageBrush supports both left and right-handed drawing modes and accepts input from Apple Pencil or even your fingers. Plus, with light and dark modes, you can create anytime, anywhere. Get started with SageBrush and unleash your artistic potential today!

The discussion on Hacker News about the SageBrush AI creativity tool started with the creator, Gregory, announcing its release on the App Store. Users were impressed with the project and praised its capabilities. One user asked if the tool can generate detailed versions of the final image, to which the creator responded that there is no explicit setting for that but the canvas can be resized. Another user mentioned the confusion caused by some screenshots and suggested adding tooltips or explanatory tips to clarify the UI. The creator acknowledged the feedback and mentioned plans to include these features in the next version.

The discussion then shifted to the token-based pricing model of SageBrush. Some users expressed concern about the cost and suggested a local model to generate images instead. The creator explained that the local model is in the works but the initial version uses a token approach to cover costs and gather feedback. There were also comments about the prompt editor, with one user stating that it's not clear how to use tokens and that the prompts are too simplistic. The creator responded that they have received feedback on the prompt editor and are working on improving it. Other users expressed their desire for an Android or web-based version of SageBrush, as well as support for devices like foldable phones. There was also a request for more information about the pricing, to which another user shared that the token resolution is mentioned on the pricing page.

The discussion also touched on the hidden pricing, with a user pointing out that the dark pattern used makes it difficult to interact with. The creator responded that the dark pattern was chosen to indicate that the $1 token pack is a low-stakes option for people to try drawing. They clarified that in the first version, 1 token represents 1 brushstroke or prompt change, resulting in a maximum image resolution of 768px. The creator mentioned that they appreciate the feedback and are considering a broader and more serious bundle offering.

Overall, the discussion involved various feedback and requests for improvements, with the creator actively engaging with the comments and taking note of suggestions for future updates.

