## AI Submissions for Thu Jan 22 2026 {{ 'date': '2026-01-22T17:15:59.265Z' }}

### GPTZero finds 100 new hallucinations in NeurIPS 2025 accepted papers

#### [Submission URL](https://gptzero.me/news/neurips/) | 900 points | by [segmenta](https://news.ycombinator.com/user?id=segmenta) | [479 comments](https://news.ycombinator.com/item?id=46720395)

GPTZero claims 100 hallucinated citations across 50+ accepted NeurIPS papers

What’s new:
- After flagging 50 bogus references in ICLR submissions last month, GPTZero scanned 4,841 accepted NeurIPS papers and says it found 100 confirmed hallucinated citations across more than 50 papers.
- Examples include fabricated authors, fake DOIs/URLs, mismatched arXiv IDs, and placeholder refs left in (“Firstname Lastname,” “to be updated”).
- GPTZero also labels sections as likely AI-assisted or AI-generated, marking “Sources” (citation issues) and “AI” (generation), with symbols indicating mixed or strong AI use.

Why it matters:
- NeurIPS’s policy treats hallucinated citations as grounds for rejection or revocation, putting these already-presented papers in a gray zone.
- The scale problem is getting worse: NeurIPS submissions grew ~220% from 9,467 (2020) to 21,575 (2025), with a 24.5% acceptance rate. Reviewer capacity, expertise alignment, and fraud detection are strained.
- A figure in the report attributes hallucination counts to author institutions, underscoring that this is a systemic, not isolated, issue.

How they did it:
- Automated “Hallucination Check” verified references against public records; flagged items where titles/authors didn’t exist or identifiers pointed elsewhere.
- GPTZero publishes a table of 100 verified cases and notes this is not an indictment of specific reviewers but evidence of peer review’s limits under volume and generative AI.

Caveats and likely debate:
- LLM- and AI-detection remains contentious; false positives and edge cases (e.g., in-press citations, late arXiv updates) can occur.
- Expect calls for automated citation validation in conference pipelines, clearer LLM usage disclosures, and post-acceptance audits—and pushback on public shaming and detector accuracy.

The discussion on Hacker News focused on the ethics of AI assistance in academic writing, the validity of "hallucination" flags, and the definition of authorship.

**Verification of Claims**
One user, **j2kun**, investigated a flagged paper co-authored by a colleague. They confirmed that GPTZero correctly identified issues—specifically omitted authors and citations fabricated via "AI autocomplete"—but argued that these were validity errors in background sections rather than fundamental flaws in the research data.

**The "Translation" Defense**
A significant portion of the thread defended LLMs as essential accessibility tools for non-native English speakers. **drfr** and others argued that using AI to structure or translate thoughts is not "shoddy" work but a valid way to overcome language barriers and democratize science. They noted that technical jargon is often easier for domain experts to verify in an LLM translation than for human translators unfamiliar with the specific field.

**Research Integrity vs. Prose**
The debate split over what constitutes "research."
*   **The Critical View:** Users like **nlv** and **i_am_proteus** viewed AI-generated text as irresponsible or a form of plagiarism ("claiming authorship of IT output"). They argued that if authors cannot verify citations, the integrity of the entire paper—including data and experiments—is suspect.
*   **The Pragmatic View:** **mchlt** and others distinguished between the *science* (hypothesis, experiments, data) and the *writing*. They contended that as long as the experimental results are genuine, using an LLM to generate the prose is acceptable, even if it requires careful auditing for hallucinations.

**Attribution and Sentiment**
There was disagreement regarding how to handle AI credit, with suggestions ranging from listing AI as a co-author to including a "tools used" section. Finally, **ydyn** suggested that strong "anti-AI" sentiment is a minority extremism mostly found on platforms like Reddit, though **fngrlcks** countered that creative professionals (like graphic designers) largely share the concern regarding generative technologies.

### Qwen3-TTS family is now open sourced: Voice design, clone, and generation

#### [Submission URL](https://qwen.ai/blog?id=qwen3tts-0115) | 681 points | by [Palmik](https://news.ycombinator.com/user?id=Palmik) | [207 comments](https://news.ycombinator.com/item?id=46719229)

I’m ready to summarize—could you share the Hacker News link or paste the submission title and article/content here? I can’t browse, so I’ll need the text (or a screenshot/PDF).

Optional: tell me your preferences:
- Length: ultra-brief (3 bullets), 100–150 words, or deeper dive
- Extras: TL;DR, why it matters, notable trade-offs, standout HN comments, who should care

Here is the summary of the discussion surrounding **Qwen3-TTS Voice Cloning**.

### **The Story**
A user shared a demo on Hugging Face for **Qwen3-TTS**, a new text-to-speech model. The specific implementation allows users to clone a voice instantly by recording a short clip via microphone and pasting text for the AI to read back in that voice. The submission highlights how trivial and accessible high-fidelity voice cloning has become.

### **The Discussion**
The Hacker News comments reflect a sharp divide between existential dread and technological inevitability.

*   **The "Chasm" Has Been Crossed:** The top comment argues that society has "crossed a chasm" where screen-based interactions can no longer be trusted. Users debated the immediate rise of "grandparent scams" (e.g., AI voices claiming to be a grandchild held hostage) and the rapid disintegration of digital evidence in courts.
*   **Family Security Protocols:** A distinct practical sub-thread emerged suggesting families need "IFF" (Identification Friend or Foe) system—essentially secret "safe words" or challenge-response phrases to verify identity over the phone, treating family communication like military operations.
*   **Open Source vs. Big Tech:** While many found the tech terrifying, user `cnplxn` argued it is better for these models to be open-source (distributed power) rather than remaining the exclusive tool of "Big Tech" leaders with histories of unethical behavior.
*   **Mitigation & Verification:** There was skepticism regarding solutions. While some suggested C2PA (cryptographic provenance) or NFTs to verify human content, others noted that analog loopholes (recording a screen) break most verification chains.
*   **Creative Optimism:** A minority argued for the benefits, comparing this to the democratization of music production or the invention of photography—tools that enable creativity for those without physical performance skills, though others countered that this threatens the livelihood of voice actors.

### **Why it Matters**
This submission underscores that voice cloning is no longer a "future threat" or a tool for experts; it is browser-accessible and instant. The discussion suggests a societal shift where "digital trust" defaults to zero, necessitating new personal security habits (safe words) and legal frameworks (chain of custody) to navigate a world where audio evidence is easily forged.

### Show HN: BrowserOS – "Claude Cowork" in the browser

#### [Submission URL](https://github.com/browseros-ai/BrowserOS) | 77 points | by [felarof](https://news.ycombinator.com/user?id=felarof) | [27 comments](https://news.ycombinator.com/item?id=46721474)

BrowserOS: an open‑source, agentic Chromium fork that keeps your AI and browsing data local

A new release of BrowserOS (v0.37.0) is making waves on HN. It’s a Chromium-based browser that runs AI agents natively on your machine, positioning itself as a privacy‑first alternative to products like ChatGPT Atlas, Perplexity Comet, and Dia. It keeps your history and agent interactions on-device, supports your own API keys, and works with local models via Ollama or LM Studio. The project mirrors Chrome’s UI and supports extensions, but adds AI-native features like agent automation and a built-in AI ad blocker. It also exposes an MCP server so you can drive the browser from tools like Claude Code or gemini-cli.

Details:
- Platforms: installers for macOS, Windows, Linux (AppImage/Debian)
- Model providers: OpenAI, Anthropic, or local via Ollama/LM Studio
- Features: agent automation (form-filling, scraping, task flows), AI ad blocker, Chrome-like UI, extension compatibility, optional Chrome data import, MCP integration
- Positioning: open-source, local-first response to cloud-centric browsers and assistants
- Repo: AGPL-3.0, incorporates privacy patches from ungoogled-chromium; 8.9k stars, 847 forks; latest release Jan 21, 2026

Why it’s resonating on HN: it blends a familiar Chromium experience with on-device AI agents and an open-source license, aiming to avoid the lock-in and data collection concerns tied to closed, cloud-first “AI browsers.” Expect discussion around security and permissions for agent actions, real-world reliability of automation, and the implications of AGPL for downstream use.

**Browser vs. Extension Architecture**
A significant portion of the discussion focused on whether BrowserOS needs to be a full Chromium fork rather than a simple Chrome extension.
*   **The Critique:** User `rjnchnt` argued that an extension would suffice for the interface and expressed skepticism about the need for a standalone browser, suggesting the real bottleneck isn't the interface but scalable cloud execution. They also warned that custom browser layers (like Comet) are easily detected and blocked by major sites like Amazon.
*   **The Defense:** The creator (`flrf`) countered that a "sidebar extension" is just UI; the fork is necessary to grant agents capabilities that standard extensions cannot provide safely, such as direct filesystem access, executing shell commands, and deeper interactions required for "Co-worker" workflows.
*   **Technical Nuance:** User `johnsmith1840` supported the fork approach, noting that standard extensions struggle with cross-origin iframes (e.g., payment forms) and JavaScript injection restrictions on complex sites like Microsoft Word Online.

**Features and Capabilities**
*   **MCP Integration:** Several users expressed interest in the Model Context Protocol (MCP) server. The creator confirmed BrowserOS exposes an MCP server out of the box, allowing tools like Claude Code or Cursor to drive the browser—a process they claim is much easier than configuring the Chrome DevTools MCP.
*   **Enterprise Guardrails (IAM):** Users `4b11b4` and `mossTechnician` were intrigued by the mention of "IAM for Agents" to reliably enforce permissions. The creator explained this functions at the Chromium level—restricting agents to specific DOM elements (e.g., a single button in SAP)—though they noted this feature is currently in early versions and not yet in the public repo.
*   **Local Hardware:** Regarding local model performance, the creator confirmed that a Mac with 32GB RAM can handle ~20B parameter models (like `gpt-4o-mini` equivalents) with a 12k context length.

**Marketing and Branding Feedback**
*   **"BrowserOS" Name:** Critics (`vysly`, `ripped_britches`) felt the name "BrowserOS" was confusing or misleading. The creator argued the name reflects the reality that for knowledge workers, the browser has effectively *become* the operating system.
*   **Use Cases:** User `jm4` advised the team to pivot marketing from technical specs to concrete problem-solving. They suggested highlighting consumer automation examples, such as checking children's grades on school portals, meal planning with grocery delivery, or booking flights for large groups.

### Show HN: Text-to-video model from scratch (2 brothers, 2 years, 2B params)

#### [Submission URL](https://huggingface.co/collections/Linum-AI/linum-v2-2b-text-to-video) | 93 points | by [schopra909](https://news.ycombinator.com/user?id=schopra909) | [21 comments](https://news.ycombinator.com/item?id=46721488)

Linum v2: a tiny, open-source text-to-video model (2B params) for short clips

- What’s new: Linum-AI released Linum v2, a 2B-parameter text-to-video model, updated within the last day and published in a Hugging Face collection.
- Capabilities: Generates 2–5 second videos at 360p or 720p.
- License: Apache 2.0 (permissive, commercial-friendly).
- Why it matters: A comparatively small, openly licensed T2V model lowers the barrier for experimentation and integration, adding to the momentum of practical, hackable video generators outside big proprietary stacks.
- Caveats: Short duration and modest resolution suggest it’s early-days utility rather than a frontier-quality rival; details on training, hardware requirements, and benchmark quality aren’t in the submission.
- Pulse: The post is still early with light traction (5 upvotes).

Here is a summary of the discussion:

**Hardware Constraints & Optimization**
The bulk of the technical discussion focused on the model's surprisingly high memory footprint (~20GB VRAM) relative to its small size (2B parameters). The author explained that the heavy lifting is actually done by the T5 text encoder (5B parameters) and the massive context window required for video generation (roughly 100k tokens for a 5-second 720p clip).
*   **Proposed Solutions:** Users suggested quantizing the text encoder (to 8-bit or 4-bit) or deleting the text encoder from memory immediately after the initial encoding step to free up resources.
*   **Author Response:** Validated these suggestions, noting they are updating code to allow for manual layer offloading and deleting text encodings to save RAM, acknowledging that the text encoder size is disproportionate to the video model.

**Learning Resources**
*   Users asked for end-to-end courses on building similar video models.
*   The author directed users to their "Field Notes" blog for technical breakdowns and promised more "0 to 1" documentation in the coming months, though they noted they don't have the bandwidth to create a full course (jokingly hoping Andrej Karpathy might cover it eventually).

**Miscellaneous**
*   A broken Hugging Face collection link was identified and fixed by the author.
*   The project received praise for being an impressive achievement for a small team.

### Composing APIs and CLIs in the LLM era

#### [Submission URL](https://walters.app/blog/composing-apis-clis) | 65 points | by [zerf](https://news.ycombinator.com/user?id=zerf) | [14 comments](https://news.ycombinator.com/item?id=46722074)

Title: The best code is no code: let agents use the shell, not bespoke tools

Thesis: Instead of stuffing agents with many fine-grained “tools,” let them call real CLIs via a single exec_bash. The Unix shell gives you composition, pipes, and scripts—so agents can chain steps in one shot, save tokens, and produce pipelines humans can read, tweak, and run.

How to expose SaaS without MCP:
- Treat OpenAPI specs as programs; use Restish as the interpreter.
- Example: For Google Docs, skip writing a custom gdrive client. Register Google’s OpenAPI spec and call endpoints directly: restish cool-api list-images | jq ...
- Friction points with Restish:
  - It wants to own auth; the author prefers injecting tokens.
  - It requires pre-registering specs; the author wants ephemeral, “just interpret this spec now.”
- Solution: Wrap Restish with a small script that:
  - Creates a temporary spec directory on the fly.
  - Performs auth separately and injects tokens.

Auth as a program:
- OAuth 2.0 is standardized; use an interpreter for it too.
- oauth2c runs the flow (opens browser) and prints tokens to stdout.
- Resulting pipeline: oauth2c "https://accounts.google.com/..." | restish google drive-files-list
- Replaces hundreds of lines of Python with a compact shell script.
- Released: bmwalters/gdrive-client (includes a neat way to propagate shell completions).

Secure token storage on macOS:
- Need to stash a long-lived refresh token safely (biometrics/passcode-gated).
- The security CLI can store secrets, but biometric-gated access isn’t straightforward.
- Likely requires a small Swift helper and proper entitlements; under-documented and finicky.

Why it matters:
- Composable, low-latency agent workflows with fewer round trips and lower token costs.
- Human- and machine-friendly pipelines you can version, reuse, and audit.
- Tradeoffs: you manage auth and wrap tooling yourself; platform-specific secure storage can be rough.

Based on the discussion, here is a summary of the reactions to the submission:

**The Debate: CLI Composability vs. MCP Structure**
The core tension in the comments is between the flexibility of the Unix shell and the safety/structure of the Model Context Protocol (MCP).
*   **CLI Advocates:** Users validated the author's thesis, noting that large language models (LLMs) are naturally adept at shell commands because existing training data is full of them. They agreed that using `curl` directly with OpenAPI specs (OAS) is often sufficient and avoids unnecessary wrappers. One user illustrated the power of using FUSE and Bash to make remote endpoints discoverable as simple files and commands.
*   **MCP Advocates:** Critics pointed out that while CLIs are composable, they rely on parsing text (`stdout`), which can be brittle. They argued that MCP is valuable because it enforces schemas and types, creating a strict contract between the model and the backend, which is critical for building reliable, non-flaky pipelines.

**The Security Bottleneck: Authentication**
A major friction point discussed is how to handle credentials safely in a shell-based agent workflow.
*   **The Problem:** Giving an agent raw shell access often requires loading API keys into environment variables. Users warned that this makes secrets visible to the context window, creating a "security incident waiting to happen" if the LLM leaks them or hallucinates.
*   **Proposed Solutions:**
    *   **Proxies:** Several commenters suggested using a proxy layer where the agent hits a generic endpoint, and the proxy injects the actual secrets, ensuring the agent never sees the token.
    *   **Dynamic Clients:** One builder described an approach where the system dynamically generates CLI clients with pre-authenticated sessions, so the agent runs commands (e.g., `list-gmail-messages`) without ever handling the auth lifecycle itself.

**Hybrid Approaches and Tooling**
Several developers shared tools attempting to bridge these philosophies:
*   Registries (like `tpm`) that categorize tools and allow agents to interact via multiple methods—generating a convenient `skills.md` for the LLM while supporting executing via CLI, REST, or hosted MCP servers.
*   CLI wrappers that treat OpenAPI specs as programs directly, acknowledging that the industry is moving toward "programs writing text files" as a primary interface.

**Skepticism**
A thread of meta-commentary expressed skepticism toward the current AI trend, describing it as a "cargo cult" where "writing markdown files" is being rebranded as advanced technology, though others retorted that most programming is effectively just "weirdly formatted lists of computer commands" anyway.

### Show HN: I've been using AI to analyze every supplement on the market

#### [Submission URL](https://pillser.com/) | 80 points | by [lilouartz](https://news.ycombinator.com/user?id=lilouartz) | [40 comments](https://news.ycombinator.com/item?id=46719423)

A new evidence explorer aims to help people navigate supplement claims by linking supplements, research papers, and health outcomes. The site touts breadth—15.9K supplements, 4.4K research papers, and 7.4K health outcomes—and lets you query examples like Vitamin D, beta carotene, or cholesterol levels. It also integrates an AI assistant; end your query with a question mark to get an AI-generated synthesis.

Why it matters:
- Supplements are a muddled space; a cross-linked database of outcomes and citations could make it easier to see what’s been studied and how often.
- Transparency on sources and study types will be key, since evidence quality varies widely.

What to try:
- Search a supplement, a biomarker, or a condition to see related studies and outcomes.
- Use the AI “?” prompt for a quick summary, then click through to the underlying papers.

Caveat: Tools like this can surface research, but they aren’t a substitute for professional medical advice.

**The Discussion:**
*   **Data Accuracy vs. Ingredient Reality:** A major thread challenged the utility of aggregating label data in an unregulated industry ("wild west"). Users pointed out that labels often don't match contents (citing issues like fillers or heavy metals) and preferred the *ConsumerLab* model of independent testing. The creator (`llrtz`) agreed, stating that while the current iteration relies on label aggregation via LLMs, the long-term goal is to fund independent lab testing using affiliate revenue.
*   **Legal Threats:** Commenters warned that "shady" supplement companies might issue Cease & Desist orders to hide negative info or price comparisons. The creator plans to comply to avoid legal costs but proposed leaving a "content redacted by manufacturer request" placeholder to signal a lack of transparency to users.
*   **Monetization:** The creator clarified the business model is 5–10% affiliate commissions (Amazon, iHerb) rather than ads or holding inventory, which allows them to remain a "solo founder" without complex logistics.
*   **Technical Feedback:** Users found specific data errors (e.g., Creatine dosage discrepancies in powders vs. pills) and bugs in the search logic. The creator attributed some extraction errors to the LLM (Opus) and promised fixes for normalizing different units of measurement.

### Satya Nadella: "We need to find something useful for AI"

#### [Submission URL](https://www.pcgamer.com/software/ai/microsoft-ceo-warns-that-we-must-do-something-useful-with-ai-or-theyll-lose-social-permission-to-burn-electricity-on-it/) | 141 points | by [marcyb5st](https://news.ycombinator.com/user?id=marcyb5st) | [196 comments](https://news.ycombinator.com/item?id=46718485)

Satya Nadella warns AI could lose “social permission” if it wastes energy, urges everyone to use it anyway (PC Gamer)

- The news: At the World Economic Forum, Microsoft CEO Satya Nadella said public support for AI will evaporate if it burns scarce energy without delivering clear, real-world gains—citing health, education, and public-sector efficiency as examples where it must “change outcomes.” He described AI as a “cognitive amplifier” that gives access to “infinite minds.”

- Supply side: Nadella called for building a “ubiquitous grid of energy and tokens,” effectively more power and compute. PC Gamer links that to today’s component crunch—soaring memory prices and constrained supply driven by AI buildouts.

- Demand side: He argued every company should start using AI now, and workers should treat “AI skills” like Excel for employability. Concrete example: AI scribes to handle clinical transcription, EMR entry, and billing codes so doctors can spend more time with patients.

- Skepticism: The piece questions surveillance and billing incentives in healthcare and notes many mainstream AI wins still boil down to transcription, summarization, and code snippet retrieval—short of the internet/PC-level revolution hype.

- Why it matters: AI’s social license may hinge on measurable outcomes vs energy costs; the push for universal adoption collides with power and supply constraints; and the “AI skillset” narrative meets real privacy, utility, and economics trade-offs.

Discussion starters:
- What metrics would prove AI’s energy burn is worth it?
- Are AI scribes net positive once you factor privacy and billing dynamics?
- Is “AI skill” the new Excel—or just hype until use cases move beyond summarize/transcribe/code search?

 **Summary of Discussion:**

The discussion focuses heavily on the practical utility of current AI models versus the "productivity expert" narrative, with many users skeptical that LLMs offer net-positive efficiency gains for complex tasks.

Key themes include:

*   **Productivity vs. Verification:** While users acknowledge LLMs are faster than Google for answering "How to" questions, many argue that the overall productivity gain is negligible. The time saved in searching is often lost verifyng results or debugging "hallucinations," such as non-existent APIs, fake documentation, or plausible-sounding but broken code libraries.
*   **The Hallucination Bottleneck:** Several commenters shared anecdotes of wasting time trying to implement code based on AI suggestions (e.g., a specific `v4l2` method or Ubuntu clock settings) that simply did not exist. Users noted that while traditional search engines might return irrelevant results, LLMs actively generate "fake content on the fly," including hallucinated URLs and academic references.
*   **Search vs. Synthesis:** There is debate over whether LLMs are replacing search engines due to superior utility or simply because Google Search's quality has declined. Some users now treat LLMs as a pre-processor to generate keywords for a traditional search, rather than trusting the AI output directly.
*   **Coding and Reliability:** Engineers pointed out that using LLMs to generate code for standard tasks is often inferior to using established, tested libraries. There is a sentiment that LLMs encourage a "lazy" approach that bypasses deep understanding, resulting in codebases that are harder to maintain or verify.

### Skill.md: An open standard for agent skills

#### [Submission URL](https://www.mintlify.com/blog/skill-md) | 45 points | by [skeptrune](https://news.ycombinator.com/user?id=skeptrune) | [12 comments](https://news.ycombinator.com/item?id=46723183)

Mintlify proposes skill.md: a standard “cheat sheet” for AI coding agents

- What’s new: Mintlify introduced skill.md, a markdown file that tells AI agents exactly how to use your product. It lives at /.well-known/skills/default/skill.md (also /skill.md on Mintlify sites) and can be installed into 20+ coding agents via Vercel’s skills CLI (e.g., Claude Code, Cursor, OpenCode). It aligns with emerging proposals from Cloudflare and Vercel. Mintlify auto-generates and refreshes this file whenever docs change, and users can override it by adding skill.md to their repo.

- Why it matters: Documentation is written for humans, but agents need a compact, always-relevant context. LLMs can’t keep entire docs in context and are often out of date. A concise, up-to-date skill.md dramatically improves agent output by encoding capabilities, limitations, best practices, and “tribal knowledge” in one place.

- What goes inside: 
  - Decision tables to guide choices (e.g., when to use components)
  - Clear boundaries (what agents can configure vs what needs dashboard setup)
  - Gotchas (e.g., deprecated files, required frontmatter)
  - Links to deeper docs via llms.txt

- Notable: Mintlify is deprecating last week’s install.md in favor of skill.md, which combines install and usage guidance and is seeing more ecosystem momentum.

- How to adopt: Use the autogenerated file on Mintlify or add your own skill.md to your repo, including any “personal taste” guidance you want agents to follow.

**Discussion Summary:**

The conversation focused heavily on the rapid deprecation of `install.md` (announced only days prior) in favor of the new `skill.md`. Users criticized this high level of "churn" and "thrash," arguing that replacing a protocol in less than a week signals a lack of conviction and erodes trust in Mintlify as a platform. While a Mintlify representative acknowledged the confusing optics—framing it as a difficult prioritization balance for an early-stage startup—commenters countered that true "standards" require stability and consensus across independent groups, rather than frantic iterations typical of the "AI hype bubble." Others expressed general fatigue with the frequency of new file specifications ("a file everyday") and noted technical errors in the launch post's links.

### Miami, your Waymo ride is ready

#### [Submission URL](https://waymo.com/blog/2026/01/miami-your-waymo-ride-is-ready) | 82 points | by [ChrisArchitect](https://news.ycombinator.com/user?id=ChrisArchitect) | [153 comments](https://news.ycombinator.com/item?id=46721418)

Waymo launches fully driverless ride-hailing to the public in Miami

- What’s new: Waymo is opening its fully autonomous ride-hailing service to public riders in Miami, inviting users on a rolling basis. Nearly 10,000 residents have already signed up.
- Service area: An initial 60-square-mile territory covering the Design District, Wynwood, Brickell, and Coral Gables, with plans to expand to Miami International Airport.
- Safety pitch: Waymo cites 127 million fully autonomous miles and a 10x reduction in serious injury crashes versus human drivers in its operating areas. The company says its stack handles bright sun and sudden tropical downpours common to Miami.
- Local reception: Miami-Dade’s commission chair welcomed the service with an emphasis on safety, transparency, and accountability. Groups like MADD South Florida and Miami Lighthouse for the Blind highlighted potential benefits for impaired-driving reduction and accessibility.
- How to ride: Access via the Waymo app; invitations will scale up as the service ramps.

**Discussion Summary:**

*   **User Experience:** Early adopters describe the experience as private and futuristic, though some noted that Waymo’s strict adherence to speed limits (e.g., doing 5mph in parking lots or 40mph on major roads while others speed) can make it feel like the slowest vehicle on the road. Others lamented the loss of serendipitous social interactions found with human Uber/Lyft drivers.
*   **Economic Impact:** A significant debate emerged regarding the local economic effects of autonomous fleets. Critics argued that while gig drivers spend their earnings in the local community, Waymo funnels revenue back to corporate headquarters and shareholders, effectively acting as a wealth transfer. This sparked a broader historical debate about Luddism, automation, and whether the displacement of labor consolidates wealth or benefits society in the long run.
*   **Privacy & Operations:** Users discussed in-car monitoring, noting that microphones are generally disabled unless a rider calls support. There was also speculation about the "humans in the loop" (remote assistance), investigating whether latency issues would require local staff or if those jobs would eventually be outsourced.
*   **Pricing:** Anecdotal price comparisons placed Waymo's cost in the mid-range—typically more expensive than a standard UberX but cheaper than Uber Black or traditional taxis.

### Show HN: First Claude Code client for Ollama local models

#### [Submission URL](https://github.com/21st-dev/1code) | 41 points | by [SerafimKorablev](https://news.ycombinator.com/user?id=SerafimKorablev) | [22 comments](https://news.ycombinator.com/item?id=46722285)

1Code: a desktop UI for Claude Code with worktree isolation and built‑in Git

What it is: 21st.dev’s 1Code is a Cursor-style desktop app (macOS/Linux/Windows) that wraps Anthropic’s Claude Code with a visual UI, local-first execution, and safer Git workflows. It’s open source (Apache-2.0), with optional paid builds and extras.

Highlights
- Git worktree isolation: Each chat runs in its own worktree, protecting your main branch.
- Background agents and parallel runs: Kick off long jobs and keep working. (Full background support is part of the subscription build.)
- Built-in Git client + diffs: Stage/commit/branch and preview changes in real time; see commands, file edits, and searches as they happen.
- Plan mode: Claude asks clarifying questions, shows a structured plan and markdown preview before executing.
- Terminal integration and MCP support; memory, slash commands, custom subagents; BYO models/providers.
- Cross-platform; Windows support improved by community contributors.

How it compares to Claude Code
- Adds a full visual UI, integrated Git client, worktree isolation, and parallel/background execution—features the stock CLI lacks.
- Checkpointing is in beta; “Tool Approve” is on the backlog; lacks “hooks” that Claude supports.

Install/try
- Build free from source with Bun (requires downloading the Claude CLI binary via bun run claude:download).
- Or subscribe at 1code.dev for pre-built releases and background agents.
- Repo: github.com/21st-dev/1code (4.2k★, 379 forks). Latest release: v0.0.33 (Jan 22, 2026).

Why it matters: For developers using Claude as a coding agent, 1Code brings a familiar, Cursor-like workflow with guardrails around Git and a clearer review/plan loop—without sending your code to a hosted service.

The discussion focuses heavily on configuring the tool to work with local LLMs, hardware requirements, and the trade-offs of using local models versus paid APIs.

**Local Model Configuration & Proxies**
*   **Bypassing Anthropic:** Users shared methods to force Claude Code (and by extension 1Code) to use local backends like **llama.cpp** and **Ollama**. Key workarounds include setting the `ANTHROPIC_BASE_URL` environment variable, disabling telemetry (`CLAUDE_CODE_DISABLE_NONESSENTIAL_TRAFFIC`), and manually editing config files (`hasCompletedOnboarding: true`) to skip login checks.
*   **Proxy Challenges:** There is ongoing friction in translating OpenAI-style local endpoints to the Anthropic format 1Code expects. While some users recommended specific proxies or custom routers (`claude-code-mix`), others reported failure with standard wrappers (like generic **litellm** setups) due to issues with tool-use definitions and function-calling translation.
*   **Successful Models:** Users reported functional successes running models like **Qwen3-Coder** and **GLM-4** locally.

**Hardware & Practicality**
*   **Resource Demands:** Participants estimated that running capable coding models (30B parameters) generally requires a GPU with at least **24GB VRAM**, though this often leaves a disappointingly small context window.
*   **Apple Silicon:** An M1 Max user reported achieving roughly 20 tokens per second with Qwen3-30B.
*   **Is it worth it?** Despite the enthusiasm for local execution, some commenters argued that the time lost debugging hallucinations from local models makes paying for **Claude Sonnet/Opus** cheaper and more efficient in a professional workflow.

### Show HN: CLI for working with Apple Core ML models

#### [Submission URL](https://github.com/schappim/coreml-cli) | 45 points | by [schappim](https://news.ycombinator.com/user?id=schappim) | [5 comments](https://news.ycombinator.com/item?id=46724565)

coreml-cli: a native macOS CLI for Core ML, no Xcode or Python required

A new open-source tool (MIT) brings a clean, scriptable command line to Apple’s Core ML workflow. schappim/coreml-cli lets you inspect models, run inference, batch-process datasets, benchmark across CPU/GPU/ANE, and compile to .mlmodelc—all from the terminal with JSON/CSV-friendly output.

Why it matters
- Cuts out Xcode sample apps and Python glue, making Core ML feel “Unix-y”
- Great for CI/CD, quick sanity checks, performance baselining, and device comparisons on Apple Silicon
- Handy for ML/QA teams to validate models, measure regressions, and automate batch jobs

Highlights
- Inspect: view model structure, inputs/outputs, and metadata; JSON output for scripting
- Predict: run inference on images, text, audio, or JSON tensors; choose device (cpu/gpu/ane); save results
- Batch: process directories with configurable concurrency; CSV/JSON outputs
- Benchmark: iterations, warmup, latency distribution (mean/min/max/stddev/P50/P95/P99) and throughput; per-device; JSON for CI
- Compile: convert .mlmodel to optimized .mlmodelc with validation
- Metadata: view model metadata

Details
- macOS 13+; native Swift binary; current release v1.0.0
- Install via Homebrew tap (recommended) or download release; build from source with Swift 5.9+
- Repo: github.com/schappim/coreml-cli (72★ at time of posting)

Use cases
- Compare CPU vs GPU vs ANE on M-series Macs
- Batch classify folders and export CSV
- Gate model changes in CI using benchmark JSON
- Quick local checks before shipping a model to an app store build

**Discussion Summary**

The conversation centered on the tool's scope and performance characteristics:

*   **Scope & Conversion:** Users clarified that `coreml-cli` is strictly for inference and benchmarking existing Core ML files, not for converting models from PyTorch or TensorFlow. The author directed users to Apple’s `coremltools` for conversion, though some commenters expressed frustration with that library's dropped support for formats like TF Lite and ONNX, suggesting native Swift conversion might be preferable.
*   **Performance & Architecture:** While the JSON output was praised for enabling easy integration with frameworks like LangGraph, concerns were raised regarding latency. Users noted that as a CLI, the tool likely incurs significant overhead by reloading the model into memory for every invocation, unlike a persistent service that keeps the model loaded.
*   **Feature Request:** There was a suggestion to add functionality that parses Xcode performance reports into human-readable formats, as users found AI models (like Gemini) struggle to interpret the raw JSON data.

### How LLM agents solve the table merging problem

#### [Submission URL](https://futuresearch.ai/deep-merge-tutorial/) | 26 points | by [ddp26](https://news.ycombinator.com/user?id=ddp26) | [4 comments](https://news.ycombinator.com/item?id=46723898)

Could you share the Hacker News submission you want summarized? A link to the HN thread (or the article itself) is perfect.

Preferences I can follow:
- Length: ultra-brief (3–4 sentences) or fuller (6–8 sentences)
- Include HN comment highlights? Yes/No
- Extras: “Why it matters,” key takeaways, notable criticisms

Paste the URL and I’ll produce a crisp, engaging digest.

Here is a digest of the discussion within the thread, reconstructed from the abbreviated text provided.

**The Topic:** Strategies for reconciling financial data (matching bank statement rows to accounting records).

**The Conversation:**
Commenters discussed the complexity of automating financial reconciliation, agreeing that real-world data is messy and creates a difficult "fuzzy threshold" problem. The consensus is that **false positives are worse than false negatives** because incorrect matches erode user trust immediately. To solve this, practitioners suggest a **tiered confidence approach**: automatically processing high-confidence matches, sending medium-confidence matches for human review, and leaving low-confidence items unmatched.

While traditional "fuzzy matching" handles typos well, it fails at semantic discrepancies (e.g., matching "PAYPAL*ACME" to "Invoice 1234 - Acme Ltd"). Discussants proposed using Large Language Models (LLMs) and web agents to bridge this gap by identifying entities and websites, though cost remains a factor.

**Comment Highlights:**

*   **The "Trust" Hierarchy:** **jckfrnklyn** argues against a binary match/no-match system. They recommend a tiered system to protect user trust, noting that fuzzy matching often fails when transaction strings are semantically linked but textually distinct.
*   **The "Cascade" Pattern:** **mcknnmyr** and **parad0x0n** champion a "cascade" architecture. Users should run cheap, strict fuzzy matching first for obvious data, and only "escalate" the remaining non-matches to an LLM. This keeps costs down while using the LLM to prevent false positives on difficult data.
*   **The Semantic Solution:** **ddp26** notes that LLMs and ReAct web agents are solving the specific problem of linking unmatched vendors (e.g., "Acme Corp" vs "my-logistics.com") by actually understanding the entity behind the data.

### Show HN: Figr – AI that thinks through product problems before designing

#### [Submission URL](https://figr.design/) | 10 points | by [Mokshgarg003](https://news.ycombinator.com/user?id=Mokshgarg003) | [5 comments](https://news.ycombinator.com/item?id=46724567)

Figr: a “product-thinking-first” design tool that turns messy requirements into production-ready prototypes and specs. The pitch: map edge cases, user flows, and decisions up front so engineering doesn’t discover them later.

What it does
- Surfaces edge cases, drafts PRDs/specs, maps journeys/IA, and generates QA/test cases.
- Produces high‑fidelity prototypes that mirror your product; one‑click export to Figma.
- Enforces design system tokens, runs accessibility checks, and can ingest analytics (e.g., CSVs) for context.
- Aims to help both PMs (PRDs, rationale, A/B variants) and designers (system‑aligned prototypes, UX reviews).

Live gallery highlights
- Zoom: detailed network degradation states (packet loss, throttling, reconnection loops) with UX decisions.
- X/Twitter: “See less for 24 hours” soft‑mute prototype.
- Wise: comprehensive test cases for card freeze flows.
- Waymo: mid‑trip stop/destination change scenarios and edge cases.
- Task assignment component: all post‑action states mapped.
- Spotify: AI playlist curation PRD and updated user flows.
- Skyscanner: elder‑friendly UX audit.
- Shopify: checkout setup redesign informed by engagement data.
- Perplexity: “source freshness” tagging in results.

Why it matters: It’s trying to compress the PM-to-design handoff by baking decisions, constraints, and tests into the prototype, reducing rework before dev. The site claims 500+ teams use it.

**Discussion Summary:**

Discussion focuses on Figr’s positioning as an "AI Product Manager" versus a simple UI generator, with specific feedback on its pricing model and the implications of using a large pattern library.

*   **Solving Enterprise Complexity:** One user (`its_down_again`) highlights the difficulty of building intuitive interfaces for enterprise AI apps, noting that maintenance often overwhelms engineering. They find Figr useful for real-time collaboration with customers—translating articulable frustrations into concrete designs and decision logs during meetings.
*   **Positioning and Logic:** `mdlndr` praises the tool for preserving design reasoning (decisions made, rejected, or considered), contrasting it with "hand-wavy product docs." They view it more as an AI PM that happens to design, rather than just a distinct screen generation tool.
*   **The "Pattern Corpus" Debate:** `mdlndr` expresses concern that the "200k UX pattern corpus" might bias designs toward generic patterns rather than product-specific needs.
    *   **Maker Response:** `Mokshgarg003` explains that the corpus is primarily used to prevent AI hallucinations on "solved problems" (standard UI components) so the AI doesn't reinvent the wheel, while the specific product context drives the actual flows and specs.
*   **Pricing Concerns:** `pdlpt` argues that credit-based pricing (per question/action) discourages usage compared to flat monthly subscriptions, specifically in B2B contexts where predicting costs is difficult.
    *   **Maker Response:** The maker clarifies that the credit system is roughly defined as "1 credit = 1 screen of design work," though admits it can be hard to explain simply.

### AI SlopStop by Kagi

#### [Submission URL](https://help.kagi.com/kagi/features/slopstop.html) | 40 points | by [janandonly](https://news.ycombinator.com/user?id=janandonly) | [13 comments](https://news.ycombinator.com/item?id=46716806)

Kagi launches SlopStop: community reporting to downrank low‑quality AI “slop”

What it is
- A user‑powered system to flag low‑quality, mass‑generated AI content (“AI slop”) across web, image, and video search.
- Kagi already downranks ad/tracker‑heavy SEO spam; SlopStop adds collaborative signals to identify domains and channels primarily pumping out AI‑generated material.

How it works
- Web: Individual pages are reviewed. If a domain is “mostly AI” (typically >80% of pages), it’s flagged and downranked. Subdomains are evaluated separately.
- Images/Videos: AI content is marked and downranked by default. If a host/channel is mostly AI, it’s flagged and further downranked. Users can filter to hide AI images/videos entirely.
- Downranking, not removal: Flagged results can still appear, but below higher‑quality, original content.

Reporting and review
- Report via the shield icon in search results. Any user can report “AI slop” or “not AI slop.”
- Every report is human‑reviewed; multiple reports help speed review. Typical turnaround: about a week.
- Appeals: “Not AI slop” triggers re‑review; flags are removed if accepted, and rankings adjust.
- Track status in Settings > Search > AI > SlopStop Reports (URL, time, status).

Philosophy
- Kagi says it supports AI that enhances creativity but opposes mass‑generated content that undermines authenticity and trust. The “mostly AI” bar aims to avoid penalizing mixed or responsibly used AI on otherwise legitimate sites.

Why it matters
- As AI‑generated SEO sludge surges, SlopStop blends algorithmic defenses with community curation to keep search results useful without outright censorship. Potential pressure points: defining “low‑quality,” avoiding false positives, and guarding against coordinated reporting—mitigated here by human review and reversible flags.

**Discussion Summary:**

Implementation details and the potential for abuse dominated the conversation regarding Kagi's SlopStop launch:

*   **Defining "Slop" vs. AI:** Users debated the terminology, with some questioning whether "slop" effectively describes all AI-generated material or only low-quality SEO spam. While some users felt the distinction was clear—targeting sites that are mass-produced with zero value—others argued that useful, AI-assisted content might be unfairly caught in the dragnet. Kagi's criteria (flagging domains that are >80% AI) was highlighted as a safeguard for mixed-use sites.
*   **Weaponization Concerns:** A significant portion of the discussion focused on the risk of users abusing the report feature to silence controversial human-written content or competitors. Commenters expressed a need for "symmetric reporting" or robust appeal processes to prevent the system from becoming a tool for censorship or false positives.
*   **Framing the Feature:** Some users suggested that labeling the feature "Report AI" creates a bias that encourages a "witch hunt" against anything resembling machine output. They proposed framing it as reporting "low-quality" or "spammy" content instead, noting that "human slop" is also prevalent and should be treated similarly.
*   **Rollout Status:** A user (appearing to speak on behalf of the project) noted that while report processing has started, the systems for handling them at scale are being finalized for an official start in January.

### Mana LLM OS

#### [Submission URL](https://www.mana.space/landing) | 20 points | by [behzadhaghgoo](https://news.ycombinator.com/user?id=behzadhaghgoo) | [11 comments](https://news.ycombinator.com/item?id=46724470)

Could you share the Hacker News submission you want summarized (HN link or the article URL/text)? Also let me know your preferred length and tone.

Default if you don’t specify:
- 150–200 words
- Headline + 3–5 key points
- Why it matters + one-line takeaway

If you want comment highlights too, paste a few top comments.

Based on the discussion provided (which appears to be abbreviated or compressed text), here is a summary of the conversation regarding a tool for building personal apps with LLMs.

### **Discussion: Building Custom Personal Apps with LLM-Generated UIs**

**Headline:** Users discuss a new tool for generating personal apps, comparing it to "Claude Code" but with a focus on interactive UIs rather than terminal interfaces.

**Key Discussion Points:**
*   **GUI vs. Terminal:** Commenters position this tool as a solution for those who want to build custom apps but prefer a visual interface over terminal-based tools like Claude Code. Note the emphasis on direct interaction (clicking, viewing details) rather than just code generation.
*   **The "Dynamic OS" Vision:** The creator describes the concept as combining LLMs, code execution, and a cloud file system to create a workspace that evolves dynamically. One analogy used is a "Notion page webapp" where an LLM turns productivity software into a flexible personal workspace.
*   **Simplifying Development:** The main technical appeal is abstracting away the complex steps of spinning up servers and hosting frontends. The tool allows generated UIs to interact directly with file systems and APIs.
*   **Local Storage Questions:** There is interest in how the system handles data persistence, specifically regarding local file systems versus relational databases, and making web-native apps fully local.

**Why it matters:** This discussion highlights the trend toward **"malleable software"**—where end-users create their own bespoke micro-apps using LLMs, moving beyond static SaaS products toward dynamic, self-generated interfaces.

**One-line takeaway:** A vision for a "dynamic OS" where users generate interactive UIs and personal apps as easily as writing in a doc, skipping the traditional dev-ops hurdles.

### Vargai/SDK – JSX for AI Video. Declarative Programming Language for Claude Code

#### [Submission URL](https://varg.ai/sdk) | 17 points | by [alex_varga](https://news.ycombinator.com/user?id=alex_varga) | [7 comments](https://news.ycombinator.com/item?id=46724675)

Varg: JSX for AI video, now in public beta

What it is
- A TypeScript SDK that lets you compose AI-generated video, image, voice, and music with a declarative JSX syntax (<Clip>, <Image>, <Speech>, <Animate>) and outputs an MP4 via FFmpeg.
- Not React—custom JSX runtime that turns components into render instructions. Designed to be “AI-native,” so LLM agents can write correct code ~95% of the time, with clear runtime errors for the rest.

Why it matters
- Unifies multiple AI providers under one API and makes complex video pipelines composable and cacheable, cutting both iteration time and cost.
- Pairs naturally with agents and automation: the declarative structure maps well to how LLMs “think,” enabling programmatic content generation at scale.

Key features
- Declarative, composable primitives (16 core components) for building everything from simple clips to talking-heads and character-driven scenes.
- Aggressive, content-addressed caching per element (same props = instant hit) that persists across restarts to save API calls.
- Clear, actionable runtime errors (e.g., “Clip duration required… add duration={5} or 'auto'”) and type-safe props to catch mistakes early.
- Works best with Bun (fast installs, native TS); Node.js supported. Requires FFmpeg. Not for the browser.

Ecosystem and support
- Providers: fal.ai (video/image/lipsync), ElevenLabs (voice/music), OpenAI Sora (video), Replicate (many models), Higgsfield (characters). Only add the keys you use.
- Next.js: supported in Server Components, API Routes, and Server Actions; not in Client Components, Edge Runtime, or Vercel Serverless (due to FFmpeg/timeouts). Recommended: separate Bun/Node service for rendering.
- Performance (typical): images 3–5s, voice 2–5s, video 90–180s + 5–30s FFmpeg composition; cached elements <100ms. A 30s video: ~3–5 min first render, ~10s on cache.

Pricing and license
- SDK is free (Apache 2.0). You pay providers directly: roughly $0.01–0.10/image, $0.50–2.00/video; ElevenLabs has a free tier (~$0.30/1K characters). Caching reduces costs.

Positioning
- Different from Remotion: Remotion is React frame-by-frame for precise motion graphics; Varg leans on AI generation + FFmpeg for agent-driven content, talking heads, UGC transformations, and ads.

Getting started
- Bun recommended: bun install varg
- CLI: varg render scene.tsx → outputs MP4
- Ships with real-world templates (mirror selfie, simple portrait, kawaii fruits, talking head) to copy, paste, render.

**Varg: JSX for AI Video Generation**
Varg is a new open-source TypeScript SDK that enables developers to compose AI-generated video, image, voice, and music using a declarative JSX syntax. Rather than using React, it utilizes a custom runtime that translates components like `<Clip>` and `<Speech>` into FFmpeg instructions to output MP4s. Designed to be "AI-native," Varg is built so that LLM agents can write correct video-generation code roughly 95% of the time. The SDK abstracts multiple providers (including Fal.ai, ElevenLabs, and OpenAI) and features aggressive content-addressed caching to minimize API costs during iteration.

**Hacker News Discussion:**

*   **Comparison to Remotion:** Users immediately noted similarities to **Remotion**, though they acknowledged Varg's distinct focus on orchestrating generative AI calls and "making them readable" rather than focusing solely on programmatic motion graphics.
*   **Workflow Preferences:** One product engineer expressed strong support for the SDK approach, noting that they prefer writing code over managing endless, complex workflows in node-based UI tools like ComfyUI.
*   **Cost Concerns:** While sticking the landing on first impressions ("phenomenal," "super cool"), some users voiced hesitation regarding the pricing of the underlying generation providers, stating they had hoped the cost per video/image would be lower.
*   **Documentation for Agents:** There was specific interest in documentation designed for LLMs (such as an `llms.txt`); the creators pointed users toward specific "skills" files in the GitHub repository intended for agent consumption.

