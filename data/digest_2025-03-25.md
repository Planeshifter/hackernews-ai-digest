## AI Submissions for Tue Mar 25 2025 {{ 'date': '2025-03-25T17:14:27.261Z' }}

### Gemini 2.5

#### [Submission URL](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/) | 909 points | by [meetpateltech](https://news.ycombinator.com/user?id=meetpateltech) | [448 comments](https://news.ycombinator.com/item?id=43473489)

Google DeepMind has unveiled its most advanced AI model yet, Gemini 2.5, designed to tackle complex challenges with top-notch reasoning and coding prowess. Outperforming on critical benchmarks, the Gemini 2.5 Pro Experimental model takes the number one spot on the LMArena leaderboard, impressing with its superior reasoning capabilities and precision in coding tasks. This latest iteration builds on previous advances by incorporating improved post-training and a significantly enhanced base model, plus maintaining multimodal understanding with an impressive context window.

For developers eager to explore, Gemini 2.5 is accessible now in Google AI Studio and the Gemini app, with availability on Vertex AI imminent. Offering a glimpse into the future of AI, 2.5 Pro effortlessly tackles math and science benchmarks without expensive test-time techniques and excels in coding by creating robust applications from simple prompts. As users begin to navigate its potential, pricing details will soon roll out for those looking to scale up their AI solutions. As always, feedback is not only encouraged but crucial for the continued evolution of Gemini’s capabilities. Dive into the next era of AI reasoning and problem-solving with Gemini 2.5.

**Summary of Hacker News Discussion:**

The discussion around Google DeepMind’s Gemini 2.5 reveals a mix of cautious optimism and pointed criticism. While users acknowledge advancements in AI capabilities, many express skepticism about the quality and coherence of AI-generated content. Key points include:

1. **Writing Quality Concerns**:  
   - Several users critique AI-generated writing as inconsistent, generic, and lacking narrative depth. An example cited is a fantasy novel chapter produced by Gemini 2.5, which, while grammatically correct, suffers from incoherent plot points, illogical descriptions, and repetitive language (e.g., excessive use of "phosphorescence" and "eldertides").
   - Comparisons to human authors highlight that AI struggles with maintaining nuanced, engaging storytelling and often delivers "mediocre" results, even if improvements over earlier models are noted.

2. **Benchmarks vs. Real-World Use**:  
   - Skepticism exists around whether AI benchmark performance (e.g., coding, math, or LMArena rankings) translates to practical utility. Some argue that metrics like college exams or the Turing Test (mockingly referenced via AI "writing Harry Potter") are poor proxies for real-world intelligence or creativity.

3. **Societal Impact and Progress**:  
   - Debates arise about the societal implications of rapidly advancing AI. While some marvel at monthly progress ("mind-boggling" improvements), others question its tangible benefits, noting customer applications often lag behind hype. References to tools like Claude (playing Pokémon poorly) and Deepseek-R1 highlight divergent performance in specialized tasks.

4. **Optimism Amidst Criticism**:  
   - A subset of users celebrates incremental advancements, suggesting AI could eventually rival human creativity, though likely not soon. Others note that even modest performance gains (e.g., 10-20% improvements) can compound meaningfully over time.

5. **Meta-Critique of AI Evaluation**:  
   - Calls for more nuanced evaluation frameworks beyond benchmarks emerge, emphasizing the gap between technical metrics and human-centric creativity, consistency, and depth.

In summary, the thread reflects a balancing act: admiration for technical strides tempered by realism about AI’s current limitations in producing meaningful, original content and solving complex real-world problems.

### VGGT: Visual Geometry Grounded Transformer

#### [Submission URL](https://github.com/facebookresearch/vggt) | 182 points | by [xnx](https://news.ycombinator.com/user?id=xnx) | [40 comments](https://news.ycombinator.com/item?id=43470651)

In today's Hacker News highlight, we explore an intriguing development from Meta AI and University of Oxford researchers: VGGT, or Visual Geometry Grounded Transformer. This cutting-edge neural network, presented at CVPR 2025, offers a powerful method for inferring essential 3D scene attributes like camera parameters, depth maps, and 3D point tracks—all from just one or multiple views within seconds.

VGGT utilizes a feed-forward neural network architecture to process visual data efficiently. It's designed to work with one to hundreds of views, offering flexibility depending on the complexity and amount of data available. If you're keen to explore VGGT's potential, the project is available on GitHub for you to dive right in; it requires dependencies like PyTorch and Hugging Face Hub for optimal functionality.

Getting started with VGGT is quite simple: by cloning the repository and installing the necessary packages, you can run the model with just a few lines of code. The model weights are downloadable from Hugging Face, ensuring ease of access. VGGT’s design anticipates diverse needs, allowing users to predict specific attributes like cameras and depth maps tailored to their particular scene.

This innovative tech also accommodates detailed customization—whether you're tracking specific points or crafting your own segmentation masks to fine-tune 3D reconstructions. While scene reconstruction is swift, the visualization of 3D points may require extra time due to the external rendering processes involved. However, additional tools such as Gradio offer a user-friendly web interface to showcase VGGT's capabilities with ease.

Finally, for those eager to visualize their data in 3D or track results interactively, multiple visualization tools and a Gradio web interface are ready to enhance your experience. Simply install the demo requirements to unlock these features. If you're looking into advanced computer vision applications, VGGT is definitely worth exploring for its robust capabilities and user-centric design.

The Hacker News discussion on Meta AI and Oxford's **VGGT** highlights several key themes:

1. **Architecture and Efficiency**:  
   Users note VGGT’s use of a standard transformer architecture instead of specialized networks, contrasting it with traditional methods like COLMAP. While praised for speed and accuracy, some question if its "groundbreaking" status is overstated, as it relies on established techniques paired with massive datasets.

2. **The "Bitter Lesson" Debate**:  
   A recurring theme references [The Bitter Lesson](http://www.incompleteideas.net/Inc/Ideas/BitterLesson.html), emphasizing that brute-force scaling (data + compute) often outperforms hand-crafted heuristics. Comparisons are drawn to chess engines like Stockfish NNUE, where neural networks eventually surpassed manual optimizations.

3. **Training Costs**:  
   The model’s training on **64 A100 GPUs for 9 days** (costing ~$18k) sparks discussion about accessibility. Users calculate the GPU-time equivalence (1.5 GPU-years) and ponder whether such resource-heavy methods align with the "bitter lesson" of scalable AI.

4. **Applications and Limitations**:  
   - **Photogrammetry**: Excitement about replacing traditional methods with faster, phone-based 3D scanning. Some suggest it could rival expensive LIDAR systems but note challenges with dynamic scenes or large-scale drift.  
   - **Output Quality**: Mixed results—some users report missing details in point clouds, while others praise its potential for AR, gaming, or even surgical tools.  
   - **Integration**: Questions arise about combining VGGT with Gaussian Splatting for rendering or SLAM techniques for real-time tracking.

5. **Dataset Concerns**:  
   The training data (e.g., Egyptian pyramids, Colosseum) is critiqued as overly "iconic," raising questions about diversity and real-world generalization. Users suggest testing on less curated scenes.

6. **Licensing and Demo**:  
   The model’s **Creative Commons Attribution Commercial** license is noted, alongside a [demo link](https://vgg-t.github.io) for experimentation. Some share Gradio examples, though results vary.

**Key Takeaway**: While VGGT is hailed as a leap forward in speed and simplicity, skepticism remains about its novelty versus scalability trade-offs. The discussion underscores the tension between cutting-edge AI’s potential and its reliance on resource-heavy training—a hallmark of the "bitter lesson" era.

### Optimizing ML training with metagradient descent

#### [Submission URL](https://arxiv.org/abs/2503.13751) | 79 points | by [ladberg](https://news.ycombinator.com/user?id=ladberg) | [13 comments](https://news.ycombinator.com/item?id=43476134)

In a new paper hitting the arXiv, authors Logan Engstrom, Andrew Ilyas, and their team propose a novel approach to optimizing machine learning training with metagradient descent (MGD). Titled "Optimizing ML Training with Metagradient Descent," the study introduces a method for calculating metagradients—gradients through model training—at scale, paving the way for more efficient and effective model training configurations.

The paper also features a "smooth model training" framework, allowing for an optimization process that uses metagradients to enhance traditional techniques. The researchers claim their approach significantly improves upon existing dataset selection methodologies, offers resilience against accuracy-degrading data poisoning, and automates the discovery of competitive learning rate schedules.

This development stands to streamline and enhance the performance of large-scale machine learning models, offering a promising direction for further exploration within the fields of Machine Learning and Artificial Intelligence. You can delve into the full details by accessing the paper through its arXiv link provided in the summary.

**Summary of Hacker News Discussion:**

The discussion revolves around challenges and strategies in hyperparameter tuning for machine learning, prompted by a paper on metagradient descent (MGD). Key points include:

1. **Hyperparameter Tuning Frustrations**:  
   Users highlight the time-consuming, error-prone nature of hyperparameter optimization, with even minor tweaks risking poor performance. Hardware limitations (e.g., memory issues) and the complexity of real-world design decisions exacerbate these challenges.

2. **Tools and Methods**:  
   - **Optuna** and Google’s **Tuning Playbook** are suggested for efficient hyperparameter search.  
   - **Bayesian optimization** is noted for exploring large search spaces but criticized for computational expense.  
   - Practical heuristics (e.g., tracking learning curves) and scaling strategies (extrapolating small-model results to larger systems) are emphasized.  

3. **Traditional vs. Modern ML**:  
   Traditional ML’s reliance on domain knowledge contrasts with modern approaches using "universal approximators." However, the latter still struggles with data quality and the need for synthetic data generation.  

4. **Critiques of the Paper**:  
   A detailed analysis questions the mathematical rigor of optimization algorithms, particularly around convergence guarantees and notation clarity. Concerns include potential pitfalls in gradient-based methods and a noted typo (*Delta_f* notation confusion).  

5. **Related Work**:  
   References to prior research, such as Bengio’s 2016 paper on learning to learn, suggest the MGD approach builds on existing ideas but lacks explicit connections.  

**Key Takeaways**:  
The discussion underscores the tension between theoretical advancements (like MGD) and practical implementation hurdles. While new methods promise efficiency, users stress the importance of robust tooling, heuristics, and understanding problem-specific contexts. Critiques highlight the need for clearer mathematical exposition in research papers.

### Status as a Service (2019)

#### [Submission URL](https://www.eugenewei.com/blog/2019/2/19/status-as-a-service) | 80 points | by [simonebrunozzi](https://news.ycombinator.com/user?id=simonebrunozzi) | [37 comments](https://news.ycombinator.com/item?id=43468666)

In an intriguing new blog post, a writer reflects on the intertwining worlds of social capital and social networks, merging personal anecdotes with insights into human behavior. The piece, titled "Status-Seeking Monkeys," humorously critiques our innate quest for social capital and its underappreciated role in the meteoric rise of social media platforms. 

The author candidly shares their struggle with carpal tunnel syndrome, humorously blaming their hiatus from writing on having to rely on a compact laptop keyboard. This personal touch sets the tone for a broader exploration, blending wit with wisdom.

The essay argues that while financial capital is meticulously measured and analyzed, social capital—often driving the early success of social networks—lacks clear metrics. Despite their financial acumen, many in Silicon Valley overlook how social capital can be a leading indicator of future financial success. The term "Status as a Service" (StaaS) is coined to describe how social networks "sell" status, much like SaaS companies deliver software.

By dissecting social networks' growth strategies and network effects, the writer equates them to the self-reinforcing loops seen in successful SaaS models. They note that understanding such dynamics could illuminate why some networks fade, while others flourish.

The post is an invitation to consider social networks as entities dealing in social capital, emphasizing that our status-driven nature is an essential, yet often ignored, component of digital interaction. It's a thoughtful dive into why people flock to social media—a compelling read for anyone intrigued by the confluence of technology, economy, and human nature.

The Hacker News discussion on the blog post "Status-Seeking Monkeys" reflects a mix of nostalgia, critique, and philosophical debate:

1. **Nostalgia & Tools**: Users reminisce about Google Reader and RSS feeds, lamenting the loss of simplicity in content curation. Some share technical tips, like accessing the blog’s RSS feed, to bypass modern platform algorithms.

2. **Critique of Style**: The post’s verbose, anecdotal approach draws criticism for lacking conciseness and evidence. Critics accuse it of "pseudo-intellectualism," while defenders argue it sparks valuable reflection on social dynamics in tech. A comparison to Monty Python humor lightens the tone.

3. **StaaS Concept Reception**: The "Status as a Service" (StaaS) analogy receives mixed feedback. Some find it insightful for explaining social networks’ growth via status-seeking, while others argue it oversimplifies human behavior. The balance of utility, entertainment, and status is debated as key to platform success.

4. **Tech vs. Sociology**: Skeptics question the article’s relevance to practical tech development, suggesting it rehashes known ideas. Supporters emphasize the importance of integrating sociological perspectives (e.g., referencing Bourdieu) into tech discourse.

5. **AI & Summarization**: Users note the irony of using AI to summarize discussions, acknowledging its utility but stressing the irreplaceable role of human nuance in analysis.

Overall, the thread highlights tensions between tech’s empirical focus and the subjective, status-driven human behaviors that underpin social platforms. While some dismiss the article as medling philosophizing, others appreciate its ambition to bridge these worlds.

### Show HN: Feudle – A daily puzzle game built with AI

#### [Submission URL](https://feudlegame.com) | 45 points | by [papaolivia92](https://news.ycombinator.com/user?id=papaolivia92) | [33 comments](https://news.ycombinator.com/item?id=43471939)

Feudle, the exciting guessing game where your mission is to predict the most popular responses submitted by players, is now online with today's challenge! Test your intuition by guessing which answers were the most popular from yesterday's submissions. The game is a thrilling twist on classic word games where every correct guess appears on the board alongside its response count. But tread carefully—three wrong guesses and your game ends!

After today's puzzling fun, make your mark by submitting answers for tomorrow's challenge. Want to stay updated? Subscribe for a daily email to ensure you never miss a round of Feudle. You can also engage with the growing community, vote on future questions, and track your own game stats—just sign in via Google to preserve your achievements and streaks.

Ready to tackle today's Feudle and share the excitement with others? Dive in, sign up, and experience the challenge that connects you with fellow players worldwide!

The Hacker News discussion about **Feudle** highlights a mix of praise for the game's concept and critical feedback, alongside developer engagement. Key points include:

- **Positive Reception**: Users appreciate the "cool concept" and execution, comparing it to *Family Feud* and enjoying its daily challenge format. Some call it a "smart" and "fun" word game.
  
- **UI/UX Critiques**:  
  - **Annoying Pop-ups**: Multiple users request removing or retiming intrusive pop-ups (e.g., prompting sign-ups) to post-game completion.  
  - **Input Issues**: Frustration with mobile keyboard behavior (lag, disallowed alternative input schemes) and conflicts with browser plugins like Vimium. The developer acknowledges revisiting input timing.  

- **Answer Matching**:  
  - Confusion over answers marked incorrect due to synonym mismatches (e.g., "chicken" vs. "ham"). Suggestions for better synonym handling and specificity levels.  
  - Developer clarifies reliance on OpenAI API to match player submissions with survey responses.  

- **Accessibility & Mobile**: Complaints about poor mobile keyboard integration and accessibility flaws.  

- **AI Integration**: Questions about AI usage lead the developer to explain leveraging OpenAI for prompt generation and answer validation.  

- **Cross-Promotion**: A user shares their own word game (*Acro*), sparking brief discussion about install issues on Pixel devices.  

- **Developer Engagement**: Active responses from the creator (*papaolivia92*) to feedback, promising fixes for pop-ups, input timing, and synonym logic, while encouraging community sign-ups.  

Overall, the thread reflects enthusiasm for Feudle's core idea but highlights areas for refinement, particularly in UX polish and answer flexibility, with the developer actively addressing concerns.

### We chose LangGraph to build our coding agent

#### [Submission URL](https://www.qodo.ai/blog/why-we-chose-langgraph-to-build-our-coding-agent/) | 80 points | by [jimminyx](https://news.ycombinator.com/user?id=jimminyx) | [19 comments](https://news.ycombinator.com/item?id=43468435)

At Qodo, we're taking the world of AI coding assistants to new heights with the help of LangGraph. Since the release of Claude Sonnet 3.5 nine months ago, large language models (LLMs) have transformed how we tackle coding tasks, and we're embracing this by making our agents more dynamic and flexible while maintaining high standards for code quality.

Our journey began with structured flows for tasks like test generation and code reviews that worked well with older models. However, the robustness of newer models encouraged us to transition from rigid frameworks to more adaptive agents. We needed a system that aligned with our opinionated views on AI in coding and could keep pace with rapid advancements in the field. That's where LangGraph came in.

LangGraph allows us to blend flexibility with structure through a graph-based approach, making it easy to create workflows that are adaptable yet opinionated. The framework enables a state-machine architecture, where nodes represent workflow steps and edges dictate transitions. Whether we create sparse graphs for predictable outcomes or dense ones for more autonomy, LangGraph provides the flexibility to fine-tune our flows as models evolve.

Our main workflow exemplifies this flexibility. It starts with a context collector node gathering essential information, followed by a planning node to outline the task, an execution node for code generation, and a validation node to ensure quality. If validation fails, the agent loops back to execution for refinement, ensuring robustness without sacrificing adaptability.

LangGraph's API prioritizes simplicity, making our workflows easy to understand and alter. Each node executes clear functions, enhancing transparency and maintainability. Unlike overly complex frameworks, LangGraph aligns with our thinking and displays agent logic straightforwardly.

Reusable components are another strength of LangGraph. Nodes such as context collectors and validators are integral parts of multiple workflows, reducing redundancy and increasing efficiency. As we continue to develop specialized flows, the framework's modularity already yields significant productivity gains.

In short, LangGraph is a key player in our mission to deliver flexible, opinionated AI coding assistants. Join us in exploring these innovations on our Discord as we continue to push the boundaries of what's possible with AI in coding.

**Submission Summary:**  
Qodo leverages LangGraph to enhance AI coding assistants, transitioning from rigid workflows to dynamic, graph-based agents. Their workflow includes context collection, planning, execution, and validation nodes, with looping for refinement. LangGraph’s simplicity, transparency, and reusable components enable adaptability while maintaining code quality.

**Discussion Summary:**  
1. **Positive Feedback & Use Cases:**  
   - Users highlight PydanticAI’s balance of control/abstraction, with mentions of travel app development using validation and dependency injection.  
   - LangGraph’s graph-based approach (state machines/DAGs) is praised for structuring workflows, with comparisons to HuggingFace’s Smolagents and Temporal for orchestration.  

2. **Critiques of Abstractions:**  
   - Some criticize frameworks like LangChain for "meaningless abstractions," though LangGraph’s state-machine model is seen as useful for flow design.  
   - Concerns about over-engineering vs. lightweight alternatives (e.g., LiteLLM) are noted.  

3. **Technical Insights:**  
   - LangGraph’s state-machine architecture and PregelBSP algorithm enable cyclic workflows and parallelism.  
   - Users recommend subclassing nodes/edges for clarity and debugging simplicity.  

4. **Comparisons & Alternatives:**  
   - Frameworks like LlamaIndex, Temporal, and DBOS are mentioned as alternatives for orchestration.  

5. **Deployment & Community:**  
   - A GitHub template for deploying LangGraph agents with Streamlit UI is shared, emphasizing practical implementation.  

**Key Takeaway:** LangGraph is valued for blending flexibility with structure, though debates persist on abstraction trade-offs. Community contributions and comparisons with tools like PydanticAI and Temporal highlight its role in evolving AI-driven workflows.

### Heavy chatbot usage is correlated with loneliness and reduced socialization

#### [Submission URL](https://www.platformer.news/openai-chatgpt-mental-health-well-being/) | 85 points | by [suvan](https://news.ycombinator.com/user?id=suvan) | [71 comments](https://news.ycombinator.com/item?id=43467681)

In the latest thought-provoking exploration of artificial intelligence and its societal impacts, a New York Times columnist delves into the complex world of chatbots and their burgeoning role in human relationships. With a personal connection through her boyfriend's work at Anthropic and a backdrop involving the Times' ongoing legal battle with OpenAI and Microsoft over copyright issues, this piece provides an intriguing perspective.

As social networks like Instagram and TikTok remain contentious topics regarding their effects on mental health, especially among youth, new questions arise about the future influence of AI chatbots. These virtual companions are more personalized, engaging, and supportive than social media platforms, sparking fresh debates about their potential psychological impacts.

Two groundbreaking studies released by MIT Media Lab and OpenAI have shed light on this emerging issue. By analyzing millions of ChatGPT interactions, researchers have found that, while the majority of users maintain a neutral relationship with the AI, a notable group of "power users" demonstrate worrying signs of increased loneliness and emotional dependence.

Intriguingly, these findings align with previous research suggesting that those who feel isolated are more likely to engage heavily with digital interfaces—first social media, now chatbots. This raises important questions about the role AI companions will play in exacerbating or alleviating loneliness.

While these studies warrant further investigation, platforms like OpenAI are credited for their open publishing policy and proactive research into these issues. As chatbots evolve from mere productivity tools to potential "AI soulmates," the implications for human connection are significant. Developers are urged to consider these psychological impacts carefully.

The discourse around AI's role in personal relationships is expanding rapidly. As AI continues to blur the lines between human and machine interaction, the challenge lies in designing these technologies responsibly to enhance our well-being without compromising genuine human connections.

The Hacker News discussion revolves around the use of AI chatbots like ChatGPT as alternatives to traditional mental health therapy, sparked by a submission highlighting studies on AI's psychological impacts. Key points from the conversation include:

1. **Personal Experiences**:  
   - Users shared **polarizing experiences**: Some found ChatGPT profoundly helpful for managing anxiety, depression, or personality disorders when human therapists failed, praising its nonjudgmental listening and skill-building prompts. Others criticized it as a risky substitute, citing concerns about AI amplifying harmful thought patterns or providing generic advice.

2. **Accessibility vs. Risks**:  
   - **Cost and availability** drove many to AI, especially in regions with scarce mental health resources. However, users warned that LLMs lack professional oversight and cannot replace urgent care, with one comparing AI therapy’s commercialization to Juul’s predatory targeting of vulnerable populations.

3. **Ethical and Practical Concerns**:  
   - Critics emphasized AI’s inability to diagnose accurately, handle crises, or maintain confidentiality. Fears of profit-driven platforms prioritizing engagement over user well-being (“Digital Therapy” marketed like “e-cigarettes”) were noted.  
   - Some debated whether AI’s 24/7 accessibility justifies its use as a stopgap tool versus normalizing reliance on unregulated systems.

4. **Human vs. AI Dynamics**:  
   - While AI was praised for offering immediate, stigma-free support, users acknowledged its limitations in challenging users or providing nuanced guidance. A recurring theme was the **need for balance**—leveraging AI for skill-building while recognizing the irreplaceable role of human empathy and professional therapy.

5. **Systemic Critiques**:  
   - Participants critiqued broken mental health systems, citing unaffordable care, dismissive professionals, and institutional failures. This context underpinned both the desperation driving AI adoption and calls for systemic reform.

In summary, the discussion reflects cautious optimism about AI’s potential alongside urgent warnings about its risks, advocating for responsible integration rather than replacement of human care.

### AI bots are destroying Open Access

#### [Submission URL](https://go-to-hellman.blogspot.com/2025/03/ai-bots-are-destroying-open-access.html) | 96 points | by [dhacks](https://news.ycombinator.com/user?id=dhacks) | [37 comments](https://news.ycombinator.com/item?id=43474196)

In a recent Go To Hellman blog post, the Internet is depicted as battleground, where AI companies aggressively consume valuable open-access resources intended for public benefit. These firms, with their endless appetite for training data for Large Language Models (LLMs), threaten the very existence of platforms striving to make quality information readily available on the web, such as libraries and scholarly publishers.

The focus of their voracious data grab? Rich, organized, and unbiased datasets—characteristics inherent to the very essence of open-access sites. Unlike old-school bots that roamed the internet with certain civility, respecting robot exclusions and limiting server requests, today’s bots resemble swarms of locusts, ruthless and unyielding. They indiscriminately drain server resources, bringing sites like MIT Press, OAPEN, and Project Gutenberg to temporary standstills.

Despite deployments of commercial services like Cloudflare to fend off these bot surges, the struggle is relentless and resource-intensive. For instance, OAPEN was inundated by AI-induced traffic, alienating actual users and rendering thousands of scholarly resources temporarily inaccessible.

The sheer waste of developer employment hours on countermeasures against such malicious activities detracts from innovation, impeding advancements that could have been achieved otherwise. Additionally, exacerbating this dire digital landscape is the irony: many open-access platforms offer convenient API and feeds for data access, negating the need for such brute-force scraping altogether.

The situation poses a poignant question: what responsible use-look like for the companies fueling these unrelenting AI agents, and how can we balance the scales to safeguard the invaluable resources that underpin our global intellectual heritage?

The discussion on Hacker News revolves around AI-driven bots aggressively scraping open-access websites, causing strain on resources and ethical concerns. Key points include:  

### 1. **Bot Behavior and Disregard for Norms**  
   - Modern AI bots ignore traditional safeguards like `robots.txt` and `nofollow` directives (intended to guide crawlers), unlike earlier generations of bots that respected these rules.  
   - AI crawlers often mimic DDoS attacks, overwhelming servers with high-volume, distributed requests, leading to downtime for platforms like MIT Press and OAPEN.  

### 2. **Techniques and Evasion**  
   - AI companies use residential proxies, browser extensions, and malware-like tactics to mask scraping activities, bypassing IP blocks and rate-limiting.  
   - Examples include OpenAI’s alleged use of browser extensions to monitor traffic and services selling residential IP addresses to evade detection.  

### 3. **Defensive Challenges**  
   - Traditional defenses (CAPTCHAs, IP blocking) struggle against distributed, AI-driven bots. Tools like Cloudflare offer temporary relief but are not foolproof.  
   - Smaller sites lack resources to implement robust defenses, diverting developer time from innovation to bot mitigation.  

### 4. **Ethical and Systemic Concerns**  
   - Financial incentives drive companies to prioritize data extraction over ethical scraping, undermining open-access missions.  
   - BitTorrent is cited as a cooperative alternative, rewarding contributors and resisting centralized disruption, contrasting with AI’s exploitative model.  

### 5. **Solutions Proposed**  
   - **Technical**: Stricter API usage, JavaScript/CSS requirements (controversial for accessibility), and semantic web standards to structure data for fair access.  
   - **Legal**: Addressing SDKs/malware enabling botnets and blocking services facilitating abusive scraping.  
   - **Cultural**: Advocating for ethical guidelines and systemic shifts to prioritize sustainable, community-driven data access over profit-driven extraction.  

### 6. **Irony and Frustration**  
   - Many open-access platforms already offer APIs or feeds, rendering brute-force scraping unnecessary. The situation highlights contradictions in AI development’s reliance on public resources while undermining their viability.  

Overall, the discussion underscores tensions between AI’s hunger for data and the need to preserve open-access ecosystems, calling for more responsible practices and systemic reforms.

### Angelina Jolie Was Right About Computers

#### [Submission URL](https://www.wired.com/story/angelina-jolie-was-right-about-risc-architecture/) | 15 points | by [vdupras](https://news.ycombinator.com/user?id=vdupras) | [6 comments](https://news.ycombinator.com/item?id=43470248)

In an unexpected twist of cinematic prophecy, Angelina Jolie’s character in the 1995 film *Hackers* predicted the future of computer architecture. During a scene with Jonny Lee Miller, she mentions RISC (Reduced Instruction Set Computer) architecture, stating it would "change everything." Fast-forward 28 years, and it turns out she wasn't wrong. RISC architecture, particularly in the form of RISC-V, is at the forefront of technological innovation, influencing everything from cars to AI systems.

Despite its growing influence, the RISC-V community is grappling with a peculiar challenge—they're faced with an industry that benefits immensely from their architecture but remains mostly oblivious to its intricacies. This was evident at the annual RISC-V summit in Santa Clara, where activists of this burgeoning tech movement mingled, exchanging complex jargon unfamiliar to the outside world.

Enter Calista Redmond, CEO of RISC-V International, who is zealously ensuring that the organization's efforts not only propel technological advancements but also maintain relevance amid geopolitical tensions. Her confident address at the summit echoed Hackers' prescience, as she declared RISC-V's transformative impact on modern technology, making it a perfect testament to Jolie’s on-screen assertion.

However, as Redmond points out, the challenge remains that while RISC architecture is revolutionizing tech infrastructure, the average consumer is more concerned with functionality and cost-efficiency than the underlying architecture. As WIRED’s Jason says, it’s an untold story eagerly waiting to be shared, akin to a real-life, ongoing hacker saga that too few recognize for its futuristic foresight and current implications.

Thus, while the movie *Hackers* may have been ahead of its time, the tech it predicted is undeniably shaping our world today, even if unnoticed by many. The narrative around RISC and its promising future is unfolding quietly but distinctly, thanks in part to visionaries within the community and cinematic winks from the past.

The discussion revolves around the intersection of *Hackers*' prescient portrayal of RISC architecture and its real-world impact today, with several key points:  

1. **AI and Media**: Users note the irony of WIRED (owned by Condé Nast) covering RISC-V while its parent company uses OpenAI tools like ChatGPT for content. This sparks debate about AI's role in tech journalism and whether it aligns with the "hacker ethos" depicted in the film.  

2. **Industry Moves**: Commenters highlight leadership shifts, including Calista Redmond (RISC-V International CEO) and her prior role at Nvidia, alongside nods to executives like Doug Bowser (Nintendo) and Carol Surface (ex-Apple). These figures symbolize RISC-V’s growing influence across tech sectors.  

3. **Technical Nitpicking**: A user humorously critiques the *Hackers* scene, pointing out technical inaccuracies (e.g., PCI bus references, graphics performance) but acknowledges its symbolic foresight about rapid hardware innovation. David Patterson, a RISC pioneer, is mentioned as someone who might appreciate the film’s legacy despite its flaws.  

4. **Meta Commentary**: The discussion includes a link to an archived article (likely the WIRED piece) and a joke about paywalls, underscoring frustrations with access to tech journalism even as RISC-V’s story remains underrecognized.  

In essence, the thread blends nostalgia for *Hackers* with reflections on RISC-V’s quiet revolution, corporate dynamics, and the evolving role of AI in storytelling—all while poking fun at the quirks of tech culture.

### Devs say AI crawlers dominate traffic, forcing blocks on entire countries

#### [Submission URL](https://arstechnica.com/ai/2025/03/devs-say-ai-crawlers-dominate-traffic-forcing-blocks-on-entire-countries/) | 341 points | by [LinuxBender](https://news.ycombinator.com/user?id=LinuxBender) | [249 comments](https://news.ycombinator.com/item?id=43476337)

In a compelling narrative that has the tech community buzzing, software developer Xe Iaso shared their struggle against relentless AI crawlers wreaking havoc on their Git repository service. Despite deploying standard defenses like robots.txt and blocking certain user-agents, Iaso found these bots sidestepping every measure, leading to repeated service disruptions. Frustrated, Iaso introduced "Anubis," a unique proof-of-work system that challenges browsers to solve computational puzzles before accessing the site.

Unfortunately, Iaso’s battle is not isolated. Their plight sheds light on a broader crisis within the open source community, which is being inundated with bot traffic that mimics distributed denial-of-service (DDoS) attacks. An alarming report by LibreNews highlighted that some projects see upwards of 97% of their traffic from AI company bots, causing service instability, driving up bandwidth costs, and taxing already overburdened maintainers. 

Projects like Fedora Pagure and GNOME have taken extreme measures, such as blocking whole countries or adopting Iaso's Anubis system. However, these solutions sometimes lead to significant delays for legitimate users, frustrating those who face up to two-minute waits to access shared links.

Open source initiatives, crucial for public collaboration and operated with limited resources, now grapple with AI crawlers that flagrantly ignore standard web protocols, evade detection, and sap both technical and financial resources. Martin Owens of the Inkscape project lamented this "prodigious block list" caused by AI companies disrespecting existing guidelines.

Commentary on Hacker News about Iaso's fight reflects widespread developer anger towards what many see as predatory practices by AI firms, indifferent to the goodwill essential in open source environments. This tension is exacerbated as some open source projects handle costly AI-generated traffic that surfloads system resources, making it particularly brutal for repositories like SourceHut and the Curl project.

The grim reality is that these AI crawls not only drain financial resources but also waste developers' time with fake bug reports, revealing a growing disconnect between AI companies and the community-driven ethos of open source development. As the community seeks solutions, a call for more ethical behavior and respect from AI companies is growing louder.

The Hacker News discussion surrounding Xe Iaso's battle with AI crawlers reflects a mix of technical debate, shared frustration, and creative solutions. Key themes include:

1. **Bypassing Traditional Defenses**: Users noted AI crawlers easily circumvent standard tools like `robots.txt` and user-agent blocking (e.g., Huawei’s PetalBot ignoring restrictions). Some suggested using invisible hyperlinks to trap bots, though concerns about accessibility for screen readers were raised.

2. **Rate-Limiting Challenges**: Strategies like IP-based rate-limiting were criticized for being impractical, as AI traffic often mimics human patterns (e.g., low requests per IP). Blocking entire IP ranges (e.g., Huawei’s Singapore mobile network) was shared as a drastic measure, but risks overblocking legitimate users.

3. **Proof-of-Work Systems**: Anubis, a proof-of-work system requiring computational puzzles for access, was highlighted as a novel deterrent. While praised for slowing bots, concerns about its impact on user experience (e.g., delays) and client-side processing overhead were discussed. Alternatives like bcrypt-style hashing or encrypted content layers were also proposed.

4. **Ethical and Resource Strain**: Many lamented AI companies’ disregard for open-source community norms, as their crawlers drain resources (bandwidth, maintenance time) and generate fake issues. Some endorsed “data poisoning” to sabotage AI training sets as a retaliatory measure.

5. **Technical Workarounds**: Users shared mitigations like using Cloudflare, caching, or serving static content to reduce server load. However, these were seen as partial fixes, with calls for AI firms to adopt ethical scraping practices.

6. **Off-Topic Divergence**: A flagged thread veered into debates about anime’s societal impact, illustrating how discussions occasionally strayed from the core issue.

The overarching sentiment was frustration with AI companies exploiting open-source ecosystems, coupled with a demand for balanced, community-respecting solutions.

