## AI Submissions for Tue Jun 20 2023 {{ 'date': '2023-06-20T17:12:58.204Z' }}

### RoboCat – A Self-Improving Robotic Agent

#### [Submission URL](https://www.deepmind.com/blog/robocat-a-self-improving-robotic-agent) | 168 points | by [l1n](https://news.ycombinator.com/user?id=l1n) | [65 comments](https://news.ycombinator.com/item?id=36406139)

DeepMind has created an AI agent called RoboCat, that learns to operate different robotic arms and perform a variety of tasks across different robots and self-generates new training data to improve its technique. The agent uses a multimodal model called Gato, which can process language, images, and actions in both simulated and physical environments. RoboCat learns much faster than other state-of-the-art models, as it can pick up a new task with as few as 100 demonstrations, reducing the need for human-supervised training. This is an important step towards creating a general-purpose robot.

DeepMind has developed an AI agent called RoboCat, which can operate different robotic arms and perform various tasks across multiple robots and environments. The agent uses a multimodal model called Gato that can process language, images, and actions in simulated and physical environments. RoboCat can learn new tasks with as few as 100 demonstrations, reducing the need for human-supervised training. Commenters discussed the high costs of building robotic platforms and the difficulties of controlling motors using torque. There was also a debate about the limitations of UI automation tools and the hostility towards the automation sector.

### Building a Slack/Discord alternative with Tauri/Rust

#### [Submission URL](https://www.linen.dev/s/linen/t/12647025/building-a-slack-discord-alternative-with-tauri-rust) | 308 points | by [cheeseblubber](https://news.ycombinator.com/user?id=cheeseblubber) | [221 comments](https://news.ycombinator.com/item?id=36408633)

Linen, a search engine friendly alternative to Slack, has launched its Mac and Windows desktop clients using Tauri, a Rust-based Electron alternative. Tauri promises to yield smaller, more performant, and more secure desktop clients compared to Electron. While Tauri uses a Webview instead of chromium, Linen ran into compatibility challenges with NextJS and had to refactor all their code out of NextJS to a separate package to enable reuse between the NextJS app and the single page app Tauri built. Nonetheless, Linen was able to get a smooth developer experience on Tauri for the desktop, though it faced various challenges such as customizing the header and notification callbacks, among others. The Linux client suffered issues with fonts and emoji not building properly.

The submission is about Linen, a search engine-friendly alternative to Slack that has launched its Mac and Windows desktop clients using Tauri, a Rust-based Electron alternative. The discussion in the comments is centered around the comparison between Electron and Tauri, with some users defending Electron and stating that it works perfectly for them while others agree that Tauri is a more performant and secure alternative. The discussion also touches on the performance and resource usage of Electron versus other alternatives, as well as the issue of the increasing memory usage of modern software. Some users argue that the majority of people care about productivity and fast, snappy programs rather than high school dropouts, and that today's software runs worse on modern hardware while others disagree and say that today's engineers focus on UX features rather than actual performance.

### Predicting hit songs with 97% accuracy

#### [Submission URL](https://www.frontiersin.org/articles/10.3389/frai.2023.1154663/full) | 96 points | by [geox](https://news.ycombinator.com/user?id=geox) | [57 comments](https://news.ycombinator.com/item?id=36403334)

A team of researchers from Claremont Graduate University and Immersion Neuroscience has developed a machine learning model that can predict hit songs based on neurophysiological responses. Traditional methods of identifying hit songs involve measuring song elements from large databases, but the team took a different approach by measuring responses to a set of songs provided by a streaming music service. They found that a linear statistical model using two neural measures identified hits with 69% accuracy, while a machine learning model classified hit songs with 97% accuracy. The results demonstrate that applying machine learning to neural data can substantially increase classification accuracy for difficult-to-predict market outcomes.

A team of researchers has developed a machine learning model that can predict hit songs based on neurophysiological responses, with a 97% accuracy rate. The approach was different from traditional methods, which involve measuring song elements from large databases, as they measured responses to a set of songs provided by a streaming music service. However, some commenters pointed out problems with the small sample size and lack of diversity in songs used for the study, making the results less reliable. Others discussed the importance of properly validating synthetic datasets and the limitations of evaluation metrics like accuracy in imbalanced datasets. Finally, there was a discussion on the music industry and how difficult it is for independent artists and music genres outside of the mainstream to be represented on Billboard's charts.

### vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention

#### [Submission URL](https://vllm.ai/) | 275 points | by [wskwon](https://news.ycombinator.com/user?id=wskwon) | [40 comments](https://news.ycombinator.com/item?id=36409082)

Today, the UC Berkeley team introduced vLLM, an open-source library for fast LLM inference and serving. This library promises to serve LLM models fast, cheaply, and efficiently, even for small research teams with limited compute resources. The team achieved up to 24x higher throughput than HuggingFace Transformers, the most popular LLM library, and up to 3.5x higher throughput than the previous state of the art, HuggingFace Text Generation Inference (TGI), in their experiments. This breakthrough was due to the library's cutting-edge attention algorithm, PagedAttention, which allows for efficient management of the large and dynamic key-value cache necessary for autoregressive decoding. PagedAttention also enabled efficient memory sharing, making complex sampling algorithms practical for LLM services. The library is already deployed at Chatbot Arena and Vicuna Demo and is available on GitHub.

The UC Berkeley team has introduced vLLM, a new open-source library for fast LLM inference and serving. This library offers fast and efficient serving of LLM models, especially for small research teams with limited compute resources, with up to 24x higher throughput than HuggingFace Transformers and up to 3.5x higher throughput than previous state-of-the-art libraries. The discussion in the comments includes technical details about the library's attention algorithm, PagedAttention, and its memory optimization techniques. Some users express interest in applying the library to their own projects, while others speculate about its practical use cases and potential limitations. Overall, the community is impressed with the vLLM library's capabilities and looks forward to further developments in the field.

### Representing Enums in PostgreSQL

#### [Submission URL](https://making.close.com/posts/native-enums-or-check-constraints-in-postgresql) | 109 points | by [wojcikstefan](https://news.ycombinator.com/user?id=wojcikstefan) | [71 comments](https://news.ycombinator.com/item?id=36403087)

In a recent discussion, the Close team debated whether to use native enums or string columns with CHECK constraints in PostgreSQL. They ultimately decided on the latter due to the limitations of enums (e.g., difficulty removing values) and the flexibility of CHECK constraints. String columns with CHECK constraints lend themselves well to enforcing data correctness in the database and make updating constraints in complex cases more manageable. However, this approach can be less space-efficient as the actual values are stored in the tuples themselves. The alternative of using native enums also has options for updating constraints without locking, but they are too complicated for most use cases and carry a higher risk of corrupting the database.

The Close team discussed whether to use native enums or string columns with CHECK constraints in PostgreSQL. They ultimately chose the latter due to the limitations of enums and the flexibility of CHECK constraints. However, the approach of using string columns with CHECK constraints can be less space-efficient. The discussion included suggestions on other methods to enforce data correctness in the database, such as using PostgreSQL domains or creating a custom CAST. There were debates on the advantages of using short string names versus full descriptions for naming database components. Some individuals recommended using natural keys while others advocated for surrogate keys. Finally, there was a discussion on the flexibility of SQLAlchemy to support various mapping options.

### Show HN: Autolabel, a Python library to label and enrich text data with LLMs

#### [Submission URL](https://github.com/refuel-ai/autolabel) | 143 points | by [nihit-desai](https://news.ycombinator.com/user?id=nihit-desai) | [17 comments](https://news.ycombinator.com/item?id=36409201)

Refuel AI has released a new open-source library called AutoLabel that allows the user to label and clean text datasets with large language models (LLMs). The library supports all GPT-3.5 and GPT-4 models, and users can easily specify labeling guidelines and model parameters in a JSON config. AutoLabel promises to save time and money compared to manual labeling efforts and provides a simple three-step process for labeling data. The library is MIT-licensed and available on GitHub.

Refuel AI has released an open-source library called AutoLabel that can label and clean text datasets with Large Language Models (LLMs). It supports all GPT-3.5 and GPT-4 models and users can specify labeling guidelines and model parameters in a JSON config. The main concerns are around privacy when using LLMs and potential inaccuracies in labeling. Some commenters recommend using self-hosted open-source LLMs or the openAI API, while others suggest that AutoLabel could be integrated with function calling to improve the labeling quality. Additionally, a commenter points out that Refuel provides confidence scores for LLMs but does not provide token-level probabilities. The original post on Hacker News linked to a benchmarking report and a GitHub repository. Some commenters note that the post may be self-promotion and not intended to share LLM labeling feedback with the community.

### Rivian embraces Tesla's charging standard for EVs

#### [Submission URL](https://ev-edition.com/2023/06/rivian-joins-forces-with-tesla-embracing-their-charging-standard-for-electric-vehicles/) | 364 points | by [belltaco](https://news.ycombinator.com/user?id=belltaco) | [718 comments](https://news.ycombinator.com/item?id=36403494)

Electric vehicle manufacturer Rivian has announced plans to adopt Tesla's charging standard and provide its customers with access to Tesla's Supercharger network of approximately 12,000 fast chargers across the US and Canada. Rivian customers will be able to use Tesla Superchargers with the help of adapters from spring 2024 and Tesla-style charging ports will be introduced as standard on Rivian vehicles from 2025. The collaboration supports Tesla's recent efforts to establish its charging standard as the industry norm and will help alleviate concerns over range anxiety among customers. The agreement also provides Tesla with the opportunity to supply power to a wider group of electric vehicle drivers.

Rivian, an electric vehicle manufacturer, recently announced that it will use Tesla's charging standard and give its customers access to over 12,000 Superchargers across Canada and the US, which will help reduce drivers' range anxiety. From spring 2024, Rivian customers will be able to use Tesla Superchargers with the help of adapters, and Tesla-style charging ports will become a standard feature on Rivian vehicles from 2025. This agreement provides Tesla with the chance to offer power to more electric vehicle owners and supports its recent efforts to establish its charging standard as the industry norm. In the comments, users discussed the advantages and disadvantages of using different charging standards, including Tesla's Superchargers and the Combined Charging System (CCS), which is commonly used in Europe. They also shared their thoughts on the practical difficulties of retrofitting EV chargers and providing homes with three-phase power in different parts of the world.

### I made a smooth shadows generator

#### [Submission URL](https://smoothshadows.com/) | 223 points | by [marttilaine](https://news.ycombinator.com/user?id=marttilaine) | [46 comments](https://news.ycombinator.com/item?id=36401676)

As an AI language model, I can certainly help you summarize the top stories on Hacker News. However, please note that I cannot guarantee 100% accuracy or perfect engagement. Is that okay with you?

The top story on Hacker News is about a CSS generator that modifies the shadows and borders of an element. One user suggests copying the CSS from Figma layers, while another mentions that copying Figma buttons can also change the background color. Some users suggest profiling CSS rendering times, while others mention the importance of designing shadows in the context of modern UX. Another user recommends adding resizable box shadows. Some users comment on the difficulties of creating perfect shadows with CSS, but recommend libraries like Tailwind CSS. A few users mention issues with copying CSS to the clipboard. Finally, one user remarks on the impact of the different layers of shadows on the element and others share their experience with using the generator.

### Emerging architectures for LLM applications

#### [Submission URL](https://a16z.com/2023/06/20/emerging-architectures-for-llm-applications/) | 239 points | by [makaimc](https://news.ycombinator.com/user?id=makaimc) | [86 comments](https://news.ycombinator.com/item?id=36409489)

this post, we introduce a reference architecture for the emerging Large Language Models (LLMs) application stack. The stack is a collection of systems, tools, and design patterns that can be used to build applications with LLMs, a powerful new primitive for building software. The design pattern we're looking at in-depth here is called in-context learning, which involves controlling LLM behavior through clever prompting and conditioning on private contextual data. This design pattern is easy to use and outperforms fine-tuning for relatively small datasets. The stack is early and will likely change, but we hope it will be useful for developers working with LLMs now.

The submission introduces a reference architecture for Large Language Models (LLMs) application stack, focusing on in-context learning design pattern. The discussion mainly revolves around how general adversarial networks have yet to work. Other topics discussed include vector databases such as Milvus, Haystack's community, and Postgres' performance in vector data. Participants also talk about the complexity of LLMs, reliability and reproducibility, technical debt, and the challenges of performance monitoring. Finally, a member clarifies the definition of in-context learning and mentions the uniqueness only lies in demonstrations beyond embedding-based retrieval.

### Petaflops to the People: From Personal Compute Cluster to Person of Compute

#### [Submission URL](https://www.latent.space/p/geohot) | 66 points | by [tosh](https://news.ycombinator.com/user?id=tosh) | [10 comments](https://news.ycombinator.com/item?id=36407269)

In the latest episode of Latent Space: The AI Engineer Podcast, host Jade Le and George Hotz of the tiny corp discuss the company's efforts to take on major players like Nvidia, Google, and PyTorch, as well as its recent announcement of the tinybox, a luxury AI computer aimed at local model training and inference. They delve into the technical details of the deep learning framework tinygrad and the importance of optimizing instruction execution for GPU. Additionally, they touch on the potential role of personal compute clusters in the future of home intelligence.

The submission is about the recent episode of the AI Engineer Podcast, where George Hotz of the Tiny Corp talks about the company's efforts to take on major players like Nvidia, Google, and PyTorch, as well as its recent announcement of the tinybox, a luxury AI computer aimed at local model training and inference. The discussion includes technical details of the deep learning framework, tinygrad and the importance of optimizing instruction execution for GPU. They also touch on the potential role of personal compute clusters in the future of home intelligence. In the comments, there is a detailed discussion about quantization research benchmarks, perplexity testing, Fabrice Bellard's methods, and the challenges with FPGA systems. Additionally, some users express surprise at Hotz's knowledge and fluency in discussing technical details, while others criticize the half-baked implementation of some of his ideas.

### Google warns its own employees: Do not use code generated by Bard

#### [Submission URL](https://www.theregister.com/2023/06/19/even_google_warns_its_own/) | 299 points | by [Brajeshwar](https://news.ycombinator.com/user?id=Brajeshwar) | [152 comments](https://news.ycombinator.com/item?id=36399021)

Google has warned its employees to avoid using code generated by its AI chatbot, Bard, due to privacy and security risks. The move has raised concerns about the suitability and reliability of privately developed AI tools. Developers may not be able to trust AI tools if even the creators themselves don't use them. The search and ads company advised its staff not to disclose confidential information or use code generated by Bard due to potential buggy programs or difficult-to-fix software. Meanwhile, voice recognition software developer Nuance, backed by Microsoft, has been sued by three people for allegedly recording and using people's voices without permission in violation of the California Invasion of Privacy Act.

Google has warned their employees to avoid using code generated by their AI chatbot, Bard, due to potential privacy and security risks. This move has raised concerns about the suitability and reliability of privately developed AI tools. Developers may not be able to trust AI tools if even the creators themselves don't use them. Meanwhile, voice recognition software developer Nuance, backed by Microsoft, has been sued by three people for allegedly recording and using people's voices without permission in violation of the California Invasion of Privacy Act. The comments on Hacker News discussed the risks and implications of monorepos, proprietary data privacy, legal liabilities, the legality of LLMs, and the potential impact on productivity. There were also concerns about copyright infringement and the need for companies to address and mitigate systemic risks. Overall, there were mixed views on the topic, with some emphasizing the need for caution and others stressing the importance of innovation and productivity.

### Security analysis of the Dominion ImageCast X

#### [Submission URL](https://freedom-to-tinker.com/2023/06/14/security-analysis-of-the-dominion-imagecast-x/) | 49 points | by [emmelaich](https://news.ycombinator.com/user?id=emmelaich) | [56 comments](https://news.ycombinator.com/item?id=36399194)

A report by J. Alex Halderman, a professor of Computer Science and Engineering at the University of Michigan, was unsealed by a US district court for the Northern District of Georgia. The report details how Dominion voting equipment used in Georgia and other states was found to have numerous security flaws, including an arbitrary-code-execution vulnerability that can be exploited to spread malware from a county’s central election management system to every BMD in the jurisdiction. The report recommends further enhancements to the software engineering, testing, and certification processes for US voting equipment, and rigorous post-election audits of every major electoral contest. Despite the public release of the report, Georgia Secretary of State, Brad Raffensperger, announced that the state will not install Dominion’s security update until after the 2024 Presidential election.

A report by J. Alex Halderman, a professor of Computer Science and Engineering at the University of Michigan, stated that Dominion voting equipment used in Georgia and other states was found to have numerous security flaws, including an arbitrary-code-execution vulnerability that can be exploited to spread malware from a county’s central election management system to every ballot-marking device (BMD) in the jurisdiction. The report recommends further enhancements to the software engineering, testing, and certification processes for US voting equipment, and rigorous post-election audits of every major electoral contest. Despite the release of the report, Georgia Secretary of State Brad Raffensperger has announced that the state will not install Dominion’s security update until after the 2024 Presidential election. In the discussion, some users express their pessimism about the security of voting machines and suggest implementing the Australian Ballot system, while others emphasize the importance of independent verification and transparency for a trusted political process. Some users believe that the risk of election fraud due to the flaws in voting machines is overstated and that the main risk is human error.

### Compilers for the Future

#### [Submission URL](https://adam-mcdaniel-blog.github.io/compilers-for-the-future/) | 78 points | by [adamthekiwi](https://news.ycombinator.com/user?id=adamthekiwi) | [48 comments](https://news.ycombinator.com/item?id=36402390)

future-proof compilers that ensure the longevity of their language. To achieve this, the proposed architecture for writing future-proof compilers involves using a minimal set of architecture-dependent primitives. This allows the language to be easily ported to different architectures without the need for major changes. Additionally, the compiler author should ensure that the language is expressive enough to represent the logic of real-world problems and implement any algorithm in the smallest possible time complexity. By balancing the complexity of the language and the source code, a compiler author can create an immortal language that will stand the test of time.

The submission proposes a method for writing future-proof compilers that ensure the longevity of their language by using a minimal set of architecture-dependent primitives. The discussion touches on various aspects, including the idea that languages should be designed to adapt to changing circumstances, the importance of balance between language and source code complexity to create an immortal language, and the role of Turing machines and control flow in computer science. The discussion also includes examples of how certain languages and programs have stood the test of time and the advantages and disadvantages of various programming approaches. Additionally, there is a discussion about program synthesis and generating code that behaves in similar or better ways to existing implementations, including examples of different research directions in the area.

### Stackoverflow is investing into baking GenAI

#### [Submission URL](https://stackoverflow.co/labs/) | 86 points | by [rounakdatta](https://news.ycombinator.com/user?id=rounakdatta) | [101 comments](https://news.ycombinator.com/item?id=36404743)

Stack Overflow Labs, the experimental arm of the popular Q&A website for developers, has been busy exploring the use of AI to improve the platform and developer experience. Some of the projects include: Experiment Question Formatting Assistant, which uses AI to improve the quality and format of questions to make reviewing easier; Title Suggestions, which utilizes AI to generate more descriptive and accurate question titles; and Chat Decipher, which can extract question topics from chat transcripts and group similar ones together. Additionally, the results of Stack Overflow's 2023 Developer Survey shed light on how developers and technologists feel about AI/ML. The website's Senior Data Scientist also delves into the creation of their course recommendation engine powered by AI. Meanwhile, their CEO will be making exciting announcements at the upcoming WeAreDevelopers event. With their mission to give technologists more time to create amazing things and make the coding field accessible to all, Stack Overflow Labs is always looking for new ways to merge emerging technologies with their platforms and services.

Stack Overflow Labs has been experimenting with AI to enhance the platform and developer experience. Some of its AI projects include an AI-powered question formatting assistant and title suggestion tool. Stack Overflow conducted a developer survey, revealing how developers feel about AI and ML. The CEO also announced exciting updates at the upcoming WeAreDevelopers event. There has been a discussion in the comments about the hype around blockchain and AI, and how they may be misunderstood and overhyped. There was also a discussion about the effectiveness of Stack Overflow's AI experiments, with some commenters offering criticism and others defending the platform's efforts to utilize AI.

### Show HN: Smolex – A code retrieval ChatGPT Plugin for Python

#### [Submission URL](https://github.com/loladotdev/smolex) | 14 points | by [Livioso](https://news.ycombinator.com/user?id=Livioso) | [4 comments](https://news.ycombinator.com/item?id=36401719)

Smolex is an experimental code retrieval ChatGPT plugin that allows for more natural conversations about code. The plugin creates embeddings for all code in the codebase and stores them locally. It also AST parses all code and stores that in a local SQLite database. When a user asks a question, Smolex looks up the requested code in the database and either returns the entire code or a summary of the code. If there is no match in the database, it uses the vector store to find the code (or interface) that might be most relevant to the question. Smolex is Python only but can extend it to other languages.

The first comment by user "lgs" suggests that they have not had time to try out Smolex yet but may give it a try in the future. The comment also mentions a requirement file that includes a reference to "llama_index". Another user, "Livioso", thanks "lgs" for their comment.

The second comment by user "znhd" expresses optimism about Smolex and suggests that it would be interesting to see it as a VSCode extension. Livioso responds by saying that Smolex is currently a temporary solution until GitHub Copilot Chat becomes publicly available.

### Tesla hacker discovers secret ‘Elon Mode’ for hands-free Full Self-Driving

#### [Submission URL](https://www.theverge.com/2023/6/20/23767041/tesla-hacker-elon-mode-hands-free-full-self-driving-autopilot) | 36 points | by [CharlesW](https://news.ycombinator.com/user?id=CharlesW) | [5 comments](https://news.ycombinator.com/item?id=36412170)

A hacker known as @greentheonly has found a "supersecret driver mode" in Tesla vehicles that allows for hands-free driving. The feature, called "Elon Mode," seems to allow Tesla vehicles with Full Self-Driving to operate without any driver monitoring. While FSD is currently in beta, it's available to anyone who has paid up to $15,000 for the option. The hacker found that the car didn't require any attention while FSD was active, which is different from Tesla's Autopilot system, their first-generation driver-assist system for highways, which requires the driver to confirm they are paying attention by nudging the steering wheel.

A hacker named @greentheonly has discovered a "supersecret driver mode" in Tesla vehicles, which allows for hands-free driving and is known as "Elon mode." The hacker found that the car did not require any attention while Full Self-Driving was active, which is different from Tesla's Autopilot system. Some commenters are concerned about the safety of hands-free driving and suggest that the NHTSA should intervene. Others express their admiration for the hacker's work and share their car collections.

### GPS alternative taps cosmic rays for underground or underwater navigation

#### [Submission URL](https://newatlas.com/technology/gps-alternative-muon-cosmic-rays-underground-underwater-navigation/) | 49 points | by [wjSgoWPm5bWAhXB](https://news.ycombinator.com/user?id=wjSgoWPm5bWAhXB) | [10 comments](https://news.ycombinator.com/item?id=36402086)

Researchers at the University of Tokyo have developed a proof-of-concept navigation system called the muometric wireless navigation system (MuWNS) that uses cosmic rays to track movement underground and underwater with precision of a few metres. Unlike GPS, which bounces signals off rocks, walls, and water, MuWNS tracks particles called muons that pass through solid materials. The particles are generated when cosmic rays enter the Earth’s atmosphere and produce a cascade of secondary particles. By tracking the paths of muons picked up by reference stations and a handheld detector, the scientists were able to trace the scientist's position with a high degree of precision deep inside a multi-story building.

The discussion on this submission revolves around the practicality and potential use cases of muon-based navigation systems. One user points out that the technology is currently expensive and difficult to implement in real-time applications. Another user questions the need for real-time tracking in underground or underwater scenarios and suggests that a stable recording receiver may be more practical. The potential of using the technology in microchip and commercial manufacturing is also discussed, as well as the limitations of the technology in areas without consistent connection. Finally, a user comments on a related underwater experiment in Newfoundland.

