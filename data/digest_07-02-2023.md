## AI Submissions for Sun Jul 02 2023 {{ 'date': '2023-07-02T17:09:42.147Z' }}

### AI and the Automation of Work

#### [Submission URL](https://www.ben-evans.com/benedictevans/2023/7/2/working-with-ai) | 196 points | by [CharlesW](https://news.ycombinator.com/user?id=CharlesW) | [214 comments](https://news.ycombinator.com/item?id=36565854)

In a recent blog post, Benedict Evans discusses the impact of generative AI, Large Language Models (LLMs), and ChatGPT on the automation of work. He acknowledges that while there is agreement in the tech industry about the transformative power of these technologies, there is much debate about the implications and future consequences.

Evans points out that automation has been happening for the past 200 years, and each wave of automation has eliminated certain jobs but also created new ones. However, when facing automation in our own generation, it's natural to worry that the new jobs won't materialize. While historical evidence suggests otherwise, it's hard to predict what new jobs will emerge.

To address this concern, Evans refers to the "Lump of Labour" fallacy, which assumes that there is a fixed amount of work to be done and automation reduces job opportunities. He argues that when automation makes things cheaper, it leads to increased consumption and the creation of new jobs. The ripple effect through the economy generates prosperity and employment.

One criticism of this model is that automation has been progressively moving up the scale of human capabilities. From physical labor to white-collar jobs, if we automate white-collar work, what's left? To counter this, Evans introduces the concept of the Jevons Paradox, which explains that as technology becomes more efficient, it's used more extensively, leading to increased resource consumption. In the case of white-collar work, automation has historically created new opportunities rather than eliminating jobs.

Evans gives examples from history, such as the impact of typewriters and adding machines on clerical employment. Although these technologies reduced the number of clerks required for certain tasks, they also increased productivity and enabled new forms of work. Similarly, he argues that automation can lead to more analysis, improved inventory management, and the creation of businesses that can only exist because of automation.

In conclusion, while concerns about job displacement due to automation are valid, historical evidence suggests that new jobs will emerge. Automation has consistently led to increased productivity and economic growth. Although we can't predict the exact nature of future jobs, Evans remains optimistic that automation will continue to present new opportunities for prosperity.

The discussion on Hacker News revolves around the capabilities and potential dangers of AI taking over various jobs. Some users express concerns about security issues and the potential for AI machines to misinterpret human intentions, leading to fatal mistakes. Others argue that AI reporting machines can be useful in certain situations but should not replace human judgment entirely. The discussion also touches on the impact of automation on the military and law enforcement sectors, with some users pointing out the risks and limitations of relying too heavily on AI in those fields. There is a debate about whether AI will truly replace human jobs or if it will primarily enhance them by taking over tasks that are repetitive or require specific expertise. Overall, the discussion raises valid concerns about the implications of AI in the workforce while also acknowledging its potential benefits.

### Show HN: Project S.A.T.U.R.D.A.Y. â€“ open-source, self hosted, J.A.R.V.I.S.

#### [Submission URL](https://github.com/GRVYDEV/S.A.T.U.R.D.A.Y) | 98 points | by [GRVYDEV](https://news.ycombinator.com/user?id=GRVYDEV) | [22 comments](https://news.ycombinator.com/item?id=36564923)

Introducing Project S.A.T.U.R.D.A.Y: A Toolbox for Vocal Computing

Project S.A.T.U.R.D.A.Y is an exciting new initiative that aims to bring the power of vocal computing to everyone. This comprehensive toolbox uses WebRTC, Audio, and AI technologies to enable users to build their own personal voice-controlled interfaces. Whether you want to create your own J.A.R.V.I.S-like assistant or experiment with vocal computing, Project S.A.T.U.R.D.A.Y has got you covered.

The project is highly modular and flexible, allowing for seamless upgrades when new AI technologies are released. It consists of three main tools: STT (Speech-to-Text), TTT (Text-to-Text), and TTS (Text-to-Speech). The STT tool processes audio input and converts it to text, the TTT tool performs text-based inference, and the TTS tool generates speech from text.

To get started with Project S.A.T.U.R.D.A.Y, you'll need Golang, Python, Make, and a C Compiler. The demo requires a decent amount of processing power, so keep that in mind. The project comes with detailed installation instructions and a demo that lets you experience your own personal voice assistant.

Project S.A.T.U.R.D.A.Y is an exciting step towards making vocal computing accessible to everyone. So, if you've ever dreamed of interacting with your computer using just your voice, now is your chance to bring that dream to life. Check out the project's GitHub repository for more information and to get started on your vocal computing journey.

The discussion about the submission "Introducing Project S.A.T.U.R.D.A.Y: A Toolbox for Vocal Computing" on Hacker News touched on various topics.

- Some users made references to fictional characters like J.A.R.V.I.S from Marvel and HOMER from The Simpsons when discussing vocal computing.
- There were discussions about the potential features and challenges of vocal computing, including the interaction between humans and computers, mind-melding interfaces, and latency issues.
- One user mentioned the possibility of integrating the project with ESP BOX or other voice-controlled systems like Alexa.
- There was an exchange about the use of long-term memory and conversational handling in vocal computing, with suggestions for implementing these features in the project.
- A user expressed curiosity about video demonstrations of the project and suggested providing more detailed information in the Hacker News post.
- One user mentioned their interest in creating voice-controlled interfaces and noted the importance of reducing latency for better interactivity.
- There was a discussion about natural language understanding and processing in vocal computing, including the use of LLMs (large language models) and other approaches like Rasa.
- One user apologized for their skepticism and praised the efforts of the project, while another appreciated the kind words.
- Users briefly mentioned speech input and diagram-making in relation to vocal computing.
- There was a brief exchange about skipping final production acronyms and stylistic choices in communication.

Overall, the discussion covered a range of topics related to vocal computing, exploring potential features, challenges, and technical considerations.

### Automated CPU Design with AI

#### [Submission URL](https://arxiv.org/abs/2306.12456) | 89 points | by [skilled](https://news.ycombinator.com/user?id=skilled) | [23 comments](https://news.ycombinator.com/item?id=36565671)

Researchers from the field of Artificial Intelligence (AI) have made a significant breakthrough in the realm of machine design. In a recent publication, titled "Pushing the Limits of Machine Design: Automated CPU Design with AI," a team of 18 authors led by Shuyao Cheng introduces a revolutionary approach to automatically designing a central processing unit (CPU) using AI techniques. CPUs are considered one of the most complex devices ever created by humans, making the successful application of AI in their design a remarkable achievement.

In their research, the team developed a method that allows machines to design a CPU solely based on external input-output observations, rather than relying on formal program code. The AI approach generates the circuit logic of the CPU design using a graph structure called Binary Speculation Diagram (BSD), ensuring both accuracy and efficiency. To demonstrate the capability of their method, the researchers explored an unprecedentedly large search space of 10 to the power of 10 to the power of 540 possibilities, which is considered the largest of all machine-designed objects to date.

After only five hours of computation, the team's approach successfully generated an industrial-scale RISC-V CPU, which was able to run the Linux operating system and perform comparably to the human-designed Intel 80486SX CPU. This breakthrough not only significantly reduces the design cycle in the semiconductor industry but also has the potential to reform it by enabling machines to learn the von Neumann architecture autonomously.

The research paper, totaling 28 pages, was submitted to the arXiv preprint server under the category of Artificial Intelligence (cs.AI) and Hardware Architecture (cs.AR). The authors provide extensive technical details and analysis of their methodology, making it a valuable resource for researchers in the field. This groundbreaking work represents a significant leap forward in machine design, opening up new possibilities for AI systems to tackle increasingly complex tasks in the future.

The discussion on this submission covers various aspects of the research paper and raises some questions and concerns.

- One commenter discusses the paper and suggests that the approach described in the abstract seems to be based on a method called Binary Speculation Diagram (BSD). They also mention that the high-level diagram in Figure 1 appears to show an expected input-output process, but the commenter is unsure if the paper discusses predefined components and their connections in the design.

- Another commenter points out that the implementation of the AI-designed CPU was executed on an Intel Xeon Gold 6230 CPU cluster, not on an FPGA as stated in the summary. They also mention that the implementation was done on 65nm technology, and the resultant CPU was manufactured in December 2021.

- A commenter expresses doubts and skepticism about the AI approach, suspecting that the discussion in the paper does not touch upon important points such as register mapping, memory access, and control registers. They question the effectiveness of the AI approach without considering these critical aspects.

- Another commenter raises concerns about the lack of verification and the complexity of implementing the generated designs, particularly noting the absence of voltage-aware translation to make the CPU comparable to an Intel 486SX.

- A discussion ensues regarding the complexity and potential limitations of the paper's approach. One commenter suggests that the design lacks certain optimizations and that the implementation of high-level details, such as placement, can be challenging. Another commenter notes that the research focuses on exploring a design space rather than optimizing existing approaches.

- The discussion briefly touches on the topic of Google's Tensor processing units and their connection to AI-driven chip designs.

- One commenter raises questions about the human-designed architecture discovery process and whether the AI approach can replicate it.

- There are suspicions raised about the validity and credibility of the paper. One commenter suggests that the claims made may be fraudulent or misleading, while another commenter expresses concerns regarding the reputation of Chinese institutions and their history of fraudulent papers.

- The discussion also touches on the topic of reproducibility and the challenges of fraudulent papers, along with a mention of the need for institutions to have strong reputations.

- Some commenters highlight the importance of considering optimal designs and point out examples of successfully documented and optimized CPU designs.

- Finally, there is a comment highlighting inaccuracies in the abstract of the research paper, mentioning that the claims made are misleading and that the approach heavily relies on Binary Decision Diagrams (BDDs), which are extensively used in hardware design automation.

Overall, the discussion raises a range of questions, concerns, and doubts about the research paper and the claims made regarding the AI-designed CPU. There are discussions about the AI approach itself, the implementation details, the replication of human-designed architecture, and the credibility of the research.

### Stanford A.I. Courses

#### [Submission URL](https://ai.stanford.edu/courses/) | 438 points | by [ecks4ndr0s](https://news.ycombinator.com/user?id=ecks4ndr0s) | [67 comments](https://news.ycombinator.com/item?id=36562502)

Introducing Stanford's A.I. Courses: Get Your Knowledge Boost with Video Introductions

Exciting news for all AI enthusiasts! Stanford University has recently released video introductions to some of their popular A.I. courses from Fall 2019 CS229. These videos offer a fantastic opportunity for learners to get a glimpse into the world of artificial intelligence. 

Covering a wide range of topics, the video introductions provide a sneak peek into the course content, allowing students to assess the relevance and level of difficulty before fully committing to the course. This is especially helpful for those who want to make informed decisions about their A.I. education. 

To access these video introductions, head over to the official website: https://ai.stanford.edu/stanford-ai-courses. The courses are sorted by term, making it easy to locate the specific one you're interested in. Whether you're a beginner or an advanced learner, you're bound to find something that suits your needs.

So, don't miss out on this golden opportunity to expand your A.I. knowledge with the esteemed faculty and resources available at Stanford University. Harness the power of technology and embark on a fascinating learning journey today!

The discussion surrounding the submission "Introducing Stanford's A.I. Courses: Get Your Knowledge Boost with Video Introductions" on Hacker News covers various topics related to artificial intelligence education. Here is a summary of the discussion:

1. Some commenters criticize the way machine learning programs are taught, pointing out that they often focus on theoretical skills rather than practical application.
2. Others highlight the importance of a solid understanding of statistics and fundamentals for tackling machine learning problems effectively.
3. There is a debate about the limitations of academic research papers and the need for evaluating real-world applications and benchmarks.
4. Recommendations for other AI courses and resources are shared, including Coursera and YouTube videos by Andrej Karpathy.
5. The difficulty of conveying information effectively in the field of AI education is discussed, with some arguing for a practical approach and others emphasizing the importance of understanding underlying concepts and theory.
6. Suggestions for learning linear regression and deep learning are provided, including online courses and reading materials.
7. The challenge of learning mathematics, such as calculus and linear algebra, for deep learning is mentioned.
8. Commenters recommend various resources and tools for learning AI, such as reading research papers, following blogs, and experimenting with OpenAI APIs and Colab notebooks.
9. There is a mention of the importance of understanding and manipulating data in Python for machine learning projects.

Overall, the discussion provides a range of perspectives on AI education and offers additional resources and advice for those interested in furthering their knowledge in the field.

### It's 2023 and memory overwrite bugs are not just a thing theyre still number one

#### [Submission URL](https://www.theregister.com/2023/06/29/cwe_top_25_2023/) | 111 points | by [LinuxBender](https://news.ycombinator.com/user?id=LinuxBender) | [50 comments](https://news.ycombinator.com/item?id=36562727)

Memory overwrite bugs continue to be the most dangerous type of software bug, according to MITRE. These bugs, also known as out-of-bounds write bugs, are responsible for 70 vulnerabilities on the US government's list of known vulnerabilities that are under active attack. Out-of-bounds write bugs occur when software or hardware alters memory it shouldn't, causing unexpected changes or crashes. Exploit code can trigger these bugs to take control of the software. MITRE recommends using memory-safe languages like Rust to prevent these bugs. Cross-site scripting bugs and SQL injection flaws are the second and third most dangerous bugs, respectively. CISA has added eight more vulnerabilities to its Known Exploited Vulnerabilities Catalog, including flaws in D-Link and Samsung devices. The list of the Top 25 most dangerous software weaknesses for 2023 remains the same as last year. MITRE will publish reports to help organizations effectively use the Top 25 list.

The discussion on this submission covers several different topics related to memory overwrite bugs and the use of memory-safe languages like Rust. Here are some key points from the discussion:

- One user argues that array bounds errors are the number one problem that can be addressed by using memory-safe languages like Rust. They suggest that existing features in these languages make it straightforward to prevent and detect such bugs.
- Another user expresses confusion about the proposal and suggests that the discussion is politically motivated. They argue that the proposal should be made public for everyone to understand and debate.
- There is a discussion about technical resistance to introducing syntactic changes in C. Some users point out the challenges and potential compatibility issues that could arise from changing existing APIs and function prototypes.
- The use of attributes for function prototypes, variable declarations, and nested structures is mentioned as a possible solution to prevent bugs and avoid the need for a complete rewrite of existing code.
- One user mentions that adding warning flags to the compiler could help address some of the issues related to passing arrays without length.
- The discussion also touches on issues related to first-class slice types and checked interfaces for accessing arrays. Some users express concerns about the complexity and potential performance impact of such features, while others argue that they could improve safety in programming languages like Rust and C#.
- The trade-offs between handling out-of-bounds errors and memory corruption are discussed. Some users suggest that handling out-of-bounds errors is generally preferred, while others argue that memory corruption is a larger problem.
- The possibility of introducing mandatory bounds checking in C++ is mentioned, with some users expressing support for the idea and others expressing concerns about the impact on performance and programming language standards.
- The resistance to standards bodies and the importance of working groups in addressing security-related improvements are discussed.
- There is a conversation about the syntax for determining dimensions in C, with some users suggesting that existing features like suffixes and access through pointers can provide access to array lengths.

### The open-source AI boom is built on Big Techâ€™s handouts. How long will it last?

#### [Submission URL](https://www.technologyreview.com/2023/05/12/1072950/open-source-ai-google-openai-eleuther-meta/) | 32 points | by [AnhTho_FR](https://news.ycombinator.com/user?id=AnhTho_FR) | [27 comments](https://news.ycombinator.com/item?id=36560473)

The rise of open-source large language models is threatening the dominance of Big Tech in the field of artificial intelligence (AI), according to a leaked memo by a senior engineer at Google. These freely available alternatives to Google's Bard or OpenAI's ChatGPT offer researchers and app developers the ability to study, modify, and build upon them. While this increased accessibility has driven innovation and democratized AI, it also poses risks. Many of these models rely on the work of big firms like Meta AI and OpenAI, which could choose to restrict access in the future. Closing down access would not only stifle the open-source community but also consolidate AI breakthroughs in the hands of the largest AI labs. However, some argue that opening up code for a limited period can drive innovation while still protecting the company's interests. The future of AI development and usage hangs in the balance as the industry grapples with the implications of open-source models.

The discussion on the Hacker News submission revolves around the rise of open-source large language models and its impact on the dominance of big tech companies in the AI field.

Some commenters express skepticism about the benefits of open-source projects, stating that companies like Google, Meta, and Microsoft do not benefit from open sourcing their projects. However, others argue that open-source models offer alternatives and drive innovation.

There is a debate about the control of AI technology, with some expressing concerns about big tech companies gaining a monopoly. However, others argue that big tech companies do not rely solely on open-source projects and have internal versions that are not shared with the world.

The discussion also touches on the importance of open-source software in the AI industry, with examples such as PyTorch and TensorFlow being mentioned. Additionally, there is a discussion about the role of open source in the survival and growth of big tech companies.

Some commenters bring up historical examples, highlighting the impact of open-source software in the past, such as the case of Sun and Linux.

Overall, the discussion delves into the benefits and limitations of open-source models and their potential implications for the future of AI development.

