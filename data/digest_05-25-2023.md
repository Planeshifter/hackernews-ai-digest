## AI Submissions for Thu May 25 2023 {{ 'date': '2023-05-25T17:11:18.302Z' }}

### Before the Fire: Saturn-Apollo Applications (1966)

#### [Submission URL](https://www.wired.com/2012/08/before-the-fire/) | 12 points | by [rbanffy](https://news.ycombinator.com/user?id=rbanffy) | [3 comments](https://news.ycombinator.com/item?id=36069260)

NASA's Saturn-Apollo Applications (SAA) program aimed to make use of Apollo hardware and investments to continue with post-Apollo manned spaceflight. Launched in 1965, the SAA program proposed two objectives: Long-Duration Flights and Spaceflight Experiments. Long-Duration Flights aimed to measure the effects of space flights of increasing duration to establish the basic capabilities required for future manned space flight goals, while Spaceflight Experiments would cover space life sciences, astronomy, space physics, advanced lunar exploration, and space technology development. The program was expected to kick off in 1968 with missions AS-209, AS-210, AS-211, and AS-212, and would see 21 Saturn IB and 16 Saturn V launches by the end of 1973 under the Case I schedule, and up to 26 Saturn IB and 17 Saturn V launches under the more ambitious Case II schedule.

The discussion on the submission includes only two comments. 

The first comment is by user "pnwrst," who provides a link to an article on the Spaceflight History Blog that talks about the NASA's Saturn-Apollo Applications (SAA) program. 

The second comment is by user "pcrds," who criticizes the lack of captions in the article's main text. In response, user "JKCalhoun" agrees with the criticism and adds that the staff at the Spaceflight History Blog is fantastic. They also mention that the fascination with space exploration dates back to the 1960s and 70s, when optimistic charts and diagrams of lifting bodies and space stations were widely available in libraries.

### How to Finetune GPT-Like Large Language Models on a Custom Dataset

#### [Submission URL](https://lightning.ai/pages/blog/how-to-finetune-gpt-like-large-language-models-on-a-custom-dataset/) | 477 points | by [T-A](https://news.ycombinator.com/user?id=T-A) | [120 comments](https://news.ycombinator.com/item?id=36068850)

Lightning AI has launched Lit-LLaMA, a minimal and optimized rewrite of LLaMA under Apache 2.0 license. This implementation uses Lit-Parrot, a nanoGPT-based model that can be fine-tuned on custom data sets, including StableLM, Pythia, and RedPajama-INCITE model weights. The tutorial details the installation process, model weight download, and data preparation for fine-tuning on the Dolly 2.0 instruction dataset. The processed data is saved to the destination path, followed by training and validation datasets for the preprocessed and tokenized prompts and labels.

The conversation touches on issues of copyright and licensing, as well as the potential benefits and drawbacks of using generative AI models for commercial purposes. Other topics include understanding alignment models, OpenAI's API terms of service, and the legal implications of using AI-generated content.

### Lessons from Creating a VSCode Extension with GPT-4

#### [Submission URL](https://bit.kevinslin.com/p/leveraging-gpt-4-to-automate-the) | 200 points | by [kevinslin](https://news.ycombinator.com/user?id=kevinslin) | [77 comments](https://news.ycombinator.com/item?id=36071342)

Developer Kevin Lin recaps his recent experiment where he used GPT-4 and the smol-ai framework to create a Visual Studio Code extension. The task required generating various files, scaffolding, and knowledge of TypeScript to create an advanced tool that enables users to adjust heading levels of selected markdown text, sans human intervention. With short and general prompts provided, Lin discovered the efficacy of the tools he used to generate code for a complex program, without any intervention from humans.

Some users express concerns about the quality and accuracy of AI-generated code, while others argue that it can improve productivity and free up developers to focus on more complex tasks. There are also suggestions for improving documentation and providing better prompts for AI-generated code.

### Chatbot Arena Leaderboard

#### [Submission URL](https://lmsys.org/blog/2023-05-25-leaderboard/) | 114 points | by [tosh](https://news.ycombinator.com/user?id=tosh) | [34 comments](https://news.ycombinator.com/item?id=36075977)

The Chatbot Arena has released its latest leaderboard update, which includes new chatbots, such as Google PaLM 2. According to the ranking system, GPT-4 is currently in first place, followed by Claude-v1 and Claude-instant-v1. PaLM 2 is ranked sixth and has shown strong performance against the top four chatbots. However, it has deficiencies in its regulation, multilingual capabilities, and reasoning abilities. The public API version of PaLM 2 available on Google Vertex API has a higher rate of abstaining from answering questions and failing to answer non-English questions.

There are discussions about naming new AI models for generative storytelling, along with critiques around testing and comparing models. Some users note missing models and the limitations of commercial and proprietary LLMs. There are discussions around the computation bottleneck of using open-source models versus proprietary ones. Users also share their experiences testing different chatbots on the leaderboard.

### New superbug-killing antibiotic discovered using AI

#### [Submission URL](https://www.bbc.com/news/health-65709834) | 153 points | by [tsenapathy](https://news.ycombinator.com/user?id=tsenapathy) | [72 comments](https://news.ycombinator.com/item?id=36071675)

Scientists have used artificial intelligence (AI) to discover a new antibiotic, abaucin, that can kill a deadly species of superbug, Acinetobacter baumannii. Using AI to narrow down thousands of potential chemicals to a handful that could be tested in the laboratory, the researchers trained the AI to learn the chemical features of drugs that could attack the problematic bacterium. In tests, abaucin proved to be incredibly potent and effective in treating infected wounds in mice and is able to kill A. baumannii samples from patients. The researchers believe the precision of the antibiotic will make it harder for drug-resistance to emerge and could lead to fewer side-effects. Discussions in the comments section touched on various aspects of the report, including the number of drugs screened, the practicality of the approach, and the role of AI in drug discovery.

### ChatGPT plugins now support Postgres and Supabase

#### [Submission URL](https://supabase.com/blog/chatgpt-plugins-support-postgres) | 149 points | by [cendenta](https://news.ycombinator.com/user?id=cendenta) | [28 comments](https://news.ycombinator.com/item?id=36072524)

OpenAI's ChatGPT is getting a boost from Supabase's recently contributed Postgres implementation to build plugins with pgvector. Retrieval plugins allow ChatGPT to access data from external sources, like PDF documents, Confluence, or Notion knowledge bases, which can be converted into smaller chunks and stored in a vector database. Both Postgres and Supabase implementations allow for storing embeddings - a way to convert text into mathematical representations to allow for similarity comparison - into the vector database. The plugins can also incorporate filtering options for source, author, document, and date to find the most relevant information for each question that ChatGPT answers. As an example, the blog post suggests ingesting all of the Postgres docs into a Supabase database to ask questions about the documentation. Some of the topics in the comments include a discussion on context warehouses, building templates, and external marketing. Additionally, one commenter mentions Faiss, which finds similar vectors, while another asks a question about ChatGPT plugins.

### Gorilla: Large Language Model connected with massive APIs

#### [Submission URL](https://gorilla.cs.berkeley.edu/) | 236 points | by [shishirpatil](https://news.ycombinator.com/user?id=shishirpatil) | [116 comments](https://news.ycombinator.com/item?id=36073241)

A team from UC Berkeley and Microsoft Research has released an open-source language AI model called Gorilla that can make the appropriate API calls. The model has been trained on three large machine learning datasets - Torch Hub, TensorFlow Hub, and HuggingFace - and outperforms GPT-4, Chat-GPT, and Claude in zero-shot evaluations. The researchers claim Gorilla is reliable and helps to reduce "hallucination errors" that often occur with LLMs, making it more applicable for real-world use. The model and code can be accessed on the project GitHub page.

Commenters in the discussion have offered diverse views on the potential and limitations of Gorilla and language models in general. Some commenters praise Gorilla for its better capability than competing existing models. Others discuss the limitations and dangers of language models, saying that they need to be properly developed, tested, and deployed to avoid any unforeseen consequences. There are also concerns about the alignment of the language models with human values and the difficulty of achieving it through current methods. Some commenters argue that we should be cautious in deploying experimental AI systems and put necessary infrastructure in place before deployment.

### Deno 1.34: Deno compile supports NPM packages

#### [Submission URL](https://deno.com/blog/v1.34) | 269 points | by [unripe_syntax](https://news.ycombinator.com/user?id=unripe_syntax) | [88 comments](https://news.ycombinator.com/item?id=36068896)

The Deno team has released version 1.34, adding several new features to boost compatibility with npm and Node.js, and enhancing overall quality of life and developer experience. Most notably, deno compile supports npm packages, allowing developers to create binary executables with all dependencies and configurations packaged alongside the executable. Additionally, glob support has been added to the configuration file deno.json and CLI arguments, and TLS certificates with IP addresses are now supported. Other improvements include exclusion of files or folders for all sub commands and configurable file system entry limits in the language server.

In the discussion, some users expressed support for Deno's focus on smaller projects and web API compatibility, while others questioned the necessity of npm compatibility and suggested that Deno should focus on developing its own ecosystem. Some users noted that Node.js and npm are separate systems and being compatible with one doesn't necessarily mean being compatible with the other. One user pointed out that Deno's unique approach to module management can simplify things, while another argued that trying to replicate the complexity of larger systems is unnecessary.

### Neuralink gets U.S. FDA approval for human clinical study of brain implants

#### [Submission URL](https://www.reuters.com/science/elon-musks-neuralink-gets-us-fda-approval-human-clinical-study-brain-implants-2023-05-25/) | 38 points | by [lopkeny12ko](https://news.ycombinator.com/user?id=lopkeny12ko) | [9 comments](https://news.ycombinator.com/item?id=36077392)

Elon Musk's Neuralink has been given the green light by the US Food and Drug Administration (FDA) to conduct its first-ever clinical trial with humans. The company didn't reveal much about the details of the study, but it called the FDA's approval an important first step for the technology and said more information would be available soon. Neuralink, which was founded in 2016, has been the subject of several federal investigations and has struggled to earn approval from the FDA in the past due to lithium battery concerns and wire migration risks. Musk has said the company's brain implants could one day help treat conditions like obesity, autism, and depression.

The comments on the submission range from political jokes about a future cyborg politician to concerns about the safety and ethics of brain implants. Some users wonder if the approval of Neuralink's clinical trial could lead to advancements in treating paralysis and blindness, while others express skepticism about the technology and its potential legal implications. One comment suggests that privately funded brain implant research could lead to a lack of transparency and potential medical malpractice. Another comment simply jokes about Elon Musk completing his homework.

### PostgresML raises $4.7M to launch serverless AI app databases based on Postgres

#### [Submission URL](https://postgresml.org/blog/postgresml-raises-4.7M-to-launch-serverless-ai-application-databases-based-on-postgres) | 52 points | by [kiyanwang](https://news.ycombinator.com/user?id=kiyanwang) | [24 comments](https://news.ycombinator.com/item?id=36072525)

PostgresML has raised $4.7M in seed funding to launch serverless AI application databases based on Postgres. By integrating leading machine learning libraries like Torch, Tensorflow, XGBoost, LightGBM, and Scikit Learn, PostgresML streamlines the infrastructure requirements for AI tasks, allowing developers to focus on creating intelligent applications. The company has created a custom Postgres load balancer tailored for machine learning workflows at scale, allowing them to pool multiple machines and connections to create a mesh of Postgres clusters that appear as independent Postgres databases. PostgresML is also an open-source extension to Postgres that brings models and algorithms into the database engine, and developers can load pre-trained algorithms and datasets from HuggingFace. With PostgresML, developers can prototype and deploy AI applications quickly and at scale in a matter of minutes. The discussion on Hacker News has varied, with some comments questioning the company's name, while others point out the limitations of the product.
