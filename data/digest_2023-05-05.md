## AI Submissions for Fri May 05 2023 {{ 'date': '2023-05-06T00:23:21.363Z' }}

### Concrete: A fully homomorphic encryption compiler

#### [Submission URL](https://www.zama.ai/post/zama-concrete-fully-homomorphic-encryption-compiler) | 80 points | by [zacchj](https://news.ycombinator.com/user?id=zacchj) | [10 comments](https://news.ycombinator.com/item?id=35826723)

Zama has released their Fully Homomorphic Encryption (FHE) compiler, Concrete, designed to simplify the management of noise, cryptographic parameters selection, and order of operations for specific computations for developers. The Compiler expects an input program in MLIR, and it can be used via Python, C++, and C APIs as well as a CLI tool for debugging. The LibrarySupport class is one of the main entry points, enabling the compilation and execution of FHE programs while storing artifacts on disk. The compiled library is stored in a sharedlib file, along with a JSON file that describes the inputs and outputs and crypto parameters for the compiled function.

One commenter questioned the necessity of using encryption for weights, while another commenter pointed out that using FHE to protect software is similar to how Syncrosoft protected software using dongles. Another commenter mentioned that they have been working on developing a CPU designed for FHE for the past decade, while others compared Concrete to Google's FHE implementation. The discussion also included references to Concrete's code repository and a podcast on the intersection of FHE and zero-knowledge proofs.

### Shap-E: Generate 3D objects conditioned on text or images

#### [Submission URL](https://github.com/openai/shap-e) | 273 points | by [tim_sw](https://news.ycombinator.com/user?id=tim_sw) | [50 comments](https://news.ycombinator.com/item?id=35836976)

OpenAI has released its code and model for Shap-E, a system that generates 3D objects based on text or images. The release includes examples of models that can generate things like chairs that look like avocados, spaceships, and birthday cupcakes. Users can install Shap-E with pip, and access a variety of notebooks that provide guidance on encoding models, sampling 3D models based on a text prompt, and more. The code and model are available on GitHub under an MIT license.

Some users discuss their experiences with generating 3D objects, while others share examples of objects they have generated. Others comment on the difficulty of generating 3D models and suggest alternative tools. There is also discussion around the licensing of the code and models, as well as related topics such as 3D printing and file formats.

### Unlimiformer: Long-Range Transformers with Unlimited Length Input

#### [Submission URL](https://arxiv.org/abs/2305.01625) | 322 points | by [shishy](https://news.ycombinator.com/user?id=shishy) | [99 comments](https://news.ycombinator.com/item?id=35832802)

Researchers have proposed a new model called Unlimiformer, which extends the capabilities of existing transformer-based models by allowing them to handle unlimited input lengths for a better long-document and multi-document summarization. The model can be applied to any existing pretrained encoder-decoder transformer, and it offloads the attention computation across all layers to a single k-nearest-neighbor index that can be kept on the GPU or CPU memory and queried in sub-linear time. Unlimiformer has been shown to perform well on several benchmarks, summarizing even 350k token-long inputs from the BookSum dataset without any input truncation at test time. The code and models are publicly available online.

The comments section discusses topics such as the quality of pre-reviewed papers, self-aggrandizing behavior in the research community, challenges with peer review processes, differences between conferences and HN commenting, and the importance of feedback from the HN community.

### Bluesky's AT Protocol - Federation Architecture Overview

#### [Submission URL](https://blueskyweb.xyz/blog/5-5-2023-federation-architecture) | 137 points | by [capableweb](https://news.ycombinator.com/user?id=capableweb) | [79 comments](https://news.ycombinator.com/item?id=35834106)

Bluesky, the new social media platform built on the AT Protocol, is set to launch a sandbox environment for testing federation with allow-listed servers. A federated networking model, AT Protocol differs from conventional social media by allowing users to run their own servers, host data on a personal data server, and use big graph services to assemble and curate a personal feed. The architecture is expected to facilitate public conversations on a global social network, with an ecosystem of app views for each lexicon, including video, long-form blogging, and groups and forums. Bluesky aims to make federation easy and accessible to all.

The discussion around the submission on Hacker News includes various opinions and insights. Some users express concerns regarding Bluesky's implementation of centralization, such as difficulty in implementing control and filtering, while others believe that Bluesky's potential for transparency and decentralized social media could be a healthier alternative to existing platforms. Additionally, there is debate regarding the comparison between ActivityPub and Bluesky, with some users pointing out differences in their respective designs and certain limitations of ActivityPub. There is also discussion about the challenges of migration between servers and ways to address the issue of lost interactions in the process. Overall, the discussion brings up various important points related to decentralization and the future of social media.

### At Musk’s brain-chip startup, animal-testing panel is rife with conflicts

#### [Submission URL](https://www.reuters.com/technology/musks-brain-chip-startup-animal-testing-panel-is-rife-with-potential-conflicts-2023-05-04/) | 137 points | by [wootland](https://news.ycombinator.com/user?id=wootland) | [90 comments](https://news.ycombinator.com/item?id=35834918)

Elon Musk's brain-implant company, Neuralink, has come under fire for filling its animal-research oversight board with company insiders who may stand to benefit financially from the venture's development goals. According to documents and interviews with employees, 19 of the board's 22 members were Neuralink employees as of late 2022, raising questions about potential violations of conflict-of-interest regulations aimed at protecting research integrity. As we've previously reported, Neuralink is seeking regulatory approval for human trials of a brain chip intended to help paralyzed people type with their minds, among other goals.

Some users argue for the need for regulated and informed animal testing, while others argue for caution in human testing. There is also a debate on the efficacy and enforceability of current regulations, with some users calling for a change in regulations to better protect animals and humans involved in scientific research.

### Show HN: UnionX – GPT4-powered Copilot for Work with Jupyter-style notebooks

#### [Submission URL](https://www.unionx.io/) | 48 points | by [gangster_dave](https://news.ycombinator.com/user?id=gangster_dave) | [5 comments](https://news.ycombinator.com/item?id=35836679)

Looking for a way to boost your productivity and streamline your workflow? Look no further than UnionX, the AI-powered platform that lets you easily analyze documents, generate insights, and create new documents in seconds. Whether you're a data scientist, legal professional, or product manager, UnionX can help you save time and work more efficiently. With powerful tools like OpenAI's GPT4 model and Jupyter-style workflows, UnionX makes it easy to gather, analyze, and generate new insights from your data. So why wait? Try UnionX today and start achieving more in less time!

Some users feel that the concept sounds exciting, but the marketing documentation is not clear enough. They suggest pushing towards a Jupyter notebook interface for non-coding tasks and adding a coding interface for more technical users. Others believe that simple notebooks and screencaps of them would be helpful in understanding the product. The conversation then shifts to coding integration and the need for actual notebooks rather than lock-in options. One user also raises a question regarding comparing the platform's source version.

### The AI PR Industrial Complex

#### [Submission URL](https://www.bigtechnology.com/p/the-ai-pr-industrial-complex) | 80 points | by [tim_sw](https://news.ycombinator.com/user?id=tim_sw) | [36 comments](https://news.ycombinator.com/item?id=35829430)

In the rush for corporations, politicians, and other thought leaders to monetize and exploit the opportunities presented by AI technology, an AI PR industrial complex is emerging. This complex operates by using AI as a pretext for problems that might have other causes such as IBM's decision to replace up to 7,800 back-office employees with AI, as opposed to using the technology to make workers more productive. Similarly, politicians and regulators running to the cameras to talk about AI raises questions about their actual understanding of the technology's opportunities and risks. While some AI announcements have real substance, the AI PR industrial complex is growing and drawing deserved skepticism.

Commenters suggest that the AI hype is similar to the cryptocurrency hype of the past, and the emerging AI PR industry is drawing deserved skepticism. Some commenters talk about how AI technology may be applied for specific domain-specific tasks, such as creating 2D and 3D animations for games and designing dashboards and data visualization tools. There is also a discussion of how AI is being used to generate advertisements and how LLMs may be used to solve real-world problems in sectors such as energy and sustainable growth. 

### MosaicML MPT-7B: A Commercially-Usable LLaMa-Quality Model

#### [Submission URL](https://www.mosaicml.com/blog/mpt-7b) | 102 points | by [ml_hardware](https://news.ycombinator.com/user?id=ml_hardware) | [11 comments](https://news.ycombinator.com/item?id=35829800)

MosaicML, an AI platform, has launched its MPT-7B model series, comprising pre-trained transformers that enable faster training and inference. The series comprises four models: MPT-7B Base, a decoder-style transformer with 6.7 billion parameters; and three finetuned variants, including the super-long context MPT-7B-StoryWriter-65k+. MosaicML also released the entire codebase for pretraining, finetuning, and evaluating MPT, a framework for building LLMs, and training and deployment instructions. The models can be licensed for commercial use and are a response to a flurry of activity focused on open-source LLMs.

Some users express confusion around the size and input of the models, but overall, people are impressed with MosaicML's documentation and instructions for training and deployment. One user notes that the MPT-7B model is similar to LLaMa but shows significant improvements in some use cases. Others discuss the practical applications of such models, such as using them for chat instruction and generating long-form text. Finally, there are some comments about the potential future release of GPT-4 and speculation on its potential impact on the AI language modeling space.

### OpenAI changed its plans and won’t train on customer data, Sam Altman says

#### [Submission URL](https://www.cnbc.com/2023/05/05/sam-altman-openai-wont-tap-into-customer-apis.html) | 41 points | by [tim_sw](https://news.ycombinator.com/user?id=tim_sw) | [10 comments](https://news.ycombinator.com/item?id=35830107)

OpenAI has stopped training its large-language models, including the popular chatbot ChatGPT, with paying customer data. This change came as a response to customers who had requested that the company not use their data. OpenAI's terms of service were quietly updated in March to reflect this shift. However, the company's privacy and data protection policy only applies to customers who use the company's API services. The change highlights growing concerns about the use of large-language models such as ChatGPT in areas such as entertainment, where intellectual property rights are being challenged.

The comments on the submission discuss various aspects related to OpenAI's decision to stop training its large-language models with customer data. Some users suggest that OpenAI should provide more control to customers over their data, while others highlight the importance of licensing in protecting intellectual property rights. There is also a discussion on the efficacy of using domain-specific tasks for training language models and the limitations of publicly available training data. One user mentions Azure OpenAI services as a potential alternative.

