## AI Submissions for Mon May 15 2023 {{ 'date': '2023-05-15T17:12:19.292Z' }}

### StarCoder and StarCoderBase: 15.5B parameter models with 8K context length

#### [Submission URL](https://arxiv.org/abs/2305.06161) | 295 points | by [belter](https://news.ycombinator.com/user?id=belter) | [149 comments](https://news.ycombinator.com/item?id=35954481)

A group of researchers from the BigCode community has introduced two new large language models for code (Code LLMs) called StarCoder and StarCoderBase. The models have 15.5 billion parameters with 8K context length, infilling capabilities, and fast large-batch inference enabled by multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced from The Stack, a large collection of permissively licensed GitHub repositories, while StarCoder was fine-tuned on 35 billion Python tokens. The researchers claim that StarCoderBase outperforms every open Code LLM and matches or outperforms the OpenAI code-cushman-001 model, and StarCoder outperforms every model fine-tuned on Python and can achieve 40% pass@1 on HumanEval. The models are now publicly available with an improved PII redaction pipeline and a novel attribution tracing tool under a more commercially viable version of the Open Responsible AI Model license.

The discussion on Hacker News includes comments on the comparison between these models and human intelligence, the importance of training data in developing models, and the limitations of LLMs in comparison to human brains. The conversation also touches on the potential applications for these models in the field of coding and artificial intelligence.

### Basic Pitch: Spotify’s Open Source Audio-to-MIDI Converter (2022)

#### [Submission URL](https://engineering.atspotify.com/2022/06/meet-basic-pitch/) | 31 points | by [andrewmcwatters](https://news.ycombinator.com/user?id=andrewmcwatters) | [3 comments](https://news.ycombinator.com/item?id=35955934)

Spotify has released an open source tool for converting audio recordings into MIDI notes using machine learning. Called Basic Pitch, the tool can transcribe the musical notes in a recording from almost any instrument, including the voice. Basic Pitch uses a neural network to predict MIDI note events given audio input, and is both versatile and accurate, as well as computationally lightweight, meaning it is faster to run. In contrast to other MIDI converter tools, Basic Pitch is polyphonic and instrument-agnostic, can detect pitch bends, and can track multiple notes at once.

The discussion includes various opinions and experiences with audio-to-MIDI conversion tools. Some users have expressed interest in trying out Basic Pitch as it seems accurate and versatile. One user mentions that they will try it out, as their current digital audio workstation (DAW) does not have a voice recorder after transcription to MIDI, which Basic Pitch could potentially solve. However, some users have shared cautionary experiences with other conversion tools, such as one service that unexpectedly started charging for usage, which led them to wish for more open source hardware. Another user mentions using Ableton's Audio-to-MIDI converter, which they found to be fast but not always accurate in detecting chords, notes, velocity, and timing. Lastly, there is a comment providing a Github repository for an earlier version of the Basic Pitch code that is recommended to be used with Python, although it is noted that the installation may have some unresolved issues.

### Google I/O and the Coming AI Battles

#### [Submission URL](https://stratechery.com/2023/google-i-o-and-the-coming-ai-battles/) | 196 points | by [Amorymeltzer](https://news.ycombinator.com/user?id=Amorymeltzer) | [176 comments](https://news.ycombinator.com/item?id=35945988)

Google’s recent keynote in Paris is being criticized as poor quality with outdated speakers and little new content. The submission suggests that this could be due to Google feeling threatened by Microsoft’s Bing announcement, which is powered by the GPT language model AI. However, Google has been talking about AI for years and is progressing significantly with its Smart Reply and Smart Compose features found in products like Gmail and Google Photos. There is a discussion in the comments suggesting that Google has a clear focus on organizing the world’s information, and that its AI capabilities are evident in its 15 products with over 500 million users. A few comments discuss how Google’s AI strategy is largely directionless, and others suggest that the company may be paying for sponsored Tweets to promote its AI products. There is also a debate about how Google's competitors are fairing in the AI space. Overall, the comments discuss Google's AI capabilities and direction, as well as the similarities and differences between Google and its competitors.

### Which kinds of GPT startups will thrive?

#### [Submission URL](https://assistedeverything.substack.com/p/the-three-hills-model-for-evaluating) | 108 points | by [gimili](https://news.ycombinator.com/user?id=gimili) | [81 comments](https://news.ycombinator.com/item?id=35948145)

A new article on the "Assisted Everything" newsletter proposes a model to evaluate the success potential of startups based on GPT, a technology that is revolutionising the white-collar industry; the "Three-Hills" model considers "Productivity Enhancements", "Non zero-sum-game Value" and "Moat = Value from Context" as the three main axes to assess a GPT application. The article provides examples of Level I GPT applications, which are those where users could perform tasks themselves but can do so faster and more efficiently assisted by GPT, and Level II GPT applications, which surpass the Tug-of-War Valley and provide value outside of existing zero-sum games.

The discussion touched on various aspects including how Microsoft and Google companies may not be well suited to run startups because they primarily build generic software targeted for business development, SaaS and mobile apps. The comments also addressed the differences between closed and open-source models in machine learning and how GPT technology may not be suitable for every vertical. They discussed the potential of GPT-powered tools to help fine-tune business models, offer legal and medical support, and revolutionize traditional markets like customer support and finance. Lastly, the conversation touched on how GPT startups may have to focus on building products that people love, reaching people to eventually scale, monetizing users, and building a kind of network effect.

### Together’s $20M seed funding to build open-source AI and cloud platform

#### [Submission URL](https://www.together.xyz/blog/seed-funding) | 73 points | by [tim_sw](https://news.ycombinator.com/user?id=tim_sw) | [22 comments](https://news.ycombinator.com/item?id=35951023)

Together, an AI platform that provides open-source generative AI models, has raised $20 million in seed funding led by Lux Capital. The platform aims to empower innovation and creativity by making AI accessible to anyone, anywhere, and establishes open-source as the default way to incorporate AI. Together's mission is to outrival closed models by creating open models and to give developers and organizations greater ability to understand, inspect, and utilize AI without vendor lock-in and with strong privacy protections.

The discussion on Hacker News mainly revolves around the trend of commercial entities claiming to be open source, but charging licensing fees, and Together's approach to using open models. Other topics of discussion include the company RedPajama's release of commercial-only licensed LLM models, the importance of open source in AI models, Stability AI platforms, and the benefits of Together's platform. Some users also discussed the funding and alignment of the company's strategy.

### HumanRF: High-Fidelity Neural Radiance Fields for Humans in Motion

#### [Submission URL](https://synthesiaresearch.github.io/humanrf/) | 60 points | by [Keats](https://news.ycombinator.com/user?id=Keats) | [15 comments](https://news.ycombinator.com/item?id=35946893)

Researchers have introduced HumanRF, a high-fidelity neural radiance field (RF) that captures human performance in motion from multi-view video input and enables playback from novel, previously unseen viewpoints. The researchers trained the model on ActorsHQ, their multi-view dataset of footage from 160 cameras providing 12MP footage of 16 sequences. While most research focuses on synthesizing at resolutions of 4MP or lower, this work operates at 12MP, making a significant step towards production-level quality novel view synthesis. Applications of high-fidelity human representation and motion capture include film production, gaming and video conferencing.

Researchers have created a high-fidelity neural radiance field (RF) called HumanRF that captures human performance in motion and enables playback from novel, unseen viewpoints. They trained the model on ActorsHQ, a multi-view dataset comprising footage from 160 cameras that provided 12MP footage of 16 sequences. The model could find applications in film production, gaming, and video conferencing. Commenters discussed the movie The Matrix's interpolation techniques, as well as the potential for gaming applications and the similarities with Nintendo Miis. Additionally, someone talking about a classic 1981 film called Looker with an ocular-oriented kinetic emotive response device called "Light Ocular-Oriented Kinetic Emotive Responses (LOOKER) dvc technology."

### Vicuna: An Open-Source Chatbot Impressing GPT-4

#### [Submission URL](https://lmsys.org/blog/2023-03-30-vicuna/) | 42 points | by [kordlessagain](https://news.ycombinator.com/user?id=kordlessagain) | [6 comments](https://news.ycombinator.com/item?id=35942654)

The Vicuna Team has introduced Vicuna-13B, an open-source chatbot that achieves more than 90% ChatGPT quality and outperforms other models such as LLaMA and Stanford Alpaca in more than 90% of cases. Vicuna-13B was trained by fine-tuning LLaMA on user-shared conversations from ShareGPT and the open-source code and weights, along with an online demo, are publicly available for non-commercial use. The preliminary evaluation of the model quality was done using GPT-4 to judge the model outputs. Vicuna-13B builds on top of Stanford’s Alpaca with improvements in memory optimization, multi-round conversations, and cost reduction through spot instances and achieved competitive performance compared to other open-source models.

The discussion involves several users providing their opinions and thoughts on the Vicuna-13B chatbot and its open-source release. One user, Animats, raises concerns about the comparison of small and large models while pointing out the importance of underlying data and verifying results. Another user, jsnll, links to an earlier discussion on the same topic. A third user, rngn, is impressed with the article and the hosting of the models. Meanwhile, brnjkng is optimistic about the commercial potential of Vicuna-13B, but stvncr expresses concerns about the terms and conditions of using open-source resources such as ShareGPT. Finally, brnjkng mentions that multiple models can be run locally for commercial purposes using CPU inference.
