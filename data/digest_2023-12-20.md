## AI Submissions for Wed Dec 20 2023 {{ 'date': '2023-12-20T17:11:44.136Z' }}

### Implementation of Mamba in one file of PyTorch

#### [Submission URL](https://github.com/johnma2006/mamba-minimal) | 391 points | by [johnma2006](https://news.ycombinator.com/user?id=johnma2006) | [107 comments](https://news.ycombinator.com/item?id=38708730)

A developer named johnma2006 has created a simplified and minimalist implementation of Mamba in PyTorch. Mamba is a linear-time sequence modeling architecture introduced by Albert Gu and Tri Dao. This implementation aims to provide equivalent numerical output as the official implementation for both forward and backward pass. Although it does not prioritize speed optimizations like the official implementation, it emphasizes readability and simplicity. The implementation does not include proper parameter initialization, but this can be added without sacrificing readability. You can check out the demo.ipynb file to see examples of prompt completions. So if you're interested in exploring Mamba in a more straightforward way, this implementation might be worth checking out.

The discussion on the submission starts with a user praising the library mentioned in the submission and mentions other libraries like EgBERT and MPT which offer support for TorchScript JIT and PyTorch. Another user appreciates the concept of the library and mentions that they have tried a similar implementation by Hugging Face and finds the API level abstraction beautiful. 
A user points out that Mamba does not prioritize speed optimizations but focuses on simplicity and readability. Another user mentions that they are interested in Mamba's implementations and that Fortran can be used as a low-level compiled language for scientific code wrapped in libraries like PyTorch and Numpy. The discussion then goes into debating the benefits of using Fortran and its growing adoption. 
One user talks about the potential of Mamba for sequence modeling beyond transformers and mentions other related models like S4, H3, and Monarch. They also discuss the potential applications of Mamba, including reduced computational effort and faster inference times. Another user adds that Mamba can be competitive in training smaller-sized models.
The conversation then shifts to discussing the difficulties of implementing Mamba and the advantages it offers in compressing context and non-dependent state variables. The topic of attention quadratics and their applications in Mamba and related models is also brought up. 
Users discuss the relevance of Mamba in relation to other models like RNNs and transformers, as well as the challenges of dealing with long-context length. The discussion also touches upon the potential of Mamba for model compression and efficient training. 
One user brings up a video that explains the paper in more detail, while another user mentions the importance of considering the computation parameters and the potential memory constraints in training and inference. They also discuss the use of minimal testing and the requirements for efficient data handling. 

Finally, users share resources such as videos and papers for further understanding and mention their excitement about the development of Mamba.

### High-Speed Large Language Model Serving on PCs with Consumer-Grade GPUs

#### [Submission URL](https://github.com/SJTU-IPADS/PowerInfer) | 380 points | by [dataminer](https://news.ycombinator.com/user?id=dataminer) | [79 comments](https://news.ycombinator.com/item?id=38708585)

SJTU-IPADS has developed PowerInfer, a powerful tool that enables high-speed serving of large language models on PCs equipped with consumer-grade GPUs. With 3k stars and 120 forks on GitHub, PowerInfer is gaining popularity in the developer community. The tool is licensed under the MIT license, making it accessible for commercial and open-source projects alike. PowerInfer leverages the computational capabilities of consumer-grade GPUs to deliver fast and efficient language model serving, opening up new possibilities for natural language processing tasks.

The team behind PowerInfer has put in significant effort to optimize the codebase and provide detailed documentation. They have also included several examples to help developers get started quickly. Additionally, frequent updates and bug fixes ensure that PowerInfer stays up to date with the latest advancements in language model serving. The tool is compatible with popular programming languages and frameworks, making it versatile and easy to integrate into existing projects.

PowerInfer has garnered positive feedback from the developer community, with users praising its performance and ease of use. It offers a cost-effective solution for serving large language models, eliminating the need for expensive hardware infrastructure. Whether you're building chatbots, recommendation systems, or language translation services, PowerInfer is a tool worth exploring. To learn more and get started with PowerInfer, visit their GitHub repository.

The discussion on the submission about PowerInfer: High-speed Large Language Model Serving on PCs with Consumer-grade GPUs touches on various topics.

- One user mentions that ReLU activation functions can cause problems in language models and suggests using alternative activation functions like SwiGLU.
- Another user raises the potential legal implications in the USA and EU when it comes to regulating language models and their computational requirements.
- A discussion emerges about the potential harmful effects of mobile games and advertising, with some users expressing concerns about addiction and privacy.
- There is a debate about the benefits and drawbacks of regulations in the technology industry, with some arguing that regulation stifles competition while others emphasize the need for consumer protection.
- Users discuss the performance and compatibility of PowerInfer, with some sharing their experiences with running language models on different GPUs and processors.

Some users also engage in discussions around specific technical details and benchmarks, as well as sharing links to related resources and YouTube videos.

### An AI that learns about chemical reactions and designs a procedure to make them

#### [Submission URL](https://new.nsf.gov/science-matters/meet-coscientist-your-ai-lab-partner) | 131 points | by [geox](https://news.ycombinator.com/user?id=geox) | [45 comments](https://news.ycombinator.com/item?id=38711174)

An artificial intelligence-driven system called "Coscientist" has successfully planned, designed, and executed complex chemical reactions in a matter of minutes. Created by a research team from Carnegie Mellon University, Coscientist used large language models and various software modules to autonomously learn about Nobel Prize-winning chemical reactions and replicate them in a laboratory setting. The AI's capabilities could potentially help increase the pace and number of scientific discoveries, as well as improve the replicability and reliability of experimental results.

The discussion around the submission revolves around several topics. 
One user expresses interest in using the ChatGPT API for genome annotation and designing experiments using CRISPR technology. Another user comments on the potential applications of machine learning in chemistry, particularly in the field of drug discovery. There is a debate about the validity and reliability of using large language models (LLMs) like ChatGPT for scientific research. Some users express concerns about the lack of proper attribution and the need for further peer-reviewed research. Others argue that LLMs can be useful in generating insights and accelerating scientific discovery.
There is also a discussion about the limitations and challenges of using AI in chemistry and the need for more independent verification. One user points out that the Coscientist AI system is designed to carry out physical actions in the lab and corrects its mistakes, making it more than just a text-based AI. However, skepticism remains about relying on information from sources like Wikipedia and the potential risks associated with AI-generated results.
There are also tangential discussions about the potential impact of AI on patent applications and the reliability of AI-generated data in the field of chemistry.

Overall, the discussion highlights both the potential benefits and limitations of AI in scientific research, with some users expressing optimism about the possibilities and others calling for caution and further scrutiny.

### IBM demonstrates 133-qubit Heron

#### [Submission URL](https://www.tomshardware.com/tech-industry/quantum-computing/ibm-demonstrates-useful-quantum-computing-within-133-qubit-heron-announces-entry-into-quantum-centric-supercomputing-era) | 115 points | by [rbanffy](https://news.ycombinator.com/user?id=rbanffy) | [61 comments](https://news.ycombinator.com/item?id=38708185)

IBM has made significant advancements in quantum computing at its Quantum Summit 2023 event. The company unveiled the 133-qubit Heron Quantum Processing Unit (QPU), its first utility-scale quantum processor, as well as the Quantum System Two, a quantum-specific supercomputing architecture. These cutting-edge devices push the boundaries of quantum computing, but further improvements are needed to overcome the plateau of understanding in quantum technology. IBM also announced breakthroughs in noise reduction algorithms and algorithmic improvements that reduce the number of qubits required for certain calculations. These advancements pave the way for a future of quantum-centric supercomputing. IBM's roadmap now focuses on scalability and qubit quality, with plans to reach 1 billion operationally useful quantum gates by 2033. The company aims to harness the power of quantum computing for tasks that are currently impossible with classical hardware.

The discussion on Hacker News revolves around various aspects of IBM's advancements in quantum computing. Some users are skeptical about the practicality and impact of quantum computing in the near term, while others highlight the potential advancements in encryption and drug discovery. There is also some discussion about the quality of the article and the writing platform used. Additionally, users point out that the AI-generated summary lacks clarity and coherence, suggesting the need for improvements in natural language processing.

### Identifying and eliminating CSAM in generative ML training data and models

#### [Submission URL](https://purl.stanford.edu/kh752sm9123) | 37 points | by [pulisse](https://news.ycombinator.com/user?id=pulisse) | [26 comments](https://news.ycombinator.com/item?id=38711135)

Researchers at Stanford have conducted a study examining the presence of child sexual abuse material (CSAM) in generative machine learning training data and models. The study focused on the LAION-5B dataset, which was used to train the popular Stable Diffusion series of models. Using a combination of perceptual hash matching, cryptographic hash matching, k-nearest neighbors queries, and machine learning classifiers, the researchers were able to detect hundreds of instances of known CSAM in the training set. They also discovered new candidates that were subsequently verified by external parties. The study provides recommendations for mitigating this issue, including altering existing models and hosting models trained on the LAION-5B dataset. This research highlights the importance of identifying and eliminating CSAM in machine learning training data to prevent the generation of explicit adult content.

The discussion on Hacker News regarding the Stanford study on the presence of CSAM in generative machine learning training data and models covered various topics and perspectives.
- Some users raised concerns about the legal implications of the study and the potential for censorship. They mentioned cases where CSAM filters were used as a means for political control and expressed the view that this article could be seen as opportunistic.
- Other users highlighted the importance of identifying and eliminating CSAM in machine learning training data, emphasizing the need for public datasets to address this issue. They mentioned that machine learning models can inadvertently generate explicit content and that efforts should be made to remove such content from training sets.
- There were discussions about the limitations of current legislation and enforcement in addressing the issue of CSAM. Some users argued that the consequences of AI-generated CSAM are significant for victims and the justice system. However, others pointed out that regulating machine-generated content is challenging and may require a nuanced understanding of regulatory frameworks.
- One user raised concerns about the training process of the models, suggesting that the training data should be modified to prevent the generation of explicit adult content.
- Another user shared their experience moderating content and provided examples of AI-generated CSAM that they had come across, highlighting the challenges in distinguishing between harmful and innocuous content.
- There were discussions about the methodology used in the study, with users noting that the LAION dataset contained a significant number of CSAM images and that it was compiled from various mainstream sources known to host such content.
- Some users expressed concerns about the potential privacy and ethical implications of using machine learning models trained on datasets containing CSAM.
- The issue of child victims of sexual abuse and the need to protect them was raised, with some users emphasizing the importance of preventing re-victimization through the distribution of CSAM and the need for agencies to make efforts to detect and remove such content from the internet.

Overall, the discussion revolved around the ethical, legal, and technical challenges associated with detecting and preventing CSAM in machine learning training data and the potential impact on victims.

### Show HN: Easily train AlphaZero-like agents on any environment you want

#### [Submission URL](https://github.com/s-casci/tinyzero) | 79 points | by [s-casci](https://news.ycombinator.com/user?id=s-casci) | [21 comments](https://news.ycombinator.com/item?id=38707475)

TinyZero is a tool that allows you to easily train AlphaZero-like agents on any environment you want. It provides a framework where you can add new environments, models, and agents to train your own AI. The process involves defining the methods specific to your environment, such as resetting the environment, taking actions, and getting game results. Similarly, you can add custom models and agents. The models should have methods to compute values and policies, while the agents should have methods to calculate values and policies for the game. TinyZero also supports wandb logging and GPU acceleration. Overall, TinyZero provides a flexible and customizable platform for training AI agents using the AlphaZero algorithm.

The discussion around the submission of TinyZero on Hacker News mainly focused on different aspects and related projects.
One commenter pointed out that the licensing details were missing from the repository. Another user acknowledged this observation and thanked them for catching that issue.
There was also a discussion about Game Description Language (GDL), where a user mentioned a project that used GDL for describing games and asked if TinyZero supports it. Another user replied that they couldn't find any relevant links but mentioned that GDL is taught in a Stanford course on General Game Players.
A user raised the topic of modifying existing environments and interfaces, suggesting that it should not be difficult and that they could submit a pull request to address it. Another user inquired if there are any formal Python libraries that support GDL. In response, someone mentioned an implementation of GDL in a Python library called pyggp.
The performance and scalability of TinyZero were also discussed, with one user mentioning that it is yet to be confirmed how well it performs compared to AlphaZero. They also noted that training multiple agents at scale may require resources that are not readily available.
Various other reinforcement learning libraries and frameworks were mentioned in the discussion, such as OpenAI's Gym, TensorFlow, TF-Agents, ReAgent, Meta, DeepMind's OpenSpiel, and Amazon SageMaker RL.
There was a question about the behavior of the get_legal_actions function, and a user asked what to expect from it. Another user replied that the expectation is that it returns a list of legal actions, but its behavior may depend on the specific implementation.
A user expressed their intention to try TinyZero for playing Carcassonne, a popular board game, and another user encouraged them to submit a pull request.
The discussion also touched on the handling of games with incomplete information and complex variants. Some related articles and concepts, such as ReBeL, BetaZero, ExIt-OOS, and Player Games, were mentioned. The limitations of traditional AlphaZero were discussed, and some users recommended looking into different variations, such as MuZero.

### Mercedes Gets Approval for Turquoise Automated Driving Lights

#### [Submission URL](https://jalopnik.com/mercedes-turquoise-automated-driving-lights-level-3-1851110043) | 23 points | by [Stratoscope](https://news.ycombinator.com/user?id=Stratoscope) | [10 comments](https://news.ycombinator.com/item?id=38706072)

Mercedes-Benz has become the first automaker to gain approval to sell a Level 3 automated driving system in the United States. The Drive Pilot system allows drivers to take their hands off the wheel and eyes off the road in traffic jam situations up to 40 mph, allowing for activities such as reading, watching movies, or using a cellphone. To indicate to other motorists and law enforcement that the Level 3 system is active, Mercedes has received permit approval for turquoise-colored exterior marker lights. The color was chosen by SAE and will be used by other brands as well. The goal is to improve road safety and public acceptance for automated driving. California and Nevada are the only states where Drive Pilot is currently allowed, but Mercedes plans to slowly roll out the system in other states as regulations allow.

The discussion on this submission primarily revolves around the use of turquoise-colored external marker lights to indicate that the Level 3 automated driving system is active. Some users express concerns about the choice of color, suggesting that a different color may have been more suitable or that it could be confusing for other drivers. Others debate the visibility and effectiveness of different colored lights at night. One commenter discusses the benefits of additional information provided by the lights, while others question whether the use of different colors could cause further confusion on the roads. One user comments on the marketing reasons behind the choice of color, while another user shares an anecdote about a similar situation with Tesla's autopilot system. Lastly, there is a comment about law enforcement potentially not pulling over drivers who are seen watching movies while using the automated system.

### Rite Aid banned from using AI facial recognition for five years

#### [Submission URL](https://www.ftc.gov/news-events/news/press-releases/2023/12/rite-aid-banned-using-ai-facial-recognition-after-ftc-says-retailer-deployed-technology-without) | 227 points | by [commoner](https://news.ycombinator.com/user?id=commoner) | [80 comments](https://news.ycombinator.com/item?id=38704830)

Rite Aid, a major retail pharmacy chain in the US, has been banned from using facial recognition technology for surveillance purposes after the Federal Trade Commission (FTC) found that the company deployed the technology without reasonable safeguards. The ban will last for five years. The FTC alleged that Rite Aid's facial recognition technology falsely tagged consumers, particularly women and people of color, as shoplifters. The company will be required to implement comprehensive safeguards to prevent harm to consumers and discontinue using the technology if potential risks cannot be controlled. Additionally, Rite Aid must implement a robust information security program to address previous charges of inadequate oversight of its service providers. The FTC highlighted the importance of preventing the misuse of biometric information and protecting consumers from unfair data security practices. Rite Aid's actions subjected consumers to embarrassment, harassment, and other harm, according to the FTC's complaint. The company did not inform consumers about the use of the technology and discouraged employees from revealing such information. Employees acted on false positive alerts, leading to confrontations with customers and accusations of shoplifting or other wrongdoing. The FTC also stated that Rite Aid's actions disproportionately impacted people of color. The company had contracted with two companies to create a database of images of individuals believed to have engaged in criminal activity, but the system generated thousands of false-positive matches. The FTC accused Rite Aid of failing to consider and mitigate potential risks to consumers, test the accuracy of the technology, prevent the use of low-quality images, monitor or test the accuracy of the technology after deployment, and adequately train employees. This case underscores the FTC's vigilance in protecting the public from unfair biometric surveillance and data security practices and follows their warning about monitoring the use of facial recognition technology.

The discussion on this submission covers various aspects of the Rite Aid facial recognition case and related topics. Some commenters express concern about the impact of facial recognition technology, discussing issues such as the potential for misidentifying individuals based on race and the sudden notice of security cameras using the technology. Others bring up examples of businesses using facial recognition systems and the problem of false positives. The commenters also discuss the FTC's previous charges against Rite Aid regarding inadequate oversight of service providers and the need for personal accountability in breaking laws. There is further discussion about the misuse of surveillance cameras for theft prevention purposes and the experience of safety threats in stores.

