## AI Submissions for Sat Aug 26 2023 {{ 'date': '2023-08-26T17:09:52.665Z' }}

### When Kraftwerk Issued Their Own Pocket Calculator Synthesizer (2019)

#### [Submission URL](https://www.openculture.com/2019/06/when-kraftwerk-issued-their-own-pocket-calculator-synthesizer.html) | 30 points | by [layer8](https://news.ycombinator.com/user?id=layer8) | [9 comments](https://news.ycombinator.com/item?id=37271644)

German electronic band Kraftwerk released their eighth studio album, Computer World, in 1981, right at the cusp of the computer revolution. The album's first single, "Pocket Calculator," featured the Casio fx-501P programmable calculator as one of the instruments used in its recording. To promote the song and allow fans to play Kraftwerk hits on their own calculators, the band commissioned a special calculator from Casio that could play music. This calculator was a modified version of Casio's VL-80 model, which was also a musical synthesizer. Today, 40 years after the release of Computer World, Kraftwerk continues to perform their music around the world. With the anniversary approaching, it might be time for the calculators to make a comeback on stage.

The discussion around the submission includes several comments and links related to Kraftwerk and their music. One user mentions that Casio calculators from the 1970s had some musical capabilities and shares a number (951) that produces a random melody when taking the square root. Another user shares a direct link to Kraftwerk's song "Pocket Calculator" and a cover of their song "The Robots" by the Balanescu Quartet. Another comment highlights the privilege of attending Kraftwerk concerts and recommends experiencing their repetitive perfection. They mention a successful backstage meeting with the Balanescu Quartet. Links to regional live performances and performances in different languages are also shared, including Japanese and English versions. Additionally, a comment suggests a reference to a Teenage Engineering product called "Pocket Operator," which is a tiny synthesizer with musical capabilities. Another user makes a joke about being a "Pocket Calculator" themselves.

### Cody â€“ The AI that knows your entire codebase

#### [Submission URL](https://about.sourcegraph.com/cody) | 162 points | by [adocomplete](https://news.ycombinator.com/user?id=adocomplete) | [59 comments](https://news.ycombinator.com/item?id=37277722)

Meet Cody BETA, the AI coding assistant that knows your entire codebase. Cody can answer code questions and even write code for you by reading your codebase and the code graph. To get started, you can sign up for free access on their website and install the Cody app BETA, a lightweight desktop version of Sourcegraph. Cody is compatible with various IDEs, such as VS Code, IntelliJ, Neovim, and Emacs. 

Cody offers a range of features to make your coding experience more efficient. It can provide code explanations, explaining what code is doing in conversational language. It can also analyze code blocks for code smells, potential bugs, and unhandled errors, pointing out issues and providing suggestions for improvement. Cody can summarize recent code changes, generate unit tests, suggest code completions while you code, and even translate code between programming languages. 

Additionally, Cody can help with code navigation, finding functions and components in your codebase, and tracking references to specific functions. It can also generate code based on your codebase's context and style, from boilerplate code to API resolvers. If you need help with debugging, Cody can assist with that too. It can even generate documentation and write code on your request. 

Developers who have tried Cody are impressed with its capabilities. Some have found it helpful in navigating code and finding solutions quickly, saving them hours of searching through documentation. Others have praised its ability to ease the process of code auditing. And for those who struggle with naming variables, Cody's feature to improve variable names is like a dream come true.

Cody is available for personal use as well as for enterprise teams with private codebases. You can sign up on their website to get access and explore the pricing and plans for Cody Enterprise. So if you're looking for an AI coding assistant that can enhance your coding productivity, give Cody BETA a try!

The discussion about Cody BETA on Hacker News is largely positive, with developers sharing their experiences and opinions on using the AI coding assistant. Some users have found Cody to be helpful in navigating code and finding solutions quickly, saving them time and effort. Others have praised its ability to ease the process of code auditing and improve variable naming. 

There is also a comparison with GitHub Copilot, with some users expressing that they prefer Cody over Copilot, citing specific features or examples of where Cody performs better. However, one user points out that they would like to see some examples of Copilot failing and how Cody handles those cases. 

There are also discussions about the user interface and user experience of Cody, with some users mentioning missing or inconsistent buttons. One user points out that they had trouble getting Cody to recognize their current code repository correctly. 

In terms of privacy, there are concerns about the third-party dependencies used by Cody and the transparency of data usage. One user mentions the need for transparency and confirmation of any downstream impacts related to partnerships with entities like Anthropic and OpenAI.

Some users also discuss their experiences with other coding tools, such as Sublime Text and Visual Studio Code, and their advantages or limitations compared to Cody.

Overall, the feedback on Cody BETA is generally positive, with developers appreciating its capabilities and potential to improve coding productivity.

### Show HN: TRS-GPT â€“ ChatGPT client/server for the TRS-80

#### [Submission URL](https://druid77.github.io/trs-gpt/) | 94 points | by [druid77](https://news.ycombinator.com/user?id=druid77) | [19 comments](https://news.ycombinator.com/item?id=37276026)

The author of this submission shares their childhood dream of interacting with a computer in an intelligent way, particularly inspired by movies like War Games. Fast forward to 2023, and they acquire a TRS-80 Model III computer from 1981. After restoring the keyboard and adding a FreHD module for storage, they realize they need internet connectivity to achieve their goal. They discover the TRS-IO, a solution that allows for both program storage and internet access. After configuring it, they are able to connect to the internet and explore basic programs that interact with WHOIS servers. To pass queries to OpenAI, the author sets up an AWS Lambda function, but encounters issues with API Gateway only supporting HTTPS connections. They decide to implement a simple EC2 server running a Python program to listen on the same port as the WHOIS server. Finally, the author achieves their dream of interacting with an intelligent computer on their TRS-80 Model III.

The discussion on this submission covers various topics related to the TRS-80 Model III computer and the author's efforts to achieve internet connectivity and interact with it in an intelligent way.

- One commenter suggests using the TRS-IO, an interface that allows for program storage and internet access, which enables the author to send and receive network requests with WHOIS servers using BASIC programs.
- Another commenter shares their nostalgic appreciation for the TRS-80 Model III and their interest in reading articles and watching YouTube videos about the computer.
- A brief discussion about playing chess on the TRS-80 Model III occurs, referencing a line from the movie War Games.
- A commenter mentions running a C++ program on the TRS-80 Model III and discusses the computer's CPU, addressing its 48k of physical memory and the upgrade options available.
- Someone shares their own experience experimenting with a TRS-80 CoCo computer and attempting to write code to interface with modern digital devices. They also mention the challenge of translating code from a TRS-80 file format to a format that could be read by modern systems.
- Another commenter suggests using GPT, a language model, to write BASIC code and mentions the model's ability to crawl online sources for training materials.
- Positive feedback is given to the author for their project.
- Some commenters express their interest and appreciation for the TRS-80 Model III and the author's achievements.
- One commenter shares a link to an article about a modern speech synthesis model as a potential replacement for the TRS-80's speech synthesizer module.
- A conversation emerges about the TRS-IO module's compatibility with Linux and the possibility of using a serial terminal to achieve connectivity.
 
Overall, the discussion includes appreciation for the nostalgia-inducing TRS-80 Model III and provides suggestions, tips, and ideas related to internet connectivity and programming on the computer.

### Deep Neural Nets: 33 years ago and 33 years from now (2022)

#### [Submission URL](http://karpathy.github.io/2022/03/14/lecun1989/) | 275 points | by [gsky](https://news.ycombinator.com/user?id=gsky) | [90 comments](https://news.ycombinator.com/item?id=37268610)

A recent blog post by Andrej Karpathy discusses the historical significance of the Yann LeCun et al. (1989) paper on handwritten zip code recognition. This paper is believed to be the earliest real-world application of a neural network trained end-to-end with backpropagation. Despite its age, the paper reads remarkably modern, covering topics such as dataset creation, neural network architecture, loss function, optimization, and reporting classification error rates. To explore the progress made in deep learning, Karpathy decided to reproduce the paper's results using PyTorch. He implemented the original network, which was written in Lisp, and successfully replicated the reported error rates. However, due to the loss of the original dataset, a simulated dataset based on MNIST had to be used. Karpathy also notes some ambiguities in the paper, such as the weight initialization scheme and the connectivity structure between layers. He concludes the post by reflecting on how much progress has been made in deep learning over the past 33 years and the potential for further improvement in the future.

The discussion on the Hacker News submission revolves around several key points. 

1. Energy Efficiency: One user points out the difference in energy consumption between the original 1989 system and Karpathy's modern implementation. The user notes that the energy efficiency of neural networks is an important consideration in evaluating their performance, while another user adds that measuring energy consumption accurately can be challenging.

2. Inference vs. Training Costs: There is a discussion about the difference between the costs of training and inference in contemporary neural networks. The conversation suggests that while training costs may be high, inference costs can be relatively low.

3. Moore's Law: One user disputes the claim that the 30,000-fold improvement in performance over 33 years is not in line with Moore's Law, which predicts a doubling of performance every two years. They explain the concept of compound annual growth rate (CAGR) and argue that the improvement is reasonable within that framework.

4. Hardware Limitations: There is a discussion about the limitations of hardware in training neural networks. It is mentioned that GPUs have limited RAM, and expanding memory can be costly.

5. Critique of Future Predictions: Some users express skepticism about making predictions about the future of artificial intelligence based on historical trends. They argue that the current state of AI may not be indicative of future breakthroughs and that extrapolating from past performance may not be accurate.

6. Fundamental Changes: A user highlights the potential for fundamental changes in AI models and approaches in the future, mentioning synthetic data generation, filtering, and other advancements. They suggest that these changes may lead to revolutionary breakthroughs.

7. Technological Progress: The discussion also touches on the progress made in self-driving cars and AGI, with some users expressing their excitement about advancements in the field and others pointing out the challenges and limitations.

8. Practical Design Considerations: One user reflects on their own experience in machine learning projects and emphasizes the importance of practical design considerations. They note that while there have been significant advancements in machine learning, there are still areas for improvement and mastery.

Overall, the discussion covers a range of topics related to energy efficiency, technological progress, future predictions, and practical considerations in machine learning and AI.

### Never-Ending Learning of User Interfaces

#### [Submission URL](https://arxiv.org/abs/2308.08726) | 54 points | by [PaulHoule](https://news.ycombinator.com/user?id=PaulHoule) | [22 comments](https://news.ycombinator.com/item?id=37275331)

Researchers from various institutions have developed an app crawler called the Never-ending UI Learner to improve the training of machine learning models for user interfaces (UIs). These models are used to make apps more accessible, easier to test, and automate certain tasks. Currently, most models rely on datasets that are collected and labeled by human crowd-workers, which can be expensive and prone to errors. The Never-ending UI Learner automatically installs real apps from a mobile app store and crawls them to discover new and challenging training examples. This approach has been successful, with the app crawler performing over half a million actions on 6,000 apps to train three computer vision models: tappability prediction, draggability prediction, and screen similarity. This research offers a promising solution to enhance the training of UI models and improve the user experience of apps.

The discussion surrounding the submission on Hacker News includes various perspectives on user interface (UI) design and the challenges involved in improving it. Some commenters highlight the complexity of UI design and the difficulties in achieving consistency across different platforms. They mention the need for standardized accessibility and the slow and expensive process of user testing and iterating on UI designs.

Others express frustrations with specific aspects of UI, such as password requirements and the need to constantly relearn UIs due to design changes. There is also discussion about the pros and cons of different UI design approaches, including flat design, gradient textures, and the use of ribbons.

In addition, some commenters bring up the challenges of legacy systems and the resistance to change in certain industries. They argue that if something is working well, there may be little incentive to transition to new systems or redesign UIs.

Overall, the discussion highlights the complexities and ongoing debates surrounding UI design and the challenges faced by developers and designers in creating effective and accessible user experiences.

### How to Search on Encrypted Data (2013)

#### [Submission URL](https://esl.cs.brown.edu/blog/how-to-search-on-encrypted-data-introduction-part-1/) | 74 points | by [gordian-not](https://news.ycombinator.com/user?id=gordian-not) | [11 comments](https://news.ycombinator.com/item?id=37272217)

Searching on encrypted data has become a significant problem in the field of security and cryptography. With search being the primary way we access our data and our increasing reliance on third-party providers, the need to search on encrypted data has become more crucial. The problem has attracted attention from various fields such as computer science, databases, security, cryptography, privacy, as well as industry and governments.

The concept of searching on encrypted data was first explored by Song, Wagner, and Perrig in a paper from 2001. This groundbreaking paper demonstrated the possibility of searching on encrypted data, which initially seemed impossible. However, prior research on oblivious RAMs and secure two-party computation provided solutions to this problem, albeit less efficiently.

In the past few decades, considerable progress has been made in the field of encrypted search. We now have a solid understanding of the subject, efficient solutions, and the potential for deployment. However, as with any cryptographic technology, there are tradeoffs to consider, and this series aims to explore these tradeoffs.

The setup for searching on encrypted data involves two parties: a client and a server. In the setup phase, the client encrypts the documents and creates an encrypted database (EDB). During the search phase, the client sends a token to the server containing the keyword it wants to search for. The server then utilizes the token with the EDB to identify the encrypted documents to send back.

There are six different ways to search on encrypted data, each based on different cryptographic primitives such as property-preserving encryption, functional encryption, fully-homomorphic encryption, searchable symmetric encryption, oblivious RAMs, and secure two-party computation. Throughout the series, these approaches will be explored, comparing their efficiency, security, and functionality tradeoffs.

Although there are still aspects of this problem that are not fully understood, such as the tradeoffs between efficiency and security, the series will delve into these topics in future posts.

Source: https://blog.cryptographyengineering.com/2013/10/06/how-to-search-on-encrypted-data-introduction-part-1/

The discussion on this submission primarily consists of comments that provide additional clarification and insights into the topic of searching on encrypted data.

- One user mentions that they have little experience in cryptography but highlights that searching for predetermined keywords poses a significant challenge.

- Another user corrects a minor error regarding the publication year of the Song, Wagner, and Perrig paper, stating that it was published in 2000, not 2001.

- A user shares a link to an article about Private Information Retrieval (PIR), which can help in retrieving information from databases without revealing the server's interests. They also mention its applications in web privacy and secure messaging.

- One user wonders about Zero-Knowledge proofs (ZK) and notes that they can solve the problem of providing proof without revealing compromising secret information.

- A user provides a high-level explanation of property-preserving encryption, one of the six methods mentioned in the original submission. They outline the idea of encrypting each keyword individually and sending encrypted messages to the server for retrieval.

- Another user requests a more explicit explanation in plain English with simple examples for the six methods discussed in the original submission.

- Several users engage in a discussion about the use of formal notation to convey cryptographic constructions effectively and compare it to explaining concepts using analogies.

- Some users express their agreement and desire for further explanations and details on the topic.

Overall, the discussion provides additional context, corrects minor errors, and expresses interest in more detailed explanations of the different methods for searching on encrypted data.

### FaceFusion: Next generation face swapper and enhancer

#### [Submission URL](https://github.com/facefusion/facefusion) | 71 points | by [pocketarc](https://news.ycombinator.com/user?id=pocketarc) | [35 comments](https://news.ycombinator.com/item?id=37270648)

Introducing FaceFusion, the next generation face swapper and enhancer! This powerful software allows you to seamlessly swap and enhance faces in images and videos. With a helpful Discord community to guide you through the installation process, FaceFusion is designed for those with technical skills. 

You can customize your FaceFusion experience by selecting from available frame processors and UI layouts. And with options like preserving frames per second, retaining temporary frames, and omitting audio from the target, you have full control over the final output.

But FaceFusion comes with a disclaimer. While it aims to contribute to the AI-generated media industry and assist artists, the developers are aware of the potential unethical applications and have implemented measures to prevent misuse. They have built-in checks to prevent the software from working on inappropriate media. Rest assured, they are committed to developing the project responsibly and in adherence to law and ethics.

So, if you're looking for a powerful face-swapping and enhancement tool, give FaceFusion a try and unlock the potential of your creative projects!

The discussion on the submission started with users discussing the practicality and limitations of face-swapping tools. Some users mentioned that face-swapping tools often produce unnatural results or have non-essential features. Others shared their experiences using similar tools and highlighted the entertainment value of creating funny and silly memes.

A few users raised concerns about unethical uses of such tools, including manipulating images of others without consent, using the tool for adult content, or generating inappropriate media. Some users mentioned the need for strict regulation and ethical considerations in AI-generated media. They also brought up examples from movies and literature that explore similar themes, such as the Black Mirror series and various dystopian stories.

In response to a comment about an AI model being disabled for NSFW content, another user mentioned an improved model called Insightface and shared GitHub links for reference. The discussion then took a philosophical turn, discussing stories and concepts related to artificial intelligence, constitutional amendments on government surveillance, and the potential risks of birth implants suppressing intelligence and beauty standards.

One user expressed their concern about the conflation of disturbing imagery, hypersexualized subjects, and the responsible use of AI technology. This led to a conversation about personal convictions and the need to provide context and clarification when discussing these topics. The user emphasized that their involvement in the project was based on their convictions and that they did not find the statements of others relevant or truthful.

Another user criticized the negative comments about the developer and emphasized the impressive nature of the project. They shared an image to showcase the capabilities of the face-swapping tool and mentioned that the majority of features added to the software were based on user requests.

The discussion ended with a message mentioning that a comment was taken out of context and that the images mentioned were hosted and deleted.

### Interpretable graph neural networks for tabular data

#### [Submission URL](https://arxiv.org/abs/2308.08945) | 75 points | by [PaulHoule](https://news.ycombinator.com/user?id=PaulHoule) | [5 comments](https://news.ycombinator.com/item?id=37269376)

Researchers have proposed a new approach called IGNNet (Interpretable Graph Neural Network) that aims to make graph neural networks (GNNs) more interpretable when applied to tabular data. GNNs have become popular for handling tabular data due to their ability to capture feature interactions through representation learning. However, the models produced by GNNs are often considered black boxes, making it challenging to understand how the predictions are computed. IGNNet addresses this issue by constraining the learning algorithm to produce an interpretable model that shows the exact computation process from the original input features. The researchers conducted a large-scale empirical investigation and found that IGNNet performed on par with state-of-the-art machine learning algorithms for tabular data, such as XGBoost, Random Forests, and TabNet. Furthermore, the explanations obtained from IGNNet were aligned with the true Shapley values of the features without incurring additional computational overhead. This approach holds promise for improving the interpretability of GNNs in real-world applications.

The discussion surrounding the submission on Hacker News includes various points of interest. 

One user, "PaulHoule," commented that significant regional articles related to the field of research don't handle tabular data and mentioned that there are other things that graph neural networks (GNNs) can't handle, similar to ChatGPT-like models. 

Another user, "wstrnr," added that TabPFN (Transformers for Tabular Data) had similar performance to models like XGBoost, Catboost, LightGBM, KNN, SAINT, Reg Cocktail, Autogluon, and Auto-sklearn. They also noted the requirement of only 5 minutes to reach 20% ROC (Receiver Operating Characteristic) compared to TabPFN1. Additionally, they shared links to related papers such as "TabPFN Transformer Solves Small Tabular Classification Problems Second 2022" and "Interpretable Graph Neural Networks for Tabular Data Aug 2023".

In response to the discussion, user "iAkashPaul" mentioned that Microsoft LIDA incorporates additional insights from tabular data sources.

Lastly, "Artgor" found the paper to be interesting and challenging.

### COBOL gets new life in the cloud thanks to Watsonx and AI

#### [Submission URL](https://www.silverliningsinfo.com/apps-services/ibm-watsonx-brings-cobol-cloud-era-ai) | 40 points | by [rmason](https://news.ycombinator.com/user?id=rmason) | [43 comments](https://news.ycombinator.com/item?id=37277191)

ðŸ“¢ Daily Digest: Top Stories on Hacker News ðŸ“¢

1. Title: "The Future of Self-Driving Cars: A Deep Dive into Autonomous Vehicle Technology"
   Points: 542, Comments: 187
   Summary: Buckle up for a fascinating deep dive into the mind-boggling world of self-driving cars. This enlightening article explores the latest advancements in autonomous vehicle technology, from perception and mapping to decision-making and control. Whether you're a tech enthusiast or simply curious about the future of transportation, this must-read will take you on an informative and exciting journey.

2. Title: "How to Secure Your Online Privacy: Tips and Tools"
   Points: 379, Comments: 98
   Summary: Concerned about protecting your online privacy? Look no further! This informative post dives into practical tips and powerful tools to safeguard your digital life. Discover techniques to protect your data, browse the web anonymously, and strengthen your online security. With these invaluable insights, you can take control of your privacy in an increasingly connected world.

3. Title: "The Art of Debugging: Techniques Every Programmer Should Master"
   Points: 285, Comments: 62
   Summary: Debugging is an essential skill in any programmer's toolkit, and this article delves into the art of debugging like never before. Discover a treasure trove of techniques, tips, and strategies to streamline the debugging process and become a debugging maestro. From debugging mindset to modern tools, this comprehensive guide will level up your debugging prowess and help you navigate those pesky bugs like a pro.

4. Title: "The rise of Headless CMS: What you need to know"
   Points: 197, Comments: 50
   Summary: Is the traditional CMS becoming obsolete? Dive into this thought-provoking post to explore the rise of headless CMS and its impact on web development. Unleash the power of decoupled content management systems and discover their potential to bring flexible and scalable solutions to the table. Whether you're a developer, content creator, or simply curious about the evolving landscape of CMS, this article offers valuable insights.

5. Title: "AI-Powered Image Recognition: Recent Advances and Future Possibilities"
   Points: 140, Comments: 34
   Summary: Step into the fascinating world of AI-powered image recognition with this captivating read. Explore the recent advances in computer vision and deep learning models that have propelled image recognition technology to new heights. From object detection to facial recognition, this article examines the current state of the art and delves into the potential future applications of this remarkable field.

That's all for today! Enjoy reading and stay informed about the latest happenings on Hacker News.

1. In the discussion about the article on COBOL programs, there is a mention of the importance of JCL and batch schedulers, as well as the use of RACF for mainframe security and porting applications.

2. One commenter is not surprised by the approach of using AI to convert COBOL code to Java, but mentions that there can be trust issues with the accuracy of the translation.

3. Another commenter shares their experience with using transpilers and suggests that AI could be useful for generating run-of-the-mill testing for testers working on COBOL projects.

4. A discussion ensues about the support for object-oriented programming in COBOL and the fashion trends in programming languages and frameworks.

5. One commenter shares a link to an article explaining the move of COBOL code to the JVM.

6. There is a mention of a programming theory thread discussing AI delivering tasks such as converting COBOL to Java.

7. A commenter mentions that replacing COBOL with Java on server systems is not a viable proposition due to reliability reasons and the successful translation of COBOL to Java. They also mention potential trouble with using AI, such as with Watson.

8. Several comments discuss the outsourcing of COBOL development to India and the high pay rates for COBOL developers in India.

9. There is a mention of experiences migrating COBOL projects using GnuCOBOL.

10. A commenter mentions that many COBOL developers in India do not touch modern software engineering practices and that there is limited expertise.

11. A discussion arises about the replacement of COBOL developers with AI and the high cost of COBOL maintenance.

12. There is a comment about the difficulty of getting COBOL developers and the focus on incentivizing experienced developers.

13. A commenter suggests that AI has replaced COBOL developers, but another commenter argues that AI is not sophisticated enough and programming decisions are made sideways in the industry.

14. There is a mention of the challenges in learning new systems and the push to hire experienced developers.

15. One commenter suggests that distressed staff is a result of the importance of critical systems and the need for regional people to maintain them.

16. A link to another COBOL thread on Hacker News is shared.

It seems that the discussion revolves around the use of AI to replace COBOL developers and the challenges and implications of such a move, as well as the role and ongoing importance of COBOL in critical systems. The outsourcing of COBOL development to India and the availability of expertise is also a topic of discussion.

