## AI Submissions for Sat Jun 08 2024 {{ 'date': '2024-06-08T17:10:53.741Z' }}

### LSP-AI: open-source language server serving as back end for AI code assistance

#### [Submission URL](https://github.com/SilasMarvin/lsp-ai) | 225 points | by [homarp](https://news.ycombinator.com/user?id=homarp) | [47 comments](https://news.ycombinator.com/item?id=40617082)

SilasMarvin's LSP-AI project is making waves with its open-source language server designed to enhance software engineering with AI-powered features, not replace human developers. This project aims to integrate with popular code editors like VS Code, NeoVim, Emacs, Helix, and more, providing completion with large language models and other AI capabilities. By centralizing AI functionality into a backend, LSP-AI simplifies plugin development, fosters collaboration, and ensures broad compatibility with any editor supporting the Language Server Protocol. With support for various backends like llama.cpp, OpenAI, and Mistral AI FIM, LSP-AI is future-ready, constantly evolving with new features on the roadmap. This innovative project is poised to revolutionize how developers interact with code editors and AI assistants.

The discussion on the Hacker News submission revolves around SilasMarvin's LSP-AI project, which is an open-source language server enhancing software engineering with AI features without replacing human developers. Some users shared their experience with installing the project, asking questions about it, and discussing potential integrations and improvements. There was also a comparison with other projects, such as Codestral Mistral and Llama CDR. The conversation touched on the integration of AI into coding workflows, challenges faced by developers, and suggestions for improving AI assistance tools like Copilot. Some users discussed the use of AI for code completion, workflow enhancement, and tool integration in various programming languages. Additionally, there were mentions of projects like Aider and discussions about the efficiency of AI assistants from companies like Jetbrains. Overall, the conversation highlighted the potential of LSP-AI and other AI-driven tools in revolutionizing how developers interact with code editors and AI assistants.

### Chat TUI for Ollama

#### [Submission URL](https://github.com/ggozad/oterm) | 34 points | by [lijunhao](https://news.ycombinator.com/user?id=lijunhao) | [3 comments](https://news.ycombinator.com/item?id=40619891)

Today on Hacker News, a new project called "oterm" by ggozad was trending. Oterm is a text-based terminal client for Ollama, offering an intuitive and simple terminal UI. With oterm, users can conduct multiple persistent chat sessions using models from Ollama or custom models. The tool provides features like customizing system prompt and parameters, easy model customization, and storage of chat sessions in a sqlite database. Users can install oterm using brew on MacOS or pip, and it requires the Ollama server to be running. The project is open source under the MIT License and supports various keyboard shortcuts for ease of use. Oterm also allows customization of models and system instructions during chat sessions.

If you're into Python, machine learning, or terminal applications, oterm might be an interesting tool to check out on GitHub with 786 stars and 40 forks.

The first comment seems to express a sentiment about text-based user interfaces (TUIs) and splitting windows in multiple sections. The second comment dives into the complexity of Ollama, noting challenges with vendor lock-in, compatibility with certain platforms, and integration issues with local Llamacpp products. Additionally, there is a reply suggesting a need for a complete string prompt and a non-proprietary API for the Ollama project.

### Evaluation of Machine Learning Primitives on a Digital Signal Processor

#### [Submission URL](http://www.diva-portal.org/smash/record.jsf?pid=diva2%3A1457863&dswid=-740) | 31 points | by [teleforce](https://news.ycombinator.com/user?id=teleforce) | [3 comments](https://news.ycombinator.com/item?id=40620401)

Today's top story on Hacker News is about the evaluation of machine learning primitives on a digital signal processor. The thesis explores the possibility of utilizing the digital signal processor as an alternative to specialized hardware for running machine learning algorithms on handheld devices. The study proposes memory management techniques and implementations for evaluating machine learning primitives like convolutional, max-pooling, and fully connected layers. It introduces new instructions for packing data and enhancing instruction pipelining, showing positive effects on implementation throughput. The findings suggest that convolutional and fully connected layers are well-suited for the processor, with considerations on kernel stride impacting hardware usage. Max-pooling layers, while not unsuitable, exhibit limitations in terms of hardware utilization. The study provides valuable insights into optimizing machine learning tasks on digital signal processors.

The discussion on the submission revolves around the research report evaluating machine learning primitives on a digital signal processor. One user, "jnnr," points out that the thesis lacks readability due to abbreviations. Another user mentioned that the research was sponsored by Mediatek, prompting a response from the initial user that they gathered that detail but found it to be probable.

### Intel CPUs run MINIX on them, in the Management Engine (2017)

#### [Submission URL](https://web.archive.org/web/20170828150536/http://blog.ptsecurity.com/2017/08/disabling-intel-me.html) | 15 points | by [tanelpoder](https://news.ycombinator.com/user?id=tanelpoder) | [5 comments](https://news.ycombinator.com/item?id=40620741)

The team at Positive Technologies has made a groundbreaking discovery, unveiling an undocumented mode that can disable Intel ME 11 after the hardware is initialized. While this finding sheds light on the inner workings of Intel's Management Engine, it comes with a fair warning about its risky nature that could potentially harm your computer. Intel ME, a microcontroller integrated into the Platform Controller Hub chip, has garnered interest from researchers worldwide due to its access to critical data on a computer. Despite the challenges posed by its proprietary nature, the researchers managed to unpack executable modules and delve into the software and hardware internals. The quest to disable Intel ME has long intrigued enthusiasts, with projects like me_cleaner attempting to strip down unnecessary components, albeit with limited success. By exploring Intel's Flash Image Tool and Flash Programming Tool, the team stumbled upon a mysterious field related to the U.S. National Security Agency's High Assurance Platform program, sparking further experimentation. This rare insight into Intel ME's workings marks the beginning of a series shedding light on its core functionality and the potential for mitigating security risks.

The discussion on the submission centers around various aspects related to operating systems and firmware:

- **h-v-rcknrll** discusses the specific characteristics and functionalities of MINIX 3 as a research OS, highlighting its microkernel architecture, similarities to NetBSD, and its use in research and teaching. The user also suggests considering Real-Time Operating Systems (RTOS) like the L4-family OS, including sel4, and INTEGRITY-178B OS, widely deployed in critical infrastructure.
- **shrbbl** mentions a common concern about the access to RAM devices in OS on shared networks.
- **tnlpdr** points out that current firmware includes a full-fledged printing system, highlighting the processes, threads, memory management, hardware drivers, andile system components involved.
- **bfrg** expresses a sentiment that rests on the idea that certain things can benefit a great deal from rest.
- **h-v-rcknrll** adds to the conversation by discussing how Rust can be helpful in addressing design, architecture, security concerns, and hardware support, as well as suggesting the relevance of Rust in formal verification for RTOS like sel4 and MINIX 3. The user also touches upon challenges in microkernel design, efficient Inter-Process Communication (IPC) handling, complex transactions, and the resolution that sel4 provides. Furthermore, the user delves into privileged events and transactions handling, emphasizing the need for locking privileged helpers to prevent security vulnerabilities, contrasting Linux's monolithic nature with flaws in design goals, simplicity, cost, and performance reasons.

### AMD's "Peano" – An LLVM Compiler for Ryzen AI NPUs

#### [Submission URL](https://www.phoronix.com/news/AMD-Peano-LLVM-Ryzen-AI) | 43 points | by [simonpure](https://news.ycombinator.com/user?id=simonpure) | [15 comments](https://news.ycombinator.com/item?id=40618880)

AMD has dropped an exciting surprise for the open-source community with the announcement of "Peano," a new LLVM compiler back-end for AMD/Xilinx AI engine processors. This project is focused on supporting the Ryzen AI SOCs, including the existing Phoenix and Hawk Point hardware, as well as the upcoming XDNA2 found in the Ryzen AI 300 series. The Peano project aims to make the Ryzen AI NPUs more useful under Linux by providing an open-source user-space codebase for compiler support. This move is significant for AMD as it complements their existing open-source XDNA Linux kernel driver for Ryzen AI hardware, which they are looking to upstream into the mainline Linux kernel. The Peano team, led by Stephen Neuendorffer of AMD/Xilinx, has already made the repository available on GitHub under Xilinx/llvm-aie.

AMD acknowledges the importance of this open-source compiler backend in accelerating the growth of the Linux ecosystem around Ryzen AI SoCs. The timing of this announcement, coinciding with Phoronix's 20th birthday, adds to the celebratory atmosphere for open-source and Linux hardware support advocates. While AMD's Ryzen AI journey may have had its delays compared to Intel's NPU Linux support, the Peano project marks a significant step forward in empowering developers to leverage AMD's AI accelerators within the Linux environment.

1. **yukIttEft** mentioned that AI Engine processors rely on xpsd-ppln VLIW processors where VLIW instruction bundles specify behavior. Functional units begin executing instructions in the processor pipeline regardless of dependencies between instructions, making scheduling instructions through compiler challenging.
2. **flknss** highlighted the importance of NPUs and their high-level APIs like DirectML for Windows and NNAPI for Android, comparing them to AMD's NPU compiler based on LLVM. The discussion touched upon the limited demands of NPUs and the involvement of CPU vendors in compilers.
3. **mtrngd** pointed out that previously programming AI tasks involved copying hundreds of gigabytes from SSD.
4. **gmby** commented on the size of the program being discussed, indicating it was relatively small compared to normal norms.
5. **user_7832** made a simple observation by saying "d srt thngs tl."
6. **lmstgtcght** stated that the LLVM fork is basically LLVM with a few modifications.
7. **yrg** shared information about Xilinx engineer Stephen Neuendorffer leading the Peano team, a backend fork for LLVM supporting Ryzen AI SoCs developed by AMD and Xilinx. They also provided a link to the GitHub repository for the project.
8. **ladyanita22** made a brief comment saying "Basically wrttn."
9. **jntywndrknd** expressed interest in non-ML applications and software compilation for things like DSP processors.
10. **Archit3ch** clarified the distinction between DSP processors and FPGAs based on hardware floating-point and faster FFTs on hardware.
11. **Neywiny** mentioned that AI engines support floating-point data processing in AMD Xilinx AI engines.
12. **lmstgtcght** recommended looking into IREE and MLIR in non-NV efforts related to MLIR in the industry.
13. **mtrngd** highlighted the performance comparison of NPUs and CPUs in handling vectorization tasks.

Overall, the discussion covered various aspects of the Peano project, ranging from the technical challenges of AI engine processors to the implications for software developers leveraging AMD's AI accelerators in the Linux environment.

### LLMs are not even good wordcels

#### [Submission URL](https://demian.ferrei.ro/blog/chatgpt-sucks-at-pangrams) | 7 points | by [epidemian](https://news.ycombinator.com/user?id=epidemian) | [3 comments](https://news.ycombinator.com/item?id=40615060)

Recently, the topic of pangrams came up among friends, sparking a quest to create self-enumerating pangrams using ChatGPT. Pangrams are phrases that contain every letter of the alphabet at least once, such as the famous example, "the quick brown fox jumps over the lazy dog."  ChatGPT-4o attempted to generate Spanish pangrams but struggled with missing letters like B, J, Ñ, P, Q, S, T, and X in its examples. Despite this, it managed to provide a correct well-known pangram upon correction, showcasing its language understanding capabilities. 

When tasked with creating a novel pangram, ChatGPT presented a quirky phrase, "El pingüino ñato y jovial, experto en boxeo, lanzó su eficaz jaque mate a la rápida bruja del volcán." However, this phrase was missing the letters H, K, and W. After being informed of the error, ChatGPT swiftly identified the missing letters and aimed to create a complete pangram. Overall, the experiment with pangrams and ChatGPT highlighted the intricacies of language generation and the importance of accuracy and attention to detail when dealing with linguistic tasks.

The discussion revolves around the exploration of language generation through the use of ChatGPT for crafting pangrams. The initial comment highlights the challenge of generating a novel pangram in Spanish using ChatGPT-4o due to missing letters and the importance of verifying each step. Further conversation delves into the intricacies of the process, including the technique of selecting random letters and ensuring grammatical correctness. Another reply reflects on the fascination with improving Language Model Tasks (LLMs) through specific strategies and effective utilization, emphasizing the need for thoughtful approaches to working with LLMs.

Moreover, the discussion addresses the importance of intelligently guiding language models like ChatGPT, rather than relying solely on random phrases, in order to generate successful pangrams. It also touches upon the significance of self-check mechanisms and contrasting the ability of LLMs to human reasoning. The engagement underscores the critical role of intelligence and problem-solving strategies in working with language models, emphasizing the need to consider constraints and possibilities step by step for effective outcomes.

