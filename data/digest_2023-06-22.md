## AI Submissions for Thu Jun 22 2023 {{ 'date': '2023-06-22T17:13:47.434Z' }}

### Generating SQL with LLMs for fun and profit

#### [Submission URL](https://iamnotarobot.substack.com/p/generating-sql-with-llms-for-fun) | 63 points | by [diego](https://news.ycombinator.com/user?id=diego) | [26 comments](https://news.ycombinator.com/item?id=36440760)

Language models are being used to generate SQL code for querying databases, but this could lead to potentially dangerous situations if the generated SQL code is not thoroughly checked. As demonstrated by Diego Basch, language models can easily generate queries that alter or drop tables, and even create infinite loops. While attempts to train the model to recognize risky queries have been made, there is still a risk of prompt injection leading to SQL injection. Basch suggests making the database read-only or creating a restricted role for the language model to use as a preventive measure. However, language models should not yet be trusted to generate executable code on the fly.

The discussion on the submission suggests that language models are still not yet able to generate executable code on-the-fly and should be used with caution. There are also concerns about data security and measures to prevent unauthorized access to databases. Suggestions were made to create a read-only database or create a restricted role for the language model to use. The discussion also touched on the importance of manual tracking and overseeing the process of generating SQL code. However, solutions such as making database read-only or creating restricted access have its limitations. There is also a mention about how language models should not be trusted to generate code for production systems before being thoroughly tested. The commenters also discussed the potential dangers of prompt injection leading to SQL injections. One solution proposed was to enforce reasonable time limits and database permissions and to add security measures such as read-write security in PostgreSQL, to prevent unauthorized access to databases. Finally, a few commenters raised concerns about proper communication to the audience, where the content generated by language models should be closely monitored to avoid misunderstandings or potential risks.

### Stability AI Launches Stable Diffusion XL 0.9

#### [Submission URL](https://stability.ai/blog/sdxl-09-stable-diffusion) | 166 points | by [seydor](https://news.ycombinator.com/user?id=seydor) | [100 comments](https://news.ycombinator.com/item?id=36435559)

Stability AI has announced the release of SDXL 0.9, marking a significant advancement in the Stable Diffusion text-to-image models. This new development offers a leap in creative use cases for generative AI imagery, providing hyper-realistic creations for films, television, music, instructional videos, design, and industrial use. SDXL 0.9 boasts a 3.5B parameter base model and a 6.6B parameter model ensemble pipeline, with a 1024x1024 resolution and the ability to generate highly detailed images. Despite its powerful output, SDXL 0.9 requires only a modern consumer GPU to run. The model is available on the Clipdrop platform, with API access coming soon. The discussions centered around the quality, resolution, use cases, and specifications of the new model, including its compatibility with different GPUs and processors. Some users speculated on the legal issues of using the model, while others discussed the possibility of generating specialized powered AI models.

### People paid to train AI are outsourcing their work to AI

#### [Submission URL](https://www.technologyreview.com/2023/06/22/1075405/the-people-paid-to-train-ai-are-outsourcing-their-work-to-ai/) | 334 points | by [kungfudoi](https://news.ycombinator.com/user?id=kungfudoi) | [221 comments](https://news.ycombinator.com/item?id=36432279)

A new study by the Swiss Federal Institute of Technology (EPFL) reveals that a significant portion of gig workers tasked with training AI models have been outsourcing their work to AI themselves. Researchers hired 44 people on Amazon Mechanical Turk to summarize medical research papers, and analyzed their responses using an AI model they trained themselves. They found that between 33% and 46% of workers used AI models like OpenAI’s ChatGPT. This percentage is expected to grow as AI models become more powerful and accessible. This could introduce further errors into already error-prone models and highlights the need for new ways to check whether data has been produced by humans or AI. Some commenters criticize the study for not accurately detecting whether responses were generated by humans or AI. Others discuss the importance of creating new ways to check whether data has been produced by humans or AI to ensure accuracy. There are also debates over the ethics of using AI to replace human workers. Some responders of the comment section shared their experiences working on Mechanical Turk, and a few cited sources to correct minor mistakes in the original post.

### Show HN: Launching Struct – Knowledge-Rich, AI-Powered Chat Platform

#### [Submission URL](https://www.struct.ai/blog/launching-struct-chat-platform) | 54 points | by [mrjn](https://news.ycombinator.com/user?id=mrjn) | [35 comments](https://news.ycombinator.com/item?id=36432743)

Chat platforms like Slack and Discord have revolutionized real-time communication, but they have inherent flaws that turn them into knowledge black holes. Information gets lost in the sea of endless messages, and finding them is akin to searching for a needle in a haystack. Manish R Jain, the founder of Dgraph Labs, introduces the CRISPY framework, outlining six principles that an ideal chat platform should uphold. Jain's new, innovative chat platform, Struct, embodies this framework, making real-time communication accessible, lasting knowledge for a change. Struct aims to reinvent the current chat platform's status quo and challenges the biggest problems that millions of users face daily.

The article discusses the inherent flaws of chat platforms like Slack and Discord, which turn them into knowledge black holes where messages can get lost, and information is hard to retrieve. The founder of Dgraph Labs, Manish R Jain, introduces the CRISPY framework that outlines six principles that an ideal chat platform should have. Jain's new chat platform, Struct, aims to address the problems that millions of users face daily and reinvent the current chat platform status quo. The comments on Hacker News highlight various aspects of the article, including the need for a chat platform that can handle knowledge, the struggle with finding information on large Discord servers, and the role of AI-powered search in chat platforms. The comments also raise concerns about the pricing and privacy policies of Struct.

### Aeon: A unified framework for machine learning with time series

#### [Submission URL](https://github.com/aeon-toolkit/aeon) | 119 points | by [megalodon](https://news.ycombinator.com/user?id=megalodon) | [23 comments](https://news.ycombinator.com/item?id=36432369)

Aeon Toolkit is a unified framework for machine learning with time series that is compatible with scikit-learn. It offers both classical techniques and the very latest algorithms for learning tasks like forecasting and classification. A broad library of time series algorithms, efficient implementations with numba, and interfaces with other time series packages make Aeon Toolkit an ideal choice for algorithm comparison. The latest release is version v0.3.0, and you can visit their website for documentation and installation instructions. Their code is licensed under the BSD-3-Clause license.

Aeon Toolkit, a unified framework for machine learning with time series, is the subject of a Hacker News discussion. The toolkit is compatible with scikit-learn and offers both classical techniques and the latest algorithms for tasks such as forecasting and classification. It has a broad library of time series algorithms, efficient implementations with numba, and interfaces with other time series packages, making it ideal for algorithm comparison. Commenters discuss various aspects of time series data and highlight the advantages of the Aeon Toolkit's friendly and flexible framework for developing machine learning models. They also mention other toolkits and packages, such as Prophet, sktime, Darts, and Weka, for working with time series data and developing machine learning models.

### OpenAI Lobbied the E.U. To Water Down AI Regulation

#### [Submission URL](https://time.com/6288245/openai-eu-lobbying-ai-act/) | 158 points | by [jlpcsl](https://news.ycombinator.com/user?id=jlpcsl) | [71 comments](https://news.ycombinator.com/item?id=36428121)

Documents obtained by TIME show that OpenAI lobbied to weaken forthcoming AI regulation in the EU while publicly calling for stronger AI guardrails. CEO Sam Altman has been touring world capitals and speaking about the need for global AI regulation, but behind the scenes, OpenAI proposed amendments that were later made to the final text of the EU law. OpenAI proposed that its general-purpose AI systems including GPT-3 and DALL-E 2 should not be considered "high risk," a designation that would subject them to stringent legal requirements including transparency, traceability, and human oversight. The lobbying efforts by OpenAI in Europe have not previously been reported, although Altman has recently become more vocal about the legislation.

The discussion on the submission revolves around the issue of OpenAI lobbying to weaken forthcoming EU AI regulation while publicly calling for stronger AI guardrails. Some comments accuse OpenAI of hypocrisy, while others argue that lobbying is a common practice among corporations and that regulations could stifle innovation. Some users criticize the lack of transparency in the AI generated content, while others express concerns about the power dynamics in the industry and the potential dangers of unregulated AI technology. Some users raise questions about the effectiveness of regulations in protecting the public and balancing the interests of all stakeholders involved. Overall, the discussion reflects a diversity of opinions and perspectives on the issue of AI regulation and its impact on industry, innovation, and society.

### Is Google reCAPTCHA GDPR Compliant?

#### [Submission URL](https://wideangle.co/blog/is-recaptcha-illegal-under-gdpr) | 143 points | by [openplatypus](https://news.ycombinator.com/user?id=openplatypus) | [222 comments](https://news.ycombinator.com/item?id=36430280)

The French data protection authority, CNIL, has imposed a penalty on Cityscoot for using Google's reCAPTCHA tool on their website and app without providing sufficient privacy information or seeking consent from visitors. The case raises questions about reCAPTCHA's compliance with EU data protection and privacy laws, with the CNIL concluding that the tool, which uses third-party cookies to distinguish bots from humans, requires consent. While some cookies are exempt from consent, this exception does not extend to cookies that are not strictly necessary, such as those used for analysis.

The discussion on the article clarifies technical terms like pixels, scripts, and beacons, and points out that some exceptions exist to the requirement of consent for cookies. The topic then shifts to CAPTCHA, with some contributors criticizing it for being a nuisance or hindrance to users, while others claim it is necessary to protect websites from spam and bot attacks. The IPv4 and IPv6 internet protocols, their limitations, and implementation challenges are also discussed. Finally, some participants share their experiences with CAPTCHA and captchas' effectiveness in preventing spam signups.



