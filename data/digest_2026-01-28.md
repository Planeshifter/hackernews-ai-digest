## AI Submissions for Wed Jan 28 2026 {{ 'date': '2026-01-28T17:21:48.791Z' }}

### Trinity large: An open 400B sparse MoE model

#### [Submission URL](https://www.arcee.ai/blog/trinity-large) | 226 points | by [linolevan](https://news.ycombinator.com/user?id=linolevan) | [73 comments](https://news.ycombinator.com/item?id=46789561)

Trinity 
\\(400B) drops as a highly sparse MoE—and you can try the Preview free on OpenRouter for a limited time.

What’s new
- Model family: Three checkpoints
  - Trinity-Large-Preview: lightly post-trained, chat-ready “instruct” model (not a reasoning model yet)
  - Trinity-Large-Base: best 17T-token pretraining checkpoint (frontier-class foundation model)
  - Trinity-Large-TrueBase: early 10T-token checkpoint with no instruct data or LR anneals (a “true base”)
- Architecture: 400B-parameter sparse MoE, 256 experts with 4 active per token (≈1.56% routed; ~13B active params/token). They doubled dense layers (3→6) to keep routing stable at this sparsity—more aggressive than most peers, rivaled mainly by Llama 4 Maverick.
- Training scale: 2048 Nvidia B300 GPUs; they claim it’s the largest publicly stated pretraining run on B300s. Finished pretraining in 33 days.

Why it matters
- Speed: Thanks to high sparsity and efficient attention, they report 2–3x faster inference/training for the same hardware versus peers in the same “weight class.”
- Performance: Trinity-Large-Base scores at frontier level across math, coding, scientific reasoning, and knowledge benchmarks, per the team.
- Data engine: 17T tokens across three phases (10T + 4T + 3T), curated by DatologyAI, including 8T+ synthetic tokens spanning web, code, math, reasoning, and multilingual (targeting 14 non‑English languages).

How they made a 400B MoE behave
- Momentum-based expert load balancing: adjusts per-expert router bias up/down with tanh clipping, momentum smoothing, plus a per-sequence balance loss to avoid within-sequence hot spots.
- z-loss: regularizes LM head to prevent logit scale creep; they monitor logit stats for early instability signals.
- Parallelism and optimizer: HSDP with expert parallelism 8 (yielding 2048 DP ranks); scaled batch size after 5T tokens. Used Muon (supports larger critical batch than AdamW), guided by MiniMax-01 batch-scaling results. Smooth loss curve with clear phase transitions.

Caveats
- The Preview is intentionally a non-reasoning instruct model; a full reasoning variant is in post-training and will take longer due to extra tokens per output, but sparsity helps speed RL rollouts.

Try it
- Trinity-Large-Preview is available free on OpenRouter for a limited time.

**The Wall, The Cost, and The Benchmarks**
The release of Trinity Large (400B) sparked a broad debate on the current state of frontier AI models, with conversation shifting from the specific model to the trajectory of the industry at large.

*   **Is AI hitting a "brick wall"?** A significant portion of the discussion focused on whether LLM progress has plateaued. Some users argued that recent gains are marginal—comparing the ELO gap between Gemini 1.5 Pro and GPT-4o as feeling like a "coin flip" (50% chance of being better) rather than a leap. Others pushed back, noting that ELO statistics imply a significant win rate (70%) despite looking close on charts, and argued that progress has shifted from linear chat improvements to step-change capabilities in coding and complex math (citing Terence Tao’s recent use of o1).
*   **Skepticism of benchmarks:** Trust in public leaderboards like LMSYS Arena is eroding among some HN users, who described them as "Markdown usage detectors" or measures of style rather than reasoning. Participants suggested that real progress is now found in private evaluations or harder, specific benchmarks (like LiveBench or METR) rather than general chat arenas.
*   **The economics of "33 days":** Commenters scrutinized the claim that the model finished pretraining in 33 days on 2048 Nvidia B300s. While some calculated the raw compute cost at roughly $10 million, critics noted this figure is misleading. They argued that quoting a "final run" cost ignores the tens of millions spent on failed experiments, dataset curation, and infrastructure setup, similar to misunderstandings around DeepSeek’s reported costs.
*   **Early feedback:** Initial user impressions were mixed to skeptical. One user reported the model failed to understand technical concepts (specifically GitHub Action DAGs), while others doubted the validity of comparing a 400B sparse MoE to "Llama 4" architecture without independent verification. Use cases for running such large models at home are also becoming increasingly infeasible due to hardware costs.

### LM Studio 0.4

#### [Submission URL](https://lmstudio.ai/blog/0.4.0) | 209 points | by [jiqiren](https://news.ycombinator.com/user?id=jiqiren) | [114 comments](https://news.ycombinator.com/item?id=46799477)

LM Studio 0.4.0: headless server, parallel inference, and a stateful chat API

What’s new
- Headless core (“llmster”): The LM Studio engine now runs as a standalone daemon without the GUI, suitable for servers, CI, Colab, or terminal-only workflows. Quick start: install (curl/irm), lms daemon up, lms get <model>, lms server start, lms chat.
- Parallel requests with continuous batching: llama.cpp 2.0.0 brings concurrent inference to the same model. New loader options:
  - Max Concurrent Predictions: cap simultaneous requests; extras queue.
  - Unified KV Cache (default): shared, non-partitioned cache for variable-length requests.
  - Note: MLX engine doesn’t support this yet; it’s coming.
- Stateful REST API: POST /v1/chat maintains conversation state via response_id/previous_response_id, keeps payloads small, and returns detailed perf stats (tokens in/out, speed, time-to-first-token). It can also use locally configured MCP tools (permission-gated).
- Permission keys: Gate which clients can access your LM Studio server (Settings > Server).
- UI refresh: Split View (side-by-side chats), chat export (PDF/Markdown/text), Developer Mode (advanced options), and in‑app docs.
- New CLI flow: lms chat for interactive terminal sessions; plus lms runtime update and other streamlined commands.

Why it matters
- Deploy anywhere: Decoupling the core from the app makes local-first models practical on cloud/GPU rigs and in automation.
- Higher throughput: Continuous batching + concurrency turns LM Studio into a more capable API server for multi-user or web workloads.
- Easier app integration: A stateful /v1/chat simplifies multi-step workflows and tool use without shipping full transcripts.

Notable fixes
- MCPs now lazy-load on demand.
- Various UI/installer bugs squashed (icons, model picker, downloads, lms import).

**LM Studio vs. Ollama:** The most active area of discussion compared the two tools. Users debated their underlying architectures:
*   **Model Storage:** several commenters criticized Ollama’s decision to mimic Docker by using a registry and "blob" storage system. While this makes pulling models easy for non-technical users, it frustrates developers because it duplicates files and makes it difficult to share model weights across different applications. In contrast, users praised LM Studio for using standard directories and GGUF files that can be easily accessed by other tools.
*   **Convergence:** Commenters noted that the tools are functionally converging from opposite directions. Ollama started as a CLI/API tool and is adding UI elements, while LM Studio started as a GUI tool and is now adding headless/CLI capabilities.

**Headless Mode & CLI:** The introduction of the headless "llmster" core was well-received by those wanting to run LM Studio on servers or CI environments without the desktop overhead.
*   There was initial confusion regarding whether the desktop app needed to remain open for the CLI to function; users clarified that the new daemon allows for a truly headless experience.
*   Mac users specifically highlighted MLX support as a primary reason for sticking with LM Studio over other runners.

**Performance & UI:**
*   **Throughput:** There was a debate regarding the new parallel request feature. While some worried that splitting resources would halve performance per user, others clarified that continuous batching allows for significantly higher total token throughput (citing up to 1300 t/s on M-series chips) by utilizing idle compute time.
*   **Visuals:** The UI refresh proved polarizing; some users complained that the new "dark mode" is actually grey and criticized the aesthetic for looking like a "toy" or "WhatsApp" rather than a professional developer tool.
*   **Licensing:** A few users expressed a wish that LM Studio itself were open source, noting that while the underlying engines (llama.cpp) are open, the wrapper remains proprietary.

### Show HN: A MitM proxy to see what your LLM tools are sending

#### [Submission URL](https://github.com/jmuncor/sherlock) | 205 points | by [jmuncor](https://news.ycombinator.com/user?id=jmuncor) | [110 comments](https://news.ycombinator.com/item?id=46799898)

Sherlock: Real-time LLM token tracker and prompt inspector for CLI tools

What it is
- A local HTTP proxy + terminal dashboard that intercepts LLM API calls to show live token usage, context-window “fuel gauge,” and recent requests.
- Automatically archives every prompt/response as markdown and JSON for debugging and review.
- Targets CLI workflows with zero config: wrap your tool and watch stats update in real time.

How it works
- Start the proxy/dashboard: sherlock start (defaults to localhost:8080; set token limit with -l).
- Run your tool through Sherlock: sherlock claude, sherlock codex, or sherlock run --provider <name> <cmd>.
- Dashboard highlights context usage (green/yellow/red) and prints a session summary on exit.

Why it matters
- Quickly spot runaway token usage and context overflow.
- Track costs via token counts during development.
- Create an audit trail of prompts to refine and debug your prompting.

Supported/limits
- Providers: Anthropic (Claude Code) and OpenAI Codex supported.
- Gemini CLI blocked by an upstream base-URL issue when using OAuth.
- Python 3.10+; local interception only (be mindful of sensitive data in saved logs).

Repo
- MIT license. ~455 stars, 16 forks.
- GitHub: https://github.com/jmuncor/sherlock

Based on the discussion, the community response focused primarily on a critical security flaw and the broader implications of AI-generated code ("vibe coding"). Here is the summary:

**Critical Security Vulnerability**
*   **TLS Disabled:** Users immediately identified that the tool unconditionally disabled TLS verification (`ssl_insecure=true` passed to mitmproxy). Code reviewers noted this exposes users to Man-in-the-Middle (MITM) attacks if used on public networks (e.g., coffee shops).
*   **The Fix:** The author (OP) acknowledged the mistake and pushed a fix during the discussion to implement a simple HTTP relay that eliminates the insecure mitmproxy implementation.

**"Vibe Coding" and AI Generation**
*   **AI Authorship:** Commenters noted the code contained artifacts indicating it was generated by AI (e.g., git commits "Co-Authored-By Claude Opus").
*   **Critique of "Vibe Coding":** The security flaw was cited as a prime example of the dangers of "vibe coding"—generating software via LLMs without possessing the technical knowledge to audit or understand the output. Critics argued that disabling SSL is a "huge red flag" that a human developer would likely catch, but an AI might insert to solve a debugging error.
*   **Open Source "Pollution":** Senior engineers expressed frustration that AI allows inexperienced developers to create projects with professional-looking READMEs, garnering hundreds of stars despite having "student-level" or dangerous code internals. This makes it difficult to distinguish production-ready software from prototypes.

**Market Need**
*   **Enterprise Governance:** Despite the code quality issues, some commenters validated the problem statements, noting a lack of standard enterprise tools for governing AI data usage, auditing prompts, and managing token budgets in large organizations.
*   **Implementation:** Some suggested this functionality would be better implemented as a standard plugin/addon for existing tools like `mitmproxy` rather than a standalone wrapper.

### UK Government’s ‘AI Skills Hub’ was delivered by PwC for £4.1M

#### [Submission URL](https://mahadk.com/posts/ai-skills-hub) | 379 points | by [JustSkyfall](https://news.ycombinator.com/user?id=JustSkyfall) | [141 comments](https://news.ycombinator.com/item?id=46803119)

UK’s £4.1m “AI Skills Hub” is mostly a link list, with busted UX and errors, says critic

- The UK government launched an AI Skills Hub to train 10 million workers by 2030. According to a detailed blog post, the site—built by PwC for £4.1m (~$5.6m)—mostly aggregates links to existing external courses (e.g., Salesforce Trailhead) rather than hosting original content.
- The author calls the UI “vibecoded” and highlights basic usability issues: a tiny “Enroll Now” button, a comment section where users expect next steps, and a prominently linked “Skills & Training Gap Analysis” that appears closed to the public.
- PwC reportedly acknowledges the site doesn’t fully meet accessibility standards, which public-sector sites are legally required to follow. The post also flags a substantive content error: teaching “fair use” (a US concept) instead of the UK’s more restrictive “fair dealing.”
- Beyond the build quality, the author is angry about value for money, arguing small UK dev shops could deliver better for a fraction of the cost, and that the public likely won’t use the site in its current state.

Why it matters: Another government IT procurement raising questions about cost, accessibility compliance, and delivery quality. Open questions include what the £4.1m covered (e.g., discovery, hosting, support, marketing, licensing) and whether further phases are planned to fix core issues.

**Discussion Summary:**

The commentary focuses heavily on the structural incentives of government procurement that lead to outcomes like this. Users argue that the "nobody gets fired for buying IBM" adage explains the selection of PwC: civil servants prioritize risk mitigation over product quality, knowing that hiring a "respectable" large firm shields them from personal blame if the project fails, whereas hiring a small, agile shop constitutes a career risk.

Key points of debate include:
*   **Barriers to Entry:** A significant portion of the thread discusses certifications (specifically ISO 9000) as a form of "grift" or structural gatekeeping. Users note these requirements filter out capable small studios that cannot afford the bureaucratic overhead, favoring large consultancies that specialize in compliance rather than software development.
*   **Process vs. Outcome:** Commenters suggest the high cost isn't just for the code, but for the immense friction of the procurement process itself (endless paperwork, insurance requirements, and audits). The consensus is that the system rewards following legal procedure perfectly rather than delivering a working product.
*   **The "Horizon" Counterpoint:** Some users pushed back on the idea that big firms are "safe," citing the Fujitsu/Post Office scandal, though others retorted that despite such scandals, the procurement habits of the civil service remain unchanged due to entrenched risk aversion.

### Will AIs take all our jobs and end human history, or not? (2023)

#### [Submission URL](https://writings.stephenwolfram.com/2023/03/will-ais-take-all-our-jobs-and-end-human-history-or-not-well-its-complicated/) | 90 points | by [lukakopajtic](https://news.ycombinator.com/user?id=lukakopajtic) | [161 comments](https://news.ycombinator.com/item?id=46797865)

Stephen Wolfram: Will AIs Take All Our Jobs? It’s Complicated

- Shock factor: ChatGPT shattered the sense that essay writing is uniquely human. It stitches together human-like text by following patterns learned from billions of webpages and millions of books.
- What’s really going on: Under the hood is a brain-like neural net doing “raw computation.” Meaning doesn’t emerge from the machine alone; it comes from the human-created data it trains on—and from the human who supplies the goal via a prompt.
- The new interface: Wolfram frames ChatGPT as a “linguistic user interface” (LUI). You provide intent in plain language; the system expands it into coherent prose grounded in shared human context.
- The big shift: “Essayification” just became cheap—like desktop publishing did for typesetting. A polished essay is no longer evidence of effort. The scarce work moves from writing to specifying intent, constraints, and judgment.
- Why prediction is hard: Expect surprises. Wolfram leans on “computational irreducibility” to argue that the trajectory of AI capabilities and social effects can’t be neatly forecast.
- What’s next (teed up for later sections): If AIs can pursue defined goals, the hard part is deciding which goals matter, how to evaluate progress, and how governance should adapt.

Takeaway: Writing as a skill is being unbundled from effort; the human edge shifts to framing problems, setting goals, and providing oversight.

**The American Fear vs. Global Safety Nets**
A significant portion of the discussion centered on whether the existential dread regarding AI displacement is a uniquely American phenomenon. User `mips_avatar` argued that Americans are more stressed because basic survival (healthcare, housing stability) and self-worth are strictly intermediated by corporate employment. By contrast, they suggested that in places with stronger social safety nets (citing Vienna and Vietnam), AI is viewed less as a threat to survival and more as a tool.

**The "Cushy Job" Theory**
Some users pushed back on the safety-net theory, suggesting the anxiety stems from the nature of the US workforce. User `nlyrlczz` posited that the US has a high density of "cushy paper-pushing" and high-paid white-collar roles that are specifically vulnerable to the "essayification" capabilities of LLMs. Others noted that anxiety is not geographically exclusive, with user `trnd` mentioning similar concerns in India.

**The Breakdown of the Labor-Capital Bargain**
Discussion moved to the macroeconomic implications (user `myrmdn`, `BirAdam`). Commenters worried that AI undermines the primary mechanism for wealth redistribution: labor. If AI makes human labor nearly valueless, the current capitalist model of "work for income" fails. The thread debated potential outcomes, ranging from a necessary shift toward socialism or UBI to a dystopian increase in inequality where class barriers become insurmountable.

**Inequality and Deflation**
User `BirAdam` offered a specific economic sequence: AI reduces production costs, which should theoretically lower prices (deflation). However, the "scary part" is the transition period where unemployment spikes before the cost of living drops to match the new reality. There was also debate regarding how deeply AI has actually permeated non-tech sectors, with users clashing over whether rural populations (like farmers) are oblivious to AI or actively using it for high-tech capital management.

### Jellyfin LLM/"AI" Development Policy

#### [Submission URL](https://jellyfin.org/docs/general/contributing/llm-policies/) | 196 points | by [mmoogle](https://news.ycombinator.com/user?id=mmoogle) | [102 comments](https://news.ycombinator.com/item?id=46801976)

The open‑source media server Jellyfin published a clear policy on AI use across its repos and community spaces, aiming to protect code quality while allowing limited, accountable assistance.

What’s new
- No AI-written communication: Issues, feature requests, PR descriptions, and forum/chat posts must be in the contributor’s own words. Exception: clearly labeled LLM-assisted translations, ideally with the original language included.
- Strict rules for code contributions: 
  - Keep PRs focused; no unrelated drive-by changes. Large changes must be split into small, reviewable commits.
  - Meet formatting and quality standards; don’t commit LLM meta/config files.
  - Explain changes yourself in the PR body; if you can’t articulate what and why, it won’t be accepted.
  - Code must build, run, and be explicitly tested.
  - Be prepared to address reviewer feedback without outsourcing fixes to an LLM.
  - “Vibe coding” (letting an LLM loose on the codebase) will be rejected.
- Reviewer discretion: Oversized, over-complex, or squashed PRs that can’t be reasonably reviewed will be rejected, LLM-assisted or not.
- Community sharing: Non-official tools primarily built with LLMs must be clearly labeled as such; users can decide if that’s acceptable. (The policy also hints at guidance for secondary AI assistance like docs/formatting.)

Why it matters
- Signals a maturing norm: maintainers embracing AI as a tool, but demanding authorship, accountability, and testable, reviewable changes.
- Sets expectations for contributors: AI can help, but you own the code and the explanation.
- Likely to spark debate across OSS: balancing velocity vs. maintainability as AI-generated contributions surge.

Based on the discussion, the community reaction focuses heavily on the nuances of communication, the "asymmetry of effort," and the role of language barriers in open source.

**Language Barriers vs. Authenticity**
A significant portion of the debate centers on non-native English speakers. While some argue that LLMs are vital accessibility tools that make open source global, many established contributors argue that "broken English" is preferable to AI-generated prose.
*   Critics note that humans are excellent at deciphering intent from imperfect grammar, whereas LLMs often bury meaning under "hyper-excited corporate drone style" fluff or introduce subtle hallucinations.
*   Several users suggested that providing the original native-language text alongside a standard machine translation (like Google Translate) is more respectful and effective than trying to pass off ChatGPT output as fluent English.

**The Asymmetry of Effort**
Commenters expressed strong feelings that using LLMs for communication is often "disrespectful."
*   The core argument is an imbalance of mental energy: the sender expends zero effort to generate a wall of text, while the recipient must spend high "mental capital" to decode it.
*   One user described this as sending 10 paragraphs of output for a 1-sentence prompt, forcing maintainers to sift through "slop."
*   However, a counter-argument appeared suggesting LLMs can be used virtuously to *shorten* and distill rambling thoughts, provided the user reviews the output.

**Writing is Thinking**
Referencing author Ted Chiang, users argued that writing isn't just a way to convey information—it is the process of thinking itself.
*   By outsourcing the description of a Pull Request (PR) or a feature to an LLM, the contributor may be bypassing the critical cognitive work required to fully understand what they have built.
*   The consensus leans toward agreement with Jellyfin: if you cannot articulate what your code does or why it is necessary, you likely do not understand it well enough to contribute it.

**Distinction Between Code and Text**
While there was near-universal agreement that "vibe coding" (blindly trusting AI code) is bad, the friction lies in the "no AI communications" rule. Some feel strict enforcement against grammar-checking tools is too harsh, while others believe that the "unresolved cognitive dissonance" of letting AI write code while banning it from explaining that code makes the policy logically consistent.

### The new era of browsing: Putting Gemini to work in Chrome

#### [Submission URL](https://blog.google/products-and-platforms/products/chrome/gemini-3-auto-browse/) | 21 points | by [xnx](https://news.ycombinator.com/user?id=xnx) | [3 comments](https://news.ycombinator.com/item?id=46799289)

Google is turning Chrome into an AI copilot powered by Gemini 3. Highlights:

- New side panel assistant: Gemini now lives in a persistent side panel so you can multitask without switching tabs—summarize pages, compare options, wrangle calendars, and more.
- On-the-fly image edits: “Nano Banana” lets you transform images directly on the page via prompts (no downloading/re-uploading).
- Connected Apps: Deeper integrations with Gmail, Calendar, YouTube, Maps, Shopping, and Flights. Examples include pulling event details from email, checking flight options, then drafting a note to teammates.
- Personal Intelligence (coming months): Opt-in memory and preferences for context-aware, proactive help across browsing. You can connect/disconnect apps and set custom instructions.
- Auto browse (agentic actions): For AI Pro/Ultra subscribers in the U.S., Chrome can handle multi-step chores—trip planning across dates, scheduling, form-filling, gathering tax docs, getting service quotes, subscription management, even shopping from a reference photo with budgets and discount codes. With permission, it can use Google Password Manager for sign-ins.

Why it matters:
- Pushes Chrome from “tool” to “agent,” competing with Microsoft’s Copilot in Edge and emerging agentic browsers.
- Big productivity upside, but raises familiar questions: privacy (what data powers context), safety (auto form-filling), and ecosystem lock-in via Connected Apps.
- Subscription gating (AI Pro/Ultra) puts the most powerful agentic features behind a paywall.

Availability:
- Gemini in Chrome on macOS, Windows, and Chromebook Plus. Side panel and image transforms available to all Gemini-in-Chrome users; Personal Intelligence rolls out in the coming months; Auto browse limited to U.S. AI Pro/Ultra.

**The Discussion**

Commenters viewed Google’s shift toward "agentic browsing" as a forced integration of LLMs, drawing negative comparisons to Microsoft’s strategy with Copilot. Skepticism ran high regarding the target audience; some noted that the Hacker News community has likely already abandoned Chrome due to the Manifest V3 controversy (which impacts ad blockers). Others wasted no time asking how to disable the new features entirely.

### Kairos: AI interns for everyone

#### [Submission URL](https://www.kairos.computer/) | 30 points | by [bamitsmanas](https://news.ycombinator.com/user?id=bamitsmanas) | [27 comments](https://news.ycombinator.com/item?id=46792225)

What’s new: Kairos pitches a doer, not a chatter—an agent with its own browser that logs into sites (even behind logins), clicks buttons, fills forms, and ships results across your stack.

Highlights
- 20+ native integrations: Gmail, Calendar, Slack, GitHub, Notion, Sheets, Outlook, HubSpot, Linear, Airtable, Drive/Docs, Teams, Zoom, Dropbox, Box, Calendly, Twitter/Reddit, and more.
- Runs while you’re away: schedule tasks or set triggers; delivers reports, summaries, and proposals.
- End‑to‑end workflows: e.g., find YC AI startups → enrich via LinkedIn → filter/process → sync to Airtable → post a Slack report.
- Real-world ops: auto‑screen candidates in Greenhouse and book screens; scan inbox for refund requests and act per policy; find mutual availability via Calendly and your calendar.
- Teach by demo: share your screen once; it learns the steps and automates thereafter.

Positioning: “ChatGPT talks. Kairos does.” It emphasizes agentic browsing plus app orchestration over pure chat.

Pricing
- Free: a handful of tasks, SMS/WhatsApp messaging, all integrations, scheduled/recurring tasks, priority support.
- Plus: $37/month for higher limits; includes the same integrations and scheduling.

Open questions for HN readers
- Security and access: how are credentials handled, 2FA/session persistence, audit logs, and data boundaries across apps?
- Reliability: how it copes with changing UIs, anti-bot measures, rate limits, and long‑running tasks.
- Scope: which tasks are truly hands‑off vs needing human-in-the-loop review, and how errors are surfaced and corrected.

**The Top Story:**
**Kairos: An “AI Intern” That Automates Browser Workflows**

Today’s discussion focused on Kairos, a new entrant in the "agentic AI" space. Unlike standard LLMs that just chat, Kairos positions itself as a "doer" that can navigate the web, log into accounts (Gmail, Slack, HubSpot, etc.), and execute multi-step workflows like candidate screening or expense reporting.

**The Discussion:**
The HN community put the "intern" through a rigorous performance review, ranging from semantic debates to live technical stress tests.

*   **The "Intern" Semantics:** The branding sparked immediate debate. Some users felt the term "intern" implies sub-par work or misses the point of internships (talent development, not just cheap labor). Others maintained that "sub-par" labor is exactly what businesses try to automate—specifically the "TPS report" style data entry that humans hate. However, one commenter noted that manually filling out forms often provides necessary mental context that gets lost when automated.
*   **Performance Review (The Birdwatcher Test):** In the most substantial critique, user `bwstrgrd` tested Kairos on a specific task: finding rare bird sightings on eBird. results were mixed. The agent struggled to navigate the site efficiently, "reinvented the wheel" by creating a complex search plan, took eight minutes to execute, and returned results that were less accurate than a quick Google search. The creator (`bmtsmns`) was active in the thread, acknowledging the feedback and limitations regarding specific niche tasks.
*   **UX and Naming Collisions:** Several users complained about "scroll-jacking" on the landing page, calling it an immediate bounce factor. Others pointed out confusion with an existing Kubernetes distribution also named Kairos.
*   **Trust and Accountability:** The "intern" metaphor broke down for some regarding supervision. Users noted that human interns are supervised in real-time, whereas an AI agent executing tasks in the background creates anxiety about accountability and potential "runaway" errors within sensitive accounts.

### Please don't say mean things about the AI I just invested a billion dollars in

#### [Submission URL](https://www.mcsweeneys.net/articles/please-dont-say-mean-things-about-the-ai-that-i-just-invested-a-billion-dollars-in) | 614 points | by [randycupertino](https://news.ycombinator.com/user?id=randycupertino) | [282 comments](https://news.ycombinator.com/item?id=46803356)

The story: A sharp McSweeney’s satire by Forest Abruzzo channels a thin-skinned tech billionaire pleading with the public to stop criticizing AI—while openly listing its worst externalities. In a breathless defense, the narrator proclaims AI “the most essential tool in human history” even as he concedes it enables scams, deepfakes, job loss, copyright scraping, surveillance creep, school degradation, and autonomous weapons. The bit riffs on a recent “stop being so negative about AI” news cycle, lampooning the industry’s PR spin and wounded-ego posture.

Why it matters: It captures a growing cultural backlash to AI boosterism and the gap between lofty promises and real harms. For builders and policymakers, it’s a reminder that trust, consent, safety, and accountability aren’t PR problems—they’re product requirements.

HN angles to watch:
- Is this satire a fair critique or a strawman?
- Training data consent/copyright, deepfake harms, and regulation
- Energy/compute costs vs. benefits
- The tech industry’s sensitivity to criticism and credibility gap

Based on the discussion, here is a summary of the comments on Hacker News:

**The Purpose of a System**
A central theme of the thread revolves around the systems thinking aphorism "the purpose of a system is what it does." Commenters debated whether the high-minded intentions of AI creators matter if the practical, dominant outputs of the technology are spam, deepfakes, and astroturfing. Several users argued that if AI’s "steady state" outcome is chaos and harm, that is its engineering purpose, regardless of intent. This led to comparisons with the internet—specifically whether AI is simply a new medium for old human behaviors or if it fundamentally alters the landscape by driving the cost of generating "infinite torrents" of noise and harassment to zero.

**Supercharging Scams**
The satire’s point about scamming the elderly resonated deeply, moving from abstract debate to concrete examples. Users shared anecdotes of AI-cloned voice scams targeting grandparents (faking emergencies) and romance scams involving cloned audio of influencers (citing a specific case involving a Brazilian TikToker). The consensus among these commenters is that AI acts as a "force multiplier" for bad actors, allowing scams to scale rapidly in a way that previous technologies did not. Some pushed back with the "guns don't kill people" argument, but others retorted that when a tool makes crime this efficient, the tool itself warrants scrutiny.

**Harms vs. Utility**
While a few users defended the technology by citing benefits like coding assistance and accessibility, the prevailing sentiment in the thread leaned toward the article's critique. Users listed non-consensual sexual imagery, copyright theft, and the degradation of online trust as immediate, tangible harms that currently outweigh potential future benefits. One commenter described AI not as a tool but as an "information nuclear reactor" spewing radioactive material (spam/slop) that is poisoning the internet.

**The "Internet Analogy" Dispute**
There was significant pushback against the idea that the internet had a similarly "rocky start." Users argued that while the internet facilitated malware and flame wars, AI’s current trajectory creates different, more invasive problems—specifically the inability to distinguish truth from fiction and the massive scale of automated harassment. The comparison was generally viewed as a failure to appreciate the specific, accelerating nature of AI risks.

### Show HN: I'm building an AI-proof writing tool. How would you defeat it?

#### [Submission URL](https://auth-auth.vercel.app/) | 10 points | by [callmeed](https://news.ycombinator.com/user?id=callmeed) | [12 comments](https://news.ycombinator.com/item?id=46799402)

Authentic Author is a browser-based writing tool that tries to verify human authorship by focusing on how text is produced, not just what it says. It gives users a prompt and requires them to write 1–2 paragraphs directly in its editor, disabling paste and common DOM tricks. Behind the scenes it records typing cadence, pauses, tab switches, and window focus changes, then outputs an “Authenticity Score” estimating the likelihood the text was originally written by a human.

The pitch targets classrooms and hiring screens battling AI-written submissions, shifting detection from content analysis to process telemetry. That could deter casual misuse, but it also raises privacy and fairness questions: keystroke-level tracking can feel invasive, and scoring may penalize slow typists, non-native speakers, or people using assistive tools. As with any anti-cheat system, workarounds (e.g., scripted keystrokes or human relays) are possible, but the approach highlights a growing trend toward provenance-by-process rather than post-hoc AI detection.

The discussion on Hacker News focused on the ease of circumventing "telemetry-based" detection, with users largely dismissing the tool's effectiveness against determined attempts to cheat.

**Key themes in the conversation included:**

*   **Trivial Technical Workarounds:** Users quickly demonstrated how to fool the system. One user (`ephou7`) wrote a simple Python script using `xdotool` and random sleep intervals to simulate human typing, achieving an "Authenticity Score" of 81. Others mentioned existing automation tools and OCR scripts that can read a prompt, query an LLM, and inject the text with simulated keystrokes.
*   **The "Transcription" Loophole:** Several commenters validated that the tool checks *how* you type, not *what* you type. Users reported generating AI text on a second monitor, phone, or separate browser window and manually typing it into the editor.
    *   User `jryan49` manually typed a response from Gemini and scored "100% genuine."
    *   User `JoshuaDavid` transcribed what they described as "obvious Claude slop" and still received an 87% authenticity rating.
*   **Hardware Spoofing:** The discussion extended to hardware solutions, noting that cheap programmable USB dongles (microcontrollers) can emulate a keyboard to input AI-generated text, making it indistinguishable from a human typist at the driver level.
*   **The Return to Analog:** The consensus was that digital verification is futile against AI. Commenters argued that the only truly "AI-proof" writing environments are physical "blue book" exams, pen and paper, or strictly invigilated offline rooms.

