## AI Submissions for Thu Apr 11 2024 {{ 'date': '2024-04-11T17:11:45.608Z' }}

### Quantum Algorithms for Lattice Problems

#### [Submission URL](https://eprint.iacr.org/2024/555) | 171 points | by [trotro](https://news.ycombinator.com/user?id=trotro) | [60 comments](https://news.ycombinator.com/item?id=39998396)

Today's top story on Hacker News is about a groundbreaking paper by Yilei Chen from Tsinghua University and the Shanghai Artificial Intelligence Laboratory. The paper introduces a polynomial time quantum algorithm for solving the learning with errors problem (LWE) with specific polynomial modulus-noise ratios. By leveraging reductions from lattice problems to LWE, the paper also presents polynomial time quantum algorithms for solving the decisional shortest vector problem (GapSVP) and the shortest independent vector problem (SIVP) for all n-dimensional lattices within certain approximation factors.

What makes this research especially exciting is the introduction of two new techniques to develop the quantum algorithm for solving LWE. The first technique involves using Gaussian functions with complex variances in designing quantum algorithms, while the second technique uses windowed quantum Fourier transform with complex Gaussian windows to combine information from both time and frequency domains.

The paper details the process of converting the LWE instance into quantum states with purely imaginary Gaussian amplitudes, followed by the conversion of these states into classical linear equations over the LWE secret and error terms, and finally solving the linear system of equations using Gaussian elimination. This innovative approach results in a polynomial time quantum algorithm for solving LWE, marking a significant advancement in quantum computing research.

The discussion on the Hacker News submission about the groundbreaking quantum algorithm for solving the learning with errors problem (LWE) involved various topics:

1. A debate on the scalability of quantum computers and their practicality, especially in the context of Post-Quantum Cryptography (PQC).
2. Insights into lattice-based cryptography, homomorphic encryption, and the potential impact of quantum computing on existing cryptographic algorithms.
3. Discussions on lattice-based systems like FrodoKEM, the security implications of Ring Learning with Errors (RLWE) versus LWE, and the complexities of existing quantum attacks on LWE schemes.
4. Analysis of post-quantum signatures like CRYSTALS-Dilithium based on lattices, Quantum Key Distribution (QKD), and the comparison of code-based systems like McEliece with quantum-resistant solutions.
5. Critiques on the credibility of quantum algorithms and the need for improving current cryptographic protocols to withstand potential quantum attacks.
6. References to the historical developments in cryptography, the challenges of quantum factorization, and contrasting perspectives on the investment in Post-Quantum Cryptography algorithms like Classic McEliece.

The discussions touched upon the implications of quantum computing advancements on cryptography, the robustness of quantum-resistant algorithms, and the ongoing efforts to secure digital communications in a post-quantum era.

### Holodeck: Language Guided Generation of 3D Embodied AI Environments

#### [Submission URL](https://yueyang1996.github.io/holodeck/) | 50 points | by [geox](https://news.ycombinator.com/user?id=geox) | [5 comments](https://news.ycombinator.com/item?id=40004935)

The Holodeck project is revolutionizing the creation of 3D embodied AI environments by allowing users to generate diverse scenes fully automatedly based on textual prompts. Leveraging a large language model, GPT-4, and a vast collection of 3D assets, Holodeck can create customized environments like apartments for researchers with cats or offices for Star Wars fans. By optimizing object positioning based on spatial relational constraints provided by GPT-4, Holodeck produces high-quality outputs preferred over manually designed procedural baselines in residential scenes. Additionally, Holodeck enables training embodied agents to navigate in novel scenes like music rooms and daycares without relying on human-constructed data, marking a significant advancement in developing general-purpose embodied AI agents. Agents fine-tuned on Holodeck demonstrate superior zero-shot generalization on diverse scenes in NoveltyTHOR compared to baseline systems.

The discussion on Hacker News regarding the Holodeck project touched upon various aspects. One user mentioned a similarity between the prompts used in Holodeck and ChatGPT, drawing a parallel with Moriarty's appearance in a Star Trek episode. Another user expressed their enthusiasm for Virtual Reality (VR) and its potential integration with games, while other users discussed the benefits of VR in standalone systems and the application of designs for products in the market.

### Pivot to AI: Hallucinations worsen as the money runs out

#### [Submission URL](https://davidgerard.co.uk/blockchain/2024/04/11/pivot-to-ai-hallucinations-worsen-as-the-money-runs-out/) | 37 points | by [awfulsystems](https://news.ycombinator.com/user?id=awfulsystems) | [25 comments](https://news.ycombinator.com/item?id=40007539)

Today's top stories on Hacker News cover the current state of the venture capital-fueled AI and machine learning industry, highlighting the issue of hallucinations in AI-driven products. The article discusses how generative AI can produce misleading and nonsensical information, leading to concerns about the credibility of AI outputs. Large language models (LLMs) are described as capable autocompletes, generating content based on statistical patterns rather than factual accuracy.

Moreover, the AI industry's reliance on funding and the challenge of tainted training data are mentioned, with some companies considering training AIs on outputs from other AIs despite the risks of producing gibberish. The narrative also touches on the concept of "emergent capabilities" in AI, where machines supposedly excel beyond their initial training, though skepticism persists about the validity of such claims.

Additionally, there are insights into the financial landscape of AI startups, noting instances where companies faced financial struggles and investor skepticism due to lack of profitable functionality. Speculation abounds regarding the potential bubble burst in AI venture capital, with projections suggesting a limited timeline before the market correction takes place. The article draws comparisons to the resilience of cryptocurrencies like Bitcoin and predicts potential repercussions on the tech sector and stock market once the AI bubble bursts.

The piece also humorously references headlines about AI models capable of "reasoning," pointing out the gradual backtracking in the article from ambitious claims to a more realistic assessment of the current limitations in AI technologies. The juxtaposition of flashy announcements and practical realities in AI development adds a touch of skepticism to the overarching narrative of technological advancement in the field.

Overall, the digest provides a comprehensive overview of the challenges and uncertainties surrounding the AI industry, offering a mix of critical analysis and witty commentary on the trends shaping the future of artificial intelligence.

The discussion on Hacker News regarding the article covers various viewpoints on the current state and future of the AI industry, particularly in relation to venture capital funding and the challenges facing AI-driven products. 

- **zer00eyz** expresses skepticism about the venture capital-funded AI industry possibly replacing humans with large-scale script including irrelevant details leading to hallucinations and the issue of leadership in AI companies. The discussion shifts to the concern of wasting electricity on GPU-intensive processes.
- **Havoc** discusses the lack of profitable functionality in AI systems, acknowledging the potential in certain AI applications but questioning the sustainability of current venture capital trends. The conversation extends to companies focusing on customer service and the balance between practical value and feasibility in AI startups.
- **bidder33** briefly mentions the exhaustion of people around cryptocurrency hype and provides a link to a search result listing books on the topic. This leads to a debate about the validity of crypto-related predictions and the potential bubble burst in the AI industry.
- **__loam** emphasizes the excitement around building new norms in parallel computing infrastructure and scientific computing, challenging the notion of AI being in a bubble. The conversation touches on the rapid evolution of language models in AI and the significance of these advancements.
- **Netcob** brings up the positive impact of cryptocurrencies in redistributing wealth and energy efficiency compared to the skepticism towards AI capabilities. The discussion veers into arguments about the implications of widespread adoption of AI technologies.

Overall, the comments reflect a mix of skepticism, excitement, and debate surrounding the evolution of AI, venture capital funding, and the potential challenges facing the industry in the near future.

### Postman Has Acquired Orbit

#### [Submission URL](https://blog.postman.com/announcing-postman-has-acquired-orbit/) | 9 points | by [mooreds](https://news.ycombinator.com/user?id=mooreds) | [4 comments](https://news.ycombinator.com/item?id=40002508)

Postman, a key player in enhancing developer productivity with its API platform, has exciting news to share. The company has recently acquired Orbit, a prominent tool used by developer companies to foster and expand their communities efficiently. This strategic move aims to integrate community-focused features into the Postman Public API Network, creating a dynamic space for API publishers and users to collaborate effectively. Orbit's expertise in enabling developers to engage, measure experiences, and enhance community interactions aligns seamlessly with Postman's mission to facilitate global developer collaboration. Led by Noah Schwartz, the Orbit team will play a vital role in enhancing the Postman Public API Network, empowering API distributors to grow their communities, boost API usage, and gather valuable feedback directly from users on the network.

As the Orbit product transitions over the next 90 days, Postman looks forward to revolutionizing the API landscape by promoting active collaboration among developers and fostering a robust API-first environment. The company is enthusiastic about the immense possibilities this acquisition brings to its customers and the broader developer community. The future of API development looks even more promising with this strategic integration. Stay tuned for more updates as Postman continues to innovate and empower developers worldwide. Join the excitement at the upcoming POST/CON 24, Postman's premier API conference on April 30 to May 1, 2024, in San Francisco. It's an event you wouldn't want to miss!

The discussion on the submission includes various comments from Hacker News users. 

- "pleb_nz" mentioned that they had recently tried Postman but found it lacking in certain ways, comparing it to Bruno.
- "dgz" shared their experience of working in an IntelliJ environment with an HTTP client based on files, finding it elegant.
- "mrwnr" expressed surprise as they have used PHPStorm for years but did not realize it had an HTTP client.
- Finally, "BotuIism" shared a link mentioning that they found support for some things missing in a service.

The conversation revolves around users sharing their experiences with different tools and services related to API development and testing.

### Transformers.js –  Run Transformers directly in the browser

#### [Submission URL](https://github.com/xenova/transformers.js) | 230 points | by [victormustar](https://news.ycombinator.com/user?id=victormustar) | [50 comments](https://news.ycombinator.com/item?id=40001193)

🚀 Exciting news on Hacker News today! A GitHub repository called "transformers.js" is making waves with its promise of state-of-the-art Machine Learning capability directly in the browser, no server required! This project, inspired by Hugging Face's transformers python library, allows users to run pretrained models for various tasks like Natural Language Processing, Computer Vision, Audio, and Multimodal tasks. The best part? You can easily convert your PyTorch, TensorFlow, or JAX models to ONNX format using 🤗 Optimum for seamless integration with Transformers.js. It's as easy as translating your existing Python code to JavaScript, with support for the convenient pipeline API. Additionally, the repository provides installation instructions, examples, and customization options for advanced users. Dive into the future of ML on the web with Transformers.js! 🤖🌐🔥

The discussion on the GitHub repository "transformers.js" includes various users sharing their projects and experiences related to using Machine Learning models directly in the browser. Users discuss issues like the limitations of running models in web browsers due to large downloads and high storage consumption. Some users mention the importance of smaller models for efficient web processing and suggest utilizing technologies like WebGPU for performance improvements. Additionally, there is a conversation about the challenges and possibilities of AI processing in browsers, including the need for pre-installed models and considerations for user experience. Overall, the discussion revolves around the practical implications and future potential of running Machine Learning models in web applications.

### Rerank 3: A new foundation model for efficient enterprise search and retrieval

#### [Submission URL](https://txt.cohere.com/rerank-3/) | 42 points | by [bguberfain](https://news.ycombinator.com/user?id=bguberfain) | [5 comments](https://news.ycombinator.com/item?id=40004741)

Cohere introduces Rerank 3, their latest foundation model designed to enhance enterprise search and Retrieval Augmented Generation (RAG) systems. Rerank 3 offers advanced capabilities such as a 4k context length for improved search quality in longer documents, searching over multi-aspect and semi-structured data, multilingual coverage for over 100 languages, improved latency, and lower total cost of ownership. By combining generative models with Rerank models, RAG solutions can optimize accuracy, latency, and cost effectively.

The model excels in ranking complex, multi-aspect data like emails, invoices, JSON documents, and code, demonstrating enhanced accuracy in data retrieval tasks. Additionally, Rerank 3 showcases strong performance in multilingual data retrieval and long context accuracy, providing a comprehensive solution for enterprises dealing with diverse data sources.

Furthermore, Rerank 3 is now natively supported in Elastic's Inference API, making it easier for organizations to integrate Cohere's advanced retrieval models into Elasticsearch for building efficient enterprise search systems. With lower latency and improved efficiency, Rerank 3 enhances the performance of RAG systems, enabling enterprises to extract valuable insights from their data with ease.

Overall, Rerank 3 stands out as a powerful tool for optimizing enterprise search and RAG systems, offering enhanced performance, multilingual capabilities, and improved efficiency for businesses dealing with complex data structures.

The discussion on the submission involves a mix of comments. One user points out that Rerank 3 incorporates embeddings and a large language model for search, as evidenced by examples provided. Another user corrects a mistake by mentioning that Cohere's approach involves semantic search with BM25, embeddings, multilingual capabilities, and other features, suggesting it is more stable and includes reciprocal rank fusion. Additionally, a commenter highlights that the 4k context window size in the Rerank model is considered large. Another user elaborates on the concept of ranking models providing relevance in search results and how the 4k document context can impact ranking and relevance based on model confidence information. Finally, there is a discussion on the number of results returned and the ranking model's approach to sorting them based on relevance to the query.

### Storm: LLM system that researches a topic and generates full-length wiki article

#### [Submission URL](https://github.com/stanford-oval/storm) | 117 points | by [GavCo](https://news.ycombinator.com/user?id=GavCo) | [95 comments](https://news.ycombinator.com/item?id=40004887)

The Stanford-oval project, named STORM, offers a fascinating LLM-powered knowledge curation system. This innovative tool is designed to research a topic and generate a comprehensive full-length report complete with citations. STORM breaks down the process into two stages: pre-writing, where it conducts Internet-based research to collect references and generate an outline, and writing, where it uses the outline and references to create the final article. 

To enhance the question-asking process, STORM employs innovative strategies like Perspective-Guided Question Asking and Simulated Conversation, making it highly modular and efficient. By simulating a conversation with a topic expert, it updates its understanding and generates insightful questions. The project shows promise in automating the research process and is built for extensibility.

If you're curious to explore STORM, you can run it locally using the provided guide. The tool has been well-received by experienced Wikipedia editors during the pre-writing stage, showing potential for assisting in knowledge exploration journeys. This project represents a significant step towards automated knowledge curation and could be a valuable resource for researchers and writers alike.

The discussion on the submission focuses on various aspects of the Stanford-oval project, named STORM, which offers a knowledge curation system powered by LLM. Some users express concerns about the accuracy levels of AI-generated content and the challenges in documenting LLM outputs accurately. There are also discussions on the potential of LLMs to summarize text and the complexities involved in verifying AI-generated content. Additionally, the conversation touches on the categorization of content, the persistence of generated content, and the importance of testing and validating AI systems systematically. Users also explore the capabilities of LLMs in assisting humans in solving tasks and the impact of LLMs on scientific discovery and language arts. Lastly, there are discussions on utilizing Wikipedia for research purposes, multilingual sources for translation, and the challenges in implementing language-based technologies.

### Huawei says it will start selling PCs powered by Intel's AI chip

#### [Submission URL](https://asia.nikkei.com/Business/Technology/Huawei-says-it-will-start-selling-PCs-powered-by-Intel-s-AI-chip) | 24 points | by [rntn](https://news.ycombinator.com/user?id=rntn) | [12 comments](https://news.ycombinator.com/item?id=40006006)

Huawei Technologies has made a bold move in the tech world by announcing their first AI-powered PC, set to run on Intel's latest chipset and their own operating system, HarmonyOS. Despite facing restrictions from the U.S., Huawei is pushing forward with innovative technology solutions. This new MateBook X Pro PC showcases Huawei's commitment to developing in-house technologies like HarmonyOS and Pangu LLM. Stay tuned for more updates on this exciting development in the tech industry.

The discussion revolves around Huawei's announcement of the MateBook Pro PC running on HarmonyOS and utilizing Huawei's Pangu Large Language Model. Some users express concerns about the compatibility of HarmonyOS with existing software, such as web browsers. There is a mention of a new native browser called ArkWeb. The conversation also delves into the technical specifications of the MateBook X Pro PC, highlighting features like the 4K 120Hz OLED display and its weight compared to the MacBook Air. Debate arises over the efficiency of the active cooling system and the processor's power consumption. Users compare the device to the MacBook Pro in terms of performance and thermal regulation. Additionally, there is a brief discussion on the weight difference between laptops and aspects of build quality. Finally, a link to an archived page related to the discussion is shared.

