## AI Submissions for Fri Nov 17 2023 {{ 'date': '2023-11-17T17:10:34.732Z' }}

### Show HN: nbi.ai â€“ Generative Business Intelligence

#### [Submission URL](https://www.narrative.bi/ai) | 94 points | by [fromthegut](https://news.ycombinator.com/user?id=fromthegut) | [26 comments](https://news.ycombinator.com/item?id=38310502)

NBI.AI, a generative AI platform for business intelligence, has released their latest update. The platform aims to drive growth by providing AI-generated data narratives that deliver actionable insights with just a few clicks. With NBI.AI, users can automate reporting with natural language stories, making it easier to understand complex analytics. The platform generates insights in plain language, eliminating technical jargon and complex data interpretations. NBI.AI also offers weekly AI-powered insights on marketing performance, as well as tools to compare and evaluate ad performance, identify top performers, and analyze conversion journeys. The platform integrates seamlessly with marketing and advertising sources, allowing users to connect in just two clicks. NBI.AI is trusted by over 2,000 growth teams worldwide and offers a 7-day free trial.

The discussion on the submission about NBI.AI, a generative AI platform for business intelligence, covers several topics. Here are the key points:

- One commenter mentions that they are skeptical about AI-driven decision-making tools and prefer a context-leading rule-based natural language generation approach. They expect divergence between rule-based statistical inference narratives and traditional business intelligence data interpretations.
- The founder of NBI.AI responds, providing additional information about their product and its capabilities. They mention that the platform was built to connect virtually structured data sources and has already helped over 2,500 teams gain insights from marketing data.
- Another commenter shares their experience with using narrative-based projects. They use high-level reports that highlight month-over-month changes in website traffic and use an alternative GA4 UI for detailed insights. They plan to implement dimensional analysis to further understand their data.
- The discussion touches on the use of AI in natural language generation and the importance of accuracy and pre-processing to ensure high-quality narratives.
- There is a brief exchange about using automation tools for basic workflows, such as checking invoices and renaming files based on invoice IDs.
- Examples of use cases for NBI.AI are shared, including reporting, anomaly detection, and natural language insights generation for marketing campaigns.
- The founder of NBI.AI clarifies that the training data used for the platform comes from various sources and is focused on behavioral data preferences and feedback to provide personalized insights.
- A user discusses their experience as a marketing specialist and mentions that instead of creating PowerPoint presentations with performance graphs and narrative ROAS, they would prefer using NBI.AI.
- There is a brief discussion about integration plans for NBI.AI and suggestions for additional features.
- Some users express their skepticism about AI-generated data narratives, mentioning that they tend to sound like corporate jargon and lack substance.
- The founder of NBI.AI responds to the feedback, stating that historically they have focused on growth, marketing, and sales data narratives, and that the AI-generated insights are written in natural language.
- There is a discussion about the interpretation and understanding of AI-generated data narratives and the importance of connecting data from various sources to generate focused growth insights and recommendations.

Overall, the discussion provides a mix of skepticism and interest in AI-generated data narratives, with some users sharing their own experiences and suggestions. The founder of NBI.AI actively participates in the discussion, addressing concerns and providing more information about the platform.

### Unauthorized "David Attenborough" AI clone narrates developer's life, goes viral

#### [Submission URL](https://arstechnica.com/information-technology/2023/11/unauthorized-david-attenborough-ai-clone-narrates-developers-life-goes-viral/) | 227 points | by [seasicksteve](https://news.ycombinator.com/user?id=seasicksteve) | [187 comments](https://news.ycombinator.com/item?id=38302319)

In a creative and unauthorized experiment, developer Charlie Holtz combined GPT-4 Vision and ElevenLabs voice cloning technology to create an AI version of David Attenborough narrating his every move on camera. Holtz used a Python script called "narrator" to take a photo from his webcam every five seconds and feed it to GPT-4V, which processed the image and generated Attenborough-style text. This text was then fed into an ElevenLabs AI voice profile trained on Attenborough's speech. The demo video of the experiment has gained significant attention on social media, with mixed reactions from the audience. While some expressed discomfort with imitating Attenborough's voice without permission, others found the demonstration amusing and creative.

The discussion on the submission starts with a comment questioning the ethical concerns of voice cloning and replicating famous individuals. Another user points out that the technology allows for the creation of commercial narrations in the styles of famous voices like Attenborough and Freeman. The conversation then shifts to a debate about the significance and influence of classic works of literature and how technology can impact their reproduction. Some users argue that technological advancements have made it easier for classics to be produced and distributed, while others argue that the quality and cultural impact of works from different time periods cannot be easily compared. Another user brings up the idea that generations often have different points of reference and familiarity with certain things, which affects artistic expression and experimentation. One user mentions a BBC documentary narrated by David Attenborough. The conversation then diverts to a discussion about the recycling of cultural content and the push for profit and nostalgia. Some users express concerns about the lack of originality and artistic challenge in replicating older works, while others discuss the dynamics of the entertainment industry and how content creation and consumption have evolved. One comment suggests that AI could potentially create new episodes of old shows like Inspector Gadget. However, another user disagrees, stating that AI-generated content eliminates creativity and renders results meaningless. The conversation then touches on the craftsmanship involved in animation and the varying levels of effort put into different animation styles. The discussion concludes with a mention of a science fiction character, Duncan Idaho.

### A PCIe Coral TPU Finally Works on Raspberry Pi 5

#### [Submission URL](https://www.jeffgeerling.com/blog/2023/pcie-coral-tpu-finally-works-on-raspberry-pi-5) | 110 points | by [mikece](https://news.ycombinator.com/user?id=mikece) | [20 comments](https://news.ycombinator.com/item?id=38308552)

The Raspberry Pi 5 can now natively support the PCIe Coral TPU, an AI accelerator used for tasks like machine vision and audio processing. Previously, getting the PCIe Coral TPU to work on a Raspberry Pi was challenging due to quirks in the Compute Module 4's PCIe implementation. However, with the improved PCIe bus on the Raspberry Pi 5, it is now possible, although a few tweaks are required. These include switching to a 4K page size, disabling PCIe ASPM, and making changes to the device tree. Additionally, due to compatibility issues, running the Coral's PyCoral library requires either Docker or installing an alternate system-wide Python version. While there are no commercially-available HATs or adapter boards for connecting the Coral TPU to the Raspberry Pi 5's PCIe header, options like the HatDrive! Top or Bottom from Pineberry Pi or the Coral B+M key module with an appropriate adapter can be used. Once set up, the Coral TPU can be used for various AI tasks, such as image classification. Overall, this development opens up new possibilities for AI acceleration on the Raspberry Pi platform.

Some notable points from the discussion on Hacker News about the Raspberry Pi 5's PCIe support for the PCIe Coral TPU are:

- The comparison is made between various AI accelerators, including HBM3E HAT mk TPUs, NVIDIA Jetson Nano, NVIDIA Orin Nano and AGX, and Coral Mini-PCIe. The discussion includes the TPU's computing power, Tensor Processing Units (TPU) architecture, DLSS architecture, and Vision and Versatile Processor Units (VPU).
- One user mentions the Radxa Rock 5B's NPU, which supports various types of acceleration such as INT4, INT8, INT16, FP16, BF16, and TF32 with a computing power of 6TOPs.
- The Coral TPU's software requirements are discussed, including the need for Python 3.9, which may be a challenge for some users.
- Discussion touches on alternative options, such as Hailo, which is considered a powerful competitor to Coral but may face power-related issues and Python's Global Interpreter Lock (GIL) limitation.
- There are mentions of alternative connectors, such as USB, for the Coral TPU.
- The software support for NPUs in general is considered lacking, highlighting the need for better development and momentum in this area.
- The compatibility of Coral TPU with Ubuntu 20.04 and Python versions is discussed, with reference to the support and versions provided by Ubuntu and AWS Lambda runtimes.
- A user mentions that binary bindings for Coral TPU are only supported on Ubuntu 18, limiting the compatibility with different system versions.
- The discussion briefly shifts towards the Orange Pi 5 RK3588 and its NPUs, with links to SDKs and quickstart guides.
- There is a mention of the Frigate object detection library gaining support for RK3588 NPUs and the need for an upgrade to support this new chip.
- One user suggests that hardware companies prefer to develop AI hardware rather than software, which can sometimes result in poor software support.
- Keeping Python versions up to date is considered important, although one user raises the point that some popular Python libraries may not work on versions beyond 3.9.
- Lastly, there is a brief comment about handling PC cooling with the Coral TPU.

### Google's Gemini model is delayed

#### [Submission URL](https://www.theverge.com/2023/11/16/23964937/googles-next-generation-gemini-ai-model-is-reportedly-delayed) | 93 points | by [keskival](https://news.ycombinator.com/user?id=keskival) | [66 comments](https://news.ycombinator.com/item?id=38300990)

Google's highly anticipated next-generation AI model, codenamed "Gemini," is reportedly facing delays. Initially expected to launch this month, sources now suggest that Gemini's release has been pushed to the first quarter of 2024. The project, which aims to rival OpenAI's GPT-4, is being led by Demis Hassabis, the leader of Google's unified AI team formed earlier this year. The team is combining the best ideas and expertise from both research groups to develop a cutting-edge, multimodal AI model. Interestingly, Google co-founder Sergey Brin is said to be actively involved in the development process, spending a significant amount of time working with the developers.

The discussion on Hacker News revolves around various aspects of Google's Gemini project and the delays it is facing. Some users speculate that Sergey Brin's involvement may be causing the project to slow down, while others argue that his contributions could be beneficial. There is also discussion about the potential impact of Gemini and its competition with OpenAI's GPT-4. Some users express skepticism about the project's ability to disrupt the AI market, while others anticipate significant advancements. Additionally, there are discussions about Google's business model, the limitations of current AI models, and the role of LLMs (large language models) in search. Overall, the discussion highlights a range of opinions and perspectives on Gemini and its significance in the AI landscape.

### AIConfig â€“ source control format for gen AI prompts, models and settings

#### [Submission URL](https://github.com/lastmile-ai/aiconfig) | 91 points | by [saqadri](https://news.ycombinator.com/user?id=saqadri) | [16 comments](https://news.ycombinator.com/item?id=38306410)

LastMile AI has released a new open-source project called aiconfig. It is a config-driven, source control friendly AI application development framework. The framework allows developers to separate prompts, model parameters, and model-specific logic from their application code, simplifying development and iteration on prompts and models. It also provides an AI Workbook editor, which is a notebook-like playground to edit aiconfig files visually, run prompts, tweak models and model settings, and chain things together. The project supports multiple AI models and modalities, including text, image, and audio. It also provides an SDK for both Python and Node.js. Overall, aiconfig aims to simplify AI application development and make it more accessible to developers.

The discussion on Hacker News about the LastMile AI's new open-source project, aiconfig, focused on various aspects of the project.

One commenter, "sqdr," mentioned that they haven't seen AI developer tools that generate config-driven AI application before. They noted that the framework separates prompts, model parameters, and model-specific logic from the application code, which simplifies development and iteration. They also mentioned the AI Workbook editor, which allows users to visually edit aiconfig files and run prompts.
Another commenter, "zby," asked about the documentation for dynamic parameters and interactive Workbook editor. They were also interested in understanding how function calls are chained and if previous function call results can be accessed.
One user, "jdwyh," shared a link to an article they published about dynamic configuration for AI prompts. They mentioned that using prompts in a code configuration format can help handle changes, allow analysts to type prompts easily, and facilitate the rollout of targeted prompts.
"ctvsctt" shared their experience getting started with aiconfig and thanked the OP for sharing the project. They mentioned that they have been copying and pasting prompts and result links in a browser back and forth. They appreciated the tool's ability to save prompts and results locally.
"sqdr" thanked "ctvsctt" for their feedback and mentioned that they are working on improving the UX and providing APIs for interacting with the configuration.
Another commenter, "kordlessgn," mentioned that they have been working on a similar project using Jinja2 templates and containerization. They shared a link to their project and said they are constantly making progress.
"sqdr" appreciated the contribution and thanked them for it.
"smy20011" mentioned that having the source controlled is easier to manage and appreciated the ability of aiconfig to connect the application code to the configuration.
"thrwnm" briefly looked at a few similar projects and mentioned their interest in trying aiconfig.
Another commenter, "smy20011," mentioned that while configuring non-business logic, such as string databases or feature flags, is straightforward, configuring prompts and business logic can become harder to read and maintain.
One user, "jshk," shared a link to a similar project called "promptflow."
"thtxlnr" compared aiconfig to Ollama and discussed low-level integration and the overlap between the two projects.

Overall, the discussion revolved around different aspects of aiconfig, including its separation of prompts and model parameters from application code, the use of dynamic parameters, the ease of iterating on prompts, and the challenge of configuring business logic.

### Wikidata, with 12B facts, can ground LLMs to improve their factuality

#### [Submission URL](https://arxiv.org/abs/2305.14202) | 210 points | by [raybb](https://news.ycombinator.com/user?id=raybb) | [84 comments](https://news.ycombinator.com/item?id=38304290)

A new research paper titled "Fine-tuned LLMs Know More, Hallucinate Less with Few-Shot Sequence-to-Sequence Semantic Parsing over Wikidata" presents a method to improve large language models' factuality by grounding them with the vast amount of information in Wikidata. The paper introduces WikiWebQuestions, a high-quality question answering benchmark for Wikidata, and proposes a few-shot sequence-to-sequence semantic parser for the dataset. The parser is trained to use either results from an entity linker or mentions in the query. The experimental results show that this methodology achieves a strong baseline of answer accuracy in the dev and test sets of WikiWebQuestions. By combining the semantic parser with GPT-3, the researchers were able to provide useful answers to 96% of the questions in the dev set. The paper also demonstrates that their method outperforms the state-of-the-art for the QALD-7 Wikidata dataset.

The discussion on this submission covers various aspects of the research paper and the use of large language models (LLMs) in general. Some key points from the discussion include:

- Some users suggest that the original source should be submitted instead of Twitter links.
- There is a discussion about the limitations of current LLMs in understanding contextual patterns and the potential benefits of training them with data from sources like Wikidata.
- The effectiveness of using Wikidata for fact-checking and improving the accuracy of responses generated by LLMs is debated.
- Retrieval Augmented Generation (RAG) is mentioned as a method to improve the performance of LLMs on knowledge-intensive tasks by combining information retrieval with text generation.
- The discussion touches on the challenges of fact-checking and the potential limitations of relying on LLMs for providing accurate information.
- There is a discussion about the role of Wikidata in improving the quality and consistency of information used by LLMs.
- The need for human validation and the limitations of post-processing techniques in ensuring accuracy are mentioned.
- Some users express skepticism about the robustness of LLMs and their ability to handle complex queries and provide accurate information.
- The importance of training LLMs with grounded and reliable data is emphasized.
- The limitations of LLMs in handling postmortem reasoning and providing robust explanations are discussed.

Overall, the discussion highlights both the potential benefits and limitations of using large language models and the challenges in improving their factuality and accuracy.

### We Automated Bullshit

#### [Submission URL](https://www.cst.cam.ac.uk/blog/afb21/oops-we-automated-bullshit) | 354 points | by [fanf2](https://news.ycombinator.com/user?id=fanf2) | [315 comments](https://news.ycombinator.com/item?id=38302635)

In a blog post titled "Oops! We Automated Bullshit.", Alan Blackwell shares his thoughts on the role of artificial intelligence (AI) and its tendency to produce bullshit. Blackwell highlights the recent attention AI has received from political leaders, such as US President Biden and British PM Rishi Sunak, who seem captivated by the idea of an AI-driven future where work becomes obsolete. However, Blackwell argues that the problem lies in AI's ability to generate text that "sounds good" but lacks evidence, logic, or truth. He references MIT Professor Rodney Brooks, who describes ChatGPT (an AI model) as "making up stuff that sounds good." Other prominent AI researchers, including Geoff Hinton, echo these concerns, warning that AI systems could become super-persuasive without being intelligent, imitating the worst behaviors of political leaders like Donald Trump or Boris Johnson. By relying on predictive text rather than factual information, these AI systems produce what Blackwell refers to as "bullshit." He cites philosopher Harry Frankfurt's concept of bullshit, which is defined as talking without knowing what one is talking about and disregarding the authority of truth. Blackwell also mentions David Graeber's analysis of "bullshit jobs," where over 30% of British workers believe their jobs contribute nothing of value to society. Graeber argues that these types of jobs, which can easily be done by AI systems, train individuals to generate bullshit. In conclusion, Blackwell raises questions about the future of work in an AI-driven world and whether producing bullshit will become the only kind of work needed.

The discussion surrounding the submission touches on various points related to language and knowledge. Some users argue that language is a representation of knowledge, while others assert that language contains non-knowledge nonsense. The concept of justified true belief is brought up, with some expressing skepticism about the possibility of true knowledge. There is also a discussion about the limitations of AI and its ability to generate knowledge. The complexity of language models and the importance of understanding their limitations are mentioned as well. Overall, the discussion explores different perspectives on the relationship between language and knowledge and the role of AI in generating meaningful information.

### Satya Nadella's Statement on OpenAI

#### [Submission URL](https://blogs.microsoft.com/blog/2023/11/17/a-statement-from-microsoft-chairman-and-ceo-satya-nadella/) | 84 points | by [sanketsaurav](https://news.ycombinator.com/user?id=sanketsaurav) | [17 comments](https://news.ycombinator.com/item?id=38312355)

Today, Microsoft shared that they are ramping up their innovation in the field of AI with over 100 new developments. These advancements span across their entire technology stack, including AI systems, models, and tools in Azure, as well as their recently introduced Copilot. The company is dedicated to bringing these innovations to their customers while also planning for future growth. They emphasized their long-term collaboration with OpenAI, ensuring access to the necessary resources for their innovation agenda. Microsoft is committed to working together with OpenAI to bring the significant advantages of AI technology to the world.

The discussion on this submission revolves around Microsoft's announcement and their collaboration with OpenAI. Some commenters express skepticism about the full capabilities of OpenAI and the need for robust fallback access to the source code. Others discuss Microsoft's past failures and the potential impact on investors. Some argue that the statement from Microsoft is just marketing and lacks substance. However, there are also comments highlighting Microsoft's commitment to innovation and the long-term partnership with OpenAI. One commenter emphasizes the importance of reliability and industry risk in Microsoft's investment. Overall, opinions are mixed, with some questioning the intentions behind Microsoft's announcement and others applauding their efforts to bring AI advancements to customers.

