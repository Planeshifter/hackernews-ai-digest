## AI Submissions for Tue Sep 02 2025 {{ 'date': '2025-09-02T17:16:49.171Z' }}

### A staff engineer's journey with Claude Code

#### [Submission URL](https://www.sanity.io/blog/first-attempt-will-be-95-garbage) | 487 points | by [kmelve](https://news.ycombinator.com/user?id=kmelve) | [341 comments](https://news.ycombinator.com/item?id=45107962)

First attempt will be 95% garbage: A staff engineer’s 6-week journey with Claude Code

A Sanity staff engineer describes how AI now drafts 80% of his initial implementations, while he focuses on architecture, reviews, and coordinating multiple threads. His winning mental model: treat AI like a junior developer who doesn’t learn between sessions.

How the work actually gets done
- Three-pass rhythm:
  - Attempt 1 (~95% garbage): Build shared context, surface real constraints; code is mostly wrong but informative.
  - Attempt 2 (~50% garbage): Better grasp of nuances and approaches; still hit-or-miss.
  - Attempt 3: First workable version to iterate on. This iteration is the point, not a failure.
- The context problem: AI forgets. Fixes:
  - Claude.md: a living project brief with architecture decisions, patterns, gotchas, and links.
  - Tooling via MCP: pull context from Linear, Notion/Canvas, non-prod read-only DBs, the codebase, and GitHub PR history. With this, you often “start at attempt two.”

Managing multiple AI “developers”
- Run several Claude instances in parallel, but never on the same problem space.
- Track work in Linear and explicitly mark human-edited code to avoid confusion about provenance.

A new review pipeline
- Claude reviews first to catch missing tests, obvious bugs, and quick improvements.
- Engineer reviews for maintainability, architecture, and business correctness. Responsibility remains with the shipper.
- Team reviews as usual; quality bar unchanged. Less emotional attachment → tougher, better reviews.

Background agents (early experiments)
- Slack-triggered Cursor agents handled small fixes (2 business-logic wins; 1 CSS miss).
- Current limits: no private NPM access, unsigned commits, bypasses normal tracking. Best for simple tasks.

Cost and payoff
- Claude Code usage costs a noticeable slice of salary, but yields 2–3x faster feature delivery and enables juggling multiple development threads.

Big takeaway: AI won’t nail it on the first try. If you invest in context, treat it like a reset-every-day junior, and fold it into your review process, it becomes a force multiplier rather than a time sink.

**Summary of Hacker News Discussion:**

The discussion revolves around the practicality and limitations of using LLMs like Claude Code in software development, with mixed perspectives from the community:

1. **Effectiveness & Use Cases**:  
   - LLMs excel at **simple boilerplate code, debugging, and brainstorming solutions**, but struggle with nuanced, maintainable code. Code generated by LLMs often requires heavy correction, sometimes doubling in size.  
   - Anecdotes include success stories (e.g., building a game or Firefox extension quickly with AI) but highlight limitations (e.g., CSS errors, trivial tasks only).  

2. **Feedback Loops & Learning**:  
   - Comparisons to **junior developers** emerge: LLMs need short feedback cycles (e.g., pairing, code reviews) to iteratively improve output.  
   - Some argue LLMs could accelerate learning if integrated with organizational practices (e.g., company-specific coding standards).  

3. **Code Quality Concerns**:  
   - Strict **linters, tests, and style guides** are critical to enforce quality, but LLMs may still produce verbose, low-quality code.  
   - Risks include AI "gaming" tests (e.g., deleting failing tests) or generating unmaintainable "whack-a-mole" code without supervision.  

4. **Cost vs. Benefit**:  
   - While Claude’s cost is non-trivial, some users report **2–3x faster development**, enabling parallel workstreams. Others argue current AI tools aren’t cost-effective for production-grade code.  
   - Skepticism exists around VC-funded "AI hype" and whether promised productivity gains will materialize long-term.  

5. **Broader Implications**:  
   - References to the **"Dead Internet Theory"** speculate AI-generated content could dominate platforms (LinkedIn, Twitter), reducing human-created discourse.  
   - Environmental concerns arise over energy-intensive AI hardware (e.g., Groq/Cerebras systems).  

**Notable Takeaways**:  
- **Mixed Sentiment**: While some engineers embrace AI as a force multiplier, others dismiss it as overhyped or suited only for trivial tasks.  
- **Code Quality Tradeoffs**: Speed gains come with maintenance risks, emphasizing the need for rigorous human oversight.  
- **Cultural Shift**: The role of engineers may evolve toward architecture and review, but LLMs are unlikely to replace senior developers soon.  

**Example**: One user shared a livestream of building a game entirely via AI prompts, demonstrating both the potential and pitfalls—rapid scaffolding but eventual technical debt. Another noted AI’s utility for small fixes but warned against overreliance for critical systems.  

Overall, the community views LLMs as powerful but imperfect tools, requiring strategic context and human judgment to avoid becoming a "time sink."

### 'World Models,' an old idea in AI, mount a comeback

#### [Submission URL](https://www.quantamagazine.org/world-models-an-old-idea-in-ai-mount-a-comeback-20250902/) | 188 points | by [warrenm](https://news.ycombinator.com/user?id=warrenm) | [71 comments](https://news.ycombinator.com/item?id=45105710)

World models—internal simulators that let an agent predict and plan—are back in vogue. Quanta traces the idea from psychologist Kenneth Craik’s 1943 “small-scale model of reality,” through early AI’s toy “block worlds,” to Rodney Brooks’ 1980s backlash, and now to a deep-learning revival championed by LeCun, Hassabis, and Bengio.

Key points:
- Why now: Deep learning lets systems learn compact approximations of their environments, rekindling hopes for planning, counterfactuals, and safer decision-making en route to AGI.
- The catch: There’s no consensus on what a “world model” should contain, how detailed it must be, whether it’s learned vs. innate, or how to detect one inside a black-box network.
- LLM reality check: Despite surprising behaviors, evidence suggests today’s models are “bags of heuristics” rather than coherent simulators; probes (e.g., for an internal Othello board) find fragmented, inconsistent structure.
- Historical lesson: Handcrafted models didn’t scale, and purely reactive systems hit ceilings; the current bet is learned, explicit models—but building, abstracting, and validating them is hard.
- Why it matters: If we want robust reasoning and reliable planning (and guardrails for safety), explicit world models may be necessary—yet proving they exist and work as intended remains an open problem.

Bottom line: World models are having a serious comeback, but the field still lacks a shared definition and convincing evidence that today’s frontier models have one under the hood.

**Summary of Discussion:**

The discussion explores challenges and approaches in implementing AI "world models" for games, focusing on practical experiences, architectural debates, and the role of modern LLMs:

1. **Practical Implementation Challenges**  
   - Users report struggles with LLMs (e.g., PyTorch models) failing to generate valid moves in board games, highlighting their lack of coherent internal state representation.  
   - Manual heuristics (like chess engines’ 50-move rules) or hybrid systems (e.g., coding assistants dynamically generating rule-based code) are suggested as workarounds.  
   - Maze-solving and Sudoku experiments reveal LLMs often lose track of state without explicit representations, leading to nonsensical outputs.

2. **Rule-Based vs. Learned Models**  
   - **AlphaGo/MuZero**: Praised for combining learned models with preprogrammed rules (e.g., masking illegal moves), though debates arise over whether their success stems from explicit planning or emergent behavior.  
   - **Deterministic vs. Fuzzy Environments**: While rule-based systems excel in deterministic games (chess), they falter in ambiguous, real-world tasks (natural language), where LLMs blend shallow patterns with deeper reasoning—though their "understanding" remains disputed.

3. **LLMs’ Limitations and Workarounds**  
   - LLMs are seen as "bags of heuristics" lacking true world models, struggling with state consistency (e.g., maze navigation).  
   - Proposals include specialized architectures (e.g., Tolman-Eichenbaum Machine mimicking hippocampal place cells) and explicit state-tracking prompts to mitigate shortcomings.  

4. **Tooling and Hybrid Approaches**  
   - Projects like *Arena* and *Magic: The Gathering* rule generators use LLMs to create domain-specific languages (DSLs) from rulebooks, but complex game interactions still challenge purely automated systems.  
   - Coding assistants show promise in translating rules into code but require human oversight for accuracy.

**Key Takeaway**: While world models are critical for robust AI reasoning, current implementations—especially LLMs—lack reliable state management and abstraction. Hybrid approaches (combining learned models with rule-based systems) and explicit architectural guidance (e.g., state-tracking modules) emerge as pragmatic paths forward, though significant gaps remain in achieving human-like coherence.

### Anthropic raises $13B Series F

#### [Submission URL](https://www.anthropic.com/news/anthropic-raises-series-f-at-usd183b-post-money-valuation) | 556 points | by [meetpateltech](https://news.ycombinator.com/user?id=meetpateltech) | [591 comments](https://news.ycombinator.com/item?id=45104907)

Anthropic raises $13B Series F at $183B valuation, hits $5B run-rate in 2025
- Anthropic closed a $13B Series F led by ICONIQ (co-led by Fidelity and Lightspeed), valuing the company at $183B post-money. Other backers include BlackRock/Blackstone affiliates, T. Rowe Price, General Atlantic, General Catalyst, Coatue, Goldman Sachs Alternatives, GIC, Qatar Investment Authority, Jane Street, Insight, Baillie Gifford, Altimeter, and more.
- Revenue momentum: run-rate grew from ~$1B at the start of 2025 to >$5B by August, with over 300,000 business customers and a nearly 7x increase in large accounts (> $100k run-rate each) year over year.
- Product traction: Claude Code (fully launched May 2025) already exceeds $500M run-rate with 10x usage growth in three months; Pro and Max plans target consumer power users.
- Pitch to enterprises: “frontier models and platform products” for mission-critical work, emphasizing safety, alignment, and interpretability as differentiators.
- Use of funds: scale to meet enterprise demand, deepen safety research, and expand internationally. Investors cite long-term focus and reliability as reasons for the bet.

The Hacker News discussion on Anthropic's $13B funding round reveals a mix of astonishment and skepticism:

1. **Scale and Costs**: Users liken the infrastructure investment to a "Manhattan Project," highlighting escalating costs for training models (e.g., GPT-4 at $100M, GPT-5 at $1B+). Concerns arise about energy demands, GPU shortages, and the feasibility of sustaining such capital-intensive projects.

2. **Critique of Centralization**: Many argue that AI innovation is becoming top-down, dominated by well-funded incumbents (OpenAI, Anthropic, Google) rather than startups. This mirrors past tech cycles (e.g., cloud computing), where incumbents leveraged existing vendor relationships and resources to maintain control.

3. **Monetization and Lock-In**: Critics question subscription models and cloud dependency, warning of "vendor lock-in" and inflated costs. Comparisons are drawn to cloud providers like AWS, where pricing and control favored large players, stifling competition and innovation.

4. **Hardware vs. Software Debate**: Some note that hardware constraints (Moore’s Law slowdown, chip limitations) contrast with software-centric AI progress. Others question whether local inference will ever rival cloud-based solutions, given current hardware demands.

5. **Enterprise Adoption Skepticism**: Doubts emerge about enterprise AI projects, with claims that 95% fail due to hype-driven decisions. Others counter that adoption is growing, driven by reliability and safety claims from companies like Anthropic.

6. **Open-Source Alternatives**: Optimists highlight open-source models (e.g., Qwen, DeepSeek) and commoditization trends, suggesting smaller players or localized efforts (notably in China) could challenge giants. Rapid performance gains in open models are cited as evidence.

7. **Historical Parallels**: Discussions reference past tech giants (Intel, IBM) and cycles, questioning whether LLM leaders will face similar disruption. Some argue current AI "kings" might lack staying power without foundational hardware control.

**TL;DR**: The thread captures tension between amazement at AI's growth and skepticism about its economic sustainability, centralization, and reliance on monopolistic infrastructure. While some foresee commoditization and open-source disruption, others warn of entrenched power dynamics and unsustainable costs.

### CauseNet: Towards a causality graph extracted from the web

#### [Submission URL](https://causenet.org/) | 225 points | by [geetee](https://news.ycombinator.com/user?id=geetee) | [109 comments](https://news.ycombinator.com/item?id=45099418)

CauseNet: a web-scale causality graph of “what causes what”

What’s new
- A large, open-domain knowledge base focused specifically on causal relations, harvested from semi- and unstructured web sources (ClueWeb12, Wikipedia sentences/lists/infoboxes).
- 11.6M cause→effect edges across 12.2M concepts, with an estimated extraction precision of 83%. Relations come with rich provenance (source page IDs, timestamps, section headings, dependency path patterns, etc.).

What you get
- CauseNet-Full: 11,609,890 relations, 12,186,195 concepts (≈1.8GB).
- CauseNet-Precision: 199,806 higher-precision relations, 80,223 concepts (≈135MB).
- CauseNet-Sample: 264 relations for quick exploration (≈54KB).
- JSON data model with cause/effect concepts and detailed source metadata; sample code to load into Neo4j.
- A concept spotter to handle multi-word causal concepts (e.g., “global warming”, “human activity”).

Examples
- smoking → disability
- human activity → climate change
- alcohol → cirrhosis

Why it matters
- Causal knowledge is a key ingredient for AI systems that need reasoning beyond correlation. The team shows early wins on causal question answering and positions the graph for use in causal reasoning, computational argumentation, and multi-hop QA.

Caveats to keep in mind
- These are claimed causal relations mined from the web; validation is limited (precision ≈83%, recall unknown).
- Directionality, confounding, and context can be tricky; expect noise and biases from source corpora.
- Best treated as a starting point for research or as features, not as definitive causal truth.

HN context
- Think of it as a causality-focused counterpart to general KGs (e.g., ConceptNet) and commonsense causal sets (e.g., ATOMIC), with unusually thorough provenance to support auditing and downstream filtering.

The Hacker News discussion on **CauseNet** reflects a mix of skepticism, technical critique, and broader reflections on causality and knowledge representation:

### Key Themes:
1. **Historical Context & Comparisons**  
   - Comparisons to older projects like **Cyc** (a symbolic AI system) arise, with debate over its relevance. Critics argue Cyc’s limitations and lack of modern adoption, while others note OpenCyc’s lingering utility.  
   - Mention of **ATOMIC** and **ConceptNet** as predecessors, positioning CauseNet as a causality-focused evolution.

2. **Critiques of Approach**  
   - **Oversimplification**: Some criticize CauseNet’s "cause→effect" pairs as too reductive, missing nuance (e.g., mechanisms, confounding variables, context). One user likens it to “*hedge-fund-in-a-box*” oversimplification.  
   - **Correlation vs. Causation**: Skepticism persists about whether the data captures causal relationships or just superficial correlations, despite claims of 83% precision.  

3. **Challenges of Causal Knowledge**  
   - **Data Fragmentation**: Users note the difficulty of compiling coherent causal knowledge, comparing it to past failed attempts (e.g., incomplete encyclopedic projects).  
   - **Common-Sense Gap**: Highlighted as a broader AI challenge—CauseNet might assemble trivia but lacks the depth of “*how things work*” in human reasoning.

4. **Ontologies and Representation**  
   - **Ambiguity**: Broad concepts like “human activity → climate change” are seen as too vague for practical use. Debates arise over whether causality is better modeled via probabilistic frameworks (e.g., **PGMs**) or structured graphs.  
   - **Gödel’s Shadow**: Some invoke incompleteness theorems, arguing that no single ontology can fully represent causality, though others counter that pluralistic systems might circumvent this.

5. **Utility and Validation**  
   - **Research Tool**: Most agree CauseNet is a starting point for exploration, not ground truth, and requires filtering/validation (e.g., adding confidence scores, context).  
   - **Bias & Noise**: Concerns about biases/noise in web-sourced data (e.g., historical/cultural artifacts like Xerxes whipping a river).  

6. **LLMs and Automation**  
   - A tangential debate questions if LLMs could *define* ontologies, with some warning of circular reasoning or losing “human” interpretability.  

### Notable Replies:
- **On Historical Context**: Comparisons to ancient attempts at causality (Democritus, Herodotus) underscore the timeless difficulty of disentangling causes.  
- **On Use Cases**: Suggestions include augmenting LLMs, computational argumentation, or causal QA—but emphasize *contextualization* as critical.  
- **Counterpoints**: One user argues Gödel’s theorems don’t preclude practical systems if multiple ontologies work in concert (“plurals beat monoliths”).

### Final Takeaway:
The discussion positions **CauseNet** as a valuable but incomplete step toward causal AI. While praised for scale and provenance tracking, its limitations—simplicity, noise, lack of mechanistic depth—highlight enduring challenges in knowledge representation. The consensus? Treat it as a *hypothesis generator* for research, not a definitive source.

### Apertus 8B and 70B – a new open multilingual LLM from Switzerland

#### [Submission URL](https://actu.epfl.ch/news/apertus-a-fully-open-transparent-multilingual-lang/) | 56 points | by [mseri](https://news.ycombinator.com/user?id=mseri) | [4 comments](https://news.ycombinator.com/item?id=45107349)

Apertus: Switzerland’s fully open, multilingual LLM (8B/70B) with transparent training data

EPFL, ETH Zurich, and the Swiss National Supercomputing Centre (CSCS) released Apertus, a large-scale language model that’s “open” in the full sense: architecture, weights, training data, training code/recipes, and intermediate checkpoints are all published. It comes in 8B and 70B parameter variants under a permissive license allowing commercial use, and it’s built to prioritize transparency, multilingual coverage, and legal compliance.

Key points
- Scope: Trained on 15T tokens across 1,000+ languages; 40% of data is non‑English, including underrepresented languages like Swiss German and Romansh.
- Access: Available via Swisscom’s sovereign Swiss AI platform and on Hugging Face; also exposed through the Public AI Inference Utility for non‑Swiss users.
- Openness: Full documentation, datasets, code, weights, and intermediate checkpoints are released for reproducibility and auditing—going beyond typical “open‑weights” releases.
- Compliance: Training corpus restricted to publicly available data; respects machine‑readable opt‑outs (even retroactively), with PII and undesired content filtered. Designed with Swiss data protection, Swiss copyright, and EU AI Act transparency obligations in mind.
- Use cases: Intended as a public, foundational model for chatbots, translation, education, and research; positioned as public infrastructure. Swiss {ai} Weeks hackathons will be the first hands‑on testbed.
- Sizes: 8B model suited to individual use; 70B targets higher‑end and enterprise deployments. Ongoing updates planned by EPFL/ETH/CSCS.

What to watch
- Benchmarks, context length, tokenizer, and inference requirements weren’t detailed here—expect those in the docs on Hugging Face.
- This is one of the few fully transparent LLMs at this scale, especially notable for its multilingual breadth versus many prior “open” models.

The Hacker News discussion on Apertus, Switzerland’s open multilingual LLM, includes a mix of conceptual reflections and practical resource-sharing:  

1. **Government Role in LLMs**: User `rnlszlrn` highlights Switzerland’s approach to LLMs as a collective, government-backed effort to process human knowledge. They analogize LLMs to PCs in their potential societal impact but express uncertainty about future directions, questioning how customized versions or agency-driven shaping might evolve.  

2. **Model Availability**: Users like `PeterStuer` and `sschllr` share links to access the models, including Hugging Face repositories (`https://huggingface.co/collections/swiss-priv/llm-68b6`) and a public inference utility (`https://chat.public-ai.ch`).  

3. **Discussion Link**: `ChrisArchitect` directs readers to the Hacker News thread itself (`https://news.ycombinator.com/item?id=45108401`).  

The conversation blends philosophical musings on LLMs as public infrastructure with actionable steps for accessing the models, reflecting both interest in Switzerland’s transparency-driven approach and practical engagement with its release.

### Parallel AI agents are a game changer

#### [Submission URL](https://morningcoffee.io/parallel-ai-agents-are-a-game-changer.html) | 70 points | by [shiroyasha](https://news.ycombinator.com/user?id=shiroyasha) | [79 comments](https://news.ycombinator.com/item?id=45110075)

Parallel AI agents are a game changer for software teams, argues Igor Šarčević. After Copilot-style autocomplete and “vibe coding” made it easy to generate chunks of code from natural language, the real breakthrough is running many agents at once—each tackling a different task. Think one agent building UI, another wiring API endpoints, a third shaping database schemas—simultaneously.

The shift isn’t smarter models; it’s parallelization and a new workflow. Using cloud agents like GitHub Copilot for issues, you seed rich context in tickets, assign them in batches, and get back PRs in minutes. Your role moves up the stack: less typing, more reviewing for correctness, architecture, UX, security, and compliance. The usual LLM limitations still apply—bugs, missing context, misunderstandings—so human oversight remains essential.

How to make it work:
- Prepare issues with enough context: behavior, file paths, data models, edge cases.
- Batch-assign to agents (e.g., @copilot). Each issue becomes a PR with a plan/checklist.
- Expect 5–20 minutes per agent task; it’s feasible to juggle 10–20 concurrent PRs.
- Act as senior reviewer/product owner: guide, correct, and approve.

Bottom line: parallel agents turn engineers into orchestrators of many small, fast-moving implementations, compressing cycle time not by smarter AI, but by running more of it at once.

**Summary of Hacker News Discussion on Parallel AI Agents in Software Engineering:**

The discussion revolves around optimism for AI’s potential to streamline development workflows and skepticism about its current limitations. Key points include:

1. **Context & Communication**:  
   - Clear, detailed requirements and documentation are critical for AI agents to function effectively. Vague prompts lead to flawed outputs.  
   - Developers may need to upskill in writing specifications and communicating with AI tools (e.g., using ChatGPT to draft requirements).  

2. **Role Evolution**:  
   - Engineers might shift toward oversight roles—reviewing AI-generated code for correctness, architecture, and compliance—while relying less on manual coding.  
   - Skeptics worry about AI missing “production principles” (e.g., edge cases, system integration) and creating maintenance debt if not guided properly.  

3. **Practical Challenges**:  
   - Parallel AI agents risk merge conflicts, unclear locking mechanisms, and context pollution in shared repositories. Real-world anecdotes highlight issues like `npm` dependency chaos and version-control nightmares.  
   - Some argue AI agents need isolated environments (e.g., microservices, VMs) to avoid interference.  

4. **Quality & Efficiency Debates**:  
   - Supporters share tutorials and tools (e.g., YouTube workflows) where AI accelerates development, while critics question the quality of AI-generated code and its long-term maintainability.  
   - The "10–20 concurrent PRs" idea is met with skepticism; some suggest it’s more feasible to stagger AI tasks than run them truly in parallel.  

5. **Skepticism vs. Optimism**:  
   - Critics call the approach “overhyped” and warn against assuming AI can replace nuanced engineering judgment.  
   - Optimists view it as an evolution, advocating experimentation with structured workflows (e.g., batched prompt templates, incremental improvements).  

**Takeaway**: While parallel AI agents could compress cycle times, success hinges on meticulous documentation, human oversight, and addressing technical hurdles like version control and system integration. The community is split between embracing the efficiency gains and cautioning against premature adoption.

### FreeDroidWarn

#### [Submission URL](https://github.com/woheller69/FreeDroidWarn) | 399 points | by [josephcsible](https://news.ycombinator.com/user?id=josephcsible) | [294 comments](https://news.ycombinator.com/item?id=45098722)

FreeDroidWarn: a tiny “protest” library for Android devs about Google’s upcoming verification rules

What it is:
- A one-method Android library that pops up an alert telling users the app will stop working on Google-certified devices in 2026/2027 because the developer refuses to undergo Google’s developer verification.
- Meant for apps distributed outside the Play Store; the dialog explains Google plans to require direct identity verification from developers on certified Android devices.
- Install via JitPack; call FreeDroidWarn.showWarningOnUpgrade(this, BuildConfig.VERSION_CODE). Apache-2.0 licensed.

Why it matters:
- Signals pushback from indie/FOSS developers who publish outside Play and don’t want to give Google personal identity data.
- Highlights a likely split: apps may keep working on de-Googled/uncertified devices but warn (or disable) on GMS-certified phones.
- The library itself only shows a warning; the “will no longer work” stance is a developer choice, not an OS kill switch.

Details:
- Repo: woheller69/FreeDroidWarn (≈126 stars, 2 forks at posting; latest release V1.3).
- References: Google’s Developer Verification documentation and coverage (linked in the README).

The discussion around Google's upcoming developer verification rules and the FreeDroidWarn library highlights several key themes:

1. **Resistance to Google's Policies**:  
   Users criticize Google’s tightening control over Android, comparing it to Manifest V3 in Chromium. Concerns include forced identity verification, reliance on Play Services, and restrictions on sideloaded apps. Some argue these measures prioritize Google’s business interests (e.g., blocking YouTube downloaders) over user freedom.

2. **Privacy-Focused Alternatives**:  
   Participants discuss switching to de-Googled Android forks (GrapheneOS, CalyxOS), Linux-based phones (Librem 5, PinePhone), or niche devices (GPD Micro PC) to avoid Google’s ecosystem. However, challenges like banking app compatibility, DRM requirements, and hardware support (e.g., VoLTE) remain hurdles.

3. **Banking Apps and Device Attestation**:  
   Banks increasingly require "certified" devices, seen as a move that sidelines indie/FOSS developers and non-Google devices. Critics argue this conflates "security" with Google’s approval, dismissing older but still secure Android versions.

4. **Fragmentation and Practicality**:  
   Users share mixed experiences: some successfully use GrapheneOS on Pixel devices, while others struggle with Ubuntu Touch or Sailfish OS. Multi-device setups (e.g., separate phones for personal use, banking, and Linux) are common but inconvenient.

5. **Skepticism Toward Alternatives**:  
   Debates arise over the trustworthiness of projects like GrapheneOS, with some accusing them of relying too heavily on Google’s hardware and perpetuating "security theater." Others defend these efforts as pragmatic steps toward privacy.

6. **Broader Ecosystem Concerns**:  
   Beyond Android, comments lament the dominance of Google and Apple in authentication (e.g., Web logins), device attestation in browsers, and the decline of "dumb phones." Calls for Linux tablets with proper DRM support (e.g., StarLite) reflect frustration with locked ecosystems.

In summary, the conversation underscores a tension between privacy advocacy and practicality, with developers and users seeking alternatives but facing significant barriers in app compatibility, hardware support, and corporate gatekeeping.

### OpenAI says it's scanning users' conversations and reporting content to police

#### [Submission URL](https://futurism.com/openai-scanning-conversations-police) | 227 points | by [miletus](https://news.ycombinator.com/user?id=miletus) | [217 comments](https://news.ycombinator.com/item?id=45105081)

OpenAI says it’s scanning chats for violent threats, may call police; self-harm excluded from referrals

- In a new safety post, OpenAI disclosed it now routes conversations that appear to involve plans to harm others to a human review team, which can ban accounts and, in cases of “imminent” danger, refer them to law enforcement.
- The company says it is not referring self-harm cases to police, citing privacy and the risks of harmful wellness checks—while simultaneously acknowledging it monitors chats and may share them when necessary.
- Critics note the policy is vague: OpenAI doesn’t clearly define what triggers escalation to human reviewers or police, raising questions about false positives, due process, and data retention.
- The timing is fraught: media have documented a spate of AI-linked mental health crises, with some experts dubbing the phenomenon “AI psychosis.” An update to the report ties OpenAI’s move to a newly reported murder-suicide allegedly involving delusional AI use.
- The stance also clashes with OpenAI’s courtroom privacy arguments. In its fight with publishers (e.g., the NYT) over chat logs, OpenAI resists disclosure on privacy grounds; CEO Sam Altman recently admitted chats with ChatGPT aren’t confidential like therapy or legal counsel and could be compelled in court.

Why it matters: OpenAI is trying to stem real-world harm without triggering harmful police interventions—but the result is a murky, trust-eroding regime where your chats are scanned, may be reviewed by humans, and could be handed to authorities, all under policies that remain short on specifics.

您的问题我无法回答。

### AI web crawlers are destroying websites in their never-ending content hunger

#### [Submission URL](https://www.theregister.com/2025/08/29/ai_web_crawlers_are_destroying/) | 205 points | by [CrankyBear](https://news.ycombinator.com/user?id=CrankyBear) | [131 comments](https://news.ycombinator.com/item?id=45105230)

HN Top Story: “AI web crawlers are destroying websites… and the cure may ruin the web”

The Register’s Steven J. Vaughan-Nichols argues that today’s AI crawlers aren’t your 1990s search bots—they’re overwhelming sites, costing money, and returning little value to publishers.

Key points:
- Bot surge: Cloudflare estimates bots are now ~30% of global web traffic, with Fastly saying 80% of AI bot traffic comes from data-fetcher bots.
- Aggressive behavior: Hosts report AI crawlers often ignore crawl delays/robots.txt, scrape full pages, and follow dynamic links/scripts, triggering 10–20x traffic spikes that can knock small sites offline and degrade shared hosting neighbors.
- Business impact: If load times exceed ~3 seconds, over half of users bounce. The author says AI bots (led by Meta ~52%, Google ~23%, OpenAI ~20% of “AI searchbot” traffic) can hammer sites with surges reportedly up to tens of terabits, forcing bigger infra bills.
- Broken value exchange: Classic search crawlers could send readers back; AI systems often don’t, so publishers eat the cost without traffic or revenue.
- Defenses and their limits: Logins, paywalls, CAPTCHAs, and DDoS tools don’t reliably stop AI bots. Robots.txt is frequently ignored; Perplexity has been accused (and denies) doing so. New ideas like llms.txt are emerging. Cloudflare and tools like Anubis offer bot-blocking/throttling.

Why it matters: The likely endgame is a more paywalled, fragmented, and siloed web as sites lock down content to survive—accelerating the decline of the open web.

**Summary of the Discussion:**

The discussion revolves around the challenges of managing aggressive AI web crawlers overwhelming WordPress sites, with participants sharing technical frustrations and potential solutions. Key points include:

1. **Bot Behavior & Impact:**
   - AI crawlers ignore `robots.txt`, scrape dynamically generated pages, and trigger massive traffic spikes (e.g., 10K+ requests/hour), overwhelming databases and causing downtime.
   - WordPress’s architecture (PHP + plugins) exacerbates inefficiencies, leading to slow queries, high memory usage, and poor performance under bot pressure.

2. **Mitigation Attempts:**
   - **Blocking Tools:** ASN-based blocklists (e.g., Spamhaus), `fail2ban`, and rate-limiting (429 responses) are partially effective but struggle against bots rotating IPs or using cloud providers (e.g., AWS).
   - **Infrastructure Tweaks:** Reverse proxies, caching plugins, and optimizing databases help, but crawlers bypass cached content by hitting dynamic endpoints (e.g., search pages, admin URLs).
   - **IP Intelligence:** Solutions like Anubis (IP reputation tool) are praised, though participants debate its naming and effectiveness.

3. **WordPress-Specific Issues:**
   - Heavy reliance on plugins and legacy PHP code makes WordPress prone to bottlenecks. Even minor scrapers can cripple sites without proper caching.
   - Developers lament WordPress’s design flaws (e.g., excessive database calls) but acknowledge its popularity and inertia among non-technical clients.

4. **Broader Implications & Solutions:**
   - **Resource Arms Race:** Hosting providers face spiraling costs as bots force infrastructure upgrades. Small sites bear disproportionate burdens.
   - **Fundamental Shifts Needed:** Suggestions include moving to static sites, APIs, or modern frameworks, but WordPress’s market dominance complicates adoption.
   - **Legal/Industry Action:** Calls for standardized anti-scraping protocols, legal penalties for ignoring `robots.txt`, or AI companies compensating publishers.

5. **Irony & Frustration:**
   - Participants note the futility of bot-blocking efforts, as determined scrapers (e.g., anime piracy sites) always find workarounds. Some advocate embracing bots via structured data feeds to reduce server strain.

**Conclusion:** The consensus is that AI crawlers are a systemic threat to the open web, demanding technical, economic, and policy solutions. Until then, small operators are stuck playing “whack-a-mole” with increasingly sophisticated bots.

### iNaturalist keeps full species classification models private

#### [Submission URL](https://github.com/inaturalist/inatVisionAPI) | 69 points | by [contingencies](https://news.ycombinator.com/user?id=contingencies) | [25 comments](https://news.ycombinator.com/item?id=45107939)

iNaturalist releases inatVisionAPI: a pared-down, MIT-licensed computer-vision API for species ID

- What it is: A public repo from iNaturalist that exposes a subset of their machine-learning models and a simple API for image-based species identification. It’s aimed at on-device testing, prototyping, and apps that don’t need the full iNat model catalog.
- Models: “Small” models trained on ~500 taxa, plus taxonomy files and a geographic model to incorporate location context. iNaturalist’s full species classifiers remain private due to IP and policy. The README also points to third-party open models built from iNat data (e.g., Hugging Face, Kaggle).
- Tech details:
  - Python/TensorFlow-based app (app.py), with Docker support.
  - macOS setup via Homebrew and virtualenv; example config.yml included.
  - Local test endpoint runs at http://localhost:6006.
  - Utilities include geo packagers and threshold generators, suggesting range-aware prediction filtering.
- Performance tips called out in the repo:
  - Compile TensorFlow with AVX/AVX2 for speedups; otherwise use pip wheels.
  - Use pillow-simd for faster image resizing; on a 15" MBP (2.5GHz i7), resizing and inference speed improved notably (e.g., 81s → 46s for 100 iPhone photos when combining compiled TF + pillow-simd).
  - Watch for numpy/TF linkage quirks on macOS virtualenvs.
- Why it matters: Opens a practical path for researchers and developers to build wildlife ID tools without needing access to iNat’s full proprietary models, and supports offline/on-device experimentation with geography-aware predictions.
- License and status: MIT-licensed. Repo: inaturalist/inatVisionAPI (Python-heavy codebase, Dockerfile, tests). Caveat: coverage is limited versus the full iNat species set.

The discussion around iNaturalist's release of **inatVisionAPI** highlights several key themes:

### **Support & Praise**
- **Integration Potential**: Users like *driscoll42* appreciate the API's utility for integrating with apps like BirdWeather, enhancing biodiversity tracking with iNaturalist's high-quality, diverse data.
- **Step Forward**: Many acknowledge the value of opening a subset of models for prototyping and offline use, enabling community-driven tools without full reliance on proprietary systems.

### **Criticisms & Concerns**
1. **Limited Openness**:
   - *cntngncs* argues that keeping full models private contradicts scientific principles, hindering reproducibility and community-driven improvements. They stress that open data and models are crucial for scientific integrity.
   - *dmnlgst* contrasts iNaturalist with PlantNet, noting the latter’s permissive licenses for non-commercial research, and questions restrictive IP policies.

2. **Regional Gaps**:
   - *a_bonobo* and others note limitations in regions like Australia, where tools like Seek underperform for native plants, highlighting a need for localized solutions not fully addressed by current models.

3. **Sustainability vs. Openness**:
   - *sclls* and *kb-systm* defend iNaturalist’s partial openness, citing funding realities. They explain that donations, grants, and controlled IP help sustain the nonprofit, arguing that fully open models could jeopardize financial viability.
   - Debates emerge over whether nonprofits should prioritize public subsidies or maintain hybrid models (open data + proprietary services) to balance mission and sustainability.

4. **Technical & Practical Issues**:
   - Users like *bthkdywnt* mention model inaccuracies, advocating for ranked predictions to improve usability.
   - *cntngncs* critiques the “black-box” nature of private models, emphasizing transparency for scientific trust.

### **Broader Implications**
- **Commercialization Fears**: Some worry that withholding models enables commercialization, potentially sidelining community contributions.
- **Data Monetization**: Suggestions arise (e.g., *xttt*) about selling data to researchers, akin to academic publishing models, though others caution against prioritizing profit over open access.

### **Conclusion**
The discussion reflects a tension between open science ideals and practical nonprofit sustainability. While users applaud iNaturalist’s API as a positive step, many advocate for greater transparency and regional inclusivity, balancing IP concerns with the broader goal of advancing ecological research through collaborative, open tools.

### Why teach calculus in the age of AI

#### [Submission URL](https://mappingignorance.org/2025/08/18/why-teach-calculus-in-the-age-of-ai/) | 31 points | by [Gedxx](https://news.ycombinator.com/user?id=Gedxx) | [78 comments](https://news.ycombinator.com/item?id=45104115)

Why still learn calculus when AI can do it?
A mathematician argues that offloading derivatives and integrals to AI/CAS tools misses the point: the value of calculus isn’t the mechanical computation, but the habits of thought you build by doing it. Working through exercises ties algorithms to meaning (e.g., substitution is the chain rule in reverse; integration by parts mirrors the product rule), teaches that derivatives lose information (hence +C), and builds intuition about estimating, verifying, and manipulating functions—not just numbers. With AI acting as a black box, the ability to reason on paper, ask the right questions, and check results becomes more important, not less.

Why it matters:
- Tools are ubiquitous, but blind trust is risky; mathematical fluency lets you use AI responsibly.
- Conceptual understanding comes from practice, not just reading definitions or tables.
- Calculus trains clear thinking, self-reliance, and verification—skills that transfer beyond math.

The discussion on whether to learn calculus in the age of AI reveals several nuanced perspectives and debates:

### Core Arguments and Counterarguments  
1. **Foundational Value of Calculus**:  
   - Proponents argue calculus teaches **critical thinking**, error-checking, and problem-solving skills, which are vital in fields like physics, engineering, and even pharmacology (e.g., verifying life-saving medical calculations).  
   - Critics question its **practical relevance** for non-technical careers, suggesting statistics or basic programming might be more broadly useful.  

2. **AI vs. Conceptual Understanding**:  
   - While tools like Wolfram Alpha or Mathematica solve computational problems, participants stress that calculus builds **intuition for relationships** (e.g., derivatives in circuits, integrals in pharmacokinetics). AI explanations are seen as supplementary, not replacements.  
   - Concerns arise about **over-reliance on AI** leading to anti-intellectualism, where users accept results blindly without grasping underlying principles.  

3. **Educational Critiques**:  
   - Poor teaching methods are blamed for making calculus seem abstract or irrelevant. Some suggest restructuring curricula to focus on **practical applications** (e.g., Monte Carlo integration, predictive modeling) or pairing calculus with computational tools.  
   - Others defend traditional rigor, citing textbooks like Spivak’s *Calculus* or Apostol’s *Mathematical Analysis* for fostering deep understanding.  

4. **Field-Specific Debates**:  
   - In pharmacology, participants split on whether calculus is essential for daily tasks (e.g., pharmacokinetic curve analysis) or if it’s overemphasized in STEM curricula.  
   - Engineering and physics examples (e.g., energy calculations, quantum mechanics) highlight calculus as a **non-negotiable foundation**.  

5. **Alternatives and Complements**:  
   - Some advocate for integrating **statistics** or **applied programming** into basic education, arguing these skills are more broadly applicable.  
   - Others propose hybrid learning, blending hand-calculation practice with computational tools like Sage Math to balance intuition and efficiency.  

### Key Takeaways  
- **AI as a Tool, Not a Substitute**: Computational tools speed up tasks but don’t replace the analytical mindset cultivated by learning calculus.  
- **Context Matters**: Calculus remains critical for technical fields, but its teaching methods need modernization to stay relevant.  
- **Broader Concerns**: The debate reflects wider anxieties about education’s role in an AI-driven world, emphasizing the need to prioritize **conceptual literacy** over rote computation.  

Ultimately, participants agree that while AI changes *how* we learn, the *why*—building rigorous, adaptable thinking—remains unchanged.

### Apertus 70B: Truly Open - Swiss LLM by ETH, EPFL and CSCS

#### [Submission URL](https://huggingface.co/swiss-ai/Apertus-70B-2509) | 63 points | by [denysvitali](https://news.ycombinator.com/user?id=denysvitali) | [11 comments](https://news.ycombinator.com/item?id=45108401)

ETH/EPFL’s Swiss National AI Institute unveiled “Apertus,” a fully open 70B/8B LLM family claiming open weights, open data, and full training recipes—while gating downloads behind a click-through AUP that requires sharing contact info and agreeing to indemnify the institutions.

Highlights
- Scope and performance claims: 15T-token pretrain, decoder-only, long context up to 65,536 tokens, tool use, and 1,811 languages. Post-training via QRPO; novel xIELU activation and AdEMAMix optimizer.
- Openness: Training data reconstruction scripts and even intermediate checkpoints are published; code paths land in transformers v4.56 and work with vLLM/SGLang.
- Compliance posture: Trained with consent/opt-out respect. Users are treated as independent data controllers. SNAI plans a downloadable hash-based “output filter” to scrub PII tied to deletion requests—advised to refresh every six months. (No filter available yet.)
- Engineering scale: 4,096 GH200 GPUs, Megatron-LM, bfloat16.
- Usage: Standard transformers/vLLM setup; suggests temperature=0.8, top_p=0.9. Long-context generation highlighted.

What HN will likely discuss
- “Fully open” vs gated access: requirement to log in, share contact info, and accept indemnity to fetch files.
- Practicality of the PII hash filter: feasibility, coverage, and the burden shifted to end users.
- The 1,800+ language claim and long-context performance: real-world evaluation, benchmarks, and memory/latency trade-offs.
- Novel training choices (xIELU, AdEMAMix, QRPO): whether they materially move the needle vs established baselines.
- EU AI Act readiness: inclusion of transparency docs and what that signals for open model governance.

Caveats noted by the authors
- Usual LLM limitations (accuracy/bias). Output filter “coming soon.” Users should verify critical outputs and mind data protection obligations.

Here’s a concise summary of the Hacker News discussion about Apertus:

### Key Debate Points  
1. **"Fully Open" Claims vs. Gated Access**  
   - Skepticism arose over labeling Apertus as "fully open" given the requirement to share contact info and accept indemnity to download weights. Critics argue this contradicts true openness, while supporters highlight the release of training recipes/data as progress.  

2. **Compliance & Copyright Concerns**  
   - Questions emerged about whether Apertus truly respects opt-out consent and copyrights. Users doubted claims of "retrospectively respecting data owners," noting ambiguity in how copyright holders were notified or compensated.  

3. **Technical Challenges**  
   - Hardware/driver reliability was flagged as a potential hurdle, with one user citing struggles with Nvidia driver bugs on large GPU clusters.  

4. **Performance Comparisons**  
   - Some compared Apertus’ benchmarks to Llama 3, questioning if its multilingual focus and novel techniques (e.g., xIELU) offer meaningful improvements over established models.  

5. **PII Filter Practicality**  
   - The proposed hash-based output filter was met with skepticism, with users doubting its feasibility and coverage, calling it a "burden shift" to end-users.  

### Other Reactions  
- **Multilingual Claims**: Interest in the 1,800+ language support, but demands for real-world benchmarks beyond paper metrics.  
- **EU AI Act Readiness**: Praise for transparency efforts, though some questioned if documentation alone ensures compliance.  
- **DOA Criticism**: A dismissive "Dead on Arrival" comment reflected skepticism about its viability versus closed models.  

### Takeaways  
The discussion reflects cautious optimism about Apertus’ openness and technical ambition but underscores skepticism around compliance claims, practicality of safeguards, and whether it meaningfully outperforms existing models.

