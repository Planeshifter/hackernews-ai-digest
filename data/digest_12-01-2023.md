## AI Submissions for Fri Dec 01 2023 {{ 'date': '2023-12-01T17:10:30.704Z' }}

### The Inside Story of Microsoft's Partnership with OpenAI

#### [Submission URL](https://www.newyorker.com/magazine/2023/12/11/the-inside-story-of-microsofts-partnership-with-openai) | 208 points | by [jyunwai](https://news.ycombinator.com/user?id=jyunwai) | [89 comments](https://news.ycombinator.com/item?id=38486394)

In a surprising turn of events, OpenAI, the artificial intelligence startup in which Microsoft had invested billions of dollars, fired its CEO and co-founder, Sam Altman. This news came as a shock to Microsoft CEO, Satya Nadella, who had a close working relationship with Altman and had just collaborated with OpenAI on a major AI rollout called the Office Copilots. The Copilots, powered by OpenAI's technology, were integrated into Microsoft's core productivity programs and allowed users to interact with software in a more natural and conversational way. However, behind the scenes, tensions had been brewing between Altman and OpenAI's board, with some members feeling that Altman had been manipulative and deceitful. This firing not only threatened the partnership between Microsoft and OpenAI but also ignited a larger debate about the responsible development and deployment of AI technology.

The discussion surrounding the firing of OpenAI CEO and co-founder Sam Altman on Hacker News revolves around several key points. 

One commenter highlights a previous post by Helen Toner, who expressed concerns about the dangers of AI and suggested that Altman may have misled board members. Another commenter argues that people should not blindly trust those who claim to be advancing AI for good and points out the controversy surrounding OpenAI's board.

There is also a discussion about Microsoft's involvement in OpenAI and the potential impact this firing may have on their partnership. Some express concern about the commercialization of AI and the spread of misinformation, while others argue that Altman's removal was necessary for the overall safety and control of AI.

Other commenters bring up the larger issue of trust and accountability in AI development, highlighting the need for responsible decision-making and the potential risks of losing control as AI becomes more powerful.

Overall, the discussion reflects a mix of skepticism, concern, and support for both Altman and OpenAI's decision to remove him as CEO.

### OpenAI delays launch of custom GPT store until early 2024

#### [Submission URL](https://www.axios.com/2023/12/01/openai-delays-launch-custom-gpt-store-2024) | 98 points | by [cloudking](https://news.ycombinator.com/user?id=cloudking) | [63 comments](https://news.ycombinator.com/item?id=38491314)

OpenAI has announced a delay in the launch of its GPT store until early 2024, according to a memo seen by Axios. The GPT store, where people will be able to distribute custom versions of ChatGPT, was initially scheduled to open last month. The store was a highly anticipated feature announced by OpenAI at last month's DevDay conference. While custom GPTs can currently be shared through links, the store will allow for broader distribution. OpenAI also intends to share some of the revenue generated from ChatGPT Plus subscriptions with creators of popular GPTs. The company stated that it has some exciting updates for ChatGPT in the meantime. This news comes amid a tumultuous period for OpenAI, which recently saw CEO Sam Altman fired and then rehired within a week.

The discussion on Hacker News regarding the delayed launch of OpenAI's GPT store is varied. Some users express frustration with the current user experience of ChatGPT and question the company's focus on plugins and features instead of addressing fundamental issues. There is criticism of the complexity and lack of control over the frontend system, as well as the slow development and disconnectedness from scaling and improvements. One user mentions the potential usefulness of OpenAI's competitor, Cohere.

Others discuss the shortcomings of the default GPT model and propose that custom GPTs could solve some of these limitations. Some users mention the difficulties in creating and using custom GPTs, such as limited context and integration issues. The need for better version control and the preference for GPTs that can be trained on-source are also mentioned.

The discussion also touches on OpenAI's business models and revenue-sharing plans. Some users express skepticism about OpenAI's monetization strategies and the impact on developers. A comparison is made to Google's Gemini project, suggesting that other AI startups are showing more coherent efforts.

There are also comments about the CEO change at OpenAI and speculation about how it may impact the product. Some users suggest that OpenAI should consult domain experts to improve GPTs.

A few users mention alternative tools and services, such as ArxivXplorer, for dealing with GPT-related challenges. The importance of feedback and open communication between OpenAI and its community is highlighted in one comment.

Overall, the discussions reflect a mix of frustrations, suggestions for improvement, skepticism about OpenAI's strategies, and explorations of alternative approaches.

### What Is Retrieval-Augmented Generation a.k.a. RAG?

#### [Submission URL](https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/) | 82 points | by [teleforce](https://news.ycombinator.com/user?id=teleforce) | [25 comments](https://news.ycombinator.com/item?id=38491251)

Today's top story on Hacker News is about a new technique in generative AI called retrieval-augmented generation (RAG). RAG is a process that enhances the accuracy and reliability of AI models by fetching facts from external sources, filling a gap in how large language models (LLMs) work. LLMs, like judges in a courtroom, can respond to a wide range of human queries, but they often require an assistant to do research and provide authoritative answers with cited sources. RAG serves as the court clerk of AI, connecting generative AI models to external resources and enabling them to cite sources, clear up ambiguity in user queries, and reduce the possibility of making wrong guesses. The technique also allows users to have conversations with data repositories, opening up new kinds of experiences and making applications for RAG multiple times the number of available datasets. Companies like AWS, IBM, Google, and Microsoft are already adopting RAG. NVIDIA has developed a reference architecture for retrieval-augmented generation to help users get started and has included it in their AI Enterprise software platform. The NVIDIA GH200 Grace Hopper Superchip is ideal for RAG workflows, as it provides massive amounts of memory and compute, resulting in a significant speedup. RAG doesn't require a data center and can be run on Windows PCs equipped with NVIDIA RTX GPUs, making it accessible to users even on their laptops. Overall, RAG represents the future of generative AI by improving accuracy, reliability, and user trust.

The discussion on the submission about retrieval-augmented generation (RAG) involves various viewpoints and topics. Some users question the accuracy and adequacy of RAG, suggesting that it may not be effective in generating high-quality answers without fine-tuning on relevant knowledge-rich question-and-answer pairs. Others point out that RAG compensates for the limitations of large language models (LLMs) by allowing them to approximate and retrieve information from external sources.

The potential use of RAG in structuring unstructured text and solving problems is also discussed. Suggestions are made to explore coupling vector embeddings with knowledge graphs to provide informed answers. The effectiveness of using vector strings and different search types is highlighted.

There are references to related articles and resources that cover topics such as semantic search and getting started with vector-based retrievals. The controversy surrounding RAG and its potential impact on the AI industry is touched upon, as well as the limitations and challenges of integrating RAG into existing systems.

One user mentions their plans to use RAG for document search and references GPT4All, a private project using GPT-2, and the RAG technique.

Additionally, there are discussions about Huggingface blocking access to certain resources and AI models and AWS's efforts in utilizing RAG and other technologies.

### Are Open-Source Large Language Models Catching Up?

#### [Submission URL](https://arxiv.org/abs/2311.16989) | 331 points | by [rkwz](https://news.ycombinator.com/user?id=rkwz) | [207 comments](https://news.ycombinator.com/item?id=38481970)

The paper titled "ChatGPT's One-year Anniversary: Are Open-Source Large Language Models Catching up?" by Hailin Chen and 7 other authors explores the progress of open-source large language models (LLMs) in comparison to closed-source LLMs. The release of ChatGPT in late 2022 had a significant impact on the AI landscape, demonstrating the ability of LLMs to answer questions and follow instructions on a wide range of tasks. Since then, there has been increased interest and development in LLMs, with many new models emerging in academia and industry. While closed-source LLMs generally outperform their open-source counterparts, the progress of open-source LLMs has been rapid, with claims of achieving equal or even better performance on certain tasks. This has important implications for both research and business. The authors provide a comprehensive overview of the success of open-source LLMs on the first anniversary of ChatGPT, surveying all tasks where an open-source LLM has claimed to be on par or better than ChatGPT.

The discussion on this submission revolves around the restrictions and access to ChatGPT in China, particularly in relation to the Great Firewall (GFW). Some users share their experiences of trying to access ChatGPT from China and Hong Kong, with some claiming that it is blocked by the firewall. There is speculation that OpenAI's decision to restrict registration using Hong Kong phone numbers and credit cards is deliberate and might be influenced by government policies. Others discuss the possibility that OpenAI is trying to slow down China's development of AI technologies. The discussion also touches on the challenges faced by AI providers in complying with different countries' regulations, such as GDPR in Europe and data control laws in China. Some users mention Baidu ChatGPT as an alternative for Chinese speakers, while others express curiosity about the performance of ChatGPT in the Chinese language.

### 1960s chatbot ELIZA beat OpenAI's GPT-3.5 in a recent Turing test study

#### [Submission URL](https://arstechnica.com/information-technology/2023/12/real-humans-appeared-human-63-of-the-time-in-recent-turing-test-ai-study/) | 19 points | by [FrustratedMonky](https://news.ycombinator.com/user?id=FrustratedMonky) | [9 comments](https://news.ycombinator.com/item?id=38494102)

A recent preprint research paper titled "Does GPT-4 Pass the Turing Test?" conducted by researchers from UC San Diego compared OpenAI's GPT-4 AI language model to human participants, GPT-3.5, and the 1960s computer program ELIZA to determine which could convincingly pass as human. Surprisingly, the study found that human participants correctly identified other humans in only 63% of interactions, and ELIZA outperformed GPT-3.5. GPT-4 achieved a success rate of 41%, second only to actual humans. The study raises questions about using the Turing test as a benchmark for evaluating AI model performance and highlights the importance of linguistic style and socio-emotional traits in determining whether a participant believes they are interacting with a human or an AI model. However, it's worth noting that the study has limitations, including potential sample bias and the absence of peer review.

The discussion in the comments revolves around the surprising results of the study comparing GPT-4, GPT-3.5, and ELIZA in passing the Turing test. Some users express doubts about the relevance of the Turing test in judging AI's ability to mimic human conversation. They argue that current AI models like GPT-3.5 may not intentionally generate human-like responses, unlike ELIZA, a program developed in the 1960s. However, others disagree, noting that GPT-4 achieved a success rate of 41%, second only to actual humans, and the study highlights the importance of linguistic style and socio-emotional traits in making participants believe they are interacting with a human or an AI model. Additionally, there is speculation about the training and feedback process for GPT-4 and its potential improvements over GPT-3.5. One user also points out that ELIZA, despite being a relatively simple program, achieved a success rate of 27% in the study. Overall, there is interest and intrigue about the performance of GPT-4 and its comparison to both GPT-3.5 and ELIZA.

### Local councillors, unaware, approve law entirely written by AI in Brazil

#### [Submission URL](https://www.smh.com.au/world/south-america/local-councillors-unaware-approve-law-written-entirely-by-ai-20231201-p5eobi.html) | 16 points | by [flykespice](https://news.ycombinator.com/user?id=flykespice) | [5 comments](https://news.ycombinator.com/item?id=38492806)

In a surprising turn of events, local councillors in the southern city of Porto Alegre, Brazil, unknowingly approved legislation that was entirely written by artificial intelligence (AI). Councillor Ramiro Rosario enlisted OpenAI's chatbot ChatGPT to draft a proposal aimed at preventing the city from charging taxpayers for stolen water consumption meters. Rosario presented the proposal to his fellow council members without disclosing its AI origin, which led to unanimous approval and the subsequent enactment of the law. The incident has sparked concerns and debates about the role of AI in public policy, especially regarding the understanding and interpretation of complex legal principles. While some experts see potential in AI-powered chatbots like ChatGPT, others worry about the unintended consequences of relying on machines for tasks currently performed by humans.

The comments on this article cover a range of perspectives on the incident. One user points out that it is common for bills to be passed without politicians thoroughly reading or understanding them, so the fact that an AI wrote this legislation is not necessarily surprising. Another user suggests that the conflicting nature of the law could have been avoided if it had been written by humans who had the opportunity to discuss and amend it. They argue that it is difficult for a machine to account for all the nuances and concerns of the public. Another user raises concerns about the risks of allowing projects approved solely based on artificial intelligence, questioning the lack of oversight. Finally, a user suggests that people should write their own legislation if they are not satisfied with the current system.

### A reality bending mistake in Apple's computational photography

#### [Submission URL](https://appleinsider.com/articles/23/11/30/a-bride-to-be-discovers-a-reality-bending-mistake-in-apples-computational-photography) | 493 points | by [indrora](https://news.ycombinator.com/user?id=indrora) | [378 comments](https://news.ycombinator.com/item?id=38482085)

In a viral social media post, a UK woman shared a photo of herself trying on wedding dresses where her reflection didn't match in two different mirrors. It turns out that this illusion was not a glitch in the Matrix, but rather a mistake in Apple's computational photography pipeline. When taking a panoramic photo, the camera captures multiple images in quick succession and stitches them together. However, when a mirror is present, the algorithm mistakenly determines that the different moments shown in each mirror are the best reflections, resulting in multiple versions of the person. This phenomenon can be recreated on recent iPhones and many smartphones due to the limitations of computational photography dealing with mirrors. Younger generations have even used this effect to create silly images for social media.

The discussion surrounding the submission centers around the limitations of computational photography and its impact on capturing mirrors. Some users express their indifference to the issue, highlighting that no photograph is entirely pixel-perfect, and artistic interpretation is part of photography. Others raise concerns about Apple's decision to automatically determine the best reflections in mirror photos, arguing that it infringes on user control. The discussion further explores the complexities of computational photography and the various distortions it can introduce. Additionally, some users mention the challenges of publishing photos on social media platforms like Facebook due to their censorship policies. Overall, the conversation highlights the intersection of technology and photography, and the trade-offs associated with computational approaches.

### Valve Launches Official Steam Link PC VR Streaming App on Quest

#### [Submission URL](https://www.uploadvr.com/valve-steam-link-quest-steamvr-streaming/) | 34 points | by [MaximilianEmel](https://news.ycombinator.com/user?id=MaximilianEmel) | [19 comments](https://news.ycombinator.com/item?id=38481414)

Valve has launched an official Steam Link app for the Meta Quest, allowing users to wirelessly play SteamVR games on their Quest headsets. The app, available on the official Quest Store, enables streaming from a gaming PC over a home Wi-Fi network. Players can also enjoy non-VR Steam games on a virtual screen. While Quest headsets already have wireless PC VR streaming capabilities through features like Air Link, Valve's solution only requires the installation of Steam and SteamVR on a PC, offering a direct and unmediated connection to SteamVR.

The discussion revolves around different aspects of the Valve Steam Link app for the Meta Quest and its implications.

1. User dpc_01234 raises concerns about Meta (previously known as Facebook) selling headsets at a loss and relying on building a network effect to make their hardware platforms profitable. They also mention that Valve effectively hijacks the platform-building effort by providing access to their SteamDeck.

2. User bonton89 mentions that they are actively using the Quest headset and they believe that even if people jailbreak it, the majority of users still want to leverage Meta's content. FirmwareBurner responds, stating that jailbreaking a Quest natively supports APK sideloading and there is a popular secondary marketplace called SideQuest for paid VR apps that don't fit within Meta's rules.

3. Several users discuss the implications of jailbreaking and collecting VR telemetry. rtrchmln expresses a desire for jailbreaking to have more telemetry collection, while rcswprk comments on Meta potentially blacklisting sideloading APKs to push their own system.

4. FirmwareBurner points out that Valve effectively hijacks the platform-building effort by offering the Steam Link app, which allows users to stream PC games to their Quest headsets. They add that the point of the Quest is to be a self-contained gaming console, so Valve's app may not be necessary for Quest owners who primarily game on the headset.

5. Users mention various streaming solutions for VR gaming, including LinkAirLink, Virtual Desktop, ALVR, and Steam Link. They discuss the integration of Steam, Oculus PCVR games, and the simplicity of the Steam Link app.

6. Some users express their interest in trying the Steam Link app, while others discuss latency issues and motion sickness that can occur during VR streaming.

Overall, the discussion includes debates about the business strategies of Meta and Valve, the benefits and limitations of VR streaming, and user experiences with different VR gaming solutions.

