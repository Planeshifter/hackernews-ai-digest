## AI Submissions for Fri Aug 22 2025 {{ 'date': '2025-08-22T17:13:52.931Z' }}

### Top Secret: Automatically filter sensitive information

#### [Submission URL](https://thoughtbot.com/blog/top-secret) | 115 points | by [thunderbong](https://news.ycombinator.com/user?id=thunderbong) | [12 comments](https://news.ycombinator.com/item?id=44981088)

Top Secret: combining regex and NER to redact PII in free text for LLM workflows

- The problem: logging/processing free-form text can leak sensitive info (emails, phone numbers, names, locations). Regex alone misses many cases; you still need the raw value for downstream APIs and to present back to users.

- The approach: Thoughtbot’s Top Secret uses both regex patterns and named-entity recognition (via MITIE Ruby) to detect and replace sensitive data with stable placeholders like [PERSON_1] and [LOCATION_1]. It returns a mapping so you can safely “restore” originals only where needed.

- LLM integration: filter every user message (and conversation history) before sending to a model; instruct the model to reference placeholders; then restore placeholders in the model’s reply using the mapping. This limits exposure to the provider while keeping responses coherent.

- Database safety: add a validation that rejects saving content containing detected sensitive data. You can disable or override specific filters (e.g., skip people/location) to tune strictness.

- Real-world ergonomics: simple Ruby APIs for filtering single strings or arrays, plus a restore helper. Designed to sit between inputs and logs/requests without breaking downstream behavior.

- Why it matters: as teams pipe user text into chatbots and other external APIs, minimizing PII exposure is critical for compliance, least-privilege data sharing, and reducing accidental logging of secrets.

- Caveats to consider: NER isn’t perfect (false positives/negatives), language/model coverage varies, and you must secure the returned mapping (it contains the raw values). Custom entity types and regexes may be needed for domain-specific secrets (API keys, account numbers, etc.).

The post includes a live demo and code samples showing message filtering, LLM prompting/restoration, and Rails model validations.

The Hacker News discussion on the **Top Secret** submission highlights key points, critiques, and complementary ideas:

- **NER Model Concerns**: Users note that MITIE, the NER library used, is nearly a decade old ([woadwarrior01](https://news.ycombinator.com/user?id=woadwarrior01)), raising questions about accuracy and modernity. Performance trade-offs (CPU usage, batch processing) are acknowledged, with clarifications that MITIE uses lightweight SVM models, not LLMs, to minimize overhead.

- **Alternatives & Comparisons**: A similar Python tool ([zink](https://github.com/deepanwadhwa/zink)) and Postgres extensions for data anonymization ([wmbtpm](https://news.ycombinator.com/user?id=wmbtpm)) are suggested, emphasizing ecosystem-specific solutions.

- **Practical Limitations**: While praised for reducing PII exposure, commenters stress that NER is imperfect ([sbpyn](https://news.ycombinator.com/user?id=sbpyn)) and advocate combining methods—regex, NER, LLMs, and human oversight—especially in high-stakes domains like aviation safety reporting ([nlv](https://news.ycombinator.com/user?id=nlv)).

- **Use Case Expansion**: Concerns extend beyond logs/APIs to scenarios like screenshot sharing ([thnkngmt](https://news.ycombinator.com/user?id=thnkngmt)), where redacted rendering might still leak data unless integrated with OS-level protections or accessibility tools.

- **Implementation Nuances**: Discussions highlight securing mappings, tuning filters, and balancing strictness (e.g., allowing locations in some contexts but not others). Real-time inference challenges and ergonomic API design are also noted.

Overall, the community applauds the approach but underscores that PII redaction demands layered, context-aware strategies rather than relying on a single tool.

### The use of LLM assistants for kernel development

#### [Submission URL](https://lwn.net/Articles/1032612/) | 70 points | by [Bogdanp](https://news.ycombinator.com/user?id=Bogdanp) | [27 comments](https://news.ycombinator.com/item?id=44990981)

HN: LWN — “On the use of LLM assistants for kernel development”

The Linux kernel community is actively debating how (and whether) to integrate LLM-based coding tools into its workflow, after an LLM-generated patch by Sasha Levin was accepted earlier this year.

Key points:
- Spark: Levin’s OSSNA talk revealed an LLM-authored kernel patch made it upstream, prompting scrutiny and new proposals.
- Proposals on disclosure: 
  - David Alan Gilbert suggests a new “Generated-by:” tag for any tool-assisted patch (LLMs and long-standing tools like Coccinelle).
  - Levin prefers using existing “Co-developed-by:” for tools, explicitly without a tool “Signed-off-by:” (DCO remains human-only), plus adding assistant configs/guidelines.
- Policy-first camp: Vlastimil Babka and Lorenzo Stoakes argue configuration patches are premature; they want an official AI policy discussed at the December Maintainers Summit to avoid signaling blanket acceptance.
- Risks cited: More low-quality or subtle-bug patches from authors who don’t understand the code; fears of “answering review with the same LLM.” QEMU’s near-ban is referenced as one model; Al Viro calls LLMs a “force multiplier” for bad machine-generated submissions.
- Pragmatic view: Mark Brown expects silent use regardless of policy, so favors transparency. Kees Cook opposes bans as unrealistic and unenforceable, noting the tools are getting useful.
- Where to disclose: Some (Konstantin Ryabitsev) say tool usage belongs in cover letters, not permanent tags; Jakub Kicinski calls tags “free advertising” and only review-relevant.
- Trajectory: An outright ban looks unlikely. Expect movement toward clearer rules on disclosure, accountability (human S-o-b), review expectations, and possibly standard assistant configs—likely shaped at the Maintainers Summit.

Why it matters: The kernel’s stance will influence norms across large open-source projects, balancing productivity gains with code quality, review burden, and contributor responsibility.

The Hacker News discussion on integrating LLMs into Linux kernel development reflects divided opinions, practical concerns, and nuanced debates about AI's role in critical software. Here's a concise summary:

### Key Themes:
1. **Code Quality & Subtle Bugs**:  
   - Many express concern that LLM-generated code could introduce subtle bugs, especially in complex subsystems like cryptography or drivers. Critics argue the kernel’s robustness makes debugging harder if bad AI-assisted code slips through.
   - Some counter that LLMs could help detect issues (e.g., uninitialized variables) with AI acting as a "supercharged linter," but others note compilers or existing tools could catch these without LLM involvement.

2. **Copyright & Legal Risks**:  
   - LLM-generated code’s copyright status is unclear. If models trained on GPL/AGPL-licensed code reproduce snippets, it risks license violations and legal fallout akin to historical SCO lawsuits.  
   - Recent court rulings (e.g., training on copyrighted data ≠ infringement) are cited, but uncertainty persists, especially for code’s licensing compliance.

3. **Maintainer Burden**:  
   - Maintainers fear an influx of low-quality AI-generated PRs (e.g., "vb-coded PRs"), worsening review workloads. However, proponents suggest AI could *reduce* burden via automated checks (e.g., enforcing conventions, spotting antipatterns).

4. **Effectiveness of AI Tools**:  
   - Pragmatists acknowledge LLMs excel at narrow tasks (drafting comments, boilerplate code) but struggle with domain-specific logic. Humans must validate outputs and retain responsibility for correctness.  
   - Skeptics question the ROI of AI tooling; some report spending days integrating tools only to achieve minimal improvements.

5. **Policy & Accountability**:  
   - Corporate influence (e.g., Linux Foundation’s ties to AI sponsors) raises concerns about transparency. Critics highlight vague AI policies as risky, favoring policies that enforce explicit human accountability (e.g., retaining `Signed-off-by` tags).

### Notable Examples:
- **Claude’s Code Review**: A user cited Claude catching an uninitialized variable in a kernel driver patch. Critics countered that compiler warnings should have flagged this, questioning whether AI added unique value.  
- **License Conflicts**: Debate arose around whether AGPLv3 code in AI training data could lead to inadvertent licensing violations in generated code, posing existential legal risks.

### Final Takeaway:  
Opinions split between cautious optimism (leveraging AI for mundane tasks) and skepticism (costs outweigh benefits). Most agree humans must stay accountable for code quality and compliance, regardless of tooling. Legal ambiguities and maintainer workloads remain unresolved, suggesting policy discussions will dominate before widespread adoption.

### Waymo granted permit to begin testing in New York City

#### [Submission URL](https://www.cnbc.com/2025/08/22/waymo-permit-new-york-city-nyc-rides.html) | 565 points | by [achristmascarl](https://news.ycombinator.com/user?id=achristmascarl) | [542 comments](https://news.ycombinator.com/item?id=44986949)

Waymo gets NYC green light — but with safety drivers

- The news: Waymo received its first permit from the NYC Department of Transportation to begin autonomous vehicle testing in Manhattan and Downtown Brooklyn. It’s the city’s first official AV testing program.
- Scope: Up to eight vehicles will run through late September, with a possible extension. New York state law requires a trained safety driver behind the wheel.
- Oversight: As a condition of the permit, Waymo must regularly report data to NYC DOT and coordinate with law enforcement and emergency services.
- Context: Waymo previously collected data in NYC with manual driving in 2021. The Adams administration set AV safety rules and opened a permit program last year.
- Expansion push: Waymo says it surpassed 10 million robotaxi trips in May, launched service in Austin, expanded in the Bay Area, and is targeting Atlanta, Miami, Washington, D.C., and Philadelphia.

Why it matters: New York’s dense, complex streets are a torturous testbed. If Waymo can operate reliably here—even with safety drivers—it’s a significant validation step and a gateway to broader Northeast deployments.

Here's a concise summary of the Hacker News discussion:

**Core Debate**:  
The discussion quickly pivoted from Waymo to broader frustrations with human driving standards and traffic enforcement. Key themes:

1.  **Licensing Futility**:  
    - Many argue driver's licenses fail to ensure safety. Licensing tests often overlook reckless behaviors (DUI, texting), focusing instead on technical vehicle control.  
    - Skepticism exists about penalties: Suspended licenses are ignored, and fines ($100 tickets) don't deter dangerous drivers.

2.  **Traffic Enforcement Decline**:  
    - **Austin case study**: Citations dropped 55% (2018-2022) with tickets falling 90% by 2021, linked to reduced police capacity.  
    - **Enforcement challenges**: Police prioritize major crimes over minor traffic violations. Resources are stretched, and funds for programs like "crosswalk decoy operations" are limited.

3.  **Surveillance & Tech Limitations**:  
    - Traffic cameras face criticism:  
        *Technically* difficult to reliably identify vehicles/behaviors at scale.  
        *Ethically* criticized as "dystopian mass surveillance" (e.g., NYC’s speed-camera backlash).  
    - Privacy-concerned users oppose AI enforcement in public spaces.

4.  **Systemic Problems**:  
    - NYC struggles to collect fines, with dangerous drivers accumulating hundreds of unpaid tickets.  
    - Cultural shift noted: Rising reckless driving (e.g., red-light running) as enforcement wanes. Police admit traffic duty lacks incentives.  

5.  **Troubling Anecdotes**:  
    - Costco confrontation over a service dog escalates into commentary on societal conflict avoidance and rule enforcement fatigue.  
    - Traffic stops seen as risky for officers due to unpredictable warrants or hostility during minor violations.

**Conclusion**:  
The thread reflects deep skepticism that licensing or enforcement currently mitigates dangerous driving. Systemic underfunding, technical limits of automation, and waning societal accountability overshadow optimism about AV safety milestones like Waymo’s.

### Making LLMs Cheaper and Better via Performance-Efficiency Optimized Routing

#### [Submission URL](https://arxiv.org/abs/2508.12631) | 124 points | by [omarsar](https://news.ycombinator.com/user?id=omarsar) | [27 comments](https://news.ycombinator.com/item?id=44985278)

Beyond GPT-5: a router beats a single giant model by mixing and matching. The paper introduces Avengers-Pro, a test-time routing framework that embeds and clusters incoming queries, then dynamically sends each one to the most suitable model based on a tunable performance–efficiency score. Think FrugalGPT/MoA-style orchestration, but optimized for a Pareto frontier across accuracy and cost.

Claims (across 6 benchmarks, 8 models including GPT-5-medium, Gemini 2.5 Pro, Claude Opus 4.1):
- +7% average accuracy over the strongest single model (GPT-5-medium) at the high-accuracy end.
- Same average accuracy as the strongest single model at 27% lower cost.
- ~90% of that performance at 63% lower cost.
- Forms a Pareto frontier: highest accuracy for any given cost and lowest cost for any given accuracy among single models.

Why it matters: No retraining, just smarter inference-time routing; one knob lets teams pick cost vs quality per request. If it holds up, this is a practical blueprint for LLM ops to cut spend without tanking quality.

Caveats to watch: router overhead and latency, robustness to out-of-distribution prompts, benchmark/cost assumptions, and safety/consistency when swapping providers. Code is “available”; ongoing work.

**Summary of Discussion:**

- **Interest & Potential**: Commenters highlight the paper's innovative routing framework as a promising alternative to monolithic models, drawing parallels to Mixture-of-Experts (MoE) architectures and noting OpenAI’s GPT-5 might already internally use similar routing. The GitHub link shared ([Avengers-Pro](https://github.com/ZhangYiqun018/AvengersPro)) suggests technical interest.  
- **Practical Concerns**: Skepticism exists around real-world applicability. Critics question latency overheads (e.g., router and embedding model delays), robustness to out-of-distribution prompts, and clustering accuracy for complex reasoning tasks. User `hbfn` argues benchmarks may overstate success if real-world queries differ.  
- **Existing Solutions**: Users mention [NotDiamond](https://notdiamond.ai/) (founded 2 years ago) as a comparable product, reflecting prior work in optimizing model routing for cost/accuracy trade-offs.  
- **Technical Nuances**: Discussions cite challenges like embedding model selection (Qwen3-Embedding-8B) and hardware scaling. Some propose hybrid systems combining large "base" models with smaller specialized ones for efficiency.  
- **Broader Implications**: Optimists see potential for cost savings and profitability in AI services, while others debate whether AGI will favor single models vs. ensembles. Concerns about dependency on multiple proprietary models (e.g., Opus, Gemini) and safety consistency arise.  
- **Benchmark Critique**: While the claimed Pareto frontier improvements (+7% accuracy over GPT-5-medium) impress, users caution that benchmarks may not reflect real-world complexity.  

**Key Takeaways**: The framework sparks excitement for smarter inference-time orchestration but faces skepticism around practicality. Latency, OOD robustness, and real-world clustering validity remain open questions. Existing tools like NotDiamond suggest market readiness, but further validation is needed.

### Fmllm: 4mb training data, 100mb model, Fibonacci embeddings, near-coherent. WTF?

#### [Submission URL](https://github.com/henrygabriels/FMLLM/blob/main/README.md) | 37 points | by [gabriel666smith](https://news.ycombinator.com/user?id=gabriel666smith) | [27 comments](https://news.ycombinator.com/item?id=44982748)

FMLLM (GitHub) — A mysterious repo surfaces, GitHub UI gets in the way
- The HN submission points to henrygabriels/FMLLM on GitHub, but the shared content is just GitHub’s session/notification chrome (“You can’t perform that action at this time”) rather than the project details.
- What we can see: it’s a public repo with early traction (12 stars, 0 forks at capture). The error text suggests a stale sign-in/session rather than anything about the project itself.
- If you clicked through and saw that message, refresh or sign in to view the README and star/watch normally.
- We’ll need the actual README or description to say what FMLLM does; the snippet doesn’t include any technical info.

Here's a concise summary of the HN discussion about the FMLLM GitHub repo:

1. **Repo Mystery & Authors Explanation**  
   - The creator (@gabriel666smith) explains FMLLM generates text using **Fibonacci sequence distances** between words. Key mechanics:  
     - **Training**: Builds forward/backward prediction models tracking words at Fibonacci positions (e.g., ±2, 3, 5, 8, 13... from a given word).  
     - **Generation**: Starts with seed words, generates candidate words at Fibonacci intervals, and validates bidirectionally to maintain contextual consistency.  
   - Updates show early tests appending Fib-interval words to LLM prompts improved creative writing outputs ([linked study](https://github.com/henrygabriels/FMLLM/blob/main/improving_l)).  

2. **Skepticism & Criticism**  
   - Critics (**jstnly**, **lttcgbln**) argue the approach resembles a **Markov chain** with Fibonacci constraints, lacking novelty. Concerns:  
     - Fails to handle complex grammar or punctuation.  
     - Outputs often incoherent, "nonsense" sentences despite syntactic structure.  
     - Questionable scalability vs. transformer models (e.g., "4MB training data breaks quickly").  
   - **lttcgbln** suggests modern LLMs could replicate results by filtering random word lists.  

3. **Comparative Experiments**  
   - **frchty** tested Fibonacci word sequences via ChatGPT/Gemini, noting outputs felt randomly assembled ("word salad").  
   - **pvtmrt** and **sbstfn** demonstrate simpler Markov chains produce similar legible-but-meaningless sentences.  

4. **Author's Response & Requests**  
   - Gabriel acknowledges FMLLM is early-stage, welcomes collaboration, and seeks advice on **code publishing standards** (e.g., academic-style repos like Phil Wang's).  
   - Defends potential in creative prompts despite limitations, promising iterative improvements.  

5. **Meta-Reactions**  
   - Many users (**frg**, **bxd**) express confusion about the repo's front-page visibility ("23 points for nonsense").  
   - **h3ctic** critiques the overly technical explanation lacking clear context.  

**Key Takeaway**: Discussion debates whether Fibonacci-structured text generation offers meaningful innovation beyond Markov chains, with the author advocating for its experimental promise amid widespread skepticism about practicality and coherence.

### Sprinkling self-doubt on ChatGPT

#### [Submission URL](https://justin.searls.co/posts/sprinkling-self-doubt-on-chatgpt/) | 140 points | by [ingve](https://news.ycombinator.com/user?id=ingve) | [95 comments](https://news.ycombinator.com/item?id=44987422)

Sprinkling Self-Doubt on ChatGPT: a simple meta-prompt that makes it smarter (and slower)

- What happened: The author set ChatGPT’s personalization to be intensely self-skeptical, to broaden its inquiry beyond assumptions, and to “red team” its own answers before declaring them done.
- What changed: Replies now open with cautious caveats, take noticeably longer to “think,” and include post-hoc adversarial checks that frequently catch and correct earlier mistakes.
- Why it matters: This lightweight prompt engineering measurably improved usefulness—especially on correctness—by building in a second-pass critique, reducing the need for the user to push back. Trade-offs: slower responses, more verbosity, and it still isn’t perfect.
- Bonus: The piece has a playful tone (e.g., “getting my money’s worth in GPU time”) and ends with plugs for the author’s RSS/newsletter/podcast.

The Hacker News discussion on the "self-doubt" ChatGPT meta-prompt reveals several key themes:

1. **Positive Reception of the Technique**:  
   Many users found value in the approach, noting that forcing ChatGPT to critique its own responses improved correctness and reduced the need for manual pushback. Comparisons were drawn to Claude’s revision-heavy behavior, though some joked about ChatGPT’s verbose "embarrassing" self-questioning style. Users highlighted trade-offs: slower responses and verbosity, but higher accuracy.

2. **Anthropomorphism Debate**:  
   A recurring argument centered on whether attributing traits like "anxiety" or "self-doubt" to AI is appropriate. Critics (*ForHackernews*, *scotty79*) called it misleading shorthand, while others (*lbrstv*) argued that LLMs exhibit rudimentary "thinking" processes (contextualizing, analyzing, self-checking). References to psychology (e.g., the Yerkes-Dodson stress-performance curve) sparked discussions about AI’s metaphorical vs. literal "stress."

3. **Technical and Model Comparisons**:  
   - Users discussed differences between ChatGPT and Claude, with the latter criticized for endless revisions and lack of clarity.  
   - Technical subthreads explored model architectures (CLIP, T5, Qwen) and prompt engineering, debating whether system prompts meaningfully alter underlying model behavior.  
   - Skeptics (*throwaway314155*) dismissed the impact of prompts, arguing models are "truth-bound" regardless of priming.

4. **Criticism of Product Design**:  
   Frustration targeted OpenAI’s design choices, such as overly verbose responses (*DrewADesign*) and declining quality in features like Advanced Voice. Users (*cj*, *NikolaNovak*) shared anecdotes of GPT-4 feeling "dumber" over time, with seasonal performance drops and superficial answers.

5. **Broader Implications**:  
   The discussion touched on AI’s role in mental workflows, with some fearing reliance on "digital Xanax" (*hnhn34*) for decision-making. Others saw potential in meta-prompts as a step toward more transparent, self-correcting systems.

In sum, the community recognized the self-doubt technique’s merits but debated its framing (anthropomorphism), practical trade-offs (speed vs. accuracy), and broader implications for AI development and user experience. Technical insights mixed with user anecdotes to paint a nuanced picture of prompt engineering’s potential and limits.

### Being “Confidently Wrong” is holding AI back

#### [Submission URL](https://promptql.io/blog/being-confidently-wrong-is-holding-ai-back) | 153 points | by [tango12](https://news.ycombinator.com/user?id=tango12) | [252 comments](https://news.ycombinator.com/item?id=44983570)

Being “Confidently Wrong” is holding AI back (Tanmai Gopal, PromptQL)

Thesis: The biggest blocker to enterprise AI isn’t lack of features—it’s confident inaccuracy. When systems sound sure but are wrong, they create a verification tax, erode trust asymmetrically, hide failure modes, and compound errors across multi‑step workflows. Accuracy multiplies like reliability doesn’t: at 90% per step, a 10‑step flow fails ~2 out of 3 times.

What to do instead: aim for “tentatively right.” Calibrate confidence, surface uncertainty causes, and abstain below thresholds—then close the loop so systems get better with each correction.

Key ideas
- Verification tax nukes ROI: if you can’t tell when it’s wrong, you must check everything.
- Trust is fragile: one high‑confidence miss outweighs many hits in serious workflows.
- Accuracy flywheel: native uncertainty → human nudge → model/plan improvement.
- Implementation path: have models generate plans in a domain‑specific DSL that compiles to deterministic actions with runtime validations and policy checks; continuously bind models to your org’s ontology, data, metrics, and edge cases to carry calibrated plan‑level confidence.
- Quick diagnostic before your next pilot: Will it tell me when it’s unsure—and why? Will it learn from my correction for the next user?

Why it matters: Citing reports that most AI pilots stall, the post argues uncertainty and plan‑level determinism—not just higher raw accuracy—are the unlocks for real enterprise adoption.

**Summary of Discussion:**

The discussion centers on the challenges of AI systems being "confidently wrong," echoing the submission’s concerns. Key points raised include:

1. **User Frustration with LLM Behavior**:
   - Users note that LLMs often fail to self-correct without explicit prompting, leading to repetitive "correction loops" where the AI reiterates incorrect responses despite feedback.
   - Example: A user highlights the cycle of "User: This is wrong → AI: Here’s a fixed answer → User: Still wrong → AI: Apologizes and repeats," which erodes trust and efficiency.

2. **Multi-Step Workflow Limitations**:
   - LLMs struggle with multi-step tasks due to training focused on single-step responses. Tools like Gemini underperform compared to Claude in handling workflows, causing restarts or inconsistent outputs.
   - Poor context retention exacerbates issues, with AI often "forgetting" prior steps or corrections, leading to spiraling inaccuracies.

3. **User Interaction Patterns**:
   - The "Pink Elephant Paradox" illustrates how LLMs fixate on prohibited concepts (e.g., mentioning "don’t think of a pink elephant" triggers the AI to reference it). Users suggest workarounds like adjusting temperature settings or automated validation tools to filter bad outputs.
   - Comparisons to debugging code emphasize the iterative, trial-and-error nature of tuning LLM interactions.

4. **Instruction Clarity and Design**:
   - Positive instructions ("Do X") work better than negative ones ("Don’t do Y"), mirroring human cognitive tendencies. Users stress the need for clear guidance and context distillation to improve reliability.
   - Critiques of tool design highlight failures to adhere to user-provided guidelines, suggesting better UX principles (e.g., Copilot’s approach of restarting conversations after errors).

5. **Skepticism About Progress and Timelines**:
   - Some express doubt about optimistic claims (e.g., "AI handling 80% of tasks in 6–18 months"), drawing parallels to overhyped promises in self-driving cars and AGI. Past failures (e.g., 2001 AGI predictions) underscore the gap between aspirations and reality.
   - Despite advances, unresolved issues like context window limitations and deterministic output remain barriers.

**Conclusion**: Participants agree that addressing "confident wrongness" requires technical improvements (e.g., uncertainty calibration, context handling) and better user-centric design. The path forward involves iterative learning from corrections, clearer instruction frameworks, and tempering expectations about near-term breakthroughs.

