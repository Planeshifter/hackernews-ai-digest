## AI Submissions for Wed Dec 24 2025 {{ 'date': '2025-12-24T17:09:47.097Z' }}

### Asterisk AI Voice Agent

#### [Submission URL](https://github.com/hkjarral/Asterisk-AI-Voice-Agent) | 159 points | by [akrulino](https://news.ycombinator.com/user?id=akrulino) | [83 comments](https://news.ycombinator.com/item?id=46380399)

Asterisk AI Voice Agent: open-source, realtime AI voice for Asterisk/FreePBX

What it is
- An MIT-licensed AI voice agent that plugs into Asterisk/FreePBX via RTP (ExternalMedia) and AudioSocket.
- Modular pipeline lets you mix and match STT, LLM, and TTS providers, or run privacy-first local pipelines.
- Ships with “golden baseline” configs validated for production, plus an Admin UI and CLI for setup and debugging.

Why it matters
- Brings modern barge-in, turn-taking, analytics, and tool integrations to existing PBX/call-center stacks without vendor lock-in (Docker, configurable providers, on-prem friendly).
- Supports both pipeline mode and “full agent” providers (e.g., Google, Deepgram, OpenAI, ElevenLabs) for native VAD/turn-taking.

What’s new in v4.5.3
- Call history and analytics: full transcripts, tool executions, errors; search/filter; export as CSV/JSON.
- Barge-in upgrades: instant interruption, platform flush, parity across RTP/AudioSocket.
- More models: Faster Whisper (GPU-accelerated STT), MeloTTS; hot-swap models from the dashboard.
- MCP tool integration: connect agents to external services via Model Context Protocol.
- RTP security hardening: endpoint pinning, allowlists, SSRC-based cross-talk prevention.
- Pipeline-first default: local_hybrid enabled by default; readiness probes reflect component health.

Getting started
- git clone, run preflight (creates .env and JWT_SECRET), docker compose up admin-ui, then ai-engine.
- Access Admin UI at http://localhost:3003 (default admin/admin), run the setup wizard.
- Add the generated dialplan to FreePBX (Stasis(asterisk-ai-voice-agent)) and verify health at http://localhost:15000/health.

Notes
- Works with both ExternalMedia RTP and AudioSocket; see the transport compatibility matrix in docs.
- Security: change the default password and restrict port 3003 in production.

Repo: https://github.com/hkjarral/Asterisk-AI-Voice-Agent

**Asterisk AI Voice Agent: open-source, realtime AI voice for Asterisk/FreePBX**
A new MIT-licensed AI voice agent brings modern features like barge-in (interruption handling), turn-taking, and analytics to existing Asterisk and FreePBX stacks. It supports a modular pipeline, allowing administrators to mix and match providers for Speech-to-Text (STT), LLMs, and Text-to-Speech (TTS), or run privacy-focused local pipelines using Docker. Version 4.5.3 introduces call history analytics, GPU-accelerated local models (Faster Whisper), and tooling integrations via the Model Context Protocol.

**Summary of Discussion on Hacker News:**

The discussion focused heavily on the user experience of AI phone systems, debating the trade-offs between efficiency, latency, and "human-like" interactions.

*   **Customer Service vs. Spam:** Opinions were split on whether this technology improves or degrades support. One user highlighted a dealership effectively using AI for appointment scheduling, which was preferable to sitting on hold. Others argued that these tools often ultimately serve to block access to human agents, citing frustrating loops with current support bots (like Amazon’s) and the potential for the technology to arm scammers with better automated tools.
*   **Latency Challenges:** A significant portion of the thread examined the "awkward silence" problem. While some users noted 2–3 second delays are still common, others argued that state-of-the-art systems (like OpenAI’s realtime API or Deepgram) are pushing latency below 500ms. User **numpad0** detailed technical strategies to mitigate this, such as pre-generating filler audio ("uh-huh"), streaming buffers, and using faster, specialized TTS models.
*   **The "Uncanny Valley" and Deception:** Several commenters emphasized that AI agents should not pretend to be human. Users expressed that while natural language processing is useful, the system should clearly identify itself as a machine. If an agent feigns humanity but fails at basic empathy or semantic understanding, it feels like a scam.
*   **Input Preferences:** There is still a strong preference among technical users for deterministic inputs. Many argued that "Pressing 1" or using a web form is superior to voice interactions, which can be difficult in noisy environments or frustrating when the AI hallucinates intent.
*   **Integration Complexity:** A few commenters touched on the difficulty of the backend work, noting that correlating Call Detail Records (CDRs) and recordings in legacy systems like Asterisk is surprisingly difficult, making a "bundled" dashboard highly valuable.

### Show HN: Vibium – Browser automation for AI and humans, by Selenium's creator

#### [Submission URL](https://github.com/VibiumDev/vibium) | 366 points | by [hugs](https://news.ycombinator.com/user?id=hugs) | [105 comments](https://news.ycombinator.com/item?id=46377597)

Vibium: a one-binary, zero-setup way to let AI agents drive a real browser

What it is
- An open-source browser automation stack built for AI agents and humans. A single Go binary (“Clicker,” ~10MB) manages Chrome’s lifecycle, proxies WebDriver BiDi over WebSocket, and exposes an MCP server so tools like Claude Code can control the browser with no manual setup. Apache-2.0 licensed.

Why it’s interesting
- Agent-first design: Native MCP integration means you can add full browser control to Claude Code with one command: claude mcp add vibium -- npx -y vibium.
- Zero drama setup: npm install vibium fetches the Clicker binary and automatically downloads Chrome for Testing to a user cache. No driver juggling.
- Modern protocol: Uses WebDriver BiDi rather than legacy CDP plumbing, with a built-in proxy on :9515.

What you get
- Clicker binary: Chrome detection/launch, BiDi proxy, MCP server over stdio, auto-wait for elements, PNG screenshots.
- JS/TS client: Simple sync and async APIs (go, find, click, type, screenshot, quit). Works via require, dynamic import, or ESM/TS.
- MCP tools out of the box: browser_launch, browser_navigate, browser_find, browser_click, browser_type, browser_screenshot, browser_quit.
- Platform support: Linux x64, macOS (Intel and Apple Silicon), Windows x64.
- Caching and control: Downloads live under a per-OS cache; set VIBIUM_SKIP_BROWSER_DOWNLOAD=1 if you manage browsers yourself.

How it compares
- Compared to Playwright/Puppeteer: similar end goal (drive a browser), but Vibium targets LLM agents and MCP workflows from the start, bundles the runtime into one binary, and speaks BiDi by default. Today it’s JS-first; Python/Java clients are on the roadmap.

Roadmap and status
- V1 focuses on core control via MCP and the JS client. Planned: Python/Java clients, a memory/navigation layer (“Cortex”), a recording extension (“Retina”), video recording, and AI-powered element locators.
- Recent updates: MCP server landed (Day 10), polish/error handling (Day 11), published to npm (Day 12).
- Repo traction: ~1.2k stars, 52 forks.

The takeaway
If you’ve struggled to glue agents to a real browser, Vibium’s “single binary + npm install” approach and native MCP tooling make it unusually frictionless to spin up reliable, BiDi-based automation for both agents and traditional testing.

**Summary of the Discussion**

The discussion on Hacker News was headlined by the project creator, Jason Huggins (`hgs`—creator of Selenium and Appium), engaging with a community heavily invested in Playwright.

**The Playwright Comparison**
The dominant theme was the comparison to Playwright. Many users expressed reluctance to switch, citing Playwright’s reliability, speed, and ability to eliminate the "flakiness" associated with older tools like Selenium.
*   **The Creator’s Take:** `hgs` acknowledged Playwright as the current "defacto standard" for developers. He positioned Vibium not as a Playwright killer, but as a bridge for the massive legacy Selenium userbase to enter the AI agent era.
*   **Agent-Native vs. Dev-Native:** While Playwright is "batteries included" for testing pipelines, Vibium aims to be "batteries included" for *agents* (bundling the browser, runtime, and MCP server in one binary).

**The "Sense-Think-Act" Vision**
When pressed by users on why Vibium is necessary when one could just wrap Playwright in MCP, `hgs` outlined a broader three-part vision:
*   **Act (V1):** The current release ("Clicker"), which handles execution.
*   **Sense (V2 - "Retina"):** A layer to record durable interaction signals and observe the world.
*   **Think (V2 - "Cortex"):** A navigation memory layer that builds a model of the workflow, so the LLM acts on a plan rather than reasoning about raw HTML from scratch.
He argued that while Playwright solves the "Act" portion perfectly, Vibium aims to build the missing "Sense" and "Think" layers required for robust robotic process automation.

**Technical Limitations & Features**
*   **Network Interception:** Users noted that Playwright excels at modifying network requests and mocking backends (crucial for testing). `hgs` confirmed Vibium currently lacks deep network interception/DOM injection capabilities but plans to extend in that direction.
*   **Simplicity:** Several users appreciated the ease of installation (`npm install` vs. complex driver setups), seeing value for quick agentic tasks where setting up a full E2E test suite environment is overkill.
*   **Competition:** Users mentioned other emerging tools in this space, such as Stagehand (Director AI) and DeepWalker (for mobile).

### AI Image Generators Default to the Same 12 Photo Styles, Study Finds

#### [Submission URL](https://gizmodo.com/ai-image-generators-default-to-the-same-12-photo-styles-study-finds-2000702012) | 14 points | by [donatzsky](https://news.ycombinator.com/user?id=donatzsky) | [3 comments](https://news.ycombinator.com/item?id=46380644)

AI image generators collapse into 12 “hotel art” styles, study finds

- What they did: Researchers (Hintze et al., in the journal Patterns) ran a “visual telephone” loop: Stable Diffusion XL generated an image from a short prompt; LLaVA described it; that description became the next prompt for SDXL. They repeated this 100 times, across 1,000 runs. They also tried swapping in other models.

- What happened: The image sequences almost always converged on one of just 12 generic motifs—think maritime lighthouses, formal interiors, urban nightscapes, rustic architecture. The original concept vanished quickly, and by ~turn 100 the style had coalesced. Extending to 1,000 turns produced variations, but still within those same motifs.

- Why it matters: It suggests strong “attractor” states and homogenization in generative pipelines—an echo of mode collapse—driven by model priors and dataset biases toward stock-like imagery. Even changing models didn’t break the trend. The authors dub the result “visual elevator music,” highlighting how easy copying style is compared to producing taste or originality.

- Takeaway for practitioners: Don’t expect open-ended creativity from iterative, model-to-model loops. To avoid sameness, you may need explicit style constraints, diversity objectives, strong negative prompts, or human-in-the-loop curation—otherwise the system drifts toward the same few safe, generic looks.

**Discussion Summary:**

Commenters split their focus between the study's methodology and the cultural implications of "visual elevator music."

*   **Critique of the "Loop":** Users argued the headline is somewhat misleading. They noted that the "mode collapse" results from the specific experimental design—feeding the output back into the input hundreds or thousands of times—rather than a flaw in a single generative prompt. One commenter wryly observed that this outcome is just a demonstration of standard "attractor dynamics."
*   **The "Sugar" Analogy:** Expanding on the paper's "elevator music" metaphor, discussion ventured into the philosophical. One user compared this hyper-optimized, generic imagery to refined sugar or a "crystalline substance"—concentrated and "shiny" enough to stimulate the senses, but ultimately devoid of nutritional substance or survival value in reality.

