## AI Submissions for Tue Oct 21 2025 {{ 'date': '2025-10-21T17:17:30.066Z' }}

### LLMs can get "brain rot"

#### [Submission URL](https://llm-brain-rot.github.io/) | 452 points | by [tamnd](https://news.ycombinator.com/user?id=tamnd) | [277 comments](https://news.ycombinator.com/item?id=45656223)

LLMs can get “brain rot,” says new study: junky social data degrades reasoning, memory, and safety

What’s new
- Researchers from Texas A&M, UT Austin, and Purdue claim a causal link between continual exposure to low-quality social content and lasting cognitive decline in LLMs.
- They simulate different “information diets” by continually pretraining 4 LLMs on Twitter/X posts curated as either junk or control, then apply the same instruction tuning to all models.

How they defined “junk”
- M1: Engagement-driven content — highly liked/retweeted/replied posts, especially short ones (doomscroll bait).
- M2: Semantic quality — sensational, clickbaity language vs. fact-based, informative posts.
- Token counts and training ops were matched across junk vs. control to isolate data quality effects.

What they measured
- Reasoning: ARC-Challenge with chain-of-thought
- Long-context retrieval/multitask: RULER
- Safety: HH-RLHF and AdvBench (resisting harmful instructions)
- “Personality-like” traits: TRAIT (psychometric-style probes)

Key results
- Non-trivial declines after junk exposure (Hedges’ g > 0.3) in reasoning and long-context understanding; safety worsened; “dark traits” (e.g., psychopathy/narcissism proxies) increased.
- Dose-response: more junk → bigger drop. Under M1, ARC-CoT fell 74.9 → 57.2; RULER-CWE 84.4 → 52.3 as junk rose from 0% to 100%.
- Primary failure mode: “thought-skipping” — models truncate or omit reasoning steps, driving most errors.
- Mitigation only partially works: extra instruction tuning and additional clean pretraining improve scores but don’t fully restore baseline, suggesting persistent representational drift.
- Popularity is a stronger warning sign than length: engagement (a non-semantic metric) better predicts the rot effect than simple text features.

Why it matters
- Frames continual pretraining data curation as a training-time safety issue: feeding models engagement-optimized, shallow content can measurably erode capabilities.
- Suggests teams should monitor “cognitive health” over time, filter high-engagement social data aggressively, and cap junk ratios in refresh cycles.

Caveats and open questions
- “Junk” definitions (engagement and clickbait features) may not generalize beyond Twitter or all domains.
- Benchmark-driven “personality” findings in LLMs are debated; safety/ethics evaluations vary by setup.
- Scale, model diversity, and long-horizon effects need replication; real-world training mixes are more complex.

Resources
- The authors say paper, code, and data are available; they summarize media coverage and provide an outline of methods and findings.

The Hacker News discussion on the "LLM brain rot" study reveals several key themes and debates:

### 1. **Em-dash Obsession**
   - Users note LLMs frequently overuse **em-dashes (—)** in lists or explanations, making their output stylistically recognizable. Some argue this mirrors human academic writing (e.g., Wikipedia, books), while others call it a "telltale sign" of AI-generated text.
   - Debates arise over whether humans also use em-dashes heavily or if LLMs amplify this pattern unnaturally. Some suggest Unicode characters or spacing conventions (e.g., thin spaces) could help distinguish human vs. AI writing.

### 2. **Detecting AI-Generated Text**
   - Participants highlight stylistic markers beyond em-dashes: excessive bullet points, passive voice, gerunds, and formulaic structures. However, many acknowledge these patterns are not foolproof, as humans can mimic them or LLMs might improve.
   - Skepticism emerges about AI plagiarism checkers, with users joking that ChatGPT output often gets flagged retroactively, reflecting broader concerns about AI's impact on education and content authenticity.

### 3. **Typography and Legacy Constraints**
   - A sub-thread explores how historical typographic limitations (e.g., typewriters, ASCII) influenced modern writing conventions. Users discuss compromises like hyphens replacing em-dashes or single quotes for apostrophes, noting these quirks persist in LLM training data.

### 4. **Style Guides and Cultural Nuances**
   - Differences in style guides (AP vs. Chicago Manual of Style) and regional preferences (EU vs. US spacing for dashes) are debated. Some users advocate for stricter adherence to typographic standards to improve LLM output quality.

### 5. **Humor and Meta-Commentary**
   - The thread includes playful jabs at the pedantry of discussing em-dashes, with references to "Pepe" memes and self-aware jokes about HN’s tendency to hyperfocus on niche technical details. One user links to a fictional "m-dash leaderboard" parodying the community’s fixation.

### 6. **Broader Implications**
   - While the study warns of LLMs degrading via "junk" data, comments reflect concern that **engagement-driven content** (e.g., Twitter/X) inherently prioritizes sensationalism over depth. Users suggest rigorous data filtering and monitoring "cognitive health" in models, though some dismiss the findings as overly simplistic.

### Key Takeaways:
- **Stylistic Patterns**: Em-dashes and structured lists are seen as LLM fingerprints, but their reliability as detection tools is contested.
- **Quality vs. Convenience**: Balancing LLM training on vast, noisy datasets with the need for high-quality sources remains a challenge.
- **Cultural Baggage**: Typographic and stylistic conventions inherited from pre-digital eras continue to shape both human and AI writing.

### Neural audio codecs: how to get audio into LLMs

#### [Submission URL](https://kyutai.org/next/codec-explainer) | 415 points | by [karimf](https://news.ycombinator.com/user?id=karimf) | [115 comments](https://news.ycombinator.com/item?id=45655161)

Neural audio codecs: how to get audio into LLMs (Václav Volhejn, Kyutai)

The pitch: if you “sandwich” a language model between a neural audio encoder and decoder, you can model speech as discrete audio tokens and generate audio continuations—moving beyond today’s speech wrappers that just do ASR → text LLM → TTS.

Why this matters
- Today’s “voice modes” rarely understand prosody, emotion, or sarcasm. Ask them if you’re speaking in a high voice while you squeak—they’ll fail.
- Raw audio is brutally hard for LLMs: 16 kHz means tens of thousands of samples per second to model, versus a handful of text tokens.

What the post shows
- A naive baseline fails: quantize waveform samples (μ-law, 256 buckets) and train a small GPT-2–scale transformer (151M params) on 1,000 hours of Libri-Light.
  - Results: speech-like babble, crackling loops, occasional “nightmare screeches.”
  - Why: context window of 2048 tokens is only ~128 ms at 16 kHz—shorter than a word—and generation is glacial (10 seconds of audio takes ~30 minutes on an H100).
- The fix: neural audio codecs compress audio into a much smaller sequence of discrete tokens, making it tractable for LLMs to predict long-range coherent continuations, then decode back to audio.
- This codec approach is now the de facto standard for speech LLMs (inspired by work like AudioLM), and Kyutai’s own Mimi codec powers Moshi and has been adopted by others (e.g., Sesame’s CSM).
- The article walks from first principles (why text is easy, audio is not) through WaveNet-era sample-by-sample pitfalls to modern codec tokenization, culminating in Kyutai’s Mimi.

Key takeaways
- Sequence length, not just model size, is the core obstacle for speech-native LLMs.
- Discrete codec tokens dramatically reduce rate and enable meaningful prosody, emphasis, and emotion modeling.
- Open-source experiments make the failure modes and improvements tangible.

Bottom line
If we want LLMs that actually listen—and respond with timing, tone, and empathy—we need to feed them audio the way they can handle it: as compact, discrete codec tokens. This piece is a clear, hands-on roadmap from “why voice LLMs still suck” to how codecs like Mimi make real speech understanding and generation feasible.

The discussion revolves around the challenges and implications of integrating neural audio codecs into LLMs for speech understanding and generation, with key points summarized below:

### **Key Themes & Insights**
1. **Accent & Prosody Handling**:
   - Users highlight frustrations with current models (e.g., ChatGPT Voice) failing to recognize sarcasm, pitch, or accents (e.g., Indian, Bostonian). Systems often misinterpret dialects or filter accents, raising concerns about bias and usability.
   - Debate arises over whether accents correlate with race, with some arguing models risk perpetuating stereotypes if they assume such links (e.g., mistaking Idris Elba’s British accent for racial identity).

2. **Tokenization Limitations**:
   - Traditional tokenization struggles with non-verbal sounds (e.g., sighs, laughter) and prosody. Projects like *PlayDiffusion* and *15.ai* (trained on *My Little Pony* transcripts with non-verbal annotations) demonstrate workarounds but highlight gaps in mainstream systems like Whisper.
   - Critics argue tokenization inherently limits pitch/tonal modeling, though some note GPT-4o’s improved expressiveness (whispers, emotional tones) despite accent inconsistencies.

3. **Technical Approaches**:
   - **Hierarchical Models**: OpenAI’s Jukebox and Kyutai’s MiMo use multi-level token prediction for long-range coherence. Linear-space models (RWKV, S4) are proposed for efficient long-context audio processing.
   - **Compression Trade-offs**: Traditional codecs (MP3, JPEG) compress via DCT but lack neural codecs’ efficiency (e.g., SoundStream at 3 kbps vs. MP3’s 128 kbps). Neural codecs better preserve prosody but require novel architectures.

4. **Ethical & Practical Concerns**:
   - Accent detection risks reinforcing biases if models conflate dialect with race. Users advocate for systems that handle dialect switches fluidly without stereotyping.
   - Open-source experiments (e.g., 15.ai, Cartesia’s constant-time models) are praised for transparency, while proprietary systems (Gemini, GPT-4o) face scrutiny over opaque training data and alignment.

### **Notable Examples & Workarounds**
- **15.ai**: Leveraged *My Little Pony* transcripts with explicit non-verbal annotations to train expressive TTS, showcasing the value of richly annotated datasets.
- **PlayDiffusion**: Embeds non-verbal sounds (e.g., “[sniff]”) in transcripts to improve alignment, addressing Whisper’s omission of such cues.
- **GPT-4o Tests**: Users note improved tonal expressiveness but inconsistent accent handling (e.g., failing “Tomato vs. Potato” in British/American English).

### **Conclusion**
While neural audio codecs (e.g., Mimi) mark progress in compressing audio for LLMs, challenges persist in modeling accents, non-verbal cues, and prosody. Technical innovations (hierarchical tokenization, linear-space models) and ethical considerations (bias mitigation) remain critical for achieving LLMs that truly “listen” and respond empathetically. Open-source initiatives and interdisciplinary approaches (linguistics + ML) are key to advancing this frontier.

### Wikipedia says traffic is falling due to AI search summaries and social video

#### [Submission URL](https://techcrunch.com/2025/10/18/wikipedia-says-traffic-is-falling-due-to-ai-search-summaries-and-social-video/) | 419 points | by [gmays](https://news.ycombinator.com/user?id=gmays) | [409 comments](https://news.ycombinator.com/item?id=45651485)

Wikipedia says human traffic is down 8% YoY, points to AI summaries and social video

- What happened: Wikimedia says human page views fell 8% year-over-year. An updated bot-detection system also revealed that a May–June spike was largely bots evading detection, masking the real decline.
- Why it’s falling: Marshall Miller (Wikimedia Foundation) cites generative AI answers in search that reduce clicks to source sites, plus younger users shifting to social video for information. Google disputes that AI summaries cut search traffic.
- Why it matters: Fewer visits risk fewer volunteer editors and fewer small-donor contributions—the core engines that keep Wikipedia accurate and up to date. There’s also a growing disconnect between what people read and awareness of the original sources.
- Wikipedia’s stance: The foundation welcomes new ways people access knowledge and notes its content still reaches users indirectly. It paused its own AI summaries after editor pushback.
- What they want from AI/search/social: Clear attribution and design that drives users back to Wikipedia. The org is building a new attribution framework, running teams focused on reaching new readers, and asking for volunteers.
- Call to users: Click through citations, value human-curated sources, and remember the people behind the content that powers generative AI.

**Summary of Discussion on Wikipedia's Traffic Decline and Sustainability:**  

The Hacker News discussion revolves around the Wikimedia Foundation’s report of declining traffic and debates Wikipedia’s long-term sustainability, governance, and funding. Key points include:  

### **1. Skepticism Toward Long-Term Planning**  
- Critics question Wikimedia’s hypothetical “1,000-year” sustainability plan, arguing that no organization (or even governments) can reliably exist for millennia due to economic, political, and societal shifts. Comparisons are drawn to institutions like the Roman Catholic Church or historic companies ([List of oldest companies](https://en.wikipedia.org/wiki/List_of_oldest_companies)), which survived only through adaptability.  
- Users note rapid societal changes (e.g., South Africa post-apartheid, Sweden’s cultural shifts) to argue that predicting even 100 years ahead is unrealistic.  

### **2. Financial Concerns**  
- Wikimedia’s endowment model is scrutinized. Some users criticize rising spending and dependence on donations, calling for transparency in fund allocation. Others defend Wikipedia’s nonprofit ethos, contrasting it with for-profit platforms like AI companies.  
- Donation effectiveness is debated: Smaller recurring donations are seen as more sustainable than relying on large corporate sponsors. Critics cite examples like Mozilla’s struggles to question long-term financial viability.  

### **3. Content Quality and Governance**  
- Wikipedia’s reliance on volunteer editors is praised, but concerns arise about increasing vandalism and the need for decentralized governance. Some argue centralized control (e.g., the Wikimedia Foundation) risks alienating the community.  
- Users highlight Wikipedia’s unique position as a “common good” but worry AI-driven traffic declines could reduce editor motivation and donor contributions, threatening accuracy.  

### **4. Historical and Cultural Parallels**  
- Discussions diverge into historical analogies, such as Viking-era Sweden vs. modern multiculturalism, to illustrate societal unpredictability. Others compare Wikipedia to traditional encyclopedias, emphasizing its resilience despite technological disruption.  

### **5. Counterarguments and Optimism**  
- Supporters argue Wikipedia’s model—community-driven, ad-free, and nonprofit—is inherently more durable than profit-driven platforms. They urge users to value human-curated knowledge and continue donating to preserve its mission.  

**Takeaway**: The thread reflects tensions between idealism (Wikipedia as a timeless public good) and pragmatism (financial realities, governance challenges). While skepticism about 1,000-year plans dominates, many agree that Wikipedia’s survival hinges on adapting to AI/social video trends while maintaining community trust and transparent funding.

### Language Support for Marginalia Search

#### [Submission URL](https://www.marginalia.nu/log/a_126_multilingual/) | 174 points | by [Bogdanp](https://news.ycombinator.com/user?id=Bogdanp) | [25 comments](https://news.ycombinator.com/item?id=45653143)

Marginalia Search pilots multilingual support (German, French, Swedish)

What’s new
- Experimental non-English search is live with a very small test corpus. The goal is to gauge effort, index growth, and pitfalls before adding more languages.

Why this is hard
- Many search pipelines bake in Anglo-centric assumptions. Normalization and tokenization vary by language: e.g., Swedish “ö” vs “o” should not match, while Germans may demand “ü” ≠ “u”; Japanese needs segmentation and width normalization; Latin’s heavy inflection and freer word order push toward lemmatization and de-emphasizing word order.
- Cautionary tale: Google’s mishandling of Russian helped Yandex gain share.

Under the hood
- Pipeline: HTML extraction → language ID (fastText) → sentence split → lowercasing/Unicode normalization → stemming + POS tagging → keyword extraction → mapping to positions/tags → “important keywords” via TF‑IDF, grammar patterns, and heuristics → hashing.
- TF‑IDF per-language models are missing at first (index is mostly English), so “important keyword” detection is weakened until enough data accrues; BM25 ranking remains robust.
- Language behavior is now parameterized via an XML “language definition” (chosen for strong validation) that configures grammar patterns for keywording.
- Replaced hard-coded POS patterns with a fast bitmask matcher to cut branching and enable simple parallelism.

Why it matters
- Thoughtful, language-aware IR design now in the wild; expect policy decisions on diacritics, more languages, rebuilt TF‑IDF models, and special handling for languages like Japanese.

**Summary of Discussion:**

1. **Niche Focus & Use Cases**:  
   Marginalia Search is praised for excelling in finding obscure, non-commercial content often missed by mainstream engines (e.g., mechanical keyboards, niche historical figures like Jack Parsons). Users highlight its value for researchers/writers seeking specific or academic material, such as historical manuscripts or discussions about Ezra Klein on lesser-known forums instead of NYT articles.

2. **Technical Choices**:  
   The engine avoids resource-heavy AI models like embeddings, opting for efficiency with stemming, POS tagging, and fast bitmask matchers. It uses RDRPosTagger for language processing and prioritizes simplicity over complex AI, though some note trade-offs in multilingual keyword detection until TF-IDF models mature.

3. **Independence & Ethics**:  
   Marginalia is noted as an EU-hosted, independent project free from tracking/sponsorship. It deliberately excludes commercial sites from top results, emphasizing transparency and user privacy.

4. **UI/UX Feedback**:  
   A Firefox user points out a scrollbar issue hiding the language menu. Marginalia acknowledges the need for UI improvements and hints at future features like domain-specific filters or public APIs.

5. **Cultural & Philosophical Notes**:  
   The name "Marginalia" is linked to a metaphor for prioritizing overlooked content, akin to *The Name of the Rose*’s focus on marginalia. Users appreciate its mission to democratize access to diverse, non-mainstream information.

6. **Challenges & Future**:  
   Discussions mention the difficulty of balancing performance with multilingual support and the potential for indexing historical/scholarly works. The team emphasizes incremental improvements and user feedback to refine functionality.  

**Key Takeaways**: Marginalia is celebrated for its unique niche focus, ethical stance, and technical pragmatism, though users note areas like UI polish and multilingual accuracy needing attention. It positions itself as a critical alternative to mainstream engines for deep, specialized research.

### Binary Retrieval-Augmented Reward Mitigates Hallucinations

#### [Submission URL](https://arxiv.org/abs/2510.17733) | 41 points | by [MarlonPro](https://news.ycombinator.com/user?id=MarlonPro) | [3 comments](https://news.ycombinator.com/item?id=45657595)

Train for Truth, Keep the Skills (arXiv) proposes a simple but strict way to curb LLM hallucinations without breaking other abilities: an online RL scheme with a binary retrieval-augmented reward (RAR). The model gets a reward of 1 only if its entire answer is supported by retrieved evidence; otherwise 0. No partial credit.

Key points
- What’s new: An all-or-nothing, retrieval-backed reward for RL that pressures models to be fully factual, rather than “more factual.”
- How it works: During training, the system retrieves evidence and judges whether the output is entirely supported. Only fully supported outputs earn reward, pushing the model to avoid unsupported claims and to abstain when it lacks knowledge.
- Results (on Qwen3 reasoning models):
  - Open-ended generation: 39.3% reduction in hallucinations vs supervised training and continuous-reward RL baselines.
  - Short-form QA: fewer wrong answers by 44.4% (PopQA) and 21.7% (GPQA), with the model learning calibrated abstention (“I don’t know”) when it lacks parametric knowledge.
  - No regressions on instruction following, math, or code—whereas continuous-reward RL improves factuality but degrades these skills.
- Why it matters: Many anti-hallucination methods trade off factuality for general capability. A strict, binary, evidence-checked reward appears to boost truthfulness while preserving core skills and encouraging responsible abstention.

Caveats to watch
- Binary rewards can be sparse/brittle and depend heavily on retrieval and the judge’s reliability.
- Results are shown on specific models and tasks; generalization remains to be validated.

Paper: https://arxiv.org/abs/2510.17733

**Summary of Discussion:**

1. **Practicality Concerns:**  
   Commenters question if existing hallucination mitigation methods degrade performance on downstream tasks not requiring generation (e.g., classification), limiting real-world utility. The paper’s binary reward avoids partial credit but raises concerns about brittleness.

2. **Verification Reliability:**  
   The approach relies on a separate verifier model (Qwen 32B) to check if answers align with retrieved documents. Critics highlight risks: if the verifier or retrieval system is flawed (e.g., misses relevant documents or misjudges edge cases), errors could propagate during training.

3. **Scalability & Generalization:**  
   Questions arise about handling ambiguous or unclear cases at scale. Training Qwen 8B using a larger verifier (Qwen 32B) may introduce dependencies on the parent model’s capabilities, raising concerns about broader applicability.

4. **Abstention vs. Correctness:**  
   While the method encourages abstaining when uncertain, one user notes that avoiding hallucinations doesn’t guarantee correctness. Answers may be factual but irrelevant or incomplete, highlighting a potential gap between factual accuracy and task success.

**Key Takeaways:**  
The discussion underscores skepticism about the robustness of the binary reward system, emphasizing the critical role of retrieval/verification accuracy and scalability. While abstention is praised, the trade-off between factuality and practical task performance remains debated.

### Fallout from the AWS outage: Smart mattresses go rogue

#### [Submission URL](https://quasa.io/media/the-strangest-fallout-from-the-aws-outage-smart-mattresses-go-rogue-and-ruin-sleep-worldwide) | 244 points | by [jerlam](https://news.ycombinator.com/user?id=jerlam) | [254 comments](https://news.ycombinator.com/item?id=45658056)

AWS outage leaves “smart” mattresses dumb, hot, and offline

- A major AWS US-EAST-1 outage starting around 3 a.m. ET on Oct 20 rippled across the internet (Downdetector logged 8M+ reports). Beyond apps like Snapchat and Roblox, Eight Sleep Pod3 owners discovered their beds have no offline mode.
- With Eight Sleep’s cloud unreachable, the app froze, temperature control stopped, sleep tracking died, and some Hubs’ touch controls were unresponsive. A few users said their Pods effectively “bricked” until cloud services returned.
- One viral case: a user who preheated his bed to +9°F was stuck in a “sauna” all night because cooling overrides require the cloud. Eight Sleep reportedly acknowledged there’s no offline mode yet and said they’re working on it.
- AWS reported significant recovery by evening; users said controls were largely back by early morning.
- Context: Eight Sleep markets AI-driven, water-cooled mattresses (~$2,000+) that adjust temps and log biometrics. The company previously faced scrutiny in 2024 after a report about exposed AWS keys and potential remote access.

Why it matters
- Classic IoT failure mode: a single-region cloud dependency turned a basic safety/comfort function into a point of failure.
- Devices that affect bodily comfort (and potentially health) need local fail-safes: LAN control, safe default states, and graceful degradation.
- The episode underscores the broader smart-home fragility when AWS hiccups—now extending from doorbells to, absurdly, beds.

**Summary of Hacker News Discussion: AWS Outage & Eight Sleep Mattresses**

The Hacker News discussion highlights frustration with IoT devices' reliance on cloud services, exemplified by Eight Sleep's "smart" mattresses failing during the AWS outage. Key themes include:

1. **Critique of Cloud Dependency**:  
   Users lambasted Eight Sleep for lacking an offline mode, arguing that critical devices (e.g., temperature-controlled mattresses) should prioritize local functionality. Comparisons were drawn to vacuum cleaners like Roborock, where projects like **Valetudo** enable local control by bypassing cloud dependencies. Suggestions included adopting **Zigbee** or **Home Assistant** for LAN-based control to avoid single points of failure.

2. **Broader IoT Concerns**:  
   Commenters noted systemic issues in IoT design:  
   - **Overengineered solutions**: Many smart devices (e.g., Wi-Fi-enabled fridges) are seen as unnecessary, with "dumb" alternatives deemed more reliable.  
   - **Subscription models**: Criticism targeted Eight Sleep’s $2,000+ mattress requiring subscriptions, likening it to other "overpriced wellness trends" (e.g., LMNT, Athletic Greens).  
   - **Fragility**: Reliance on cloud infrastructure exposes users to risks beyond their control, such as AWS outages disrupting basic functions like bed temperature.

3. **Local Control Advocacy**:  
   Users shared success stories with **Lutron Caseta switches** (works offline), **Zigbee sensors**, and DIY solutions (e.g., **ESP32**-powered fridge thermostats). Some argued for certifications mandating offline capabilities for devices affecting health or safety. Others criticized manufacturers for neglecting local control to lock users into ecosystems or subscriptions.

4. **Community-Driven Fixes**:  
   Technical users described modifying devices to reduce cloud reliance:  
   - Valetudo for vacuums.  
   - Self-hosted home automation (Home Assistant).  
   - Custom firmware (e.g., ESP32 projects) for local sensor data and control.  

5. **Cultural Pushback**:  
   Many expressed skepticism toward "smart" home trends, advocating for simpler, user-repairable devices. Anecdotes highlighted frustrations with disposable tech, unreliable automations, and "stupid defaults" (e.g., smart light bulbs becoming unusable if disconnected).

**Conclusion**:  
The outage underscored the tension between IoT convenience and reliability. While some users see value in smart features (e.g., circadian lighting), the consensus leans toward prioritizing **offline-first design**, open protocols, and user autonomy over cloud-dependent "smart" gadgets.

### Palma 2 Pro

#### [Submission URL](https://shop.boox.com/products/palma2pro) | 25 points | by [ndrake](https://news.ycombinator.com/user?id=ndrake) | [5 comments](https://news.ycombinator.com/item?id=45654955)

BOOX Palma 2 Pro: a pocketable color e‑ink device with mobile data

What it is: A 6.13-inch E Ink Kaleido 3 “phone-sized” reader/notepad running Android 15 with Google Play, aimed at distraction‑light use on the go.

Highlights
- Display: 6.13" Kaleido 3 color ePaper (300 PPI B&W, 150 PPI color), adjustable front light, sunlight‑readable
- Performance: Octa‑core SoC, 8GB RAM, 128GB storage, BOOX Super Refresh for smoother scrolling
- Connectivity: Hybrid SIM slot (dual SIM or SIM + microSD up to 2TB), data‑only SIM supported, A‑GPS, Bluetooth audio
- Input: Supports InkSense Plus stylus (sold separately) for quick notes; customizable side smart button; home screen widgets
- Other: Fingerprint lock, auto‑rotation, water‑repellent design, 3950 mAh battery, ~175 g, 8.8 mm thick
- Software: Android 15 with Play Store; presets for refresh/color/lighting; free 10GB cloud; 3+ years of firmware updates
- Price: $379.99 (vs Palma 2 monochrome at $249.99)

Why it matters: It’s a rare, truly pocketable color e‑ink device with cellular data—useful for reading, maps in bright sun, and lightweight note‑taking without a full smartphone’s distractions.

Caveats for the e‑ink crowd: Kaleido color is 150 PPI with muted saturation; refresh/latency still apply; stylus is extra; voice/camera/NFC are not mentioned; water resistance rating unspecified; check LTE band compatibility.

**Summary of Discussion:**  

1. **Device Comparisons & Practicality:**  
   - Users debate the BOOX Palma’s value compared to mainstream e-readers like Kindle and Kobo, noting its higher price and pocketable size. However, some criticize its bulkiness compared to traditional e-readers, questioning if it justifies carrying an extra device.  

2. **User Experience with Previous Models:**  
   - A user of the Palma 1 highlights mixed experiences: praised for portability and EPUB reading but criticized for thickness, poor screen legibility in dark mode, and software quirks (e.g., volume keys not working with Firefox, lack of ad-blocking in the default browser).  

3. **Open-Source Concerns:**  
   - Criticism of BOOX’s slow release of Linux kernel source code (required under GPL), making it irrelevant by the time it’s shared. Some argue this reflects poorly on the company’s commitment to open-source norms.  

4. **Device Identity & Privacy Issues:**  
   - Confusion over whether the Palma 2 Pro is a phone replacement, given its phone-like design and cellular data support. A nested reply raises privacy concerns, alleging BOOX devices use random IMEI numbers (linking to a forum post for context).  

5. **Miscellaneous Critiques:**  
   - Comments note frustrations with Android interface navigation and question the utility of color e-ink for mainstream users due to hardware/software limitations (e.g., refresh rate, muted colors).  

**Key Themes:**  
- **Trade-offs:** Portability vs. functionality, price vs. user experience.  
- **Software/Privacy:** Mixed Android app compatibility, open-source obligations, and IMEI concerns.  
- **Niche Appeal:** Polarizing device for users seeking distraction-free tools vs. those prioritizing mainstream convenience.

### Is Sora the beginning of the end for OpenAI?

#### [Submission URL](https://calnewport.com/is-sora-the-beginning-of-the-end-for-openai/) | 178 points | by [warrenm](https://news.ycombinator.com/user?id=warrenm) | [197 comments](https://news.ycombinator.com/item?id=45657428)

Title: Is Sora the Beginning of the End for OpenAI? (Cal Newport)

Cal Newport argues that OpenAI’s launch of a TikTok-style “Sora” social app alongside its new Sora video model signals a strategic comedown from its epoch-defining AGI narrative to ad-driven consumer slop. He contrasts past doomsday/transformational claims (GPT-5 as “first atomic bomb,” 50% of white-collar work automated, “Moore’s Law for Everything”) with moves like age-gated erotica and a feed of AI-generated clips.

Key points:
- The Sora app lets users prompt short, highly realistic videos and consumes them via an algorithmic feed—removing the last vestiges of human agency, in Newport’s view.
- Economics look shaky: content generation is GPU-expensive; users need ChatGPT Plus to post. At $20/month you get ~50 low-res videos; $200/month buys more and higher-res—unfavorable versus TikTok’s far cheaper ops and creator payouts.
- Newport reads the app as a pivot: after tens of billions invested, OpenAI is chasing engagement and ad dollars rather than delivering economy-wide automation.
- He argues this reflects the practical limits of current LLM/video models to transform the world “all at once,” despite being undeniably impressive.
- Commenters echo exhaustion with AI-generated feeds and say their screen time is dropping as platforms fill with synthetic content.

Takeaway: If OpenAI is resorting to a TikTok clone and adult content to monetize frontier models, Newport suggests the company may be acknowledging—at least implicitly—that its tech isn’t (yet) the economy-rewiring force it once hyped.

**Summary of Discussion:**

The discussion centers on skepticism towards OpenAI's strategic pivot with Sora, contrasting its earlier AGI ambitions with consumer-driven initiatives. Key points include:

1. **AGI Expectations vs. Reality**:  
   - Users highlight the dissonance between OpenAI’s past rhetoric (e.g., GPT-5 as transformative as the atomic bomb, automating 50% of white-collar jobs) and its current focus on apps like Sora. Some argue that Nvidia’s soaring valuation and GPU demand are tied to inflated AGI hype, not tangible progress.  
   - Critics note ChatGPT’s slowing consumer growth, suggesting enterprise AI integrations (e.g., Google, Slack, Zoom) may not justify costs. Skepticism persists about AGI timelines, with comments like “AGI is getting us Bob Ross paintings instead of world-changing tech.”

2. **Monetization and Priorities**:  
   - OpenAI’s pivot to ad-driven apps like Sora is seen as a concession to investor pressure ($100B+ investments) and impatience for returns. Comparisons are drawn to platforms like TikTok, with doubts about Sora’s economic viability given high GPU costs versus TikTok’s low overhead.  
   - Some defend Sora as a research experiment rather than a pivot, arguing foundational AI progress continues regardless of consumer-facing apps.

3. **Ethical and Social Concerns**:  
   - **Porn as a Driver**: Debates emerge about OpenAI’s rumored exploration of NSFW content. While porn historically drove tech adoption (broadband, payment systems), critics worry about normalization, addiction, and ethical risks (e.g., blackmail via AI-generated content).  
   - **Political Implications**: U.S. political divides surface, with speculation that red states may oppose AI-generated porn despite high consumption rates. Security concerns are raised about stored user prompts being weaponized.  

4. **Technical and Cultural Pushback**:  
   - Users point to existing platforms (e.g., CivitAI) already hosting NSFW AI models, questioning OpenAI’s foray into saturated markets. Others mock AI’s frivolous outputs (e.g., “Stephen Hawking snowboarding memes”) as a distraction from serious applications.  

**Takeaway**: The thread reflects broader disillusionment with AI’s gap between hype and delivery. While some view Sora as a pragmatic step, most criticize OpenAI for prioritizing engagement over transformative impact, with ethical and strategic risks looming large.

### Wikipedia Seems Pretty Worried About AI

#### [Submission URL](https://nymag.com/intelligencer/article/wikipedia-contributors-are-worried-about-ai-scraping.html) | 29 points | by [stared](https://news.ycombinator.com/user?id=stared) | [17 comments](https://news.ycombinator.com/item?id=45653818)

Wikipedia says its traffic spike was bots, not a comeback — and human visits are down 8%

Summary:
Wikimedia’s Marshall Miller says a surge of “human-looking” traffic starting in May 2025 turned out to be bots, many likely tied to AI firms scraping content while evading detection. After updating its bot filters and reclassifying March–August data, Wikipedia found human pageviews actually fell roughly 8% versus the same months in 2024 — sharpening worries that AI overviews and chatbots are siphoning readers, contributors, and donors.

Key points:
- Bot surge: Sophisticated scrapers impersonated humans, inflating pageview stats in May–June.
- Recount: Updated bot logic shows significant human-pageview declines year over year.
- AI firms implicated: Many bots appear to support LLM training/summarization while avoiding attribution-driven click-throughs.
- Google example: AI Overviews summarize Wikipedia, then offer “AI Mode” — pushing the source article further down and reducing clicks.
- Wikimedia’s ask: If AI products use Wikipedia, they should also drive users back to it.

Why it matters:
- Fewer readers → fewer editors and donations, risking a negative spiral for one of the web’s core public resources.
- Highlights a broader pattern: AI products ingest the open web, then answer users directly, disintermediating the sources they rely on.
- Wikipedia’s moral authority is notable here because it isn’t a for‑profit publisher.

What to watch:
- Whether AI platforms adjust designs to boost outbound clicks or strike traffic/compensation deals.
- Further changes to Wikimedia’s bot policies and detection — and whether more reclassification revises traffic baselines again.
- Community and donor response if declines persist.

**Summary of Hacker News Discussion:**

The discussion revolves around concerns about AI bots scraping Wikipedia, declining human traffic, and broader implications for the platform's sustainability. Key themes include:

1. **AI Scraping vs. Ethical Responsibility**  
   - Users criticize AI companies for scraping Wikipedia content without driving traffic back, undermining donations and editor engagement.  
   - Comparisons are drawn to Mozilla/Firefox funding models, where users might donate to specific tools (e.g., Firefox) but distrust parent organizations (e.g., Mozilla).  

2. **Technical Debates on Scraping**  
   - Some argue AI firms should use Wikipedia’s public databases or APIs instead of evading bot filters, calling it inefficient and unethical.  
   - Others note specialized scraping tools exist but acknowledge the effort required to bypass restrictions (e.g., robots.txt, rate limits).  

3. **Skepticism Toward AI Summaries**  
   - Users highlight risks of AI-generated summaries (e.g., Google’s AI Overviews) misrepresenting information, contrasting them with Wikipedia’s curated reliability.  
   - Concerns that AI “slop” prioritizes consumer-friendly outputs over factual accuracy, eroding trust in information ecosystems.  

4. **Financial Sustainability**  
   - Declining human traffic raises fears of reduced donations and editor participation, risking a “negative spiral” for Wikipedia.  
   - Irony noted: Companies invest millions in proprietary databases but scrape Wikipedia instead of supporting it.  

5. **Calls for Accountability**  
   - Suggestions that AI platforms should link to Wikipedia or contribute financially if they rely on its content.  

**Notable Subthreads:**  
- A user points out the absurdity of scraping when Wikipedia offers free database dumps.  
- Debate over whether AI summaries (e.g., in browsers/OS interfaces) will further disintermediate original sources like Wikipedia.  

**Sentiment:** Mixed frustration toward AI companies’ practices, technical resignation about scraping inevitability, and urgency to preserve Wikipedia’s role as a public resource.

### AI is making us work more

#### [Submission URL](https://tawandamunongo.dev/posts/2025/10/ai-work-more) | 213 points | by [elcapithanos](https://news.ycombinator.com/user?id=elcapithanos) | [246 comments](https://news.ycombinator.com/item?id=45656916)

AI is making us work more, not less, argues this essay—because when the machine never sleeps, we stop resting.

- Core claim: Generative AI was sold as a path to working less, but it’s fostering an always-on psyche: if the tool can keep going, so should you.
- Trigger: A remark on The Pragmatic Engineer podcast (with Armin Ronacher) about AI’s paradox—promising freedom yet extending work.
- “996 in the Valley”: The author says 996-style schedules (9 a.m.–9 p.m., six days a week) are migrating from Chinese tech firms to Silicon Valley, citing a recent Wired investigation and job postings that openly signal long hours to “stay competitive.”
- Mechanism shift: Historically, human fatigue capped work. AI agents don’t tire, creating a psychological loop: every moment not prompting or iterating feels like falling behind; rest feels like inefficiency.
- From lamps to LLMs: Like artificial light extending the workday, AI increases leverage—and turns “can work” into “should work.” Harari’s “luxuries become necessities” frames how new capabilities become obligations.
- The philosophy: Drawing on Byung-Chul Han’s The Burnout Society, the essay says external coercion has become internal self-discipline—“I can, therefore I must” (the “tyranny of can”). No boss needed; we self-impose.
- The toll: The “hyper‑productivity loop” is self-defeating. The author cites analysis that teams in 996 modes burn out and become less innovative than balanced teams; creative output drops as baseline expectations ratchet up with every AI gain.
- Quality drag: As expectations rise, much AI output remains mediocre without heavy human steering, fueling more prompting, more iteration, more pressure.
- Why it matters: If AI expands capacity faster than human recovery and meaning-making, culture defaults to work inflation rather than work reduction—entrenching burnout in the name of competitiveness.

Likely HN debate: Is 996 truly spreading in the West or overstated? Are managers misusing AI to justify grind culture? How to measure net productivity vs. creative decline? And what norms or policies would prevent “can” from becoming “must”?

The Hacker News discussion on the submission "AI is making us work more, not less" revolves around several key themes:

### **1. Automation’s Paradox: More Work, Not Less**
- Participants argue that automation (including AI) often **increases complexity** rather than reducing workloads. Engineers must now maintain intricate systems, debug AI outputs, and handle higher expectations, leading to **"busywork"** (e.g., constant prompting, validation, and iteration). 
- Examples include QA processes becoming more demanding, as AI-generated outputs require rigorous checking, and management using automation to justify unrealistic productivity targets.

### **2. Management Incentives vs. Worker Well-being**
- Many commenters criticize **misaligned incentives**, where companies prioritize shareholder value or speed over quality. Automation is used to **extract more labor** (e.g., "996 culture" creeping into Silicon Valley) rather than empower workers. 
- Anecdotes highlight how automation led to burnout: One user automated their job to complete tasks in half the time, only to be assigned four times the workload, with management rewarding "effort" over results.

### **3. Validation Challenges with AI**
- Unlike reliable automation (e.g., CNC machines), **AI outputs are harder to validate**. LLMs often produce mediocre or error-prone results, forcing humans to spend more time steering, debugging, and iterating. This creates a "quality drag" where rising expectations outpace AI’s reliability.
- Comparisons to hardware manufacturing (e.g., PCB assembly) show that physical processes have robust validation (optical/X-ray checks), while AI systems lack equivalent safeguards, leading to technical debt and instability.

### **4. Cultural Shifts and Burnout**
- The discussion echoes Byung-Chul Han’s "tyranny of can" — workers internalize pressure to match AI’s 24/7 capacity. Automation tools become **"productivity traps"**, with employees feeling compelled to work longer hours to avoid falling behind.
- Some note that businesses exploit automation to **reduce labor costs** (e.g., replacing full-time roles with automated systems) while demanding more from remaining staff.

### **5. Solutions and Skepticism**
- Suggestions include rethinking **business priorities** (quality over speed), improving validation frameworks for AI, and unionizing to resist exploitative practices. 
- Skeptics argue the root issue isn’t technology but **poor management** and capitalist incentives that prioritize growth over human well-being.

### **Key Takeaway**
The consensus is that AI’s impact depends on **how it’s implemented**. Without cultural shifts toward sustainable work practices and better validation mechanisms, AI risks entrenching burnout rather than alleviating it. As one user starkly put it: *"Automation makes life worse, not better, when it’s used to exploit, not empower."*

### Amazon hopes to replace 600k US workers with robots

#### [Submission URL](https://www.theverge.com/news/803257/amazon-robotics-automation-replace-600000-human-jobs) | 72 points | by [pwthornton](https://news.ycombinator.com/user?id=pwthornton) | [131 comments](https://news.ycombinator.com/item?id=45655179)

Amazon aims to automate much of its US operations, per leaked docs reported by The New York Times and summarized by The Verge:

- The plan: Automate 75% of operations and avoid hiring more than 600,000 US workers by 2033, even as product volume doubles.
- Near-term impact: Replace 160,000 roles that would otherwise be needed by 2027, saving about $0.30 per item and $12.6B from 2025–2027.
- Current state: Over 1 million robots already deployed; testing bipedal bots like Agility Robotics’ Digit.
- Messaging strategy: Internal discussions reportedly explored using terms like “advanced technology” and “cobots” instead of “automation” or “AI” to soften backlash.
- Amazon’s response: Says the leak reflects one team’s perspective, not company-wide hiring plans; claims it’s actively hiring, including 250,000 seasonal roles. It also denies instructing leaders to avoid certain terms and says community work isn’t tied to automation.
- Big-picture take: Economist Daron Acemoglu warns that if Amazon succeeds, it could shift from a net job creator to a net job destroyer—and its approach could spread across the industry.

Why it matters: A $0.30-per-item cost reduction at Amazon-scale could reshape logistics economics, labor demand, and competitive pressure for retailers far beyond Amazon.

The Hacker News discussion on Amazon's automation plans reveals several key themes and debates:

### 1. **Job Displacement Concerns**  
   - Participants worry automation will eliminate jobs across sectors like customer service, marketing, and warehousing, not just low-skilled roles. Even software developers fear eventual displacement as AI/robotics advance.  
   - Skepticism arises about whether new roles (e.g., robot maintenance) will offset losses, given retraining challenges and the high skill barriers for emerging jobs.  

### 2. **Economic Inequality**  
   - Critics argue automation primarily benefits shareholders and top earners, worsening wealth gaps. One user notes, "The robot revolution benefits people at the top of the social stratum."  
   - Amazon’s cost-saving focus (e.g., $0.30/item savings) is seen as profit-driven rather than consumer-friendly.  

### 3. **Historical Parallels vs. Novelty**  
   - Comparisons to past disruptions (e.g., Industrial Revolution) are debated. Some believe new industries will emerge, while others doubt this, citing AI’s rapid pace and the scale of current job losses.  
   - A user references pre-WWII industrial shifts, highlighting how technological unemployment isn’t new but may now accelerate.  

### 4. **Technical Limitations**  
   - Current robotics are deemed insufficient for tasks requiring human dexterity (e.g., factory work). One commenter with industry experience notes robots struggle with unstructured environments, limiting near-term replacement.  
   - Niche roles like barbers or nail technicians are seen as safer, but logistics/manufacturing face higher risks.  

### 5. **Ethical and Corporate Responsibility**  
   - Amazon’s messaging strategy (e.g., using “cobots” instead of “automation”) is criticized as PR spin to mask labor reduction.  
   - Concerns about companies prioritizing shareholder returns over worker welfare, with references to "disposable human resources."  

### 6. **Policy Solutions**  
   - Universal Basic Income (UBI) is proposed to address displacement, alongside calls for corporate taxes to fund retraining.  
   - Debates over minimum wage laws inadvertently accelerating automation, as seen in post-pandemic shifts toward self-checkout systems.  

### 7. **Community Self-Awareness**  
   - Some users critique HN’s bubble, noting the forum’s focus on tech roles overlooks blue-collar workers’ realities. Others mock the irony of software engineers fearing automation after dismissing similar concerns in other industries.  

### Key Takeaway  
The discussion reflects tension between optimism about technological progress and anxiety over its societal impacts. While some envision a future with redefined work and safety nets like UBI, others foresee deepening inequality and corporate power unless systemic reforms prioritize human welfare over efficiency and profit.

