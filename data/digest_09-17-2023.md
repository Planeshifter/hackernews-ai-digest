## AI Submissions for Sun Sep 17 2023 {{ 'date': '2023-09-17T17:10:53.971Z' }}

### Large Language Models for Compiler Optimization

#### [Submission URL](https://arxiv.org/abs/2309.07062) | 202 points | by [og_kalu](https://news.ycombinator.com/user?id=og_kalu) | [100 comments](https://news.ycombinator.com/item?id=37549216)

A recent paper on arXiv titled "Large Language Models for Compiler Optimization" explores the use of large language models for code optimization. The authors present a 7B-parameter transformer model trained to optimize LLVM assembly for code size. The model takes unoptimized assembly as input and outputs a list of compiler options to best optimize the program. During training, the model predicts instruction counts before and after optimization, as well as the optimized code itself, which significantly improves its performance. The model outperforms two state-of-the-art baselines that require thousands of compilations, achieving a 3.0% improvement in reducing instruction counts over the compiler. Additionally, the model demonstrates strong code reasoning abilities, generating compilable code 91% of the time and perfectly emulating the output of the compiler 70% of the time. This work showcases the potential of large language models in compiler optimization.

The discussion revolves around various aspects of the paper on large language models (LLMs) for compiler optimization. One comment notes that the paper does not discuss the importance of generated code semantics and suggests trying the approach on larger benchmarks. Another commenter highlights the misconception that LLMs directly model instructions, explaining that they instead generate passes for the compiler optimization. Another thread focuses on the challenges of achieving correctness in LLMs and the difficulty of measuring correctness. There is also a discussion about the potential of LLMs in addressing compilation optimization problems, but some express skepticism and suggest alternative approaches. The importance of both correctness and performance in compiler optimization is emphasized, as well as the need for further research in this area.

### Apple’s new Transformer-powered predictive text model

#### [Submission URL](https://jackcook.com/2023/09/08/predictive-text.html) | 495 points | by [nojito](https://news.ycombinator.com/user?id=nojito) | [241 comments](https://news.ycombinator.com/item?id=37541093)

Apple's upcoming iOS and macOS versions will feature a predictive text feature powered by a "Transformer language model." Despite Apple's focus on polish and perfection, this may be one of the first Transformer-based models they will ship. However, many details about the feature remain unclear. The feature suggests completions for individual words, occasionally suggesting multiple words when they are obvious. A Python script was used to snoop on AppleSpell activity and stream the most probable suggestions from the predictive text model. The model was located in a bundle file, which contains Espresso model files used during typing. Although the model couldn't be reverse-engineered, it is believed that the predictive text model is kept in this location. The vocabulary set for the model consists of 15,000 tokens, including special tokens, contractions, emojis, and a list of normal-looking tokens. The model's architecture appears to be GPT-2-based.

The discussion on this submission revolves around various aspects of Apple's predictive text feature powered by a "Transformer language model." Some users express surprise and disappointment that Apple's model is generating seemingly irrelevant and grammatically incorrect suggestions. Others speculate on the capabilities and limitations of the model, comparing it to GPT-2 and discussing the quality of its predictions. There is also discussion about the potential for Apple to improve the feature by incorporating higher-quality data or using GPT-3. Several users highlight the challenges of text prediction and autocorrection, including issues with slang and abbreviations. Some users share their experiences with Apple's spell checker and suggest using other tools like Google's spell check for better accuracy. In addition, there are comments about the nature of AI and the potential for AI technologies to be oversold or misused. Finally, there is a brief discussion about the practical limitations and privacy concerns of hosting large AI models on servers.

### A.I. and the Next Generation of Drone Warfare

#### [Submission URL](https://www.newyorker.com/news/news-desk/ai-and-the-next-generation-of-drone-warfare) | 73 points | by [fortran77](https://news.ycombinator.com/user?id=fortran77) | [97 comments](https://news.ycombinator.com/item?id=37549529)

The Deputy Secretary of Defense, Kathleen Hicks, has announced the Replicator initiative, an effort to modernize the American arsenal by adding fleets of artificially intelligent, unmanned, and relatively cheap weapons and equipment. These "attritable" machines can suffer attrition without compromising a mission. The initiative aims to field attritable autonomous systems at scale within the next eighteen to twenty-four months. Instead of concentrating resources on expensive and complicated equipment, Replicator aims to deploy equipment with a shorter shelf life, allowing for constant reinvention of technologies. The use of inexpensive aerial vehicles in concert with one another, known as drone swarms, is a key aspect of Replicator. This approach is based on "iPhone economics," where inexpensive physical devices with expensive software are deployed, so if the enemy destroys them, expensive software is not lost. The war in Ukraine provided proof of concept for drone swarms, as Ukraine used cheaper unmanned aerial vehicles to counter Russia's costly missile systems.

The discussion surrounding the announcement of the Replicator initiative has touched on a variety of topics. Some users have expressed concerns about the potential dangers of developing AI weapons and the implications for democracy. Others have referenced movies like Terminator and Slaughterbots, highlighting the ethical dilemmas associated with deploying such technology. The use of drones in the war in Ukraine has been cited as proof of concept for the effectiveness of drone swarms. There is also discussion about the challenges of balancing cost-effective defense with the need for human-designed weapons, as well as the difficulty of countering cheap and rapidly produced drone technology. The potential for AI-powered killbots and the threat they pose to humans is another topic of concern. Overall, there is a mix of skepticism, caution, and ethical considerations in the discussion.

### The Home Assistant Green is here

#### [Submission URL](https://www.theverge.com/23875557/home-assistant-green-announcement-price-specs-ten-year-anniversary) | 80 points | by [Tomte](https://news.ycombinator.com/user?id=Tomte) | [22 comments](https://news.ycombinator.com/item?id=37548884)

The Home Assistant Green is a new product introduced by the creators of Home Assistant, a software commonly used by privacy-focused individuals who want the benefits of smart home technology without compromising security. The Home Assistant Green aims to make the software more accessible to a wider range of users by providing an all-in-one box with a palatable price. Priced at $99, the Home Assistant Green features powerful hardware, including a RK3566 quad-core CPU, 32GB eMMC storage preloaded with Home Assistant's platform, 4GB of LDDR4x RAM, USB 2.0 slots, HDMI out, and a microSD slot for expansion. The device is designed to run solely on the Home Assistant Operating System and simplifies the onboarding process for users. To get started, users simply plug in the device, connect it to their router via ethernet, and go through the setup process using their phone or computer. The system will automatically detect compatible devices on the user's network. The Home Assistant Green is a convenient and affordable option for those who want to try out Home Assistant without the hassle of setting up hardware.

The discussion on this submission covers a variety of topics related to the Home Assistant Green and the Home Assistant software. Here are some key points:

- One commenter mentions the differences between the Home Assistant Green and the Home Assistant Yellow. They note that the Yellow version supports Raspberry Pi CM4-based boards, including CPU, RAM, networking, ZigBee, and Thread capability, while the Green version does not have these features. They speculate that the Green version might be a way to target a lower price point and simplify the installation process for new users.

- Another user mentions that they recently discovered the Home Assistant Supervisor, a well-designed and actively maintained open-source Python application. They appreciate the high-quality Python libraries and frameworks used in the project, but note a shortage of such libraries for studying open-source applications.

- Some users comment on the price and availability of the Home Assistant Green, noting that it is priced at $99 and that 1,000 units are available today, with 14,000 units available in October.

- The discussion also touches on the compatibility of Home Assistant with various smart home protocols. One user mentions their interest in Zigbee and Thread protocols, while another expresses disappointment with the current state of smart home integration in Home Assistant.

- There is some discussion about voice assistants and the integration of Home Assistant with Google Assistant. One user asks if Home Assistant supports voice commands, and another mentions that they are working on making a voice assistant for Home Assistant.

- Some users discuss the integration of Home Assistant with Apple HomeKit, noting that it provides HomeKit integration that works well.

- The conversation also touches on the use of Home Assistant for non-technical people and the discovery of smart home devices for touch support.

- Finally, there are some comments about other technologies and services that users have integrated with Home Assistant, such as YoLink, AdGuard, and Tailscale. Users share their experiences and discuss the ease of setting up these integrations.

### Spellburst: LLM–Powered Interactive Canvas

#### [Submission URL](https://arxiv.org/abs/2308.03921) | 95 points | by [araes](https://news.ycombinator.com/user?id=araes) | [13 comments](https://news.ycombinator.com/item?id=37540109)

Spellburst: An Innovative Node-based Interface for Creative Coding with Natural Language Prompts

Researchers Tyler Angert, Miroslav Ivan Suzara, Jenny Han, Christopher Lawrence Pondoc, and Hariharan Subramonyam have introduced an exciting new creative-coding environment called Spellburst. The interface is built on a large language model (LLM) and aims to enhance the process of exploratory creative coding.

In the field of digital artwork, artists often start with a high-level semantic concept, like a "stained glass filter," and then programmatically implement it by tweaking code parameters such as shape, color, lines, and opacity. However, translating these semantic constructs into program syntax can be challenging, and existing programming tools do not lend themselves well to rapid creative exploration.

Spellburst addresses these challenges by providing a node-based interface that allows artists to create generative art and explore variations through branching and merging operations. The platform also incorporates expressive prompt-based interactions, enabling artists to engage in semantic programming. Moreover, Spellburst offers dynamic prompt-driven interfaces and direct code editing, allowing users to seamlessly switch between semantic and syntactic exploration.

The researchers evaluated Spellburst with artists and found that it has the potential to enhance creative coding practices. This innovative tool not only facilitates the translation of artistic ideas into code but also bridges the gap between semantic and syntactic spaces. The findings from this study could inform the design of future computational creativity tools.

Spellburst's novel approach to creative coding has the potential to revolutionize the way artists bring their ideas to life through code. With its user-friendly interface and powerful features, Spellburst opens up new possibilities for exploratory creative coding.

The discussion on Hacker News mainly centers around the novelty and potential applications of Spellburst, the new creative-coding environment introduced by the researchers. 

One user shares a link to a video about the system on YouTube, highlighting its extensive design and development process. Another user mentions that Large Language Models (LLMs) have been gaining popularity and provides a link to a User Interface conference paper discussing their working structures and applications.

Some users express interest in trying out Spellburst but have trouble finding where they can access it. One user mentions that there have been previews and tweets about its release but cannot find any public release. Another user comments that they are excited to try it and are willing to participate in testing.

The discussion also touches on the use of metaphorical scratch paper in creative coding and the potential benefits it can offer. Some users mention the challenges of version control in creative coding tasks and express enthusiasm about the innovative features that Spellburst offers.

One user comments on the post itself, providing an introduction and expressing their interest in the application of Large Language Models in creative endeavors.

