## AI Submissions for Fri May 10 2024 {{ 'date': '2024-05-10T17:10:39.780Z' }}

### ENIAC Simulator

#### [Submission URL](http://zuse-z1.zib.de/simulations/eniac/) | 39 points | by [nanna](https://news.ycombinator.com/user?id=nanna) | [12 comments](https://news.ycombinator.com/item?id=40317375)

The ENIAC simulation, a Java applet recreating the first electronic digital computer built at Pennsylvania University in the 1940s, has an interesting history. Originally developed in 2003/2004 at the Free University of Berlin, it now has a new home at http://zuse-z1.zib.de/simulations/eniac. The project has also moved to GitHub, with bug fixes and enhancements. Over the years, it has gained recognition, including being installed at the University of Pennsylvania for the ENIAC's 60th anniversary. If you're interested in learning more about this pioneering computer and its simulation, be sure to check out the various resources available on the project's website.

The discussion on the ENIAC simulation submission includes various recommendations for further reading and resources related to the ENIAC computer:

- One user suggests exploring a book titled "Eniac Triumphs Tragedies Worlds Computer" by Scott McCartney, and another strongly recommends reading "ENIAC in Action" by Thomas Haigh for a technically detailed look at ENIAC and its code samples.
- There is a recommendation for the book "Proving Ground: The Untold Story of the Women Who Programmed the First Electronic Computer" by who highlights the contribution of women programmers to ENIAC.
- Another user recommends a book titled "ENIAC in Action" and provides a link to an interactive website exploring the history and definitions of computers.
- There is a discussion on the definition of a computer and comparisons of various machines like EDSAC, SSEM, EDVAC, Zuse, Colossus, and Harvard Mark ASCC.
- A user provides a detailed comparison between ENIAC and other earlier electronic computers like Zuse and Aiken, highlighting their different memory structures and programming methods.
- The conversation further delves into the development of the Atanasoff computer, its legal battles with ENIAC, and the contributions of John Atanasoff and others to the early computer industry.
- A technical discussion on the memory capacity of ENIAC and its limitations compared to later developments like dynamic random-access memory (DRAM) is also present in the comments.
- Lastly, there is a brief exchange about Java Runtime Environment (JRE) requirements in running the ENIAC simulation.

Overall, the discussion touches on various aspects of the historical and technical details of the ENIAC computer and its significance in the evolution of computing technology.

### Trip C++Now 2024 – think-cell

#### [Submission URL](https://www.think-cell.com/en/career/devblog/trip-report-cpp-now-2024) | 73 points | by [coffeeaddict1](https://news.ycombinator.com/user?id=coffeeaddict1) | [40 comments](https://news.ycombinator.com/item?id=40316814)

At the recent C++Now 2024 conference, attendees were treated to a plethora of insightful talks that delved into the intricacies of C++, Rust, and efficient programming practices. One standout presentation by David Sankel emphasized the importance of using the right tool for the job, whether it be Rust for safety-critical code or C++ for specific scenarios.

Additionally, Louis Dionne showcased hardening techniques in C++ to prevent issues like buffer overflows, while John Bandela discussed Google's alternative range library design that focuses on composing algorithms rather than iterators to enhance performance and avoid dangling references.

Google's approach to coroutines, presented by Aaron Jacobs, showcased clever strategies to prevent lifetime issues, showcasing the importance of careful design in modern programming.

Gašper Ažman introduced a library for monadic error handling in Functional C++, offering solutions for handling different types of errors efficiently. The conference was also graced by talks on value-oriented programming and variadics, shedding light on innovative approaches to writing optimal code.

Overall, C++Now 2024 provided attendees with a wealth of knowledge and insights to enhance their programming skills and efficiency.

The discussion on Hacker News revolves around various aspects of C++ programming, particularly in the context of complexity, performance, and modern programming practices. 

1. **AnonSixOneTh** shares their experience of a developer on LinkedIn showcasing C++ skills, questioning the level of complexity in C++ and its significance in high-frequency trading contexts. **SassyBird** and **chpdrt** provide contrasting opinions on the complexity of C++ and its relevance in high-performance scenarios.

2. **lbhdc** mentions improvements in the latest version of C++, especially in compile-time performance, while **pxlpt** discusses experiences in Germany related to C++ development and intriguing work opportunities.

3. **gpdrtt** delves into Google's strategies in programming, particularly related to performance and avoiding common pitfalls in C++ programming. The conversation touches on the evolution of best practices and design considerations in modern programming.

4. **f1shy** brings up the issue of missing language lawyers in C++ projects, drawing attention to the challenges and nuances in C++ development. **Scubabear68** shares their thoughts on the complexity of C++ leading to high ceilings in skill levels.

5. **nxbjct** raises points about the declaration of functions in modern C++ development practices, suggesting changes and improvements for a more streamlined approach.

6. **pnstrmk** initiates a comparison between C++ ranges and Rust iterators, discussing differences in their implementations and behavioral patterns, sparking a conversation on their respective strengths and use cases.

Overall, the discussion highlights different perspectives on C++ programming, ranging from complexity and performance considerations to comparisons with other programming languages like Rust, shedding light on evolving practices in the programming landscape.

### Is the largest root of a random real polynomial more likely real than complex?

#### [Submission URL](https://mathoverflow.net/questions/470951/is-the-largest-root-of-a-random-polynomial-more-likely-to-be-real-than-complex) | 200 points | by [jjgreen](https://news.ycombinator.com/user?id=jjgreen) | [104 comments](https://news.ycombinator.com/item?id=40316788)

Today on Hacker News, a thought-provoking question is being discussed regarding the likelihood of the largest root of a random polynomial being real or complex. The question delves into the surprising observation that despite there being exponentially more complex roots than real roots in random polynomials, the largest and smallest roots are more likely to be real. The post includes insights from experiments and simulations, sparking curiosity about why this bias exists and whether the probabilities converge as the degree of the polynomial increases.

Math enthusiasts on the forum are engaged in exploring the reasons behind this phenomenon and sharing relevant literature on eigenvalues of random matrices. The discussion also touches on examining discrete cases and potential relationships with earlier research papers by prominent mathematicians. The post has garnered significant attention and continues to attract interest and input from the community.

The discussion on the Hacker News post revolves around a thought-provoking question regarding the likelihood of the largest root of a random polynomial being real or complex. Participants delve into topics such as the properties of primes, their growth patterns, and potential relationships with earlier research in number theory and math literature. The conversation also touches on experimental simulations, random matrix eigenvalues, and the bias towards real roots in random polynomials. Math enthusiasts are actively engaged in exploring the reasons behind this phenomenon and sharing insights on related concepts. The thread attracts an ongoing exchange of thoughts and theories as the community delves deeper into the intriguing question posed in the submission.

### The API database architecture – Stop writing HTTP-GET endpoints

#### [Submission URL](https://www.fabianzeindl.com/posts/the-api-database-architecture) | 90 points | by [fzeindl](https://news.ycombinator.com/user?id=fzeindl) | [146 comments](https://news.ycombinator.com/item?id=40318538)

There is a new trend in software architecture called the "API database architecture" that suggests moving away from writing traditional HTTP GET endpoints and utilizing a server called PostgREST to interact directly with a PostgreSQL database. This approach simplifies data retrieval by serving views and tables as a RESTful HTTP API, supporting various functionalities like filtering, sorting, paging, and authentication. 

The architecture distinguishes between data retrieval via HTTP GET and data modification through other HTTP verbs or non-HTTP mechanisms. While PostgREST can handle data modification using stored procedures, it is recommended to implement business logic in the backend for more complex operations. 

The API database architecture is compatible with different API development paradigms such as REST, CQRS, and GraphQL. It can replace GET endpoints in existing RESTful APIs and aligns well with the principles of CQRS, separating querying and command operations. While GraphQL offers a different approach to data retrieval and modification, the API database architecture provides flexibility and efficiency in handling data aggregation.

Adapting existing architectures to the API database model involves restructuring data-modifying APIs to align with the new architecture. Strategies for transitioning from monolithic systems to microservices with multiple databases are also outlined, emphasizing the benefits of leveraging PostgREST for efficient data retrieval and modification.

The discussion on the submission about the "API database architecture" on Hacker News involved various viewpoints:

1. **CafeRacer** expressed frustration with PostgREST, mentioning the challenge of maintaining complex functions and the need for better testing support. They highlighted the difficulty in incorporating schema changes across different environments effectively.

2. **ftnh** discussed the challenges of database migrations and suggested approaches like using migration scripts and tools like SQLPackage for managing database changes efficiently.

3. **nskng** pointed out the importance of column-level security and recommended avoiding redundant changes in views.

4. **tyr** and **drgnwrtr** discussed database privileges in PostgreSQL and the nuances of granting and revoking permissions at different levels.

5. **CuriouslyC** and **throwaway11460** delved into issues related to column access in PostgREST and compared it with tools like Postgraphile.

6. **mnhks** mentioned an extension related to masking sensitive data in PostgreSQL and highlighted some trade-offs associated with its implementation.

7. **th0ma5** and **vrfrwrd** touched upon data access control and security considerations in database management, emphasizing the importance of handling permissions and avoiding common vulnerabilities like SQL injection attacks.

8. **o1o1o1** provided suggestions for improving the current implementation, while **kbln** discussed the benefits of a modern web and mobile app architecture with a focus on database security.

9. **dvntmhsr** brought up the importance of third-party solutions for security aspects like user authentication processes.

10. **binary132** discussed the differences between database and file systems in terms of architecture and functionality.

The overall discussion highlighted the complexities and considerations involved in modern database architecture, focusing on security, migration strategies, and efficient data management practices.

### Microsoft confirms Windows 11 24H2 turns on Device Encryption by default

#### [Submission URL](https://www.windowslatest.com/2024/05/08/microsoft-confirms-windows-11-24h2-turns-on-device-encryption-by-default/) | 50 points | by [truro](https://news.ycombinator.com/user?id=truro) | [24 comments](https://news.ycombinator.com/item?id=40323966)

In the latest update from Windows Latest, Microsoft is set to introduce Device Encryption as a default feature in the upcoming Windows 11 24H2 version. This encryption will be automatically activated on Pro and Home editions of Windows 11 PCs, ensuring enhanced security for users. The encryption utilizes BitLocker to secure data on system drives, requiring users to back up their BitLocker key to their Microsoft account or an external USB disk. Failure to do so could result in data loss if access is lost. 

Moreover, the article provides tips on how to disable automatic Device Encryption during installation through a Registry hack or by using Rufus to create a modified Windows 11 installation media. These workarounds may be useful for those who prefer more control over their encryption settings. Stay updated on the latest Windows features and security enhancements with Windows Latest.

The discussion on the Hacker News post revolves around the new default Device Encryption feature in Windows 11, set to be introduced in the upcoming 24H2 version. Users express various opinions and concerns regarding this update:

- **Concerns about Data Security**: Some users express concerns about how Windows does not notify users about ransomware attacks and the importance of storing recovery keys securely. There are discussions on potential security risks and the hacking of Microsoft's security practices.

- **BitLocker and BIOS**: There are discussions about the advantages and disadvantages of BitLocker and BIOS settings, including issues related to linking Microsoft accounts during Windows installation. Users also share their experiences with various devices and encryption methods.

- **Alternative Encryption Solutions**: Some users suggest alternative encryption solutions like VeraCrypt and mention the need for better enterprise features in Windows to compete with Linux. Others discuss the implications of double-encrypting with OPAL self-encrypting drives and potential challenges with drive encryption if not done correctly.

- **Criticism and Suggestions**: There are criticisms of Microsoft's encryption methods and suggestions for local key-saving options. Users point out potential vulnerabilities if recovery keys are not encrypted properly. Some express concerns about privacy and the transition from Windows 12 to Vista.

- **Practical Concerns**: A user raises concerns about encrypting drives when crossing borders and being stopped by border patrol, while another highlights the importance of encrypting disks to prevent unauthorized access to sensitive data.

Overall, the discussion covers a wide range of topics related to data security, encryption methods, privacy concerns, and the implications of Microsoft's default encryption feature in Windows 11. Users share their experiences, suggestions, and criticisms regarding the upcoming changes.

### Energy-Efficient Llama 2 Inference on FPGAs via High Level Synthesis

#### [Submission URL](https://arxiv.org/abs/2405.00738) | 92 points | by [PaulHoule](https://news.ycombinator.com/user?id=PaulHoule) | [28 comments](https://news.ycombinator.com/item?id=40315022)

The latest submission on Hacker News highlights a paper titled "HLSTransform: Energy-Efficient Llama 2 Inference on FPGAs Via High Level Synthesis." This paper discusses the development of an accelerator for transformers, specifically Llama 2, using high level synthesis (HLS) on Field Programmable Gate Arrays (FPGAs) to improve energy efficiency and speed in inference tasks. The authors showcase significant reductions in energy usage compared to CPUs and GPUs, while also increasing the speed of inference. The open-sourcing of their code aims to democratize the use of FPGAs in transformer inference, contributing to more energy-efficient methods for machine learning applications.

The discussion around the latest submission on Hacker News revolves around the paper discussing the development of an accelerator for transformers, particularly Llama 2, using high level synthesis on FPGAs to enhance energy efficiency and speed in inference tasks. Several users engaged in detailed technical discussions regarding the comparison between GPUs and FPGAs for inference tasks, highlighting factors like memory bandwidth limitations in FPGAs and the potential benefits of using custom ASICs. Some users mentioned the cost implications and performance trade-offs between GPUs and FPGAs, with considerations for specific use cases and workloads. There were also mentions of the challenges and advantages of designing custom ASICs for training and inference, along with insights into the efficiency of FPGA solutions compared to GPUs. Additionally, comments touched on the democratization of AI inference hardware and the potential of FPGA synthesis in optimizing hardware performance.

### Show HN: Meemaw – Trustless and grandma-friendly wallet as a service

#### [Submission URL](https://github.com/getmeemaw/meemaw) | 27 points | by [marceaul](https://news.ycombinator.com/user?id=marceaul) | [11 comments](https://news.ycombinator.com/item?id=40318468)

🚀 **Trustless and Grandma-friendly Wallet as a Service**

- **What's New**: Meemaw offers a groundbreaking solution to simplify onboarding users to Web3 projects securely at scale.
- **Features**: Trustless MPC-TSS wallets, easy deployment with Docker, seamless integration for Web & iOS, compatibility with Auth providers, and battle-tested technologies.
- **Exciting Updates**: Future plans include Android & cross-platform frameworks support, multi-device access, biometrics & passkeys integration, cloud hosting options, and account abstraction for gas payment.
- **Getting Started**: Meemaw's documentation provides a straightforward guide for developers to deploy servers and integrate client SDKs.
- **Sample Code**: A snippet showcases how to create a wallet and sign transactions using Meemaw SDK with Web3.js for web applications and Swift for iOS.

Explore the innovative world of Meemaw and revolutionize your Web3 project's user experience! 🌟

(Source: [getmeemaw.com](https://getmeemaw.com))

1. **RileyJames** pointed out that they are no longer involved in the cryptocurrency world as they had issues with a previous employer, mentioning the solution offered here seems to address problems they faced.

2. **Atotalnoob** praised the impressive features of the service but critiqued the pricing page, suggesting it lacks transparency for enterprise users. They also raised concerns about personal data access in Enterprise companies.

3. **whlrwj** asked a question about the cryptocurrency community's interest in wallets and why Meemaw has chosen this specific space to focus on now. **mrcl** responded, highlighting that while typical wallets are business-to-consumer tools, Meemaw offers a business-to-business service that helps companies deploy wallets securely.

4. **vrblstn** made a short comment.

5. **DerekRodriguez** mentioned a lighter version of "Gridlock" for frontend development. **dng** referenced guidelines for posting on Hacker News and requested constructive criticism rather than dismissals. **mrcl** clarified the differences between Gridlock and Meemaw, emphasizing Meemaw's focus on mitigating risks for companies.

6. In response, **RileyJames** commented on how developers' products should enable ease of use for end-users, highlighting the importance of user experience.

### Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?

#### [Submission URL](https://arxiv.org/abs/2405.05904) | 35 points | by [Jimmc414](https://news.ycombinator.com/user?id=Jimmc414) | [17 comments](https://news.ycombinator.com/item?id=40324064)

The latest research on whether fine-tuning large language models with new knowledge leads to hallucinations has been explored in a paper by Zorik Gekhman, Gal Yona, and their team. The study investigates how exposing models to new information during fine-tuning affects their ability to incorporate and utilize this data while maintaining accuracy. The results indicate that models struggle to learn new factual knowledge during fine-tuning, with a linear increase in the tendency to produce incorrect responses (hallucinations) as they integrate new information. This raises concerns about introducing new facts during fine-tuning, suggesting that models primarily rely on pre-existing knowledge rather than new inputs for factual understanding. This insightful study sheds light on the delicate balance between model training and knowledge acquisition in language models.

The discussion on Hacker News around the submission involves a deep dive into the topic of fine-tuning large language models (LLMs) with new knowledge and its implications on hallucinations. Users engage in a technical conversation analyzing the nuances of fine-tuning LLMs and the impact on model performance. There is a debate around the effectiveness of fine-tuning models with new data, with some users pointing out potential flaws in the process and others sharing their experiences with different techniques.

One user highlights the difficulties in training models with changeable contexts, questioning the feasibility of such approaches. Another user brings up Twitter as an example of handling explicit fine-tuning in context. The conversation also touches on the challenges of representing collective knowledge in LLMs and the slow pace of model training to avoid hallucinations.

Towards the end of the discussion, a user emphasizes the probabilistic nature of LLMs and the importance of considering different perspectives when assessing the success of fine-tuning. The conversations range from technical details about model training to broader questions about the future of fine-tuning LLMs and the continuous efforts required in this field.

### EA CEO: "Real hunger" among developers to use AI to speed up development

#### [Submission URL](https://www.videogameschronicle.com/news/ea-ceo-says-theres-a-real-hunger-among-developers-to-use-ai-to-speed-up-development/) | 22 points | by [jarsin](https://news.ycombinator.com/user?id=jarsin) | [32 comments](https://news.ycombinator.com/item?id=40319644)

Electronic Arts CEO Andrew Wilson emphasizes the importance of generative AI in speeding up game development during a Q&A session following the company's financial results briefing. Wilson highlights how AI has significantly reduced the time required to create stadiums and add animations in games like EA Sports FC, ultimately enhancing player immersion and engagement. Wilson also envisions using AI to revolutionize over half of EA's developmental processes within the next five years, aiming to build more expansive game worlds with unique storylines.

With the goal of creating bigger, more innovative games at a faster pace, Wilson expresses enthusiasm from developers to leverage generative AI to enhance creativity and efficiency. The potential benefits of AI in game development, including efficiency gains and increased player engagement, are driving EA towards a future where technology augments and extends the nature of interactive entertainment.

The discussion on the Hacker News submission about EA's CEO emphasizing generative AI in game development touches on several key points. 

1. The importance of game-breaking DLCs and the role of artists, programmers, and designers in creating realistic game worlds are acknowledged, along with the idea that AI can simplify certain tasks but may not fully replace human creativity and decision-making. There is also a mention of the necessity of skilled humans in critical roles despite advancements in AI. 

2. The conversation also delves into the potential future impact of AI on society, including its role in jobs, transportation, and construction. The concept of a future where AI significantly influences daily life is discussed, with differing opinions on whether AI will lead to a better or worse world.

3. There is a debate on whether AI will reduce the need for human labor and the potential consequences of AI advancements, including concerns about inequality, job displacement, and the impact on the workforce. Some commenters point out that AI may replace high-skilled jobs while others argue that AI can complement human abilities in various fields.

4. The discussion also includes skepticism about corporations' claims regarding the adoption of AI and highlights a retrospective on how AI has evolved in the gaming industry over the years. Some users express doubts about EA's intentions and emphasize the importance of trust in gaming companies, especially favoring smaller publishers and developers.

Overall, the comments reflect a mix of optimism about AI's potential to enhance creativity and efficiency in game development, as well as concerns about the broader societal implications of its widespread adoption.

