## AI Submissions for Fri May 10 2024 {{ 'date': '2024-05-10T17:10:39.780Z' }}

### Energy-Efficient Llama 2 Inference on FPGAs via High Level Synthesis

#### [Submission URL](https://arxiv.org/abs/2405.00738) | 92 points | by [PaulHoule](https://news.ycombinator.com/user?id=PaulHoule) | [28 comments](https://news.ycombinator.com/item?id=40315022)

The latest submission on Hacker News highlights a paper titled "HLSTransform: Energy-Efficient Llama 2 Inference on FPGAs Via High Level Synthesis." This paper discusses the development of an accelerator for transformers, specifically Llama 2, using high level synthesis (HLS) on Field Programmable Gate Arrays (FPGAs) to improve energy efficiency and speed in inference tasks. The authors showcase significant reductions in energy usage compared to CPUs and GPUs, while also increasing the speed of inference. The open-sourcing of their code aims to democratize the use of FPGAs in transformer inference, contributing to more energy-efficient methods for machine learning applications.

The discussion around the latest submission on Hacker News revolves around the paper discussing the development of an accelerator for transformers, particularly Llama 2, using high level synthesis on FPGAs to enhance energy efficiency and speed in inference tasks. Several users engaged in detailed technical discussions regarding the comparison between GPUs and FPGAs for inference tasks, highlighting factors like memory bandwidth limitations in FPGAs and the potential benefits of using custom ASICs. Some users mentioned the cost implications and performance trade-offs between GPUs and FPGAs, with considerations for specific use cases and workloads. There were also mentions of the challenges and advantages of designing custom ASICs for training and inference, along with insights into the efficiency of FPGA solutions compared to GPUs. Additionally, comments touched on the democratization of AI inference hardware and the potential of FPGA synthesis in optimizing hardware performance.

### Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?

#### [Submission URL](https://arxiv.org/abs/2405.05904) | 35 points | by [Jimmc414](https://news.ycombinator.com/user?id=Jimmc414) | [17 comments](https://news.ycombinator.com/item?id=40324064)

The latest research on whether fine-tuning large language models with new knowledge leads to hallucinations has been explored in a paper by Zorik Gekhman, Gal Yona, and their team. The study investigates how exposing models to new information during fine-tuning affects their ability to incorporate and utilize this data while maintaining accuracy. The results indicate that models struggle to learn new factual knowledge during fine-tuning, with a linear increase in the tendency to produce incorrect responses (hallucinations) as they integrate new information. This raises concerns about introducing new facts during fine-tuning, suggesting that models primarily rely on pre-existing knowledge rather than new inputs for factual understanding. This insightful study sheds light on the delicate balance between model training and knowledge acquisition in language models.

The discussion on Hacker News around the submission involves a deep dive into the topic of fine-tuning large language models (LLMs) with new knowledge and its implications on hallucinations. Users engage in a technical conversation analyzing the nuances of fine-tuning LLMs and the impact on model performance. There is a debate around the effectiveness of fine-tuning models with new data, with some users pointing out potential flaws in the process and others sharing their experiences with different techniques.

One user highlights the difficulties in training models with changeable contexts, questioning the feasibility of such approaches. Another user brings up Twitter as an example of handling explicit fine-tuning in context. The conversation also touches on the challenges of representing collective knowledge in LLMs and the slow pace of model training to avoid hallucinations.

Towards the end of the discussion, a user emphasizes the probabilistic nature of LLMs and the importance of considering different perspectives when assessing the success of fine-tuning. The conversations range from technical details about model training to broader questions about the future of fine-tuning LLMs and the continuous efforts required in this field.

### EA CEO: "Real hunger" among developers to use AI to speed up development

#### [Submission URL](https://www.videogameschronicle.com/news/ea-ceo-says-theres-a-real-hunger-among-developers-to-use-ai-to-speed-up-development/) | 22 points | by [jarsin](https://news.ycombinator.com/user?id=jarsin) | [32 comments](https://news.ycombinator.com/item?id=40319644)

Electronic Arts CEO Andrew Wilson emphasizes the importance of generative AI in speeding up game development during a Q&A session following the company's financial results briefing. Wilson highlights how AI has significantly reduced the time required to create stadiums and add animations in games like EA Sports FC, ultimately enhancing player immersion and engagement. Wilson also envisions using AI to revolutionize over half of EA's developmental processes within the next five years, aiming to build more expansive game worlds with unique storylines.

With the goal of creating bigger, more innovative games at a faster pace, Wilson expresses enthusiasm from developers to leverage generative AI to enhance creativity and efficiency. The potential benefits of AI in game development, including efficiency gains and increased player engagement, are driving EA towards a future where technology augments and extends the nature of interactive entertainment.

The discussion on the Hacker News submission about EA's CEO emphasizing generative AI in game development touches on several key points. 

1. The importance of game-breaking DLCs and the role of artists, programmers, and designers in creating realistic game worlds are acknowledged, along with the idea that AI can simplify certain tasks but may not fully replace human creativity and decision-making. There is also a mention of the necessity of skilled humans in critical roles despite advancements in AI. 
2. The conversation also delves into the potential future impact of AI on society, including its role in jobs, transportation, and construction. The concept of a future where AI significantly influences daily life is discussed, with differing opinions on whether AI will lead to a better or worse world.
3. There is a debate on whether AI will reduce the need for human labor and the potential consequences of AI advancements, including concerns about inequality, job displacement, and the impact on the workforce. Some commenters point out that AI may replace high-skilled jobs while others argue that AI can complement human abilities in various fields.
4. The discussion also includes skepticism about corporations' claims regarding the adoption of AI and highlights a retrospective on how AI has evolved in the gaming industry over the years. Some users express doubts about EA's intentions and emphasize the importance of trust in gaming companies, especially favoring smaller publishers and developers.

Overall, the comments reflect a mix of optimism about AI's potential to enhance creativity and efficiency in game development, as well as concerns about the broader societal implications of its widespread adoption.

