## AI Submissions for Mon Jul 03 2023 {{ 'date': '2023-07-03T17:11:17.405Z' }}

### The industry behind the industry behind AI

#### [Submission URL](https://restofworld.org/2023/exporter-industry-behind-ai/) | 36 points | by [marban](https://news.ycombinator.com/user?id=marban) | [12 comments](https://news.ycombinator.com/item?id=36573813)

The hidden labor behind artificial intelligence (AI) is brought to light in a recent feature by The Verge. The article focuses on a Remotasks office in Nairobi which is a subsidiary of Scale AI, where workers perform annotation tasks to improve AI algorithms. These tasks range from identifying human or robotic voices in audio clips to rating the sexual provocation of online ads. The article highlights the reliance of generative AI on human labor and the low wages that workers in this field often receive. The similarities between AI annotation work and moderation contractors—who clean up platforms like Facebook and YouTube—are noted, as many of these companies operate in both areas. The article discusses the industry of business process outsourcing (BPO), which includes call centers and various types of work. While annotation work may not be traumatic like moderation, the lack of better pay and visibility for workers remains a concern.

The discussion on this submission covers various aspects related to the use and regulation of AI and data mining. 

One user suggests the use of open-source software for training AI models to reduce reliance on proprietary sources and increase transparency. Another user mentions HuggingFace, a platform that provides pretrained models for natural language processing tasks.

A link to an interesting article on The Verge about AI and its reliance on human labor is shared. Another user compares this topic to the movie "Parasite" and its depiction of hidden labor.

A user shares that they couldn't find any articles on AI training jobs, and another user discusses how AI companies pay workers involved in AI training and annotation.

An insightful comment points out that the focus should primarily be on regulating data, algorithms, and compute power in governing AI. The user mentions that the regulation should cover aspects like collection, processing, privacy, and hardware.

A discussion on the regulation of personal data collection and privacy ensues, with a mention of the General Data Protection Regulation (GDPR). One user expresses skepticism about the effectiveness of GDPR enforcement.

Lastly, a user brings up the topic of undergraduate researchers and their involvement in data annotation tasks, suggesting that they receive meaningful research credit for their work.

### Automate Your Network

#### [Submission URL](https://github.com/automateyournetwork/automate_your_network) | 149 points | by [hjuutilainen](https://news.ycombinator.com/user?id=hjuutilainen) | [32 comments](https://news.ycombinator.com/item?id=36573881)

Title: "Automate Your Network: A Free Book on Network Automation"

Summary: John Capobianco, an advocate for open source software, has released his book, "Automate Your Network," in PDF format for everyone to enjoy. While he no longer advocates for Ansible, the book serves as a valuable resource for those starting their journey into the world of network automation. With 359 stars and 13 forks on GitHub, it's evident that this book has caught the attention of the developer community. If you're interested in learning more about network automation, give it a read!

Source: [GitHub - automateyournetwork/automate_your_network](https://github.com/automateyournetwork/automate_your_network)

The discussion on the submission "Automate Your Network: A Free Book on Network Automation" on Hacker News mainly revolves around different tools and approaches for network automation.

One user suggests looking into alternatives to Ansible, such as NAPALM and Nornir, for network automation. They mention that these tools provide better features and integration with Python for network engineers.

Another user mentions that network automation tools like Ansible and Terraform are technically compilers that use proprietary object models. They argue that abstraction is a key benefit of automation and using object models can help simplify and standardize network configurations.

There is a discussion about the benefits and challenges of using custom object models in network automation. One user explains that object models allow for flexible inputs and lifecycle management of network configurations. However, another user points out that if the object model is too specific, it may not scale well.

Some users discuss their experiences with using Ansible and Terraform for network automation. They mention advantages like faster local state manipulation and standardization through writing HCL (HashiCorp Configuration Language) files. However, they also acknowledge that these tools may behave differently and suggest exploring other options.

There is a mention of a similar tool called Pulumi, which is compared to Terraform and Ansible. It is suggested that Pulumi might be more suitable for component-based descriptions of networks.

The discussion also touches on topics like the practicality of network automation in different network infrastructure setups, the relevance of 802.1q VLAN tags, and the complexity of modern networks.

Overall, the comments provide insights into different approaches and tools for network automation and highlight various considerations and challenges in the field.

### Bfloat16 support coming to Apple's Metal and PyTorch [video]

#### [Submission URL](https://developer.apple.com/videos/play/wwdc2023/10050/?time=590) | 90 points | by [dlewis1788](https://news.ycombinator.com/user?id=dlewis1788) | [52 comments](https://news.ycombinator.com/item?id=36575443)

Apple made several updates and enhancements to machine learning (ML) in Metal at the WWDC event. The Metal Performance Shaders framework provides APIs for ML, with MPSGraph as a general-purpose compute graph that extends support to multi-dimensional tensors. MPSGraph supports training frameworks like TensorFlow and PyTorch, as well as inference frameworks like CoreML. PyTorch and TensorFlow now have Metal acceleration, enabling developers to leverage highly efficient kernels for optimal performance on Macs. PyTorch 2.0 introduces new features such as profiling support for MPS operations and support for popular Torch operators. Furthermore, PyTorch models like YOLO have adopted the MPS backend, improving performance. The latest PyTorch builds also include features like custom kernels and Automatic Mixed Precision support. The updates to Metal and PyTorch demonstrate Apple's commitment to enhancing ML capabilities and providing developers with powerful tools for ML tasks.

The discussion on the submission revolves around various aspects of Apple's updates and enhancements to machine learning in Metal. Some users discuss the benefits and trade-offs of using 4-bit weights compared to 16-bit weights, highlighting the potential for improved performance with the former. Others recommend papers on 4-bit quantization and its impact on training and model accuracy. There is also mention of the lowered precision of 1-bit weights and the benefits of using 2-bit quantization. 

Regarding bfloat16 support, it is confirmed that the Apple M1 lacks complete support for bfloat16, but the transition from FP32 to bfloat16 does not necessarily bring significant performance benefits depending on the hardware design. Users discuss the advantages and challenges of converting between different numeric formats, such as float32 and bfloat16, and how it affects memory bandwidth and compute efficiency.

There is a mention of Sonoma and bfloat16 support in Metal, as well as the addition of bfloat16 support at the hardware level in the M2 chip. Users also highlight the transparency of software emulation for bfloat16 on Macs that don't have native support.

In terms of investment considerations, one user suggests that Apple's in-house technology and its combination of speed and efficiency make it a compelling choice. However, there are also discussions about alternative options, such as Windows, Linux, and platforms like AWS.

The discussion further touches on the comparison between Apple's GPU compatibility and NVIDIA, as well as the performance and quality trade-offs between bfloat16 and fp16 in training and inference tasks. Some users share their experiences and observations regarding the use of lower-precision formats and their impact on neural networks.

### Show HN: JobLens – AI-powered job search for 'Who Is Hiring'

#### [Submission URL](https://www.kadoa.com/joblens) | 95 points | by [hubraumhugo](https://news.ycombinator.com/user?id=hubraumhugo) | [18 comments](https://news.ycombinator.com/item?id=36574090)

Introducing PlaygroundAPI, a new AI-powered job search tool called JobLens. With JobLens, you can select your sources, define your preferences, and find your dream job. The available sources include Ask HN: Who is hiring? from July and June 2023, and various employer types and role types to choose from. You can also filter jobs based on remote or on-site work, location (with countries offering available jobs), industry, minimum salary (where available), and technology. While the tool is currently in development, it already leverages the power of large language models (LLMs) to reformat unstructured job postings into a unified structure, making it easier for job seekers. In the future, you will have the option to personalize monitoring for jobs, events, and real estate, thanks to AI. If you have any feedback, reach out to Kadoa via email. Stay tuned for more updates from PlaygroundAPI!

The discussion around the JobLens project on Hacker News revolves around several different topics:

1. Parsing and transformation: Some users are impressed by JobLens' ability to extract URLs and candidate profiles automatically, highlighting the time-saving aspect of automating these tasks using large language models (LLMs).

2. Conversational AI potential: A user suggests that conversational AI has the potential to make job searches more interesting by providing a back-and-forth discussion and helping to discover relevant jobs beyond conventional search systems.

3. Design and categorization: The topic of design and categorization is brought up, specifically regarding the scattered nature of design jobs and the difficulty in searching for them, as well as the decision-making process when it comes to job searching.

4. OpenAI functions API: One user shares their positive experience with using OpenAI's functions API for structured text extraction, expressing interest in the possible applications of the technology and the problem-solving potential it offers.

5. Field identification: A user points out that JobLens seems to be incorrectly identifying certain fields, specifically mentioning global location matching for job locations. However, they also appreciate the functionality of OpenAI's functions and direct prompt YAML parsing.

6. Leveraging unstructured job postings: One user expresses interest in leveraging unstructured job postings and using GPT-3.5-turbo's chatting API, noting that the system prompt describes the desired format as nested JSON format.

7. LinkedIn integration: There is a brief mention of scraping LinkedIn job postings and the requirement of a login, with one user suggesting that the site might be hindered by the need for a login.

8. Site feedback: A few users offer minor suggestions for improvement, suggesting that the site might not be suitable for the Hacker News audience or mentioning similar platforms for freelancing.

Overall, there is a mix of positive feedback, suggestions for improvement, and discussions about the potential applications and challenges of using AI in the job search process.

### Disabling Matrix Portalling

#### [Submission URL](https://libera.chat/news/matrix-deportalling) | 101 points | by [matricaria](https://news.ycombinator.com/user?id=matricaria) | [35 comments](https://news.ycombinator.com/item?id=36573873)

Libera.Chat, a popular IRC network, has announced that they will be disabling Matrix Portalling due to issues with the Libera.Chat<>Matrix bridge. The decision comes after closely monitoring the developments and concerns surrounding the bridge. The network will request that EMS (Element Matrix Services) disable Portalled channels by the end of July 2023, or disable the full Matrix bridge if necessary. This change aims to make the Matrix bridge an opt-in feature for channel operators, improving the Matrix experience on the IRC side. However, plumbed channels will remain unaffected. Libera.Chat recognizes that this change may have a significant impact on some communities and is exploring options to introduce a new channel mode that would allow the Matrix bridge back in. Users with questions or concerns can reach out to Libera.Chat staff in the designated channels for assistance and discussion.

The discussion on Hacker News revolves around the decision made by Libera.Chat to disable Matrix Portalling due to issues with the Libera.Chat<>Matrix bridge. Some commenters express their frustration with the problems that have been occurring, including delays and dropped messages. Others mention concerns about spam, harassment, and the lack of moderation tools in Matrix. The conversation also touches on the stability and scalability of the Matrix bridge, with some users pointing out that the number of connected IRC users through Matrix is much higher than expected. There is also discussion about alternative solutions and the development of Matrix as a whole, with mixed opinions on its progress and stability. Some users express disappointment with the decision to disable Matrix Portalling, while others understand the reasoning behind it.

### Google claims to have proved its supremacy with new quantum computer

#### [Submission URL](https://www.telegraph.co.uk/business/2023/07/02/google-quantum-computer-breakthrough-instant-calculations/) | 250 points | by [andrewstuart](https://news.ycombinator.com/user?id=andrewstuart) | [220 comments](https://news.ycombinator.com/item?id=36567839)

Google has developed a quantum computer that can perform calculations that would take the best existing supercomputers 47 years to complete, according to a paper published by Google researchers. The technology, which relies on the peculiar states of quantum physics, has the potential to create powerful machines that can address climate change and develop breakthrough drugs. However, it also poses a risk to current encryption systems, making it a national security priority. This development comes four years after Google claimed to achieve "quantum supremacy," with rivals disputing this claim. The researchers' latest paper aims to settle the debate by demonstrating a more powerful quantum computer. The new machine has 70 qubits, or quantum bits, making it 241 million times more powerful than the 2019 machine. The researchers claim that their quantum computer is more powerful than demonstrations from a Chinese lab, which is considered a leader in the field. Quantum computers must now demonstrate more practical functions to prove their value beyond academic study.

The discussion on this submission covers various aspects of quantum computing. Here are some key points:

- Some commenters discuss the limitations of current quantum computers, such as error correction and system reliability.
- There is a debate about the practical applications of quantum computing and whether it will be able to outperform classical computers in certain tasks.
- The potential impact of quantum computing on cryptography and encryption systems is also mentioned. Some speculate that it could lead to the breaking of encryption algorithms, while others point out that there are still many challenges to overcome.
- The scalability and feasibility of building quantum computers with a large number of qubits is discussed, along with the need for advanced cooling and refrigeration techniques.
- Commenters also mention the historical progress and challenges in developing different types of quantum computers, such as photonic quantum computers.

Overall, the discussion highlights both the potential and the challenges of quantum computing, with various perspectives on its practicality and future developments.

### A Quick Overview of Matrix

#### [Submission URL](https://juliette.page/b/matrix) | 42 points | by [julietteeb](https://news.ycombinator.com/user?id=julietteeb) | [36 comments](https://news.ycombinator.com/item?id=36570339)

Matrix: A Federated Chat Platform with Unique Features

In the midst of the 2020 quarantine, Discord became a haven for many, offering a plethora of communities to interact with and find solace in. Countless close friendships were formed, leaving behind nostalgic memories. However, some have gradually migrated to a new platform called Matrix, which has a promising future.

Matrix, a federated chat platform, works differently from Discord. Instead of being one centralized platform, Matrix is a network of interconnected communities known as spaces. Similar to Discord servers, you can join different spaces and enjoy features like voice chat, direct messages, group chats, and channels, just as you would expect in a chat platform.

One standout feature of Matrix is its bridges, which enable users to connect with other chat platforms. For instance, if you want to communicate with someone on Discord while on Matrix, all you need to do is install the Matrix Discord bridge. This allows you to converse with your Discord friends from within the same Matrix app, eliminating the need for multiple chat applications.

Furthermore, Matrix allows users to create their own clients or custom apps to access their chats. If you're not a fan of the official Matrix app, Element, you have the option to install alternative clients like FluffyChat or even develop your own. This freedom empowers users to tailor their chat experience to their preferences.

In summary, Matrix offers an exceptional chat platform experience, placing control in the hands of users rather than a large corporation. With its federated structure, seamless integration with other chat platforms through bridges, and the ability to customize clients, Matrix presents itself as an attractive alternative for those seeking a new chat platform.

The discussion on Hacker News about Matrix, a federated chat platform, covers several topics. One user mentions a presentation about Matrix happening at FOSDEM 2023 in February. Another user expresses frustration with the lack of documentation and user experience on Matrix, contrasting it with the more polished Element client. The discussion also touches on the stability of the JavaScript SDK and the potential for improvements in the user interface. Some users express concerns about the broken notifications and the encryption of messages on Matrix. The conversation shifts to the comparison between Matrix and IRC, with one user mentioning the lack of features in Matrix compared to IRC and the performance concerns with the Matrix protocol. The discussion also covers the Matrix IRC bridge and its limitations, including issues with logging chat messages and the visibility of channels. Additionally, there are mentions of SourceHut and individual experiences with Matrix as a client.

### Online Safety Bill: WhatsApp, Signal issue final warning against mass snooping

#### [Submission URL](https://www.standard.co.uk/tech/online-safety-bill-whatsapp-signal-element-breaking-encryption-mass-surveillance-messaging-apps-b1091873.html) | 38 points | by [concertina226](https://news.ycombinator.com/user?id=concertina226) | [7 comments](https://news.ycombinator.com/item?id=36580363)

The heads of messaging apps WhatsApp, Signal, and Element have warned that the UK government's Online Safety Bill could lead to mass surveillance and the destruction of London's tech scene. They argue that the bill's vague language opens the door for surveillance and the nullification of end-to-end encryption, creating vulnerabilities that could be exploited. The tech firms claim that if the bill passes into law, tech companies will leave London and the UK will lose its reputation as a place to do business. A survey of 2,000 UK citizens found that 70% of respondents do not believe scanning online messages will stop criminal activity and nearly half think it will make the UK more vulnerable to cyberattacks.

The discussion on this submission revolves around concerns regarding the UK government's Online Safety Bill and its potential impact on tech companies and privacy. 

Some users express their agreement with the heads of messaging apps in their warnings about the bill. They believe that the vague language in the bill could lead to mass surveillance and the destruction of London's tech scene. They argue that tech companies will leave London if the bill becomes law, harming the UK's reputation as a place for business.

Others point out that the tech industry is constantly evolving and facing challenges, such as Reddit, Twitter, Facebook, Amazon, and Google dealing with various issues. They suggest that now is the perfect time for solutions like the merger of Federation to handle hosting and social media.

One user disagrees, mentioning the thriving young comments on Hacker News and suggesting looking for thriving communities on platforms like IRC and internet forums.

Another user expresses concerns about the impact of the bill and calls for the government to apologize for its potential negative consequences. They also mention the political factor, stating that the Tory party is in opposition until 2025. 

There is also discussion about government policies and the influence of extremist wings on decision-making.

### Classifier Free Guidance works on LLMs with a significant boost in performance

#### [Submission URL](https://arxiv.org/abs/2306.17806) | 22 points | by [og_kalu](https://news.ycombinator.com/user?id=og_kalu) | [8 comments](https://news.ycombinator.com/item?id=36573254)

The paper "Stay on topic with Classifier-Free Guidance" introduces a technique called Classifier-Free Guidance (CFG) that can improve language modeling performance. The authors demonstrate that CFG can enhance models across a range of tasks including question-answering, reasoning, code generation, and machine translation. They show that CFG can achieve state-of-the-art results on the LAMBADA dataset and bring improvements equivalent to doubling the parameter-count of a model. Additionally, CFG can be combined with other inference-time methods, such as Chain-of-Thought and Self-Consistency, to further improve performance. In a human evaluation, the authors show that assistants using CFG are preferred by 75% over baseline systems in terms of faithfulness and coherence. Overall, the paper highlights the effectiveness of CFG as a technique for improving language modeling performance in various applications.

The discussion around the submission "Stay on topic with Classifier-Free Guidance" on Hacker News touched on various aspects of the paper and the topic in general.

- One user pointed out that stability and core weaving are important factors in implementing the technique, and that funding from Coreweave helped in the realization of the paper's implementation.
- Another user shared a link to the GitHub issues page of the library "Hugging Face Transformers," suggesting that the paper's implementation could be contributed there.
- There was a mention of advancements in diffusion community learning for text generation, indicating that the community might not be interested in limiting content to specific local norms.
- A user discussed the perceived lack of readability in papers and the need for changes in function notation to improve understanding. There was agreement that papers often lack readability, but it was mentioned that the chance of any given week ending without a function change is only 95%.
- In response to a comment about implementing negative prompts, it was highlighted that there are technical reasons preventing the implementation of everything in natural language processing (NLP).
- Towards the end of the discussion, a user made a brief comment about the dimensions of neural networks being an essential consideration in implementing the technique presented in the paper.

### Nvidia’s H100: Funny L2, and Tons of Bandwidth

#### [Submission URL](https://chipsandcheese.com/2023/07/02/nvidias-h100-funny-l2-and-tons-of-bandwidth/) | 129 points | by [picture](https://news.ycombinator.com/user?id=picture) | [48 comments](https://news.ycombinator.com/item?id=36569044)

Nvidia has released its latest compute-oriented GPU, the H100, built on the Hopper architecture. The GPU features 144 Streaming Multiprocessors, 60 MB of L2 cache, and 12 512-bit HBM memory controllers. The PCIe version of the H100, tested on Lambda Cloud, offers 114 SMs, 50 MB of L2 cache, and 10 HBM2 memory controllers. The SXM form factor H100, on the other hand, can draw up to 700W and has 132 SMs enabled, along with HBM3 memory for additional bandwidth. The H100 boasts higher boost clock speeds than its predecessor, the A100, but sometimes drops to 80% of its maximum boost clock during microbenchmarking. The H100 features larger L1/Shared Memory capacity and a 50 MB L2 cache, with access to the "far" L2 partition taking nearly twice as long. Overall, the H100 represents a significant improvement over the A100 in terms of cache capacity and latency.

Discussion Summary:

- One user mentioned that there are still issues with running certain frameworks like Onnx PyTorch on the H100, as they haven't been addressed yet. Another user added that running PyTorch on the H100 led to incorrect assumptions being made due to underlying implementation details.
  
- One user found it interesting that the power and spending are focused on the DRAM transfer, with HBM3 consuming 5 pJ/bit. They mentioned that the H100's 335TB/s bandwidth requires 135 watts of RAM. They also suggested that it would be more cost-effective to build a chip with all-narrow-bandwidth read-fully capability rather than fancy compute elements.
  
- There was a discussion about Nvidia's support for OpenCL's FP16 extension. Some users mentioned that Nvidia's hardware does not fully support standards, while others pointed out that Nvidia has their own proprietary stack. One user mentioned that they did not test the TPUv5.
  
- The importance of RAM skimming was mentioned, with one user referencing an article on Tom's Hardware that stated 80GB boards cost around $3,500. Another user suggested converting the price from yen to dollars and found it to be over $30,000, but later realized there was a mistake in the conversion. Another user pointed out that GDDR6 currently costs $300 for 80GB, and in 1-3 years, it may increase to $1,000, making the GPU more expensive.
  
- One user shared their perspective on AI workloads and mentioned that CPUs have slower branch execution than GPUs. They also discussed the importance of memory and how GPUs have improved in parallel calculations and memory performance.
  
- There was a discussion about FPGAs and their potential use for deep neural networks. One user mentioned that large FPGA arrays can potentially verify chips, but another user pointed out that people have used FPGAs for DNNs before.
  
- Some users discussed the physical specifications of the H100 and the differences between the PCIe version and the SXM version. They also mentioned the number of enabled SMs, power draw, and constraints on power supply and cooling.
  
- The segmentation of the market and power envelope for PCIe cards were discussed, with one user mentioning that market segmentation and yield enhancement can be limiting factors. Another user added that power envelopes are often disregarded.
  
- There was a discussion about the naming of the H100, with one user suggesting that it sounds like a product by George Hotz. Other users mentioned research projects and the direction of computing fabrics and non-memory computation.
  
- One user pointed out that the discussion was not mentioning GPUs' execution of non-readers that are typical in modern high-performance CPUs.
  
- The topic of memory preparedness and addressing random access of weights in machine learning models was discussed. One user mentioned that large-scale models with random access to weights could improve performance.
  
- The limitations of the H100, such as the number of SMs and power constraints, were discussed along with market segmentation and the power envelope.
  
- One user mentioned that they are testing the H100 on Lambda Cloud and provided details about the SMs, L2 cache, and memory controllers enabled. Another user expressed confusion about the number of enabled elements and requested clarification.

### Tesla is valuing Full Self-Driving high only when it’s convenient

#### [Submission URL](https://electrek.co/2023/07/03/tesla-valuing-full-self-driving-high-when-convenient/) | 39 points | by [mfiguiere](https://news.ycombinator.com/user?id=mfiguiere) | [12 comments](https://news.ycombinator.com/item?id=36578225)

Tesla's valuation of its Full Self-Driving (FSD) package appears to be inconsistent, with the company valuing it high when it's convenient and lower for trade-ins. Tesla CEO Elon Musk has previously claimed that vehicles equipped with the FSD package would be "appreciating assets" as the package improved through software updates. However, trade-in estimates for Tesla vehicles with the FSD package seem to be lower than the package's actual price. This has led to frustration among Tesla owners who feel that their FSD package is being devalued, despite Tesla not delivering on its promises of full autonomy. Some users have suggested that Tesla should allow the transfer of the FSD software to new vehicles to incentivize current owners to upgrade. This would help increase sales and create goodwill around the FSD package.

The discussion on Hacker News regarding the submission revolves around frustration with Tesla's valuation and handling of its Full Self-Driving (FSD) package. Some users express frustration with Elon Musk's claims about the FSD package being an appreciating asset despite the package not delivering on its promises of full autonomy. Others suggest that allowing the transfer of the FSD software to new vehicles would incentivize current owners to upgrade, increase sales, and create goodwill. Some users argue that Tesla doesn't value the FSD package highly because it isn't willing to help sales by creating goodwill around it. There is also mention of the frustration over not being able to trade-in software and the misunderstanding that Tesla doesn't allow it, with clarification that it's about not being able to transfer the software to third-party buyers. The discussion also touches on skepticism about Musk's claims and connects it to potential stock market dynamics. Overall, the discussion highlights concerns and dissatisfaction with Tesla's valuation and handling of the FSD package.

### Self-driving cars are surveillance cameras on wheels

#### [Submission URL](https://www.schneier.com/blog/archives/2023/07/self-driving-cars-are-surveillance-cameras-on-wheels.html) | 261 points | by [activiation](https://news.ycombinator.com/user?id=activiation) | [270 comments](https://news.ycombinator.com/item?id=36572401)

Self-driving cars may be convenient for commuting, but they're also becoming a new tool for law enforcement surveillance. Police are increasingly using footage from self-driving cars as video evidence in criminal investigations. While security cameras are already common in cities, self-driving cars offer a new level of access and coverage. They capture a wider range of footage as they navigate the city, making it easier for law enforcement to turn to one company with a large repository of videos instead of reaching out to multiple businesses with their own security systems. However, this raises concerns about privacy and the erosion of personal freedom. Advocates argue that individuals should be able to go about their daily lives without constant surveillance, unless they are suspected of a crime. As self-driving cars become more prevalent, it's likely that video evidence will play a larger role in criminal cases.

The discussion on Hacker News revolves around the topic of self-driving cars and their use in law enforcement surveillance. Some users express concerns about the erosion of privacy and personal freedom, while others argue that video evidence from self-driving cars can be helpful in criminal investigations. There is also a discussion about the limitations and potential abuses of surveillance systems, as well as the role of speed cameras and the impact of government-owned cameras. Additionally, there are debates about the importance of privacy and the potential benefits and drawbacks of data collection by connected cars. Some users highlight the need for regulation and accountability in the use of surveillance technology.

### Valve responds to claims it has banned AI-generated games from Steam

#### [Submission URL](https://techcrunch.com/2023/07/03/valve-responds-to-claims-it-has-banned-ai-generated-games-from-steam/) | 23 points | by [lsllc](https://news.ycombinator.com/user?id=lsllc) | [12 comments](https://news.ycombinator.com/item?id=36580344)

Valve, the developer of the Half-Life series and operator of the Steam games store, has clarified its policy on games with AI-generated assets. This comes after rumors spread that Valve was rejecting games utilizing AI-generated content. The company stated that its policy is not a stand against AI, but rather an evolving approach to content approval. Valve's rules on content can be unclear until developers test them with unique cases. One developer had their game rejected due to having AI-generated assets that potentially infringed on intellectual property rights. Valve cited unclear legal ownership of such assets as the reason for rejecting the game. The use of AI as a game development tool is not controversial, with major developers like Ubisoft embracing the technology. However, the issue arises when AI-generated content involves unpaid artists. It remains unclear who bears liability for the generated art. Valve clarified that its review process is based on current copyright laws, not personal opinion. In cases where this policy decision influences game rejection, Valve will refund the app submission fee. While some developers may utilize AI-generated content for quick profits, as the use of AI tools becomes more widespread and sophisticated, the matter becomes less straightforward.

The discussion on the submission primarily revolves around the implications of using AI-generated assets in games and the legal and ethical challenges associated with it. Some users express concerns about the potential low quality and lack of originality in AI-generated content, suggesting that it may lead to an influx of low-quality games on platforms like Steam. Others raise issues of copyright infringement and the difficulty in determining ownership of AI-generated assets. 

There is also a discussion about the regulations and policies surrounding AI-generated games. Some users reference Google's policies regarding AI-generated content and the potential legal battles that could arise from such regulations. One user argues that AI-generated assets should be allowed under Creative Commons licenses, while another highlights the challenges posed by AI-generated textures and the need for greater compensation for content creators.

One user brings up the topic of attributing AI-generated content, noting that it can be challenging to trace the original source of such content. Another user argues that using AI for content generation is similar to using tools like Photoshop and should be seen in the same light.

Overall, the discussion highlights the complex legal, ethical, and quality-related considerations surrounding AI-generated assets in games.

