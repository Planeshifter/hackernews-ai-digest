## AI Submissions for Sun May 28 2023 {{ 'date': '2023-05-28T17:11:43.785Z' }}

### Easy Effects: Audio effects for PipeWire applications

#### [Submission URL](https://github.com/wwmm/easyeffects) | 148 points | by [marcodiego](https://news.ycombinator.com/user?id=marcodiego) | [34 comments](https://news.ycombinator.com/item?id=36108927)

Easy Effects is a collection of audio effects for PipeWire applications that includes limiter, compressor, convolver, equalizer, and auto volume plugins. Previously known as PulseEffects, the application was renamed to Easy Effects after it started using GTK4 and native PipeWire filters. Users have full control over the order of effects with the up/down arrows next to the effect labels on the left side. Easy Effects requires a number of dependencies including plugins from Linux Studio and Calf Studio, libebur128, Zamaudio, zita-convolver, soundtouch, and RNNoise. Easy Effects can be installed via Flatpak or via package managers on Linux distributions. Donations are welcome to help with further development of the project.

Easy Effects, a collection of audio effects for PipeWire applications, has been released, replacing the previously known PulseEffects. Easy Effects can be installed via package managers on Linux distributions or by using Flatpak and consists of a limiter, compressor, convolver, equalizer, and auto volume plugins among other components. It requires several dependencies such as plugins from Calf Studio and Linux Studio, soundtouch, RNNoise, Zamaudio, libebur128, and zita-convolver. Users can control the order of effects with up/down arrows on the left side of the effect labels. While users have praised its effectiveness, some have criticized the UI design as not suitable for touchscreens and lacking respect for the display's space. Nonetheless, it has been recommended for projects like Noise Reduction, filtering, and software graphic equalizers. Other users discussed a variety of related topics, such as alternative effects libraries with GitHub links, headphone parameters, multi-device redirection, among others.

### ARM’s Cortex A53: Tiny but Important

#### [Submission URL](https://chipsandcheese.com/2023/05/28/arms-cortex-a53-tiny-but-important/) | 253 points | by [ingve](https://news.ycombinator.com/user?id=ingve) | [87 comments](https://news.ycombinator.com/item?id=36102406)

ARM's Cortex A53 is a low-power, dual issue, in-order CPU architecture that has been used in a multitude of devices, including set-top boxes and edge servers. Although not as powerful as its larger cousins, the A53 has the advantage of being able to handle tasks that don't require a lot of processing power while minimizing power and area. Compared to its contemporaries, the A53 has a tiny branch predictor with only 3072 entries, an eight-deep return stack, and a 256 entry indirect target array. While it may not be the most sophisticated CPU, the A53 has proven to be a reliable workhorse in numerous devices.

The submission discusses ARM's Cortex A53, a low-power, dual issue, in-order CPU architecture that has been used in various devices. The A53 has a tiny branch predictor with only 3072 entries, an eight-deep return stack, and a 256 entry indirect target array. However, some commenters noted that it is not a very sophisticated CPU and its performance varies depending on the workload. Some users shared their experiences with A53-based devices and how they perform in terms of speed and battery efficiency. There was also some discussion about the bigLITTLE configuration, which uses a combination of A53 cores for low-power tasks and more powerful cores for high-performance tasks.

### The halting problem is decidable on a set of asymptotic probability one (2006)

#### [Submission URL](https://projecteuclid.org/journals/notre-dame-journal-of-formal-logic/volume-47/issue-4/The-Halting-Problem-Is-Decidable-on-a-Set-of-Asymptotic/10.1305/ndjfl/1168352664.full) | 145 points | by [Schiphol](https://news.ycombinator.com/user?id=Schiphol) | [116 comments](https://news.ycombinator.com/item?id=36105717)

A new study by researchers at the University of Notre Dame, has found that the halting problem for Turing machines is decidable on a set of asymptotic probability one. The result is dependent on the particular computational models used and has significant implications for the future of computing and computational theory.

Researchers at the University of Notre Dame have discovered that the halting problem for Turing machines is decidable on a set of asymptotic probability one. Some commenters discussed their experiences solving the Brainfuck problem and creating non-halting programs. Others discussed the implications of the study, including the possibility of future developments in computing and computational theory. Some commenters discussed the lambda calculus and its role in describing mathematical functions and programs. There was also discussion of the definition of state space and its relationship to program checking and software verification.

### The AI Report #3: No-gradient optimization, open-source lags behind and more

#### [Submission URL](https://theaireport.substack.com/p/ai-report-3) | 16 points | by [primordialsoup](https://news.ycombinator.com/user?id=primordialsoup) | [4 comments](https://news.ycombinator.com/item?id=36108195)

Welcome to the third edition of The AI Report! In this issue, we highlight key trends in AI, including the rising popularity of no-gradient optimization methods, and the increasing use of long context lengths like Anthropic's 100k context length. We also discuss a recent paper that challenges the notion that open-source AI is closing in on proprietary tech. In our research section, we explore some fascinating papers, including Voyager, an AI-powered agent in Minecraft, and Med-PaLM 2, an advanced AI model that outperforms its predecessor in medical question answering.

The first comment thanks the user for sharing interesting news related to MLLLM, while the second comment expresses surprise at the large size of the open-source models found through crowd-sourcing. A user in response shares their thoughts about distributing crypto contributed to training models in a similar way. Another user mentions a project, Worldcoin, which has recently been gaining attention but is still struggling to find strong reasons.

### Retrowin32: Async, DLL loading, tracing execution, and Zig

#### [Submission URL](https://neugierig.org/software/blog/2023/05/retrowin32-async-dll-tracing-zig.html) | 78 points | by [goranmoomin](https://news.ycombinator.com/user?id=goranmoomin) | [12 comments](https://news.ycombinator.com/item?id=36104608)

The author of retrowin32, a Win32 emulator, has been working on loading external DLLs to play music in a demo. However, the DLL loading function uses a DllMain that needs to be invoked when the DLL is loaded for initialization purposes, which required some effort to figure out. To handle this, the author used Rust's async support to define EnumDisplayModes, which awaits a call to a callback and can potentially generalize well to cases where emulated code wants to synchronously perform an operation that ends up asynchronously in the web platform. To isolate a bug in the emulator that causes the demo to fail a self-check, the author wrote a Windows debugger program in Zig to introspect a debuggee's behavior. The author also explains the interesting parts of how a debugger works, which involves overwriting an address with an int3 instruction to stop execution at a certain point.

The discussion on this submission covers several topics related to Windows development and debugging. One user discusses their experience with disassembling native Windows machine code and finding undocumented parts of the architecture, while another suggests that many software programs don't use Windows system calls directly. A few users share their thoughts on DLL usage in Windows and some of the problems they've encountered, including the potential for proxy DLLs to interfere with the workings of an application. One user suggests that a proxy DLL located alongside the .exe file may work better than one located in the system directory. Overall, users seem to find the submission informative and share their own experiences and insights on the topic.

### Mirages: On Anthropomorphism in Dialogue Systems

#### [Submission URL](https://arxiv.org/abs/2305.09800) | 38 points | by [frabcus](https://news.ycombinator.com/user?id=frabcus) | [18 comments](https://news.ycombinator.com/item?id=36102082)

A new paper titled "Mirages: On Anthropomorphism in Dialogue Systems" warns about the potential harm of encouraging people to relate to automated dialogue systems as if they were human. The authors argue that conscious and unconscious design choices can guide users to personify these systems to varying degrees, which can lead to transparency and trust issues, as well as high-risk scenarios due to over-reliance on their outputs. They recommend that future dialogue system developers take particular care in their design, development, release, and description, and attend to the many linguistic cues that can elicit personification by users.

The paper "Mirages: On Anthropomorphism in Dialogue Systems" discusses the dangers of encouraging people to relate to automated dialogue systems as if they were human. The subconscious design choices of dialogue system developers can influence users to personify these systems, which can cause transparency and trust issues and lead to high-risk scenarios. Some individuals argue that the linguistic expressions used in dialogue systems do not necessarily imply anthropomorphism. Others believe that it is essential to develop dialogue systems that integrate natural language models to prevent anthropomorphization. There is a debate about whether confabulation and hallucination justify anthropomorphism, with some arguing that dialogue systems do not convey non-factuality or correctness in answers to the expected response. The discussion also touches on the benefits and harms of using linguistic factors that contribute to anthropomorphism and gender and cultural stereotypes.

### The First 'Apple Silicon' – The Aquarius Processor Project

#### [Submission URL](https://thechipletter.substack.com/p/the-first-apple-silicon-the-aquarius-7cb) | 65 points | by [tim_sw](https://news.ycombinator.com/user?id=tim_sw) | [11 comments](https://news.ycombinator.com/item?id=36104044)

In the early 1980s, almost every computer maker wanted their own processor design, and Apple was no exception. In 1986, Apple set out to design its own processor, known as "Project Aquarius," which had access to extensive resources thanks to CEO John Sculley's backing. The project even purchased Cray's supercomputer for simulation and design tasks, for which the machine became the first Cray installation in the world to run Unix. The Aquarius team went with a stack architecture design instead of a conventional cache and relied heavily on software and Cray's vector machine. However, the project faced multiple setbacks, and eventually, Apple abandoned the project and turned to ARM-based chips instead.

The article discusses how Apple attempted to design its own processor, known as "Project Aquarius" in 1986, but the project faced multiple setbacks, and eventually, Apple abandoned the project and turned to ARM-based chips instead. In the comments, users discussed the expensive hardware purchased for the project and how the failure of Project Aquarius was not uncommon for larger organizations. Additionally, it was noted that Cray's vector machine was used heavily, and the team went with a stack architecture design instead of a cache. Users also mentioned Seymour Cray's involvement in the project and discussed the Hobbit processor's relationship to ARM. Some users remarked that the failure of the project was due to politics within Apple.

### Stuxnet (2010) [pdf]

#### [Submission URL](https://www.wired.com/images_blogs/threatlevel/2010/11/w32_stuxnet_dossier.pdf) | 59 points | by [redbell](https://news.ycombinator.com/user?id=redbell) | [7 comments](https://news.ycombinator.com/item?id=36105263)

I'm sorry, but it looks like you uploaded a PDF file. Could you please provide the text of the top stories on Hacker News instead?

The submission, which requests a summary of the top stories on Hacker News instead of a PDF file, sparked a discussion. One user nostalgically recalls reading a technical article on Wired many years ago and finding it fascinating despite not fully understanding it. Others bring up topics related to cybersecurity, such as the Snowden revelations, Predator Android malware, Stuxnet, CosmicEnergy, Flame, Duqu, and DEADF007. Some users also mention their personal experience with integrating PCS7 feed processing plants in Western Europe and express a desire to learn more about Nitro Zeus. There are a few comments about whether the requested summary is necessary or not, with one user saying "true" to indicate they agree with the submission's request.

### Inner workings revealed for “Predator,” Android malware that exploited 5 0-days

#### [Submission URL](https://arstechnica.com/information-technology/2023/05/inner-workings-revealed-for-predator-the-android-malware-that-exploited-5-0-days/) | 108 points | by [Brajeshwar](https://news.ycombinator.com/user?id=Brajeshwar) | [46 comments](https://news.ycombinator.com/item?id=36101029)

Researchers from Cisco’s Talos security team have uncovered the sophisticated mobile spyware Predator, which can surreptitiously record voice calls, collect data from apps and hide or prevent them from running upon device reboots. Developed by digital security firm Cytrox, the malware predominantly targets Android and iOS devices and often comes bundled with five separate zero-day exploits. Predator works with an accompanying component known as Alien, which manages the low-level capabilities required to surveil its victims. Alien took hold of targeted devices by exploiting five vulnerabilities in Google Chrome and Android.

The submission summarizes the uncovering of mobile spyware called Predator by a Cisco security team. Researchers found that the malware can record voice calls, collect data from apps, and hide or prevent them from running. It predominantly targets Android and iOS devices, and often comes bundled with five separate zero-day exploits. The following discussion covers the insecurity of mobile phones, the importance of supporting vulnerable devices, and the potential ethics of commercial companies developing and selling spyware. Some users suggest using minimum mandatory support, while others recommend carrying laptops instead of phones. There are also discussions on the politics and morality surrounding the development and sale of spyware.

### Ford’s Deal to Use Tesla Charging Connector and Superchargers Could Kill CCS

#### [Submission URL](https://www.forbes.com/sites/bradtempleton/2023/05/28/fords-deal-to-use-tesla-charging-connector-and-superchargers-could-kill-ccs/) | 25 points | by [mhb](https://news.ycombinator.com/user?id=mhb) | [3 comments](https://news.ycombinator.com/item?id=36107327)

Ford recently announced that its electric vehicles will be able to use Tesla's supercharger network via an adapter, and future Ford EVs will have the Tesla connector built-in. This move may spell the end of the "standard" CCS connector used by non-Tesla EVs and could make the Tesla connector the new industry standard. With Ford as the #2 EV vendor in North America, it adds more weight to the argument for the Tesla connector being the common standard. Furthermore, experts consider Tesla's connector to be superior to the CCS and CHAdeMO connectors, with its small size and ability to handle both slow and fast charging up to 250kW.

The discussion on this submission mainly focused on the difference between the CCS and Tesla connectors. Some commenters pointed out that the article did not mention the European version of CCS, which has technical differences and requires three-phase charging, while others noted that the Tesla connector can handle both slow and fast charging up to 250kW and is considered superior. There was also a discussion about the different plug types used in North America and China, with some commenters mentioning that three-phase power is more common in Europe.

### Private spies hired by FBI and corporations infiltrate Discord, Reddit, WhatsApp

#### [Submission URL](https://www.leefang.com/p/private-spies-hired-by-the-fbi-and) | 216 points | by [walterbell](https://news.ycombinator.com/user?id=walterbell) | [83 comments](https://news.ycombinator.com/item?id=36100994)

Threat intelligence firms are using fake online personas to gather information from private corners of the internet, including Discord, Reddit, WhatsApp, and dark web message boards. These firms provide corporate and government clients with insight into potential threats such as political hacktivists and illegal markets trafficking stolen passwords and intellectual property. Many of these firms maintain close ties to law enforcement and government agencies, and some are under contract with the FBI or military intelligence. The recent disclosure of classified Pentagon documents on an invitation-only Discord chatroom is fueling a new push for increased surveillance of private online communities.

### Large language models do not recognize identifier swaps in Python

#### [Submission URL](https://arxiv.org/abs/2305.15507) | 73 points | by [lnyan](https://news.ycombinator.com/user?id=lnyan) | [98 comments](https://news.ycombinator.com/item?id=36101429)

A new paper titled "The Larger They Are, the Harder They Fail: Language Models do not Recognize Identifier Swaps in Python" explores the limits of large language models (LLMs) in understanding programming. The authors demonstrate that despite their impressive performance, LLMs lack a deep understanding of programming semantics, particularly invariances and equivariances like the renaming of identifiers. This shortcoming makes them unsuitable for tasks that statistically deviate from their training data, and even increases prediction errors with larger model sizes - an instance of the phenomenon known as Inverse Scaling.

The top submission on Hacker News is about the limitations of large language models (LLMs) in understanding programming semantics. Despite their impressive performance, LLMs lack a deep understanding of programming semantics and increase prediction errors with larger model sizes. The discussion in the comments covers a variety of topics, including the usefulness of LLMs for certain tasks, the limitations of training data, the reliability of LLMs, and the differences between human understanding and that of LLMs. Some commenters express disappointment in the level of credulity given to LLMs and suggest that the limitations of artificial intelligence should be more widely acknowledged.

