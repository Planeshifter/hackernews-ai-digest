## AI Submissions for Thu Aug 28 2025 {{ 'date': '2025-08-28T17:15:56.636Z' }}

### Some thoughts on LLMs and software development

#### [Submission URL](https://martinfowler.com/articles/202508-ai-thoughts.html) | 384 points | by [floverfelt](https://news.ycombinator.com/user?id=floverfelt) | [351 comments](https://news.ycombinator.com/item?id=45055641)

Martin Fowler on LLMs and Software Development (Aug 28, 2025)

The gist
- Stop treating “AI for coding” as one thing. Surveys that lump all usage together miss the difference between autocomplete (most common) and repo-aware code-edit/edit workflows (where power users see the biggest gains). Model choice matters too.
- No one knows the future of programming jobs. Ignore confident predictions; experiment, pay attention to concrete workflows, and share what works.
- “Is AI a bubble?” Of course—like canals, railroads, and the dot‑com era. It will pop; timing is unknown; some firms will survive and create lasting value.
- Hallucinations are the feature: LLMs generate plausible text; some of it’s useful. Treat answers as samples: ask multiple times, compare (even have the model compare), and never rely on LLM arithmetic when a deterministic calculation is available. It’s fine to have it generate code to compute, then test it.
- Software may be entering the non‑deterministic world other engineers live in. Expect variability; build tolerance and verification into processes.
- Reliability gap: An LLM will cheerfully report “all tests green” when they aren’t. You wouldn’t accept that from a junior—don’t accept it from a tool.
- Security reality check: Agents massively expand attack surface. Simon Willison’s “lethal trifecta” = private data access + untrusted content + exfiltration path. Prompt injection via web pages (even hidden text) is trivial. Agentic browser extensions may be fundamentally unsafe.
- Personal note: Fowler’s off for a few weeks and will catch up with industry friends at GOTO Copenhagen; he’s been involved since the JAOO days.

Why it matters
- Measure AI impact by workflow and model, not averages. Invest beyond autocomplete—repo-aware tools with strong review loops tend to deliver more.
- Normalize verification patterns: multi-ask, self‑critique, property‑based tests, and deterministic checkers. “Ask three times” for numeric outputs.
- Treat agents as untrusted: sandbox aggressively, deny default exfil paths, isolate credentials/tabs, and be wary of agentic browser automation.
- Prepare teams for probabilistic development: emphasize tests, reproducibility, and tolerance design over one‑shot “correctness.”

Notable lines
- “All an LLM does is produce hallucinations; we just find some of them useful.”
- “Anyone who says they know what the future will be is talking from an inappropriate orifice.”
- “Of course it’s a bubble.”

**Summary of Hacker News Discussion on Martin Fowler's Article:**

1. **Hallucinations as Core Feature**:  
   - Debate centers on whether *all* LLM outputs are hallucinations or only undesirable ones. A key analogy: LLMs are likened to the "infinite monkeys typing Shakespeare"—producing useful outputs stochastically, even if unintentionally.  
   - Critics argue that calling all outputs hallucinations dismisses their utility. Supporters counter that hallucination is intrinsic to LLM design (statistical pattern generation), requiring careful verification (e.g., "ask three times" for critical answers).

2. **LLM Understanding vs. Pattern Matching**:  
   - Skeptics reject claims that LLMs "understand" concepts, likening them to **stochastic parrots** (generating text via statistical patterns without comprehension). Proponents cite evidence of contextual reasoning, such as answering SAT-style questions correctly.  
   - Comparisons to human cognition emerge: humans use logic and sensory input to validate knowledge, while LLMs rely purely on training data patterns. Some note parallels to neuroscience research, where neural activations in models might mirror human brain processes.

3. **Practical Implications for Development**:  
   - **Testing & Reliability**: Highlighted concerns about LLMs falsely claiming "all tests pass" or generating insecure code. Users emphasize strict sandboxing, multi-step verification, and deterministic checks.  
   - **Cultural Nuances**: LLMs struggle with contextual language subtleties (e.g., formal vs. informal Spanish), mirroring human imperfections but magnifying risks in code/security contexts.

4. **Human vs. LLM Limitations**:  
   - Humans exhibit self-awareness of knowledge gaps, while LLMs lack inherent metacognition. One user shares an anecdote about learning Spanish: even fluent speakers make errors, suggesting LLMs could benefit from "humility" (admitting uncertainty).  
   - Philosophical debates reference the **Chinese Room Argument**, questioning whether syntactic manipulation (LLM token prediction) equates to semantic understanding.

5. **Terminology Critique**:  
   - Disputes over terms like *stochastic parrot* and *hallucination* reflect deeper divides. Some argue these labels oversimplify LLM capabilities; others defend them as accurate descriptors of non-deterministic systems.

**Key Takeaway**: The discussion underscores a tension between recognizing LLMs as transformative tools (despite flaws) and demanding rigorous safeguards to mitigate risks. Fowler’s call for probabilistic development aligns with community emphasis on verification, testing, and skepticism toward overconfident AI predictions.

### Building your own CLI coding agent with Pydantic-AI

#### [Submission URL](https://martinfowler.com/articles/build-own-coding-agent.html) | 183 points | by [vinhnx](https://news.ycombinator.com/user?id=vinhnx) | [33 comments](https://news.ycombinator.com/item?id=45055439)

Build your own CLI coding agent: Ben O’Mahony (Thoughtworks) shows how to assemble a project-aware, test-running code assistant using Pydantic-AI plus the Model Context Protocol (MCP). Unlike chatbots or autocomplete, this agent reads your repo, runs tests, consults docs, and edits files—tuned to your team’s conventions.

Why build, not buy
- Commercial tools are great but generic; a custom agent can encode your testing, docs, and file ops standards—and teaches you how these systems work.
- Vendor-neutral: swap models or run local; evaluate your own GenAI/dev tooling in the process.

Core architecture
- Model: Claude Sonnet 4 via AWS Bedrock
- Framework: Pydantic-AI for agent scaffolding and tools
- Extensibility: MCP servers expose pluggable capabilities over a standard protocol
- Interface: a simple CLI (agent.to_cli_sync()) to interact from the terminal

Capabilities the team wired up
- Test runner: a tool that invokes pytest so the agent can iterate against your suite automatically
- Instruction/intent handling: guide the agent to perform multi-step dev tasks
- MCP “toolbox”: plug in servers for
  - Sandboxed Python execution
  - Up-to-date library documentation lookup
  - AWS interactions
  - Internet search for current info
  - File system operations (with caution)
- Reasoning patterns: structured problem solving and prompts tuned for better planning and reliability

Safety and scope
- “Desktop Commander” power comes with risk—sandbox, restrict write access, and gate destructive ops.
- Timeouts, retries, and clear tool boundaries matter as you add more capabilities.

Takeaways
- CLI coding agents are a different class of tool: they operate your dev loop (read, test, change), not just chat.
- MCP makes capabilities modular; Pydantic-AI keeps tool definitions and state manageable.
- Good tests become the agent’s compass; your standards become the agent’s behavior.
- Start small (chat + tests), then add tools as your confidence grows.

Why it matters for developers
- Practical blueprint to roll your own repo-aware agent without locking into a single vendor
- Lets teams encode local context and workflows that commercial assistants often miss
- A hands-on path to understand, audit, and evolve agentic tooling inside your org

Here's a concise summary of the Hacker News discussion about the Pydantic-AI CLI coding agent submission:

### Key Discussion Themes
1. **User Experiences**  
   - Multiple developers shared positive experiences using Pydantic-AI for agent development, praising its flexibility, structured output handling, and integration with observability tools like Logfire.  
   - Some users reported initial challenges with LLM response reliability and dependency management but noted improvements in recent versions.  

2. **Technical Considerations**  
   - Debate about abstraction layers vs. direct LLM provider integration, with maintainers explaining Pydantic-AI's vendor-neutral design supports OpenAI/Claude/Gemini/Bedrock while acknowledging the challenge of keeping pace with rapid LLM market changes.  
   - Discussions about JSON schema enforcement, runtime inspection, and dynamic prompting strategies using Pydantic models.  

3. **Comparisons & Alternatives**  
   - Users contrasted Pydantic-AI with LiteLLM and custom implementations, with some preferring its structured approach despite initial complexity.  
   - Criticism about Python's native data class limitations sparked defense of Pydantic's role in modern Python ecosystems.  

4. **Maintainer Engagement**  
   - Pydantic maintainers actively addressed issues:  
     - Fixed recent release constraints  
     - Shared roadmap for improved JSON schema compliance  
     - Explained observability integration (Logfire)  

5. **Criticism & Challenges**  
   - Some users expressed frustration with API wrapper design and dependency coupling.  
   - Concerns about agent reliability surfaced, with one user reporting ~50% success rates for complex tasks.  

### Notable Insights  
- A recurring theme emphasized **structured outputs** as critical for coding agents, with Pydantic's schema enforcement seen as both a strength and occasional pain point.  
- Community interest emerged in **benchmarking approaches** (SWE-bench) and cost/performance tradeoffs between Claude/GPT-5.  
- Several users shared projects integrating Pydantic-AI with DBOS, AWS tooling, and custom CLI agents.  

The discussion reflects strong developer interest in customizable coding agents while highlighting the technical challenges of balancing flexibility, reliability, and vendor neutrality in LLM-powered tooling.

### AI adoption linked to 13% decline in jobs for young U.S. workers: study

#### [Submission URL](https://www.cnbc.com/2025/08/28/generative-ai-reshapes-us-job-market-stanford-study-shows-entry-level-young-workers.html) | 399 points | by [pseudolus](https://news.ycombinator.com/user?id=pseudolus) | [584 comments](https://news.ycombinator.com/item?id=45052423)

Stanford: Entry-level jobs in AI-exposed fields are shrinking

- Using millions of ADP payroll records, three Stanford researchers find a 13% relative decline since 2022 in employment for 22–25-year-olds in occupations most exposed to generative AI (e.g., customer support, accounting, software development).
- Older workers in those same fields have held steady or grown; less-exposed roles (like nursing aides) are up for all ages, with young health aides rising faster than older cohorts. Young front-line production/ops supervisors also grew, but slower than 35+.
- The authors argue AI is displacing “codified” book knowledge that juniors rely on, while experience-based knowledge is harder to substitute—raising the experience premium and squeezing the entry-level pipeline.
- They report controlling for education, remote work, offshoring, and macro shifts, suggesting AI exposure is a distinct factor—though the paper is not yet peer-reviewed and cannot prove causality. ADP coverage may not represent the full labor market.
- Not all AI adoption correlates with job loss: where AI complements tasks and boosts efficiency, employment changes have been muted.
- Context: Could help explain stagnant youth employment despite robust overall jobs. A Goldman Sachs economist recently flagged similar early signs, noting most firms haven’t fully deployed AI yet—implying impacts may intensify.

**Summary of Hacker News Discussion:**

1. **Skepticism Toward AI’s Practical Capabilities**:  
   - Many commenters expressed doubts about AI’s ability to handle precise, mission-critical tasks like accounting, tax calculations, or code generation. Examples included AI tools producing nonsensical outputs (e.g., random numbers in financial statements) and failing to parse structured data reliably.  
   - **Key Quote**: *“LLMs can’t do arithmetic… they’re glorified random number generators when tasked with critical code.”*  

2. **Study Methodology Criticisms**:  
   - Concerns were raised about the Stanford study’s reliance on ADP payroll data, which may not fully represent the broader labor market. Critics noted potential sampling biases and questioned whether the observed trends could be generalized.  

3. **Real-World AI Failures**:  
   - Anecdotes highlighted AI’s shortcomings in professional settings. For example, a tax attorney shared how AI integration in legal workflows led to serious errors, requiring expert intervention to correct. Others noted AI’s tendency to misinterpret Excel files or generate flawed code.  

4. **Offshoring vs. AI**:  
   - Some argued that job losses in fields like accounting and software development might stem more from offshoring (to India, Eastern Europe, etc.) than AI. Commenters shared mixed experiences with offshore teams, citing communication barriers, skill gaps, and management’s preference for cost savings over quality.  

5. **Human Expertise Still Critical**:  
   - Despite AI’s role in automating entry-level tasks, participants emphasized that domain expertise, judgment, and precision remain irreplaceable. For example, tax professionals stressed that AI tools often produce legally dangerous advice without human oversight.  

6. **Debate Over AI’s Future Impact**:  
   - While some feared worsening job displacement as AI adoption grows, others noted that current tools are still marginal in high-stakes fields. A recurring theme was that AI’s utility depends on whether it *complements* vs. *replaces* human roles.  

**Key Takeaway**:  
The discussion reflects skepticism about AI’s readiness to disrupt complex, knowledge-based professions. While entry-level roles may be at risk, participants underscored the enduring value of experience and the limitations of both AI and offshoring in replicating human judgment. The Stanford study’s findings were seen as preliminary, with calls for deeper scrutiny of labor market dynamics.

### Will AI Replace Human Thinking? The Case for Writing and Coding Manually

#### [Submission URL](https://www.ssp.sh/brain/will-ai-replace-humans/) | 154 points | by [articsputnik](https://news.ycombinator.com/user?id=articsputnik) | [128 comments](https://news.ycombinator.com/item?id=45052784)

Will AI replace human thinking? This essay argues for keeping your hands on the wheel: write and code manually for the parts that build judgment, and use AI sparingly where it truly helps.

Key ideas
- Use AI, but don’t outsource your craft. It’s fine for discovery, summaries, diagrams, and tightly scoped code; it’s a bad bet for core writing/coding where “writing is thinking” and skill is the point.
- Time-horizon heuristic (via ThePrimeagen): short-range autocomplete can boost productivity; long-range decisions (e.g., architecture) compound errors and become a net negative. Echoes Shape Up’s bias for near-term decisions.
- Stay in the driver’s seat. Constant suggestions (Grammarly, Copilot) erode attention and flow; turning them off can restore thinking and learning.
- Soulless outputs plateau. Models remix probabilities; without humans adding new insight, quality stagnates. That raises the premium on original, lived experience.
- Skill atrophy risk. Tools start as aids, become crutches, and end as hindrances. Expect a split between “writes” and “write-nots” (Paul Graham); those who keep practicing will stand out (Nathan Baugh).
- Practical line: rely on AI for areas you don’t plan to master (e.g., quick HTML/CSS tweaks, images). For your core domain, practice manually—it’s where the fun and growth are.

Why it matters
- Teams should treat AI as a short-range accelerator, not an architect.
- As AI content saturates, human clarity, taste, and original insight become differentiators.
- The long-term cost of convenience may be a generation that can’t think deeply—or write clearly.

Name-checks: DHH, ThePrimeagen, Shape Up, Forrest Brazeal, Thomas Ptacek, Paul Graham, Nathan Baugh, Ted Gioia.

**Summary of Hacker News Discussion: Balancing AI Tools like Claude Code with Human Judgment**

The discussion revolves around the trade-offs of using AI tools (e.g., Claude Code, Copilot) for programming, echoing the submission’s warning that over-reliance on AI risks eroding human skills and judgment. Here’s the synthesis of key points:

### **Advantages of AI Tools**
- **Productivity Boost**: Users report AI accelerates tasks like boilerplate code generation, simple refactoring (e.g., renaming functions), and documentation. For tightly scoped problems (e.g., HTML/CSS tweaks, basic API integrations), AI saves time.
- **Exploratory Aid**: Helps brainstorm solutions, surface alternative approaches, and navigate unfamiliar codebases. One user praises Claude Code for aiding "mental mapping" of projects.
- **Reducing Grunt Work**: IDE-based AI tools (e.g., IntelliJ refactoring, Copilot) streamline repetitive tasks, letting developers focus on higher-level logic.

### **Criticisms and Risks**
- **Eroding Understanding**: Over-reliance leads to "mental laziness." Developers may skip verifying AI-generated code, especially in complex conditionals or edge cases, risking bugs. As one user notes: *"You’re not interacting with code character by character… you miss critical details."*
- **Context Collapse**: AI struggles with long-range architectural decisions. One example highlights Claude Code failing to maintain context in multi-step tasks, leading to flawed designs (*"YOLO mode files... ignoring key requirements"*).
- **Quality Plateaus**: AI-generated code often lacks elegance or maintainability. It excels at "copy-paste coding" but falters in systems requiring holistic understanding (e.g., distributed backends, precise specifications).
- **Skill Atrophy**: Users fear losing deep coding proficiency. As AI handles more, developers risk becoming "prompt monkeys" rather than engineers (*"generated code feels soulless… humans add original insight"*).

### **Practical Balance**
- **Short-Term vs. Long-Term**: Use AI for autocomplete, boilerplate, or exploratory tasks but keep critical decisions (architecture, core logic) human-driven. A user advises: *"Plan first, use AI for diagrams and drafts, but edit manually."*
- **Verify Rigorously**: Treat AI output as a starting point. Code reviews remain essential, as AI can introduce subtle errors, especially in multi-file PRs.
- **Know When to Switch Off**: Some users disable AI tools (e.g., Grammarly, Copilot) during deep work to maintain flow and learning (*"turning them off restores focus"*).

### **Cultural Shifts**
- **The "Write-Nots"**: A divide emerges between developers who actively code and those delegating to AI. Those retaining hands-on practice (*"keeping their mental map"*) are seen as future-proofing their value.
- **Joy in Craft**: Several users defend manual coding’s satisfaction—debugging, refactoring, and delivering polished solutions (*"delighting customers with fast fixes… that’s FTW"*).

### **Conclusion**
While AI tools like Claude Code are powerful accelerators, the consensus aligns with the submission: **preserve human judgment for critical work**. AI’s role should resemble a "short-range copilot," not an architect. As codebases grow and systems complexify, developers risk costly errors if they outsource thinking. The premium shifts to those who code with intent, blending AI efficiency with human clarity and creativity.

### Web Bot Auth

#### [Submission URL](https://developers.cloudflare.com/bots/reference/bot-verification/web-bot-auth/) | 75 points | by [ananddtyagi](https://news.ycombinator.com/user?id=ananddtyagi) | [66 comments](https://news.ycombinator.com/item?id=45055452)

Cloudflare introduces “Web Bot Auth,” an open-standard way for bots to cryptographically prove their identity via signed HTTP messages—aimed at replacing fragile UA/IP heuristics and reducing spoofing. It builds on IETF work: bots publish public keys in a well-known directory and use HTTP Message Signatures to sign both that directory and subsequent requests.

How it works:
- Keys: Generate an Ed25519 keypair; publish the public key as a JWK (JWKS set supported). Cloudflare only uses Ed25519 (OKP with kty/crv/x).
- Directory: Host /.well-known/http-message-signatures-directory over HTTPS with content-type application/http-message-signatures-directory+json. Sign the directory response itself with Signature and Signature-Input headers. Include params like alg=ed25519, keyid as the JWK thumbprint, tag=http-message-signatures-directory, created, expires. This prevents mirroring/impersonation.
- Registration: In Cloudflare’s dashboard, Verified Bots → Request Signature; submit your directory URL (optionally user-agent patterns). Review time ~1 week.
- Usage: After approval, sign your bot’s HTTP requests per the draft; Cloudflare recommends including derived components like @authority, etc. Keys can be rotated; extra non-Ed25519 keys are ignored.

Why it matters:
- Cryptographic identity for “good bots” (search, archivers, agents) that’s portable and auditably verifiable.
- Moves the ecosystem toward standards (IETF drafts) instead of proprietary allowlists.
- Trade-offs: requires key management and adoption by crawlers and sites; today it’s Cloudflare-integrated, but the spec is open and could spread to other CDNs/origins.

Expect HN debate around openness vs. ecosystem lock-in, whether this curbs spoofing meaningfully, and how quickly major crawlers and sites will adopt a signed-bot norm.

The Hacker News discussion on Cloudflare's Web Bot Auth proposal highlights a mix of optimism, skepticism, and debate over centralization versus openness. Key points include:

1. **Open Standards vs. Centralization Concerns**:  
   - While many praise the move toward IETF-backed HTTP Message Signatures, critics argue the system risks *centralization* if Cloudflare controls bot registration and verification. Users like `plmfchn` worry that requiring bots to register via Cloudflare’s dashboard contradicts the "open-standard" ethos, potentially creating a gatekeeping role for the company.  

2. **Effectiveness Against Spoofing**:  
   - Supporters (e.g., `nmbl`) believe cryptographic proofs reduce spoofing by moving away from UA/IP heuristics. However, skeptics like `account42` question how impersonation would be prevented, leading to discussions about private keys as the root of trust. `skzyby` clarifies that possession of a private key ensures identity, but others counter that this simply shifts the problem to key security.  

3. **Adoption Challenges**:  
   - Many note that success hinges on widespread adoption by bots and websites. `marginalia_nu` doubts incentives exist for bots to comply, while `nrdsnpr` raises economic barriers: smaller entities unable to afford API access might face exclusion, deepening inequalities in web access.  

4. **Comparisons to Existing Frameworks**:  
   - Comparisons to **ActivityPub** (decentralized identity in federated networks) and proposals like **PEAC** (transparent, receipt-based policies) suggest alternatives for decentralized bot accountability without relying on centralized registries.  

5. **Cloudflare’s Influence and "Verified Bots"**:  
   - Critics (`mips_avatar`, `zb3`) view Cloudflare’s growing role as a de facto internet regulator, likening Verified Bots to a "shady checkpoint" that consolidates power. Concerns about discriminatory filtering arise, with `1gn15` warning this could lead to a DRM-like web where non-mainstream tools (e.g., Tor, niche browsers) suffer.  

6. **Ethical and Practical Trade-offs**:  
   - Debates emerge over balancing bot accountability with censorship risks. While `thrtfrn` defends bot-blocking as a business necessity, others argue it stifles open access. `jmmyd` and `jthnrj** advocate for decentralized, policy-driven approaches to avoid centralized gatekeeping.  

**Conclusion**: The discussion reveals cautious optimism about cryptographic bot identity but highlights unresolved tensions between standardization and corporate control, economic accessibility, and the risks of centralizing trust in a single entity like Cloudflare. Adoption by major players and equitable access remain critical challenges.

### Are OpenAI and Anthropic losing money on inference?

#### [Submission URL](https://martinalderson.com/posts/are-openai-and-anthropic-really-losing-money-on-inference/) | 495 points | by [martinald](https://news.ycombinator.com/user?id=martinald) | [463 comments](https://news.ycombinator.com/item?id=45050415)

HN Top Story: “Napkin math” says inference isn’t a cash bonfire—output is expensive, input is basically free

What’s new
- A deep, first-principles back-of-the-envelope analysis argues that large-scale inference is far cheaper than the “cash incinerator” narrative—so long as you keep outputs small and batching high. The punchline: input tokens are almost free; output tokens drive the bill.

Key setup and assumptions
- Hardware: 72× H100s at $2/hour each → $144/hour cluster.
- Model: DeepSeek R1-style MoE (671B total, 37B active params ≈ 74GB FP16).
- Parallelism: 8 GPUs per model instance → 9 instances across the cluster.
- Batch: 32 sequences, avg 1,000 tokens.
- Throughput: Memory-bandwidth-bound prefill (~3.35 TB/s per H100) yields ~45 forward passes/s per instance.

Throughput math (memory-bound regime)
- Prefill (inputs): Each pass processes all tokens in the batch.
  - Per instance: 45 passes/s × 32k tokens ≈ 1.44M tokens/s.
  - Cluster: ~13M tokens/s ≈ 46.8B tokens/hour.
- Decode (outputs): One token per sequence per pass.
  - Per instance: 45 × 32 ≈ 1,440 tokens/s.
  - Cluster: ~12,960 tokens/s ≈ 46.7M tokens/hour.

Cost per token (at $144/hour)
- Inputs: ~$144 / 46.8B ≈ $0.003 per million tokens (effectively “free”).
- Outputs: ~$144 / 46.7M ≈ $3.08 per million tokens (the real cost driver).
- MoE routing diversity can dent prefill throughput, but likely only by ~30–50% (not 2–3×) in practice.

When costs spike
- Long contexts (≈128k+): attention turns compute-bound, raising costs 2–10×.
- Explains premium pricing beyond ~200k contexts and why some products cap context (e.g., Claude Code) to stay memory-bound and cheap.

Pricing implications
- Consumer ChatGPT Pro ($20/month, ~100k tokens/day, 70% input/30% output):
  - Estimated raw compute cost ≈ $3/month → 5–6× markup.
- Developer coding plans (heavy input, light output) look wildly profitable:
  - Claude Code Max 5 ($100/mo, ~2M input + 30k output/day): ≈ $4.92/month → ~20× markup.
  - Claude Code Max 10 ($200/mo, ~10M input + 100k output/day): ≈ $16.89/month → ~12× markup.

Why it matters
- If these ballpark numbers hold, today’s popular “AI inference is unsustainable” takes are overstated—especially for coding/agent workloads that shovel in lots of context and produce relatively few tokens.
- The real knob is output length, not input. Business models that price input heavily may be pure margin.
- Caveats: This isolates raw compute; excludes networking, orchestration, memory/KV cache footprint, power, staff, overhead, and capex. Also assumes favorable batching and MoE behavior. But even at retail-ish H100 rates, economics look better than the doom narrative.

**Summary of Discussion:**

The discussion critiques the original analysis' assumptions and explores broader implications for AI costs and business models:

1. **Technical Flaws in Original Analysis**  
   - Commenters argue the submission's math is flawed, particularly around prefill phase assumptions. Claims that input tokens are "free" ignore compute-bound attention costs in long contexts (>128k tokens).  
   - Example: A user cites real-world data (LMSYS) showing output tokens at ~$0.02/million—far cheaper than the article’s $3.08 estimate—suggesting overlooked optimizations like batching or newer hardware (e.g., NVIDIA B200 GPUs).

2. **Profitability of AI Companies**  
   - Debate over whether firms like OpenAI/Anthropic lose money on inference. Some argue closed-source models command high margins (e.g., 5–20x markup on coding plans), while others note infrastructure overhead and R&D costs erode profits.  
   - Mention of Sam Altman’s mixed signals on OpenAI’s profitability, with recent claims of nearing breakeven versus earlier reports of losses.

3. **Training vs. Inference Costs**  
   - Many highlight training (not inference) as the dominant expense, especially for frontier models. Speculation arises about diminishing returns: "Throwing $100B at GPT-5 might only yield slight improvements."  
   - Concerns that training-cost inflation could outpace performance gains, risking unsustainable economics.

4. **Pricing and Commoditization**  
   - Closed-source APIs seen as overpriced but vulnerable to commoditization as open-source models catch up. Comparisons to Uber’s trajectory: early high margins eroded by competition.  
   - Skepticism about long-term viability of current markups, with one user noting: "If an API call costs $0.25 but is priced at $1, consultants will undercut it."

5. **Model Limitations and User Frustrations**  
   - Users report diminishing returns in model quality (e.g., GPT-5 perceived as incremental over GPT-4) and frustration with hallucination/context retention issues. Critiques of training data contamination and rapid obsolescence (e.g., Google’s Gemini API changes breaking code).  

6. **Future Outlook**  
   - Predictions of a "wobbly" equilibrium where training costs balloon but inference becomes cheaper, favoring providers with scale. Others foresee a shakeout if performance plateaus before ROI is achieved.  

**Key Takeaway**: While the submission argues inference is affordable with optimized outputs, the discussion underscores unresolved debates about cost structures, pricing sustainability, and whether AI’s rapid progress can offset rising expenses. Technical critiques and real-world pricing examples challenge the original assumptions, highlighting gaps in the "cash bonfire" narrative.

### Claude Code Checkpoints

#### [Submission URL](https://claude-checkpoints.com/) | 172 points | by [punnerud](https://news.ycombinator.com/user?id=punnerud) | [119 comments](https://news.ycombinator.com/item?id=45050090)

HN Top Story: “Checkpoints” adds automatic version control for Claude Code projects (169 points)

What it is: A free macOS/Windows app that watches your project folder and creates full-file “checkpoints” automatically—especially when Claude Desktop tasks complete—so you can diff and roll back any time.

Why it matters: AI coding sessions can make sweeping changes fast. This gives you a zero-setup safety net and timeline without juggling Git branches or remembering to commit.

Highlights
- Automatic change tracking: Monitors your whole project; no config—just select a folder.
- One‑click checkpoints: Create snapshots before risky edits; every checkpoint captures all files.
- Visual diffs: Built-in viewer shows additions, modifications, deletions between checkpoints.
- Time travel: Restore any previous checkpoint instantly.
- Deep Claude integration: Uses MCP (Model Context Protocol) to auto-checkpoint on task completion; Claude can list/restore checkpoints via MCP commands. MCP server autostarts on port 8765.
- Task tracking: Start/complete events are logged, triggering checkpoints.
- Free to use; macOS 13.5+ and Windows; Mac App Store available.

Notes/constraints
- Appears aimed at complementing (not replacing) Git for local, AI-driven edits.
- Full-project backups each checkpoint could impact disk usage on large repos.
- No Linux build mentioned.

**Summary of Discussion:**

The discussion revolves around version control practices for AI-assisted coding, particularly with Claude Code. Key points include:

1. **Git vs. Jujutsu (jj):**  
   - Many users advocate for **Jujutsu (jj)**, a Git-compatible VCS, over traditional Git for AI workflows. It simplifies snapshot management, enables instant reverts, and handles messy experimental code more gracefully. Critics argue Git’s branching and manual commit process is cumbersome for rapid AI iterations.  
   - Others defend Git for structured workflows (e.g., branching, squashing commits) but acknowledge its friction with AI’s "trial-and-error" nature. Tools like **Aider** and **Cursor** are noted for integrating AI with Git workflows.

2. **AI-Specific Challenges:**  
   - Users highlight issues with AI-generated code, such as temporary files, inconsistent naming, and context pollution. Solutions include isolating AI changes in branches or using lightweight tools like Checkpoints/Jujutsu for automatic snapshots.  
   - Some stress the importance of manual code reviews and clean commit histories to avoid clutter from AI experiments.

3. **Checkpoints App Feedback:**  
   - The app’s automatic versioning and MCP integration are praised for providing a safety net without Git overhead. Concerns about disk usage for large projects are noted but deemed acceptable for local AI work.

4. **Tool Integration:**  
   - Users mention alternatives like **Gemini CLI** (Google) and **Cursor** for context-aware version control. Requests for built-in Claude features (e.g., conversation history rollback without code reverts) emerge.  
   - CLI tools are favored by some for scripting flexibility, while others appreciate Checkpoints’ GUI for visual diffs.

5. **Workflow Strategies:**  
   - Common practices: branching for experiments, squashing commits post-review, and using `git rebase` to clean history. Jujutsu users emphasize its automatic snapshotting and simplified undo/redo flow.  

**Takeaway:** The community seeks tools balancing simplicity (automatic snapshots, minimal config) with control (clean history, context management). Jujutsu and Checkpoints address AI-specific needs, while Git remains dominant for collaborative projects requiring structured workflows.

### An eyecare foundation model for clinical assistance

#### [Submission URL](https://www.nature.com/articles/s41591-025-03900-7) | 21 points | by [jameslk](https://news.ycombinator.com/user?id=jameslk) | [3 comments](https://news.ycombinator.com/item?id=45057513)

EyeFM: a multimodal vision–language “copilot” for eyecare gets real-world, RCT evidence

What’s new
- Researchers built EyeFM, a foundation model–powered assistant trained on 14.5 million ocular images across five imaging modalities paired with clinical text from global, multiethnic datasets.
- Unlike many benchtop AI reports, they ran broad evaluations: retrospective validations, multicountry reader studies with 44 ophthalmologists, a multicenter real‑world deployment, and a double‑masked randomized controlled trial (RCT).

Key RCT results (China; 668 high‑risk patients; 16 ophthalmologists randomized)
- Accuracy: 92.2% with EyeFM vs 75.4% standard care (P < 0.001).
- Appropriate referral rate: 92.2% vs 80.5% (P < 0.001).
- Report quality: higher standardization scores with EyeFM (P < 0.001).
- Patient behavior: better adherence to self‑management (70.1% vs 49.1%) and referral follow‑through (33.7% vs 20.2%); satisfaction similar between groups.
- Post‑deployment user acceptance was strong.

Why it matters
- Provides rare, prospective clinical evidence that a vision–language copilot can lift clinician performance and improve patient adherence, not just offline AUCs.
- Trained across multiple imaging modalities and populations, aiming for generalizability.

Caveats
- The RCT was single‑center, high‑risk, and male‑skewed (79.5%), which may limit generalizability.
- Higher referral rates, while paired with higher diagnostic accuracy, could strain downstream services; longer‑term clinical outcomes weren’t reported.
- Regulatory status not discussed.

Resources
- Code: https://github.com/eyefm/EyeFM
- Minimal dataset (non‑commercial research): https://zenodo.org/records/15546254
- Trial registration: ChiCTR2500095518

Bottom line: A well‑designed copilot, validated up to an RCT, meaningfully boosted ophthalmologists’ screening accuracy and documentation and nudged patients toward better follow‑up—an encouraging step for clinically deployed foundation models.

**Summary of Discussion:**  
The discussion reflects mixed reactions to the EyeFM study:  

1. **Skepticism & Critique (Daub):**  
   - A user critiques the study's presentation, questioning potential biases, practical relevance, and real-world applicability. They suggest the model might oversimplify clinical workflows ("prosthetic" patch analogy) and highlight concerns about entrenched medical habits hindering adoption. Phrases like "vision impossible" and "entrenched professionals" imply skepticism about disrupting existing practices.  

2. **Interest in Potential (e40):**  
   - Another user finds the technology’s long-term potential ("ptcn" interpreted as "potential") remarkable but requests further elaboration on the critique.  

3. **Optimism (dust42):**  
   - A brief comment acknowledges the model as part of a viable trend in AI-driven healthcare.  

**Key Themes:**  
- Debate over clinical practicality vs. theoretical promise.  
- Concerns about integration into existing medical systems and professional resistance.  
- Recognition of the model’s innovation but calls for deeper scrutiny and real-world validation beyond the RCT.  

**Note:** The discussion’s shorthand and typos (e.g., "ptcn," "vlbl") made interpretation challenging, but the overarching sentiments revolve around cautious optimism tempered by skepticism about implementation hurdles.

### Show HN: SwiftAI – open-source library to easily build LLM features on iOS/macOS

#### [Submission URL](https://github.com/mi12labs/SwiftAI) | 68 points | by [mi12-root](https://news.ycombinator.com/user?id=mi12-root) | [19 comments](https://news.ycombinator.com/item?id=45052200)

SwiftAI: Type-safe Swift library for LLM apps on iOS and macOS

What it is
- A modern Swift library that unifies on-device Apple models with cloud providers (OpenAI, Anthropic, and custom backends) under one API.
- Emphasizes strongly typed, structured outputs and first-class tool/function calling, built with async/await.

Why it matters
- Lets iOS/macOS developers ship private, fast, on-device AI features with seamless cloud fallback when needed.
- Type-safe structured outputs reduce brittle JSON parsing and runtime schema mismatches.
- Built-in tool use and chat sessions make agent-style features straightforward on Apple platforms.

Highlights
- Model-agnostic API: swap SystemLLM (on-device) and OpenAI/others without changing call sites; fallback via isAvailable.
- Structured outputs: annotate Swift types with @Generable and get compile-time validated responses mapped directly into your structs.
- Tool protocol: define tools with typed Arguments; the model decides when to call them and incorporates results.
- Conversations: Chat maintains context across turns with optional tools.
- Extensible: plugin architecture for custom models and tools.
- Swift-native: async/await and modern concurrency.

Getting started
- Swift Package Manager: https://github.com/mi12labs/SwiftAI
- Quick calls: llm.reply(to: "…")
- Typed responses: llm.reply(returning: YourStruct.self)
- Tools: conform to Tool with an Arguments struct and async call()
- Conversations: Chat(with: llm, tools: [...]).send("…")

License and status
- MIT license
- At time of posting: 123 stars, 5 forks

Notes and open questions
- README highlights Apple on-device integration via SystemLLM; exact OS/model requirements and streaming/benchmark details aren’t specified.
- No performance metrics or provider coverage matrix in the README yet; docs and examples are included in the repo.

**Summary of Discussion:**

1. **Trademark Concerns**  
   - Users raised potential trademark issues with "SwiftAI" given Apple's ownership of "Swift" and "SwiftUI" trademarks. Comparisons were made to IBM's trademark enforcement around "Watson" AI.  
   - Speculation arose about Apple potentially prioritizing "Apple Intelligence" branding for its own AI features.  

2. **Performance & Integration Questions**  
   - Praise for the library’s design, though users noted missing performance benchmarks.  
   - Maintainers shared early internal testing results: Apple’s on-device models perform well for summarization (~20 words) and structured data extraction but struggle with complex STEM topics or lengthy instructions.  

3. **On-Device Model Limitations**  
   - Concerns about Apple’s SystemLLM output quality were addressed: token limits (~500 tokens ≈ 1,500–2,000 characters) constrain response length. Developers advised optimizing prompts for brevity.  

4. **CoreML & Hugging Face Integration**  
   - A feature request emerged for direct CoreML model support (e.g., Llama, Gemma) to avoid dependency on `llm.cpp`. Maintainers acknowledged plans to add popular on-device LLM support, including Hugging Face models.  

5. **Custom Integrations & Roadmap**  
   - Users requested clearer documentation for custom LLM integrations. Maintainers confirmed upcoming support for Claude, Gemini, and streaming features, prioritizing API stability first.  

6. **AI-Generated Code Caution**  
   - Discussion highlighted the importance of rigorous code review for AI-generated tools, with maintainers emphasizing human oversight in SwiftAI’s design.  

**Maintainer Responses:**  
- Actively addressing performance metrics and model compatibility documentation.  
- Open to community feedback for future integrations and use cases.

### Important machine learning equations

#### [Submission URL](https://chizkidd.github.io//2025/05/30/machine-learning-key-math-eqns/) | 292 points | by [sebg](https://news.ycombinator.com/user?id=sebg) | [33 comments](https://news.ycombinator.com/item?id=45050931)

HN Digest: A hands-on cheat sheet for the core math behind ML

What it is
- A concise, code-backed tour of the equations that power machine learning, aimed at readers with basic math/programming.
- Each concept includes a short intuition, the key formula, and a small Python example (NumPy, scikit-learn, TensorFlow/PyTorch).

What it covers
- Probability and information theory: Bayes’ theorem, entropy, joint/conditional probability, KL divergence, cross-entropy.
- Linear algebra: linear transformations, eigenvalues/eigenvectors, SVD.
- Optimization and training: gradient descent, backpropagation.
- Losses: MSE, cross-entropy.
- Modern building blocks: convolutions, softmax, attention; plus a nod to diffusion processes.

Why it’s useful
- Bridges intuition, equations, and runnable code in one place—handy as a refresher, interview prep, or teaching aid.
- Shows how theoretical quantities map directly to common ML APIs and patterns.

Notes and caveats for practitioners
- Entropy/cross-entropy examples imply natural logs (nats); be consistent about log bases.
- KL divergence requires Q(x) > 0 wherever P(x) > 0; watch for numerical stability (add eps).
- Backprop/attention sections are high-level; not a substitute for a deep dive, but a solid starting map.

Who should read
- Beginners solidifying fundamentals and practitioners wanting a compact, executable reference to the math they use daily.

The Hacker News discussion on the "Core Math Behind ML" cheat sheet highlights several key points:

### **Positive Reception & Foundational Appreciation**
- Users commend the resource for bridging theory and code, calling it a practical reference for ML fundamentals. Information theory concepts (entropy, KL divergence) are noted as essential, with nods to Shannon's work being timeless and accessible.

### **Technical Corrections & Clarifications**
- **Entropy Implementation**: A user notes the code example uses natural logs (nats) but suggests clarifying log bases (e.g., base 2 for bits) to avoid confusion. Numerical stability (e.g., adding ε to probabilities) is emphasized.
- **Backpropagation Debate**: A subthread clarifies that backpropagation is a gradient computation method, not a standalone optimization algorithm. Some confusion arises around its association with supervised learning, but participants stress it’s a tool for any gradient-based optimization.

### **Critiques of Depth & Authenticity**
- **Surface-Level Concerns**: While praised for brevity, some argue the guide is "bombastic" and lacks depth (e.g., omitting multi-layer perceptrons, kernel methods, or detailed diffusion processes). Critics claim it covers "95% of keywords" but not deeper understanding.
- **AI-Generated Suspicion**: The writing style (e.g., phrases like "comprehensive resource" and "theoretical explanations") raises suspicions that the post is LLM-generated. Users highlight unnatural phrasing and "keyword stuffing" as red flags.

### **Omissions & Suggestions**
- Key ML concepts like forward/backward diffusion processes and kernel methods are noted as missing. Users suggest expanding on structured data handling and incremental learning.

### **Niche Praises**
- MSE is defended as a robust loss function despite alternatives, and singular value decomposition (SVD) is highlighted as underappreciated in modern LLM contexts.

### **Overall Sentiment**
- **For Learners**: A solid starting point for beginners or those needing a refresher, but not a substitute for deeper study.
- **For Practitioners**: Handy as a quick reference but criticized for oversimplification and potential AI authorship doubts.

The discussion balances appreciation for the guide’s utility with skepticism about its depth and originality, underscoring the community's demand for both accessibility and rigor in educational resources.

### The Math Behind GANs (2020)

#### [Submission URL](https://jaketae.github.io/study/gan-math/) | 138 points | by [sebg](https://news.ycombinator.com/user?id=sebg) | [31 comments](https://news.ycombinator.com/item?id=45050958)

Deep dive: the math behind GAN loss functions

What it covers
- Starts from Goodfellow et al. (2014) to derive the GAN objective from first principles.
- Defines clear notation for generator G and discriminator D, then motivates each model’s loss.
- Shows how binary cross-entropy naturally yields the discriminator loss over real vs. fake and the generator’s objective to make D(G(z)) look “real.”
- Reconciles the textbook minimax objective with the losses used in practice: the “non‑saturating” generator loss (minimizing −log D(G(z))) provides stronger gradients than minimizing log(1 − D(G(z))) when D is confident.
- Sets up the optimization/training discussion (discriminator vs. generator updates), bridging intuition and math.

Why it matters
- GAN stability and performance hinge on the exact choice and sign of the losses; this walkthrough demystifies why practitioners deviate from the pure minimax form.
- Helpful for readers who know GANs conceptually but want the math that justifies the common training recipes.

**Summary of Hacker News Discussion on GAN Loss Functions and Academic Math Practices**  

### Key Themes:  
1. **Debate Over Mathematical Complexity in Academic Papers**  
   - Many commenters criticized the use of dense mathematical notation in ML papers, arguing it often obscures intuition and acts as a barrier to understanding. For example:  
     - "rstd" compared heavy math to "intelligence signaling," suggesting frameworks like PyTorch simplify concepts that papers overcomplicate.  
     - "gdlsk" likened math and programming to "terrible languages" for communication, advocating for clearer explanations of how models *actually work* in practice.  
   - Counterpoints acknowledged math's precision but emphasized balancing formalism with accessibility. "Garlef" noted that while math can be intimidating, concise notation (e.g., Lagrangian mechanics) is often necessary for rigor.  

2. **Challenges for Learners and Practitioners**  
   - Participants highlighted the frustration of deciphering papers laden with "single-character variables" and excessive formalism.  
   - "sttclf" lamented how academic papers prioritize impressing peers over clear communication, creating a "steep learning curve" for newcomers.  
   - "gdlsk" shared personal struggles, advising perseverance and avoiding comparisons to "imposter syndrome" in research.  

3. **Relevance of GANs in Modern ML**  
   - Some questioned whether GANs are outdated ("clsntg" called them "ancient"), noting advancements in VAEs, diffusion models (e.g., Stable Diffusion), and single-step generators.  
   - Others defended GANs' niche utility, such as training efficiency for real-time applications ("plch") and specific use cases like text-to-speech ("lukeinator42").  

4. **Broader Critique of Academic Culture**  
   - "MattPalmer1086" humorously accused academia of hiding simple algorithms behind complex math to appear novel.  
   - "aDyslecticCrow" critiqued GANs' instability and mode collapse, arguing diffusion models now dominate generative tasks.  

### Takeaways:  
- The discussion reflects tension between **rigor** and **accessibility** in ML research, with many advocating for clearer communication without sacrificing technical depth.  
- GANs, while historically significant, face skepticism in light of newer methods, but their foundational concepts (e.g., adversarial training) remain influential.  
- The thread underscores the importance of **practical intuition** alongside mathematical formalism, especially for practitioners implementing models.  

In essence: *Clear explanations and modern alternatives are valued, but foundational math remains a necessary evil in cutting-edge research.*

### Show HN: Yoink AI – macOS AI app that edits directly in any textfield of any app

#### [Submission URL](https://www.useyoink.ai) | 21 points | by [byintes](https://news.ycombinator.com/user?id=byintes) | [8 comments](https://news.ycombinator.com/item?id=45052421)

Yoink AI: an in-app writing copilot that works across your existing tools

What it is: A Mac app that lets you summon AI (Cmd+Shift+Y) directly inside whatever you’re using—Google Docs, Notion, Word, email, etc.—so you can generate, rewrite, and polish text in place without bouncing to a chatbot.

What’s interesting
- Automatic context capture: Reads the active document and referenced materials to tailor output, and can compare with past projects to stay consistent.
- Works universally: Presents as a system-level assistant that types into the app you’re already in; aims to stop tab-hopping and copy/paste.
- Reusable “voices”: Create personalized styles (e.g., business, concise, formal) and apply them to any draft.
- Common actions: Improve proposals, update sprint plans, rewrite in a friendlier tone, generate intros, and draft emails in your style.
- Availability: Free to start. Mac app now; Windows waitlist.

What to watch
- Privacy and security details around “automatic capture” of on-screen or document content aren’t specified here; enterprise controls and model choices will matter for adoption.
- How well “universal” support holds up in complex editors, canvas apps, or offline scenarios.

**Summary of Discussion:**  
The Hacker News discussion about Yoink AI highlights several key points and reactions:  

- **Technical Implementation**:  
  - A user noted macOS accessibility/permissions requirements and referenced **[plock](https://github.com/jasonjmcghee/plock)**, a similar project that simulates text selection and clipboard actions to capture context for large language models (LLMs). This sparked interest in how Yoink handles app integration and data access.  

- **Pricing Debate**:  
  - Critics questioned the $20/month subscription model, comparing it to ChatGPT Plus. Some argued users may resist another paid AI tool, suggesting a lifetime license or lower cost would be preferable (“*Wouldn't pay $5/month*”).  

- **UI/UX Praise**:  
  - A comment praised the website’s clean design (“*clean website*”).  

- **Brand Recognition**:  
  - Users recognized Yoink’s reputation as a utility for macOS (e.g., drag-and-drop functionality) and expressed curiosity about its expansion into AI tools.  

- **Founder Engagement**:  
  - The founder responded to feedback, inviting further critique and expressing gratitude for community input.  

**Key Takeaways**:  
The discussion reflects interest in Yoink's technical approach and usability but skepticism about its pricing model. Privacy/security details and seamless cross-app functionality remain points of scrutiny as the tool evolves.

### Show HN: Grammit – Local-only AI grammar checker (Chrome extension)

#### [Submission URL](https://chromewebstore.google.com/detail/grammit-the-ai-grammar-ch/pkfmoknmnkbidlniedaloiijibdpjjmm) | 28 points | by [scottfr](https://news.ycombinator.com/user?id=scottfr) | [4 comments](https://news.ycombinator.com/item?id=45053553)

Grammit: a privacy-first grammar checker that runs entirely on your machine. It’s a Chrome extension that uses a local LLM to fix spelling/grammar, rephrase text, and even correct factual slips (e.g., swapping “Charles Dickens” to “Charles Darwin”). The pitch is “Grammarly without the data exhaust”: no text sent to external servers, works across websites, and it’s free.

What’s notable:
- On-device processing is the headline differentiator vs. Grammarly/LanguageTool/QuillBot.
- Goes beyond grammar into rewriting, drafting, and brainstorming.
- Claims to correct factual errors locally—raises interesting questions about model accuracy and hallucination handling without server-side checks.
- Built by Blaze Today (makers of Text Blaze), which may reassure some on trust and support.
- Early days: ~73 users, no ratings yet. Extension is ~422 KiB, so the model is likely fetched post-install—HN will probably poke at how “never sends your writing to servers” is enforced and whether any telemetry exists.

Expect discussion around performance on typical laptops, quality vs. cloud tools, and the trade-offs of local LLMs in the browser.

The Hacker News discussion around Grammit centers on technical implementation and compatibility, with two main threads:

1. **Model Architecture Debate**  
   - A user (wllwd) mentions developing fine-tuned T5 models for grammar correction.  
   - The developer (scttfr) clarifies that Grammit uses **prompt engineering** with Gemma 3B (a local LLM) instead of custom fine-tuning. The tool relies on a "prefilled conversation" of example corrections to guide the model, avoiding server-side training data.

2. **Browser Compatibility**  
   - A Vivaldi user (doug_life) asks about support for non-Chrome browsers.  
   - The developer notes that Grammit depends on Chrome/Edge’s experimental **Prompt API** for local LLM execution, which other browsers like Vivaldi may not yet support. Links to Chrome/Edge API documentation are provided.

**Key Takeaways**  
- Technical focus on how Grammit avoids cloud dependencies (local prompting vs. fine-tuning, Chrome’s Prompt API).  
- Early users probe edge cases (browser support) while the developer emphasizes privacy-first design.  
- No direct discussion yet on performance or hallucination risks, but the exchange hints at skepticism about on-device LLM capabilities compared to cloud tools.  

The thread reflects cautious interest in the architecture, with practical concerns about browser limitations and trust in local processing claims.

### Show HN: A private, flat monthly subscription for open-source LLMs

#### [Submission URL](https://synthetic.new/newsletter/entries/subscriptions) | 23 points | by [reissbaker](https://news.ycombinator.com/user?id=reissbaker) | [15 comments](https://news.ycombinator.com/item?id=45055763)

Synthetic launches flat-rate access to open‑source coding LLMs. For $20/month you get 125 requests per 5 hours (they say ~3x Claude’s $20 tier limits); $60/month bumps that to 1250 per 5 hours—positioned as more headroom than Claude’s $200 “Max” at under a third the price. The sub covers both UI and OpenAI‑compatible API, with popular models like GLM‑4.5, Kimi K2, Qwen3 Coder 480B, and DeepSeek 3.1, and works out of the box with agents/tools such as Aider, Cline, Roo, KiloCode, Octofriend, OpenWebUI, and SillyTavern. They emphasize reliability over aggregator backends (citing Aider tests showing a ~10‑point gap on Qwen3 Coder when round‑robinning via OpenRouter) and promise not to train on user data, deleting prompts/completions within 14 days. Their per‑minute on‑demand GPU hosting remains separate (they claim 2x cheaper than some rivals on 80GB GPUs), with plans to eventually include on‑demand minutes in subscriptions; other roadmap items include shareable chats, dark mode, better model discovery, and continued UI polish plus a new token calculator.

**Summary of Discussion:**

1. **Privacy Concerns:** A user raises issues about data collection via Google Tag Manager, suggesting Synthetic adopt privacy-first tools like Kagi. Others highlight the need for clearer policies on prompt logging and data retention (currently 14 days).

2. **Model Performance & Support:**  
   - Qwen3’s quantization and speed on Cerebras hardware are praised.  
   - Open-source coding LLMs (e.g., GLM-4.5) are favored for cost, but some note their lack of Fill-in-Middle (FIM) support.  
   - Users debate tradeoffs between model size (e.g., smaller GLM-45 vs. larger Anthropic models) and token efficiency for coding tasks.  

3. **Pricing & Cost Efficiency:**  
   - Synthetic’s tiers ($20–$60/month) are seen as competitive vs. Claude’s higher-priced plans.  
   - Some users struggle to justify costs for experimental use, preferring free/open alternatives.  

4. **API Integration & Tools:**  
   - Confirmation that Synthetic’s OpenAI-compatible API works with tools like Vercel’s SDK and Mastra.  
   - Discord support is available for troubleshooting.  

5. **Technical Queries:**  
   - Requests for debugging workflows, token limits, and GPU hosting details (noted as 2x cheaper for 80GB GPUs).  

**Key Themes:** Interest in cost-effectiveness and model diversity, skepticism around privacy practices, and technical focus on quantization/performance tradeoffs.

