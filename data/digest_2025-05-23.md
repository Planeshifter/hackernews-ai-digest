## AI Submissions for Fri May 23 2025 {{ 'date': '2025-05-23T17:10:53.767Z' }}

### Positional preferences, order effects, prompt sensitivity undermine AI judgments

#### [Submission URL](https://www.cip.org/blog/llm-judges-are-unreliable) | 142 points | by [joalstein](https://news.ycombinator.com/user?id=joalstein) | [74 comments](https://news.ycombinator.com/item?id=44074668)

In the realm of artificial intelligence, the decisions made by Large Language Models (LLMs) are increasingly stepping beyond chat-based interactions into domains like hiring, healthcare, law, and civic engagement. However, their reliability in these sensitive arenas is being called into question. In an insightful article by James Padolsey, titled "LLM Judges Are Unreliable," the impact of positional preferences, order effects, and prompt sensitivity on AI judgments is examined.

Padolsey highlights the fragile nature of LLM decision-making methods, which encompass A/B testing, ranking, classification, and even panels of LLM “judges.” These methods subject decisions to various biases and inconsistencies, underscoring the vulnerabilities in prompt engineering. Prompt engineering—a practice akin to "playing" rather than "engineering"—suffers from anecdotal reliance without empirical backing, influencing how LLMs arrive at decisions based on mere changes in phrasing or label usage.

Padolsey's research reveals that LLMs have inherent biases similar to human cognition, such as serial position, framing, and anchoring effects. For example, when choosing between options labeled 'Response A' and 'Response B,' LLMs show a significant bias toward 'Response B.' This preference is influenced by small changes in prompt phrasing or labeling, akin to human order and labeling biases.

Additionally, the order in which criteria are evaluated impacts LLMs' scoring tendencies. A criterion like "Clarity," when judged last, suffers from a recency bias, affecting its average score. Moreover, the context during evaluation—whether isolated or holistic—plays a critical role, often diluting scores for negative traits.

Interestingly, LLMs often misinterpret scale ranges; they associate higher scores with positive traits due to training data biases. This misalignment can skew the severity of negative trait assessments, like sexism or toxicity.

System prompt unpredictability further complicates LLM behavior. Instructions designed to reduce bias sometimes have the opposite effect. A de-biasing directive, depending on labeling schemes, can inadvertently increase bias instead of mitigate it.

Overall, Padolsey's analysis emphasizes the frailties in using LLMs for sensitive decision-making tasks, urging a reconsideration of their role and the underlying methods of prompt engineering. As AI continues to evolve, understanding and mitigating these biases will be crucial to its effective deployment in critical areas.

**Summary of Discussion:**

The discussion around James Padolsey’s article "LLM Judges Are Unreliable" highlights technical and philosophical debates about LLM limitations, biases, and interpretability. Key points include:

1. **Prompt Sensitivity & Engineering**:  
   Users note LLMs’ extreme sensitivity to phrasing, labeling, and prompt structure, which can drastically alter outputs. Prompt engineering is likened to “playing” rather than rigorous design, with outcomes often anecdotal and context-dependent. For example, minor tweaks (e.g., switching "Response A/B" labels) introduce positional biases, mirroring human cognitive biases like recency or anchoring effects.

2. **Statistical vs. Semantic Understanding**:  
   While Anthropic’s research suggests LLMs develop abstract, language-agnostic concepts in intermediate layers, skeptics argue models rely on statistical correlations (e.g., Word2Vec-style embeddings) rather than true semantic understanding. This raises questions about whether cross-linguistic consistency reflects genuine reasoning or surface-level pattern matching from training data (e.g., synthetic translations).

3. **Cultural & Training Biases**:  
   LLMs inherit biases from their training data, such as associating specific cultural concepts (e.g., desserts) with languages. Users debate whether “universal” representations in models are meaningful or artifacts of data curation (e.g., cherry-picked multilingual corpora).

4. **Transparency & Interpretability**:  
   Concerns about opacity persist, particularly in closed models like Anthropic’s Claude. While some users highlight efforts to trace internal reasoning (e.g., cross-lingual concept activation), others dismiss interpretability research as speculative or prone to anthropomorphizing model behavior.

5. **Clever Hans Effects**:  
   Analogies to the “Clever Hans” phenomenon—where models exploit subtle cues instead of true understanding—emerge. Users warn that LLMs may prioritize plausible-sounding answers over accuracy, especially in high-stakes scenarios (e.g., rejecting a $250K pitch based on prompt wording).

6. **Human vs. LLM Judgment**:  
   Some argue LLMs replicate human biases (e.g., order effects, framing), while others stress that human decisions often lack rigorous logic too. The debate centers on whether LLMs amplify existing flaws or introduce critical critical critical domains.

**Consensus**:  
While LLMs show promise, their deployment in sensitive areas requires caution. Reliability hinges on addressing prompt brittleness, improving transparency, and disentangling statistical artifacts from genuine reasoning. The discussion underscores the need for rigorous empirical validation over anecdotal prompt-tuning and a clearer distinction between correlation and comprehension in AI systems.

### Beyond Semantics: Unreasonable Effectiveness of Reasonless Intermediate Tokens

#### [Submission URL](https://arxiv.org/abs/2505.13775) | 122 points | by [nyrikki](https://news.ycombinator.com/user?id=nyrikki) | [63 comments](https://news.ycombinator.com/item?id=44074111)

A groundbreaking paper has just been published on arXiv, titled "Beyond Semantics: The Unreasonable Effectiveness of Reasonless Intermediate Tokens." Written by Kaya Stechly, Karthik Valmeekam, and their colleagues, the study dives deep into the surprising success of reasoning models trained with seemingly nonsensical input. It challenges the conventional wisdom surrounding the Chain of Thought (CoT) methodology in machine learning, revealing that models can still perform remarkably well, even when trained on corrupted or irrelevant reasoning traces.

Through their meticulous research, the authors found that models produced correct solutions even when their reasoning pathways—a trail of tokens often thought to mimic human-like thoughts—didn't align logically with the intended outcomes. Surprisingly, this approach sometimes improved performance and robustness across tasks that the models weren't directly trained for.

This research calls into question our understanding of how language models process and generate reasoning pathways, highlighting that the presence of correct intermediate reasoning is not necessarily indicative of a model's performance. It serves as a cautionary tale against attributing human-like interpretative behaviors to these advanced neural networks.

This paper doesn't just stir the pot in the machine learning community; it serves as a beacon, urging us to rethink how we approach AI training methodology. So, if you're ready to contribute to a domain that lies at the intersection of science and technology, arXiv might be the place for you. Whether you want to make your mark at arXiv or get swept up in this latest ML revelation, the opportunities are abundant.

**Hacker News Discussion Summary:**

The discussion revolves around the paper's challenge to the assumption that coherent intermediate reasoning (Chain of Thought, or CoT) is necessary for language model performance. Key points include:

1. **Token Generation vs. Meaningful Reasoning**:  
   Participants debate whether intermediate tokens in reasoning traces reflect genuine "thought" or are merely projections from high-dimensional latent spaces. Some argue that models generate tokens probabilistically based on learned distributions, not logical reasoning. As user `vln` notes, "Tokens don’t attend to tokens; transformers attend to high-dimensional latent states."

2. **Latent Space Dynamics**:  
   The paper’s finding that corrupted reasoning traces still yield correct answers is linked to the idea that reasoning occurs in latent representations rather than token sequences. `jacob019` suggests that intermediate steps may encode contextual information in high-dimensional latent spaces, which are then simplified into tokens.

3. **Anthropomorphism Critique**:  
   Several users caution against interpreting model outputs as human-like reasoning. `gdlsk` warns that attributing "internal dialogue" to models risks anthropomorphizing their mechanics, while `x_flynn` questions whether latent reasoning aligns with ground-truth procedures.

4. **Philosophical References**:  
   The paper’s title nods to Eugene Wigner’s famous essay *The Unreasonable Effectiveness of Mathematics in the Natural Sciences*. This sparks a meta-discussion about parallels between mathematical abstraction in physics and latent reasoning in AI. `gdlsk` connects this to debates about whether mathematics is "discovered" or "invented."

5. **Broader Implications**:  
   The community reflects on how this research might shift focus toward understanding latent space dynamics over token-level interpretability. Some express skepticism about overhyping CoT methods, while others see value in re-examining training paradigms.

**Notably**, the arXiv DevOps job posting mentioned in the submission receives no direct discussion. The thread instead delves deeply into the paper’s technical and philosophical implications, highlighting ongoing tensions between interpretability and the opaque, high-dimensional mechanics of modern language models.

### I Hated Smart Glasses, but Google's Android XR Let Me See a New Future

#### [Submission URL](https://www.cnet.com/tech/computing/i-hated-smart-glasses-but-googles-android-xr-let-me-see-a-new-future/) | 23 points | by [mosura](https://news.ycombinator.com/user?id=mosura) | [3 comments](https://news.ycombinator.com/item?id=44075046)

In a recent jaunt at the Google I/O developer conference, CNET's Patrick Holland had a firsthand peek at Google's prototype for Android XR glasses. As a self-proclaimed skeptic of smart glasses, Holland has often viewed them as more hassle than they're worth. However, this new tech venture from Google, in collaboration with Samsung and Qualcomm, might just change his mind.

With an unassuming design reminiscent of ordinary prescription glasses, these futuristic frames pack advanced features behind their conventional exterior. Equipped with a small, right-lens display, the glasses can showcase everything from the time and weather to full-color photos. A natural extension of one's smartphone, the glasses are intuitive to operate, using gesture controls embedded along the temple's edges and a physical camera shutter button.

During his brief hands-on encounter, Holland likened the experience to being in a spy movie, as the glasses seamlessly integrated into his surroundings. Powered by Gemini AI, the frames offered voice-activated responses and directions projected in an augmented reality format using Google Maps—making navigation not just easier, but enjoyable.

While still in prototype stages and with many questions left unanswered—like battery life, cost, and practicality for everyday use—Holland's initial skepticism waned, leaving him intrigued and optimistic about the glasses' potential to appeal to a broader audience beyond early adopters. As Google refines this tech, these Android XR glasses could become a mainstream staple, redefining how we interact with the digital world without being tethered to a phone screen.

Here's a summary of the discussion:

1. **Flemlo** expresses cautious optimism, noting that current smart glasses feel underdeveloped ("shit") but imagines they could become practical tools for tasks like walking, video calls, or coding within 3–7 years as the tech matures.  
   
2. **Brnt-rsstr** critiques previous smart glasses (e.g., Ray-Ban’s collab) as bulky, plasticky, and only a "halfway" solution. They prefer fully transparent, minimalist designs like Google’s XR prototype, which avoids the "chunky" aesthetics of older models.  

3. **Dcdvt** raises skepticism, arguing smart glasses won’t replace smartphones due to poor battery life and unresolved issues like attention fragmentation. They believe devices like smartwatches and AirPods are better at minimizing screen time, while smart glasses risk exacerbating smartphone addiction by blending digital distractions into everyday life.  

**Key Takeaway**: The discussion reflects a mix of cautious hope for future iterations and skepticism about current limitations (battery, design, usability). Users debate whether smart glasses will solve or worsen tech dependency, with some favoring incremental improvements to existing wearables over untethered AR glasses.

