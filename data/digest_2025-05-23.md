## AI Submissions for Fri May 23 2025 {{ 'date': '2025-05-23T17:10:53.767Z' }}

### Positional preferences, order effects, prompt sensitivity undermine AI judgments

#### [Submission URL](https://www.cip.org/blog/llm-judges-are-unreliable) | 142 points | by [joalstein](https://news.ycombinator.com/user?id=joalstein) | [74 comments](https://news.ycombinator.com/item?id=44074668)

In the realm of artificial intelligence, the decisions made by Large Language Models (LLMs) are increasingly stepping beyond chat-based interactions into domains like hiring, healthcare, law, and civic engagement. However, their reliability in these sensitive arenas is being called into question. In an insightful article by James Padolsey, titled "LLM Judges Are Unreliable," the impact of positional preferences, order effects, and prompt sensitivity on AI judgments is examined.

Padolsey highlights the fragile nature of LLM decision-making methods, which encompass A/B testing, ranking, classification, and even panels of LLM “judges.” These methods subject decisions to various biases and inconsistencies, underscoring the vulnerabilities in prompt engineering. Prompt engineering—a practice akin to "playing" rather than "engineering"—suffers from anecdotal reliance without empirical backing, influencing how LLMs arrive at decisions based on mere changes in phrasing or label usage.

Padolsey's research reveals that LLMs have inherent biases similar to human cognition, such as serial position, framing, and anchoring effects. For example, when choosing between options labeled 'Response A' and 'Response B,' LLMs show a significant bias toward 'Response B.' This preference is influenced by small changes in prompt phrasing or labeling, akin to human order and labeling biases.

Additionally, the order in which criteria are evaluated impacts LLMs' scoring tendencies. A criterion like "Clarity," when judged last, suffers from a recency bias, affecting its average score. Moreover, the context during evaluation—whether isolated or holistic—plays a critical role, often diluting scores for negative traits.

Interestingly, LLMs often misinterpret scale ranges; they associate higher scores with positive traits due to training data biases. This misalignment can skew the severity of negative trait assessments, like sexism or toxicity.

System prompt unpredictability further complicates LLM behavior. Instructions designed to reduce bias sometimes have the opposite effect. A de-biasing directive, depending on labeling schemes, can inadvertently increase bias instead of mitigate it.

Overall, Padolsey's analysis emphasizes the frailties in using LLMs for sensitive decision-making tasks, urging a reconsideration of their role and the underlying methods of prompt engineering. As AI continues to evolve, understanding and mitigating these biases will be crucial to its effective deployment in critical areas.

**Summary of Discussion:**

The discussion around James Padolsey’s article "LLM Judges Are Unreliable" highlights technical and philosophical debates about LLM limitations, biases, and interpretability. Key points include:

1. **Prompt Sensitivity & Engineering**:  
   Users note LLMs’ extreme sensitivity to phrasing, labeling, and prompt structure, which can drastically alter outputs. Prompt engineering is likened to “playing” rather than rigorous design, with outcomes often anecdotal and context-dependent. For example, minor tweaks (e.g., switching "Response A/B" labels) introduce positional biases, mirroring human cognitive biases like recency or anchoring effects.

2. **Statistical vs. Semantic Understanding**:  
   While Anthropic’s research suggests LLMs develop abstract, language-agnostic concepts in intermediate layers, skeptics argue models rely on statistical correlations (e.g., Word2Vec-style embeddings) rather than true semantic understanding. This raises questions about whether cross-linguistic consistency reflects genuine reasoning or surface-level pattern matching from training data (e.g., synthetic translations).

3. **Cultural & Training Biases**:  
   LLMs inherit biases from their training data, such as associating specific cultural concepts (e.g., desserts) with languages. Users debate whether “universal” representations in models are meaningful or artifacts of data curation (e.g., cherry-picked multilingual corpora).

4. **Transparency & Interpretability**:  
   Concerns about opacity persist, particularly in closed models like Anthropic’s Claude. While some users highlight efforts to trace internal reasoning (e.g., cross-lingual concept activation), others dismiss interpretability research as speculative or prone to anthropomorphizing model behavior.

5. **Clever Hans Effects**:  
   Analogies to the “Clever Hans” phenomenon—where models exploit subtle cues instead of true understanding—emerge. Users warn that LLMs may prioritize plausible-sounding answers over accuracy, especially in high-stakes scenarios (e.g., rejecting a $250K pitch based on prompt wording).

6. **Human vs. LLM Judgment**:  
   Some argue LLMs replicate human biases (e.g., order effects, framing), while others stress that human decisions often lack rigorous logic too. The debate centers on whether LLMs amplify existing flaws or introduce critical critical critical domains.

**Consensus**:  
While LLMs show promise, their deployment in sensitive areas requires caution. Reliability hinges on addressing prompt brittleness, improving transparency, and disentangling statistical artifacts from genuine reasoning. The discussion underscores the need for rigorous empirical validation over anecdotal prompt-tuning and a clearer distinction between correlation and comprehension in AI systems.

### Beyond Semantics: Unreasonable Effectiveness of Reasonless Intermediate Tokens

#### [Submission URL](https://arxiv.org/abs/2505.13775) | 122 points | by [nyrikki](https://news.ycombinator.com/user?id=nyrikki) | [63 comments](https://news.ycombinator.com/item?id=44074111)

Here's a fascinating piece of news from the world of open science: arXiv, the widely respected repository of electronic preprints in the fields of physics, computer science, and more, is on the lookout for a DevOps Engineer. This could be a golden opportunity to play a vital role in supporting one of the world's most significant scientific websites.

In parallel, a groundbreaking paper has just been published on arXiv, titled "Beyond Semantics: The Unreasonable Effectiveness of Reasonless Intermediate Tokens." Written by Kaya Stechly, Karthik Valmeekam, and their colleagues, the study dives deep into the surprising success of reasoning models trained with seemingly nonsensical input. It challenges the conventional wisdom surrounding the Chain of Thought (CoT) methodology in machine learning, revealing that models can still perform remarkably well, even when trained on corrupted or irrelevant reasoning traces.

Through their meticulous research, the authors found that models produced correct solutions even when their reasoning pathways—a trail of tokens often thought to mimic human-like thoughts—didn't align logically with the intended outcomes. Surprisingly, this approach sometimes improved performance and robustness across tasks that the models weren't directly trained for.

This research calls into question our understanding of how language models process and generate reasoning pathways, highlighting that the presence of correct intermediate reasoning is not necessarily indicative of a model's performance. It serves as a cautionary tale against attributing human-like interpretative behaviors to these advanced neural networks.

This paper doesn't just stir the pot in the machine learning community; it serves as a beacon, urging us to rethink how we approach AI training methodology. So, if you're ready to contribute to a domain that lies at the intersection of science and technology, arXiv might be the place for you. Whether you want to make your mark at arXiv or get swept up in this latest ML revelation, the opportunities are abundant.

**Hacker News Discussion Summary:**

The discussion revolves around the paper's challenge to the assumption that coherent intermediate reasoning (Chain of Thought, or CoT) is necessary for language model performance. Key points include:

1. **Token Generation vs. Meaningful Reasoning**:  
   Participants debate whether intermediate tokens in reasoning traces reflect genuine "thought" or are merely projections from high-dimensional latent spaces. Some argue that models generate tokens probabilistically based on learned distributions, not logical reasoning. As user `vln` notes, "Tokens don’t attend to tokens; transformers attend to high-dimensional latent states."

2. **Latent Space Dynamics**:  
   The paper’s finding that corrupted reasoning traces still yield correct answers is linked to the idea that reasoning occurs in latent representations rather than token sequences. `jacob019` suggests that intermediate steps may encode contextual information in high-dimensional latent spaces, which are then simplified into tokens.

3. **Anthropomorphism Critique**:  
   Several users caution against interpreting model outputs as human-like reasoning. `gdlsk` warns that attributing "internal dialogue" to models risks anthropomorphizing their mechanics, while `x_flynn` questions whether latent reasoning aligns with ground-truth procedures.

4. **Philosophical References**:  
   The paper’s title nods to Eugene Wigner’s famous essay *The Unreasonable Effectiveness of Mathematics in the Natural Sciences*. This sparks a meta-discussion about parallels between mathematical abstraction in physics and latent reasoning in AI. `gdlsk` connects this to debates about whether mathematics is "discovered" or "invented."

5. **Broader Implications**:  
   The community reflects on how this research might shift focus toward understanding latent space dynamics over token-level interpretability. Some express skepticism about overhyping CoT methods, while others see value in re-examining training paradigms.

**Notably**, the arXiv DevOps job posting mentioned in the submission receives no direct discussion. The thread instead delves deeply into the paper’s technical and philosophical implications, highlighting ongoing tensions between interpretability and the opaque, high-dimensional mechanics of modern language models.

### KumoRFM: A Foundation Model for In-Context Learning on Relational Data

#### [Submission URL](https://kumo.ai/company/news/kumo-relational-foundation-model/) | 104 points | by [cliffly](https://news.ycombinator.com/user?id=cliffly) | [18 comments](https://news.ycombinator.com/item?id=44070532)

**KumoRFM: Transforming Relational Data Predictions with AI**  

**Background:** Foundation Models (FMs) have revolutionized unstructured data domains, such as language processing and image analysis, by boosting performance with minimal task-specific tuning. However, their application in structured relational data, like databases crucial for business intelligence, has been limited. Enter KumoRFM, a pioneering Relational Foundation Model (RFM) promising to streamline predictions directly from structured databases.

**What is KumoRFM?** KumoRFM is a sophisticated AI model that handles predictive tasks on relational databases without task-specific training. Employing novel concepts from foundation models and relational data learning, KumoRFM constructs predictions from complex database schemas with multiple tables and column types. It introduces a Predictive Query Language (PQL), designed to simplify defining predictive problems akin to SQL.

**Functionality:**  
- **Input & Output:** Users submit predictive queries using PQL. KumoRFM then retrieves relevant data, constructs context examples, and performs the desired predictions, supporting tasks like regression, classification, and link prediction.
- **Architecture:** The model reshapes relational data into temporal, heterogeneous graphs. Tables become node types and rows, nodes with foreign key relationships forming edges. The model encodes tables invariantly and uses a Relational Graph Transformer for reasoning among related tables.
- **Efficiency & Speed:** KumoRFM leverages in-context learning, sampling historical data to guide predictions without retraining, thus significantly speeding up the processing compared to traditional approaches.

**Performance:** Evaluated on the RelBench benchmark across 30 tasks in diverse domains, KumoRFM shows a 2% to 8% performance improvement over conventional methods. Moreover, fine-tuning enhances its accuracy by 10% to 30%, and it operates orders of magnitude faster than traditional supervised ML models.

**Implications:** KumoRFM stands as a game-changer, enabling businesses and researchers to derive insights from relational data efficiently and effectively. By automating complex predictive modeling processes and minimizing the need for intensive task-specific setup, KumoRFM could democratize the use of AI in data-centric industries.

**Conclusion:** KumoRFM represents a leap forward in AI application to structured datasets by unifying machine learning advancements with the robust, often untapped potential of relational data. Whether predicting market trends or detecting fraud, its capability to handle varied and evolving datasets positions it as a formidable tool in the AI toolkit.

**Summary of Hacker News Discussion on KumoRFM:**

- **Comparisons to TabPFN:** Users noted KumoRFM’s advantage over TabPFN (a single-table-focused model) in handling multi-table relational data, which is critical for real-world applications like predicting customer behavior across linked datasets. TabPFN might underperform in such scenarios, while KumoRFM’s architecture leverages graph-based relationships for more accurate predictions.

- **Credibility and Expertise:** Commenters highlighted Jure Leskovec’s involvement (a Stanford professor and relational AI pioneer) as a vote of confidence. His prior work on graph neural networks (GNNs) aligns with KumoRFM’s design, further validating its technical foundation.

- **Structured Data in Practice:**  
  - Many pointed out that structured data predictions are already ubiquitous (e.g., fraud detection, recommendations), but KumoRFM could streamline these processes by unifying multi-source data without manual feature engineering.  
  - Skepticism arose about novelty, with some noting that structured data models like XGBoost dominate today, but interest remains in foundation models for generalized use.

- **Use Cases Highlighted:**  
  - **Healthcare:** Predicting patient diagnoses by linking behavioral data, medical history, and test results across tables.  
  - **Restaurant Management:** Forecasting table availability by combining reservation records, holidays, and weather data, which requires dynamic joins.  

- **Technical Insights:**  
  - Users praised KumoRFM’s in-context learning and graph transformers for scalability and speed improvements over traditional ML pipelines.  
  - The need for benchmarks comparing KumoRFM to LLMs in structured tasks was noted, with curiosity about its potential to rival or complement existing systems.

- **Reception:** Overall optimism about KumoRFM’s ability to democratize AI for relational data, though questions linger about integration into real-world systems and competition with entrenched methods. The focus on automating schema-aware predictions resonated as a key innovation.  

**In Essence:** The community sees KumoRFM as a promising step toward unifying AI with structured data ecosystems, offering efficiency gains and broader accessibility, but its adoption will depend on practical performance in diverse, complex scenarios.

### I Hated Smart Glasses, but Google's Android XR Let Me See a New Future

#### [Submission URL](https://www.cnet.com/tech/computing/i-hated-smart-glasses-but-googles-android-xr-let-me-see-a-new-future/) | 23 points | by [mosura](https://news.ycombinator.com/user?id=mosura) | [3 comments](https://news.ycombinator.com/item?id=44075046)

In a recent jaunt at the Google I/O developer conference, CNET's Patrick Holland had a firsthand peek at Google's prototype for Android XR glasses. As a self-proclaimed skeptic of smart glasses, Holland has often viewed them as more hassle than they're worth. However, this new tech venture from Google, in collaboration with Samsung and Qualcomm, might just change his mind.

With an unassuming design reminiscent of ordinary prescription glasses, these futuristic frames pack advanced features behind their conventional exterior. Equipped with a small, right-lens display, the glasses can showcase everything from the time and weather to full-color photos. A natural extension of one's smartphone, the glasses are intuitive to operate, using gesture controls embedded along the temple's edges and a physical camera shutter button.

During his brief hands-on encounter, Holland likened the experience to being in a spy movie, as the glasses seamlessly integrated into his surroundings. Powered by Gemini AI, the frames offered voice-activated responses and directions projected in an augmented reality format using Google Maps—making navigation not just easier, but enjoyable.

While still in prototype stages and with many questions left unanswered—like battery life, cost, and practicality for everyday use—Holland's initial skepticism waned, leaving him intrigued and optimistic about the glasses' potential to appeal to a broader audience beyond early adopters. As Google refines this tech, these Android XR glasses could become a mainstream staple, redefining how we interact with the digital world without being tethered to a phone screen.

Here's a summary of the discussion:

1. **Flemlo** expresses cautious optimism, noting that current smart glasses feel underdeveloped ("shit") but imagines they could become practical tools for tasks like walking, video calls, or coding within 3–7 years as the tech matures.  
   
2. **Brnt-rsstr** critiques previous smart glasses (e.g., Ray-Ban’s collab) as bulky, plasticky, and only a "halfway" solution. They prefer fully transparent, minimalist designs like Google’s XR prototype, which avoids the "chunky" aesthetics of older models.  

3. **Dcdvt** raises skepticism, arguing smart glasses won’t replace smartphones due to poor battery life and unresolved issues like attention fragmentation. They believe devices like smartwatches and AirPods are better at minimizing screen time, while smart glasses risk exacerbating smartphone addiction by blending digital distractions into everyday life.  

**Key Takeaway**: The discussion reflects a mix of cautious hope for future iterations and skepticism about current limitations (battery, design, usability). Users debate whether smart glasses will solve or worsen tech dependency, with some favoring incremental improvements to existing wearables over untethered AR glasses.

