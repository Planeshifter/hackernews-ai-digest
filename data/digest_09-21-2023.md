## AI Submissions for Thu Sep 21 2023 {{ 'date': '2023-09-21T17:11:22.986Z' }}

### Game: ASM â€“ Das Computer-Spiel (The Director's Cut) released

#### [Submission URL](https://www.amiga-news.de/en/news/AN-2023-09-00044-EN.html) | 22 points | by [doener](https://news.ycombinator.com/user?id=doener) | [5 comments](https://news.ycombinator.com/item?id=37595642)

German computer game "ASM - Das Computer-Spiel (The Director's Cut)" has been released, offering players a nostalgic trip back to the world of the former computer game magazine "ASM - Aktueller Software Markt". Originally created as part of a programming competition in 1991, the graphic adventure game has been revamped, expanded, and given new music over the course of a year. In the game, players assume the role of the chief editor of the magazine and navigate through the editorial building, searching for the secret behind their own memory loss. The game aims to entertain with its humorous style, inspired by movies like "Spaceballs," "The Naked Gun," and Monty Python. "ASM - The Computer Game" is available as a download or a physical product, complete with a CD for Amigas, a poster, disk stickers, an ASM wooden cube, and a manual styled after an ASM from 1991. The game is compatible with any Amiga from Kickstart 1.3 with at least 1 MB RAM, and a joystick can be used in addition to the keyboard and mouse controls. A hard disk installer is also included.

The discussion on the submission revolves around different aspects of the game "ASM - Das Computer-Spiel (The Director's Cut)". 

User "rob74" shares a link to the game and mentions that it is in German but there are screenshots available in English. They also note that the game seems to have a lot of verb-based commands and references popular movies like "Spaceballs," "The Naked Gun," and Monty Python.

User "pvtz" responds, expressing their dislike for games that rely heavily on 13 commands. They mention that games like Indiana Jones and the Fate of Atlantis had 14 memory serves, which they found to be more normal.

User "rob74" then points out that the memory in those LucasArts games appeared to be skewed. They provide a link to screenshots of other games, including Indiana Jones and the Fate of Atlantis and Monkey Island 1, to support their point.

User "snckrr" jumps in to say that the game is available for purchase on their website.

Finally, user "rrx" mentions that the game features lovely boxed packaging and is available for 35 euros.

### Matrix 2.0: The Future of Matrix

#### [Submission URL](https://matrix.org/blog/2023/09/matrix-2-0/) | 562 points | by [jrepinc](https://news.ycombinator.com/user?id=jrepinc) | [237 comments](https://news.ycombinator.com/item?id=37599510)

Element X, the new Matrix 2.0 client, is now available for users to try out. Matrix, an open standard for secure and decentralized communication, has come a long way since its inception in 2014. With over 111 million matrix IDs on the public network and a growing ecosystem of clients, bots, and bridges, Matrix is being adopted by a wide range of organizations and individuals. The importance of decentralization is becoming increasingly clear as centralized Internet services pose risks such as corporate takeover, state censorship, surveillance, and data breaches. The Matrix team has been working on solving the hard problems of decentralization and end-to-end encryption, but now they're focusing on improving performance. Matrix 2.0 introduces features like instant login and sync, industry-standard authentication, end-to-end encrypted voice and video conferencing, and lazy-loading room state. These features aim to create blazingly fast and hyper-efficient communication apps that can outperform mainstream messaging services. Element X, driven by the matrix-rust-sdk codebase, serves as a test-bed for the new Matrix 2.0 functionality. It demonstrates that Matrix is capable of powering communication for billions of users. The release of Element X marks a significant milestone in terms of usability and performance for Matrix.

The discussion about the Matrix 2.0 client, Element X, on Hacker News covers a range of topics related to the functionality and implementation of the Matrix protocol. Some users express excitement about the new features, such as sliding sync support and improved performance, while others discuss the technical details and potential drawbacks of the implementation.

One user mentions that the Matrix team is proposing new APIs and implementing protocols like sliding sync to improve the functionality of Matrix servers. However, another user points out that the implementation of sliding sync is complex and may not be necessary for all users. There is also a discussion about the maintenance and deployment of the sliding sync proxy.

The discussion also touches upon the compatibility of Matrix with other platforms. Users mention that Matrix has the potential to support features similar to Discord, such as screen sharing, and suggest trying out a Matrix client called Cinny for a Discord-like user interface.

Some users express disappointment with the development of Element, the Matrix client, and suggest that there is a need for simpler fixes and improvements. They also mention issues with the end-to-end encryption functionality and express concerns about the centralized nature of Matrix.

Other topics discussed include the simplicity of Jabber and the complexity of implementing notifications, as well as the need to resolve vulnerabilities in the Matrix protocol, particularly related to server-controlled group membership.

Overall, the discussion covers various perspectives on the new Matrix client and raises important points about the functionality, implementation, and future development of the Matrix protocol.

### Open source AI will win

#### [Submission URL](https://varunshenoy.substack.com/p/why-open-source-ai-will-win) | 177 points | by [nocturnalowl](https://news.ycombinator.com/user?id=nocturnalowl) | [161 comments](https://news.ycombinator.com/item?id=37602674)

In his article "Why Open Source AI Will Win," author Varun Shenoy argues that open source AI will have a greater impact on the future of large language models (LLMs) and image models than many believe. He addresses common arguments against open source AI, such as the belief that it cannot compete with industry labs in terms of resources and that it is not safe or capable of reasoning. Shenoy counters these arguments by highlighting the importance of LLMs for AI native businesses and the need for these companies to have control over their core products, as well as the fact that open source models excel at the most valuable tasks. He also emphasizes the rapid advancements and scalability of open source models, such as the extended context length and reduced hardware requirements. Overall, Shenoy believes that open source AI will play a significant role in the future of AI development.

The discussion surrounding the submission revolves around various aspects of open source AI and its implications. Some commenters mention the potential legal issues, such as copyright infringement, that may arise when training models with copyrighted materials. Others discuss the advantages of open source AI, including the ability to control core products and the scalability of open source models. The role of government and the potential risks associated with large language models are also debated. Additionally, there is a discussion about the use of pseudonyms and the importance of legal advice in online forums.

One commenter brings up the issue of copyright lawsuits and the importance of intellectual property lawyers in the field of AI. Another commenter mentions the potential conflict between Palantir's proprietary models and open source models, highlighting the different domains in which each approach may be more suitable. The potential challenges and legal uncertainties surrounding copyright, particularly in training models with copyrighted material, are also addressed. Finally, a comparison is made between the copyright disputes faced by the RIAA and MPAA in the past and the potential issues that may arise in open source AI.

### RAG is more than just embedding search

#### [Submission URL](https://jxnl.github.io/instructor/blog/2023/09/17/rag-is-more-than-just-embedding-search/) | 148 points | by [jxnlco](https://news.ycombinator.com/user?id=jxnlco) | [55 comments](https://news.ycombinator.com/item?id=37599873)

Today's top story on Hacker News discusses the limitations of using retrieval augmented generation (RAG) models that rely on simple query embedding and search. The author points out several drawbacks of this approach, including query-document mismatch, a monolithic search backend, limited ability to handle complex queries, and a lack of query understanding.

The author proposes an improved RAG model that incorporates query understanding to enhance precision and recall. They highlight the collaboration with clients at new.computer, Metaphor Systems, and Naro in developing this approach.

The article then delves into a real-world example from Metaphor Systems, a company that specializes in converting natural language queries into search-optimized queries. They use a library called Pydantic to define complex query objects that contain structured information like date ranges and domain filters. By restructuring the user's query, Metaphor Systems improves the search performance without requiring users to understand the intricacies of the search backend.

The article concludes by demonstrating how this approach can also be used in building a personal assistant agent that utilizes query understanding to provide tailored and nuanced searches.

Overall, the article emphasizes the importance of moving beyond simple query embedding and search in RAG models, and highlights the benefits of incorporating query understanding to enhance search capabilities. This approach enables more powerful and flexible search queries while minimizing the burden on users to understand the underlying search infrastructure.

The discussion on this article covers a range of topics related to retrieval augmented generation (RAG) models and query understanding in search engines. Here are some key points from the discussion:

- One commenter suggests that instead of using smart nested queries, developers often try overly complex solutions when it comes to search, but it's better to stick to established practices and not reinvent the wheel.
- Another person shares their success in using ChatGPT with Redshift queries based on natural language text to render interesting results. Slow performance and limitations in ChatGPT's search flow are mentioned as common issues.
- There is agreement that query understanding and structured query attributes are valuable for improving search results.
- The scalability and complexity of building search engines with AI models and the challenges they pose are discussed.
- The limitations of RAG models in terms of query-document mismatch and the need for more sophisticated search capabilities are highlighted.
- There is a mention of the difficulty of implementing search in the space domain and the importance of having skilled developers in the field.
- Password reset requests are made in one comment, followed by a request for the deletion of a database.
- The effectiveness of RAG systems and the challenges in designing query prompts are discussed.
- The importance of having clear metrics and expectations for AI products is emphasized.
- Recommendations for further reading and approaches to solving search problems are shared.
- The discussion touches on the use of linear transformations and symmetric embeddings in search, as well as approaches based on knowledge graphs and question-answering systems.

Overall, the discussion provides additional insights into the challenges and potential solutions related to using RAG models and improving search capabilities in AI-powered systems.

### Gaudi 2 â€“ An AI Accelerator Card with 96GB of HBM2E Memory

#### [Submission URL](https://habana.ai/products/gaudi2/) | 27 points | by [peter_d_sherman](https://news.ycombinator.com/user?id=peter_d_sherman) | [5 comments](https://news.ycombinator.com/item?id=37597030)

Habana Labs has unveiled its Gaudi2 accelerator, which is designed to improve price-performance and operational efficiency for training and running deep learning models. The Gaudi2 offers architectural features such as heterogeneous compute, 24 Tensor Processor Cores, and 100 Gigabit Ethernet integrated on chip. It also supports massive scale-out with flexible scalability options. The Gaudi2 delivers strong performance, making it a viable alternative for training large language models like GPT-3. Furthermore, Habana provides the SynapseAI Software Stack, which is optimized for Gaudi platform performance and offers over 50,000 models through the Habana Optimum Library on the Hugging Face hub. Developers can easily build or migrate models on the Gaudi2 using this software stack. Habana's Gaudi2 accelerator brings improved AI capabilities to the cloud and data centers, offering the industry much-needed choice for efficient scalability.

The discussion on this submission seems to touch upon various aspects of Habana Labs' Gaudi2 accelerator. One user points out the impressive performance of the accelerator but mentions that the price is not mentioned on the Habana Labs website. Another user brings up unrelated information about Habana Labs being a company from Pyongyang. They also mention that the city is crumbling and that Intel's AI products are of interest to the public. Another user expresses anticipation for the migration of PyTorch to the Gaudi2 accelerator. They mention that they hope it will break the stronghold of CUDA. One user discusses the energy efficiency of the Gaudi2 accelerator compared to the H100, providing calculations to support their point. Finally, another user speculates that the H100 has higher energy overhead due to the need to connect multiple GPUs, and mentions the benefits of Habana Labs' offering.

### Church uses ChatGPT for AI-generated service

#### [Submission URL](https://www.kxan.com/news/local/austin/austin-church-holds-ai-generated-service-uses-chatgpt/) | 17 points | by [gmays](https://news.ycombinator.com/user?id=gmays) | [10 comments](https://news.ycombinator.com/item?id=37597011)

In a unique twist, the Violet Crown City Church in north Austin recently hosted a Sunday service entirely created by AI. Pastor Jay Cooper used ChatGPT, an AI technology that can respond to questions and generate a variety of content. Cooper said that while the AI-generated sermon was relevant, it lacked emotion, highlighting the importance of the human touch in worship. The purpose of using AI in the service was to ponder the question of what is sacred and whether a prayer written by AI can communicate truth. Church attendee Ernest Chambers expressed his belief that AI cannot express the emotions of love, kindness, and empathy and that humans should actively practice and express these feelings. While the AI-generated service was a one-time event, it sparks a debate about the appropriate use and integration of AI in different spaces, including places of worship. Samantha Shorey, a communication studies assistant professor, emphasized the need for communities to critically think about AI content and not blindly trust text generated by computers.

The discussion on the submission revolves around the use of AI in religious spaces, particularly in this case, a church service. Some users express their skepticism about AI-generated content, highlighting the importance of human connection and emotion in worship. Others point out that AI lacks the ability to express emotions such as love, kindness, and empathy. There is a debate about the appropriateness of using AI in places of worship and the need for critical thinking when considering AI-generated content. One user recommends the use of AI to assist in sermon preparation rather than completely replacing human involvement. Another user raises concerns about the potential dangers of relying too heavily on AI for inspiration in religious contexts, emphasizing the significance of the Holy Scriptures and the role of the Holy Spirit in guiding believers. The discussion also touches on the role of language models in translating religious texts and their limitations in conveying nuanced meanings. Overall, there is a mix of perspectives on the topic, with some expressing cautious acceptance of AI's role in religious spaces and others emphasizing the importance of human agency and connection in matters of faith.

### OpenAI and ChatGPT Lawsuit List

#### [Submission URL](https://originality.ai/blog/openai-chatgpt-lawsuit-list) | 24 points | by [cdme](https://news.ycombinator.com/user?id=cdme) | [9 comments](https://news.ycombinator.com/item?id=37605334)

Two lawsuits have been filed against OpenAI and its AI language model, ChatGPT. The first lawsuit, Authors Guild et al v. OpenAI Inc. et al, alleges copyright infringement, claiming that OpenAI used the plaintiffs' works without permission to train its ChatGPT model. The second lawsuit, Chabon v. OpenAI, Inc., also alleges copyright infringement and involves Pulitzer Prize-winning author Michael Chabon and several other writers. They claim that OpenAI copied their works without permission to train ChatGPT. Both cases raise important questions about the copyright implications of training large language models on copyrighted data. The outcomes of these cases could significantly impact the development and use of such models.

In the discussion on Hacker News, one user finds it interesting that Sarah Silverman tweeted about the lawsuits and wonders if she is involved in them. Another user points out the irony that internet history is preserved by Chrome browsers but automatically deleted after three months.

Another user comments on the claims made in the lawsuits, mentioning that one of them states that OpenAI may have copied incorrect statements from journalists. Another user clarifies that the journalist's article was not published, and OpenAI's claims were based on misinformation. They also find it amusing that the court asked ChatGPT to summarize a real federal court case, demonstrating the AI's inability to retrieve URLs by default.

There is discussion about the knowledge gap regarding AI among law practitioners, and one user finds it amusing that the conversation has strayed from discussing the copyright infringement claims to exploring the limitations of ChatGPT.

Overall, the discussion touches on various aspects of the lawsuits, including incorrect statements made by journalists, the AI's capabilities and limitations, and the potential impact on civil rights.

### Microsoft Copilot, your everyday AI companion

#### [Submission URL](https://blogs.microsoft.com/blog/2023/09/21/announcing-microsoft-copilot-your-everyday-ai-companion/) | 71 points | by [benryon](https://news.ycombinator.com/user?id=benryon) | [19 comments](https://news.ycombinator.com/item?id=37598650)

Microsoft is introducing a new AI-powered feature called Copilot, which will act as an everyday AI companion across various Microsoft products. Copilot will leverage the context and intelligence of the web, user data, and current PC activities to provide better assistance. It will be available in Windows 11, Microsoft 365, Edge, and Bing. The feature aims to unify AI capabilities and provide a seamless experience. In addition to Copilot, Microsoft announced updates to Windows 11, Bing, and Microsoft 365, as well as new Surface devices. The Windows 11 update will bring over 150 new features, including enhancements to apps like Paint and Photos with AI capabilities. Bing will support the latest DALL.E 3 model from OpenAI and offer personalized answers based on user search history. Microsoft 365 Copilot will be available for enterprise customers on November 1, 2023, along with Microsoft 365 Chat, a new AI assistant.

The discussion on this submission covers various topics related to Microsoft's AI-powered feature, Copilot, as well as other Microsoft products.

One user, CoreyFieldens, mentions the use of cryptographic methods and invisible digital watermarks in AI-generated images on Bing, including time data originally created, to support content credentials. Another user, rjh29, suggests using watermarking, resizing, cropping, and redrawing techniques to remove visible watermarks. FrostKiwi adds that traditional steganography solutions may not survive resizing and suggests a scrambled content approach as a solution.

Another user, thrsvnths, mentions that content credentials can be included in images touched by Adobe Firefly, hoping for interoperable implementations of these watermarks.

A user named kpnd mentions installing Firefox and receives a response from brryrndll, who jokingly expects a heated argument, specifically to prevent using Edge. Another user, dmntr, expresses a desire to install Linux and mentions some issues they have encountered with Discord and screen sharing. They also mention the availability of Easy Anti-Cheat on Linux for multiplayer AAA titles, depending on the distribution and Steam Proton runtime.

Another user, thbtg, finds it funny but good that WordPress and YouTube videos generate content using AI, and expects a future filled with AI-generated content campaigns.

Prct mentions Microsoft's marketing division and their use of Large Language Models, referring to it as a virtual playground.

The user kkkdcjkfk wonders about the type of crunch time experienced by Microsoft and Google.

Mhstr mentions the specific features added and removed from Microsoft products, such as removing the background from MS Paint and introducing AI content generation.

Msntr says goodbye to Cortana, which receives mixed reactions. Rckstnly believes Cortana deserved more recognition within the Microsoft ecosystem, while desi_ninja jokes that Copilot killed Cortana, consistent with the Halo game. Mshnn expresses satisfaction with the removal of the Cortana branding.

Finally, srfngdn mentions Paint and Microsoft Designer.

### Language Modeling Is Compression

#### [Submission URL](https://huggingface.co/papers/2309.10668) | 26 points | by [haltist](https://news.ycombinator.com/user?id=haltist) | [3 comments](https://news.ycombinator.com/item?id=37600376)

The featured paper on Hacker News today explores the connection between language modeling and lossless data compression. The authors argue that there is an equivalence between predictive modeling and compression, and that large language models like Transformers can serve as powerful compressors. They demonstrate that even language models trained primarily on text can achieve impressive compression rates on datasets like ImageNet images and LibriSpeech audio. However, the authors also note that model size plays a role in compression performance, and there is an optimal tradeoff between model size and dataset size. Tokenization is also discussed as a form of compression, with simpler tokenizers like ASCII leading to better compression rates. The paper concludes by highlighting that the prediction-compression equivalence allows any compressor to be used as a conditional generative model. Overall, the paper provides a compression viewpoint on language modeling and large models, shedding light on in-context learning, scaling laws, tokenization, and generation.

The discussion on Hacker News mainly revolves around the connections between language modeling and lossless data compression mentioned in the featured paper. One user expresses confusion about the relevance of the PAC learnability framework and its relationship to compression in the context of the paper. Another user suggests reviewing the Ergotic theory and PAC learnability to gain a better understanding of the topic.

There is a comment providing a direct link to the paper on arXiv, which is hosted on the HuggingFace website.

