## AI Submissions for Thu May 04 2023 {{ 'date': '2023-05-04T15:01:43.244Z' }}

### Rest in Peas: The Unrecognized Death of Speech Recognition (2010)

#### [Submission URL](https://robertfortner.posthaven.com/rest-in-peas-the-unrecognized-death-of-speech) | 37 points | by [jawns](https://news.ycombinator.com/user?id=jawns) | [43 comments](https://news.ycombinator.com/item?id=35800935)

Computer speech recognition hit a flatline in 2001 before it even reached human levels of accuracy, largely due to computers being unable to properly understand language. While progress has been made since the 1950s and 1960s, relying mainly on fast computers and digital text to supplement decades-old language machinery, current accuracy rates still hover around 80%, with humans at 98%. Despite billions of text at their disposal, machines are prone to risky guessing and limited by parsers and recognition systems that work only in certain linguistic domains. Efforts to allow programs to understand grammar and word meaning have been largely unsuccessful, leaving them with a significantly different understanding of language than humans.

The discussion among commenters touches on various topics, including the challenges of recognizing different dialects and accents, reinforcement learning and human feedback, the accuracy of speech recognition software and the importance of context in understanding speech. Some commenters also mention their experiences with specific speech recognition programs and datasets, such as Common Voice and Whisper.

### OpenLLaMA: An Open Reproduction of LLaMA

#### [Submission URL](https://github.com/openlm-research/open_llama) | 463 points | by [sadiq](https://news.ycombinator.com/user?id=sadiq) | [175 comments](https://news.ycombinator.com/item?id=35798888)

OpenLLaMA, an open-source reproduction of Meta AI's LLaMA language model, has been released on GitHub. In this release, a public preview of the 7B OpenLLaMA model trained with 200 billion tokens has been provided, along with PyTorch and JAX weights of pre-trained OpenLLaMA models, and their evaluation results and comparison against the original LLaMA models. Additionally, a new checkpoint of OpenLLaMA 7B trained on 300B tokens has been released to make the model broadly compatible with existing implementations. The results indicate that OpenLLaMA exhibits comparable performance to the original LLaMA and GPT-J across a majority of tasks and outperforms them in some.

In the comments, there is a discussion about the resources required for ML models and their training cost, along with recommendations to understand ML terms and concepts. There is also debate about the use of the word "hallucination" to describe the output of language models.

### Distilling Step-by-Step Outperforming Larger Language Models with Less Training

#### [Submission URL](https://arxiv.org/abs/2305.02301) | 144 points | by [verdverm](https://news.ycombinator.com/user?id=verdverm) | [33 comments](https://news.ycombinator.com/item?id=35810663)

Researchers have developed a new mechanism called "Distilling Step-by-Step" that trains smaller models to outperform larger language models (LLMs) using less training data. This method extracts LLM rationales to provide additional supervision for small models in a multi-task training framework, leading to better performance with fewer labeled/unlabeled training examples. Distilling Step-by-Step achieves better performance with substantially smaller model sizes and reduces both the model size and the amount of data needed to outperform LLMs. The method was successful in four NLP benchmarks and could help make LLMs more memory-efficient and compute-intensive for practical applications.

The comment section discusses the importance of smaller, task-specific models and the challenges of deploying large language models in practical applications due to memory and computation constraints. Furthermore, the commenters discuss related approaches like "Alpaca" and the impact of model licensing and commercialization.

### The first empirical study of the real-world economic effects of new AI systems

#### [Submission URL](https://www.npr.org/sections/money/2023/05/02/1172791281/this-company-adopted-ai-heres-what-happened-to-its-human-workers) | 112 points | by [SirLJ](https://news.ycombinator.com/user?id=SirLJ) | [72 comments](https://news.ycombinator.com/item?id=35809397)

A recent study conducted by economists at Stanford University and MIT found that implementing an AI chatbot into customer service workflows resulted in a significant increase in productivity and customer satisfaction. The study looked at the effects of incorporating ChatGPT, a popular interactive AI chatbot, on a Fortune 500 company's customer support team. The chatbot, which was trained by reading previous conversations between reps and customers, helped customer support agents more effectively assist customers in real time and provided them with links to internal company information to solve technical problems. The results suggest that AI could have positive economic effects in improving productivity, but also highlight the potential for disruptive change and income inequality.

One commenter expressed skepticism about the study's premise, stating that deflection in the customer service world means preventing customers from talking to humans and that AI chatbots cannot completely replace skilled workers. Other discussions revolve around the possibility of AI systems having internal biases, lowering the skill bar, reducing human job function, and potential privacy concerns. Another commenter suggested ingesting data from different platforms such as Slack, Gmail, Jira, Meet, etc., associated with timestamps, as it can help assign higher importance to documents and policies based on their relevance. Another comment discusses a company that believes AI systems that do not employ cash but focus on skilled employees' knowledge are vital to maintain the system. However, this leads to wider income inequality between low and high-skilled workers.

### Chatbot Arena: Benchmarking LLMs in the Wild with Elo Ratings

#### [Submission URL](https://lmsys.org/blog/2023-05-03-arena/) | 46 points | by [MMMercy2](https://news.ycombinator.com/user?id=MMMercy2) | [6 comments](https://news.ycombinator.com/item?id=35806589)

Chatbot Arena, a new benchmark platform for large language models (LLMs), has been released. The platform features anonymous, randomized battles in a crowdsourced manner, and uses the Elo rating system for ranking models â€“ a system widely used in competitive games like chess. The platform allows users to contribute new models and evaluate them based on anonymous votes for which model performs better during chat interactions. The platform already has a leaderboard featuring Elo ratings of popular open-source large language models.

There were several comments on the submission. One user, HoshinoAI, mentioned that the platform uses a ranking algorithm called Glicko. Another user, zhsbg, provided a reference variant ELO. HoshinoAI then pointed out a matchmaking section on Dota 2's website and a Wikipedia page on the Glicko rating system. 

Another user, wchng, was surprised to learn that StableLM was not included in the LLaMA leaderboard. Another user, circuit10, stated that they had heard about it before. Another user, frdvr, simply commented, "good day." Finally, two users, lee101 and aaron695, left comments but they were not clear on their meaning.

### Poisoning Language Models During Instruction Tuning

#### [Submission URL](https://arxiv.org/abs/2305.00944) | 83 points | by [hardmaru](https://news.ycombinator.com/user?id=hardmaru) | [4 comments](https://news.ycombinator.com/item?id=35801673)

A new paper titled "Poisoning Language Models During Instruction Tuning" warns that adversaries can contribute poisoned examples to instruction-tuned language models (LMs), allowing them to manipulate model predictions whenever a desired trigger phrase appears in the input. By using as few as 100 poison examples, the model struggles to classify, summarize, edit, or translate that input accurately. The paper also shows that larger LMs are increasingly vulnerable to poisoning and that existing defenses provide only moderate protection while reducing test accuracy, raising concerns about the robustness and security of these models.

The discussion around this submission includes three comments. The first commenter shared a headline about a WhatsApp landing court in India and also mentioned some important points about dynamic malware detection and LLM. The second commenter mentioned something about a Manchurian GPU, but the context of this comment is unclear. The third commenter shared a meme about the background of the researchers involved in the creation of the paper, which features a clown and a conspiracy theory involving George Soros.

### SparseGPT: Language Models Can Be Accurately Pruned in One-Shot

#### [Submission URL](https://arxiv.org/abs/2301.00774) | 209 points | by [tosh](https://news.ycombinator.com/user?id=tosh) | [62 comments](https://news.ycombinator.com/item?id=35804556)

Researchers have developed a new pruning technique, called SparseGPT, that can efficiently and accurately reduce the size of large-scale generative pretrained transformer (GPT) models without impacting their accuracy. The method can prune models at least 50% sparsity in one-shot without retraining, such that over 100 billion weights from the models can be ignored at inference time. The project team could execute SparseGPT on the largest available open-source GPT-family models in less than 4.5 hours, making the method compatible with semi-structured patterns and weight quantization approaches.	Code for SparseGPT is available for use at GitHub.

Comments discuss the use of L1 regularization and random pruning techniques, the benefits and tradeoffs of different methods for training, and the practical applications of compressed models for inference. One commentator references a larger model than GPT-3 called PaLM 540B, which generates exciting possibilities for future research.

### The Full Story of Large Language Models and RLHF

#### [Submission URL](https://www.assemblyai.com/blog/the-full-story-of-large-language-models-and-rlhf/) | 107 points | by [pk3](https://news.ycombinator.com/user?id=pk3) | [20 comments](https://news.ycombinator.com/item?id=35803522)

This article provides a thorough overview of language models, from their fundamental ideas to the latest advancements. Language models are probabilistic models designed to learn statistical patterns in natural language, and they can predict the most probable words to follow a given input sentence. They are trained through self-supervised learning, a process that uses unannotated text to generate labels for training. One way to unlock the potential of language models is via the process of fine-tuning, which refines and adapts their knowledge to more specialized domains. However, the risks and misuse of language models have become a primary concern, leading to a demand for methods such as Reinforcement Learning from Human Feedback (RLHF) to control and steer these large-scale AI systems.

The discussion revolves around the potential of AI to find vulnerabilities in software and the ethics of using AI to find weaknesses that can be exploited by hackers. The conversation also touches on the importance of finding vulnerabilities in software and the cost of fixing them. Additionally, the community discusses the role of machine learning in finding and fixing vulnerabilities and the importance of transferring learning data to scale AI. Another topic of discussion was the advancements in language models and their potential to revolutionize the field of natural language processing. Finally, Reinforcement Learning from Human Feedback (RLHF) for training Language Models (LLMs) is highlighted as a critical development in the field of AI.
