## AI Submissions for Fri Mar 29 2024 {{ 'date': '2024-03-29T17:10:04.551Z' }}

### Qwen1.5-Moe: Matching 7B Model Performance with 1/3 Activated Parameters

#### [Submission URL](https://qwenlm.github.io/blog/qwen-moe/) | 102 points | by [GaggiX](https://news.ycombinator.com/user?id=GaggiX) | [8 comments](https://news.ycombinator.com/item?id=39867551)

The Qwen Team has introduced the Qwen1.5-MoE-A2.7B model, which boasts impressive performance metrics comparable to state-of-the-art 7B models while utilizing only one-third of the activated parameters, resulting in decreased training costs and faster inference speed. By implementing a unique MoE architecture with enhancements like fine-grained experts and a refined routing mechanism, they achieved a 75% decrease in training expenses and a 1.74x acceleration in inference speed. The model's architecture includes 64 fine-grained experts, an efficient initialization process called "upcycling," and a novel routing mechanism with shared and routing-specific experts. Through thorough evaluations on benchmark datasets, Qwen1.5-MoE-A2.7B demonstrated competitive performance against leading 7B models like Mistral-7B and Gemma-7B, as well as other MoE models. Notably, the model excelled in various assessments, including language understanding, mathematics, and multilingual proficiency.

Despite the model's efficiency and cost-effectiveness, the team aims to further enhance finetuning strategies for MoE models, particularly in the domain of chat models. By reducing training costs through sparse parameter utilization and optimizing performance, Qwen1.5-MoE-A2.7B showcases the potential for advancing MoE model research and application.

- **cchnc** and **klqt** discuss the question of smaller MoE diffusion models, with **klqt** pointing out that the modified models serve different purposes and need to be addressed accordingly.
- **rdq** raises concerns about the activation of 13rd parameters requiring 2 times VRAM, leading to a conversation with **YetAnotherNick** highlighting the trade-offs in MoE models between sacrificing VRAM and computational resources.
- **Havoc** mentions correcting the template for EOS tokens.
- **trnsfrm** shares a comparison involving higher MMLU and GSM8k scores for ph-2, leading to a response from **sp332** rationalizing the statistics provided and pointing to a Microsoft blog post for further details.
- Lastly, **hdlktrpc** points out dead links (404 errors) in the discussion.

### OpenVoice: Versatile instant voice cloning

#### [Submission URL](https://research.myshell.ai/open-voice) | 439 points | by [ulrischa](https://news.ycombinator.com/user?id=ulrischa) | [235 comments](https://news.ycombinator.com/item?id=39861578)

OpenVoice is making waves in the world of instant voice cloning by offering a versatile approach that can replicate a speaker's voice using just a short audio clip. This innovative technology goes beyond just mimicking the voice, allowing for control over various aspects like emotion, accent, rhythm, and intonation. Notably, OpenVoice also excels in cross-lingual voice cloning, enabling the generation of speech in languages not covered in the training dataset, all while being more computationally efficient than existing solutions. The project's technical report and source code are available for exploration, promising exciting possibilities for the future of voice cloning technology.

The discussion on Hacker News covers various aspects of the OpenVoice project, instant voice cloning technology, hardware capabilities, and AI development. Users are exploring different tools like XTTS2, Gradio, and RVCProject for voice cloning and speech generation, comparing their performance and limitations. Some users share their experiences with setting up AI models on gaming PCs and recommending hardware like the Nvidia P40 for AI workloads. Discussions also touch on AI benchmarks, AI servers, and potential challenges in the realm of AI and hardware integration. Furthermore, there are conversations about the ethical implications of advanced AI technology, such as the potential misuse of realistic voice clones and the impact on personal identity and privacy. Some users reflect on the implications of AI-generated voices in personal interactions and entertainment, referencing real-world examples and cultural influences. Overall, the comments showcase a mix of technical insights, practical experiences, ethical considerations, and speculative discussions related to AI, voice cloning, and hardware infrastructure.

### TnT-LLM: Text Mining at Scale with Large Language Models

#### [Submission URL](https://arxiv.org/abs/2403.12173) | 64 points | by [PaulHoule](https://news.ycombinator.com/user?id=PaulHoule) | [7 comments](https://news.ycombinator.com/item?id=39869603)

The paper titled "TnT-LLM: Text Mining at Scale with Large Language Models" introduces a two-phase framework that leverages Large Language Models (LLMs) to automate the process of generating and assigning labels to unstructured text. The framework, applied to analyzing user intent and conversational domain for Bing Copilot, demonstrates improved accuracy and efficiency in label taxonomy generation and classification compared to existing methods. The authors highlight the potential of LLMs for large-scale text mining applications, showcasing the benefits of using advanced language models in real-world scenarios.

The comments on the Hacker News discussion thread revolve around the effectiveness and implications of the TnT-LLM framework introduced in the paper. 
- One user mentions the extensive experiments showcasing how TnT-LLM generates correct relevant label taxonomies efficiently compared to existing methods, particularly in unstructured data scenarios like surveillance and information prediction. The user believes that the framework is effective in creating detailed databases for various applications, such as behavior prediction in insurance companies.
- Another user brings up Enron's fraudulent activities, mentioning how the TnT-LLM-like projects could uncover hidden insights and interactions between individuals and organizations, even touching on surveillance topics like Edward Snowden's revelations in the IT industry.
- There is a discussion about the challenges of training Machine Learning (ML) models, particularly around text and visual models, the need for larger models, and the potential benefits of using Microsoft Copilot to leverage larger LLMs like GPT-4 for training qualitative text models efficiently.
- The conversation further explores issues related to the reinforcement and correction mechanisms in large language models, with some users suggesting surgical weight removal from models and highlighting the importance of diverse training sets in model development to avoid negative feedback loops.

### AutoBNN: Probabilistic Time Series Forecasting

#### [Submission URL](https://blog.research.google/2024/03/autobnn-probabilistic-time-series.html?m=1) | 57 points | by [simonpure](https://news.ycombinator.com/user?id=simonpure) | [3 comments](https://news.ycombinator.com/item?id=39862828)

Google Research has introduced AutoBNN, a groundbreaking open-source tool that combines the interpretability of traditional Bayesian approaches with the power and scalability of neural networks for time series forecasting. This new package, written in JAX, automates the process of discovering interpretable forecasting models and provides reliable uncertainty estimates, all while delivering efficient performance on large datasets. AutoBNN utilizes a compositional structure, similar to Gaussian processes, with learned Bayesian neural networks (BNNs) that replace GPs while retaining the compositional kernel architecture. The BNNs offer advantages over GPs, such as improved computational efficiency, better hardware acceleration compatibility, and the ability to seamlessly integrate with deep neural networks for feature discovery.

By leveraging techniques such as Sequential Monte Carlo and incorporating innovative components like a OneLayer kernel, a ChangePoint operator, and a WeightedSum operator with learnable mixing weights, AutoBNN enables users to perform "soft" structure discovery by training a linear combination of various potential models. Overall, AutoBNN brings together the best of both worlds by enhancing predictive accuracy, interpretability, and scalability in time series forecasting, making it a valuable addition to the toolkit of data scientists and researchers working in this domain.

1. **HuShifang**: HuShifang emphasizes the advantages of using Bayesian Neural Networks (BNNs) over Gaussian Processes (GPs) for time series forecasting. They mention that training large GPs can be computationally expensive due to the traditional training algorithms scaling poorly with the number of data points in the time series. In contrast, training BNNs scales approximately linearly with the number of data points. BNNs are also well-suited for hardware acceleration on GPUs and TPUs. They suggest that Hilbert Space Gaussian Processes (HSGPs) might be relevant in this context and highlight their potential benefits over conventional GPs, such as improved performance. However, they point out that HSGPs lack domain expertise and suggest that AutoBNN could be a suitable alternative.

2. **chaz6**: chaz6 mentions their work on developing a non-parametric matrix model for the National Grid to predict gas supplies. The model considers various factors like weather-related variables, temperature, wind, and public holidays to forecast against the current Service Level Agreement (SLA) performance. This approach aims to enhance the response to variable factors affecting gas reserves.

3. **mlndnky**: mlndnky expresses amazement at the rapid pace of advancements in time series frameworks. They find the theoretical aspects intriguing but suggest that in practice, it may be challenging to implement and benefit from such cutting-edge technologies. They point out the inherent trade-offs between practicality and theoretical sophistication in the field. Additionally, they mention the preferences of the Deep Learning (DL) community for practices like using PyTorch over TensorFlow and suggest exploring new avenues in time series forecasting research that leverage novel techniques like LSTMs.

### Can Demis Hassabis save Google?

#### [Submission URL](https://www.bigtechnology.com/p/can-demis-hassabis-save-google) | 129 points | by [laurex](https://news.ycombinator.com/user?id=laurex) | [124 comments](https://news.ycombinator.com/item?id=39866795)

Demis Hassabis, the renowned founder of DeepMind, is now at the helm of Google's AI research efforts, aiming to keep the tech giant at the forefront of innovation. With a string of groundbreaking achievements under his belt, including mastering games like Go with AlphaGo and decoding proteins with AlphaFold, Hassabis faces the challenge of translating these successes into practical advancements for Google's multi-trillion-dollar business. Despite past setbacks, his colleagues and collaborators believe he is well-suited for the task. Hassabis' journey from a young chess prodigy to a leader in AI showcases his brilliance and strategic vision. As he navigates the complex landscape of AI within Google, all eyes are on him to see if he can lead the company to continued success in the rapidly evolving tech world.

The discussion on Hacker News regarding the submission about Demis Hassabis taking the lead of Google's AI research efforts digs deep into the challenges and potential of large language models (LLMs). Users share insights on the limitations of LLMs in tasks like game playing and the need to tackle problems with reward mechanisms and interpretability. There is also a debate about the ability of AI models to mimic human voters and the relevance of social media sentiment. Furthermore, the conversation delves into recent research on generating coherent thoughts by LLMs, the training and efficiency of LLMs for specific problems, as well as approaches like "Chain Thought." Discussions also touch upon the leadership dynamics at Google, with contrasting views on Sundar Pichai and suggestions for potential strategies and changes within the company. Additionally, comparisons are made between OpenAI and Google in terms of technological advancements and strategic positioning in the AI landscape.

### Maersk names first vessel of its large methanol-enabled fleet "Ane Maersk"

#### [Submission URL](https://www.maersk.com/news/articles/2024/01/26/maersk-names-first-vessel-of-its-large-methanol-enabled-fleet-ane-maersk) | 54 points | by [doener](https://news.ycombinator.com/user?id=doener) | [72 comments](https://news.ycombinator.com/item?id=39861391)

Maersk, a global leader in logistics services, has named its first large methanol-enabled container vessel "Ane Mærsk" at a ceremony in Ulsan, South Korea. This vessel is part of Maersk's commitment to pioneering low-emissions shipping solutions and marks a significant milestone in their sustainability efforts. The innovative design of the vessel positions the bridge and accommodation at the front, ensuring fuel-efficient operations. "Ane Mærsk" and her sister vessels will operate on green methanol, contributing to Maersk's goal of reaching net zero emissions by 2040. This move demonstrates Maersk's dedication to a more sustainable industry and their commitment to reducing emissions in supply chains.

The discussion on the submission about Maersk's first methanol-enabled container vessel on Hacker News covers various aspects of the use of methanol in shipping, comparing energy densities between methanol and diesel fuel, the potential challenges and benefits of using methanol in the industry, and considerations for alternative energy sources like nuclear power, batteries, and hydrogen. There are concerns about the energy density of methanol and its practicality for large cargo ships, with comparisons to other alternative fuels and their feasibility for long-distance shipping. The conversation also delves into the logistics and environmental impact of different fuel options, as well as the potential economic incentives and regulatory frameworks for promoting sustainable shipping practices. Some users bring up the importance of considering emissions from the entire supply chain and the need for a comprehensive approach to reducing greenhouse gas emissions in the shipping industry.

