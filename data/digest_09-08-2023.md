### GitHubGuessr

#### [Submission URL](https://github-guessr.vercel.app/) | 115 points | by [tan-z-tan](https://news.ycombinator.com/user?id=tan-z-tan) | [49 comments](https://news.ycombinator.com/item?id=37432067)

Introducing GitHub-Guessr, a fun and exciting game that will put your coding knowledge to the test! Can you guess the correct GitHub repository just by looking at the code? Get ready to showcase your skills and prove that you're a true code sleuth.

GitHub-Guessr presents you with snippets of code taken from various repositories. Your mission is to analyze the code and guess which repository it belongs to. Each correct guess earns you points and brings you closer to becoming the ultimate GitHub-Guessr champion.

Whether you're a seasoned developer or a coding enthusiast, this game offers a thrilling challenge that will keep you on your toes. It's a great way to explore different coding styles, learn from other developers, and discover interesting projects hosted on GitHub.

So, get your detective hat on, flex your coding muscles, and dive into the world of GitHub-Guessr. Can you guess the GitHub repository from the code? Start playing now and let the coding adventure begin!

In the discussion on this submission, there are several threads of conversation. 

One commenter, gshgg-blk, expressed appreciation for the game but pointed out that it would be helpful to have more information or references when trying to guess the repositories. Another commenter, stvbmrk, apologized for not reading the dropdown options correctly and offered suggestions for improving the game by implementing autocomplete and sharing buttons.

dysc shared three suggestions: making it easier to select options, gradually increasing the difficulty by introducing more complex puzzles, and adding a share button to challenge friends.

tn-z-tn responded to dysc's suggestions, stating that they had implemented some improvements but found it challenging to change the selection style. FireInsight commented on the unbalanced character distribution in a Swift code snippet.

arh68 simply found the game interesting, while trln and zX41ZdbW provided feedback on minor issues they encountered while playing.

smrphc- suggested improving the dropdown guess box by sorting the options alphabetically, and tn-z-tn agreed.

sxstrngthry praised the circular perspective view concept and suggested using smaller options to make scrolling through the list more convenient.

tn-z-tn created the GitHub-Guessr game and shared their thoughts on its development. Other commenters, such as wbdvvr and jtwlsn, offered suggestions for improving the game's functionality and expressed interest in using similar tools in their work.

lnkr suggested including reverse engineering puzzles that target web development. And cglng found the game challenging but admitted to being unfamiliar with most of the listed projects.

bllq clarified a confusion regarding scrollbar visibility in the MongoDB repository, and skttr pointed out that sometimes the highlighted code moment moves unexpectedly.

nvdmf mentioned a sudden scroll jump issue, and 38 reported a compatibility error with Firefox.

Overall, the discussion included suggestions for improvements, feedback on code snippets, and appreciation for the concept of the game.

### TSMC warns AI chip crunch will last another 18 months

#### [Submission URL](https://www.theregister.com/2023/09/08/tsmc_ai_chip_crunch/) | 156 points | by [rntn](https://news.ycombinator.com/user?id=rntn) | [94 comments](https://news.ycombinator.com/item?id=37432948)

TSMC has warned that the chip shortage for high-spec GPUs, such as Nvidia's A100 and H100, will continue until at least the end of 2024. The issue lies with the lack of advanced packaging capacity, which is used to assemble the silicon chips. TSMC can currently only meet around 80% of the demand for its chip on wafer on substrate (CoWoS) packaging technology, particularly used in chips for AI purposes. TSMC expects additional CoWoS capacity to be available within the next 18 months. In the meantime, the shortage will impact the availability of Nvidia's H100 and A100, as well as AMD's upcoming Instinct MI300-series accelerators.

The discussion on Hacker News revolves around the chip shortage and its impact on the availability of high-spec GPUs. Some users speculate about the reasons for the shortage, with one user noting that TSMC's packaging technology and capacity are the main bottlenecks. Others discuss the involvement of Chinese companies and the use of stolen intellectual property. There is also a discussion on the availability and pricing of Raspberry Pi boards, with some users pointing out the challenges faced by hobbyists due to increased prices. The topic of global chip manufacturing capacity and demand is also debated, with some users suggesting that increased supply would lead to smaller profit margins. There are also discussions about the German region of Saxony and its perceived xenophobia, as well as the issues related to staffing and assembly in the chip manufacturing industry. Overall, the discussion covers various aspects of the chip shortage, its causes, and its impact on different segments of the market.

### Show HN: Rivet â€“ open-source AI Agent dev env with real-world applications

#### [Submission URL](https://rivet.ironcladapp.com/) | 160 points | by [gogwilt](https://news.ycombinator.com/user?id=gogwilt) | [30 comments](https://news.ycombinator.com/item?id=37433218)

Ironclad, the leading digital contracting platform, has released Rivet, a visual programming environment for building AI agents with large language models (LLMs). Rivet allows teams to effectively design, debug, and collaborate on complex LLM prompt graphs, and deploy them in their own environment. It provides a visual interface to visualize and build complex chains for AI applications, a remote debugger to observe prompt chain execution in real-time, and the ability to version and review prompt graphs using YAML files. The community has praised Rivet's game-changing nature, its ability to address limitations and facilitate collaboration, and its power in rapidly prototyping and understanding complex AI workflows. Rivet has already been used by Ironclad to develop their virtual contract assistant, powered by AI agents. To get started with Rivet, the Getting Started guide, integration with Node or TypeScript applications, and an example application are available.

The discussion on Hacker News surrounding the submission about Rivet, an open-source visual AI programming environment, covered various topics.

One user mentioned their interest in a native Rust and TypeScript solution for Rivet and asked if there were plans to add plugins to dynamically change and manage prompt graphs. Another user replied, stating that Rivet does not have built-in chat GPT plugins and that prompt graphs are explicitly built by publishing prompt chains as a publicly available plugin.

Another user shared that they have been following two similar services that solve the problem of building workflows with large language models and recommended checking out FlowiseAI and lgspc-lngflw on GitHub.

Another user expressed excitement about trying Rivet and mentioned considering supporting open-source language models, inference servers, and LocalAI.

Someone else mentioned that they recently open-sourced a platform for building workflows with language models visually and locally and provided a link for it.

Another user stated that they haven't seen this kind of support in a while and highlighted the importance of requirements for production applications.

Someone else praised the work done and mentioned the usefulness of the dev tools for structuring functions and integrating powerful tools for careful spending in a controlled manner.

Another commenter mentioned that they have implemented AI features recently and found Rivet to be a perfect match for building large language model applications.

A user expressed their support for Rivet in building large language model applications and mentioned collaborating with the Ironclad team on integrating user experience paradigms.

Another user mentioned their interest in real-world projects and suggested open-sourcing the implementation and possibly using multiple APIs for specific purposes.

Someone else mentioned their excitement about trying AI efforts and praised Rivet's applicability in the field.

Another commenter asked if Rivet supports TypeScript and plans to have a desktop version, to which the Rivet team clarified that while it runs locally on a web app, there are plans to add a desktop version in the future.

Several comments congratulated the Rivet team on the launch and expressed support for the project.

Lastly, there was a brief discussion about the target audience for Rivet, with one user asking if it targets enterprise startups. The response clarified that Rivet is designed for growth-stage startups and partners who collaborate with Ironclad.

Overall, the discussion highlighted enthusiasm for Rivet's capabilities and potential applications, as well as requests for additional features and integrations with different technologies.

### Show HN: Find jobs at top AI startups

#### [Submission URL](https://workinai.xyz/) | 27 points | by [himanshujaju](https://news.ycombinator.com/user?id=himanshujaju) | [5 comments](https://news.ycombinator.com/item?id=37435689)

Looking to land a job at one of the hottest AI companies? Well, you're in luck! WorkinAI.xyz has compiled a comprehensive list of over 450 job openings at more than 20 AI companies. This is your chance to get in on the cutting edge of technology and work alongside some of the brightest minds in the industry. Just make sure to view the page on a desktop, as it allows you to easily sort, filter, and search through the available positions. And if you have any suggestions, feedback, or inquiries, don't hesitate to reach out to hello@workinai.xyz. Get ready to take the next big step in your career!

The comments on the submission are brief but positive. One user suggests starting with a small company to gain experience, another user expresses happiness and shares the link on Twitter, one user thanks the OP for the helpful job application, and another user commends the work and hopes that it expands.

### We built an AI-powered Magic the Gathering card generator

#### [Submission URL](https://txt.cohere.com/urzas-ai/) | 126 points | by [MWil](https://news.ycombinator.com/user?id=MWil) | [85 comments](https://news.ycombinator.com/item?id=37427854)

Magic the Gathering (Magic) is a beloved collectible card game that has captivated players for years with its unique asymmetrical gameplay. However, one complaint is that there aren't enough cards to satisfy the players' thirst for new content. That's where Urza's AI comes in. Urza's AI is a website created by three individuals who wanted to generate more Magic cards using the power of artificial intelligence (AI). The team utilized a combination of language AI and text-to-image AI to generate playable Magic cards based on prompts. First, they used a large language model (LLM) to generate text information about the card, such as its cost, type, subtype, and description. They finetuned the model with a dataset of existing Magic card descriptions, leading to more realistic and playable card outputs.

But a Magic card isn't complete without an accompanying image. To generate card images, the team employed the Wombo Art API, which uses text input and image output. They provided the API with the card name, types, and subtypes, resulting in impressive and thematic card illustrations. Not stopping there, the team also used AI to generate the card back and Mana icons. By creating prompts and leveraging the Wombo API, they successfully produced card backs and Mana icons that complemented the generated cards.

With all the components ready, the team built the Urza's AI website where users can enter a card name and have a complete card rendered. The outcome has been astounding, with thousands of visitors trying out the site within the first four days of its launch. Magic enthusiasts and players now have access to an ever-expanding pool of custom Magic cards, giving them the opportunity to explore new strategies and deck combinations. Urza's AI has tapped into the power of AI to bring excitement and novel experiences to the Magic community.

The discussion surrounding the Urza's AI submission on Hacker News covers various aspects of the topic:

- Some commenters express their appreciation for the quality and capabilities of the AI-generated Magic cards. They highlight the impressive text-to-image generation and note that the cards are suitable for standard, historic, and explorer formats.

- Others are amazed by the use of AI in generating card backs and Mana icons for the Magic cards. They commend the team's work and express gratitude for the exciting possibilities this brings to the Magic community.

- Commenters discuss the potential legal issues surrounding AI-generated cards and the involvement of Wizards of the Coast (WOTC), the company behind Magic the Gathering. One user mentions a previous AI-generated Magic card project called RoboRosewater.

- The topic of AI playing Magic the Gathering is also touched upon. Some users mention instances where AI players have defeated professional human players, highlighting the capabilities of AI in the game.

- The balance and power level of AI-generated cards are also a point of discussion. Commenters debate the ability of AI to generate cards with comparable power levels and the potential impact on the game's balance.
- The topic diverges to discuss AI and machine learning in creating game content in general. Users share insights on AI-driven card generation, win conditions in games, and the challenges of achieving proper balance.
- A few commenters draw parallels between AI-generated Magic cards and existing Magic cards, highlighting similarities and discussing potential strategies and interactions.
- The conversation also touches on the consideration of AI-generated Magic cards as collectibles and potential market value.
- One user jokingly mentions the association of Magic the Gathering with MtGox, a defunct cryptocurrency exchange platform, due to the similar abbreviation.
- The discussion concludes with references to related articles and posts from previous years that explored similar AI-generated Magic cards.

Overall, the discussion on Hacker News explores the excitement and implications of AI-generated Magic cards, delves into gameplay and balance considerations, and provides references to additional resources for those interested in the topic.

### NVIDIA introduces TensorRT-LLM for accelerating LLM inference on H100/A100 GPUs

#### [Submission URL](https://developer.nvidia.com/blog/nvidia-tensorrt-llm-supercharges-large-language-model-inference-on-nvidia-h100-gpus/) | 67 points | by [mkaushik](https://news.ycombinator.com/user?id=mkaushik) | [21 comments](https://news.ycombinator.com/item?id=37439280)

NVIDIA has announced the upcoming release of TensorRT-LLM, an open-source software for accelerating large language model (LLM) inference on NVIDIA GPUs. The software, which integrates the TensorRT deep learning compiler, includes optimized kernels, pre- and post-processing steps, and multi-GPU/multi-node communication primitives for improved performance. TensorRT-LLM aims to make LLMs more accessible and customizable for developers by providing a modular Python API for defining, optimizing, and executing new architectures. It also supports in-flight batching, allowing for efficient execution of dynamic workloads with varying output sizes. The software has already been integrated by several leading companies and has demonstrated significant performance improvements on NVIDIA GPUs.

The discussion around NVIDIA's announcement of TensorRT-LLM on Hacker News includes various comments from users discussing different aspects of the software.

One user mentions the availability of optimized versions of well-known LLMs like OpenAI's GPT-2 and GPT-3 in TensorRT-LLM. Another user asks about the availability of special weights, to which another user responds that the weights for GPT-3 have not been released openly.

Another user comments that TensorRT-LLM is available through the NVIDIA NeMo framework, which is part of the NVIDIA AI Enterprise software platform. They also mention that developers and researchers can access TensorRT-LLM through the NVIDIA Developer Program.

The efficiency and performance of TensorRT-LLM are discussed by users. One user mentions that acceleration work on H100/A100 GPUs can improve performance by 30-40 times compared to regular GPUs. Another user questions why there is only a 15GB/s communication rating on PCIe for TensorRT-LLM, to which another user responds that it is due to the difference in GPU-to-GPU communication capabilities between NVLink and PCIe.

There are also comments about compatibility and hardware requirements. One user notes that future versions of TensorRT may require higher-end GPUs. Another user shares a link to a GitHub issue where someone mentioned that TensorRT-LLM did not work well with the 30-series GPUs.

The discussion also touches on the flexibility and extensibility of TensorRT-LLM. One user points out that it provides a modular Python API for defining and optimizing new architectures, making it customizable with deep knowledge of C++ and NVIDIA CUDA.

The comparison between NVIDIA and AMD is brought up by a user commenting that while AMD may have caught up with NVIDIA in some areas, NVIDIA still has a more mature and fully optimized software stack.

Another user brings up the similarity between TensorRT-LLM and vLLM (very Large Language Model), mentioning that vLLM uses intermediate-level tensor parallelism and that both technologies have their own advantages and applications.

Lastly, there is a brief discussion about the cost of running H100 and A100 GPUs on the cloud, with one user mentioning that H100 costs around $4 per hour and A100 costs around $2 per hour on the public market.

### Scientific sleuths spot dishonest ChatGPT use in papers

#### [Submission URL](https://www.nature.com/articles/d41586-023-02477-w) | 93 points | by [pseudolus](https://news.ycombinator.com/user?id=pseudolus) | [81 comments](https://news.ycombinator.com/item?id=37431946)

Researchers have been using OpenAI's ChatGPT to write scientific papers without disclosing its use, leading to concerns about the integrity of peer-reviewed publications. ChatGPT is an AI chatbot that generates fluent text in response to user prompts. However, some researchers have been found to have used ChatGPT to help draft their manuscripts without declaring it. In one case, a paper was retracted by the journal Physica Scripta because the authors did not disclose their use of the tool. Since April, more than a dozen journal articles have been flagged for containing telltale ChatGPT phrases without proper disclosure. This raises concerns about the widespread use of AI language models in scientific publishing.

The discussion on this submission covers a range of topics related to the use of AI in scientific publishing. Some commenters express frustration with the current state of academic journals, noting that they are slow, expensive, and often prioritize profit over the dissemination of research. Others argue that the current peer review system is flawed and prone to biases and conflicts of interest. Some commenters defend the anonymity of peer reviewers, while others suggest alternative systems for collecting review data.

There is also a discussion about the use of AI in academia, with some commenters arguing that it can be a helpful tool for researchers, while others express concerns about its impact on the integrity of scientific publications. Some commenters posit that the problem lies not with AI itself, but with the lack of transparency and honesty among researchers in disclosing their use of AI tools.

Other points raised include the need for better document hosting services and functionality, the importance of replication in scientific publishing, and the systemic issues within the university system that may contribute to the current state of academic publishing.

Overall, the discussion reflects a variety of perspectives on the use of AI in scientific publishing and raises important questions about transparency, integrity, and the future of the peer review system.

### Large Language Models as Optimizers. +50% on Big Bench Hard

#### [Submission URL](https://arxiv.org/abs/2309.03409) | 92 points | by [og_kalu](https://news.ycombinator.com/user?id=og_kalu) | [33 comments](https://news.ycombinator.com/item?id=37434069)

Researchers from Google AI have proposed a new approach called Optimization by PROmpting (OPRO) that leverages large language models (LLMs) as optimizers for solving complex optimization problems. Traditional derivative-based optimization algorithms face challenges when dealing with tasks that lack gradients. OPRO uses LLMs to generate new solutions based on a prompt that contains previously generated solutions. These new solutions are then evaluated and added to the prompt for the next optimization step. The researchers demonstrate the effectiveness of OPRO on linear regression and traveling salesman problems. They also show that prompts optimized by OPRO outperform human-designed prompts by up to 8% on GSM8K and up to 50% on Big-Bench Hard tasks. The paper, "Large Language Models as Optimizers," is available for download.

The discussion surrounding the submission about Google AI's OPRO approach includes various viewpoints and comments. One commenter argues that traditional derivative-based optimization algorithms cannot handle tasks without gradients, but another points out that these distinctions are inventions and not necessarily applicable in the realm of technology. Another user brings up Microsoft's Tay as an example of the unpredictability of training models. 

There is also a discussion about the importance of understanding the underlying system and the role of advanced technology in distinguishing it from magic. Some users emphasize the potential capabilities of language models (LLMs), while others express the difficulty in comprehending LLMs and their workings due to the lack of mathematical details. 

Another user mentions Charlie Stross's similar submission from a month ago, which leads to a brief conversation about whether one can credibly learn a service in a short period of time. Another thread veers off-topic into discussing Skyrim, the internet in general, and breaking large problems into smaller ones. 

The next set of comments focuses on the paper itself. One commenter highlights that OPRO outperforms human-designed prompts, while another comments on the difficulty in understanding the paper beyond its "academic crust." There are some exchanges regarding the process of optimization and the potential of LLMs to handle optimization problems. One user suggests that LLMs might be able to offer insights into proving optimization problems, while another brings up the distinction between text-based models like PaLM and 2L-Thought. 

There is further discussion about the challenges of providing good prompts and the potential for LLMs to help explain their thinking process. One user mentions their related work on black-box optimization using LLMs. Finally, there is a brief conversation about solving Sudoku puzzles and the potential applications of LLMs in solving complex problems.

