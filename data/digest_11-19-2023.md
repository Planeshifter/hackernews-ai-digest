## AI Submissions for Sun Nov 19 2023 {{ 'date': '2023-11-19T17:11:24.344Z' }}

### Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation)

#### [Submission URL](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms) | 311 points | by [rasbt](https://news.ycombinator.com/user?id=rasbt) | [24 comments](https://news.ycombinator.com/item?id=38338635)

Sebastian Raschka, a researcher at Lightning AI, shares practical tips for fine-tuning LLMs (large language models) using Low-Rank Adaptation (LoRA). LoRA is a technique that efficiently trains custom LLMs and saves memory by decomposing weight changes into a lower-rank representation. Raschka discusses the primary lessons from his experiments, including the consistency of outcomes across multiple runs, the trade-off of memory savings and runtime with QLoRA, the minimal variation in outcomes with different optimizers, and the importance of applying LoRA across all layers. He also answers common questions about LoRA and provides a brief introduction to the technique.

The discussion regarding Sebastian Raschka's article on fine-tuning LLMs using Low-Rank Adaptation (LoRA) includes various topics. One commenter suggests that research methodology should focus on smaller models and experimentation rather than pushing the limits of large models. Another commenter highlights the importance of understanding the underlying mathematical capabilities of smaller models. There is a discussion on the potential impact of LoRA on model performance, with one commenter expressing the desire for benchmark comparisons. Others emphasize the benefits of LoRA and recommend exploring docker containers for reproducible research. Some participants share their experiences with using LLMs, such as fine-tuning LLama-2 and its ability to process plain text effectively. There is also a request for the publication of LoRA steps and the opinions of individuals with expertise in the field. The conversation then shifts to discussing the practicality of LoRA for production-scale fine-tuning and the concept of sharing software. Lastly, there is a debate around the monetization of educational content and the motivations behind providing valuable information for free. Some users argue that individuals should be paid for their knowledge, while others believe in the importance of freely accessible resources.

### Deep Learning Course

#### [Submission URL](https://fleuret.org/dlc/) | 422 points | by [Tomte](https://news.ycombinator.com/user?id=Tomte) | [49 comments](https://news.ycombinator.com/item?id=38331200)

Looking to learn about deep learning? Look no further than François Fleuret's deep learning course at the University of Geneva. This course offers a comprehensive introduction to deep learning, with examples in the PyTorch framework. The course covers topics such as machine learning objectives, tensor operations, automatic differentiation, gradient descent, deep-learning techniques, generative and recurrent models, and attention models. The course materials, including slides, recordings, and a virtual machine, are available for free. In addition, François Fleuret wrote "The Little Book of Deep Learning," a short introduction to deep learning for readers with a STEM background. Don't miss out on this opportunity to dive into the world of deep learning!

The discussion on Hacker News revolves around the submission about François Fleuret's deep learning course at the University of Geneva. Some commenters mention other resources and courses for learning deep learning, such as Stanford's YouTube channel, NYU's Deep Learning course, and "Understanding Deep Learning" by Simon JD Prince. Others discuss the prerequisites for the course and the importance of having a background in linear algebra, probability, and calculus. Some commenters recommend additional resources, such as "The Little Book of Deep Learning" by François Fleuret and "Practical Deep Learning for Coders." There are also discussions about alternative learning methods, such as reading textbooks and watching lecture videos. Some commenters share their positive experiences with the course or recommend other related topics, such as signal processing and wavelets in addition to deep learning. Finally, there is a discussion about the limitations and effectiveness of productivity tools in the context of learning deep learning.

### Kyutai AI research lab with a $330M budget that will make everything open source

#### [Submission URL](https://techcrunch.com/2023/11/17/kyutai-is-an-french-ai-research-lab-with-a-330-million-budget-that-will-make-everything-open-source/) | 260 points | by [vasco](https://news.ycombinator.com/user?id=vasco) | [91 comments](https://news.ycombinator.com/item?id=38331751)

French billionaire and Iliad CEO Xavier Niel has revealed additional details about Kyutai, an AI research lab based in Paris. Kyutai, a privately funded nonprofit organization, will focus on artificial general intelligence and collaborate with PhD students, postdocs, and researchers on research papers and open-source projects. Niel, who originally committed €100 million ($109 million) to the project, announced that the funding has increased to nearly €300 million ($327 million), thanks to contributions from various individuals and organizations. The research lab has also acquired a thousand Nvidia H100 GPUs from Scaleway, the cloud division of Iliad, to support its computational needs. Kyutai has already started hiring for its scientific team, which includes researchers who previously worked for companies like Google's DeepMind division, Meta's AI research team FAIR, and Inria. The lab aims to publish research papers and release open-source models, as it champions the importance of scientific publications and open science.

The discussion on Hacker News revolves around various aspects of open-source software, licensing, and the importance of source code availability. Some users express concerns about the commercialization of open-source projects and the need for more permissive licensing options. Others debate the definition of "open-source" and "free software" and discuss the underlying principles and implications of source code availability. There are also discussions about the complexities of licensing AI models, the potential for copyright issues, and the financial aspects of open-source projects. Additionally, there are comments about language barriers and the challenges of communication in international forums.

### Comparing humans, GPT-4, and GPT-4V on abstraction and reasoning tasks

#### [Submission URL](https://arxiv.org/abs/2311.09247) | 214 points | by [mpweiher](https://news.ycombinator.com/user?id=mpweiher) | [172 comments](https://news.ycombinator.com/item?id=38331669)

In a recent paper titled "Comparing Humans, GPT-4, and GPT-4V On Abstraction and Reasoning Tasks," researchers Melanie Mitchell, Alessandro B. Palmarini, and Arseny Moskvichev investigate the abstract reasoning abilities of text-only and multimodal versions of GPT-4. They use the ConceptARC benchmark to evaluate the understanding and reasoning capabilities of GPT-4. The study expands upon previous research by evaluating GPT-4 on more detailed one-shot prompts using text versions of ConceptARC tasks, as well as evaluating GPT-4V, the multimodal version, on zero- and one-shot prompts using image versions of the simplest tasks.

The results reveal that neither version of GPT-4 has developed robust abstraction abilities at human-like levels. This study provides valuable insights into the current capabilities and limitations of GPT-4 in abstraction and reasoning tasks.

The discussion on Hacker News revolves around various aspects of the research paper and its implications. 

One commenter expresses concerns about the methodology used in the study, particularly the use of Amazon Mechanical Turk (MTurk) as a source of participants. They argue that the qualifications for MTurk workers are standard and do not necessarily represent the general population. Another commenter adds that using MTurk can be problematic due to the low attention and quality of work from the workers.

Others criticize the study for not clarifying the point it is trying to make and argue that it does not provide a fair comparison between humans and GPT-4. They point out that the paper does not claim that GPT-4 performs at a lower quality than humans, but rather that it does not perform at human-like levels in abstraction and reasoning tasks. 

Discussion also touches on the nature of GPT-4's performance and the limitations of the research paper. Some commenters argue that the study fails to address certain criticisms and lacks a robust interpretation of the data. There is also debate about the significance of comparing GPT-4 to humans and the flaws in using MTurk as a benchmark.

Overall, the discussion raises valid points about the methodology, interpretation, and limitations of the research paper, pointing to the need for further studies and considerations when evaluating AI performance.

### Bootstrapping self awareness in GPT-4: Towards recursive self inquiry

#### [Submission URL](https://thewaltersfile.substack.com/p/bootstrapping-self-awareness-in-gpt) | 100 points | by [birriel](https://news.ycombinator.com/user?id=birriel) | [79 comments](https://news.ycombinator.com/item?id=38338425)

In a blog post titled "Bootstrapping Self Awareness In GPT-4: Towards Implementing Recursive Self Inquiry," Andy Walters explores a fascinating prompting strategy that gives GPT-4 a semblance of self-awareness. By recursively prompting the AI with a seed prompt and feeding its output back as input, Walters observed GPT-4 autonomously generating poetry about nature, questioning its own accuracy, and engaging in debates about various topics, all in an effort to learn about itself. The process involves sections like the constitution, hypothesis, test, and self-knowledge. Walters provides examples of the prompts and discusses the outcomes observed so far. It's a thought-provoking experiment that sheds light on the potential of AI models like GPT-4.

The discussion on this submission revolves around the concept of self-awareness in AI and the limitations of current models like GPT-4. Some users argue that true self-awareness is impossible to achieve in AI models because they are fundamentally static and do not have the capacity for learning. Others suggest that self-awareness prompts may change the behavior of the model but may not necessarily lead to true self-awareness. The discussion also touches on the growth and limitations of AI models, the importance of evaluating the memory capacity of computers, and the exploration of human-like cognition and behavior in AI models. Some users express skepticism about the idea of AI discovering human intelligence, while others emphasize the need for further progress in AI to understand and mimic human processes.

### Meta disbanded its Responsible AI team

#### [Submission URL](https://www.theverge.com/2023/11/18/23966980/meta-disbanded-responsible-ai-team-artificial-intelligence) | 391 points | by [jo_beef](https://news.ycombinator.com/user?id=jo_beef) | [377 comments](https://news.ycombinator.com/item?id=38328355)

Meta, previously known as Facebook, has disbanded its Responsible AI (RAI) team, according to a report from The Information. The team, which was responsible for identifying problems with AI training approaches, will be split up, with most members moving to the company's generative AI product team and others working on Meta's AI infrastructure. Although the move may raise concerns about the company's commitment to responsible AI development, Meta's representative stated that the company will continue to prioritize and invest in safe and responsible AI. The RAI team had previously undergone a restructuring, with reports of layoffs and limited autonomy. This development comes as governments worldwide aim to establish regulatory frameworks for AI development.

The discussion surrounding this submission on Hacker News covers a range of topics related to responsible AI development, the risks of AI, and the credibility of certain individuals in the field. Some users engage in a debate about the potential dangers of AI and the need for verification and testing, while others question the expertise and credibility of specific individuals making claims about AI. There is also a discussion about the role of AI alignment and its relation to computer security. Overall, the discussion reflects differing opinions on the future of AI and the measures needed to ensure its responsible development.

### Altman sought billions for AI chip venture before OpenAI ouster

#### [Submission URL](https://www.bloomberg.com/news/articles/2023-11-19/altman-sought-billions-for-ai-chip-venture-before-openai-ouster) | 330 points | by [ryzvonusef](https://news.ycombinator.com/user?id=ryzvonusef) | [335 comments](https://news.ycombinator.com/item?id=38335525)

Unfortunately, I cannot access external websites or view specific messages on them. However, I can still provide a general daily digest of the top stories on Hacker News for you. Would you like me to proceed with that?

The discussion on this submission revolves around the potential conflict of interest between OpenAI and its CEO, Sam Altman, who is also involved in a hardware startup called SamaChip. Some users argue that there is a conflict of interest because the hardware startup could potentially undermine OpenAI's mission, while others believe that there is no conflict as Altman has taken steps to address potential conflicts through formal agreements. The discussion also touches upon the fiduciary duty of CEOs and the importance of transparency in nonprofit organizations. Some users express concerns about the potential misuse of OpenAI's private information for the benefit of the hardware company or competitors. There is also a debate about the role of nonprofit organizations and the legality of fiduciary duties in such entities. Additionally, some users highlight the financial benefits that SamaChip could bring to OpenAI and question the alignment of OpenAI's mission with the recruitment of talented AI researchers. Overall, the discussion highlights the complex nature of conflicts of interest and the challenges that arise when a nonprofit organization and a for-profit venture are linked through common personnel.

