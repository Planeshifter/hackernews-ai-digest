## AI Submissions for Sun Jun 18 2023 {{ 'date': '2023-06-18T17:12:12.467Z' }}

### Show HN: Answer Overflow  – Indexing Discord content into the web

#### [Submission URL](https://www.answeroverflow.com/) | 296 points | by [rhyssullivan1](https://news.ycombinator.com/user?id=rhyssullivan1) | [87 comments](https://news.ycombinator.com/item?id=36383773)

A new open source project called Bringing your Discord channels to Google aims to bring all your Discord channels to your favorite search engine, allowing you to quickly find the information you need. This project uses Answer Overflow to index Discord channels into Google and offers tools tailored to your unique community, including indexing content into Google, AI question answering, community insights, and hosting on your own domain. The project is already serving over 450,000 users across 80 servers and has plans for new features, such as marking questions as solved and advanced analytics.

A new open source project called Bringing your Discord channels to Google intends to enable users to quickly find their required information by indexing Discord channels into Google. This project uses Answer Overflow to perform the indexing and offers tools such as AI question answering, indexing content into Google, community insights, and hosting on one's domain. Some users suggested that Discord began as a secure platform for private communication between friends, but as it has evolved, there seem to be concerns about the lack of privacy and archival functionality. The lack of good search functionality is also mentioned, especially when compared to other chat platforms like Slack and forums like Reddit. Some users also discussed the difficulty of finding relevant information in Discord channels due to the unstructured nature of conversations in Discord. Overall, the project was well-received, and users suggested it might be helpful for gaming and support communities with large Discord channels.

### Migrating Netflix to GraphQL safely

#### [Submission URL](https://netflixtechblog.com/migrating-netflix-to-graphql-safely-8e1e4d4f1e72?gi=4217a3fd9c5c) | 222 points | by [theptip](https://news.ycombinator.com/user?id=theptip) | [194 comments](https://news.ycombinator.com/item?id=36381764)

Netflix recently migrated their iOS and Android apps to GraphQL with zero downtime, replacing their internal API framework, Falcor, which powered their mobile apps. To ensure a safe migration for their millions of customers, the company utilized three testing strategies: AB Testing to compare Falcor versus GraphQL, Replay Testing to ensure idempotent APIs were migrated correctly, and Sticky Canaries to test non-functional requirements like caching and logging user interaction. These techniques allowed for a successful transition to Federated GraphQL, which enables domain teams to manage specific sections of the API independently.

The submission describes how Netflix successfully migrated their iOS and Android apps to GraphQL using three testing strategies for zero downtime and improved flexibility. The comments section includes a debate over whether GraphQL is a good alternative to REST and its potential benefits and drawbacks, including security, versioning, and the level of complexity it adds. Some users argue that GraphQL solves more problems front-end developers face that REST does not address, while others argue that a split between front-end and back-end development is necessary and the communication between the two can become challenging. There are also discussions around GraphQL tooling and optimization and how to ensure satisfaction for both front-end and back-end teams.

### A single line of code made a 24-core server slower than a laptop (2021)

#### [Submission URL](https://pkolaczk.github.io/server-slower-than-a-laptop/) | 506 points | by [xk3](https://news.ycombinator.com/user?id=xk3) | [162 comments](https://news.ycombinator.com/item?id=36376669)

If you've ever found yourself excited by the possibility of a program using more cores for faster processing, only to find the opposite happening, join the club. The author of a Cassandra benchmarking tool called Latte ran into this problem when testing the scalability of his program on a large multiprocessor machine. He ended up integrating the tool with an embedded Rune interpreter that allowed for easy definition of workloads without the need for recompiling the whole program. The resulting scripts used minimal code and were acceptably fast, making the benchmarking program lightweight enough to avoid adding significant overhead in real measurements.

The submitted article discusses the scalability of a Cassandra benchmarking tool called Latte that was tested on a large multiprocessor machine. The author integrated the tool with an embedded Rune interpreter that allowed for easy definition of workloads without recompiling the entire program. In the comments, there is a discussion regarding data structures and parallel processing. Some commenters suggest that data structures are fundamental to solving problems and it's possible to optimize the algorithm by exploiting the hardware, while others share their own experiences with parallel processing and offer tips for improving performance. There is also a debate about the best approach for solving certain algorithmic problems, such as the Traveling Salesman problem.

### The legend of “x86 CPUs decode instructions into RISC form internally” (2020)

#### [Submission URL](https://fanael.github.io/is-x86-risc-internally.html) | 183 points | by [segfaultbuserr](https://news.ycombinator.com/user?id=segfaultbuserr) | [174 comments](https://news.ycombinator.com/item?id=36380149)

An analysis of how different x86 processors handle a simple loop of code has revealed that the idea that they decode complex instructions into simpler RISC-like ones may be true, at least for some processors. The first microarchitecture to use out-of-order processing and capable of decoding x86 code into RISC-like micro-operations was P6. Its successor, the Pentium M, introduced micro-operation fusion so that some pairs of micro-operations could be joined together. The Core architecture further improved this process, leading to a near-perfect match between x86 and RISC-V in terms of micro-operations.

A recent analysis has revealed that x86 processors decode complex instructions into simpler RISC-like micro-operations. This capability was first introduced in the P6 microarchitecture and improved in subsequent designs, especially in the Core architecture, which achieves a nearly perfect match between x86 and RISC-V in micro-operations. However, there is some debate in the comments about the significance of the distinction between CISC and RISC architectures, with some arguing that it is largely meaningless and others arguing that it remains an important consideration. Additionally, there are discussions about the current state of RISC-V CPUs and their potential for high performance, as well as debates about the merits of complex vs. simple instruction sets and the importance of ISA purity. There are also some technical discussions about microcode and the implementation of the RISC architecture.

### OpenLLaMA 13B Released

#### [Submission URL](https://huggingface.co/openlm-research/open_llama_13b) | 221 points | by [tosh](https://news.ycombinator.com/user?id=tosh) | [102 comments](https://news.ycombinator.com/item?id=36381136)

OpenAI has open-sourced its large language model, OpenLLaMA, which is a reproduction of Meta AI's LLaMA. OpenLLaMA is trained on 1T tokens and comes in 3B, 7B, and 13B models, with PyTorch and JAX weights. The models were trained using the RedPajama dataset and EasyLM, a JAX-based training pipeline. OpenLLaMA also offers evaluation results that indicate its performance is comparable to that of GPT-J and LLaMA on various tasks. It can be directly loaded from Hugging Face Hub, but it is recommended that users avoid using the fast tokenizer and instead use the LlamaTokenizer class or pass the `use_fast=False` option for the AutoTokenizer class.

OpenAI has released OpenLLaMA, a large language model trained on 1T tokens and coming in 3B, 7B, and 13B models. The models were trained using the RedPajama dataset and EasyLM, a JAX-based training pipeline. It offers evaluation results indicating its performance is similar to that of GPT-J and LLaMA on various tasks. It was noted that the models' code can be accessed on GitHub, and the models can be used directly from the Hugging Face Hub. However, it is recommended that users avoid using the fast tokenizer and instead use the LlamaTokenizer class or pass the `use_fast=False` option for the AutoTokenizer class. In the discussion, people shared their experience using the models, suggested improvements such as larger context sizes, discussed licensing issues, and shared links to resources for language model training.

### Shannon’s Source Coding Theorem (2020)

#### [Submission URL](https://mbernste.github.io/posts/sourcecoding/) | 122 points | by [ElFitz](https://news.ycombinator.com/user?id=ElFitz) | [37 comments](https://news.ycombinator.com/item?id=36380198)

e distribution \(X\), with entropy \(H(X)\), Shannon's Source Coding Theorem tells us that, on average, we need at least \(H(X)\) bits to accurately communicate samples drawn from that distribution. In other words, the entropy of the distribution gives us a lower bound on the amount of compression we can achieve in our description of the samples before risking the loss of information. This post walks us through the theorem using the example of two people, Person A and Person B, trying to communicate by encoding and decoding symbols from a distribution. The post introduces coding theory concepts such as code functions, code alphabets, and code words to explain how we encode symbols before communication.

The submission discusses Shannon's Source Coding Theorem and how it sets a lower bound on the compression of samples drawn from a distribution. The discussion dives further into related concepts like Kolmogorov Complexity and practical compression techniques, such as those used with JPEGs and other formats. Some users question the universality and usefulness of these theories, while others offer examples and insights into how they are applied in practice. There is also discussion around alternative ways to encode and transmit information, such as using symbols instead of binary numbers.

### ChatGPT, Google Bard Generates Generic Windows 11, Windows 10 Pro Keys

#### [Submission URL](https://www.tomshardware.com/news/chatgpt-generates-windows-11-pro-keys) | 112 points | by [el_hacker](https://news.ycombinator.com/user?id=el_hacker) | [41 comments](https://news.ycombinator.com/item?id=36385032)

Open-source technology has made it possible to get valid keys for different operating systems without having to buy one outright. Popular AI platform, ChatGPT, has been generating working keys for both Windows 10 and 11 Pro. These are similar to the Keys Management Service (KMS) keys published on Microsoft's website, although using enterprise keys to activate and access some of the premium features of the operating system carries risks. Siddiqi, aka @immasiddtweets on Twitter, successfully created generic keys for several Windows editions by tricking ChatGPT into generating keys that should only be used by enterprise clients, such as multi-national companies, without attribution.

The submission on Hacker News discusses the ability of ChatGPT, an AI platform, to generate valid keys for different operating systems, including Windows 10 and 11 Pro, using open-source technology. However, enterprise keys used to activate and access premium features through this method carry risks. Siddiqi successfully created generic keys for several Windows editions, including those that should only be used by enterprise clients, without attribution. Additionally, there are debates about the legality and morality of using this technology to obtain keys. Some comments include discussions about the technical aspects of key activation and licenses while others are skeptical or critical of the usefulness and relevance of the article.

### Infinite Photorealistic Worlds Using Procedural Generation

#### [Submission URL](https://arxiv.org/abs/2306.09310) | 294 points | by [cpeterso](https://news.ycombinator.com/user?id=cpeterso) | [75 comments](https://news.ycombinator.com/item?id=36376071)

A team of researchers has introduced Infinigen, a procedural generator of photorealistic 3D scenes of the natural world. The generator uses randomized mathematical rules to generate every asset, from shape to texture, allowing for infinite variation and composition without using any external sources. Infinigen covers objects and scenes in the natural world, such as plants, animals, terrains, and natural phenomena like fire, cloud, rain, and snow. It can be used to generate diverse training data for various computer vision tasks, including object detection, semantic segmentation, optical flow, and 3D reconstruction, making Infinigen a useful resource for computer vision research and beyond. The paper has been accepted to CVPR 2023 and is available for download along with code and pre-generated data.

A team of researchers has developed Infinigen, a procedural generator of photorealistic 3D scenes of the natural world that uses mathematical rules to generate every asset, allowing for infinite variation and composition without using external sources. The generator covers objects and scenes in the natural world, such as plants, animals, terrains, and natural phenomena, making it a useful resource for computer vision research and beyond. In the comments, users discuss the relevance of previous work in this field, the hardware requirements for using the generator, and the potential applications of the technology. Additionally, there are comparisons made to other procedurally generated games, such as No Man's Sky and Elite Dangerous.

### Hybrid SDF-Voxel Traversal

#### [Submission URL](https://www.shadertoy.com/view/dtVSzw) | 80 points | by [michaelsbradley](https://news.ycombinator.com/user?id=michaelsbradley) | [4 comments](https://news.ycombinator.com/item?id=36376982)

Shadertoy, a web-based platform for coding and sharing OpenGL ES 2.0 and WebGL graphics shaders, is experiencing an issue with its WebGL rendering. Visitors to the platform are being met with a message that reads “No WebGL available” and a frame rate of 0.00 fps. The platform allows users to create real-time graphics, animations, and interactive applications. The cause of the issue is likely related to the WebGL standard or security restrictions.

The discussion about the Shadertoy platform's WebGL rendering issue revolves around various topics related to its graphics and shader programming capabilities. One user, Extigy, shares a couple of links to articles on rendering worlds and describing techniques, while another user, ta988, recommends a smooth-terrain SDF tool. A user named lnbrnstn mentions using SDF techniques for procedural rendering, commenting on the optimal presentation format and quality of the code. Finally, a user named Exuma simply comments on the overall impressiveness of the platform.

### Moldable Live Programming for Clojure

#### [Submission URL](https://github.com/nextjournal/clerk) | 103 points | by [simonpure](https://news.ycombinator.com/user?id=simonpure) | [13 comments](https://news.ycombinator.com/item?id=36380636)

Clerk, a notebook library for Clojure, aims to address issues faced by computational notebooks such as notebook code being hard to reuse, less helpful than an editor, problems with archival, and out-of-order execution. Clerk builds a dependency graph of Clojure vars and only recomputes the needed changes to keep the feedback loop fast. Clerk notebooks are either regular Clojure namespaces or regular markdown files, which can be stored in source control. Clerk runs inside the Clojure process, giving access to all code on the classpath. Clerk also comes with a demo repository with many use cases, and users can add their own via pull requests.

Clerk is a Clojure notebook library that builds a dependency graph of Clojure vars and recompiles the needed changes for fast feedback. The feedback loop is helpful for monitoring/training and provides robust and changeable examples to explore and jumpstart understanding. Clerk notebooks are either regular Clojure namespaces or markdown files, making them simple to store in source control. Clerk runs within the Clojure process, providing access to all the code on the classpath. Under the hood, Clerk analyzes code blocks and determines their dependencies efficiently and calculates changes for control and rendering. The comments discuss Clerk's efficient rendering and its ability to handle large data sets quickly. One user recommends Clerk for producing reproducible and stable software, while others discuss Clerk's similarities to other common notebook libraries. Another user suggests trying Nextjournal for those looking for a free and effortless service that provides collaboration and sharing.

### Hexa Lift: Single person drone

#### [Submission URL](https://www.liftaircraft.com) | 65 points | by [thunderbong](https://news.ycombinator.com/user?id=thunderbong) | [73 comments](https://news.ycombinator.com/item?id=36381177)

LIFT, a new aviation company, has announced a completely new and thrilling way to fly without the need for a runway or a pilot's license. The electric multi-rotor aircraft provides a personal, vertical flying experience that anyone can enjoy. LIFT aims to bring this experience to major US cities, tourist destinations, and entertainment hubs, with the first cities chosen based on demand from potential customers. The company's concept has been compared to iFLY's indoor skydiving, which made the once-inaccessible sport accessible to everyone.

LIFT, a new aviation company has announced a new way to fly without the need for a runway or a pilot's license, offering a personal vertical flying experience. The company aims to bring this experience to major US cities and tourist destinations based on demand from potential customers. The discussion on the submission includes a comparison between LIFT and other similar aviation companies, while some commenters question the regulation around ultralight powered aircraft and express concerns about safety and FAA regulations. There is also extensive discussion about the weight and battery capacity of the aircraft, as well as the safety equipment required. Some users argue that there are already regulatory systems in place to handle the availability and safety of such products, while others maintain that regulatory systems need to be clearer and more coherent.

### Follow up to “I booted Linux 293k times”

#### [Submission URL](https://rwmj.wordpress.com/2023/06/18/follow-up-to-i-booted-linux-292612-times/) | 307 points | by [pabs3](https://news.ycombinator.com/user?id=pabs3) | [32 comments](https://news.ycombinator.com/item?id=36379615)

Richard WM Jones, a kernel developer, gained a lot of attention when he mentioned that he had booted Linux 292,612 times in his attempt to track down a bug that was causing qemu to hang. Jones, in his latest update, shared that his initial bisection that took several days got the wrong commit and that he posted his findings on LKML, but they still did not fully understand how to trigger the hang. Eventually, the Amazon thread has led to Thomas Gleixner suggesting a fix, which Jones tested and that worked.

A kernel developer, Richard WM Jones, received attention for booting Linux 292,612 times in his efforts to track down a bug that caused qemu to hang. He spent several days attempting to bisect the issue, but the initial commit that he found was incorrect, and after posting his findings on LKML, they still did not fully understand how to trigger the hang. Eventually, through an Amazon thread, Thomas Gleixner suggested a fix which Jones tested that worked. The comments discussed the difficulty in debugging kernel issues as well as how race conditions make it challenging to reproduce the problem. Additionally, they touched on the importance of acceptance tests and integration tests in software development. Some commenters also shared resources related to testing and problem-solving in software development.

### Does Lemmy benefit from Rust? Is code execution speed the bottleneck?

#### [Submission URL](https://programming.dev/post/50696) | 50 points | by [dragontamer](https://news.ycombinator.com/user?id=dragontamer) | [23 comments](https://news.ycombinator.com/item?id=36383120)

Welcome to the Hacker News Daily Digest! We bring you the top stories from the world of tech, startups, and hacking. Our AI-powered summary feature will provide you with a brief and engaging take on each submission, so you can stay informed and on top of the latest news.

Stay tuned for daily updates on the buzzing conversations happening on Hacker News!

The discussion revolves around Lemmy, a Reddit alternative designed to operate on the Fediverse. The post discusses the scalability limits of Lemmy, with some suggesting that issues like missing databases, RAM constraints, and indexing could negatively affect the system's ability to scale despite its modern language. Further, the developers explain that the Rust language utilized by Lemmy could alleviate some of the bottlenecks due to its strict packing and data structure modification capabilities. The discussion also touches on the use of Lemmy and other Fediverse platforms in general, with some expressing support and optimism for the platform's growth. Lastly, there is some disagreement over whether the problems with Lemmy can be attributed to its language choice or its running instance limitations, among other factors.

### Squeezing a Little More Performance Out of Bytecode Interpreters

#### [Submission URL](https://stefan-marr.de/2023/06/squeezing-a-little-more-performance-out-of-bytecode-interpreters/) | 90 points | by [abhi9u](https://news.ycombinator.com/user?id=abhi9u) | [23 comments](https://news.ycombinator.com/item?id=36377280)

eter and benchmarked against the original ordering. By repeating this process over many generations, the genetic algorithm can converge on an optimal order. Wanhong and Tomoharu found that their method was able to improve the performance of several popular interpreters. Their approach is a fascinating example of how machine learning can be used to optimize complex systems, even when we don't fully understand why certain optimizations work.

The submission is about a genetic algorithm used to optimize the performance of interpreters. Comments include discussions on the effectiveness of direct threading, the impact of benchmarks on interpreter performance, and the practicality of optimizing for specific applications. Other discussions include the problem of hardware differences affecting interpreter behavior, practicality issues in optimizing for specific processors and devices, and suggestions for further research in optimizing interpreter performance.

### Tzap: Supercharged GitHub Copilot – Includes your whole code repository

#### [Submission URL](https://github.com/tzapio/tzap) | 23 points | by [bevenky](https://news.ycombinator.com/user?id=bevenky) | [3 comments](https://news.ycombinator.com/item?id=36377918)

Tzap, an easy-to-use CLI tool, has been launched to streamline GPT-based code generation tasks. It simply indexes the project with embeddings and extracts relevant contextual information like interfaces, types, and database models, resulting in Tzap's combination with the prompt creating a suitable prompt for the GPT model. Tzap can create highly specific and complex code through GPT generation, but as it's still in the beta phase, existing files could be overwritten, so local changes should be committed first.

The discussion in the comments of the submission seems to be focused on the limitations and potential use cases of the Tzap tool. Users have raised concerns regarding the reliability of the generated code, with one user noting that beta tools can sometimes overwrite existing files, so caution is necessary. Another user suggested using local models in combination with Tzap for better results. There was also a comment about limitations on processing large amounts of code and GPU memory requirements. Overall, the discussion seems to highlight the potential of Tzap for streamlining code generation tasks while also highlighting the need for caution and consideration of specific use cases.

### $27 PineTime smartwatch that runs open-source software

#### [Submission URL](https://liliputing.com/the-27-pinetime-smartwatch-runs-open-source-software-and-now-its-ready-for-non-developers/) | 50 points | by [hosteur](https://news.ycombinator.com/user?id=hosteur) | [22 comments](https://news.ycombinator.com/item?id=36381436)

Pine64 is now selling a version of its PineTime smartwatch that is geared towards casual users. This version has a sealed case, so it cannot be opened for low-level debugging, but it is more sturdy and has the features you'd expect from an entry-level wearable, including support for Bluetooth 5, a vibration motor, accelerometer, and a heart rate sensor. It also has a 1.3 inch, 240 x 240 pixel capacitive touchscreen display, a Nordic Semiconductor nRF52832 processor, and a 170-180 mAh battery. The unit will come pre-installed with InfiniTime 1.2 firmware, but it also supports other firmware such as the MicroPython-based Wasp OS and the upcoming Malila OS. The dev version is still available for those who prefer it, and both versions are hackable but affordable.

Pine64 has released a new version of its PineTime smartwatch geared towards casual users that has a sealed case, making it more sturdy, and has features expected in entry-level wearables. PineTime has a Nordic Semiconductor nRF52832 processor, a 1.3 inch 240 x 240-pixel display, a heart rate sensor, and a 170-180mAh battery, among other features. The smartwatch is also hackable yet affordable. Comments compare PineTime with Amazfit GTS 2, Amazfit BIP and BangleJS. Some express skepticism that PineTime's software would match up with other higher-end smartwatches while others consider it a good alternative to feature-rich but expensive models. Some also note that PineTime could be a useful wearable for a minimalist smartwatch that allows users to check notifications or the time without needing more advanced features. Furthermore, some express concern about PineTime's tiny device memory and some questioned how the backup device works.

### LLMs can label data as well as human annotators, but 20 times faster

#### [Submission URL](https://www.refuel.ai/blog-posts/llm-labeling-technical-report) | 49 points | by [nihit-desai](https://news.ycombinator.com/user?id=nihit-desai) | [27 comments](https://news.ycombinator.com/item?id=36384015)

Refuel, an AI company, has introduced a benchmark for evaluating performance of Language Model Models (LLMs) for labeling text datasets. The benchmark aims to compare the performance of LLMs and human annotators on three axes: quality, turnaround time and cost. The company found that LLMs can label text datasets with comparable quality to skilled human annotators, but 20 times faster and seven times cheaper. The benchmark also found that GPT-4 is the best choice for achieving the highest quality labels, while GPT-3.5-turbo, PaLM-2 and open source models like FLAN-T5-XXL are compelling for achieving the best tradeoff between label quality and cost.

Refuel, an AI company, has introduced a benchmark for evaluating performance of Language Model Models (LLMs) for labeling text datasets, which aims to compare the performance of LLMs and human annotators along the three axes of quality, turnaround time, and cost. The comments on the Hacker News submission express some skepticism of the benchmark and discuss concerns around LLMs replacing human annotators, PII (personally identifiable information) privacy issues, and the existence of true ground truth in human annotation. Some users share their own experiences with text labeling and suggest alternatives to LLMs, while others argue for the benefits of LLMs in speeding up and reducing the cost of text labeling tasks.

### Google Go language goes with opt-in telemetry

#### [Submission URL](https://www.theregister.com/2023/05/17/googles_go_data_collection/) | 37 points | by [el_hacker](https://news.ycombinator.com/user?id=el_hacker) | [12 comments](https://news.ycombinator.com/item?id=36380292)

The stewards of Google's open-source Go language (Golang) have announced that they will implement software telemetry on an opt-in basis rather than turning data collection on by default and requiring developers to opt-out. This decision was made due to objections from Go developers who were concerned about data collection without permission. Many users also pointed out that Google, an advertising platform, had consistently opposed opt-in data collection. The telemetry will consist of recording various events like cache hits, feature usage, latency, and more to a local file. According to Russ Cox, the Golang tech lead at Google, even with "tens of thousands of users opted in, we should be able to get helpful data".

The discussion on the submission revolves around the topic of telemetry and whether it should be opt-in by default or not. Some believe that opt-in is the right approach, and Google, being an advertising platform, was hypocritical for not adopting this approach earlier. Others argue that telemetry is reasonable and essential for improving user experience and finding and fixing bugs. However, some users question the effectiveness of telemetry in environments with forwarded or repurposed networks and whether it can cause performance delays and false configuration overhead. There is also a debate over what level of telemetry should be considered reasonable and sufficient. Some respondents suggest an explanatory approach to telemetry, and others point out that the concern is overstated, citing examples of other applications and services that use telemetry.

