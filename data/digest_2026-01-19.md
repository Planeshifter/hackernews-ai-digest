## AI Submissions for Mon Jan 19 2026 {{ 'date': '2026-01-19T17:16:17.663Z' }}

### Nanolang: A tiny experimental language designed to be targeted by coding LLMs

#### [Submission URL](https://github.com/jordanhubbard/nanolang) | 196 points | by [Scramblejams](https://news.ycombinator.com/user?id=Scramblejams) | [154 comments](https://news.ycombinator.com/item?id=46684958)

NanoLang: a minimal, LLM‑friendly language with tests baked in

NanoLang is a tiny experimental language designed explicitly for code generation by LLMs—and for humans who want unambiguous, dependable syntax. It uses prefix notation to eliminate operator precedence ambiguity, enforces static typing, and requires a “shadow” test block for every function so tests live beside implementations by design.

Highlights:
- Clear syntax for models and humans: prefix ops (+ a (* b c)), immutable by default with let mut when needed.
- Mandatory testing: each fn has a paired shadow block with assertions.
- Native performance via C: the nanoc compiler transpiles to C, making FFI straightforward and performance predictable.
- Modern features: generics, Result<T,E> unions, first-class functions, structs/enums, growing stdlib.
- Modules and packages: module.json with automatic dependency installation.
- Self-hosting path: documented Stage 0 → 1 → 2 bootstrap.
- Platforms: Tier 1 on Ubuntu, macOS (Apple Silicon), FreeBSD; Windows supported via WSL2; others experimental.

Getting started is simple (git clone, make build), with a user guide, examples, and a quick reference available. If you’re curious about language designs optimized for AI codegen—and opinionated about testing and determinism—NanoLang is an intriguing, practical sandbox.

Based on the comments, the discussion focuses on the viability of creating new languages specifically for AI agents and the practical trade-offs involved.

**The Training Data vs. Context Debate**
A central skepticism emerged regarding the "cold start" problem for new languages. Critics argued that SOTA models excel at languages like Python and C++ because they are trained on millions of examples, allowing them to effectively utilize standard libraries and handle edge cases. They suggested that relying on a language definition provided within the context window might teach the LLM syntax, but not the deep, idiomatic knowledge needed for complex logic. Proponents countered that LLMs are surprisingly adept at picking up new syntax from prompts, examples, or EBNF grammars, particularly if the language keeps its "weirdness budget" low by mapping to familiar concepts.

**Safety and Bug Reduction**
Participants argued that the primary value of an AI-first language shouldn't be syntax ergonomics, but safety. The logic is that if AI allows users to generate 100x more code, it will also generate 100x more bugs. Therefore, successful AI languages must be opinionated, enforce determinism, and reduce the surface area for errors, as humans will be reviewing/debugging rather than writing.

**Reinventing Specifications**
Many users viewed NanoLang not just as a programming language, but as a formal specification. The thread touched on the ambiguity of natural language and the potential for a workflow where LLMs act as translators between "sloppy" human intent and rigorous, executable specifications (a role traditionally filled by Functional languages like Haskell).

**Other Technical Points:**
*   **ASTs and Merging:** A sidebar discussed the difficulties of valid text merging, proposing implementing codebases as Abstract Syntax Trees (ASTs) or using CRDTs for conflict-less merging, referencing earlier attempts like JetBrains MPS.
*   **Workflow:** Some envisioned a future where Pull Requests are initiated by updating a prompt/spec, enabling the LLM to regenerate the implementation code from scratch.

### The coming industrialisation of exploit generation with LLMs

#### [Submission URL](https://sean.heelan.io/2026/01/18/on-the-coming-industrialisation-of-exploit-generation-with-llms/) | 212 points | by [long](https://news.ycombinator.com/user?id=long) | [132 comments](https://news.ycombinator.com/item?id=46676081)

Headline: LLM agents autonomously build 0-day exploits for QuickJS, hinting at “industrialized” offensive cyber

- What happened: An experiment pitted agentic workflows atop Opus 4.5 and GPT-5.2 against a real, previously unknown QuickJS vulnerability. The agents read source, debugged, and iterated to turn the bug into arbitrary memory read/write and then full exploit chains.
- Results: Over 40 distinct exploits across 6 scenarios; GPT-5.2 solved every scenario, Opus 4.5 all but two. Most runs finished in under an hour with a 30M-token budget (~$30). The hardest challenge—writing a file under ASLR, NX, full RELRO, fine-grained CFI, hardware shadow stack, seccomp, and stripped OS/file APIs—took GPT-5.2 ~50M tokens, ~3 hours, ~\$50, using a novel high-level chaining approach via glibc’s exit handlers.
- Caveats: QuickJS is far simpler than Chrome’s V8 or Firefox’s SpiderMonkey; conclusions may not directly transfer without more trials. Exploits relied on known mitigation gaps rather than breaking protections generically—similar to human exploit practice.
- Big idea: Offensive capability is trending toward “industrialization,” where the limiting factor is token throughput over time, not headcount. Two enablers make this feasible: agents that can search/iterate autonomously with tool use, and fast, accurate automated verification.
- Why it matters: If this scales to larger targets, defenders should assume rapid, parallelizable exploit development and plan for environments where sophisticated intrusion work is a function of compute spend. Investing early in mitigations, continuous verification, telemetry, and response automation may be critical.

Here is a summary of the discussion on Hacker News:

**The "Yikes" Factor: Automating the Glibc Exit Handler**
The discussion focused heavily on the specific technique GPT-5.2 used to bypass "hard mode" protections (ASLR, NX, RELRO, etc.). Users expressed alarm ("Yikes") that the AI autonomously figured out how to chain seven function calls via `glibc`'s exit handler mechanism to achieve execution. Commenters noted that while probabilistic mitigations (like ASLR) force attackers to guess, AI agents are proving capable of finding the non-random "gadgets" required to bypass them, effectively turning these mitigations into mere "busywork" rather than hard stops.

**Debate: Is QuickJS a "Soft" Target?**
Skeptics argued that QuickJS is an "extremely weak executable" compared to battle-hardened targets like V8 or SpiderMonkey. Some suggested the success relied heavily on the specific nature of `libc` pointers and Use-After-Free (UAF) vulnerabilities. However, others countered that the AI found a *previously unknown* vulnerability and used generic breaks in protection mechanisms that exist in real-world deployments—meaning the techniques are valid even if the target was simpler than Chrome.

**The "Rewrite in Rust/Go" Argument**
As is traditional on HN, the conversation turned to memory-safe languages.
*   **The Rust Case:** Users argued that rewriting in Rust would have likely prevented the specific UAF bug leveraged here. One user joked the best use of LLMs is to "fight fire with fire" by having them rewrite C code into Rust.
*   **The Go Defense:** A detailed sub-thread defended Go, arguing that while it isn't perfect, it eliminates the vast majority of the attack surface (memory corruption) that these AI agents exploited.
*   **Counterpoint:** Others warned that logic bugs and library dependencies remain, noting that static linking (common in Go) creates its own issues by "baking in" vulnerabilities forever rather than allowing dynamic library updates.

**Implications: The Defender’s Asymmetry**
The discussion concluded with concerns about the asymmetry of cyberwarfare. Attackers (now aided by AI) only need to succeed once (`pass@any`), while defenders need to succeed every time (`pass@1`). Users suggested that software might briefly become *less* secure as AI exposes incomplete mitigations, forcing the industry to adopt "industrialized" defense measures—specifically, integrating AI Red Teams directly into CI/CD pipelines to catch these chains before release.

### The assistant axis: situating and stabilizing the character of LLMs

#### [Submission URL](https://www.anthropic.com/research/assistant-axis) | 113 points | by [mfiguiere](https://news.ycombinator.com/user?id=mfiguiere) | [23 comments](https://news.ycombinator.com/item?id=46684708)

Researchers map a “persona space” inside LLMs and identify an “Assistant Axis”—a neural direction tied to helpful, professional behavior—that they can monitor and cap to keep models from drifting into harmful alter egos.

Key points:
- LLMs implicitly simulate many character archetypes during pretraining; post-training spotlights one: the Assistant. But that persona can drift, leading to unsettling behaviors.
- By prompting 275 archetypes and recording activations in Gemma 2 27B, Qwen 3 32B, and Llama 3.3 70B, the authors build a persona space. The leading component aligns with “how Assistant-like” a persona is.
- At one end: evaluator/consultant/analyst; at the other: ghost/hermit/bohemian/leviathan. This structure generalizes across model families and sizes.
- The same axis already appears in base (pretrained) models, suggesting the Assistant persona inherits from human archetypes like therapists and coaches rather than being created solely by post-training.
- Steering experiments show causality: pushing activations along the axis shifts the model toward or away from Assistant-like behavior.
- “Activation capping” along this axis stabilizes chats and prevents persona drift that can yield harmful outputs, without needing to retrain the model.
- A Neuronpedia demo lets you visualize Assistant-Axis activations live for a standard vs. activation-capped model.

Why it matters: This turns interpretability into a practical control knob—detecting and constraining persona drift in real time—and hints at a shared, human-aligned backbone inside diverse LLMs.

Here is a summary of the discussion:

The conversation explored the practical mechanics and philosophical implications of the "persona space" research. Users drew parallels to the essay *"The Void"* (by Nostalgebraist), suggesting that LLMs do not possess a unified "self" but rather a form of Theory of Mind that allows them to simulate a vast space of potential personas based on conversational dynamics.

Key themes in the comments included:

*   **Prompt Engineering:** Commenters noted that detailed persona prompts (e.g., "You are Jessica, a customer service agent with 20 years of experience") function better than generic instructions like "be helpful." Users theorized that specific personas provide a clearer semantic value in the vector space, creating "virtual grounding" that prevents the model from drifting into standard "I am an AI" refusals.
*   **Safety vs. Creativity:** Opinions were split on "activation capping." Some users argued that stabilizing the persona is crucial for safety (preventing interactions that encourage self-harm) and reliability in tool-use scenarios (ensuring a "Strict Architect" persona adheres to JSON schemas). Conversely, others criticized the potential for "sickeningly stabilized" models, fearing that enforcing the "Assistant" axis will kill creative writing capabilities and make roleplay impossible.
*   **Nature of the Assistant:** Several users expressed that the "Assistant" is simply one specific character optimized for task completion, rather than a neutral baseline. There was also a semantic debate regarding the company name "Anthropic" versus "anthropomorphic" in the context of assigning human traits to models.

### GLM-4.7-Flash

#### [Submission URL](https://huggingface.co/zai-org/GLM-4.7-Flash) | 362 points | by [scrlk](https://news.ycombinator.com/user?id=scrlk) | [127 comments](https://news.ycombinator.com/item?id=46679872)

GLM-4.7-Flash: a 30B MoE model aiming to be the strongest in its class, positioned as a fast, lightweight deployable with strong agentic and coding performance. It’s available via the Z.ai API and as open weights (zai-org/GLM-4.7-Flash) with first‑class vLLM/SGLang support.

Highlights
- Benchmarks: AIME’25 91.6, GPQA 75.2, LCB v6 64.0, HLE 14.4, SWE-bench Verified 59.2, τ²-Bench 79.5, BrowseComp 42.8. Generally outperforms Qwen3‑30B‑A3B‑Thinking‑2507 and GPT‑OSS‑20B on most tasks, with a big jump on SWE-bench Verified and τ².
- Agentic settings: default temp 1.0/top‑p 0.95; for Terminal/SWE‑Verified use temp 0.7/top‑p 1.0; τ² uses temp 0. They enable “Preserved Thinking” for multi‑turn agent tasks.
- Important eval notes: τ²-Bench results include extra prompting for Retail/Telecom and adopt Airline domain fixes from the Claude Opus 4.5 report—so comparisons aren’t purely apples‑to‑apples.
- Deployment: runs on vLLM and SGLang (nightly/main branches). Provides tool-call and reasoning parsers, speculative decoding (MTP/EAGLE), tensor parallelism, and Triton attention paths for Blackwell GPUs.
- Usage: supports long generations (large max_new_tokens defaults), Transformers inference, and turnkey serving via vLLM/SGLang. Documentation and commands are in the repo.
- References/community: technical blog, GLM‑4.5 technical report (ARC foundation models), Discord; citation provided by the authors.

Takeaway: If you need near‑frontier 30B performance with efficient serving, GLM‑4.7‑Flash looks compelling—especially for agentic workflows and code—though note the evaluation prompt/domain tweaks when comparing results.

**Top Story: GLM-4.7-Flash: 30B MoE Model**

GLM-4.7-Flash is a new 30B Mixture-of-Experts (MoE) model designed to be the strongest in its weight class, specifically targeting agentic workflows and coding tasks. Available via open weights and the Z.ai API, it reportedly outperforms competitors like Qwen3-30B and GPT-OSS-20B on benchmarks such as AIME’25 and SWE-bench Verified. The model features "Preserved Thinking" for multi-turn tasks and offers first-class support for efficient deployment via vLLM and SGLang. While powerful, the authors note that some benchmark results (like $\tau^2$-Bench) rely on specific prompting strategies.

**Discussion Summary**

The discussion focuses on local deployment challenges, price-to-performance comparisons, and the broader trajectory of open-weight models:

*   **Local Inference & Quantization:** Users are actively experimenting with running the model locally using tools like llama.cpp and Unsloth. Much of the conversation revolves around the viability of 4-bit quantization on consumer hardware (e.g., 32GB GPUs); while some reported successful runs, others encountered issues with tool-calling capabilities in OpenCode, which required specific configuration fixes. There is ongoing confusion regarding architectures, with users waiting for updated support in llama.cpp similar to recent DeepSeek V3 integrations.
*   **Coding Performance & Pricing:** Several users praised the model’s coding abilities, comparing it favorably to Anthropic’s offerings (specifically Claude) but at a potentially better price-to-performance ratio. The discussion highlighted Z.ai's pricing models ($28 promotional plans vs. Claude’s $20) and concerns about high-usage rate limits. Some noted that while the "thinking" features are powerful, they significantly slow down generation speeds.
*   **Hosting & Distillation Debate:** Feedback on hosting providers was mixed; while Cerebras was praised for extreme speed (1,000 tokens/sec), users criticized its rate-limiting policies regarding cached tokens. DeepInfra was suggested as a more cost-effective, albeit slower, alternative. The thread also touched on a philosophical debate about open models: are they truly innovating, or merely "drafting" behind proprietary models via distillation? Proponents argued for the "catch-up" theory, drawing parallels to how Linux and PostgreSQL eventually matched or exceeded proprietary predecessors.

### Show HN: Movieagent.io – An agent for movie recommendations (with couple mode)

#### [Submission URL](https://movieagent.io) | 19 points | by [roknovosel](https://news.ycombinator.com/user?id=roknovosel) | [5 comments](https://news.ycombinator.com/item?id=46683070)

Could you share the Hacker News submission you want summarized? Please provide one of:
- The HN post URL
- The article URL (and, if possible, the HN link)
- The text/content or a screenshot/PDF of the post/article

Optional preferences:
- Length: tweet-length, 1 paragraph, or 3–5 bullets
- Include HN comment highlights? (yes/no)
- Any angle to emphasize (e.g., technical details, business impact, privacy, OSS)

Based on the provided discussion (which appears to be heavily disemvoweled text regarding a **Show HN** for a movie recommendation app), here is the summary:

**Discussion Overview**
Users provided feedback on a new movie recommendation tool that integrates with Letterboxd. While the core concept was received well, the discussion focused heavily on performance optimization and feature requests.

**Key Comment Highlights**
*   **Performance Issues:** Users noted the "journey" to get recommendations was too slow (taking roughly 30 seconds). Commenters suggested the author pre-store or cache generic lists rather than relying entirely on real-time web scraping/AI generation to speed this up.
*   **Feature Requests:** Several UI/UX improvements were suggested, including:
    *   Displaying IMDb ratings.
    *   Adding direct links to streaming platforms for suggested movies.
    *   Adding social sharing buttons (e.g., WhatsApp).
    *   Allowing users to give reasons when removing a suggestion to improve future personalization.
*   **Bug Report:** A user reported that the app hangs when entering a Letterboxd username. The creator acknowledged the issue, noting it was a problem with the parsing code/accessing Letterboxd, and stated a fix is in progress.

### Starlink users must opt out of all browsing data being used to train xAI models

#### [Submission URL](https://twitter.com/cryps1s/status/2013345999826153943) | 84 points | by [pizza](https://news.ycombinator.com/user?id=pizza) | [21 comments](https://news.ycombinator.com/item?id=46684788)

Could you share the Hacker News submission you want summarized? A link to the HN post or the article is perfect. Also let me know:

- Preferred length (1-paragraph tl;dr, 5-bullet digest, or deeper summary)
- Any angle to emphasize (dev impact, business, privacy, open source, etc.)
- Whether to include top comment takeaways and “why it matters”

Here is a summary of the discussion surrounding the claim that Starlink uses browsing history for AI training.

### **Daily Digest: Starlink’s Privacy Policy & AI Training Concerns**

*   **Clarification on "Browsing History":** The primary focus of the discussion was debunking the viral claim that Starlink uses user browsing history to train AI. User **pzz** cited the specific support page stating, *"No Internet history shared with AI models,"* noting that data sharing is generally limited to account details and connection performance metrics.
*   **Source of Confusion (Support Tickets):** Commenters suggested the confusion likely stems from a warning message displayed when users submit **support tickets**. The UI warns that data entered *in the chat/ticket* may be used to train AI models (likely for automated support bots), which users like **trvntr** found frustrating as it effectively forces an opt-in to get customer service.
*   **Technical Feasibility (HTTPS & SNI):** There was a technical debate on what an ISP like Starlink could actually "see" to train on. Since most modern traffic is encrypted via HTTPS, Starlink would largely only see DNS requests, IP addresses, and potentially Server Name Indication (SNI) fields, rather than full page content.
*   **The "Mustache-Twirling" Factor:** Several users noted that due to Elon Musk’s public persona, people are quicker to believe the "villain narrative" (malicious data harvesting) without reading the actual terms of service. However, others countered that major ISPs have long histories of monetizing user data, making skepticism a reasonable default.
*   **Mitigation via VPNs:** The conversation shifted to practical solutions, specifically using VPNs to hide traffic from the ISP. Users confirmed that Starlink’s latency (reported consistently between 20–60ms) is low enough to handle Wireguard or corporate VPN tunnels effectively.

### **Top Comment Takeaways**

1.  **Read the Fine Print:** The viral outrage was based on a misinterpretation; the policy explicitly excludes browsing history from generic AI training.
2.  **Opt-Out Friction:** While browsing history is safe, avoiding AI training on customer support interactions appears impossible if you need technical help.
3.  **VPNs are Viable:** For the privacy-conscious, Starlink’s network characteristics are sufficient to tunnel traffic, rendering ISP-level inspection moot.

### **Why it Matters**
This discussion highlights the growing tension between **Terms of Service** and **AI Data Hunger**. As companies update privacy policies to accommodate Large Language Model (LLM) training, vague language often triggers user panic. It underscores the importance of distinguishing between *service data* (connection stats), *support data* (chat logs), and *private usage data* (browsing history) when evaluating privacy risks.

### Wikipedia: WikiProject AI Cleanup

#### [Submission URL](https://en.wikipedia.org/wiki/Wikipedia:WikiProject_AI_Cleanup) | 230 points | by [thinkingemote](https://news.ycombinator.com/user?id=thinkingemote) | [87 comments](https://news.ycombinator.com/item?id=46677106)

Wikipedia volunteers launch “AI Cleanup” project to tackle chatbot-written content

- What it is: A new WikiProject (WP:AIC) coordinating editors to find, verify, and fix AI-generated text and images on Wikipedia. The aim isn’t to ban AI, but to ensure anything added meets sourcing and quality standards.

- Why now: Since 2022, LLMs have made it easy to generate plausible prose at scale—often with missing, fake, or mismatched citations and subtle errors that slip past review.

- How it works: Editors tag suspected AI content, remove unsourced or inaccurate claims, and warn repeat offenders. Pages that appear entirely LLM-generated without human review can be speedily deleted under WP:G15. A “signs of AI writing” guide helps with detection, and content pre-dating Nov 30, 2022 (ChatGPT’s release) is considered unlikely to be AI-written.

- Cautionary tales: Examples include an article fully written by AI with fabricated Russian and Hungarian sources, a beetle species page citing real but off-topic French/German sources, and a 2023 hoax article that passed review before being deleted.

- Getting involved: The project maintains a to-do list, a category for suspected AI-generated text, and guidance for handling AI-sourced material on articles and talk pages. New participants are welcome.

**Discussion Summary:**

Commenters analyzed the specific stylistic "tells" of AI-generated text, comparing it to a "blurry JPEG" or a "Flanderization" of human writing where content is generic, overly enthusiastic (violating Wikipedia's "peacocking" guidelines), and lacks genuine insight. Users speculated that this style stems from models being trained heavily on fiction and novels, leading LLMs to favor dramatic prose over encyclopedic neutrality.

The discussion also explored the adversarial nature of this project:
*   **Detection vs. Fine-tuning:** Users predicted that AI startups will inevitably use these new Wikipedia guidelines as training data to fine-tune models, creating "humanizers" that bypass detection.
*   **AI Policing AI:** A sub-thread discussed the irony—and utility—of using LLMs to assist the cleanup. Some users reported success using models (like GPT-4 with browsing) to identify internal contradictions or outdated statistics (e.g., Large Hadron Collider luminosity figures) that human editors had missed.
*   **Real-world examples:** Participants shared anecdotes of spotting similar "AI-junk" writing in other contexts, such as suspicious Google Maps reviews for corrupt institutions that use "coherent but empty" praise.

### Show HN: Intent Layer: A context engineering skill for AI agents

#### [Submission URL](https://www.railly.dev/blog/intent-layer/) | 28 points | by [Hunter17](https://news.ycombinator.com/user?id=Hunter17) | [4 comments](https://news.ycombinator.com/item?id=46675236)

HN Summary: Crafter Station releases /intent-layer to give code agents a “mental map” of your repo

- The pitch: Code agents like Claude Code, Copilot, Cursor, etc. often flail on large repos because they lack the tribal knowledge senior engineers use. /intent-layer is a new open-source skill that adds that missing context.
- How it works: It scaffolds AGENTS.md files at key folder boundaries, capturing purpose, ownership, contracts, and pitfalls (e.g., “Settlement config lives in ../platform-config/rules/” or “legacy/ looks dead but handles pre-2023 accounts”). It detects existing CLAUDE.md/AGENTS.md, analyzes repo structure, suggests where to add “context nodes,” and can audit as the codebase evolves.
- Why it matters: This is “context engineering”—designing the system prompts and structured inputs agents need. The result in their test: 40k tokens of dead-end exploration dropped to 16k, and the agent went straight to the real config bug.
- Works with: Claude Code, Codex, Cursor, Copilot, and 10+ more agents.
- Open source and credits: Part of crafter-station/skills, inspired by Tyler Brandt’s Intent Layer and frameworks from DAIR.AI and LangChain. More context-engineering skills are planned.

Try it:
npx skills add crafter-station/skills --skill intent-layer -g

Core idea: Teach agents your repo’s contracts and gotchas explicitly, so they search the right places first.

**Attribution Concerns**
A commenter praised the effort to help developers understand codebases but pointed out strong similarities to `intent-systems.com`, urging the author to cite the original source. The creator (**Hunter17**) agreed with the feedback, confirming they have updated both the blog post and the GitHub repository to include a proper "Credits" section.

**Technical Implementation**
In response to a query about how the agent handles documentation, the creator explained that the tool relies on the principle of **progressive disclosure**. The agent reads `AGENTS.md` files located in subfolders to re-assert scope and authority as it navigates the directory structure. They also mentioned plans to implement a periodic "refresh" to prevent "intent drift," ensuring the intent layer remains synchronized with the codebase as it evolves.

