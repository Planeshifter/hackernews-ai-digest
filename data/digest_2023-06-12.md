## AI Submissions for Mon Jun 12 2023 {{ 'date': '2023-06-12T17:12:54.838Z' }}

### Orca: Progressive Learning from Complex Explanation Traces of GPT-4

#### [Submission URL](https://arxiv.org/abs/2306.02707) | 100 points | by [kordlessagain](https://news.ycombinator.com/user?id=kordlessagain) | [24 comments](https://news.ycombinator.com/item?id=36299246)

Researchers have developed a new model, called Orca, that learns to imitate the reasoning process of large foundation models (LFMs). Orca analyzes rich signals from GPT-4, including explanation traces and step-by-step thought processes, guided by teacher assistance from ChatGPT. To promote progressive learning, the researchers tap into diverse imitation data with judicious sampling and selection. Orca surpasses conventional state-of-the-art models and shows competitive performance in professional and academic examinations like the SAT, LSAT, GRE, and GMAT, both in zero-shot settings without CoT. The study suggests that learning from step-by-step explanations, whether they are human-generated or AI-generated, improves model capabilities and skills.

The discussion covers topics such as the extent of GPT-4's restrictions on commercially generated models. Some users acknowledge the potential of Orca's approach in training LLMs, while others debate the usefulness of step-by-step explanations in improving model capabilities and other related topics, such as Pareto lows and quantum physics software.

### Understanding DeepMind's sorting algorithm

#### [Submission URL](https://justine.lol/sorting/) | 337 points | by [jart](https://news.ycombinator.com/user?id=jart) | [49 comments](https://news.ycombinator.com/item?id=36297476)

DeepMind, the popular AI research lab, has applied its deep learning expertise gained from building AlphaGo to develop improved sorting algorithms. They recently published a blog post detailing a paper they wrote on the use of smaller kernels for sorting algorithms. However, their code left some confusion as it was found that their kernel was intended to be used as a building block for a sorting function rather than the entire algorithm itself. The team’s contribution to LLVM libcxx aims to optimize compilers in designing sorting networks. Their novel discovery was that sometimes the “mov” instruction is unnecessary, and it shouldn’t come as a surprise that this went unnoticed for decades.

In the comments, users discussed the merits of different sorting algorithms and the importance of considering cache locality and stack size. Critiques were also made of programming languages' built-in sorting algorithms, with some suggesting using customized code instead. Finally, users shared their own experiences and research, including one user who suggested a new optimal sorting network research topic.

### Escape from Silicon Valley (alternative visions of computation)

#### [Submission URL](https://monroelab.net/escape-from-silicon-valley-alternative-visions-of-computation) | 69 points | by [jrepinc](https://news.ycombinator.com/user?id=jrepinc) | [26 comments](https://news.ycombinator.com/item?id=36298655)

In a recent article titled "Escape from Silicon Valley (alternative visions of computation)," the author discusses how the potential of language models and other machine learning methods is being wasted by privately owned things like ChatGPT. Instead of using all these resources to enhance profit objectives, the author recommends exploring alternative visions such as the DAIR Institute and Logic(s) magazine. The article highlights the difference between what we have and what is portrayed in science fiction, using Star Trek's LCARS system as an example. The author ultimately argues that the tech industry’s greatest lie is that it is building the future with human needs centered, and instead suggests we turn to more productive visions found in science fiction.

In the comments, there was a discussion about the merits of private versus public products, the differences between various computer systems, and the need for a more integrated and flexible, yet fully integrated system that can communicate across platforms. Contributors also mentioned the benefits of language models and the potential of AI in building a future where human and artificial intelligence can work together more productively.

### Self-healing code is the future of software development

#### [Submission URL](https://stackoverflow.blog/2023/06/07/self-healing-code-is-the-future-of-software-development/) | 53 points | by [kiyanwang](https://news.ycombinator.com/user?id=kiyanwang) | [82 comments](https://news.ycombinator.com/item?id=36288834)

Generative AI is being applied to software development, with the potential to automate the creation, maintenance and improvement of code at an unprecedented level. While large language models (LLMs) can produce large chunks of code, developers are concerned about the quality of what is being generated. The ability of LLMs to rapidly create more code than before also poses challenges for quality control. Companies will need to come up with new best practices for using generative AI effectively to avoid accumulating lots of shoddy code, according to Armando Solar-Lezama, a professor at the Massachusetts Institute of Technology's Computer Science & Artificial Intelligence Laboratory.

In the comment section, discussions range from the capabilities of LLMs to the limitations of empirical evidence in testing AI's abilities. Some users express concern that LLMs may exacerbate existing social problems, such as biases. Others highlight the potential benefits of generative AI in creating new language models or solving challenges in various domains.

### AI-powered church service in Germany draws a large crowd

#### [Submission URL](https://arstechnica.com/information-technology/2023/06/chatgpt-takes-the-pulpit-ai-leads-experimental-church-service-in-germany/) | 39 points | by [samizdis](https://news.ycombinator.com/user?id=samizdis) | [12 comments](https://news.ycombinator.com/item?id=36297251)

Last Friday, an experimental ChatGPT-powered church service took place in Germany, drawing over 300 attendees. The 40-minute sermon used text generated by OpenAI's ChatGPT chatbot and was delivered by avatars on a television screen above the altar. The service took place as part of a convention called Deutscher Evangelischer Kirchentag, and was created by Jonas Simmerlein, a theologian and philosopher from the University of Vienna. While reactions were mixed, Simmerlein believes that AI could provide ideas for upcoming sermons or expedite the sermon-writing process to free up pastors to devote more time to individual spiritual guidance.

The discussion on the submission about an experimental ChatGPT-powered church service involved mixed reactions. Some commenters expressed skepticism about the use of AI-generated content, as they believe that Christianity focuses on the personal interaction between people and God. Others believe that the use of AI in church services may attract younger people to attend. Some commenters noted that religion should not rely too much on technology and should embrace community and spirituality. Others questioned the underlying use cases of AI-based religious content creation, with some speculating about the possibility of AI-based televangelists spamming people with content and hiding subscription-based services. Additionally, some commenters expressed concern about the idea of artificial religion.

### Artificial intelligence warning over human extinction labelled “publicity stunt”

#### [Submission URL](https://www.independent.co.uk/tech/rishi-sunak-university-of-oxford-san-francisco-government-people-b2349105.html) | 18 points | by [ojosilva](https://news.ycombinator.com/user?id=ojosilva) | [6 comments](https://news.ycombinator.com/item?id=36302594)

A University of Oxford professor of technology and regulation has called a recent open letter warning of the risk of AI wiping out humanity a "science fiction fantasy" and a "publicity stunt". Sandra Wachter, who described the probability of a "Terminator scenario" occurring as close to zero, said the attention the letter had drawn had detracted from pressing environmental, job-replacing and discriminatory issues. Dozens of experts in AI had signed the Centre for AI Safety's letter, which called for action to mitigate the risk of AI leading to human extinction, and was shared on social media by the UK Chancellor of the Exchequer, Rishi Sunak.

The discussion on the submission mainly consists of differing opinions on the validity of the warning about the risk of AI, with some users supporting the need to mitigate potential risks, and others dismissing the idea as a publicity stunt and highlighting other pressing issues like the environment and job replacement. There is also a discussion about the level of expertise of those who signed the warning letter and whether they have biased beliefs. One user believes that people warning of AI risks are doing so to protect their positions and interests, while another argues that such warnings should be taken seriously. The issue of the motives of those warning about AI is also noted, with one user suggesting that some experts have vested interests in building AIs in competition with others. Finally, there is a comment about the high marketing value of such warnings, suggesting that some warnings may be exaggerated for publicity purposes.
