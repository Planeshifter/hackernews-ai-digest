## AI Submissions for Fri Jan 09 2026 {{ 'date': '2026-01-09T17:09:24.747Z' }}

### “Erdos problem #728 was solved more or less autonomously by AI”

#### [Submission URL](https://mathstodon.xyz/@tao/115855840223258103) | 561 points | by [cod1r](https://news.ycombinator.com/user?id=cod1r) | [309 comments](https://news.ycombinator.com/item?id=46560445)

I’m missing the submission to summarize. Please share one of the following:
- The Hacker News link plus the article text (or key excerpts)
- A paste/screenshot of the post and notable comments

Also tell me your preferred format: quick TL;DR (50–75 words), 3–5 bullets, or a 1–2 paragraph digest.

Based on the discussion provided, here is the summary in a **bulleted format**:

**Discussion Summary:**

*   **The "Specification" Problem:** The primary debate focuses on where the risk lies in AI-generated formal proofs. Users agree that while the Lean theorem prover guarantees the *logic steps* are valid (verification), it cannot guarantee that the **theorem statement** itself describes the intended problem correctly (validation). If the AI translates the English prompt into the wrong mathematical definition, the proof is correct but irrelevant.
*   **Comparison to Software Verification:** Commenters draw a distinction between "bugs" in code versus proofs. Reading AI-generated code (like Python) requires checking for subtle logic errors. In contrast, verifying AI-generated Lean is easier because the compiler catches the logic errors; the human only needs to verify the initial definitions and the final theorem statement.
*   **Difficulty of Formalization:** A heated sub-thread debates whether formalizing math problems is "trivial." While one user dismisses the example problem as "homework level" (simple algebra/integrals), another—self-identifying as a math professor—argues that formalizing "intuitive" branches of math (like geometry or topology) is typically much harder for computers than formalizing complex algebra or logic puzzles.
*   **Trust and Process:** The consensus suggests a workflow where humans act as the "client" defining the specification (the theorem statement), and the AI acts as the builder finding the proof path. As long as the human understands the specific syntax of the statement, the actual proof text generated by the AI does not need to be read or understood in detail.

### My article on why AI is great (or terrible) or how to use it

#### [Submission URL](https://matthewrocklin.com/ai-zealotry/) | 152 points | by [akshayka](https://news.ycombinator.com/user?id=akshayka) | [214 comments](https://news.ycombinator.com/item?id=46557057)

AI Zealotry: A senior OSS Python dev’s field notes on building with AI (Claude Code) today. The pitch: experienced engineers should lean in—AI makes development more fun, faster, and expands your reach (e.g., frontend). The catch: LLMs do produce junk, code review is still the bottleneck, and naive “click yes” workflows feel dehumanizing. The remedy is to climb the abstraction ladder and automate the glue.

Why it matters
- Treat AI like the compiler shift from assembly: you trade some low-level understanding for massive leverage—if you add the right guardrails.
- Senior engineers are best positioned to “vibe code” without shipping slop.

Big ideas
- Minimize interruptions: stop doing simple, repetitive tasks; automate them so you can think and design.
- Don’t rely on agents remembering CLAUDE.md/AGENTS.md; encode rules as enforceable automation.

Practical tactics (Claude Code)
- Hooks > docs: Use Hooks to enforce workflow rules the agent forgets.
  - Example: intercept bare “pytest” and force “uv run pytest”.
- Smarter permissions: The built-in allowlist is too coarse (prefix-only). Replace it with a Hook-backed Python/regex policy so you can express nuanced, composable rules. Let the agent propose updates when it hits new patterns.
- Fail-once strategy: It’s often faster to let the agent fail, learn, and correct than to over-spec upfront, once guardrails are in place.
- Quality-of-life hooks: Add sound/notification hooks for long runs to reduce context switches.

Caveats acknowledged
- LLMs can generate junk; writing code yourself builds understanding; review remains the slow part; naive permission prompts are alienating. The article’s stance: accept the tradeoff, but engineer your workflow so AI removes toil and you keep the thinking.

Here is a summary of the discussion regarding the post "AI Zealotry: A senior OSS Python dev’s field notes on building with AI."

**The Abstraction Ladder vs. "Glue Code"**
A central theme of the debate is the nature of the code AI produces. Commenters questioned whether developers are becoming mere implementers of middleware, simply "wrapping existing APIs" rather than innovating.
*   **The Assembly Analogy:** Users debated the OP’s comparison of AI to the shift from Assembly to high-level languages. While some agreed that abstracting away "toil" is natural, others argued that targeting Python/JS is purely about leveraging large training datasets, not performance.
*   **Mechanical Reproduction:** One perspective suggested AI operates as "mechanical reproduction" of existing logical patterns. Users noted that while AI excels at boilerplate (like setting up bare-bones HTML/CSS/JS without frameworks), it often results in "mashup" projects rather than novel architecture.

**Complexity and Context Limits**
Despite the post's optimism for senior engineers, commenters highlighted the boundaries of current models (like Opus or Claude 3.5).
*   **Small vs. Large Projects:** AI was praised for scoped, single-purpose tools—such as a user who quickly wrote a C++ CLI for beat-matching MP3s or a Postgres BM25 search extension.
*   **The Enterprise Wall:** Conversely, developers working on massive, complex codebases (e.g., the Zed editor in Rust or 200+ project enterprise repos) noted that AI struggles with large contexts. It is useful for explaining bugs or writing docs, but often "hallucinates" or fails when managing intricate dependencies across massive scopes.

**Workflow Tactics: Fun with "Hooks"**
The article's mention of using "hooks" to automate workflow rules sparked a specific sub-thread on improving quality of life.
*   **Auditory Feedback:** Rather than staring at a terminal, users shared scripts to make their systems verify completion via sound—ranging from simple Morse code audio pings to using ElevenLabs API calls to have the computer verbally announce, "I have finished your project," allowing the developer to step away during generation.

**Skepticism: Licensing and "Vibe Coding"**
Significant distinct criticisms arose regarding the ethics and long-term viability of AI-generated code.
*   **"LLM-Washing":** One commenter argued that relying on AI for UI components or logic is essentially "laundering" open-source licenses—copying code without attribution and stripping away the original license constraints.
*   **Disposable Code:** The concept of "vibe coding" (treating code as a compile cache you don't need to read) faced pushback. Critics argued that unless the code is truly disposable, readability and correctness still matter, and debugging "slop" generated by an LLM can be more painful than writing it from scratch.

### Show HN: EuConform – Offline-first EU AI Act compliance tool (open source)

#### [Submission URL](https://github.com/Hiepler/EuConform) | 68 points | by [hiepler](https://news.ycombinator.com/user?id=hiepler) | [41 comments](https://news.ycombinator.com/item?id=46557823)

EuConform is a new open‑source tool to help teams prep for the EU AI Act. It walks you through risk classification aligned with Article 5 (prohibited) and Article 6 + Annex III (high‑risk), runs bias checks using the CrowS‑Pairs methodology with log‑probability scoring, and exports Annex IV‑style technical documentation as a PDF.

Notable: everything runs 100% client‑side via transformers.js (WebGPU), so no data leaves your browser. It’s privacy‑first (no tracking/cookies), WCAG 2.2 AA accessible, dark‑mode friendly, and offers English/German UI. For stronger bias testing, it can hook into local models via Ollama (supports Llama, Mistral, Qwen; best results with logprobs-enabled models).

The project maps to key EU AI Act sections (Arts. 5–7, 9–15; Recital 54) and flags that high‑risk obligations phase in by 2027. It’s a technical aid—not legal advice—and doesn’t replace notified-body assessments.

Getting started: deploy on Vercel or run locally (Node 18+, pnpm). Repo: Hiepler/EuConform (MIT; EUPL file also present). Why it matters: with compliance deadlines approaching, this offers an offline, auditable way to classify risk, measure bias with reproducible protocols, and generate documentation early.

Here is a summary of the discussion:

The project's name sparked immediate feedback, with several users remarking that "EuConform" sounds dystopian ("You Conform") or reminiscent of *1984*, though the creator explained it is simply a contraction of "EU Conformity."

A significant portion of the technical discussion focused on the author's use of AI coding assistants. One user argued for "intellectual honesty" and explicit disclosure when using AI to generate code, but others countered that the final utility matters more than the tooling used. The author (`hplr`) jumped in to clarify that while AI helped with boilerplate and architecture, the core logic and compliance mapping were developed manually.

The substantial remainder of the thread evolved into a debate regarding the EU's regulatory environment. Critics described compliance-first tools as symptomatic of a bureaucratic culture that stifles innovation, characterizing the EU market as difficult or "anti-business" compared to the US and East Asia. Defenders of the regulations argued that these frameworks are necessary to protect consumers and human rights, contrasting EU protections with the privacy practices of major American tech companies. This escalated into a philosophical exchange on whether checking technological progress with regulation is "anti-human" or necessary to prevent societal harm.

### Anthropic blocks third-party use of Claude Code subscriptions

#### [Submission URL](https://github.com/anomalyco/opencode/issues/7410) | 592 points | by [sergiotapia](https://news.ycombinator.com/user?id=sergiotapia) | [490 comments](https://news.ycombinator.com/item?id=46549823)

Claude Max appears to be down for many OpenCode users. A GitHub issue titled “Broken Claude Max” (#7410) reports that Claude Max abruptly stopped working in OpenCode v1.1.8 and continues to fail after reconnect attempts. The thread has hundreds of thumbs-up reactions, suggesting it’s widespread; there are no steps to reproduce, screenshots, or clear environment details, and it’s tagged as a bug. No official fix or root cause has been posted yet, so affected users are watching the issue for updates and likely falling back to other models in the meantime.

**OpenCode vs. Anthropic: The $200 Token Arbitrage**
Commenters suspect the outage is an intentional block by Anthropic to close a pricing loophole. Users note that OpenCode allowed developers to bypass standard pay-as-you-go API costs (which can exceed $1,000/month for heavy users) by leveraging Anthropic’s flat-rate $200/month "Claude Code" subscription. By using the third-party client to access this "all-you-can-eat" token buffet, users were effectively getting enterprise-level compute at a massive discount, making the crackdown financially inevitable.

**The Battle for Developer Mindshare**
The discussion pivots to business strategy, with users arguing that Anthropic is trying to avoid becoming a "dumb pipe" for other tools. By forcing users onto their official CLI, Anthropic protects its brand interface and direct customer relationship. While some defend this as necessary to prevent "intermediation" by competitors, others criticize the closed-source nature of the official tool, arguing that wrapper tools like OpenCode provided necessary flexibility (like provider swapping) that the official ecosystem lacks.

**Tool Quality and "Loss Leaders"**
Opinions on the tools themselves are mixed. Some users praise the official Claude Code TUI and the performance of the Opus model within it, suggesting the $200 subscription is a "loss leader" specifically designed to capture market share from tools like GitHub Copilot. However, others express frustration with the lack of local model support and the fragility of the official CLI, noting that OpenCode’s disruption leaves them without a reliable workflow until an official fix or a new workaround emerges.

### Slopware.wtf – Roasting AI-Generated Garbage Software

#### [Submission URL](https://slopware.wtf/) | 22 points | by [airhangerf15](https://news.ycombinator.com/user?id=airhangerf15) | [8 comments](https://news.ycombinator.com/item?id=46549897)

Slopware.wtf launches: roasting AI‑generated apps so bad they’re good

What it is:
- A new site cataloging “beautiful disasters” of AI‑assisted development—think The Daily WTF for the LLM era.
- Kicks off with an intro roast (2/10) and “TodoApp Supreme” (3/10), a wildly overengineered React to‑do list.

How it works:
- Readers submit GitHub repos or site URLs; the team “roasts the code, not the people.”
- Newsletter via Buttondown; tongue‑in‑cheek stats include ∞ bugs found, 42 LOLs per post, 0 feelings hurt.

Why it matters:
- Captures growing backlash to AI‑generated slop flooding the web and GitHub.
- Uses humor to highlight real pitfalls: overcomplication, cargo‑cult patterns, and fragile scaffolding from AI tools.
- Sparks a broader question for HN: Can public roast culture improve code quality without veering into dunking, and how should teams gate AI‑assisted contributions?

Hacker News users wasted no time turning the premise back on the creators, pointing out the meta-irony that a site dedicated to mocking "AI slop" appears to be built from the very same material. Commenters criticized the site's own design, describing the "horrendous" colors as "frying eyeballs" and noting the reliance on generic "CSS glow" and fabricated statistics common in AI-generated templates.

Key points from the discussion:
*   **The Irony:** Multiple users labeled the site "self-fulfilling humor," observing that it looks and functions exactly like the "garbage software" it intends to roast.
*   **Technical Failures:** One user attempted to submit Slopware.wtf to its own submission form, discovering validation errors and a broken RSS feed—effectively roasting the roaster.
*   **Design Critique:** The visual choices drew harsh feedback, with users complaining about the aesthetic and suggesting the creators likely prompted an LLM to "create the biggest pile of garbage" to critique others' work.

