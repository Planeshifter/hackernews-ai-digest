## AI Submissions for Fri Jun 09 2023 {{ 'date': '2023-06-09T17:10:47.491Z' }}

### U-Net CNN in APL: Exploring Zero-Framework, Zero-Library Machine Learning

#### [Submission URL](https://dl.acm.org/doi/10.1145/3589246.3595371) | 83 points | by [tosh](https://news.ycombinator.com/user?id=tosh) | [11 comments](https://news.ycombinator.com/item?id=36256869)

Researchers at Dyalog have explored the U-net architecture for convolutional neural networks (CNNs) using the APL notation. They compared the performance and language design for neural network programming in APL against specialized neural network frameworks such as PyTorch. The researchers found that the "from scratch" APL source code was more concise and complete than PyTorch's reference implementation, although more foreign in language design. Despite the naive implementation of Co-dfns and their own code, GPU and CPU performance were within a factor of 2.2-2.4 compared to PyTorch. This suggests significant avenues of exploration for machine learning language design and implementation, inside and outside the APL community.

The discussion on the submission mostly focuses on the implementation and benchmarking of the neural network architecture using APL notation. Several comments point out that the APL implementation is concise and impressive, but the benchmarking results are questionable due to the small batch size used. Others suggest related work in GPU-based array programming, such as Futhark, and wonder about using kdb+. Some members of the discussion share their own experiences implementing backpropagation and finding it helpful to understand the mathematics behind it. Someone else suggests that this research could be useful for ML compiler engineers and suggests reaching out to graduate students for further research.

### Re: I Don't Use Copilot

#### [Submission URL](https://vivekhaldar.com/articles/re--why-i-don-t-use-copilot/) | 48 points | by [gandalfgeek](https://news.ycombinator.com/user?id=gandalfgeek) | [93 comments](https://news.ycombinator.com/item?id=36261909)

In a recent blog post, Vivek Haldar responds to Ed Summers's piece on why he doesn't use Copilot. Haldar argues that while there are concerns about Copilot's use of open source code on GitHub and the potential for it to stifle innovation, the benefits of using the AI writing assistant far outweigh the risks. Haldar uses Copilot for personal scripts and has found it extremely helpful in reducing the time and effort required to complete projects. He believes that while the tool may lead to more code being generated, it can also increase overall productivity by reducing friction and frustration. Ultimately, Haldar believes that the benefits of using Copilot as a tool far outweigh the potential negative consequences.

The discussion in the comments is mixed, with some users stating that they sometimes accept Copilot's suggestions that are incorrect and others stating that the tool has increased their productivity. Additionally, there is a debate about the accuracy and performance of Copilot, with some users questioning its usefulness and others suggesting ways to improve its performance. Some users also discuss the potential privacy concerns of using Copilot.

### Microsoft DeviceScript – TypeScript for Tiny IoT Devices

#### [Submission URL](https://github.com/microsoft/devicescript) | 259 points | by [stunt](https://news.ycombinator.com/user?id=stunt) | [143 comments](https://news.ycombinator.com/item?id=36254135)

Microsoft has released a new project, DeviceScript, which is a TypeScript-based language for IoT devices such as ESP32 and RP2040. The project's custom VM bytecode can run in constrained environments, and the platform is designed to provide a TypeScript developer experience to low-resource microcontroller-based devices. Interested parties can join discussions on the project's technical preview and provide feedback. Contributions to the open-source project are also welcome.

### Who is working on forward and backward compatibility for LLMs?

#### [Submission URL](https://huyenchip.com/2023/04/11/llm-engineering.html#backward_and_forward_compatibility) | 94 points | by [nijfranck](https://news.ycombinator.com/user?id=nijfranck) | [36 comments](https://news.ycombinator.com/item?id=36258882)

The use of large language models (LLMs) is on the rise, but productionizing LLM applications can be difficult due to the ambiguous nature of natural languages and the nascent nature of the field. The flexibility in user-defined prompts could lead to silent failures, while the ambiguity in LLMs’ generated responses can lead to ambiguous output formats and inconsistency in user experiences. To mitigate these issues, OpenAI is working to increase their models’ reliability, while some developers accept the ambiguity and build their workflows around it. Prompt engineering using fewshot learning and as much engineering rigor as possible can help make prompt engineering systematic.

The discussion on this submission revolves around potential issues and solutions related to productionizing large language models (LLMs), especially regarding their reliability and consistency in user experience. Some users suggest that OpenAI needs to increase the reliability of their models, while others believe that developers should build their workflows around the ambiguity of LLMs. There are also discussions about backward compatibility and version control of LLMs. Some users argue that changing prompts and pre-processing can affect the results, while others focus on the differences between LLMs that support natural language and those that don't. Users also mention fine-tuning, embedding, and filtering prompts as possible solutions to address issues related to productionization. Additionally, users discuss the importance of testing LLMs on multiple benchmark generation tasks.

### Artificial brains help understand real brains

#### [Submission URL](https://www.economist.com/science-and-technology/2023/05/24/artificial-brains-are-helping-scientists-study-the-real-thing) | 80 points | by [Jeff_Brown](https://news.ycombinator.com/user?id=Jeff_Brown) | [46 comments](https://news.ycombinator.com/item?id=36263105)

The use of artificial neural networks (ANNs) is helping neuroscientists understand the inner workings of the human brain, despite the fact that ANNs are mere cartoons of real brains and fail to capture their complexity. By programming ANNs to identify objects from photographs, neuroscientists have discovered some remarkable similarities in the workings of the human brain compared to the artificial brain. The study of ANNs has also led to discoveries in the human visual and auditory cortex, such as the discovery of a region in the human brain that responds to pictures of food after an ANN trained on image-recognition tasks produced a group of artificial neurons devoted to classifying foodstuffs specifically. Additionally, there is apparent interaction between software and natural biology, with researchers finding it possible to feed brain activity into ANNs.

The discussion in the comments covers various topics, including debates over whether ANNs truly capture the complexity of the human brain, discussions on the limitations of current research, and arguments over the validity of comparing ANNs to the human brain. Some commenters also touch on the practical applications of this research, such as human speech processing and the development of natural language processing algorithms.

### Flipper Zero Self Destructs an Electricity Smart Meter

#### [Submission URL](https://www.rtl-sdr.com/flipper-zero-self-destructs-an-electricity-smart-meter/) | 176 points | by [kungfudoi](https://news.ycombinator.com/user?id=kungfudoi) | [207 comments](https://news.ycombinator.com/item?id=36253591)

A recent video on YouTube demonstrated the destructive power of the Flipper Zero, an affordable handheld RF device for pentesters and hackers. In the video, Peter Fairlie used the Flipper Zero to wirelessly turn a power meter on and off, which controlled the power to a large AC unit. Switching the meter on and off under a heavy load resulted in the meter self-destructing and releasing smoke. While smart meters are supposed to disconnect and reconnect under load, the disconnection items are not built for frequent switching, resulting in the possibility of overloads and damage.

A video on YouTube recently demonstrated the destructive power of the Flipper Zero, an affordable handheld RF device for pen testers and hackers. The video shows Peter Fairlie using the device to wirelessly turn a power meter on and off, resulting in the meter self-destructing and releasing smoke. Commenters on Hacker News discussed the potential dangers of such a device, with some expressing concerns about the security of the electric grid and others emphasizing the importance of proper security protocols and practices. Others criticized the complexity of IoT devices and the lack of trust in infrastructure and security systems. Some commenters also noted the importance of contracting documentation and the problems with overcomplicating security measures.

### Lawyers blame ChatGPT for tricking them into citing bogus case law

#### [Submission URL](https://apnews.com/article/artificial-intelligence-chatgpt-courts-e15023d7e6fdf4f099aa122437dbb59b) | 147 points | by [glitcher](https://news.ycombinator.com/user?id=glitcher) | [173 comments](https://news.ycombinator.com/item?id=36257545)

Two lawyers appearing before a judge in Manhattan federal court have blamed ChatGPT, a groundbreaking AI program, for tricking them into including invented legal research in a court filing. Attorneys Steven A. Schwartz and Peter LoDuca included references to past court cases they thought were real, but which were untrue and had been suggested by the chatbot. Schwartz said that he was “operating under a misconception... that this website was obtaining these cases from some source I did not have access to”. Microsoft has invested around $1bn in OpenAI, the company behind ChatGPT.

Two lawyers appearing before a judge in federal court blamed AI program ChatGPT, developed by OpenAI and backed by Microsoft, for tricking them into including invented legal research in a court filing. Commenters on Hacker News debated whether ChatGPT's erroneous recommendations could be attributed to "lazy, sloppy, fraudulently billed" lawyers, or the AI's lack of nuance. Some argued that it was a shared responsibility, while others blamed the lawyers for not verifying the information provided by ChatGPT. Overall, many users highlighted the importance of diligence in verifying sources, particularly in a legal context.

### OpenAI faces defamation suit after ChatGPT fabricated another lawsuit

#### [Submission URL](https://arstechnica.com/tech-policy/2023/06/openai-sued-for-defamation-after-chatgpt-fabricated-yet-another-lawsuit/) | 27 points | by [ndsipa_pomu](https://news.ycombinator.com/user?id=ndsipa_pomu) | [27 comments](https://news.ycombinator.com/item?id=36261069)

Mark Walters, a prominent commentator on gun rights and the Second Amendment Foundation (SAF), is suing OpenAI for defamation resulting from the misinformation generated by its generative AI chatbot, ChatGPT. Journalist Fred Riehl uncovered that ChatGPT wrongly connected the dots and made false and potentially libellous statements about Walters, including accusing him of embezzlement and fraud in relation to his time serving at SAF. Walters' lawsuit is seeking unspecified monetary damages, citing reputational loss that could impact future job opportunities or radio listenership. This is likely the first defamation lawsuit resulting from ChatGPT's so-called "hallucinations", where the chatbot fabricates information.

The discussion on Hacker News debates the legal implications of the lawsuit and the liability of OpenAI for the actions of its chatbot. Some commenters argue that OpenAI should include disclaimers to absolve themselves of liability, while others suggest that the allegations against Walters need to be proven before a defamation claim can be made.

### Covid-19 can cause brain cells to ‘fuse’

#### [Submission URL](https://qbi.uq.edu.au/article/2023/06/covid-19-can-cause-brain-cells-%E2%80%98fuse%E2%80%99) | 45 points | by [belltaco](https://news.ycombinator.com/user?id=belltaco) | [10 comments](https://news.ycombinator.com/item?id=36251906)

Researchers at The University of Queensland have found evidence that COVID-19 can cause brain cells to fuse, leading to chronic neurological symptoms. The study showed that the SARS-CoV-2 virus alters the function of the nervous system by causing neurons to undergo a cell fusion process. This causes the neurons to either start firing synchronously or stop functioning altogether. The discovery offers an explanation for persistent neurological effects after a viral infection, suggesting the potential for new treatments for neurological diseases. The research was published in Science Advances.

Some commenters discuss the potential for new treatments for neurological diseases using this research, while others question the efficacy of mRNA vaccines and their ability to replicate the spike protein produced by the virus. One user notes that the mRNA vaccines do not include the full-length spike protein and do not bind directly to brain cells, while another user corrects some of the scientific terminology used in previous comments. Overall, the discussion emphasizes the importance of understanding the potential effects of the spike protein on the brain and the development of new treatments for neurological symptoms.

