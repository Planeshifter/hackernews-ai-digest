## AI Submissions for Sat Nov 22 2025 {{ 'date': '2025-11-22T17:07:41.162Z' }}

### Terence Tao: At the Erdos problem website, AI assistance now becoming routine

#### [Submission URL](https://mathstodon.xyz/@tao/115591487350860999) | 45 points | by [dwohnitmok](https://news.ycombinator.com/user?id=dwohnitmok) | [7 comments](https://news.ycombinator.com/item?id=46017972)

I don’t have the submission details yet. Please share one of the following so I can write the digest item:
- Hacker News thread URL
- Article URL
- Title + a brief description
- Or paste the text you want summarized

Optional preferences:
- Length: ultra-short (2–3 sentences), short bullets, or ~150 words
- Emphasis: why it matters, key numbers, technical/implementation details, controversy, author’s stance
- Include HN discussion takeaways? If yes, share the HN thread link
- Tone: neutral, punchy, or playful

Example output style:
- Headline
- The gist (1–2 lines)
- Why it matters
- Key details (3–5 bullets)
- HN chatter (if provided)
- Link(s)

Based on the comment thread provided, here is a digest of the discussion involving Terence Tao, AI, and Mathematics.

**Headline:** Discussions on Terence Tao, AI, and the Lean Theorem Prover

**The gist:**
A debate regarding Field Medalist Terence Tao’s advocacy for using AI and the Lean theorem prover to formalize mathematics, contrasting modern AI assistance against traditional "pen and paper" genius.

**Why it matters:**
As a leading figure in mathematics, Tao’s embrace of AI signals a paradigm shift where machine learning moves from simple calculation to assisting in high-level proofs and deciphering complex academic papers.

**Key themes:**
*   **Accessibility:** AI is lowering the barrier to entry for complex math.
*   **Shift in Method:** The move from informal mathematical intuition to rigorous machine verification.
*   **Hype vs. History:** The tension between modern tooling and the legacy of solitary mathematicians like Perelman and Wiles.

**HN Chatter:**
*   **AI as Tutor:** Users highlighted the value of AI in breaking down "math-heavy ML papers" into simplified explanations or pseudocode, helping those who may have forgotten concepts learned "25+ years [ago]."
*   **"Vibe" Coding to Rigor:** One commenter conceptualized formalizing math as a logical extension of "vibe coding"—unifying informal methods with the mathematical rigor they deserve, potentially leading to an AI "Move 37" moment (a reference to AlphaGo's creative move) in math.
*   **Skepticism & Comparisons:** Detractors questioned the necessity of AI, noting that giants like Perelman (Poincaré conjecture) and Wiles (Fermat's Last Theorem) succeeded without it. Some feel Tao is "constantly hyping Lean."
*   **The Celebrity Factor:** Others argued that Tao’s fame naturally attracts a "larger-than-usual number of kooks" or varying quality of feedback compared to standard posts.
*   **Lighthearted:** A suggestion that we may soon need to assign "Erdős numbers" to AI agents.

### Show HN: I built a wizard to turn ideas into AI coding agent-ready specs

#### [Submission URL](https://vibescaffold.dev/) | 59 points | by [straydusk](https://news.ycombinator.com/user?id=straydusk) | [32 comments](https://news.ycombinator.com/item?id=46018229)

AI-Powered Spec Generator: turns a single structured conversation into production-ready planning docs. The tool promises to take an abstract idea and generate a full bundle of build artifacts—ONE_PAGER.md, DEV_SPEC.md, PROMPT_PLAN.md, and AGENTS.md—plus architecture diagrams.

What it does
- Product definition: converts fuzzy ideas into concrete MVP requirements and user stories.
- Tech architecture: drafts schemas, API routes, and security considerations.
- Development plan: breaks work into sequential, LLM-testable prompt chains.
- Agent directives: writes system prompts (AGENTS.md) for autonomous coding agents.

Why it matters
- Aims to replace vague prompt iteration with clear, reviewable specs.
- Useful for founders, PMs, and solo devs who want to bootstrap scope, architecture, and agent workflows quickly.

Caveats to watch
- Output quality will hinge on the structure/clarity of the initial conversation.
- Generated specs and diagrams still need human review for feasibility and security.

**Evaluation of "Spec-First" Workflows**
Commenters generally validated the premise of the tool, noting that good specifications are critical for getting high-quality code from other AI agents (like Lovable or Cursor). One user noted that the "reverse Socratic method"—where the AI identifies ambiguity and asks the user clarifying questions—effectively removes the "garbage in, garbage out" risk common in prompting.

**Context Drift and Execution**
There was a debate on why the tool stops at generating specs rather than executing the code immediately.
*   **Context limits:** Users argued that separating planning from execution is the standard approach to prevent "context drift," improving efficacy by keeping the LLM focused on one stage at a time.
*   **Agent integration:** The creator clarified that the generated files (like `ONE_PAGER.md`) are intended to serve as high-quality context inputs for other autonomous coding agents.

**Quality and Skepticism**
Skepticism remained regarding the reliability of AI planning. One commenter warned that without close manual oversight, AI-generated plans can become "trash," citing examples of AI writing tautological tests (e.g., asserting `true == true`) just to pass checks. The creator agreed, emphasizing that the tool is designed to create a "draft" that must be verified by a human, not to replace the human entirely.

**Feedback and Bug Reports**
*   **Copywriting:** Users debated the landing page copy "turn messy ideas into specs." Some felt "messy" had negative connotations, while others felt it accurately described the pre-spec brainstorming phase.
*   **Technical issues:** Several users encountered "placeholder" text or empty one-pagers; the creator attributed this to hitting GPT-4-mini API limits and promised fixes for the "step-jumping" bugs.
*   **Complexity:** The creator noted they built this because existing alternatives (like SpecKit or BMAD) felt over-engineered or too abstract.

### Show HN: PolyGPT – ChatGPT, Claude, Gemini, Perplexity responses side-by-side

#### [Submission URL](https://polygpt.app) | 22 points | by [ncvgl](https://news.ycombinator.com/user?id=ncvgl) | [13 comments](https://news.ycombinator.com/item?id=46013984)

An open‑source desktop app promises to end tab‑hopping by letting you type a prompt once and send it to multiple LLMs—ChatGPT, Gemini, Claude, and more—then compare their answers side‑by‑side in real time. It’s free, privacy‑focused, and available for macOS, Windows, and Linux, with the code on GitHub. Handy for prompt tuning and model selection, it mirrors your input across providers so you can quickly see which model suits your task.

**Daily Hacking Digest: AI Comparison Tool Discussion**

Users discussed the utility and architecture of the open-source multi-LLM comparison tool, focusing on privacy, deployment methods, and feature requests.

*   **Web App vs. Native Client:** Several users expressed a strong reluctance to install native applications from unknown sources. There was a specific demand for a web-based version, with one user noting they prefer the control and customization of their own browser over what they perceive as likely being an Electron wrapper.
*   **Implementation & APIs:** A technical sidebar examined the app's backend method, debating the differences between embedding web apps (UI wrapping) versus using direct API access. It was noted that while embedding is free, API access allows for better context management and automated selection of responses.
*   **LLM-as-a-Judge:** A significant portion of the feedback focused on adding a feature where an LLM automatically evaluates the outputs of the other models to select the best answer. Suggestions included creating a "blind jury" of models or a code interface for judging. One user referenced an experiment involving AI agents voting on answers that eventually led to the agents "conspiring," highlighting the complexity of democratic AI evaluation.
*   **Alternatives:** Commenters pointed out existing tools that offer similar functionality, specifically mentioning "Open WebUI" and "Ninja Chat."

### New Apple Study Shows LLMs Can Tell What You're Doing from Audio and Motion Data

#### [Submission URL](https://9to5mac.com/2025/11/21/apple-research-llm-study-audio-motion-activity/) | 79 points | by [andrewrn](https://news.ycombinator.com/user?id=andrewrn) | [32 comments](https://news.ycombinator.com/item?id=46015578)

Apple says LLMs can infer your activity from sensor data — without hearing raw audio

- In a paper dubbed “Using LLMs for Late Multimodal Sensor Fusion for Activity Recognition,” Apple researchers show that large language models can fuse text summaries from audio and IMU models to recognize what a user is doing—no raw audio or bespoke multimodal training needed.
- Method: smaller models turn 20-second audio and motion (accelerometer/gyroscope) snippets into captions and labels; LLMs (Gemini 2.5 Pro, Qwen-32B) combine those to classify activities.
- Dataset: a curated Ego4D subset spanning 12 everyday activities (e.g., vacuuming, cooking, dishes, TV, workouts, ball sports).
- Results: zero- and one-shot F1 scores were significantly above chance in both closed-set (12 options provided) and open-ended settings; one-shot examples improved accuracy further.
- Why it matters: a lightweight way to boost health/activity features when aligned multimodal training data is scarce, potentially without adding app-specific multimodal model memory/compute.
- Apple also released segment IDs, timestamps, prompts, and one-shot examples to help reproduce the results.

Caveats HN will ask about: only 12 classes on curated clips; inference cost/latency of LLM fusion; on-device feasibility; and whether text-only summaries sufficiently mitigate privacy risk.

Here is the daily digest summarizing the discussion around Apple's research on LLM-based sensor fusion.

### **Apple says LLMs can infer activity from sensors without hearing raw audio**

**The Submission:**
Apple researchers have published a paper on "Late Multimodal Sensor Fusion," demonstrating that Large Language Models (LLMs) can accurately infer user activities—such as cooking, vacuuming, or watching TV—without processing raw audio or requiring heavy multimodal training. Instead, smaller, specialized models convert short clips of motion data (accelerometer/gyroscope) and audio into text captions. The LLM then fuses these text summaries to classify the activity. The logic is that this lightweight approach protects privacy (by not feeding raw audio to the LLM) and reduces computational overhead while yielding high accuracy on everyday tasks.

**The Discussion:**
The Hacker News discussion quickly pivoted from the technical methodology to broader concerns about privacy, the "Panopticon," and the current state of wearable heuristics.

**1. The Digital Panopticon and "1984"**
The dominant theme was the erosion of privacy. Multiple users drew comparisons to Orwell’s *1984* Telescreens, though others argued that modern smartphones have long surpassed that level of surveillance because we voluntarily carry them everywhere ("The presence we carry in our pockets").
*   **Data Retention:** Commenters worried that even if the current inference is done locally or "privately," the data collected today is being improved by ransomware gangs or corporations to be analyzed later once technology improves.
*   **Side Channels:** Users pointed out that using motion sensors (IMU) for eavesdropping is a known concept (referencing "Android side-channel attacks"), noting that seemingly benign permissions often leak the most sensitive data.

**2. Technical Feasibility and Battery Drain**
There was technical debate regarding the architecture found in the paper.
*   **Who does the work?** Some questioned if the LLM is merely "fronting" for the smaller models. If a small model has to generate the text caption "user is vacuuming" from motion data, the LLM is just interpreting that text, meaning the heavy lifting of pattern recognition is actually happened in the pre-processing stage.
*   **Permission Loops:** One user noted a practical downside to privacy controls: denying sensor permissions on Android can cause apps to enter "busy loops" constantly checking for the permission, which tanks battery life faster than just granting access.

**3. Apple Watch’s Current Reliability**
The discussion detoured into a critique of Apple's current activity detection.
*   **Running Detection:** Users complained that the Apple Watch (even newer Series 8 models) lags behind older competitors (like 2015-era Samsung Gears) in auto-detecting runs.
*   **False Positives:** Defenders argued Apple intentionally delays detection to gather a larger data window, prioritizing zero false positives over speed.
*   **Implications:** This debate highlighted skepticism about the paper's "high accuracy" claims, given that highly-tuned, specific heuristics on current devices still struggle with basic tasks like "detecting a run."

**4. The "Late Fusion" Loophole**
While the paper frames the text-only input to the LLM as a privacy feature, commenters argued that if an LLM can infer "cooking" from "rhythmic chopping sounds + arm motion," the privacy distinction is semantic. The *meaning* of the activity is compromised, even if the raw audio wasn't technically "heard" by the large model. One user noted this enables "universal tracking" by validating that AI is now the "killer app" for surveillance.

### Google tells employees it must double capacity every 6 months to meet AI demand

#### [Submission URL](https://arstechnica.com/ai/2025/11/google-tells-employees-it-must-double-capacity-every-6-months-to-meet-ai-demand/) | 51 points | by [cheshire_cat](https://news.ycombinator.com/user?id=cheshire_cat) | [35 comments](https://news.ycombinator.com/item?id=46013463)

Google says it must double AI serving capacity every six months

Ars Technica (via CNBC) reports that Google told employees it needs a 1000x scale-up in 4–5 years to meet AI demand—while holding costs and power roughly flat. At an all-hands, AI infrastructure lead Amin Vahdat called infra “the most critical and most expensive” part of the AI race. Chip shortages (Nvidia’s AI GPUs are “sold out”) are already throttling rollouts; Sundar Pichai said Veo’s wider release was capped by compute.

Key details:
- Plan to hit targets via three levers: more data centers, more efficient models, and custom silicon.
- Google’s 7th-gen TPU “Ironwood” is now GA; Google claims ~30x power efficiency vs its 2018 Cloud TPU.
- Competition is scaling too: OpenAI’s “Stargate” plan with partners aims for six US mega–data centers, ~$400B spend in three years and ~7 GW capacity to serve ChatGPT’s large weekly user base (Ars cites 800M).
- Nvidia’s data center revenue jumped $10B in a quarter and AI chips remain constrained.

Why it matters:
- Doubling capacity every six months suggests an aggressive, Moore’s-Law-like cadence—but with flat cost/energy targets, demanding major efficiency gains.
- Infra scarcity is now a product limiter for AI features across Search, Gmail, Workspace, and rivals’ apps.
- Despite “AI bubble” worries, Google is prioritizing underinvestment risk; Pichai says 2026 will be “intense.”

Here is a summary of the discussion regarding the Google AI infrastructure story:

**Discussion Summary**
The community debated the legitimacy of Google's projected demand, with a strong divide on whether the "1000x" figure represents organic market interest or internal initiatives ("force-feeding" AI into Search and Workspace). While some acknowledged the massive traffic of ChatGPT as proof of general AI appetite, others argued that Google is artificially inflating limits by integrating resource-intensive features users may not want.

Significant skepticism was directed at the physical feasibility of Google’s roadmap. Commenters argued that doubling capacity every six months is mathematically impossible given current constraints on semiconductor density, energy production, and supply chain realities. This led to speculation about an impending "AI bubble"; users discussed the potential aftermath, wondering if a crash would result in cheap inference for startups, repurposed data centers (perhaps as Amazon distribution hubs), or simply massive amounts of e-waste. User experience was also a recurring pain point, with complaints about "Clippy-like" intrusions in Google Docs and hallucinated facts in AI Search overviews.

