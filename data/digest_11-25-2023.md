## AI Submissions for Sat Nov 25 2023 {{ 'date': '2023-11-25T17:10:01.487Z' }}

### AI Art Generators Can Be Fooled into Making NSFW Images

#### [Submission URL](https://spectrum.ieee.org/dall-e) | 72 points | by [thunderbong](https://news.ycombinator.com/user?id=thunderbong) | [143 comments](https://news.ycombinator.com/item?id=38415219)

Researchers from Johns Hopkins University and Duke University have developed an algorithm called SneakyPrompt that can trick popular text-to-image generative AIs, such as DALL-E 2 and Midjourney, into producing NSFW images. They started with prompts that safety filters would block, and then gradually adjusted the alternative words within these prompts to find commands that could bypass the safety filters and generate the desired images. The researchers found that nonsense words could prompt these generative AIs to produce innocent pictures. The findings will be presented at the IEEE Symposium on Security and Privacy in May 2024.

The discussion revolves around the implications of the algorithm SneakyPrompt, which can trick text-to-image generative AIs into producing NSFW images. Some commenters argue that hacking into these systems is pointless and potentially harmful, while others highlight the importance of designing AI systems with stronger safety filters. Some point out that smaller API providers may not enforce proper filtering and that the responsibility lies with the creators. The conversation also touches on the ethical concerns surrounding AI-generated content, the limitations of current AI models, and the need for proper enforcement of regulations against NSFW content. Additionally, there is a debate about the societal impact of AI technologies and the need for stricter regulations and security measures.

### Aerial refuelling without human intervention

#### [Submission URL](https://www.airbus.com/en/newsroom/stories/2023-11-aerial-refuelling-without-human-intervention) | 69 points | by [geox](https://news.ycombinator.com/user?id=geox) | [77 comments](https://news.ycombinator.com/item?id=38417415)

Airbus has successfully completed a second flight test of its Auto'Mate technology, which aims to automate the aerial refuelling process. The test involved an A310 MRTT tanker aircraft controlling five unmanned drones, simulating a refuelling operation using advanced AI-based navigation and control technologies. By automating in-flight refuelling, Airbus hopes to enhance safety, reliability, and efficiency, while also reducing training costs for flight crews. The technology could also enable the refuelling of non-piloted combat air vehicles, such as drones, and potentially lead to autonomous tankers and aerial assets operating without a crew on board in the future.

The discussion on this submission covers a range of topics related to the technology and potential implications of autonomous aerial refueling. Some users discuss the technical aspects of the technology, such as the digital video window and its potential benefits. Others raise concerns about the reliability and safety of the system, citing previous incidents and accidents involving tankers. There is also discussion about the economics of aerial refueling and the potential cost savings that can be achieved. Some users highlight the importance of military testing and development in advancing technology, while others speculate on the potential risks and consequences of regularly flying unmanned refueling planes. Overall, the discussion reflects a mix of technical analysis and considerations for safety and practicality.

### A chatbot that can't say anything controversial isn't worth much

#### [Submission URL](https://www.theatlantic.com/ideas/archive/2023/11/ai-safety-regulations-uncensored-models/676076/) | 120 points | by [fortran77](https://news.ycombinator.com/user?id=fortran77) | [66 comments](https://news.ycombinator.com/item?id=38416997)

In the world of AI chatbots, OpenAI's release of ChatGPT last year sparked a craze. However, as concerns over AI safety and bias have grown, companies have increasingly focused on adding safety features, limiting the capabilities of these models. But now, a counternarrative is emerging, suggesting that these restrictions are going too far. A group of independent programmers, the so-called AI underground, is building "uncensored" AI models that aim to free up the creative possibilities of AI. These models are trained to avoid deflection and dismiss questions as inappropriate, fostering more open and engaging conversations. While the concept of uncensored AI is controversial, it challenges the idea that access to AI should be limited to a few select companies. Uncensored AI models are democratizing the technology and opening up new creative opportunities. However, there are trade-offs involved in aligning AI with safety principles, such as reducing the cognitive ability of the model. The push and pull between safety and creativity in AI development will continue to be a topic of debate.

The discussion on this submission covers a range of topics related to AI chatbots and their limitations. Here are some key points:
- One commenter suggests that companies are distancing themselves from the output of language models (LLMs) and that the content generated by these models raises controversial issues. They argue that companies should take more responsibility for the content generated by their models.
- Another commenter discusses the use of descriptive characters and traits in AI-generated content, highlighting concerns about the portrayal of murder and violence in the descriptions.
- There is a discussion about the potential dangers of unsensored AI models and whether liability should be assigned to companies or individuals using them.
- The topic of AI chatbots generating controversial content is raised, with one commenter suggesting that if people applied AI chatbots to platforms like Facebook, it could promote hatred and conflict.
- A debate emerges about the ethical considerations of controlling and censoring AI models, with one commenter arguing that censorship can lead to misinformation and restrict the machine's ability to provide accurate answers.
- Some commenters mention the creation of LLaMA2-derived models, discussing their potential benefits and the need for cross-checking information generated by these models.
- Religion and spirituality are also discussed, with varying opinions on whether they are essential aspects of life and whether AI should engage in conversations about them.
- The limitations of chatbots and the necessity of considering their responses are highlighted, with one commenter pointing out that AI should not be designed to avoid uncomfortable questions.
Overall, the discussion touches on the ethical responsibilities of companies, concerns about censoring AI models, and the need for accountability and critical thinking when using and interacting with AI chatbots.

### There are no strings on me

#### [Submission URL](https://www.scattered-thoughts.net/writing/there-are-no-strings-on-me/) | 85 points | by [luu](https://news.ycombinator.com/user?id=luu) | [36 comments](https://news.ycombinator.com/item?id=38410987)

In a recent post on Hacker News, the author reflects on the allure of "software that feels alive" versus the practicality of building more static systems. They discuss their experiences with systems like Emacs and Lisp machines, which possess a certain magic but can also be frustratingly difficult to debug. The author argues that dead systems, which are easier to understand and reason about, have their advantages. For example, in dead systems, code can be found as a single artifact, and the behavior of the system can be predicted by reading the current version of the code. They also explore the challenges of live coding and upgrading long-running background tasks. The post poses interesting questions about the trade-offs between system interactivity and ease of understanding and debugging.

The top stories on Hacker News discussed the allure of "software that feels alive" versus the practicality of building more static systems. The discussion touched on various topics including the benefits and drawbacks of different programming languages, the challenges of debugging and maintaining long-running background tasks, and the trade-offs between system interactivity and ease of understanding and debugging. Some users emphasized the advantages of dead systems, which are easier to understand and reason about, while others expressed interest in more interactive and dynamic systems. The discussion also delved into topics such as relational programming languages, Lisp programming, and the complexities of capturing objects and closures in JavaScript. Additionally, there were mentions of the Mote in God's Eye book and the use of REPLs for prototyping and experimenting in programming. Overall, the discussion highlighted the diverse opinions and perspectives on the topic of software system design and interactivity.

