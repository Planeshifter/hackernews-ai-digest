## AI Submissions for Thu Jul 18 2024 {{ 'date': '2024-07-18T17:13:22.769Z' }}

### Devzat – Chat over SSH, with some nice quality-of-life features

#### [Submission URL](https://github.com/quackduck/devzat) | 380 points | by [humanperhaps](https://news.ycombinator.com/user?id=humanperhaps) | [90 comments](https://news.ycombinator.com/item?id=40998158)

The top story on Hacker News today is about a project called Devzat, a custom SSH server that functions as a chat platform instead of a traditional shell prompt. Devzat allows users to connect via SSH using any device, including phones, making it accessible to everyone. The project provides clear instructions on how to use it, such as logging in with a nickname or joining on a specific port if behind a firewall. It also offers features like rooms, Markdown support, code syntax highlighting, direct messages, timezone support, games like Tic Tac Toe and Hangman, and emoji replacements. Devzat even has integrations with Slack, Discord, and Twitter. The project's creator, quackduck, has made it open source on GitHub with detailed documentation for self-hosting instances and a plugin API for further customization. The project has garnered significant attention with 2.5k stars and 93 forks on GitHub, receiving positive feedback from notable individuals in the tech community like Zach Latta and Ant Wilson. So if you're looking for a unique way to chat over SSH, Devzat might be worth checking out!

- hiAndrewQuinn: Described setting up a Raspberry Pi read-only server for friends to chat like IRC using Termux and SSH, leading to a discussion on installing Finger protocol, the complexity of the project, and understanding the approach.
- rwmj: Wondered about similar SSH chat projects like Talk software and discussed potential issues with binary connections, running an IRC client, and tackling security concerns.
- qdt: Shared an implementation of a similar IRC chat app on SSH, delving into the security concerns and potential vulnerabilities.
- lngghckr: Raised security concerns about attackers crafting messages to execute commands on the client terminal, leading to a discussion on securing SSH and the risks associated with SSH daemon passing terminal commands.
- Tepix: Highlighted the privacy aspect of using public SSH keys for a chat server and suggested a workaround to address this concern.
- phyd: Expressed interest in setting up passwordless SSH accounts for public service and discussed the safety implications involved.
- cdtrttr: Introduced the ssh-chat project written in Go, sparking a conversation on its introduction and the motivation behind it, ultimately leading to a divergence in opinions on its creation.
- sdsd: Mentioned Urbit as a peer-to-peer chat option and shared a widget designed for message management on MacOS.
- yu3zhou4: Recommended Devzat as a beginner-friendly way to start machine hacking on HackTheBox, encouraging users to use a regular instance of Devzat for this purpose.
- cdzd: Experimented with writing a shell replacement program and shared a GitHub project for a shell replacement written in Go, leading to a discussion on program runtime and scalability.
- jsnjyr: Advised caution when using the SSH-agent forwarding default connection to prevent exposing hosts to potential security risks.
- 1vuio0pswjnm7: Shared links related to the OpenBSD Packet Filter (PF) authentication mechanism.
- ingen0s: Commented on the inclusion of their recent work in the list.
- tcsnp: Praised the power of the server setup and expressed a preference for assisting others with their instances.
- xyst: Explored connectivity issues with an ed25519 key configured in the SSH configuration and speculated about a potential Denial of Service (DoS) attack on the server.
- ndpt: Appreciated the project but sought clarification on its unique aspects compared to others.

### Transcribro: On-device Accurate Speech-to-text

#### [Submission URL](https://github.com/soupslurpr/Transcribro) | 134 points | by [thebiblelover7](https://news.ycombinator.com/user?id=thebiblelover7) | [50 comments](https://news.ycombinator.com/item?id=40997850)

Today's top story on Hacker News is about the open-source project "Transcribro," a private and on-device speech recognition keyboard and service for Android. This innovative project uses whisper.cpp to run the OpenAI Whisper family of models and Silero VAD for voice activity detection. Transcribro features a voice input keyboard that allows users to type with speech, making it a convenient tool for Android users. The project is available on the Accrescent app store and GitHub releases, with Accrescent being the recommended platform due to its enhanced security measures. Users are encouraged to verify the authenticity of the app when downloading it. Additionally, there are opportunities for community engagement through the Matrix space provided for discussions and contributions. If you find Transcribro useful, you also have the option to support the lead developer, soupslurpr, through donations.

The discussion on Hacker News about the open-source project "Transcribro" covered various aspects and opinions. Some users highlighted similarities with other input keyboards, mentioned the availability of Transcribro on iOS, and pointed out the importance of accurate voice transcription. There were discussions about the lack of documentation, the possibility of streaming capabilities, and the challenges in integrating with different Android apps. Users also delved into technical details such as the use of models for speech recognition, the effects of streaming on latency, and comparisons with existing solutions. The debate touched upon the complexities of voice recognition technology, including aspects like partial results during speech, various levels of processing, and the impact of different architectures on performance. Additionally, there were mentions of practical considerations like model sizes, latency, and user experience in speech-to-text applications.

### Show HN: How we leapfrogged traditional vector based RAG with a 'language map'

#### [Submission URL](https://twitter.com/mutableai/status/1813815706783490055) | 111 points | by [oshams](https://news.ycombinator.com/user?id=oshams) | [34 comments](https://news.ycombinator.com/item?id=40998497)

Today's top story on Hacker News is about the latest advancements in self-driving car technology. The submission discusses how major companies like Tesla and Google are pushing the boundaries of autonomous driving systems, potentially paving the way for a future where cars can navigate roads safely without human intervention. The article highlights the challenges and benefits of self-driving cars, sparking a lively debate among the tech community. Stay tuned for more updates on this exciting development in the world of transportation.

1. **krdlssgn**: Shared insights on building a terminal solution called "Webwright," utilizing Python for scanning projects and managing functions, imports, decorators, etc., with AI interaction capabilities for application handling.

2. **nthrplg**: Discussed AI coding tools aiming to enhance RAG code development by providing efficient context extraction, aiding in understanding large codebases.

3. **mnshshrn**: Explored the experience of using Aider for databases in Clojure, Clojurescript, and Java, highlighting the project similarities to Plandex1.

4. **lmyrv**: Described a graph-based approach for RAG systems to improve document summarization, content retrieval, and link analysis for efficient information retrieval and knowledge graph building.

5. **tlrkwrthy**: Explored programming databases representing diversified diagrams and links at https://bsrvblhq.com for improved understanding.

6. **mxsg**: Introduced Mutableai's projects and tools, including Deepfakes Faceswap, VLLM Project, and others, for diverse AI applications.

7. **snk**: Linked Pythagora CodeMonkey's AI projects to the discussion on RAG code development and expressed interest in tackling the RAG problem with a vector-based approach.

8. **kleneway1**: Shared experiences in developing pre-trained models using RAG, highlighting the challenges faced and tools utilized for researching and understanding codebases.

9. **lngcss**: Explored the use of RAG and LLM for decision-making and quality assessment in source code analysis, emphasizing the importance of vector databases for efficient prompting.

10. **PhilippGille**: Corrected misconceptions around RAG's structure and the process of indexing contents and generating backtracked results.

11. **J_Shelby_J**: Appreciated the detailed information shared in the conversation.

12. **spirobelv2**: Raised questions about sponsoring public searches for source code repositories and the challenges faced during specific repository searches.

13. **mgclhpp**: Shared considerations for launching a detailed work code for developer years or generated work for developers, in response to the conversation about efficient source code analysis and understanding.

### USPS shared customer postal addresses with Meta, LinkedIn and Snap

#### [Submission URL](https://techcrunch.com/2024/07/18/usps-shared-customer-postal-addresses-with-meta-linkedin-and-snap/) | 368 points | by [leotravis10](https://news.ycombinator.com/user?id=leotravis10) | [186 comments](https://news.ycombinator.com/item?id=40998647)

The U.S. Postal Service has been caught up in a controversy after it was discovered that it was sharing customer postal addresses with tech giants Meta, LinkedIn, and Snap. A TechCrunch investigation revealed that USPS was unknowingly passing on this information through tracking pixels on its website. This data sharing included the postal addresses of users of the Informed Delivery service, which allows customers to preview their incoming mail. USPS claims to have addressed the issue and stopped the practice, stating that they were unaware of it initially. The investigation found that not just addresses, but also tracking numbers and in-transit tracking data were being shared with various companies. While USPS has taken steps to rectify the situation, questions remain about the extent of the data collection and any potential consequences for the affected customers. This incident adds USPS to the list of organizations facing scrutiny over data privacy concerns, with a growing trend of companies being more cautious about their use of web tracking technologies.

The discussion on Hacker News regarding the incident where the U.S. Postal Service was found sharing customer postal addresses with tech giants like Meta, LinkedIn, and Snap through tracking pixels on its website covers various perspectives. One user criticized the USPS for unintentionally allowing tracking pixels on its Informed Delivery page, highlighting concerns about privacy violations. Another user shared their experience of implementing arbitrary pixels on client websites and raised technical concerns about the use of temporary forms in code for tracking purposes. There was a debate about the extent of data sharing and the implications for customers, with some expressing skepticism about the USPS's claims of ignorance.

Additionally, there were discussions about the practices of tech companies in leveraging user data for business purposes, balancing between marketing needs and data privacy concerns. Some users pointed out the complexities of tracking technologies and the challenges faced by marketers in understanding the implications of data sharing. The debate also touched upon the role of government agencies like the USPS in utilizing analytics tools and data collection methods, with concerns raised about potential surveillance activities and privacy risks associated with such practices. Some users argued for stricter regulations and transparency in data handling by both government and private entities to protect user privacy.

Furthermore, there were discussions about the usage of analytics tools like Matomo by government websites and the implications of relying on third-party analytics services for data analysis. The conversation also delved into the legal and ethical considerations surrounding data privacy regulations like GDPR and the accountability of government agencies in safeguarding user data. Overall, the discussion highlighted the importance of data privacy, government transparency, and ethical practices in data collection and analysis in the digital age.

### A type system for RCL, part 2: The type system

#### [Submission URL](https://ruudvanasseldonk.com/2024/a-type-system-for-rcl-part-2-the-type-system) | 28 points | by [todsacerdoti](https://news.ycombinator.com/user?id=todsacerdoti) | [5 comments](https://news.ycombinator.com/item?id=40998931)

The developer behind RCL, a new configuration language extending JSON, is seeking validation and feedback from the programming language community on its type system. Part II of the series delves into the type system, highlighting its strong typing and foundation principles. RCL enforces type constraints to prevent implicit type conversions, ensuring type errors are caught early. The type system allows for values to fit multiple types, facilitating flexibility and abstraction. RCL’s type lattice organizes types into a partially ordered set, enabling subtype relationships and type inference. The language is statically typed, capable of detecting type errors at compile time, but also supports gradual typing, deferring certain checks to runtime. The subtype check plays a crucial role in ensuring expressions align with expected types. This insightful exploration sheds light on the intricacies of RCL’s evolving type system.

In the discussion about the RCL configuration language and its type system, one user found the generalized subtype check interesting, noting that traditional statically typed languages usually rely on type checks at runtime. They mentioned Haskell's approach and other languages like Data.Coerce and Maybe that handle type variables differently. Another user appreciated the thoughtful article and discussed the importance of runtime type checking, mentioning the need for explicit type assertions in some languages. This user also highlighted the potential benefits of configuration languages evolving into full-blown programming languages. 

A different user argued that relying solely on runtime type checks in a language makes it harder to verify problems, as demonstrated by an example involving type assignments and dependencies. They emphasized the advantages of static typing in catching bugs early. Lastly, a user commented on the plethora of type systems across different programming languages and the need to choose the right one with the potential to create typing issues.

Additionally, a side conversation emerged with a user expressing frustration with type systems in general and advocating for more flexible systems like in interpreted languages. Another user disagreed, pointing out the issues with flexibility and the importance of avoiding reflection and casting.

### Overcoming the limits of current LLMs

#### [Submission URL](https://seanpedersen.github.io/posts/overcoming-llm-limits) | 112 points | by [sean_pedersen](https://news.ycombinator.com/user?id=sean_pedersen) | [105 comments](https://news.ycombinator.com/item?id=40991549)

Today on Hacker News, a post delves into the limitations of large language models (LLMs) that have been dominating the field. These models, while impressive, face issues like hallucinations, lack of confidence estimates, and citations. Hallucinations occur when the content generated by LLMs sounds convincing but is actually inaccurate—something we definitely want to avoid. Lack of confidence estimates can make it hard to determine the reliability of predictions, while citations are crucial for verifying information sources.

The post highlights a recent release by OpenAI that focuses on teaching models to express uncertainty in words, offering a possible solution to the confidence estimate problem. Additionally, techniques such as RAG (retrieval-augmented generation) can be used to incorporate citations into LLM outputs, creating more reliable content. Several resources like perplexity.ai and wikichat.genie.stanford.edu are mentioned as good examples in this regard.

One interesting approach suggested in the post is the idea of "consistency bootstrapping" for LLMs. By excluding contradictory training data and training the model to identify logical inconsistencies within the context provided, researchers hope to create more reliable and accurate models. MIT researchers have already made strides in this area, as outlined in a paper referenced in the post.

By curating high-quality training data and building models based on consistent worldviews, it may be possible to mitigate the limitations currently faced by LLMs. The proposed approach of gradually expanding training data with consistent text documents offers a promising pathway for improving these powerful language models.

The post provides a wealth of references and resources for those interested in exploring these topics further. It's exciting to see the ongoing efforts to enhance LLM performance and accuracy in text generation tasks.

The discussion about the limitations of large language models (LLMs) on Hacker News revolves around various aspects such as hallucinations, confidence estimates, training data quality, and tackling logical inconsistencies within LLMs.

- Users like "mitthrowaway2" and "dwns" emphasize the fundamental design flaw of LLMs in dealing with hallucinations due to the distribution of training data.
- "sean_pedersen" points out the importance of quality over quantity in training data, suggesting that focusing on quality data is crucial for improving LLMs.
- Discussions on confidence scores, training data sources, and the integration of semantic search contexts like in RAG (retrieval-augmented generation) models are highlighted by users such as "nthpcrt" and "bbr."
- The necessity of training high-quality and consistent datasets to address hallucinations is emphasized by "_venkatasg" and "bosch_mind."
- "RodgerTheGreat" discusses the challenges in manually creating properly licensed and verified datasets, while users like "thrsd" and "nyrkk" delve into the ethical considerations and difficulties in developing universally coherent data for LLMs.
- Users like "js8" and "darby_nine" explore the concept of uncertainty in logic and the difficulties in handling logical contradictions and uncertainties within LLMs.

Overall, the discussion delves into the complexities and challenges associated with improving the reliability and accuracy of LLMs by addressing issues such as hallucinations, confidence estimates, data quality, and logical inconsistencies.

### Mistral NeMo

#### [Submission URL](https://mistral.ai/news/mistral-nemo/) | 401 points | by [bcatanzaro](https://news.ycombinator.com/user?id=bcatanzaro) | [158 comments](https://news.ycombinator.com/item?id=40996058)

Today, the Mistral AI team announced the release of Mistral NeMo, a cutting-edge 12B model developed in collaboration with NVIDIA. This new small model boasts a significant 128k context length and promises top-tier performance in reasoning, world knowledge, and coding accuracy within its size class. The model, released under the Apache 2.0 license, includes pre-trained base and instruction-tuned checkpoints to facilitate adoption by both researchers and enterprises.

Mistral NeMo is tailored for global, multilingual applications, excelling in languages such as English, French, German, Spanish, Italian, Portuguese, Chinese, Japanese, Korean, Arabic, and Hindi. The model introduces Tekken, a new tokenizer, that demonstrates superior compression capabilities in various languages compared to previous models, making it a more efficient choice for processing natural language text and source code.

Furthermore, the Mistral NeMo model underwent extensive fine-tuning to enhance its ability to follow precise instructions, excel in reasoning, handle multi-turn conversations, and generate code effectively. The model's performance has been benchmarked against other recent open-source models like Gemma 2 9B and Llama 3 8B, showcasing its competitive edge.

For those interested in exploring Mistral NeMo, the model's weights are hosted on HuggingFace, and tools like mistral-inference and mistral-finetune are available for experimentation. Additionally, NVIDIA has packaged Mistral NeMo as an inference microservice on its platform, further expanding accessibility to this advanced AI technology.

The submission on Hacker News discusses the release of Mistral NeMo, a 12B model created in collaboration with NVIDIA. The model offers a large context window of 128k tokens and excels in reasoning, world knowledge, and coding accuracy. It includes pre-trained base and instruction-tuned checkpoints and is tailored for multilingual applications. The model introduces Tekken, a new tokenizer with superior compression capabilities. The discussion on Hacker News dives into topics such as model performance, benefits of small models, challenges related to high memory requirements, comparisons with other models like Gemma 2 9B and Llama 3 8B, and the implications of different quantization levels on model quality and memory usage. There are also mentions of Mistral NeMo being hosted on Hugging Face, its accessibility through NVIDIA's platform, and insights into the tokenization process using Tekken. Additionally, there are comments on the trend of increasing model sizes, the trade-offs of model training and inference on various hardware, and the impact of large models on the tech industry.

### Show HN: Llm2sh – Translate plain-language requests into shell commands

#### [Submission URL](https://github.com/randombk/llm2sh) | 59 points | by [RandomBK](https://news.ycombinator.com/user?id=RandomBK) | [22 comments](https://news.ycombinator.com/item?id=40991661)

Today's top story on Hacker News is about a fascinating project called "llm2sh," a command-line utility that utilizes Large Language Models (LLMs) to translate natural language requests into shell commands. This tool allows users to interact with their systems using plain language, making it easier to execute commands. "llm2sh" supports multiple LLMs for command generation, has a customizable configuration file, and even a "YOLO mode" for running commands without confirmation. The project is open-source and aims to be easily extensible with new LLMs and system prompts.

Users can install "llm2sh" using pip and use it by providing their requests as input. The tool supports various LLMs such as OpenAI, Claude, and Groq, necessitating API keys for some services. It also provides options like specifying a particular model for command generation, running multiple commands in sequence, and even running commands without confirmation. The project is actively developed and welcomes contributions from the community.

"llm2sh" emphasizes privacy by not storing user data or command history, although the LLM APIs may collect information for their own purposes. The tool may send some system information to LLMs to help generate better commands. Overall, "llm2sh" is an experimental yet promising tool for streamlining command-line interactions using the power of language models.

(Source: GitHub - randombk/llm2sh)

- Users expressed their experiences with using "llm2sh," with some finding themselves Googling shell commands, highlighting its multiple LLM support, YOLO mode, and the mix of excitement and caution while using it in workflows.
- A user shared their experimentation with Docker containers for sandboxing critical operations, acknowledging the risks involved in networking resources and worker nodes.
- There was a discussion about experimentation, confidence in running certain operations, and the desire to run containerless Docker in a sandboxed box for fun and experimentation, potentially leading to building AI platforms for deterministic tasks.
- Comments discussed the GPLv3 license, the simplicity of the CLI experience, the ability to set local URI settings in the configuration, and the positive feedback received for the clean CLI experience.
- Users mentioned creating similar projects, such as a builder for natural language to command translation and a dispatcher for OpenAI-compatible APIs, with the intention of submitting pull requests to improve the projects.
- Various users appreciated the tool for its purpose, acknowledged the existence of different versions for comparison, and highlighted the importance of understanding commands to make Nvidia drivers work.
- Some users expressed curiosity about compressing a Python interpreter and revisiting language rewriting for portability, discussing potential approaches for a single binary excluding models, plans for an interesting project in the hack control logic space, and the experience of learning from mistakes and perspectives in development.

### He created Oculus headsets as a teenager, now he makes AI weapons for Ukraine

#### [Submission URL](https://www.npr.org/2024/07/09/nx-s1-4985981/oculus-ai-weapons-ukraine-palmer-luckey) | 80 points | by [tosh](https://news.ycombinator.com/user?id=tosh) | [94 comments](https://news.ycombinator.com/item?id=40995531)

Palmer Luckey, known for creating Oculus headsets as a teenager and selling his company to Facebook for $2 billion, is now making AI weapons for Ukraine through his company, Anduril. Luckey's unconventional style, complete with a mullet and Hawaiian shirts, is reflected in his innovative approach to developing autonomous weapons like drones and submarines for the Pentagon and other countries. Anduril's goal is to revolutionize the defense industry by producing AI weapons faster and cheaper than traditional military contractors. While these technologies have the potential to change warfare, they are still facing challenges and critics. Despite the hurdles, Anduril is actively involved in arming Ukraine in its conflict with Russia, providing high-tech solutions in a rapidly evolving battleground.

The discussion on Hacker News regarding the submission about Palmer Luckey and Anduril making AI weapons for Ukraine covers various aspects. Some users express skepticism about the effectiveness of the drones being provided to Ukraine by Anduril, hinting that they may not be the game-changer they are made out to be. Others discuss the ethical implications of tech companies like Google, Facebook, and Apple refusing to work on national security projects, contrasting this with the involvement of companies like Anduril in the defense industry. The conversation also touches on the debate surrounding national security, individual freedoms, and corporate responsibility. Additionally, there are references to historical analogies like the roles of civilizations in world peace and conflict. The discussion also delves into the concept of mandatory service and the deployment of military resources. Ultimately, the dialogue reflects a range of opinions on the intersection of technology, national defense, corporate ethics, and international relations.

### Everyone Is Judging AI by These Tests. Experts Say They're Close to Meaningless

#### [Submission URL](https://themarkup.org/artificial-intelligence/2024/07/17/everyone-is-judging-ai-by-these-tests-but-experts-say-theyre-close-to-meaningless) | 28 points | by [billybuckwheat](https://news.ycombinator.com/user?id=billybuckwheat) | [17 comments](https://news.ycombinator.com/item?id=40992762)

In the tech world's race for AI supremacy, companies like Google and Meta showcase their AI models through tests known as benchmarks. However, experts caution that these tests may not provide a meaningful understanding of AI capabilities. The benchmarks, often outdated and based on amateur content, fail to evaluate crucial aspects like the ability to make informed decisions in high-stakes fields like healthcare or law.

Moreover, the AI industry heavily relies on these benchmarks to compare models and demonstrate progress, despite concerns raised by researchers about their validity. As the debate on AI's impact intensifies, policymakers are considering new regulations in states like California and Colorado to govern the AI landscape.

Ultimately, the quest for AI excellence through benchmarks may not be as telling as it seems, underscoring the need for a more comprehensive evaluation of AI systems beyond standardized tests.

The discussion on the Hacker News thread revolves around the limitations and shortcomings of using benchmarks in evaluating AI models. Some users point out that benchmarks like those used by companies such as Google and Meta may not accurately depict the true capabilities of AI systems, especially in complex fields like healthcare and law. There is skepticism regarding the effectiveness of benchmarks in measuring crucial aspects of AI's decision-making abilities.

Additionally, there is a debate on the role of benchmarks in the AI industry, with concerns raised by researchers about their validity and the industry's heavy reliance on them for model comparison and progress demonstration. Some users emphasize the need for a more comprehensive evaluation of AI systems beyond standardized tests.

On a related note, there is a discussion about AI models like LLMs (Large Language Models) and their sudden appearance, with users expressing varying opinions on their benefits, internal workings, and applications in text prediction and other tasks.

Furthermore, users discuss the progress of AI in recent years and how it has led to advancements in capabilities that were previously intangible to people. The conversation also touches on AI's impact on job interviews, testing environments, and the need for more nuanced evaluations in the field.

Overall, the dialogue highlights the complexities of assessing AI systems through benchmarks and the evolving landscape of AI evaluation methods.

### Proton Mail Adds an Open-Source AI Writing Assistant to Take on Gmail

#### [Submission URL](https://news.itsfoss.com/proton-mail-ai-assistant/) | 60 points | by [elashri](https://news.ycombinator.com/user?id=elashri) | [36 comments](https://news.ycombinator.com/item?id=40995817)

Proton Mail, known for its privacy-centric approach, has upped its game by introducing an open-source AI writing assistant called "Proton Scribe". This AI tool is designed to help users compose, proofread, and even adjust the tone of their emails within the Proton Mail platform. The best part? All processing happens locally on the user's device, ensuring privacy with zero access to sensitive information.

Proton Scribe is available for Proton Mail business plans at a cost of $2.99 per user per month, with a 14-day free trial option. Users of Proton Visionary and Lifetime plans get access to it for free. The tool uses open-source models and aims to provide a privacy-first AI experience directly within Proton Mail, eliminating the need to rely on third-party services with questionable privacy practices.

For those interested, the source code of Proton Scribe is available on Proton Mail's GitHub page. As the tool continues to roll out for web and desktop clients, non-business plan users may have to wait for access, possibly as part of an existing plan in the future. Proton Mail is setting the bar high in the email service arena with its commitment to privacy and innovative features like Proton Scribe.

The discussion on Hacker News regarding the introduction of Proton Scribe by Proton Mail covers a variety of topics. 

1. Some users express skepticism about the need for the AI assistant and question whether it is necessary for enhancing email composition within Proton Mail in order to compete with Gmail.
2. WithinReason engages in a detailed conversation about privacy concerns, discussing the limits of control over shared information and privacy implications when using email service providers.
3. There is a brief exchange about the functionality of generative language models and the handling of confidential information.
4. Users mention switching email providers, with suggestions for Fastmail as an alternative to Proton Mail.
5. A debate arises about the reconciliation of privacy concerns with the use of AI systems like Proton Scribe, taking into account machine learning processes and data handling.
6. The conversation extends to the technical aspects of Proton Scribe, including the local processing of data and potential security measures in place.
7. Concerns about the source code, availability, and duplicity of the content lead to discussions about privacy, AI-generated emails, and the convenience they offer.

Overall, the community is engaged in a thoughtful dialogue about privacy, AI technology, and the implications of using such tools within the context of email services.

### Forewarn gives realtors access to buyers' histories with a phone number

#### [Submission URL](https://therecord.media/forewarn-app-real-estate-homebuyer-data) | 75 points | by [compootr](https://news.ycombinator.com/user?id=compootr) | [62 comments](https://news.ycombinator.com/item?id=40998481)

A new tool is causing a buzz in the real estate industry by providing instant access to extensive homebuyer data. Forewarn, an app that costs about $20 a month, allows real estate professionals like Susan Hicks to retrieve detailed information on prospective clients with just their phone number. While some find the amount of data accessible through the app mind-blowing, others are wary of its potential privacy implications.

Forewarn, owned by data broker red violet, offers a range of information from criminal records to financial indicators, all at the touch of a button. This has led to rapid adoption within the real estate industry, with associations and MLSs in various states signing on to use the service. Despite its popularity among real estate agents, Forewarn remains relatively unknown outside the industry, raising concerns among privacy advocates about mass profiling and potential discrimination.

As Forewarn continues to expand, questions loom about its handling of private data and the need for transparency with clients. With a unique offering unmatched by competitors, the app's impact on the real estate market is undeniable. However, the balance between convenience and privacy remains a key issue as Forewarn faces increased scrutiny from regulators and advocates alike.

The discussion on Hacker News around the Forewarn tool in the real estate industry covers various perspectives. One user points out that Forewarn determines who might be eligible for certain jobs, suggesting the potential for discrimination based on financial success. Another user brings up concerns about arbitrary discrimination, mentioning that certain states essentially screen people based on their possessions, such as owning a Ford truck.

There is a debate on the legality and ethics of data brokering, with some users highlighting the need for clearer laws and regulations around personal information sales. Some users raise the issue of public versus private information access, as well as the implications of government records being used by commercial entities. Additionally, there is a discussion on privacy laws and the challenges of ensuring effective privacy protections in different countries.

Furthermore, users share personal experiences and opinions related to real estate transactions, such as the practices of real estate agents, buyers, and sellers. Some users mention their encounters with misleading or questionable tactics in the industry, like misrepresenting property prices or commission rates. Overall, the discussion reflects a mix of viewpoints on privacy, discrimination, data transparency, and industry practices in the context of the Forewarn tool and the real estate market.

### An Algorithm Told Police She Was Safe. Then Her Husband Killed Her

#### [Submission URL](https://www.nytimes.com/interactive/2024/07/18/technology/spain-domestic-violence-viogen-algorithm.html) | 9 points | by [jryb](https://news.ycombinator.com/user?id=jryb) | [15 comments](https://news.ycombinator.com/item?id=40994402)

Today's top story on Hacker News is about Spain's algorithm, VioGén, used to combat gender violence, which has sparked controversy due to its impact on victims' safety. The algorithm, integrated into law enforcement, determines risk levels for victims with the intention of preventing repeat attacks. However, there have been cases where victims deemed at low risk by the algorithm have been harmed again, sometimes with fatal consequences. The reliance on VioGén has raised concerns about victims falling through the cracks and the lack of transparency regarding the algorithm's effectiveness. This issue highlights the broader trend of governments worldwide turning to algorithms for making critical societal decisions, raising questions about accountability and the ethical implications of such systems.

The discussion on the Hacker News submission revolves around the use of Spain's algorithm, VioGén, to combat gender violence and the broader implications of relying on algorithms for critical societal decisions. Here are some key points from the discussion:

- There is a disagreement on the effectiveness and accountability of algorithms compared to human decision-making, with some users arguing that humans cannot handle statistical pattern recognition effectively.
- The discussion delves into the accountability of algorithms and the potential dangers of trusting them too much, highlighting concerns about errors and lack of transparency.
- There is a debate about accountability and the ethical implications of relying on algorithms for making decisions that impact individuals' lives, with some users pointing out the psychological distinctions between algorithmic processes and human judgment.
- Some users argue that individuals should still be held accountable even when decisions are made based on algorithms, while others express concerns about the lack of clear responsibility when errors occur.
- The debate also touches on the need for proper investigation and accountability when errors occur, whether they are the result of human negligence or algorithmic flaws.

Overall, the discussion reflects a concern about the increasing reliance on algorithms in crucial decision-making processes and the importance of ensuring accountability and transparency in such systems.

