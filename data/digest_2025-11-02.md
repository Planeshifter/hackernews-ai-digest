## AI Submissions for Sun Nov 02 2025 {{ 'date': '2025-11-02T17:15:06.503Z' }}

### New prompt injection papers: Agents rule of two and the attacker moves second

#### [Submission URL](https://simonwillison.net/2025/Nov/2/new-prompt-injection-papers/) | 90 points | by [simonw](https://news.ycombinator.com/user?id=simonw) | [34 comments](https://news.ycombinator.com/item?id=45794245)

Two new papers land on prompt-injection/agent safety—and they pull in opposite but compatible directions: tighten design assumptions, and raise the bar for evaluations.

What Meta proposes (Agents Rule of Two)
- Heuristic for agent design: in any one session, allow at most two of:
  A) process untrusted inputs
  B) access sensitive systems or private data
  C) change state or communicate externally
- If a task needs all three, don’t run it autonomously—use a fresh session and/or human-in-the-loop approval.
- Why it matters: it generalizes Simon Willison’s “lethal trifecta” beyond data exfiltration by explicitly flagging state-changing tools as risky when combined with untrusted inputs.
- Willison’s critique: marking “untrusted inputs + change state” as safe is wrong—harm can still occur without any sensitive data involved. So treat the Rule of Two as a useful mental model, not a guarantee.
- Meta’s headline takeaway: prompt injection isn’t solved; rely on system design, not filters.

What the cross-org team shows (The Attacker Moves Second)
- 14 authors from OpenAI, Anthropic, and Google DeepMind evaluate 12 recent defenses under adaptive attacks (not one-shot jailbreak strings).
- Results: most defenses were bypassed with >90% attack success; a 500-person human red-team hit 100%. Many of these defenses had previously reported near-zero attack success.
- Methods: gradient-based attacks (least effective), reinforcement learning against black-box systems, and search-based LLM loops with LLM-as-judge; plus a prize-backed human competition.
- Message: static jailbreak examples are weak tests; defenses must be evaluated against adaptive, iterative attackers. Authors urge simpler, analyzable defenses and higher evaluation standards.

Practical takeaways for builders
- Assume prompt injection will land; reduce blast radius.
- Enforce least privilege per session: pick only two of untrusted inputs, sensitive access, and state-changing/external actions; require approvals or new sessions when all three are needed.
- Gate tools and state changes with explicit policies, allowlists, rate limits, and human review for high-impact actions.
- Isolate contexts, renew sessions frequently, and avoid carrying tainted prompts across tool-invoking steps.
- Test like an attacker: run adaptive, multi-round red teaming (human and automated), not just canned jailbreak strings; measure attack success rates realistically.

Net: Design conservatively (Meta’s heuristic) and evaluate adversarially (adaptive attacks). Filters alone won’t save you; architecture and rigorous testing might.

**Summary of Hacker News Discussion:**

The discussion revolves around the proposed frameworks for addressing prompt injection and agent safety, with mixed reactions and deeper explorations of challenges:

1. **Skepticism Toward Meta’s "Rule of Two":**  
   - Critics argue the heuristic oversimplifies risks. For example, combining "untrusted inputs + state changes" (even without sensitive data) can still enable harm (e.g., triggering destructive external actions).  
   - Parallels drawn to traditional security concepts like the CIA triad (confidentiality, integrity, availability) and CAP theorem, emphasizing trade-offs in distributed systems.  

2. **Intent Tracking and Human Oversight:**  
   - Debates about practical implementation of "intent tracking" (e.g., DeepMind’s CaMeL paper) to isolate untrusted inputs. Concerns include reliability and the need for human review to validate high-risk actions.  
   - Proposals for minimal context transfer, session isolation, and UI-driven verification to reduce tainted prompt propagation.  

3. **Real-World Vulnerabilities:**  
   - Examples of prompt injection in academic settings, where hidden prompts in papers attempted to manipulate LLM-driven peer reviews. Highlighted ICML 2025’s ban on LLM-generated reviews to counter this.  
   - Consumer LLM tools (e.g., code assistants) criticized for enabling indirect prompt injection via document ingestion or external tool invocations.  

4. **Criticism of LLM Security Posture:**  
   - Comparisons to traditional input sanitization (e.g., XSS, SQL injection defenses) suggest LLM architectures need similar mechanical safeguards, not just heuristic rules.  
   - Concerns that current LLM workflows inherently violate the "Rule of Two" by blending untrusted inputs, state changes, and data access (e.g., auto-updating prompts based on external content).  

5. **Human-in-the-Loop (HITL) Challenges:**  
   - Skepticism about HITL as a scalable solution due to human error, incentive misalignment (e.g., rubber-stamping approvals), and productivity trade-offs.  
   - Anecdotes highlight LLMs’ role in accelerating workflows but note verification remains a bottleneck.  

6. **Broader Philosophical Debates:**  
   - Calls to avoid reinventing security principles (e.g., Chesterton’s Fence analogy) and instead adapt proven practices (isolation, least privilege) to LLM architectures.  
   - Emphasis on adversarial testing, simpler analyzable defenses, and transparency in evaluations.  

**Key Takeaway:**  
While Meta’s "Rule of Two" and rigorous adversarial testing frameworks are seen as steps forward, the discussion underscores the complexity of securing LLM ecosystems. Practical solutions demand hybrid approaches: architectural safeguards (isolation, intent tracking), human oversight with accountability, and iterative red-teaming—not just theoretical heuristics.

### Lisp: Notes on its Past and Future (1980)

#### [Submission URL](https://www-formal.stanford.edu/jmc/lisp20th/lisp20th.html) | 186 points | by [birdculture](https://news.ycombinator.com/user?id=birdculture) | [99 comments](https://news.ycombinator.com/item?id=45792579)

John McCarthy on Lisp’s past and future (1980, with a 1999 note)
- What it is: A concise reflection by Lisp’s creator arguing why Lisp endured its first 21 years and what needed fixing. McCarthy revisited the piece in 1999, saying it still matched his views.
- Core idea: Lisp survived because it sat at an “approximate local optimum” in language design—good enough across many axes to outlast rivals.
- What he wants scraped off: accumulated “barnacles” in the language and ecosystem. He calls for cooperative maintenance, especially robust, shared program libraries.
- Formal correctness: Computer-checked proofs are already possible for pure Lisp (and some extensions), but realizing Lisp’s mathematical promise requires more theory and some “smoothing” of the language.
- Sections to look for: Survival of Lisp, Improvements, Proving Correctness of Lisp Programs, and Mysteries.
- Why it still lands: 
  - Anticipates modern package management and shared libraries.
  - Frames Lisp’s homoiconicity and simplicity as enduring advantages.
  - Connects language design to formal methods long before mainstream interest.
- Pull quote: Lisp “is an approximate local optimum in the space of programming languages.”

Link: John McCarthy’s Stanford page (1999 note and 1980 paper).

The discussion revolves around comparing Clojure and Rust, with insights into their design philosophies, trade-offs, and practical use cases:

1. **Clojure's Strengths**:
   - Praised for its **REPL-driven development**, enabling interactive coding where developers modify running applications, inspect state, and test snippets in real-time (compared to "brainstorming with a whiteboard").
   - **Immutable data structures** and **Software Transactional Memory (STM)** simplify concurrency by avoiding shared mutable state, reducing race conditions.
   - Emphasizes simplicity (Rich Hickey’s "Simple Made Easy") and functional programming principles.

2. **Rust's Approach**:
   - Focuses on **safety and performance** via its ownership/borrowing system, enabling controlled mutable state without garbage collection.
   - Addresses concurrency by enforcing compile-time checks for shared data, but introduces complexity (steep learning curve).

3. **Trade-offs**:
   - Clojure/Elixir prioritize developer ergonomics and correctness via immutability, accepting runtime performance costs (e.g., copying data).
   - Rust shifts costs to the compiler for zero-cost abstractions, appealing for systems programming but requiring strict adherence to its rules.

4. **Other Languages**:
   - **F#** is suggested as a middle ground—statically typed, immutable-by-default, and commercially viable (.NET integration).
   - **Erlang/Elixir** use actor models (message-passing) for concurrency, avoiding shared state entirely.

5. **Community Sentiment**:
   - Clojure’s REPL and interactive workflow are seen as transformative for productivity.
   - Rust’s strictness is polarizing: powerful for resource-critical tasks but frustrating for those prioritizing rapid iteration.
   - Debate reflects broader tensions: simplicity vs. control, dynamic vs. static typing, and runtime vs. compile-time safety.

**Key Takeaway**: Clojure excels in interactive, high-level applications where correctness and developer experience matter, while Rust targets performance-critical domains requiring fine-grained control, albeit with added complexity.

### Tongyi DeepResearch – open-source 30B MoE Model that rivals OpenAI DeepResearch

#### [Submission URL](https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/) | 347 points | by [meander_water](https://news.ycombinator.com/user?id=meander_water) | [140 comments](https://news.ycombinator.com/item?id=45789602)

Tongyi DeepResearch: open-source “Deep Research” agent claims parity with OpenAI’s

Key points
- What it is: An open-source web agent that tackles complex, multi-step information-seeking and reasoning tasks. Authors claim parity with OpenAI’s DeepResearch on a broad benchmark suite.
- Scores: 32.9 on HLE (Humanity’s Last Exam), 43.4 on BrowseComp, 46.7 on BrowseComp-ZH, and 75 on xbench-DeepSearch, reportedly outperforming other open and proprietary agents.
- What’s new: A fully synthetic, end-to-end training pipeline spanning Agentic Continual Pre-training (CPT), Supervised Fine-Tuning (SFT), and on-policy Agent RL—no human-in-the-loop data. They introduce AgentFounder, a “data flywheel” that turns agent rollouts back into training data.
- How it works:
  - Agentic CPT with entity-anchored knowledge memories and both first-order and higher-order action synthesis to explore the reasoning–action space offline.
  - Post-training data via graph- and table-driven QA synthesis, difficulty control with formal set-theoretic modeling, and an automated engine that iteratively “upgrades” questions to PhD-level complexity.
  - Inference modes: native ReAct (no prompt engineering) to show intrinsic ability, and Heavy Mode (test-time scaling) to push planning depth.
- Infra: Full-stack RL setup with automated data curation and robust rollout infrastructure; emphasis on reproducibility and scalability.
- Availability: Code, models, and demos are listed as available on GitHub, Hugging Face, ModelScope, and a showcase page.

Why it matters
- Strong open baseline for deep research agents, with a repeatable synthetic-data recipe that could lower reliance on expensive human annotations or commercial APIs.

Caveats
- The post flags limitations (details not included here), and real-world generalization beyond curated benchmarks will be important to validate.

**Summary of Discussion:**

1. **General vs. Specialized Models Debate**:  
   - Participants discuss trade-offs between large general-purpose LLMs (e.g., GPT-3.5/4) and smaller, domain-specific models. Smaller models are seen as cost-effective, lower-latency alternatives, especially if frontier model development slows.  
   - **Key points**:  
     - Smaller models can outperform larger ones in niche tasks (e.g., chess) with targeted training.  
     - Infrastructure costs (energy, hardware) favor smaller models for deployment.  

2. **Chess as a Benchmark**:  
   - GPT-3.5’s chess performance is criticized as subpar, attributed to lack of explicit chess training data.  
   - Specialized smaller models (e.g., 4B-8B parameters) are noted to achieve SOTA in chess, with benchmarks like [r/LLMChess](https://www.reddit.com/r/llmchess) cited.  

3. **Mixture of Experts (MoE) Architecture**:  
   - MoE models (e.g., DeepSeek-R1) are highlighted for efficiency, using only 1/18th of total parameters per query.  
   - Debate arises over whether experts are explicitly domain-trained or emerge organically.  

4. **Real-World Performance vs. Hype**:  
   - Skepticism about GPT-5’s claimed improvements, with users reporting underwhelming real-world reliability compared to marketing.  
   - Benchmarking pitfalls noted, as metrics may not reflect practical usability.  

5. **Infrastructure and Cost Analogies**:  
   - Costco’s bulk model vs. boutique wine shops used as metaphors for economies of scale (large models) vs. specialization (small models).  
   - Norway’s alcohol sales model (high taxes, limited selection) illustrates trade-offs in quality and accessibility.  

6. **AI in Gaming**:  
   - Brief mention of Game-TARS, a vision-language model for generalist game-playing, showcasing broader AI applications.  

7. **Legal and Compliance Challenges**:  
   - Discussion on the hidden costs of regulatory compliance (e.g., UK Online Safety Act), emphasizing the need for AI tools to streamline legal research.  

**Key Takeaways**:  
- Specialized models are gaining traction for cost and performance benefits in niche tasks.  
- Skepticism persists about the real-world utility of frontier model advancements.  
- Infrastructure efficiency and benchmarking transparency remain critical concerns.

### Meta readies $25B bond sale as soaring AI costs trigger stock sell-off

#### [Submission URL](https://www.ft.com/content/120d2321-8382-4d74-ab48-f9ecb483c2a9) | 108 points | by [1vuio0pswjnm7](https://news.ycombinator.com/user?id=1vuio0pswjnm7) | [163 comments](https://news.ycombinator.com/item?id=45788042)

Meta is lining up a roughly $25 billion multi-tranche bond sale—one of the year’s biggest—aimed at financing its surging AI infrastructure spend. The move follows a sharp share sell-off after Meta warned that AI outlays would remain elevated for years, reflecting the massive costs of data centers, custom chips, and power-hungry buildouts.

Why it matters
- Big Tech taps debt for AI: Even with hefty cash piles, hyperscalers are leaning on bond markets to fund capex at scale while keeping flexibility for buybacks and M&A.
- Likely strong demand: Meta’s high-grade credit profile and low leverage should draw investors, though pricing versus Treasuries and tenor mix will be closely watched.
- Signals AI’s true cost: The AI race is shifting from model demos to industrial-scale infrastructure, pushing annual capex into the tens of billions.

What to watch
- Final size, maturities, and spread versus peers
- Any explicit earmarks for AI/data centers versus “general corporate purposes”
- Read-through for suppliers (chips, networking, power) and whether Amazon, Microsoft, or Google follow with similarly sized deals

Bottom line: Meta is turning to one of the largest corporate bond offerings this year to bankroll the AI bet that just rattled its stock—proof that the AI arms race now runs through the credit markets.

**Summary of Hacker News Discussion on Meta's $25B AI Bond Sale:**

1. **Financial Strategy & Risk Concerns**  
   - Users debated Meta’s decision to issue bonds despite holding $43B in cash reserves. Some argued it reflects risk aversion (preserving cash for buybacks/M&A) and preferential treatment for bondholders in insolvency scenarios. Others countered that Meta is far from insolvency and that bond issuance shifts risk away from shareholders.  
   - Skepticism arose about whether Meta’s AI investments can justify costs, given its lack of a direct revenue stream like Google/Microsoft’s cloud businesses. Comparisons were drawn to TikTok’s rise, with users noting Meta’s reliance on acquisitions (Instagram, WhatsApp) to retain relevance.

2. **AI Infrastructure Costs & Competition**  
   - Comments highlighted the staggering scale of AI infrastructure spending ($300B+ annually industry-wide), low data-center vacancy rates (~1%), and power/network constraints. Meta’s bet on AI recommendations (e.g., competing with TikTok) was seen as necessary but risky.  
   - Concerns were raised about whether hyperscalers (Meta, Amazon, Google) can sustain this spending long-term, especially if consumer-facing AI products fail to monetize effectively.

3. **Social Responsibility Criticisms**  
   - A vocal segment criticized the prioritization of AI infrastructure over addressing global poverty, citing child hunger in Congo and Meta’s role in resource allocation. This sparked debates about corruption, governance, and foreign intervention, with examples like Afghanistan’s instability and Elon Musk’s ventures.  
   - Some argued that infrastructure investment is necessary for progress, while others viewed it as a misallocation of capital benefiting corporations over societal needs.

4. **AI’s Broader Implications**  
   - Tangential discussions explored whether AI could improve governance, referencing fiction (e.g., AI leaders) and real-world tech’s mixed impact (e.g., smartphones didn’t eliminate poverty). Skepticism prevailed, with users noting that AI might entrench existing power structures or corruption.  

5. **Ethical & Cultural Debates**  
   - A off-topic thread debated religion’s role in morality, questioning whether self-identified “good Christians” align with ethical behavior. This reflected broader concerns about corporate accountability and ethical decision-making in tech.  

**Key Takeaways**: The discussion underscored skepticism about Meta’s AI ROI, ethical concerns over capital allocation, and debates about the societal role of tech giants. Critics highlighted tensions between innovation and social responsibility, while supporters emphasized infrastructure’s necessity for competitiveness.

### Why do AI models use so many em-dashes?

#### [Submission URL](https://www.seangoedecke.com/em-dashes/) | 82 points | by [ahamez](https://news.ycombinator.com/user?id=ahamez) | [87 comments](https://news.ycombinator.com/item?id=45788327)

Why LLMs Won’t Stop Using Em Dashes: It’s the Books

- The author surveys popular theories for the em‑dash tic in AI writing and rejects them: it’s not simply mirroring normal English, not a “keep options open” next‑token trick, and not a token‑efficiency hack.
- RLHF dialect influence also looks weak. While “delve” may reflect African English preferences, a Nigerian English corpus showed far fewer em dashes per word (~0.022%) than general historical English (~0.25–0.275%), so RLHF raters aren’t the likely source.
- The timing lines up with a data shift. GPT‑3.5 barely used dashes; GPT‑4‑series and competitors do. Between 2022 and 2024, labs moved beyond web scrapes and pirated ebooks to large‑scale scanning of print books (Anthropic started in Feb 2024 per court filings; OpenAI likely similar).
- Older print books use more em dashes, peaking around the mid‑1800s. If the newly digitized “high‑quality” training corpora skew toward late‑19th and early‑20th century titles, you’d expect a noticeable uptick in dash usage.
- Bottom line: the em‑dash habit is probably a stylistic fossil from the older print era that modern LLMs learned when their training diet shifted toward scanned books.

Implication: stylistic tells in LLM prose may reveal corpus composition; fixing this is less about prompts and more about retraining or style‑regularizing on contemporary text.

The Hacker News discussion on em-dash usage in LLMs expands on the original hypothesis (training data shifts to older books) while debating alternative explanations and implications:

### Key Themes:
1. **Training Data Influence**:
   - Many users support the idea that older, book-heavy corpora (e.g., 19th-century literature) introduced em-dash overuse. Others note early 2000s essays and prestigious publications (*The New Yorker*, *The Atlantic*) as stylistic templates that RLHF might favor.

2. **Detection and Style**:
   - Em-dash frequency is seen as a potential “AI tell,” but users disagree on its reliability. Some argue sophisticated users can mimic or avoid it, while others call it a lazy detection heuristic.
   - Critiques of AI writing styles emerge: overly generic, verbose, or “Failed LinkedIn Marketer”-esque. Some praise AI’s concision, contrasting it with human redundancy.

3. **Typographical Debates**:
   - Users discuss alternatives like semicolons or parentheses, with debates over readability. One thread compares em-dashes to historical underlining conventions.
   - Medium’s typographic choices (auto-replacing hyphens with em-dashes) are cited as a potential influence on LLMs.

4. **Cultural and Technical Factors**:
   - Keyboard shortcuts (e.g., `Cmd+-`) and regional punctuation norms (e.g., Nigerian English) are noted as practical contributors.
   - A subthread critiques AI’s tendency toward “sophisticated” vocabulary (e.g., “delve”), seen as alienating or overly formal.

5. **Broader Implications**:
   - Some lament AI’s homogenization of writing styles, stripping personality from text. Others see utility in AI’s condensed formats for technical or SEO-driven content.
   - Humor and frustration surface, with jokes about “AI tics” and debates over whether typos or dashes are worse.

### Notable Rebuttals:
   - **Medium’s CTO** clarifies the platform’s design decisions, distancing em-dash rules from Ev Williams.
   - **Stylistic Trade-offs**: Users acknowledge that while em-dash overuse is jarring, fixing it may require retraining models on modern corpora rather than prompt hacks.

### Conclusion:
The consensus leans toward data-driven explanations (older books and prestige publications), but acknowledges intertwined factors like RLHF preferences, typographic history, and technical constraints. The debate reflects broader tensions between AI’s mimicry of “polished” styles and its struggle to replicate authentic, human nuance.

### Context engineering

#### [Submission URL](https://chrisloy.dev/post/2025/08/03/context-engineering) | 90 points | by [chrisloy](https://news.ycombinator.com/user?id=chrisloy) | [60 comments](https://news.ycombinator.com/item?id=45788842)

Context engineering: from magic prompts to programmable context

- Thesis: The fad of “prompt engineering” (clever wording to coax outputs) is giving way to “context engineering,” a disciplined, programmatic way to design every token fed to an LLM.
- Core idea: LLMs still just predict the next token. What changed wasn’t the architecture, but how we frame inputs—especially chat-style turns, system messages, and structured context.
- Context window: Each model has a fixed token budget. The real work is deciding what fills it and in what order.
- Why prompts fell short: Incantation-like phrasing is fragile and offers no guarantees. Systematic control comes from shaping the entire token sequence, not just the instruction.
- In-context learning: Models can generalize from examples you place in the window, enabling predictable behavior via curated demos, schemas, and constraints.
- What to include: programmatic examples, retrieved docs/summaries (RAG), tool/function-call affordances, multi-modal tokens, and distilled memory/history.
- Example: Ask “best sci‑fi film?” A “film critic” system role biases toward Blade Runner; inject box office data or critics’ lists and the answer shifts accordingly.

Why it matters
- Reliability comes from inputs as a system, not clever phrasing.
- Product teams must budget tokens across instructions, evidence, tools, and memory.
- Treat the context as the real “program” the model executes over.

Bottom line: Stop casting spells; start engineering the sequence.

**Summary of Discussion:**

The discussion revolves around whether "context engineering" for LLMs qualifies as true "engineering," sparking debates about terminology, methodology, and comparisons to traditional disciplines.

### Key Points:
1. **Terminology Debate**:
   - Critics argue labeling LLM techniques as "engineering" dilutes the term, as traditional engineering (civil, mechanical) relies on rigorous scientific principles, reproducibility, and deterministic outcomes. LLM outputs are probabilistic and lack guarantees.
   - Proponents defend the term, likening it to "software engineering," which also deals with uncertainty. They emphasize systematic approaches (e.g., RAG systems, structured context design) as engineering practices.

2. **Engineering vs. Craft**:
   - Comparisons to woodworking ("craft") highlight distinctions: engineering prioritizes predictability and theoretical models, while crafts involve skilled improvisation. Critics argue LLM work resembles the latter.
   - Counterpoints note that even traditional engineering involves trial-and-error phases (e.g., early steam engines) and that context engineering aims for structured, repeatable systems.

3. **Practical Applications**:
   - Examples like DSPy, retrieval-augmented generation (RAG), and constrained decoding libraries illustrate efforts to systematize LLM inputs. Users stress the importance of "context as code" for reliability.
   - Some acknowledge LLMs’ stochastic nature but argue engineering principles (e.g., constraints, testing) can stabilize outputs.

4. **Humor and Sarcasm**:
   - Jokes about "casting spells" and "criminal" misuse of "engineering" reflect skepticism. References to Poe’s Law and debates over semantics underscore the tension between innovation and traditional definitions.

### Conclusion:
The debate reflects a cultural clash between emerging AI practices and established engineering norms. While critics demand stricter adherence to scientific rigor, proponents advocate for expanding the term to include systematic, context-driven approaches to LLMs. The consensus leans toward recognizing "context engineering" as valid when applied methodically, even if it diverges from classical engineering paradigms.

### Show HN: Anki-LLM – Bulk process and generate Anki flashcards with LLMs

#### [Submission URL](https://github.com/raine/anki-llm) | 53 points | by [rane](https://news.ycombinator.com/user?id=rane) | [21 comments](https://news.ycombinator.com/item?id=45790443)

What it is
- A CLI toolkit that connects Anki to modern LLMs for large-scale note cleanup, enrichment, and new card generation.
- Built for workflows that don’t scale in the Anki UI (e.g., verifying translations, adding grammar/context fields, generating examples).
- MIT-licensed. Repo: github.com/raine/anki-llm (≈94★ at post time)

Why it matters
- Power users often sit on thousands of cards that need consistent tweaks. This brings templated, repeatable, and resumable AI processing to Anki—without manual copy/paste chaos.

How it works
- Two modes:
  - File-based: export deck to clean CSV/YAML, process with an LLM, then import—supports resume and incremental saves.
  - Direct: process and update notes in-place via AnkiConnect.
- Custom prompt templates let you define exactly how fields are transformed.
- Concurrency, retries, and auto-resume make long jobs practical.
- “Copy mode” supports pasting responses from ChatGPT/Claude if you don’t want to use API keys.

Supported models and pricing (per README; per million tokens)
- OpenAI: gpt-4.1, gpt-4o, gpt-4o-mini, gpt-5, gpt-5-mini, gpt-5-nano ($0.05–$10/M range depending on model, input/output differ)
- Google Gemini: gemini-2.0-flash, 2.5-flash, 2.5-flash-lite, 2.5-pro ($0.10–$10/M)
- Configure via OPENAI_API_KEY or GEMINI_API_KEY env vars.

Notable features
- Batch verify translations; add “Key Vocabulary” fields with readings/meanings/HTML context; generate multiple contextual cards for a term and approve them interactively.
- Export/import cleanly to CSV/YAML with key-field matching to update existing notes.
- Scriptable access to AnkiConnect from the CLI (useful for agents/automation).

Commands (high level)
- export, import, process-file, process-deck, generate-init, generate, query
- anki-llm config lets you set defaults (e.g., model).

Requirements and install
- Node.js v18+, Anki Desktop running, AnkiConnect add-on installed.
- Install: npm install -g anki-llm

HN takeaway
- For heavy Anki users and language learners, this bridges the gap between LLM-assisted editing and the realities of large decks: templated prompts, safe batch ops, and resumable pipelines.

The discussion around the Anki-LLM tool highlights several key debates and perspectives among users:

### **Efficiency vs. Effort in Card Creation**
- **Pro-LLM Automation**: Many users argue that AI-generated cards (e.g., via ChatGPT) streamline the creation of example sentences, grammar points, and translations, saving significant time. This is especially valuable for language learners who face bottlenecks in sourcing high-quality, context-rich material.
- **Skepticism of Blind Trust**: Users caution against uncritically accepting AI outputs, particularly for non-native languages. Manual verification is emphasized to ensure accuracy and cultural/demographic relevance.

### **Spaced Repetition & Retention**
- **Anki’s Strengths**: Participants praise Anki for enabling long-term retention of low-frequency vocabulary and complex grammar through spaced repetition. It addresses the "exposure gap" for learners not immersed in a target language environment.
- **Active Engagement Matters**: Some cite research suggesting that the act of creating cards (rewriting, synthesizing) enhances memory formation, similar to reviewing. However, others counter that overly time-intensive card design (e.g., context-heavy cards) can be counterproductive.

### **Workflow & Tool Appreciation**
- **Tool Features**: Users applaud Anki-LLM’s integration with AnkiConnect, batch processing, and detailed documentation. These features reduce manual copy-pasting and enable scalable deck management.
- **UI Limitations**: A critique of Anki’s native UI emerges, with users seeking more efficient card-editing interfaces (e.g., spreadsheet-like views) to complement AI automation.

### **Broader Learning Strategies**
- **Source Material First**: Some stress the importance of engaging directly with books, films, or real-world content before creating cards, arguing that Anki should supplement—not replace—immersion.
- **Balancing Methods**: A recurring theme is balancing Anki with other methods (e.g., reading, media consumption) to maintain engagement and avoid the "dryness" of pure flashcard repetition.

### **Controversial Take**
- One user provocatively questions whether AI-generated cards risk creating "mindless" learners reliant on stochastic outputs rather than deep understanding, though this view is contested.

### **Conclusion**
The community largely views Anki-LLM as a powerful tool for scaling card creation, particularly for language learners, while emphasizing the need for human oversight and integration with broader learning practices. The debate reflects a tension between efficiency gains from AI and the cognitive benefits of active, manual engagement.

### How I use every Claude Code feature

#### [Submission URL](https://blog.sshh.io/p/how-i-use-every-claude-code-feature) | 509 points | by [sshh12](https://news.ycombinator.com/user?id=sshh12) | [182 comments](https://news.ycombinator.com/item?id=45786738)

Shrivu Shankar shares a practical, opinionated playbook for getting real work done with Claude Code—both as a hobbyist (sometimes running with --dangerously-skip-permissions) and at scale on a team spending billions of tokens per month. His north star: judge by the final PR, not vibes or chat style.

Highlights
- CLAUDE.md as the “constitution”: Keep a tightly curated root CLAUDE.md with guardrails and the 80% use cases, not a full manual. In his monorepo it’s ~13KB and only includes tools used by ≥30% of engineers.
- Don’t bloat context: Avoid @-referencing big docs; instead, tell the agent when and why to read external files. Allocate a “token budget” per tool’s docs (like selling ad space)—if a tool can’t be explained concisely, it’s not ready.
- Write wrappers, not essays: If a CLI is complex, simplify it with a small bash wrapper and document that instead of patching complexity with prose.
- Avoid negative-only rules: Don’t say “never use flag X” without a preferred alternative; it makes agents stall.
- Portability: Mirror CLAUDE.md into an AGENTS.md so other AI IDEs work with your repo.
- Context management: Use /context mid-session to see what’s filling your 200k tokens; his org sees ~20k baseline just to start in a large monorepo.
- Reset strategies:
  - /compact: Avoid—opaque and error-prone.
  - /clear + custom /catchup: Default reset; clear state, then have the agent reread all changed files.
  - “Document & Clear”: For long tasks, dump plan/progress to a .md, clear, then resume from that doc.

Why it matters
- Concrete, battle-tested tactics for running AI coding agents in real codebases, not just demos.
- A blueprint for making repos “agent-friendly” without drowning context windows.
- A useful lens on the emerging AI-IDE race (Anthropic vs. OpenAI) and why developer “vibes” matter less than repeatable outcomes.

The discussion around Shrivu Shankar's Claude Code practices highlights several key debates and practical insights:

### File Management & Compatibility
- **CLAUDE.md vs. AGENTS.md**: Users debated symlinking these files for cross-IDE compatibility. While symlinks work in Unix-based systems, Windows and Docker environments may face issues. Some argued for separate files to avoid confusion, while others preferred mirroring content.
- **Context Bloat Concerns**: Reverse-engineering Claude’s HTTP traffic revealed that referencing AGENTS.md automatically includes its content in the system prompt, raising worries about wasted tokens and opaque context usage.

### MCP (Model Control Protocol) Feedback
- **Mixed Reactions**: MCP was praised for simplifying API/tool integration but criticized for lacking scoping and real-world use cases. Some users proposed using MCP as a proxy for agent actions, while others found it impractical without tighter constraints.
- **Tool Integration Challenges**: Users noted difficulties in getting Claude to reliably use CLIs, advocating for clear, minimal file structures instead of verbose documentation.

### Reactions to AI-Generated Content
- **Writing Style Debate**: Some criticized the post’s AI-assisted writing as "disrespectful" or impersonal, while others defended its utility. A meta-discussion emerged about detecting AI text and valuing content quality over authorship.

### Practical Claude Usage Struggles
- **Reset Strategies**: Users shared frustrations with `/compact` (opaque, unreliable) and favored `/clear` + custom `/catchup`. Token management issues, like residual context post-compaction, were highlighted.
- **Mental Fatigue**: Developers expressed exhaustion from coaxing Claude to follow instructions, comparing it to "mind games" that strain productivity.

### Post Structure Feedback
- **Brevity vs. Depth**: Some wished for longer examples and case studies, while others appreciated the concise, actionable format for small projects.

### Overall Sentiment
The thread reflects a blend of technical problem-solving (file strategies, token budgets) and philosophical debates (AI’s role in writing, agent autonomy). Users seek balance between Claude’s potential and its current limitations, emphasizing clear, reproducible workflows over flashy demos.

