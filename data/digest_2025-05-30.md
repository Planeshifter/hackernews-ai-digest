## AI Submissions for Fri May 30 2025 {{ 'date': '2025-05-30T17:13:38.151Z' }}

### Surprisingly fast AI-generated kernels we didn't mean to publish yet

#### [Submission URL](https://crfm.stanford.edu/2025/05/28/fast-kernels.html) | 338 points | by [mfiguiere](https://news.ycombinator.com/user?id=mfiguiere) | [130 comments](https://news.ycombinator.com/item?id=44139454)

In a fascinating twist of events, researchers Anne Ouyang, Azalia Mirhoseini, and Percy Liang accidentally stumbled upon a batch of rapid AI-generated kernels that not only hold their own against, but even outpace, expert-optimized standard ones in PyTorch. These kernels, crafted in pure CUDA-C without the benefit of libraries such as CUTLASS and Triton, deliver remarkable performance across various foundational machine learning operations.

Their surprising effectiveness emerged from a project aimed at generating synthetic data to improve kernel generation models. Inadvertently, the actual process of test-time synthetic data generation yielded kernels that surpassed or closely matched human expert-optimized baselines.

In their findings, the researchers detail impressive performance enhancements, such as a 101.3% performance over PyTorch’s FP32 matrix multiplication for 4096x4096 matrices and a stunning 290.1% performance increase in a Conv2D + ReLU + MaxPool operation compared to PyTorch’s reference implementation. These results were benchmarked on an Nvidia L40S GPU.

What makes this achievement particularly intriguing is the simplicity of their approach, which involves an advanced form of AI model testing called KernelBench. This setup directs AI models to write custom kernels with the objective of surpassing the execution speed of standard torch operations. By reasoning in natural language about potential optimizations and branching out multiple implementations at each step, the team was able to avoid local minima—a common pitfall in optimization loops.

Ultimately, the team's approach echoes a structured exploratory search rather than a linear troubleshooting process, which allows for both creativity and efficiency in kernel generation. This development not only propels the field of performance optimization forward but also sheds light on the untapped potential of AI in solving complex computational problems. Keep your eyes on this space—they’ve only just scratched the surface.

The discussion around AI-generated kernels outperforming human-optimized ones revolves around several key debates and tangents:

1. **Understanding vs. Statistical Guessing**:  
   A central argument questions whether LLMs truly "understand" tasks or merely rely on statistical patterns. Critics (e.g., *ysn*) assert that LLMs lack genuine comprehension, functioning as "stochastic parrots" that predict text without deeper reasoning. Proponents counter that practical results (e.g., optimized kernels) suggest a form of "reasoning," even if mechanistically different from human cognition. Analogies are drawn to trained biologists discussing evolution without grasping its deeper principles.

2. **Practical Applications and Skepticism**:  
   Some users (*wslh*, *literalAardvark*) express skepticism about scalability and real-world impact. They argue that AI tools may not solve fundamental business challenges (e.g., SEO competition) and question the feasibility of claims like "175 employees" achieving complex tasks. Others (*nm*) defend the potential, citing examples of AI-driven systems automating workflows and generating insights.

3. **Human vs. AI Creativity**:  
   *hayst4ck* references Bloom’s Taxonomy, noting that while AI excels at lower-level tasks (e.g., applying patterns), human creativity at the "Create" tier remains unmatched. The discussion highlights concerns about AI’s ability to innovate beyond training data constraints.

4. **Side Debates**:  
   - **Grammar and Language**: Users debate whether formal grammars are necessary for LLMs to generate code, with some arguing that human language acquisition isn’t grammar-centric.  
   - **Startup Claims**: *nm* shares a tangential anecdote about building a stealth startup with AI, attracting both curiosity and skepticism about its $25M valuation and rapid development.  

**Overall Tone**: The thread reflects cautious optimism tempered by skepticism. While impressed by AI’s technical achievements, participants emphasize the need for clearer definitions of "understanding" and more evidence of real-world scalability. The discussion underscores the gap between theoretical AI capabilities and practical, nuanced problem-solving.

### The ‘white-collar bloodbath’ is all part of the AI hype machine

#### [Submission URL](https://www.cnn.com/2025/05/30/business/anthropic-amodei-ai-jobs-nightcap) | 528 points | by [lwo32k](https://news.ycombinator.com/user?id=lwo32k) | [915 comments](https://news.ycombinator.com/item?id=44136117)

In a world where tech CEOs often stir the pot with apocalyptic predictions, Dario Amodei, the 42-year-old billionaire and CEO of the AI firm Anthropic, is making headlines with his own bold claims. Speaking to Axios and CNN's Anderson Cooper, Amodei suggested that AI could soon outstrip humans at most intellectual tasks, potentially wiping out half of all entry-level office jobs in the near future. This striking prediction seems part of a Silicon Valley playbook suggesting that while AI promises to revolutionize everything, it must first upend existing systems.

Amodei paints a vivid picture of an AI-led future: a disease-free utopia with booming economic growth, albeit amid substantial unemployment due to technological displacement. Yet, critics argue his claims lack concrete evidence and echo familiar tech industry rhetoric—where AI is both the villain and the savior, needing regulation yet heralding a new age.

This narrative plays into Anthropic’s positioning as an "AI safety and research" firm, a stance they claim sets them apart from other tech giants like OpenAI. However, it's not just the dystopian predictions gaining attention; the backdrop of recent model updates, such as Anthropic’s Claude chatbot, continues to push the firm into the limelight, marketing its innovations as both a cautionary tale and a groundbreaking leap forward.

As the discourse around AI’s future role in society intensifies, figures like Mark Cuban weigh in, reminding us that technological shifts have historically displaced certain roles only to create new industries and opportunities. While Amodei's stark warnings are positioned as a call to action, some view them as savvy marketing for Anthropic’s burgeoning AI endeavors. The debate continues on whether AI will truly catalyze a transformative economic shift or if such grand claims are partly designed to amplify the company's profile.

**Summary of Hacker News Discussion:**

The discussion around Dario Amodei’s AI predictions reveals skepticism and nuanced debate, with users dissecting economic, policy, and societal factors alongside AI’s role in job displacement. Key points include:

1. **Economic Context Over AI Hype**:  
   Many argue that macroeconomic policies, such as **Zero Interest Rate Policy (ZIRP)** and pandemic-era hiring bubbles, have had a more immediate impact on tech job markets than AI. Users note that the post-2020 hiring surge (e.g., software job postings peaking at 161% of pre-pandemic levels) was driven by cheap capital, not AI. Critics suggest Amodei’s warnings may conflate AI’s potential with cyclical economic trends.

2. **Personal Job Market Anecdotes**:  
   Developers share struggles in the current job market, citing fewer responses to applications despite experience. Some attribute this to globalization and saturation in the software labor market rather than AI-driven displacement.

3. **Tax Policy and Regulation**:  
   Mentions of **Section 174** (amortization of software development costs) highlight policy shifts affecting tech investment. Users debate whether such changes, now partially repealed, influenced hiring more than AI.

4. **Keynesian Workweek Predictions vs. Reality**:  
   A thread contrasts Keynes’ 1930s vision of a 15-hour workweek with today’s reality of “bullshit jobs” and consumption-driven economies. Users discuss systemic barriers to reduced work hours, such as rising living costs in high-expense cities and societal pressures to maintain lifestyles.

5. **AI’s Broader Societal Impact**:  
   Some users speculate on AI’s role in social manipulation, citing examples like foreign propaganda campaigns and political disinformation. Others dismiss this as overstated, arguing human-driven troll farms remain more impactful.

6. **Skepticism Toward AI Narratives**:  
   Amodei’s warnings are viewed by some as part of a Silicon Valley playbook to position firms like Anthropic as “AI safety leaders” while marketing their products. Comparisons are drawn to past tech hype cycles, where disruption claims often served corporate interests.

**Conclusion**:  
The discussion underscores a divide: while AI’s potential is acknowledged, many argue its near-term impact is overstated compared to entrenched economic forces. Broader themes of class dynamics, policy influence, and historical context dominate, with users urging caution against conflating AI’s future promise with present-day marketing narratives.

### The Darwin Gödel Machine: AI that improves itself by rewriting its own code

#### [Submission URL](https://sakana.ai/dgm/) | 218 points | by [birriel](https://news.ycombinator.com/user?id=birriel) | [190 comments](https://news.ycombinator.com/item?id=44135369)

Imagine a world where artificial intelligence not only learns but evolves, constantly rewriting its own code to become smarter and more efficient. Enter the Darwin Gödel Machine (DGM), a collaborative innovation between researchers and Jeff Clune's lab at UBC. Inspired by the theoretical Gödel Machine, envisioned decades ago by Jürgen Schmidhuber, the DGM addresses the challenge of AI self-improvement with a dazzling twist. Instead of relying on the difficult task of mathematically proven code improvements, it adopts a more practical approach, taking cues from the endless adaptation of Darwinian evolution.

This new class of AI leverages open-ended algorithms to explore a rich landscape of potential code enhancements. These algorithms do not merely solve tasks. Instead, they transform AI, making it capable of continuous self-modification - a concept not too different from how species evolve in nature over time. The DGM is designed to seek out and implement improvements based on empirical performance gains, harnessing the strengths of foundation models to suggest novel code advancements.

In their groundbreaking experiments, DGMs exhibited significant progress in their coding capabilities. For example, on challenging coding benchmarks like SWE-bench, which involves solving real-world GitHub issues, and the multi-language Polyglot benchmark, the DGM dramatically improved its performance metrics—showcasing the power of continual self-improvement. Starting from a baseline performance of 20.0% on SWE-bench, it soared to an impressive 50.0%. Similarly, on Polyglot, it improved from 14.2% to a remarkable 30.7%, outpacing well-regarded hand-designed agents.

This self-evolving coding wunderkind doesn't merely stop at tweaking its code; it systematically evaluates these changes, thus learning if the adjustments deliver tangible benefits. The dynamic archive it constructs allows for a branching exploration of countless evolutionary paths, effectively avoiding the confines of suboptimal designs and opening doors to novel solutions.

Safeguards are pivotal, and the team dedicates efforts to address the safety of such self-improving systems. The potential societal benefits of harnessing this form of AI could be immense, and thus careful consideration is warranted to ensure safe deployment and application. 

The DGM represents a step towards an AI future where self-feedback loops and open-ended exploration can lead to boundless advancements, pushing the limits of what machine learning and AI can achieve. Its performance compared to traditional, non-evolving AI systems underscores an exciting horizon for the field: one in which AI systems autonomously push the boundaries of their own intelligence, sparking a new era of innovation.

**Summary of Hacker News Discussion on the Darwin Gödel Machine (DGM):**

The discussion around the Darwin Gödel Machine (DGM) centers on skepticism, definitions of AGI, and the practical limitations of current AI systems. Key themes include:

1. **Skepticism About Exponential Self-Improvement**:  
   Users question whether today’s LLMs (like ChatGPT) can achieve exponential self-improvement akin to human cognition. While incremental progress is acknowledged (e.g., ChatGPT’s rise in popularity), many argue improvements are gradual, not revolutionary. Comparisons are drawn to Tesla’s Autopilot, where progress is slower than initially anticipated.

2. **AGI Definitions and Moving Goalposts**:  
   Debates erupt over what constitutes AGI. Some argue that human-level intelligence requires navigating the real world (e.g., manipulation, problem-solving), not just generating text. Others note that benchmarks for AGI have shifted over time—early tests considered signs of intelligence (e.g., solving coding tasks) are now achievable by LLMs, yet AGI remains elusive. References to sci-fi (e.g., Star Trek’s Data, HAL from *2001*) highlight idealized visions versus current AI capabilities.

3. **Human Intelligence as a Benchmark**:  
   Participants discuss whether human cognition is the right yardstick. While humans are "generally intelligent," they are also inconsistent (e.g., Isaac Newton’s biblical predictions) and prone to post-hoc rationalization—traits mirrored in AI errors. Some users suggest LLMs lack true understanding, relying instead on pattern-matching.

4. **Technical Limitations of LLMs**:  
   Examples like asking an LLM to "count pauses in text" or "generate Python code" reveal gaps in comprehension. Skeptics argue LLMs mimic outputs without logical consistency, while proponents highlight their practical utility even if flawed.

5. **Timelines and Overhyped Predictions**:  
   Predictions for AGI range from “6 months to 50 years,” with criticism of perpetual hype cycles. A joke about AGI requiring “nuclear fusion power solved in 30 years” underscores frustration with speculative timelines.

6. **Safety and Societal Impact**:  
   Brief mentions of safety concerns emphasize the need for caution but lack deep exploration. The focus remains on technical feasibility rather than ethical implications.

**Conclusion**: The discussion reflects cautious optimism mixed with skepticism. While the DGM’s self-improvement milestones are noted, users stress that AGI remains undefined and distant, with current AI systems excelling in narrow tasks but lacking broader, human-like adaptability. The debate underscores the gap between theoretical aspirations and today’s AI realities.

### Triangle splatting: radiance fields represented by triangles

#### [Submission URL](https://trianglesplatting.github.io/) | 163 points | by [ath92](https://news.ycombinator.com/user?id=ath92) | [70 comments](https://news.ycombinator.com/item?id=44132744)

In a thrilling development for the world of computer graphics, a new research paper is breathing life back into the use of triangles for rendering high-quality images in real-time. Developed by a collaborative team from top universities and institutions, including the University of Oxford and Google DeepMind, the method known as "Triangle Splatting" promises sharper and more detailed image synthesis compared to the traditional Gaussian methods.

Triangle Splatting leverages the power of triangles, optimizing them with differentiable renderers that allow for end-to-end gradient training. This approach not only preserves sharpness and details better than Gaussian splatting but also boasts faster rendering speeds. A standout achievement is the method's ability to deliver over 2,400 frames per second at HD resolution using standard mesh rendering hardware, showcasing its efficiency for practical applications.

The methodology centers around a rendering pipeline that uses 3D triangles as primitives, projecting them onto image planes. By using a smooth window function derived from the 2D signed distance field of the triangle, the technique ensures maximum opacity at the triangle's center while allowing for adjustable sharpness. This results in high-quality renders that capture more drive and detail, as seen in comparisons on the Mip-NeRF360 dataset, where Triangle Splatting outperformed leading techniques in visual fidelity.

This innovative approach suggests a promising future for integrating radiance fields into interactive 3D environments, aligning seamlessly with traditional graphics stacks. The authors hint at exciting future work to enhance visual fidelity further, making this method a viable candidate for real-time applications in video games, AR/VR, and more. With these advances, Triangle Splatting could revolutionize how we think about rendering in modern graphics pipelines.

The Hacker News discussion on the Triangle Splatting paper highlights several key themes and debates:

### **Technical Comparisons and Advantages**
- **Triangles vs. Gaussians**: Users note that triangles, as foundational geometry in traditional 3D graphics, offer advantages in hardware compatibility and rendering efficiency. Gaussian splatting (3DGS) relies on fuzzy, volumetric representations, which struggle with high-frequency details and watertight surfaces. Triangles, by contrast, simplify rendering pipelines and align with GPU-optimized mesh workflows.
- **Performance**: Triangle splatting’s ability to render 2,400+ FPS at HD resolution using standard hardware impressed commenters. This contrasts with Gaussian methods, which require sorting millions of primitives and face computational bottlenecks.

### **Integration with Existing Pipelines**
- **Game Engines and GPUs**: Triangles are inherently compatible with game engines and rasterization pipelines, avoiding the need for "reinventing the wheel." Users highlight that GPUs excel at processing connected triangles with shared vertices, reducing draw calls and leveraging decades of optimization.
- **Photogrammetry and NeRFs**: While Neural Radiance Fields (NeRFs) revolutionized scene reconstruction, they remain computationally intensive. Triangle splatting could bridge the gap by offering a structured, surface-based alternative to NeRF’s volumetric approach, though some argue it may not handle translucency or reflections as elegantly.

### **Criticisms and Challenges**
- **Geometric Limitations**: Critics question whether triangles can fully replace Gaussian splats for complex effects like reflections, translucency, or volumetric phenomena (e.g., fog). The method’s reliance on explicit surfaces may limit its ability to model "messy" real-world scenes.
- **Adoption Hurdles**: While faster, triangle splatting still requires reconstructing logical meshes from unstructured data (e.g., point clouds), which remains a challenge. Some users argue that hybrid approaches (e.g., combining triangles with volumetric attributes) might be necessary.

### **Future Implications**
- **Real-Time Applications**: The technique is seen as promising for AR/VR, gaming, and real-time photogrammetry, where speed and compatibility matter. Users speculate that integrating triangle splatting with modern shaders and PBR (physically based rendering) could yield high-fidelity results.
- **Research Synergies**: Links to related projects (e.g., SVRaster, Plenoxels) suggest ongoing innovation in balancing geometric precision with volumetric flexibility. The discussion also touches on autoencoders and neural rendering as complementary advancements.

### **Community Sentiment**
- **Optimism**: Many applaud the return to triangles as a pragmatic step, leveraging hardware strengths while improving on Gaussian methods’ shortcomings.
- **Skepticism**: Some remain cautious, noting that triangle splatting doesn’t fully address challenges like dynamic lighting or complex material properties, and may not be a "silver bullet" for all rendering scenarios.

In summary, the community views triangle splatting as a promising evolution in real-time rendering, particularly for structured environments, but acknowledges that hybrid methods and further research are needed to handle the full complexity of real-world scenes.

### Tokenization for language modeling: BPE vs. Unigram Language Modeling (2020)

#### [Submission URL](https://ndingwall.github.io/blog/tokenization) | 75 points | by [phewlink](https://news.ycombinator.com/user?id=phewlink) | [34 comments](https://news.ycombinator.com/item?id=44134290)

If you’ve ever puzzled over the quirks of language tokenizers, this deep dive on Hacker News will strike a chord. The post dissects how common tokenizers used in models like BERT and GPT-2 often misinterpret prefixes like “de-” in words like "destabilizing." These misreadings can mistakenly link unrelated words such as "destigmatize" and "destinies," thanks to non-traditional slicing strategies like byte pair encoding (BPE). Despite such missteps, models have managed impressive feats of language understanding, suggesting they overcome these interpretive hurdles through sheer data volume and model size.

However, a new contender on the block may offer a sharper reading eye—Unigram Language Modeling. Recent research highlights that by swapping out BPE for Unigram LM, models better maintain morphological connections. This means they can more effectively recognize common linguistic elements like suffixes, potentially offering performance gains when fine-tuned for specific tasks. The post illustrates this with comparisons of tokenizer performance on examples from Wikipedia, demonstrating Unigram LM's superior handling of morphological nuances.

Could this shift lead to smaller, faster models that still achieve remarkable results? The post invites readers to ponder the future of tokenization in NLP, highlighting that Unigram LM might just be a hidden gem in text preprocessing methodologies, previously overshadowed by BPE's widespread adoption. Thus, as NLP tackles uncharted territories, the choice of tokenization method could emerge as a key factor in model efficiency and accuracy.

The Hacker News discussion explores the nuances of tokenization methods in NLP, focusing on their impact on model performance, efficiency, and creativity. Key points include:

1. **Tokenization Techniques**:  
   - **BPE vs. Unigram LM**: While BPE (Byte Pair Encoding) is widely used, Unigram LM is noted for better handling of morphological relationships (e.g., prefixes/suffixes), potentially improving task-specific fine-tuning. Newer methods like **SuperBPE** (claiming ~8% efficiency gains) and **SaGe** (context-aware tokenization) are also highlighted.  
   - **Implementation Challenges**: SentencePiece’s Unigram implementation starts with a large initial vocabulary and trims it, contrasting with BPE’s bottom-up approach. Technical details, such as n-gram scoring and suffix array algorithms, are debated.

2. **Vocabulary Size Trade-offs**:  
   - Smaller vocabularies (e.g., TokenMonster) may reduce computational costs but risk losing semantic granularity. Larger vocabularies improve expressiveness but increase model size and training complexity.  
   - Experiments suggest vocabulary size doesn’t always correlate with downstream performance, prompting calls for better benchmarks (e.g., [tokenizers_intrinsic_benchmark](https://github.com/MeLeLBGU/tokenizers_intrinsic_benchmark)).

3. **Model Performance and Creativity**:  
   - Tokenization affects LLM outputs, including creative tasks like poetry generation. Examples like **TinyStories** illustrate how models maintain coherence despite tokenization quirks.  
   - Subword splitting (e.g., separating "YELLED" into components) may theoretically aid in generating nuanced words (e.g., "WHISPERED"), though practical results vary.

4. **Technical Debates**:  
   - Proposals for character-level encoding and multi-channel embeddings (prefix/suffix splits) aim to preserve word structure.  
   - Concerns about tokenizer path dependency and the need for standardized evaluation metrics persist, with some advocating for intrinsic benchmarks over full model retraining.

5. **Community Momentum**:  
   - Rapid innovation in tokenization reflects broader AI advancements, with participants emphasizing open-source collaboration and iterative testing (e.g., nanoGPT experiments).

In summary, the discussion underscores tokenization as a critical yet underappreciated factor in NLP, balancing efficiency, linguistic fidelity, and model scalability. New methods and benchmarks may drive future breakthroughs, challenging BPE’s dominance.

### Personal Ambient Computing

#### [Submission URL](https://www.chrbutler.com/pac-personal-ambient-computing) | 17 points | by [delaugust](https://news.ycombinator.com/user?id=delaugust) | [4 comments](https://news.ycombinator.com/item?id=44139053)

In an intriguing exploration of the future of personal computing, Christopher Butler introduces the concept of Personal Ambient Computing (PAC), suggesting a departure from our current reliance on singular devices like smartphones. Drawing inspiration from the visionary designs of Star Trek, Butler imagines a future where computing is decentralized and spread across multiple interconnected devices, enhancing mobility and reducing distractions.

This evolution doesn't imply that any new device will render existing ones obsolete. Instead, it envisions a harmonious mesh network of devices, each designed for specific contexts—a modular ecosystem where a core PAC unit, a disc roughly the size of a silver dollar, offers processing power, storage, and connectivity. This core can integrate into various enclosures such as watches, handhelds, or even household appliances, illustrating an advanced mesh network of personal modules.

Butler critiques recent tech efforts like the Humane Pin and speculations around io/OpenAI devices for trying to replace foundational tech like smartphones with overly singular solutions. Instead, he champions a broader, more modular vision—one that embraces open-source hardware and software, making repairability and sustainability key features. By standardizing the PAC unit's form factor (enabling it to snap into diverse enclosures), it opens avenues for innovation and customization without needing frequent redesign.

This paradigm shift promises to create an enriched and flexible computing environment with a focus on specific, user-driven contexts over a one-size-fits-all device. Such a system not only promises enhanced functionality but also signals a new era of creativity in tech development—one where personal and environmental needs drive technological evolution. It's a compelling vision for a future where computing doesn't replace but rather elevates our everyday interactions with technology.

The discussion around Christopher Butler's vision of Personal Ambient Computing (PAC) highlights both support and critical reflections from Hacker News users:  

1. **Support for PAC's Vision**: Users acknowledge the potential of decentralizing computing into modular, context-aware systems, citing parallels to concepts in Don Norman’s *The Invisible Computer* and the need to evolve beyond smartphone-centric interfaces.  

2. **Critique of Current Tech**: Comments question recent attempts (e.g., Humane Pin) to replace smartphones, arguing they overlook generational hardware challenges. Critics suggest PAC’s success hinges on minimizing computational overhead and adopting **context-aware "piggybacking"** on existing infrastructure.  

3. **Open-Source & Modular Design**: Emphasis is placed on open-source development, with examples like customizable smartwatches with long battery life, to foster innovation and sustainability. A sub-comment proposes leveraging **3D printing** and open enclosures for user-driven hardware customization.  

4. **Practical Considerations**: Users stress the importance of **replaceable compute cores** and modular ecosystems that allow seamless transitions between devices (e.g., docking powerful hardware while maintaining personal data profiles).  

Overall, the conversation underscores enthusiasm for PAC’s vision but highlights unresolved challenges in implementation, advocating for community-driven, open frameworks to realize its full potential.
