## AI Submissions for Sun Jan 18 2026 {{ 'date': '2026-01-18T17:13:41.528Z' }}

### Predicting OpenAI's ad strategy

#### [Submission URL](https://ossa-ma.github.io/blog/openads) | 561 points | by [calcifer](https://news.ycombinator.com/user?id=calcifer) | [512 comments](https://news.ycombinator.com/item?id=46668021)

HN Top Story: “The A in AGI stands for Ads” argues ChatGPT is becoming a search-like ad machine

What’s new
- The post pushes back on “OpenAI is doomed” narratives and frames OpenAI’s next act as an ads business.
- OpenAI reportedly began rolling out ads on Jan 16, 2026 to Free and Go tiers: clearly labeled units at the bottom of answers, with controls to see “why this ad” or dismiss.
- Stated principles: ads won’t influence answers, conversations aren’t shared/sold, users can turn off personalization. Plus/Pro/Business/Enterprise remain ad‑free.

Rollout the author expects
- Q1 2026: limited beta with select advertisers.
- Q2–Q3: expansion into ChatGPT Search for free users.
- Q4: sidebar sponsored content + affiliate/checkout features.
- 2027: international expansion and a self‑serve ads platform.
- Possible “conversational ads” where you can ask follow‑ups about products.

By the numbers (author’s cites/estimates)
- 2025: $10B ARR by June; first $1B revenue month in July; aiming for $20B ARR; burn $8–12B.
- Usage: ~800M WAU, ~190M DAU, 35M paying subs, 1M business customers; approaching 1B WAU in 2026; ~2.5B prompts/day.
- Ads revenue targets reported elsewhere: $1B in 2026, scaling to $25B by 2029 (OpenAI hasn’t confirmed).

Positioning vs incumbents
- Google: high intent + full adtech stack → ~$296B ads run rate; implied ARPU ≈ $59/user/yr.
- Meta: low intent + full stack → ~$50 global ARPU.
- X: engagement + limited stack → ~$5.5 ARPU.
- ChatGPT: high intent, massive scale, but no vertical ad stack yet. The author places its ARPU potential closer to Search than Social—somewhere between X and Meta initially, with upside if it builds the stack (auction, targeting, commerce/affiliate, checkout).

Why it matters
- Assistants are converging with search: both Google (Gemini + AI Overviews ads) and OpenAI are racing to monetize “answer engines.”
- Trust will hinge on whether “answer independence” holds up, how clearly ads are labeled, and regulatory scrutiny.
- For marketers: a new, high‑intent channel; expect self‑serve auctions, affiliate hooks, and conversational product flows.
- For developers/publishers: more zero‑click answers and shifting discovery dynamics as assistant surfaces become ad inventory.

Author’s stance: OpenAI isn’t dying or angling for acquisition; it’s gearing toward a giant IPO and building the next big ad platform disguised as AGI.

**Discussion Summary**

While the original post focused on OpenAI's pivot to advertising, the comments veered into a broader debate on the macroeconomic friction of advertising costs and their relationship to wages.

*   **The Cost of Customer Acquisition:** The discussion sparked when a founder lamented that their robotic pharmacy startup spends $40M annually on advertising against only $5M on software and facilities. They argued that tech platforms (Google, Meta) function as rent-extractors, taking up to 50% margins and forcing businesses to act as pass-through entities for ad revenue.
*   **Ad Spend vs. Wages:** A commenter proposed that high advertising spend is a symptom of income inequality. The theory is that suppressed wages lower general purchasing power, forcing companies to spend aggressively to capture the remaining demand. They cited *The Spirit Level*, noting a correlation between high inequality and high ad spend in rich nations, attributed to status anxiety.
*   **The Henry Ford Debate:** Users debated the "Henry Ford" economic theory—that raising wages creates the customers needed to buy products. 
    *   **Skeptics** argued the math doesn't work for individual firms: if a car company doubles wages, those workers only spend a tiny fraction (e.g., 7%) of that new income on cars, resulting in a net loss for the firm.
    *   **Proponents** countered that this is a collective action problem. While it fails for a single "island economy" or firm, widespread wage increases boost aggregate demand because workers have a higher marginal propensity to consume than investors.
*   **Productivity vs. Distribution:** The thread devolved into a dispute over economic modeling. Some users argued that raising wages without increasing productivity (the "coconut island" model) merely causes inflation. Others pushed back, arguing that modern economies suffer from distribution issues and demand shortages rather than a lack of supply or productivity.

### Starting from scratch: Training a 30M Topological Transformer

#### [Submission URL](https://www.tuned.org.uk/posts/013_the_topological_transformer_training_tauformer) | 135 points | by [tuned](https://news.ycombinator.com/user?id=tuned) | [51 comments](https://news.ycombinator.com/item?id=46666963)

Tauformer: a topological twist on Transformers, with early signs of promise at 30M params

- What’s new: Instead of dot‑product attention, Tauformer compresses each head to a single “taumode” scalar via a Graph Laplacian over a domain embedding (“domain memory”). Attention logits are then just the negative distance between these scalars. The aim: bias attention toward domain-relevant structure rather than generic geometric similarity.

- How it works:
  - Keep the usual Q/K/V, RoPE, causal masking, and softmax/value aggregation.
  - Replace Q·K scoring with a bounded Rayleigh-quotient energy xᵀLx/(xᵀx+ε) mapped to [0,1), yielding a per-token, per-head scalar λ.
  - Logits = −|λ_q − λ_k| / temperature.

- Why it could be cheaper:
  - KV-cache can store (V, λ_k) instead of (K, V), trimming per-layer cache by roughly ~50% for typical head dims.
  - With a sparse, precomputed Laplacian from a domain manifold, computing λ can depend on nnz(L) rather than dense D² operations.

- The 30M run (TauGPT, GPT‑2–style):
  - 6 layers, 6 heads, d_model=384, seq_len=1024, vocab=30,522.
  - AdamW, base LR 5e‑4, 100‑step warmup; routed validation (~5%).
  - Throughput ~60K tokens/s on ~7 GB VRAM; total tokens ~655M over 5,000 steps.
  - Validation loss drops from ~4.93 (step 100) to ~2.36 (step 2000), with a best ~1.91 at step 4500; later steps show volatility/regression.
  - Notably, taumode (the attention geometry) was fixed throughout this run.

- Takeaways:
  - Early learning is strong and fast at this small scale even without updating the domain geometry.
  - The later instability suggests the geometry may need to adapt as weights evolve.

- What’s next:
  - “Adaptive” taumode strategies that periodically recalibrate (including gradient‑gated schemes to detect energy drift).
  - Larger‑scale tests (~100M parameters).
  - Release of code, configs, data, and logs under a permissive license once consolidated.

- Why it matters:
  - If the Laplacian‑guided scalar attention holds up, it could bring domain-aware inductive bias, smaller KV caches, and potentially cheaper inference—especially compelling for long‑context or memory‑constrained deployments.

Bonus curiosity: The author flags an open question—how does cross‑entropy relate to the learned taumode distribution during training?

Here is a summary of the discussion:

**Scaling and Viability**
Much of the discussion focused on whether a 30M parameter test is sufficient to predict success at scale. User `Lerc` and others cautioned that performance differences at 30M rarely represent behavior at 30B or 72B, urging the author to run "real-world" benchmarks before claiming victory. The author (`tnd`) acknowledged this, noting the current roadmap includes scaling to 100M, but emphasized that early benchmarks against a Karpathy nanoGPT fork showed comparable generation quality with ~20% faster inference.

**Architecture and Efficiency**
The reduction of the KV-cache size (by roughly 50%) was identified by users like `fby` as a "massive win" if the model quality holds up. When asked if this could be swapped into existing models, the author clarified that the architecture requires a total redesign—shifting from vector interactions to scalar interactions in a topological space—meaning current model weights cannot simply be converted.

**Technical Constraints and Comparisons**
Discussion arose regarding the efficiency of the fixed Laplacian matrix. While some feared the pre-computation might be heavy, the author argued that calculating sparse Laplacian vectors is negligible ("infinitely cheaper") compared to the dense dot-product operations required by standard attention. Other commenters drew parallels to older graph-based language modeling research, such as Sparse Non-Negative Matrix models, and discussed how this approach differs from how standard Transformers handle locality via position vectors.

**Tokenization Tangent**
A side conversation explored replacing discrete tokens with embeddings or byte-level encodings to better capture code syntax (e.g., for IDEs). The author noted that the Tauformer's manifold Laplacian approach might be particularly well-suited for such structure-heavy domains like code snippets.

### Show HN: Figma-use – CLI to control Figma for AI agents

#### [Submission URL](https://github.com/dannote/figma-use) | 109 points | by [dannote](https://news.ycombinator.com/user?id=dannote) | [37 comments](https://news.ycombinator.com/item?id=46665169)

Figma, meet the command line: figma-use brings full read/write control (not just read) to Figma via a CLI and JSX renderer, aimed at humans and AI agents alike.

Why it’s interesting
- LLM-friendly interface: use compact CLI commands or describe UIs in JSX (echo '<Frame…>' | figma-use render). The author argues this is cheaper and more natural for agents than verbose MCP/JSON-RPC schemas.
- Goes beyond Figma’s official MCP plugin, which is read-only; this supports creating/editing nodes, styles, variables, components, exports, and more.

How it works
- Install via npm, add the companion Figma plugin, and run a local proxy. Then:
  - Imperative mode: figma-use create frame --width 400 --fill "#FFF" ...
  - Declarative mode: pipe pure JSX over stdin; for logic/variants, use .figma.tsx files.

Notable features
- Components and real ComponentSets with variants; define once, instantiate many.
- Iconify integration: drop in 150k+ icons by name (e.g., mdi:home, lucide:star).
- Figma Variables as design tokens with fallbacks in JSX; CLI supports var:Tokens or $Tokens.
- Diffs: structural patches with validation and visual diffs highlighting pixel changes.
- Exports and inspection: one-liners to export nodes/screenshots; readable page tree.
- 100+ commands; designed for chaining in agent workflows.

Caveats
- stdin render only accepts pure JSX (no variables/logic).
- Requires running the plugin/proxy with Figma desktop (Development mode).
- MCP/JSON-RPC are supported, but the author notes extra overhead vs CLI/JSX.

Details
- Open source (MIT). Repo: dannote/figma-use (≈220★). Package: @dannote/figma-use on npm.

Here is a summary of the discussion:

**The "Code-First" vs. "Design-First" Workflow**
The most active debate centers on how AI tools like Cursor and Claude are changing the design-to-engineering loop.
*   One user shared their experience prototyping with Cursor, noting it saved them 10 hours a month and allowed them to build rapidly using natural language. However, this resulted in a monolithic "10k line index.html" file, creating a new problem: high technical debt and a difficult handoff process.
*   The community debated whether "figma-use" could solve this by syncing that code *back* into Figma to create a structured design system.
*   Some argued that syncing back is necessary to maintain a "source of truth," while others cautioned against letting LLMs write directly to component libraries, citing governance issues and "design drift." A proposed ideal stack involved `Figma Variables -> Token Studio -> Storybook -> MCP` to maintain order.

**CLI vs. MCP (Model Context Protocol)**
Users discussed the efficiency of the tool’s CLI approach compared to the emerging MCP standard.
*   **Token Efficiency:** Several commenters, including the author, noted that CLIs are more "token-efficient" for agents than MCP. MCP relies on verbose JSON-RPC schemas, whereas CLIs use concise text commands, which saves on context window usage and cost.
*   **Session Management:** Counter-arguments highlighted that MCP provides consistent session management, whereas CLI tools might struggle with state or security (e.g., handling secrets/passwords via environment variables vs. clear text).

**Feature Requests & Capabilities**
*   **CSS/Tailwind Integration:** Users asked if they could apply styles from an existing codebase (e.g., Tailwind classes) directly to Figma. The author (`dnnt`) explained that while the tool currently binds colors to Figma variables, parsing CSS/Tailwind configs automatically is a planned future feature.
*   **Vector Manipulation:** In response to a challenge to "make a pelican riding a bicycle" (a litmus test for vector complexity), the author pushed an update enabling vector path manipulation (commands like `path set`, `move`, `scale`, `flip`) and provided example commands to modify path data.
*   **Licensing:** There was a brief discussion on whether this bypasses Figma's licensing models. The author clarified the tool relies on the Plugin API, which interacts with the free component of Figma's ecosystem.

**Other Comparisons**
*   **PenPot:** Users asked about support for PenPot (an open-source Figma rival). The discussion briefly touched on it being "developer-friendly" but shifted back to Figma’s dominance.
*   **ASCII Art:** One user shared a similar projects that generates ASCII wireframes to allow for "zero friction" UI critiques.

### How scientists are using Claude to accelerate research and discovery

#### [Submission URL](https://www.anthropic.com/news/accelerating-scientific-research) | 119 points | by [gmays](https://news.ycombinator.com/user?id=gmays) | [72 comments](https://news.ycombinator.com/item?id=46664540)

Anthropic case study: Claude is moving from “chatbot” to lab coworker

- What’s new: Anthropic highlights how researchers are using Claude for end-to-end scientific work, not just literature review or coding. Since launching “Claude for Life Sciences,” the Opus 4.5 model reportedly improved at figure interpretation, computational biology, and protein benchmarks. An AI for Science program offers free API credits to high-impact projects.

- The big idea: Agentic AI that orchestrates many domain tools to plan experiments, clean/merge messy data, run analyses, and interpret results—compressing workflows that take months into hours while keeping experts in the loop.

- Standout example — Stanford’s Biomni:
  - A Claude-powered agent that routes plain‑English requests across hundreds of bio tools, packages, and databases spanning 25+ subfields.
  - GWAS pipeline ran in ~20 minutes versus months, handling cleaning, confounders, missingness, hit interpretation, and pathway context.
  - Validation case studies:
    - Designed a molecular cloning experiment whose blinded evaluation matched a postdoc with 5+ years’ experience.
    - Processed 450+ wearable data files (CGM, temp, activity) from 30 people in 35 minutes vs an estimated 3 weeks for a human expert.
    - Analyzed 336k single cells from human embryonic tissue, confirming known regulatory relationships and surfacing new candidate transcription factors.
  - Safety/quality: Guardrails detect when the agent drifts; labs can “teach” domain SOPs as skills. In rare-disease diagnosis work, encoding a clinician’s step‑by‑step method notably improved outcomes.

- Why it matters: If reliable, agentic AI that natively navigates the fragmented bioinformatics ecosystem could remove major bottlenecks (data wrangling, tool selection, protocol design) and enable new research strategies.

- Caveats and open questions:
  - External benchmarking and reproducibility beyond handpicked case studies.
  - Error modes, oversight burden, and how often guardrails trigger.
  - Data governance/privacy for sensitive biomedical data.
  - Cost, vendor lock‑in, and maintenance across fast‑evolving tools and databases.

- Also noted: The post teases additional, more specialized systems (e.g., automating interpretation of large-scale CRISPR knockout screens), but details weren’t included in the excerpt.

Bottom line: Labs are starting to package expert SOPs and a zoo of bio tools behind agentic interfaces powered by Claude. Early results suggest big productivity gains; the next hurdle is rigorous validation and trustworthy deployment at scale.

Here is a summary of the discussion on Hacker News:

**Reliability and Confidence Intervals**
A significant portion of the discussion focused on whether LLMs can genuinely provide "confidence levels" for their scientific findings. Skeptics argued that LLMs are fundamentally text generators that often "hallucinate" numbers, citing anecdotes of models failing basic tasks like reading street numbers accurately. Conversely, technical commenters argued that models can be calibrated to treat token probabilities as confidence values, comparing the architecture to how CNNs handle classification confidence in computer vision.

**Computational Prediction vs. "Wet Lab" Reality**
Users debated the practical utility of Claude in the biological sciences. While conceding that LLMs excel at finding patterns in massive datasets (such as genomics), critics cautioned that biology is notoriously difficult to model ("really damn hard"). One commenter noted that computational predictions for things like molecular binding often fail when tested physically (the "wet lab" phase), leading to skepticism about claims of massive productivity leaps without rigorous experimental validation.

**The Human Element**
The conversation touched on the comparison between AI and human error. When critics pointed out AI "hallucinations," others countered that human scientists are also prone to errors, bias, and producing low-quality research ("slop"). The consensus leaned toward viewing LLMs not as truth machines, but as tools to narrow down infinite search spaces and generate hypotheses that humans must then verify.

**Technical Nuances**
Smaller side discussions explored the mechanics of how Vision Transformers process images compared to LLMs, and the potential barriers (such as reproducibility and copyright) preventing models like Gemini from ingesting entire scientific corpora for synthesis.

### Erdos 281 solved with ChatGPT 5.2 Pro

#### [Submission URL](https://twitter.com/neelsomani/status/2012695714187325745) | 304 points | by [nl](https://news.ycombinator.com/user?id=nl) | [284 comments](https://news.ycombinator.com/item?id=46664631)

Got it—please share the Hacker News submission you want summarized.

To proceed, please provide:
- HN thread URL (news.ycombinator.com/item?id=…)
- Article link or the key text/excerpts
- Optionally: a few top comments you want included

Also tell me your preferences:
- Length: ultra-brief TL;DR, short (2–3 paragraphs), or fuller digest
- Tone: punchy/engaging or neutral
- Include extras? (discussion highlights, why it matters, quick stats like points/comments)

**Thread:** [LLM Moves Section 2 of Terence Tao’s Wiki](https://news.ycombinator.com/item?id=46601932)
**Article:** [Erdos Problems Forum](https://www.erdosproblems.com/forum/thread/281#post-33252)

Here is a summary of the discussion.

### The Digest
The submission details how an LLM helped resolve "Section 2" of Terence Tao's Erdos Problem #21 project. Crucially, the AI did not generate a *new* mathematical proof; rather, it successfully identified an existing, obscure solution in the literature that human experts had overlooked. The discussion on Hacker News immediately pivoted to a debate on **retrieval versus reasoning**. While users were impressed that the model found a needle in a haystack that even Tao’s colleagues missed, skepticism ran high regarding whether the model understood the math or simply regurgitated training data.

A fascinating technical theory emerged regarding **"mechanical obfuscation."** several commenters suggested that modern LLMs are fine-tuned to avoid outputting verbatim text (to side-step copyright liability). Consequently, when a model finds a direct answer, it might rewrite the text enough to look like a "novel derivation," masking the fact that it is simply quoting a source. The consensus leaned toward the idea that LLMs currently function best as "super-search engines," connecting dots across disparate literature that humans struggle to manually correlate.

### Discussion Highlights
*   **The "Shadow" Metaphor:** User `Workaccount2` offered a vivid analogy: the LLM doesn't have the book (the object), but it can project a hand-shadow that looks exactly like the object. It produces a derivative shape, not the original data.
*   **Copyright Conspiracies:** A significant sub-thread argued that "anti-overfitting" mechanisms in models like Gemini are actually legal shields, specifically designed to obfuscate sources so output cannot be flagged as copyright infringement.
*   **Math Formalization:** The thread touched on the difficulty of using AI to check human math. While some see a future where AI formalizes all literature to find errors, others (`mlpknbj`) argued that human math is often too informal and "vague but correct" for current AI to formalize without massive human intervention.
*   **Mod Note:** The thread got heated enough over the definition of "source" that a moderator (`dng`) had to step in to warn a user against using violent metaphors.

### Tired of AI, people are committing to the analog lifestyle in 2026

#### [Submission URL](https://www.cnn.com/2026/01/18/business/crafting-soars-ai-analog-wellness) | 83 points | by [andy99](https://news.ycombinator.com/user?id=andy99) | [50 comments](https://news.ycombinator.com/item?id=46671020)

HN Top Story: The “analog lifestyle” backlash to AI is gaining steam

- What’s happening: CNN Business reports a growing movement to “go analog” as homes fill with AI assistants and algorithmic feeds. It’s less a detox, more a long-term shift toward tangible, offline tasks and entertainment to reclaim attention, creativity, and privacy.

- By the numbers: Craft retailer Michaels says searches for “analog hobbies” are up 136% in six months; guided craft kit sales rose 86% in 2025 and are expected to grow another 30–40% this year; searches for yarn kits surged 1,200% in 2025. The chain is expanding shelf space for knitting.

- Why people are doing it: Fatigue with repetitive, low-effort “AI slop” and doomscrolling; desire to “cut the internet off from the information about me,” as UC Riverside’s Avriel Epps puts it. Mental health and post-pandemic coping are strong drivers.

- What it looks like: Landlines and “dumb phone” modes, tech-free craft and wine nights, screen-free Sundays, swapping Spotify for an old iPod, shooting film, even buying a physical alarm clock. It isn’t anti-tech—just selective tech.

- The catch: Going fully offline is hard. Even analog advocates rely on the internet to run small businesses or communities. The article’s author tried 48 hours “like it’s the ’90s” and found parts of the exercise inherently performative in a digital-media world.

Why it matters for HN: This signals demand for offline-first, privacy-preserving tools and simpler devices that decouple function from feeds. Expect more products and communities built around intentionality, low/no connectivity, and human-paced creation—plus a cultural counterweight to AI-generated content.

**The “Analog Lifestyle” Backlash to AI**

**What’s happening:** CNN Business reports a growing movement to “go analog” as homes fill with AI assistants and algorithmic feeds. It’s less a detox, more a long-term shift toward tangible, offline tasks and entertainment to reclaim attention, creativity, and privacy.

**By the numbers:** Craft retailer Michaels says searches for “analog hobbies” are up 136% in six months; guided craft kit sales rose 86% in 2025 and are expected to grow another 30–40% this year; searches for yarn kits surged 1,200% in 2025. The chain is expanding shelf space for knitting.

**Why people are doing it:** Fatigue with repetitive, low-effort “AI slop” and doomscrolling; desire to “cut the internet off from the information about me,” as UC Riverside’s Avriel Epps puts it. Mental health and post-pandemic coping are strong drivers.

**What it looks like:** Landlines and “dumb phone” modes, tech-free craft and wine nights, screen-free Sundays, swapping Spotify for an old iPod, shooting film, even buying a physical alarm clock. It isn’t anti-tech—just selective tech.

**The catch:** Going fully offline is hard. Even analog advocates rely on the internet to run small businesses or communities. The article’s author tried 48 hours “like it’s the ’90s” and found parts of the exercise inherently performative in a digital-media world.

**Why it matters for HN:** This signals demand for offline-first, privacy-preserving tools and simpler devices that decouple function from feeds. Expect more products and communities built around intentionality, low/no connectivity, and human-paced creation—plus a cultural counterweight to AI-generated content.

***

**Summary of Discussion:**

The discussion on Hacker News validates the article's premise while offering deeper critiques on the nature of "offline" trends and digital fatigue.

*   **Digital Exhaustion & "Enshittification":** The overarching sentiment is that AI is merely the "final straw" in a long line of user-hostile tech trends, including subscription fatigue, data breaches, and intrusive ads. Commenters frequently cited Cory Doctorow’s concept of "enshittification" and used the term "slop" to describe the current state of internet content. Users see the analog shift as a rejection of an internet that has been hollowed out by algorithmic engagement farming.
*   **The Superiority of Paper for Learning:** A significant thread debated the cognitive differences between physical books and screens (Kindles/iPads). Arguments favored paper for its "spatial awareness" and lack of context-switching distractions. Anecdotes highlighted a pushback against ed-tech in schools (citing examples from Denmark), with parents noting that restricted, screen-based learning often hampers retention and critical thinking compared to traditional note-taking.
*   **The "Performative" Value of Analog Kits:** Skeptics analyzed the statistics regarding the surge in "craft kits" (specifically knitting). They argued this trend is actually driven by *online* algorithms (TikTok trends and influencers), creating a paradox where people buy mass-produced kits to perform an "offline lifestyle" for an online audience. Some argued true offline hobbies don't require pre-packaged, viral products.
*   **Friction vs. Addiction:** The conversation touched on the design philosophy of modern tech, which aims to remove all friction. Commenters noted that "friction" (or inherent limits) is often necessary for self-control. The rise of "dumb phones" was compared to addiction recovery, with users debating whether willpower alone is enough to combat devices literally designed to hijack dopamine loops.
*   **Semantics & Workarounds:** There was a side discussion regarding the definition of "analog" (technically continuous signals vs. colloquially "non-digital"). Additionally, complaints about the CNN website's UI led to users suggesting technical workarounds, such as using `lite.cnn.com` or archive links to read the text without "bloat."

**Top Comment:** "Is AI the final straw? Social media exhaustion, ending accounts, wars, subscription stupidity, smart devices/appliances... Chatbots are the latest in a long line of digital rent-seeking/privacy-invading scams."

