## AI Submissions for Sat Jan 27 2024 {{ 'date': '2024-01-27T17:10:59.460Z' }}

### Implementing a ChatGPT-like LLM from scratch, step by step

#### [Submission URL](https://github.com/rasbt/LLMs-from-scratch) | 621 points | by [rasbt](https://news.ycombinator.com/user?id=rasbt) | [90 comments](https://news.ycombinator.com/item?id=39156778)

Today's top story on Hacker News is about implementing a ChatGPT-like Large Language Model (LLM) from scratch. The project, available on GitHub, provides step-by-step instructions on building your own LLM using Python and PyTorch. This resource from Manning Publications aims to enable developers to understand the inner workings of chat-oriented language models and enhance their natural language processing skills. With an impressive 2.9k stars and 144 forks on GitHub, it's clear that this project has generated significant interest in the developer community. So, if you're interested in learning more about LLMs and building your own ChatGPT-like model, this resource is definitely worth checking out!

The discussion on the Hacker News submission revolves around the topic of building large language models (LLMs) from scratch and the available resources for understanding and implementing them. Here are some key points from the discussion:

- A user mentions that they are currently writing a good book on the stages of completion of building an LLM and recommends a useful resource for anyone interested in LLMs.
- Another user appreciates the resource and mentions that it can be beneficial in demystifying LLMs and helping people build their own models.
- One user expresses their primary motivation behind helping people understand how LLMs work and building LLMs, emphasizing the importance of a hands-on approach to learning.
- The author of the submission thanks everyone for their support and mentions that they are glad to see people thinking about the inner workings of LLMs.
- A user asks if the specific section or chapter on "RAG" is included in the book and suggests the addition of a section on building one's own LLM from scratch.
- The author responds that the table of contents (TOC) is currently being developed and supplements the book with additional text.
- A user recommends a book on the principles of building modern computers and another resource on neural networks.
- The discussion delves into the challenges and complexities of implementing LLMs and the importance of understanding differentiable programming and programming languages like Python.
- Some resources and courses on AI and machine learning are recommended, including ones focused on transformers and LLMs.
- A user expresses their difficulty in comprehending working with AI and machine learning from scratch and mentions the risks involved in investing time and resources into it.
- The discussion concludes with a user highlighting the challenge of creating educational materials and how it requires significant work and expertise, while acknowledging that companies like OpenAI keep knowledge private to make money.

Overall, the discussion includes praise for available resources, requests for specific content in the book, and concerns about the challenges and risks of building AI models from scratch.

### Serious New Warning as Google AI Targets Billions of Private Messages

#### [Submission URL](https://www.forbes.com/sites/zakdoffman/2024/01/27/new-details-free-ai-upgrade-for-google-and-samsung-android-users-leaks/) | 79 points | by [cdme](https://news.ycombinator.com/user?id=cdme) | [41 comments](https://news.ycombinator.com/item?id=39159527)

Google has announced the introduction of an AI upgrade called Bard to its Android messaging app, which will read and analyze users' private messages. While the AI assistant will enhance messaging experiences and tailor responses based on users' mood and relationship dynamics, concerns about privacy have been raised. Google plans to process users' Messages requests in the cloud for training purposes, and this data will be stored for 18 months. However, Google has assured users that Bard's analysis will happen on their devices and that they will have control over what data is analyzed. Critics warn about the potential for data leaks, misuse, and hidden data sharing practices, and emphasize the importance of transparency and granular control over data analysis.

The discussion on this submission revolves around concerns about privacy and data security in light of Google's introduction of the AI upgrade, Bard, to its Android messaging app. 

Several commenters express skepticism and worry about the potential for misuse and data leaks. They emphasize the importance of transparency and granular control over data analysis to address these concerns. Some highlight the need for alternative operating systems like GrapheneOS to ensure better privacy and control over data.

Others discuss the possible advantages and disadvantages of AI analyzing private conversations, with some noting the potential benefits of personalized responses but cautioning against the loss of privacy.

There are also discussions about the role of regulation in protecting privacy and the influence of big business in shaping security practices. Some commenters argue that increased regulation is necessary to address these issues.

Overall, the commenters express a mix of concern and skepticism about the impact of AI analyzing private messages and emphasize the need for greater transparency and control over data analysis.

### A hacked Microsoft test account was assigned admin privileges

#### [Submission URL](https://arstechnica.com/security/2024/01/in-major-gaffe-hacked-microsoft-test-account-was-assigned-admin-privileges/) | 234 points | by [taimurkazmi](https://news.ycombinator.com/user?id=taimurkazmi) | [46 comments](https://news.ycombinator.com/item?id=39157888)

Microsoft's recent network breach, in which hackers gained access to top executives' emails for two months, was made possible due to a major mistake on the company's part. The hackers exploited an old test account with administrative privileges that was not protected by multifactor authentication. After gaining access to this account, the hackers used the OAuth authorization protocol to create a malicious app and give it access to every email address on Microsoft's Office 365 email service. This incident highlights the importance of implementing strong security measures, such as multifactor authentication, to prevent such breaches.

The discussion on this submission revolves around various topics related to security practices and the Microsoft network breach. 

One user mentions a similar issue with a non-production staging version of a website where administrator accounts were compromised, highlighting the importance of properly securing test accounts. Another user shares their experience working with small retail stores and the use of test environments for Stripe, mentioning that the implementation of security measures can sometimes be overlooked.

A discussion ensues about the existence of token fields and the vulnerability of certain systems. Some users express their concerns about the lack of thorough checks during the implementation process.

One user suggests establishing boundaries in large companies to restrict access to sensitive information. Others point out the importance of strong security policies, the need for accountability, and the enforcement of access rights. There is also mention of the need for proper documentation and communication regarding changes in access rights.

Another user suggests making configuration errors impossible to make, while another suggests implementing multiple approvals and verification processes for configuration changes.

The topic of cybersecurity as an industry and the neglect of security practices by companies is also brought up. Some users express frustration with the lack of attention given to security measures and the perception that they are often ignored.

There is criticism towards Microsoft's handling of security, with some users pointing out their long track record of incompetence and their emphasis on features over security. One user shares their experience with passwords being stored in plain text, emphasizing the need for better security practices.

Overall, the discussion highlights the importance of strong security measures, proper implementation of access rights, and the need for companies to prioritize security over other considerations. There are also criticisms of Microsoft's security practices.

### Vectorizing Unicode conversions on real RISC-V hardware

#### [Submission URL](https://camel-cdr.github.io/rvv-bench-results/articles/vector-utf.html) | 71 points | by [camel-cdr](https://news.ycombinator.com/user?id=camel-cdr) | [5 comments](https://news.ycombinator.com/item?id=39157061)

In this article, the author explores how to achieve a significant speedup for converting UTF-8 to UTF-16 using the RISC-V Vector extension. They focus on developing an optimized RISC-V implementation that can be integrated into the simdutf library, which is used by Node.js and Bun. The RISC-V Vector extension adds 32 vector registers that can operate on multiple elements at a time, potentially providing a large speed boost compared to scalar code. Although hardware supporting RVV is currently limited, the author has access to the Kendryte K230 and a Milk-V Pioneer server with 64 C920 cores for development and testing. The author provides code examples and explains the basics of RVV and Unicode for those unfamiliar with the topics.

The comments on Hacker News provide some additional insights and perspectives on the article.

- User "cml-cdr" mentions that the simdutf project has been working on vectorized implementations for a couple of years for various architectures.
- User "0x000xca0xfe" finds it fascinating that RISC-V binary runways could become widely compatible with different CPU implementations. They also mention the potential for new developments and growth, such as stable micro-architecture-agnostic ABIs and Just-in-Time compilers.
- User "rnx" notes that a stable runtime is an important part of any implementation, and as far as they know, implementing a CPU simulator for the RISC-V ISA is not complicated, except for exceptions like OpenPower being based on a different CPU ISA.
- User "shsh" adds that precompiled binaries might work well as long as they don't rely on I/O, memory mapping, or specific device implementation. JIT can be targeted to the device ISA.

Overall, the discussion touches on topics such as ongoing work on simdutf, the potential for compatibility and growth in the RISC-V ecosystem, and the importance of a stable runtime for successful implementations.

### LoMA: Lossless Compressed Memory Attention

#### [Submission URL](https://arxiv.org/abs/2401.09486) | 92 points | by [PaulHoule](https://news.ycombinator.com/user?id=PaulHoule) | [8 comments](https://news.ycombinator.com/item?id=39157735)

Researchers Yumeng Wang and Zhenyang Xiao have published a paper titled "LoMA: Lossless Compressed Memory Attention" that addresses the challenge of handling long texts in Large Language Models (LLMs) while reducing resource consumption. The paper introduces a new method called Lossless Compressed Memory Attention (LoMA), which allows for lossless compression of information in memory token key-value (KV) pairs. The authors conducted experiments that demonstrated the efficiency and effectiveness of LoMA. By achieving remarkable results, this method could have significant implications for improving the performance of LLMs.

The discussion about the paper "LoMA: Lossless Compressed Memory Attention" on Hacker News revolves around various aspects of the paper and its implications. 

One commenter points out that the paper is well-written and highlights specific sections in the paper that they found interesting. They also ask about certain terminology and figures in the paper. In response, another commenter suggests that many machine learning research papers do not delve into unnecessary details and that attention to detail in a paper like this matters. They mention that it is important to review papers properly, including paying attention to math and density functions.

Another commenter expresses their skepticism about the novelty and claims of the paper, stating that they believe the work is not truly innovative and that the reviewer should have rejected it. They elaborate on their reasoning, mentioning the lack of substantiated claims and explanations in the paper. They also mention that it can be difficult to make certain types of fixes to a method interesting, which may have affected the paper's acceptance.

In response to a question about the necessity of lossless compression in reading memory, a commenter argues that the paper's title is confusing and mentions observed results that contradict the paper's claims.

The discussion also touches on the review process for papers in the academic community. A comment suggests that the interest in a paper is subjective and varies depending on where it is published and where it is reviewed (e.g., HN, Twitter, Reddit, or a conference).

### Using AI to teach how to code, remember you still need to think for yourself

#### [Submission URL](https://www.theregister.com/2024/01/27/ai_coding_automatic/) | 103 points | by [rntn](https://news.ycombinator.com/user?id=rntn) | [66 comments](https://news.ycombinator.com/item?id=39158468)

Learning how to code has become easier with the help of AI tools that suggest or generate source code. However, it is important to use these tools wisely and not rely on them entirely. While AI assistants like ChatGPT can generate simple code and provide solutions, they can also hinder the learning process by making it too easy to jump to the right answer. To address this, computer science teachers at Harvard University introduced a virtual rubber duck, a coding chat-bot powered by GPT-4. This bot helps students debug their code by engaging in a conversation, simulating the process of talking through a problem with an inanimate object. The CS50 duck debugger has been well received, allowing students to seek help at any time without the risk of cheating. Teachers also benefit from the tool, as it frees up more time to assist students in other areas. While AI models like the CS50 duck can be useful, they are not always accurate and can make mistakes. It is important for programmers to have a solid understanding of code so they can identify errors in machine-generated code and not rely solely on AI assistants. The goal is to use AI tools as a supplemental resource while also developing critical thinking and the ability to review and improve code independently.

The discussion on this submission started with users sharing their experiences and thoughts on AI-generated code. One user mentioned that they have found AI tools like ChatGPT and Grimoire to be very helpful in generating code, while another user expressed disappointment in the performance of AI assistants. The conversation then shifted to the topic of copyright and intellectual property related to AI-generated content, with users discussing the legality of using AI-generated images and the Copyright Office's stance on copyrighting AI-generated works.

There was also a discussion about the importance of distinguishing between AI-generated and human-generated content and the need for critical thinking and human judgment in AI-generated code. Users shared their opinions on the capabilities and limitations of AI models like Midjourney V6 and DALL-E.

The conversation touched on various related topics such as the use of AI to improve text generation, the complexities of AI-generated images, the use of AI in productivity tools, and the challenges faced by professionals working with AI.

Some users discussed the potential benefits of AI tools in coding education, while others highlighted the importance of not relying on AI entirely and the value of understanding code independently. The topic of AI's role in problem-solving and its limits was also brought up.

The discussion wrapped up with users talking about the availability of AI tools for students and the potential advantages and drawbacks they may bring to the learning process. Some users emphasized the role of human instructors in providing explanations and documentation to complement AI tools, while others highlighted the potential of AI in improving code comprehension and understanding.

### U.S. will soon stop Chinese companies from using American clouds for AI training

#### [Submission URL](https://www.tomshardware.com/tech-industry/artificial-intelligence/us-takes-the-china-chip-war-to-the-next-level) | 49 points | by [geox](https://news.ycombinator.com/user?id=geox) | [31 comments](https://news.ycombinator.com/item?id=39158747)

The U.S. government is proposing a new regulation that would prevent foreign entities, particularly those from China, from using U.S. cloud computing for AI model training. This initiative aims to protect national security and maintain U.S. technological superiority. The proposal, called 'Know Your Customer,' would require U.S. cloud companies to rigorously identify their foreign users. The regulation aims to prevent countries like China from accessing U.S. cloud resources for AI development. It is seen as a way to close potential avenues for malicious activities using U.S. technology on American soil. The proposal comes as part of a larger strategy to ensure that U.S. cloud platforms are not exploited for potentially hostile AI development. However, these measures have faced criticism, with some arguing they could deter international collaboration in AI.

The discussion on this submission revolves around the proposed U.S. government regulation to prevent foreign entities from using U.S. cloud computing for AI model training. Some users argue that slower chip development elsewhere could lead to an exponential delay in technological progress. Others point out that the Soviet Union had a large technological gap compared to the Western world during the Cold War. There is a debate about China's nuclear weapons capabilities and its intention to conquer Taiwan. Some users discuss the historical competition between the Soviet Union and the Western world in scientific computing. There are also discussions on restricting food exports, promoting technological innovation, and the need for stricter identification of foreign entities accessing American Clouds. Some users express concerns about IP infringement and the potential for attacks on U.S. infrastructure.

### Microsoft CEO calls for tech industry to 'act' after AI photos of Taylor Swift

#### [Submission URL](https://www.themirror.com/entertainment/celebrity-news/microsoft-ceo-calls-tech-industry-308830) | 9 points | by [taimurkazmi](https://news.ycombinator.com/user?id=taimurkazmi) | [4 comments](https://news.ycombinator.com/item?id=39157858)

Microsoft CEO Satya Nadella has called on the tech industry to take action after AI-generated pornographic images of Taylor Swift circulated online. In an interview with Lester Holt on NBC News, Nadella expressed concern and urged the industry to create a safe online environment for both content creators and consumers. He emphasized the need for collaboration between law enforcement and tech platforms to address the issue. While Nadella called for "guardrails," others advocate for stricter content moderation and legislation. SAG-AFTRA, the union Taylor Swift is a member of, issued a statement supporting her and calling for the regulation of fake explicit images. The White House also vowed to address the issue of lax online enforcement, particularly in relation to the harassment of women and young girls. It is unclear whether Swift will pursue legal action in response to the AI-generated images.

The discussion around the submission on Hacker News seems to have focused on various aspects of the issue. One user, ENGNR, seemed surprised by the emergence of the AI-generated images of Taylor Swift. Another user, MS Drone, expressed concern about the potential harm caused by such images. Ekaros proposed implementing software prevention measures to restrict the spread of manipulated videos and suggested that effective regulations could help in addressing the problem. Another user, cynydz, made a cryptic comment that is difficult to interpret. Lastly, MaxikCZ suggested that making the creation and distribution of such manipulated images illegal could be a possible solution.

