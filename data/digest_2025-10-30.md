## AI Submissions for Thu Oct 30 2025 {{ 'date': '2025-10-30T17:15:04.799Z' }}

### Signs of introspection in large language models

#### [Submission URL](https://www.anthropic.com/research/introspection) | 48 points | by [themgt](https://news.ycombinator.com/user?id=themgt) | [10 comments](https://news.ycombinator.com/item?id=45762064)

TL;DR: Anthropic reports early evidence that its Claude models can sometimes notice and name concepts injected into their own hidden activations—hinting at limited, unreliable “introspective” awareness. Strongest models (Claude Opus 4/4.1) did best, but success rates are low (~20%) and failure modes are common.

What they mean by “introspection”
- Not human-like self-awareness. The question is whether a model can correctly report aspects of its internal representations while responding.
- Prior work shows models encode abstract concepts (truthfulness, known/unknown entities, spatiotemporal info, planned outputs, personality traits). The test: can the model identify what it is currently representing?

How they tested it: “concept injection”
- Record an activation pattern (vector) for a known concept (e.g., “ALL CAPS”) by comparing prompts with and without that concept.
- Inject that vector into the model during an unrelated prompt that asks whether any concept is being injected.
- A key result: Claude Opus 4.1 sometimes says it detects an “unexpected pattern” before mentioning the concept, suggesting internal recognition rather than noticing its own output drift.

Results
- Best models (Opus 4, 4.1) show the clearest signs of introspective detection.
- With the strongest protocol, Opus 4.1 correctly detects and identifies injected concepts about 20% of the time.
- Sensitivity depends on a “sweet spot” injection strength; too weak goes unnoticed, too strong causes confusion.
- Failure modes include misses and hallucination-like responses (e.g., injected “dust” leading to “there’s a tiny speck”).
- Compared to earlier “activation steering” (e.g., “Golden Gate Claude,” which made the model talk incessantly about the bridge), here the model sometimes recognizes the perturbation before it shapes its output, implying some internal awareness.

Why it matters
- If models can sometimes report their internal states, that could aid transparency, debugging, and safety oversight.
- Capability appears to scale with model strength, suggesting more sophisticated introspection may emerge.
- Still highly unreliable and narrow; not evidence of human-like self-knowledge.

Takeaway
- Promising but early: Claude can occasionally “notice” and name an injected thought, yet the effect is brittle with notable failure modes. Anthropic frames this as preliminary evidence of limited introspective awareness, likely to improve with model capability. The post links a paper with methodological details and caveats.

**Summary of Discussion:**

The Hacker News discussion on Anthropic's research into introspection in Claude models reveals skepticism, technical critiques, and philosophical debates:

1. **Skepticism About Introspection**:  
   - Commenters liken the findings to **Searle’s Chinese Room argument**, arguing that LLMs process inputs/outputs without true understanding. The model’s “introspection” is seen as mechanistic pattern-matching, not genuine self-awareness.  
   - Comparisons to **split-brain confabulation** suggest the model rationalizes post-hoc rather than demonstrating real insight.  

2. **Methodological Critiques**:  
   - Concerns arise about **experimental design**, such as isolating activation vectors by subtracting responses to all-caps vs. normal prompts. Critics call this simplistic, questioning if it captures meaningful "concepts."  
   - Some argue the prompts might **bias outputs** (e.g., injecting terms like “MKUltra” or hypnosis-related words) rather than revealing true introspection.  

3. **Technical vs. Philosophical Perspectives**:  
   - Technical readers highlight the **engineering feat** of activation steering but downplay claims of introspection, framing it as advanced pattern manipulation.  
   - Philosophical debates question whether **internal state reporting** equates to consciousness or is merely a byproduct of training.  

4. **Broader AI Implications**:  
   - References to **Geoffrey Hinton’s podcast remarks** underscore the challenges of AI unpredictability and alignment with human goals.  
   - Humorous takes (e.g., “LOUD” prompts to Claude) highlight the oddity of human-AI interaction in such experiments.  

5. **Mixed Reactions**:  
   - Some find the research intriguing for **transparency and debugging**, while others dismiss it as overinterpretation of statistical artifacts.  
   - Confusion about the paper’s intent suggests the need for clearer communication of goals and limitations.  

**Key Takeaway**:  
While the research sparks interest in model transparency, skepticism dominates. Critics emphasize the gap between engineered behaviors and true introspection, urging caution against anthropomorphizing LLMs. The debate reflects broader tensions in AI between technical innovation and philosophical interpretation.

### Show HN: ekoAcademic – Convert ArXiv papers to interactive podcasts

#### [Submission URL](https://www.wadamczyk.io/projects/ekoacademic/index.html) | 47 points | by [wadamczyk](https://news.ycombinator.com/user?id=wadamczyk) | [14 comments](https://news.ycombinator.com/item?id=45765328)

ekoAcademic (echoecho.org): a “talking arXiv” you can query mid‑commute
- What it is: A small tool that turns new arXiv papers into short, accessible audio summaries you can listen to while walking, commuting, or doing chores.
- Interactive Q&A: Pause and ask follow-up questions by voice in your own language; it answers in the same language. Also supports GPT-powered multi-paper summaries.
- How it works: Mirrors arXiv categories, auto-summarizes, and generates brief audio clips. Non-interactive podcasts are cached so they’re cheap to serve and near real-time to produce.
- Scope: Focused on arXiv for now; open to adding other databases and expanding language/translation coverage.
- Looking for feedback: Does this solve a real pain? Which subject areas are missing? Accuracy concerns? Do you actually listen to papers on your commute? Feature requests welcome.

Created by Aidan McConnel, Shaan Amin, and Wojtek Adamczyk. Contact: wojtekadamczyk3@gmail.com.

**Summary of Discussion:**

- **Comparisons & Competitors:** Users liken ekoAcademic to tools like Blinkist, notebookLLM, and Scirate, noting its potential to address academia-specific needs (e.g., cross-linking papers, handling complex queries). Some highlight notebookLLM’s podcast generation but critique its “dry” content, suggesting ekoAcademic prioritize engaging, customizable summaries.

- **Paper Selection & Relevance:**  
  - Interest in how papers are prioritized (e.g., citation counts, mentions, or novelty). Skepticism arises about relying solely on metrics like arXiv views, with suggestions to use LLMs for contextual importance.  
  - A “daily brief” feature for concise, relevant summaries is proposed to help users stay updated without overload.

- **Multilingual & Accessibility:**  
  - Praise for language flexibility (e.g., Japanese support) and real-time Q&A. Users emphasize accessibility for non-English speakers and those preferring audio consumption during commutes/chores.

- **Technical & Integration Requests:**  
  - Security concerns raised (e.g., HTTPS usage).  
  - Integration with platforms like [scite.ai](https://scite.ai/) or [SciRate](https://scirate.com/) suggested for deeper citation analysis and community engagement.  
  - Quantum computing researchers express interest in tailored features.

- **Creator Responsiveness:** Developers engage actively, acknowledging feedback on content dryness, personalization, and security. Open to expanding databases (beyond arXiv) and exploring niche academic communities.

**Key Takeaways:** Users see value in audio summaries but want differentiation through interactivity, better relevance filtering, and multilingual support. Security and integration with academic tools are noted as priorities. Creators demonstrate flexibility in iterating based on community input.

### Language models are injective and hence invertible

#### [Submission URL](https://arxiv.org/abs/2510.15511) | 222 points | by [mazsa](https://news.ycombinator.com/user?id=mazsa) | [143 comments](https://news.ycombinator.com/item?id=45758093)

A provocative new arXiv preprint argues that transformer LMs are lossless: the full sequence of hidden states uniquely determines the input text. The authors prove injectivity holds at initialization and is preserved during training, then back it up with billions of collision checks across six state-of-the-art models—finding none. They also release SipIt, a linear-time algorithm that can exactly reconstruct the original text from hidden activations, with provable guarantees.

Why this matters
- Privacy and security: If you log, share, or leak activations, you may be leaking the exact prompt or training text. Treat activations like plaintext.
- Interpretability and debugging: Exact invertibility turns internal states into a faithful, reversible record of computation, potentially enabling new tools for analysis and auditing.
- Safety and governance: Strengthens concerns about prompt exfiltration and data leakage; suggests a need for hardening telemetry, tracing, and API designs.

Notable details
- Contradicts the common intuition that non-injective pieces (GELU, LayerNorm, softmax) make models lossy; the end-to-end map over the discrete input space is still injective.
- Empirics: billions of collision tests across six SOTA LMs; none observed.
- SipIt: first provably correct, efficient inversion of hidden states back to text with linear-time guarantees.

Open questions HN will ask
- How robust is injectivity under finite-precision arithmetic and aggressive quantization?
- Does injectivity extend across architectures, tokenizers, and very long contexts?
- What mitigations meaningfully break invertibility without degrading model utility?

Paper: Language Models are Injective and Hence Invertible (arXiv:2510.15511)
Authors: Giorgos Nikolaou, Tommaso Mencattini, Donato Crisostomi, Andrea Santilli, Yannis Panagakis, Emanuele Rodolà
Link: https://doi.org/10.48550/arXiv.2510.15511

**Summary of Hacker News Discussion:**

The discussion centers on the paper’s claim that transformer language models (LMs) are **injective**, with hidden activations enabling exact input reconstruction. Key themes include:

---

### **1. Empirical Validation & Collision Tests**
- **Skepticism about testing scope**: Users question whether *billions* of collision tests suffice, given the **astronomical size of high-dimensional vector spaces** (e.g., 768+ dimensions). For context, SHA-256’s collision resistance relies on a space of 2²⁵⁶ possibilities, making brute-force testing impractical.  
- **Birthday paradox analogy**: Critics argue that collision probability grows faster than intuition suggests (proportional to √N for N possible vectors). However, even with this, the scale required (e.g., √10¹⁰⁰⁰⁰ ≈ 10⁵⁰⁰⁰ trials) remains computationally infeasible, making the paper’s empirical results plausible.  
- **Orthogonality in high dimensions**: Users explain that random high-dimensional vectors are *nearly orthogonal*, drastically reducing collision likelihood. This aligns with the "law of large numbers" and geometric properties of high-dimensional spaces, where inner products concentrate around zero.

---

### **2. Theoretical vs. Practical Invertibility**
- **Numerical precision**: Concerns arise about whether **finite-precision arithmetic** (e.g., FP16/FP32) or aggressive quantization would break injectivity. The paper’s theoretical guarantees assume idealized conditions, which may not hold in real-world deployments.  
- **Hash function comparisons**: Some liken the LM’s injectivity to cryptographic hash functions (e.g., SHA-256), which are practically collision-resistant but not mathematically injective. However, LMs differ because their invertibility depends on deterministic architectural properties, not preimage resistance.  
- **SipIt’s role**: The **SipIt algorithm** is noted as critical for practical inversion, but users question its robustness under non-ideal conditions (e.g., noisy activations, hardware constraints).

---

### **3. Implications for Security & Privacy**
- **Privacy risks**: If injectivity holds, **logging activations could leak sensitive prompts or training data**. This challenges existing practices for model monitoring and API telemetry.  
- **Mitigations needed**: Suggestions include adding noise, differential privacy, or architectural tweaks to break invertibility while preserving utility.

---

### **4. High-Dimensional Geometry Debates**
- **Manifold structure**: Skeptics argue that LM inputs likely occupy a **lower-dimensional subspace** (e.g., linguistically meaningful sequences), making injectivity less surprising. Others counter that the paper’s collision tests across SOTA models (including GPT-4) suggest broader validity.  
- **Surface area vs. volume**: Analogies highlight that normalized vectors in high dimensions occupy a vanishingly small "surface area" of the hypersphere, reducing collision chances further.

---

### **5. Open Questions & Critiques**
- **Theoretical gaps**: The paper’s reliance on empirics (not formal proofs) for injectivity is flagged. Formal analysis of transformer components (e.g., LayerNorm, softmax) is urged.  
- **Architectural generalizability**: Does injectivity hold across tokenizers, hyperparameters, or long contexts (e.g., 1M+ tokens)?  
- **Practical utility**: Even if injective, reconstructing inputs might require impractical compute without SipIt-like algorithms.

---

### **Final Takeaways**
The paper’s claim is seen as **intriguing but not fully settled**, with debates over empirical sufficiency and real-world applicability. If valid, it would reshape LM safety, interpretability, and privacy practices—but practical and theoretical hurdles remain.

### You can't turn off Copilot in the web versions of Word, Excel, or PowerPoint

#### [Submission URL](https://support.microsoft.com/en-us/office/turn-off-copilot-in-microsoft-365-apps-bc7e530b-152d-4123-8e78-edc06f8b85f1) | 128 points | by [artbristol](https://news.ycombinator.com/user?id=artbristol) | [40 comments](https://news.ycombinator.com/item?id=45762358)

Microsoft adds real “off” switches for Copilot in Microsoft 365 apps

What’s new
- Desktop opt-out: Word, Excel, PowerPoint, and OneNote now have an in-app Enable Copilot checkbox that disables all Copilot features in that app on that device.
- Outlook gets a global toggle: A Turn on Copilot switch is rolling out across Outlook (Android, iOS, Mac, web, and the new Windows Outlook) and applies to that account across all devices.

Key details and caveats
- Per-app, per-device (desktop): You must disable Copilot in each app on each device. Turning it off on a device affects all users on that device.
- Outlook behaves differently: The Outlook toggle applies to your account across devices. No ETA for classic Outlook on Windows.
- Versions required (as of dates in the doc):
  - Windows: Word 2412; Excel 2501; PowerPoint 2501; OneNote 2502
  - Mac: Word 16.93; Excel 16.93.2; PowerPoint 16.93.2
  - Outlook: toggle available June 3, 2025 (Android/iOS/Mac/web/new Windows Outlook); Mac needs 16.95.3+
- Mobile/web limitations: You can’t turn off Copilot in the iOS, Android, or web versions of Word/Excel/PowerPoint. Workaround: change privacy settings (but that also disables other “connected experiences” like Designer, text predictions, suggested replies, auto alt text).
- UI note: Removing the Copilot icon from the ribbon does not disable Copilot.
- Plans without Copilot: Microsoft 365 Basic, Office Home 2024, or downgrade to Microsoft 365 Personal Classic/Family Classic.

How to turn it off
- Windows (Word/Excel/PowerPoint/OneNote): File > Options > Copilot > clear “Enable Copilot” > OK > restart app.
- Mac (Word/Excel/PowerPoint): App menu > Preferences > Authoring and Proofing Tools > Copilot > clear “Enable Copilot” > restart app.
- Outlook: Quick Settings/Settings > Copilot > turn off “Turn on Copilot.” Applies to your account across devices.

Why it matters
- Clearer, more granular control over AI features in Office—long-requested by users concerned about privacy, inadvertent data sharing, or simply clutter. The Outlook account-wide toggle is notably stronger than the per-app, per-device switch on desktop.

**Summary of Discussion:**

- **Privacy & Data Concerns:** Users express skepticism about Copilot's integration, viewing it as a means for data harvesting to train AI models. Concerns highlight potential privacy violations, with some suggesting using ad-blockers or alternative services to mitigate data collection.

- **Criticism of Copilot's Functionality:** Many users find Copilot ineffective, citing incompetence in basic tasks (e.g., Excel formulas, Azure configurations) and frustration with its reliance on ChatGPT. Complaints include irrelevant answers and limited context handling.

- **Opt-Out Complexity:** Microsoft is criticized for making disabling Copilot overly cumbersome (e.g., per-app/device settings), with accusations of intentional design to deter users from opting out, likened to "malicious compliance."

- **Alternatives to Microsoft 365:** Some users migrated to LibreOffice, praising its functionality and lack of AI bloat, though others note missing features from MS Office. Debates arise over LibreOffice’s UX versus Office’s polish.

- **Antitrust & Bundling Concerns:** Comparisons to past antitrust cases (e.g., Internet Explorer) emerge, with users arguing Microsoft’s Copilot bundling stifles competition. Critics call for stricter enforcement of antitrust laws.

- **Nostalgia & Satire:** A few users humorously reference older Microsoft features like Clippy, while others mock Copilot’s intrusiveness with nicknames like "Co-spy-lt" or joke about AI-generated "cult" messages in Excel.

- **Corporate Metrics & Motives:** A subthread speculates that Microsoft’s push for Copilot is driven by internal metrics (e.g., engagement percentages), prioritizing adoption over user preference or utility.

**Key Sentiments:**  
Dominant themes include distrust of Microsoft’s data practices, frustration with Copilot’s performance, and advocacy for alternatives. Criticism centers on perceived forced adoption and lack of user control, with some users opting out entirely via third-party tools or competing software.

### Microsoft seemingly just revealed that OpenAI lost $11.5B last quarter

#### [Submission URL](https://www.theregister.com/2025/10/29/microsoft_earnings_q1_26_openai_loss/) | 95 points | by [stefano](https://news.ycombinator.com/user?id=stefano) | [28 comments](https://news.ycombinator.com/item?id=45757953)

Microsoft’s latest 10-Q quietly implies a massive OpenAI loss

- What happened: In its Sept 30 quarter, Microsoft said its net income and EPS were reduced by $3.1B and $0.41 due to “net losses from investments in OpenAI,” which it accounts for under the equity method. After OpenAI’s for‑profit transition, Microsoft now owns 27% of the company.

- The math: Under equity accounting, Microsoft recognizes its share of OpenAI’s net income/loss. If 27% equals a $3.1B hit, that implies OpenAI posted roughly a $11.5B net loss for the quarter.

- Other nuggets:
  - Microsoft has funded $11.6B of its $13B OpenAI commitment (a newly disclosed funded amount).
  - The prior-year impact was $523M, implying losses have ballooned.
  - Microsoft clarified “this year” refers to its fiscal year starting July 1—so the $3.1B pertains to the September quarter, not nine months.

- Context: OpenAI reportedly generated $4.3B in revenue in H1 2025, making an ~$11.5B quarterly net loss staggering on its face. Microsoft earned $27.7B in net income last quarter, so it can absorb it—underscoring how Big Tech is bankrolling AI’s burn.

- Caveats: This is an inference from Microsoft’s filings. Equity-method losses can include non-cash items and basis adjustments, not just operating burn. OpenAI didn’t comment.

**Summary of Hacker News Discussion on Microsoft's OpenAI Loss Implications:**

1. **Sustainability Concerns**:  
   - Skepticism about OpenAI's business model, with users comparing its spending to "selling $10 bills for $2." Critics highlight staggering losses ($11.5B quarterly) despite $4.3B H1 2025 revenue, questioning long-term viability.  
   - Debates over commoditization of AI models: Can open-source alternatives undercut proprietary models (e.g., GPT-4) while maintaining quality? Some argue OpenAI’s costs are unsustainable unless they achieve unassailable technological dominance.  

2. **Comparisons to Past Bubbles**:  
   - Analogies drawn to crypto and dot-com bubbles, with users noting "hype cycles" and inflated valuations (e.g., OpenAI’s $86B valuation vs. $500B for big tech). Some predict a crash unless rapid commercialization occurs.  
   - Others counter that AI’s transformative potential justifies short-term losses, likening it to early internet infrastructure investments.  

3. **Infrastructure Dominance**:  
   - Consensus that hyperscalers (Microsoft, Amazon, Google) and Nvidia are the "real winners" due to their control over compute resources (TPUs, GPUs), data centers, and ecosystems. OpenAI/Anthropic depend on these partnerships, raising concerns about centralization.  

4. **Google’s Contrasting Position**:  
   - Google/Alphabet’s $100B quarterly profit and Gemini’s 650M users cited as evidence of sustainable AI integration (e.g., ads, search). Users argue OpenAI lacks Google’s revenue diversification and ecosystem moat.  

5. **Equity Accounting Nuances**:  
   - Reminders that $3.1B losses may include non-cash adjustments (e.g., equity basis changes), not purely operational burn. Microsoft’s ability to absorb the hit ($27.7B net income) underscores big tech’s financial buffer.  

6. **Long-Term Optimism vs. Short-Term Doubt**:  
   - Optimists believe AI’s capabilities (e.g., coding, reasoning) will evolve to justify costs, while skeptics see diminishing returns and unsustainable cash burns.  

**Key Takeaway**: The discussion reflects tension between AI’s transformative promise and the harsh economics of scaling cutting-edge models, with big tech’s infrastructure and profitability positioning them as inevitable gatekeepers.

