## AI Submissions for Fri Sep 05 2025 {{ 'date': '2025-09-05T17:14:13.061Z' }}

### Tesla changes meaning of 'Full Self-Driving', gives up on promise of autonomy

#### [Submission URL](https://electrek.co/2025/09/05/tesla-changes-meaning-full-self-driving-give-up-promise-autonomy/) | 323 points | by [MilnerRoute](https://news.ycombinator.com/user?id=MilnerRoute) | [398 comments](https://news.ycombinator.com/item?id=45144900)

Tesla narrows “Full Self-Driving” to supervised ADAS, rewrites FSD metric in Musk mega-pay plan

- What changed: Tesla now sells “Full Self-Driving (Supervised)” with fine print stating it does not make the vehicle autonomous and doesn’t promise it will. This departs from years of promises that FSD would enable unsupervised autonomy via software updates.

- Legacy owners: Tesla has acknowledged vehicles built from 2016–2023 lack the hardware for unsupervised self-driving; despite talk of computer upgrades, there’s no concrete retrofit plan.

- Compensation link: A new CEO pay package reportedly worth up to $1T ties a milestone to “10 million active FSD subscriptions.” In the filing, “FSD” is redefined broadly as an advanced driving system capable of “autonomous or similar functionality under specified conditions”—a definition today’s supervised system could satisfy.

- Pricing trend: After years of saying the price would rise as autonomy neared, Tesla has cut FSD prices by about $7,000 from 2023 highs, coinciding with softer sales.

- Why it matters: Electrek argues Tesla’s legal language diverges from its marketing, enabling it to meet subscription targets without delivering unsupervised autonomy—raising false-advertising concerns. Critics also warn Tesla could nudge buyers toward FSD (e.g., by lowering price or de-emphasizing base Autopilot) to hit the metric.

- Community reaction: Top comments frame the move as shifting goalposts—“technically correct” but far from what many early buyers believed they were getting.

The Hacker News discussion on Tesla's revised Full Self-Driving (FSD) strategy and hardware choices revolves around several key themes:

### 1. **Critique of Tesla's Vision-Only Approach**
   - **Skepticism about reliability**: Users shared personal experiences with phantom braking, erratic lane swerving, and "hallucinations" where Tesla’s FSD misinterprets road conditions (e.g., mistaking sunlight glare for obstacles). Critics liken these errors to AI hallucinations, arguing that vision-only systems struggle with edge cases like poor weather, glare, or dirty cameras.  
   - **LiDAR advocacy**: Many argue that Tesla’s rejection of LiDAR is short-sighted, as LiDAR provides critical 3D spatial data that complements vision. Critics suggest Tesla prioritizes cost savings over safety, while others note LiDAR prices are dropping (e.g., BYD offers LiDAR-equipped cars at $140k, with components approaching $1k).

### 2. **Sensor Fusion vs. Simplicity**
   - **Redundancy concerns**: Users compared Tesla’s single-sensor strategy to the Boeing 737 MAX’s flawed reliance on a single sensor, emphasizing the need for multi-sensor fusion (LiDAR + vision) to enhance safety and detect sensor failures.  
   - **Engineering challenges**: Some acknowledge the complexity of sensor fusion (calibration, synchronization) but argue Tesla’s vision-only approach is a pragmatic cost-timeline tradeoff. Others counter that Tesla’s promises (e.g., autonomy by deadlines) remain unfulfilled, undermining its engineering rationale.

### 3. **FSD Performance and Hardware Updates**
   - **Mixed user experiences**: While some Bay Area users report improved FSD performance (e.g., reduced phantom braking with HW4), others remain skeptical of Tesla’s ability to achieve unsupervised autonomy. Critics highlight that even HW4 still struggles with basic scenarios, questioning Elon Musk’s aggressive timelines.  
   - **Comparison to Waymo**: Waymo’s high-definition mapping and LiDAR-based approach are praised for reliability in geofenced areas, though deemed less scalable. Tesla’s "general self-driving" ambition is seen as riskier but potentially revolutionary if solved.

### 4. **Cost and Strategic Criticism**
   - **Missed opportunities**: Users criticize Tesla for not adopting LiDAR as costs fell, suggesting they could have pivoted years ago. Some argue Tesla’s focus on vision is now a liability, with competitors leveraging cheaper LiDAR for faster progress.  
   - **Business and leadership concerns**: Critics tie Tesla’s safety issues to Musk’s leadership style, citing his dismissiveness of regulation and controversial public statements. Others defend Tesla’s engineering but concede its marketing overhypes capabilities.

### 5. **Broader AI/LLM Parallels**
   - **Hallucination analogies**: FSD’s flaws are compared to LLM errors, with users stressing that neither should be fully trusted without scrutiny. Debates emerge about whether developers overhype AI’s infallibility, though some push back against strawman arguments.

### Key Takeaways:
- **Skepticism dominates**: Most doubt Tesla’s vision-only FSD can achieve unsupervised autonomy, citing technical limitations and unkept promises.  
- **LiDAR seen as viable**: Despite Tesla’s stance, users believe LiDAR’s falling cost and sensor fusion’s safety benefits make it essential for reliable autonomy.  
- **Strategic and leadership scrutiny**: Musk’s decisions and Tesla’s marketing face backlash, with some viewing the FSD subscription push as a metric-gaming tactic.  

The discussion underscores a divide between Tesla’s ambitious vision and practical challenges, with many advocating for hybrid sensor approaches to bridge the gap.

### ML needs a new programming language – Interview with Chris Lattner

#### [Submission URL](https://signalsandthreads.com/why-ml-needs-a-new-programming-language/) | 291 points | by [melodyogonna](https://news.ycombinator.com/user?id=melodyogonna) | [258 comments](https://news.ycombinator.com/item?id=45137373)

Ron Minsky sits down with LLVM/Swift/MLIR creator Chris Lattner to unpack Mojo, his bid to make programming modern GPUs both productive and fun without sacrificing control. The core argument: ML developers need a language that exposes hardware realities for peak performance, but wraps that complexity in type-safe metaprogramming so patterns like tiling, memory layouts, and vectorization are reusable and shareable. The goal is to specialize code to both the computation and the target hardware—while pushing toward an ecosystem that isn’t dominated by a single vendor.

Highlights:
- Productivity with control: write state-of-the-art kernels without dropping to hand-tuned CUDA/C++ for everything.
- Hardware-aware by design: programmers “reckon with the hardware,” but ergonomics come from safe, composable metaprogramming.
- Specialization as a feature: adapt kernels to specific accelerators and workloads rather than one-size-fits-all abstractions.
- Open compiler foundations: ideas build on infrastructure like MLIR, aiming for portability and less vendor lock-in.
- Bigger picture: “Somebody has to do this work” to democratize AI compute and broaden who can write fast kernels.

Episode: Signals and Threads, Season 3 Episode 10 (Sept 3, 2025). Related topics mentioned: Modular AI, Mojo, MLIR, Swift, “Democratizing AI compute” series.

**Summary of Hacker News Discussion:**

The discussion revolves around Chris Lattner’s Mojo programming language and its potential to address challenges in ML/GPU programming. Key points include:

1. **Mojo’s Goals & Features**:  
   - Users highlight Mojo’s aim to solve the "two-language problem" by enabling high-level Python ergonomics with low-level control (via MLIR/LLVM), allowing GPU kernel programming directly in Python-like syntax.  
   - Emphasis on MLIR’s role in hardware specialization and avoiding vendor lock-in.  

2. **Python’s Dominance**:  
   - Debate over why Python remains dominant in ML: its rich ecosystem (PyTorch, NumPy), seamless C/C++ integration, and high-level APIs abstracting GPU complexity. Skepticism arises about new languages displacing Python’s entrenched tooling.  
   - Counterpoints mention alternatives like Elixir/Nx (with BEAM’s distributed systems strengths) and Triton’s Python-based JIT kernels.  

3. **Technical Challenges**:  
   - CUDA/C++ ecosystems are mature but fragmented. Criticism targets NVIDIA’s proprietary hold and ROCm’s instability. Some praise CUTLASS 3/4 for simplifying GPU kernels but note industry complexity.  
   - Concerns about Mojo’s ecosystem maturity vs. Python’s "fragmented functionality."  

4. **Industry Inertia**:  
   - Skeptics argue new languages face uphill battles against Python’s momentum, despite Mojo’s technical merits. Others note niche successes (e.g., Julia, Elixir) but concede widespread adoption is rare.  

5. **Optimism for Mojo**:  
   - Supporters highlight Mojo’s MLIR foundation, type-safe metaprogramming, and Lattner’s track record (Swift, LLVM). Some see potential in unifying high-level expressiveness with hardware-specific optimizations.  

**Notable Comparisons**:  
- **Elixir/Nx**: Praised for distributed systems and LiveView, but seen as complementary rather than a Python replacement.  
- **Triton**: Python JIT kernels already bridge some gaps Mojo targets.  
- **Julia**: Similar goals but struggles with ecosystem traction.  

**Sentiment**: Cautious optimism about Mojo’s vision, tempered by skepticism about overcoming Python’s ecosystem and industry inertia. The discussion underscores the tension between technical innovation and practical adoption barriers.

### Should we revisit Extreme Programming in the age of AI?

#### [Submission URL](https://www.hyperact.co.uk/blog/should-we-revisit-xp-in-the-age-of-ai) | 72 points | by [imjacobclark](https://news.ycombinator.com/user?id=imjacobclark) | [56 comments](https://news.ycombinator.com/item?id=45143945)

Should we revisit Extreme Programming in the age of AI? (9 min read)

- TL;DR: AI has made code creation cheap and fast, but delivery outcomes are still poor. The bottleneck isn’t typing speed—it’s alignment, validation, and learning. Extreme Programming (XP) adds “good friction” (pairing, small batches, tests, CI) to slow down locally so teams can go faster overall.

- Key points:
  - Output isn’t the constraint: despite decades of acceleration (frameworks, DevOps, serverless, AI), large studies still show most projects miss expectations; speed alone hasn’t fixed delivery.
  - XP as counterweight: practices like pair programming intentionally trade raw throughput for shared understanding, trust, quality, and team capability—sociotechnical benefits that guide direction, not just speed.
  - AI magnifies risk: agentic systems and rapid code gen can pile on unvalidated logic, increasing complexity and brittleness. Research notes LLM accuracy can degrade over long contexts, compounding “vibe-coded” entropy.
  - Software is still human: the persistent blockers are alignment, feedback, clarity of outcomes, and user validation. XP’s values—simplicity, communication, feedback, respect, courage—directly target these.

- Data points:
  - CHAOS report: on-time/on-budget delivery was 16% (1994), 37% (2012), and slipped to 31% (2020).
  - McKinsey: ~70% of digital transformations fail.

- Why it matters: As code gets cheaper, the hard part is building the right thing and keeping it changeable. XP’s constraints help manage quality, risk, and learning in an AI-accelerated environment.

- Suggested shifts:
  - Prioritize flow over raw velocity; feedback over feature count.
  - Double down on small batches, CI, automated testing, pairing, and shared ownership.
  - Invest in outcome-generating capabilities: tighter team collaboration, clearer product direction, stronger user feedback loops.
  - Make the process more human, not less—optimize operating rhythms for collaboration, clarity, and flow.

- Bottom line: Yes—revisit XP. In the AI era, disciplined, human-centered practices are the steering wheel we need when the engine keeps getting faster.

The discussion on revisiting Extreme Programming (XP) in the AI era highlights several key themes:

### **Support for XP's Relevance**
1. **Human-Centric Practices**: Many argue XP’s emphasis on **pair programming, TDD, small iterations, and feedback loops** is critical to managing AI's risks. These practices ensure alignment, validation, and quality control as AI accelerates code generation but struggles with context and long-term accuracy.
   - Examples: Teams combining XP with AI tools report efficiency gains without sacrificing senior developer time. AI aids individual tasks (e.g., code suggestions), while XP’s collaborative structure maintains coherence.

2. **Quality Over Speed**: Participants stress that AI’s "cheap code" exacerbates brittleness and complexity. XP’s **testing, CI/CD, and simplicity** act as "good friction" to prevent entropy, ensuring maintainability and correct outcomes.

3. **Feedback Loops**: XP’s tight feedback cycles are seen as complementary to AI. While AI enables rapid prototyping, XP’s iterative validation (e.g., user stories, automated tests) ensures AI-generated code meets actual needs.

### **Critiques and Counterpoints**
- **Agile’s Corruption of XP**: Some lament that Agile diluted XP’s rigor into "checklist rituals," but others note modern workflows (e.g., CI/CD) now embody XP principles.
- **AI’s Limitations**: LLMs lack deep understanding of requirements or system-wide implications, making human oversight (via pairing, refactoring) indispensable.
- **Waterfall Comparisons**: A tangent debates whether Waterfall is resurging with AI. Critics clarify Waterfall’s rigidity (vs. XP/Agile’s adaptability) remains ill-suited for dynamic projects, though some organizations still use hybrid or Waterfall-like processes.

### **Organizational Context**
- **Startups vs. Enterprises**: Startups lean into Agile/XP for flexibility, while large firms often default to Waterfall-esque planning due to risk aversion, despite inefficiencies.
- **Methodology Blurring**: Many note "Agile" implementations often resemble Waterfall, highlighting the gap between theory and practice. XP’s structured yet adaptive approach is seen as a remedy.

### **Conclusion**
The consensus leans toward **reviving XP’s core principles** (simplicity, communication, testing) to steer AI’s potential. While AI transforms coding speed, XP’s focus on collaboration, validation, and incremental delivery addresses the persistent challenges of alignment and quality—making it a vital "steering wheel" in the AI age.

### Using AI to perceive the universe in greater depth

#### [Submission URL](https://deepmind.google/discover/blog/using-ai-to-perceive-the-universe-in-greater-depth/) | 52 points | by [diwank](https://news.ycombinator.com/user?id=diwank) | [22 comments](https://news.ycombinator.com/item?id=45134489)

- Researchers working with LIGO report a reinforcement learning–based controller that cuts control noise 30–100× in LIGO’s most unstable mirror feedback loop, field-tested at the Livingston observatory.
- The method, Deep Loop Shaping, optimizes in the frequency domain to avoid injecting noise in LIGO’s observation band—the range where the interferometer must be ultrasilent to see gravitational-wave signals.
- Why it matters: Lower control noise means more stable mirrors and cleaner strain data at 10^-19 m sensitivities, potentially enabling detection and richer characterization of hundreds more events per year, including elusive intermediate-mass black holes.
- What’s new vs. today: It moves beyond traditional linear control design, actively reducing “control noise” that can otherwise amplify vibrations and swamp signals.
- Broader impact: The approach could generalize to vibration/noise suppression in aerospace, robotics, and structural engineering.

Quote: “Studying the universe using gravity instead of light is like listening instead of looking… This work allows us to tune in to the bass.”

**Summary of Discussion:**

The discussion revolves around the use of AI terminology in scientific research, skepticism about hype, and debates over incremental engineering progress versus genuine scientific breakthroughs. Key points include:

1. **AI Terminology & Hype Concerns**:  
   - Users criticize the headline for potentially overhyping the work as "AI-driven," arguing it risks misleading the public into conflating specialized tools (e.g., reinforcement learning here) with general AI like ChatGPT. Terms like "AI-assisted" are seen as technically accurate but prone to buzzword-driven misinterpretation.  
   - Comparisons are drawn to the 1955 definition of AI, emphasizing that current engineering improvements (e.g., noise reduction in LIGO) represent incremental steps, not revolutionary "intelligence."

2. **Science vs. Engineering Debate**:  
   - Some argue that the work is more about applied engineering (solving a control-loop problem) than fundamental science, questioning whether incremental progress qualifies as AI research. Others counter that even pragmatic engineering advances contribute to scientific goals, such as improving gravitational-wave detection.

3. **Corporate Research & Priorities**:  
   - A subthread discusses whether tech companies (e.g., DeepMind, OpenAI) should prioritize scientific machine learning over generic LLM development. Concerns are raised about funding shifts toward trendy AI areas at the expense of niche, impactful research.

4. **Public Perception vs. Technical Reality**:  
   - Users note that consumers often fail to distinguish between narrow AI applications (like this control system) and broad "intelligence," leading to inflated expectations. The debate highlights the challenge of communicating technical work without sensationalism.

5. **Broader Reflections**:  
   - Some humorously liken the excitement to "TV narrator hype," while others reflect on buzzwords (e.g., AI, blockchain, metaverse) cycling in and out of fashion without lasting impact.

**Takeaway**: The discussion underscores a tension between celebrating technical progress and maintaining skepticism about AI branding, emphasizing the need for clarity in scientific communication to avoid misrepresentation.

