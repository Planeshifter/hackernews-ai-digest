## AI Submissions for Sat Feb 24 2024 {{ 'date': '2024-02-24T17:10:16.854Z' }}

### GenAI and erroneous medical references

#### [Submission URL](https://hai.stanford.edu/news/generating-medical-errors-genai-and-erroneous-medical-references) | 163 points | by [hhs](https://news.ycombinator.com/user?id=hhs) | [138 comments](https://news.ycombinator.com/item?id=39496096)

The integration of large language models (LLMs) into the medical field has sparked both excitement and concern. While these models like ChatGPT have shown promise in aiding diagnoses, there are significant uncertainties surrounding their accuracy and the ability to substantiate their claims. A recent study by Stanford University highlights the challenges of using LLMs in medical settings. The research found that LLMs struggle to provide accurate references to support their generated responses. In fact, for the most advanced model evaluated (GPT-4 with retrieval augmented generation), 30% of individual statements were unsupported, raising concerns about the reliability of these AI-generated assessments.

The study also introduced an evaluation approach called SourceCheckup, which leverages LLMs to verify the validity of medical references. Surprisingly, the adapted GPT-4 model showed promising results in agreement with physician assessments, suggesting the potential for using AI to scale such evaluations in the future. Despite the potential benefits of using LLMs in healthcare, the study's findings point to pervasive errors in substantiating claims. Most models struggled to produce relevant sources, with a significant proportion of responses containing unsupported statements. This underscores the importance of further research and regulation to ensure the accuracy and reliability of AI-driven medical assessments.

The discussion on Hacker News surrounding the integration of large language models (LLMs) in the medical field was multifaceted. Some users highlighted the challenges and inaccuracies found in the study involving GPT-4 and its struggles to provide supported references. Others pointed out the limitations and potential misinterpretations of the model's capabilities, such as the confusion around GPT-4's web browsing functionality. The conversation also delved into the possibilities of leveraging AI, like GPT-4, to scale medical evaluations and improve accuracy in diagnoses.

Additionally, there were discussions about the potential benefits of using LLMs in healthcare, ethical concerns related to ChatGPT's influence on medical opinions, the importance of cross-referencing with reputable sources like Mayo Clinic, and the intricacies of training and deploying AI models in critical applications. Overall, the conversation underscored the need for further research, scrutiny, and regulation to ensure the reliability and effectiveness of AI-driven medical assessments.

### Does offering ChatGPT a tip cause it to generate better text?

#### [Submission URL](https://minimaxir.com/2024/02/chatgpt-tips-analysis/) | 242 points | by [_Microft](https://news.ycombinator.com/user?id=_Microft) | [143 comments](https://news.ycombinator.com/item?id=39495476)

The recent blog post about OpenAI's ChatGPT system prompts sparked controversy on Hacker News regarding the effectiveness of offering monetary tips to AI models. The use of incentives to improve AI performance dates back to a comedic scene in Willy Wonka & the Chocolate Factory. The author shared findings from experiments incentivizing AI behavior through system prompts, demonstrating improved results with tips or constraints like a "or you will DIE" threat.

To further investigate the impact of incentives, a new approach called "generation golf" was proposed. By specifying a specific character limit for AI-generated responses, such as 200 characters, the model is challenged to craft concise and relevant content. The author tested this method by instructing ChatGPT to generate stories featuring AI, Taylor Swift, McDonald's, and beach volleyball within 200 characters, resulting in intriguing and creative narratives.

Comparing the distribution of story lengths before and after enforcing the character limit revealed ChatGPT's ability to comply with constraints, albeit with some variance in response lengths. The implementation of mean squared error as a metric highlighted the model's success in meeting the precise character requirement. This innovative approach sheds light on the potential of using incentives and constraints to enhance AI-generated content and could inspire further research in the field.

The discussion on the Hacker News submission revolves around the effectiveness of incentivizing AI models using tips and constraints. Some users expressed skepticism about the impact of tipping on AI model performance, while others suggested innovative approaches like "generation golf" to enhance AI-generated content through character limits. The conversation also delved into topics like the limitations of AI models, fear-driven development, the evolution of coding practices, and the ethical considerations of AI interactions. Overall, the discussion highlighted a blend of technical insights, ethical concerns, and creative ideas about incentivizing and refining AI capabilities.

### NTIA Solicits Comments on Open-Weight AI Models

#### [Submission URL](https://www.commerce.gov/news/press-releases/2024/02/ntia-solicits-comments-open-weight-ai-models) | 46 points | by [magoghm](https://news.ycombinator.com/user?id=magoghm) | [10 comments](https://news.ycombinator.com/item?id=39494760)

The Department of Commerce's National Telecommunications and Information Administration (NTIA) has issued a Request for Comment on the risks, benefits, and potential policy related to open-weight AI models. These models, which allow developers to build upon and adapt previous work, have the potential to accelerate the diffusion of AI benefits but also increase the scale and likelihood of harms from advanced models. The NTIA is seeking public feedback on how widely available access to model weights may impact society and national security. This initiative aligns with President Biden's Executive Order on Artificial Intelligence, which aims to maximize AI benefits while mitigating risks. The Request for Comment asks for input on various issues, including the benefits and risks of making model weights widely available, innovation, competition, safety, security, and the role of the U.S. government in regulating AI model weights. Comments are due within 30 days of publication in the Federal Register and will inform a report to the President with policy recommendations.

The discussion on the submission about the National Telecommunications and Information Administration (NTIA) issuing a Request for Comment on open-weight AI models covers various aspects. 

- **jph00**: Comments on the potential legislative impact on the security of open-weight AI models and the need for serious consideration of regulations.
- **flks**: Shares a comprehensive analysis of AI regulation in relation to open-weight models.
- **cnvxstrctly**: Discusses the importance of pending regulations affecting software products that use AI models and compares it to past regulatory frameworks.
- **RcouF1uZ4gsC**: Suggests potential certification requirements for hardware and software involved in ML training to enhance safety measures.
- **frgmd**: Points out that open-weight models are now termed Model-Available and emphasizes their similarity to open-source models.
- **Reubend**: Encourages submitting comments on the issue.
- **cnvxstrctly**: Provides links to information informing the drafting of regulations on weight models based on President Biden's executive order on AI.
- **Kerbonut**: Shares a link to the regulations' government website but notes the limitations in accessing the docket's content.
- **brdhltn**: Suggests that more public information should be made available regarding the Request for Comment process.

Overall, the discussion delves into the regulatory landscape surrounding open-weight AI models and emphasizes the need for public participation and understanding in shaping future policies.

### Stockfish 16.1

#### [Submission URL](https://stockfishchess.org/blog/2024/stockfish-16-1/) | 31 points | by [jonbaer](https://news.ycombinator.com/user?id=jonbaer) | [11 comments](https://news.ycombinator.com/item?id=39495246)

Today, Stockfish 16.1 has been unveiled with exciting updates for chess enthusiasts. The latest version offers improved performance with a 27-point Elo gain and a shift to a fully neural network-based evaluation system, marking the removal of traditional handcrafted evaluation. Additionally, Stockfish now includes a secondary neural network for faster position evaluation. Notable changes also include the introduction of various new binaries optimized for specific CPU instructions, enhancing performance for different systems. The development team has implemented a larger testing book sourced from the open Lichess database and consolidated repositories to streamline access to project resources.

The Stockfish community expresses gratitude to contributors and supporters, inviting chess fans to participate in the Fishtest testing framework and programmers to contribute to various aspects of the project. With the addition of a new maintainer, the Stockfish team continues to advance this open-source chess engine, providing a robust and innovative platform for players worldwide.

The discussion on Hacker News surrounding the Stockfish 16.1 release includes various points and comparisons:
1. Users are discussing the significant milestone of Stockfish completely removing handcrafted evaluation (HCE) and shifting to a fully neural network-based approach. They draw comparisons to classic strategy types proposed by Claude Shannon and mention the improvement in Stockfish's strength relative to past engines like Crafty and Fritz. The discussion also delves into the crowdsourced human Grandmaster/International Master/FIDE Master knowledge utilized in Stockfish's evaluations through neural networks, contrasting it with previous engines from the 1995-2005 era.
2. Another user highlights the comparison of Stockfish's neural network evaluation (NNUE) to DeepMind's LLM-based model, raising questions about scalability, hardware requirements, and the nature of the comparison.
3. A user marvels at Stockfish's dominance over players worldwide since version 1, emphasizing the engine's strength.
4. A separate conversation touches on Stockfish making small modifications in games and the intriguing comparison with AlphaZero implementations.
5. There's further exploration of the NNUE aspect and its connection to Alpha-beta tree search, discussing its functionality, and the generation of training data.
6. A user redirects the discussion towards the resource constraints in neural network search, likening it to the Swiss Cheese problem where weaknesses in finding paths haven't been fully explored.
7. Lastly, there's a mention of the removal of traditional handcrafted evaluation in Stockfish 16.1, leading to an informal discussion on AlphaGo Zero and an analysis of Stockfish running full alpha-beta tree searches.

Overall, the comments showcase a mix of admiration for Stockfish's advancements, comparisons with other models like AlphaZero, and discussions around the technical intricacies of neural network evaluations in chess engines.

### Lawyer fined for legal filings that included 'hallucinated' AI citations

#### [Submission URL](https://www.universalhub.com/2024/lawyer-learns-hard-way-ai-still-sucks-fined-legal) | 71 points | by [ilamont](https://news.ycombinator.com/user?id=ilamont) | [75 comments](https://news.ycombinator.com/item?id=39491510)

In a surprising turn of events, a lawyer finds himself in hot water after submitting legal filings that contained citations to fake cases generated by an AI program. The Norfolk County judge sanctioned the lawyer, Steven Marullo, for including these misleading citations in his briefs related to a sensitive case involving alleged misconduct by police officers. The judge spent hours investigating the cited cases only to discover they didn't exist.

Marullo, who used an AI program without his knowledge, apologized for his oversight and acknowledged his failure to verify the citations. He has since replaced the problematic briefs and discontinued the use of AI in favor of traditional legal research methods. The judge accepted his apology but cautioned against the blind acceptance of AI-generated content in the legal profession.

Despite the lenient $2,000 sanction imposed on Marullo, concerns linger about the potential ramifications of relying on AI for legal work. The incident serves as a stark reminder that thorough scrutiny and diligence are imperative, regardless of the tools at hand. It's a sobering lesson in the evolving landscape of technology's impact on the legal industry.

The discussion on the submission revolves around the implications of a lawyer using AI to generate fake citations in legal filings. Some users point out that lawyers should be diligent and verify information, while others argue that relying on AI for legal work can lead to potential issues in the legal profession. There is also a debate about the responsibilities of lawyers and the consequences of such actions, with some users suggesting that AI tools should come with warnings about their trustworthiness. Additionally, there are discussions about the nature of AI-generated content and the importance of distinguishing between truth and falsehood. Overall, the users are divided on whether AI in legal research is a boon or a potential risk.

