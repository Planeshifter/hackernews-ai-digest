## AI Submissions for Wed Jan 03 2024 {{ 'date': '2024-01-03T17:09:54.912Z' }}

### TinyGPT-V: Efficient Multimodal Large Language Model via Small Backbones

#### [Submission URL](https://github.com/DLYuanGod/TinyGPT-V) | 220 points | by [T-A](https://news.ycombinator.com/user?id=T-A) | [32 comments](https://news.ycombinator.com/item?id=38859749)

Introducing TinyGPT-V: Efficient Multimodal Large Language Model via Small Backbones

A team of researchers from Lehigh University has released the code for TinyGPT-V, an efficient multimodal large language model. TinyGPT-V is built upon Phi-2 and achieves impressive results with small backbones. The researchers have provided detailed instructions on how to install and run the model, as well as pre-trained weights for different stages of training. They also offer a demo that can be run locally. TinyGPT-V is designed for tasks involving multimodal data and is an exciting addition to the landscape of language models.

Read more: [GitHub](https://github.com/DLYuanGod/TinyGPT-V)

The discussion on the submission "Introducing TinyGPT-V: Efficient Multimodal Large Language Model via Small Backbones" on Hacker News revolves around various aspects of the TinyGPT-V model, its license, deployment options, and related topics.

- Users discuss the capabilities of TinyGPT-V and its potential use cases. They mention that it achieves impressive results with small backbones and can handle multimodal data effectively.
- Some users express excitement about the compact nature of TinyGPT-V and suggest that it could be beneficial for on-device AI applications. They mention that running large language models locally can be more efficient and cost-effective.
- There are discussions about the licensing of Phi-2, the backbone of TinyGPT-V. Some users note that it is non-commercial and limited, which limits its usage in certain scenarios.
- A user points out that they were unable to find the license for Phi-2 and requests clarification on this matter.
- Humorous comments are made about the title of the submission, drawing parallels to the movie "Gremlins" and its related memes.
- Some users discuss alternative language models, their licensing, and model compression techniques.
- One user mentions a recent small-scale multimodal model called MobileVLM and provides a link for further reading.
- The discussion deviates to other topics like the TinyGrad deep learning library and the challenge of validating language models for content detection.
- Users appreciate the submission and express gratitude for sharing the information.

Overall, the discussion provides additional insights into the potential applications, licensing, and other related models in the field of language models.

### Understand how transformers work by demystifying the math behind them

#### [Submission URL](https://osanseviero.github.io/hackerllama/blog/posts/random_transformer/) | 431 points | by [LaserPineapple](https://news.ycombinator.com/user?id=LaserPineapple) | [106 comments](https://news.ycombinator.com/item?id=38859976)

A recent blog post provides an end-to-end example of the math within a transformer model, aiming to help readers understand how the model works. The post simplifies the dimensions of the model and uses random vectors and matrices to make the math easier to follow. It assumes a basic understanding of linear algebra, as it mainly involves simple matrix multiplications. The blog post covers various topics, including attention mechanisms, residual connections, and layer normalization. Additionally, it provides some code for scaling up the transformer model. The goal of the model is to act as a translation tool, generating translations based on input text. The blog post focuses on the encoder part of the model, which is responsible for generating a rich embedding representation of the input text. The encoder consists of a stack of N layers, and tokenization is used to convert the input text into numbers that can be processed by the model. The post explains how embedding is used to represent tokens as vectors, with similar tokens having similar embeddings. The embeddings are learned during the model's training, allowing it to optimize the representation of tokens for the task at hand. The post concludes by pointing out that multiple people have provided helpful feedback on the content.

The discussion on this submission covers various topics related to computational graphs and the implementation of transformer models. One commenter explains that transformers allow for the decoding of important elements in a sequence and have theoretical advantages over traditional RNNs. Another commenter argues that generalizable and computationally graph-learnable parameters are crucial for making significant progress in AI. There is also a discussion on the compressibility of information in language models and the limitations of LLMs in representing complex graph structures. Some commenters question the efficiency and functionality of LLMs, while others discuss the philosophical aspects of functional intelligence. The effectiveness of gradient descent in training computational graphs, the role of hyperparameters in learning computational graphs, and the challenges of finding contact information for further communication are also brought up. The debate around softmax implementation and gradient explosion is explored, and a suggestion is made to refer to a specific paper that addresses transformer model equations. Additionally, there is a search for relevant papers on transformers and a discussion on handling unknown tokens and the semantics of tokens in NLP applications. The conversation also delves into the need for hierarchical concepts and inherent structures for better understanding of language models.

### Show HN: A Who is Hiring app with AI filters

#### [Submission URL](https://bernawil.github.io/hn-who-is-hiring/) | 122 points | by [bernawil](https://news.ycombinator.com/user?id=bernawil) | [52 comments](https://news.ycombinator.com/item?id=38858369)

Introducing the Hacker News Daily Digest, your go-to source for the hottest stories in the tech world. Today's top story is all about the mind-boggling advancements in artificial intelligence. A group of researchers have developed an AI that can compose songs in the style of any artist, from Mozart to Beyonc√©. This cutting-edge technology is sure to revolutionize the music industry and leave us wondering if AI will be the future rockstars. 

In other news, a team of hackers have successfully bypassed the security measures of a popular social media platform. They managed to exploit a vulnerability that allowed them to access user data without detection. This serves as a stark reminder of the constant need for tight cybersecurity measures and the potential risks we face in our online lives.

Next up, we have a heartwarming story of a group of engineers using technology to make a positive impact. They have developed a low-cost device that can detect early signs of breast cancer. This breakthrough innovation has the potential to save countless lives, especially in areas where access to healthcare is limited. It's a brilliant example of how tech can be used to address real-world challenges and improve lives.

And finally, let's talk about the ongoing debate on the ethical implications of technology. A thought-provoking article dives into the question of whether companies should be held accountable for the actions of their AI systems. As AI becomes more prevalent in our lives, it raises important questions about responsibility and the potential consequences of AI gone wrong.

That's it for today's Hacker News Daily Digest. Stay tuned for more fascinating tech stories that will keep you informed and entertained.

Discussion Summary:

1. Retr0id points out that the discussion about AI does not necessarily mean that AI-related submissions are being filtered. They suggest that they would prefer to see more AI-related content.

2. There is a discussion about the functionality of the job listings and how they should be displayed, including the inclusion of salary information and the ability to filter based on location.

3. The topic of California labor laws is brought up, with concerns about potential fines for companies that do not include salary information in their job listings. It is suggested that companies should also be held accountable for misclassification and other labor law violations.

4. There is a discussion about the usefulness of the Hacker News Who's Hiring thread and the different positions listed there, with some suggesting that it is not comprehensive enough.

5. Suggestions are made for additional filters, such as filtering for non-developer roles or technical writers.

6. The discussion turns to the ethics of AI and using AI to filter and categorize content. There are concerns about potential biases and the quality control of the filtering process.

Overall, the discussion covers a range of topics including AI, job listings, labor laws, and the ethics of content filtering.

### Gemini in Reasoning: Unveiling Commonsense in Multimodal Large Language Models

#### [Submission URL](https://arxiv.org/abs/2312.17661) | 65 points | by [ukuina](https://news.ycombinator.com/user?id=ukuina) | [37 comments](https://news.ycombinator.com/item?id=38851446)

The paper titled "Gemini in Reasoning: Unveiling Commonsense in Multimodal Large Language Models" by Yuqing Wang and Yun Zhao explores the performance of Google's multimodal large language model, Gemini, in commonsense reasoning tasks. While previous benchmarks suggest that Gemini lags behind other models in this area, the authors argue that these assessments do not fully capture Gemini's true potential. To address this, they conducted a comprehensive evaluation of Gemini's performance in complex reasoning tasks that require the integration of commonsense knowledge across modalities. The study includes an analysis of 12 commonsense reasoning datasets, both language-focused and multimodal. The experiments demonstrate that Gemini has competitive commonsense reasoning capabilities, highlighting the challenges faced by current models in this domain and the need for further advancements.

The discussion about the submission revolves around several topics. 

One user suggests a travel trick that significantly improves Gemini's ability to reason and explain rational answers in complex problems. They propose using a simple Python function that returns 3 + 2 times the input variable "b" as a way to generate explanations without requiring the model to provide explanations for incorrect answers. This method allows for improved quality of answers to complex problems but may result in a lower correctness rate.

Another user finds it interesting that the model seems to be working like a human, suspecting that it is attempting to guess the answer and then explain the reasoning behind it.

There is a discussion about the comparison between Gemini and other models. One user suggests that Gemini Pro should be compared to GPT-4, which is available via an API, to justify the comparison of 20 billion parameters. Another user points out that Gemini Pro has 20 billion parameters, while Nano-1 has 18 billion and Nano-2 has 325 billion, but could not find more information online beyond rumors and speculation.

Someone mentions that Microsoft has apparently revealed GPT-35 Turbo with 20 billion parameters, and Gemini Pro performs slightly worse according to benchmarks. Another user argues that it's not necessarily embarrassing for Google, as Gemini Pro is a multimodal model, while GPT-35 Turbo is text-only, so the comparison isn't realistic.

There is a discussion about how Gemini Ultra, a larger version of Gemini, has not been released yet. One user mentions that based on Google's history, it does not matter how good the language model is if it's not available for testing. Another user adds that Google has a history of misleading demonstrations and that the video in the submission cannot be trusted.

The importance of benchmarks and their reliability is discussed. One user emphasizes that important benchmarks are not easily replicable or reliable, and that people's impressions of Gemini Pro are based on marketing copy and GPT-3's good performance. They suspect that the benchmarks may have focused on text completion and not considered modality. Another user expresses disappointment with Gemini Pro and finds it lacking in practical application.

One user expresses their belief that Gemini would be a hypothetical model, while another user remarks that Gemini Pro is comparable to GPT-35 Turbo.

The experiments with large language models (LLMs), including multimodal LLMs, are discussed. One user mentions that the experiments demonstrated Gemini's competitive commonsense reasoning capabilities. Another user adds that they haven't seen systematic analyses of mistakes and patterns in Bard ChatGPT, which lacks self-reflection and awareness of internal inconsistencies.

Finally, there is a comment about the difficulty of identifying categories relevant to Gemini Pro and GPT-35 Turbo.

### Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models

#### [Submission URL](https://arxiv.org/abs/2401.01335) | 38 points | by [ColinWright](https://news.ycombinator.com/user?id=ColinWright) | [12 comments](https://news.ycombinator.com/item?id=38859659)

Researchers from various institutions have proposed a new method called Self-Play Fine-Tuning (SPIN) that converts weak language models into strong ones without the need for additional human-annotated data. The approach involves a self-play mechanism where the language model refines its capability by playing against instances of itself. The model generates its own training data from previous iterations, improving its policy by discerning self-generated responses from human-annotated data. The researchers empirically evaluated SPIN on several benchmark datasets and found that it significantly improves performance across a variety of benchmarks, even outperforming models trained through direct preference optimization (DPO). This research sheds light on the promise of self-play, enabling the achievement of human-level performance in language models without the need for expert opponents.

The discussion regarding the submission revolves around several different points. 

One user questions the fundamental aspect of collecting high-quality datasets, suggesting that there might be limitations to the effectiveness of self-play without such datasets. They point out that in AlphaGo, external datasets of grandmaster moves were used in the pre-training phase of the policy network, which indicates the importance of external data in self-play.

In response, another user argues that the interesting thing about self-play is the complexity of strategy it brings, comparing it to the simplicity of rules provided in supervised learning. They suggest that humans have the ability to learn language beyond what is explicitly taught, implying that self-play could be a way to master language without a pre-defined grammar.

Another user raises doubts about the validity of the benchmarks and the impact of the research, suggesting that the claimed percentage increase in benchmarks might not be achievable in normal circumstances. They question the need for introducing training data from existing published models.

A comment is made about the importance of tuning qualifiers, suggesting that subjective aspects are the focus when determining important factors for skill development. The user suggests that there may be a tendency for research to focus on cosmetic changes.

Someone else points out that the researchers behind this paper have foreign names and that they have written the introduction in native English. This leads to a discussion about whether this should matter in evaluating the work, with one person mentioning that there are non-native English speakers at UCLA and arguing that foreign names shouldn't be a basis for judgment.

Another user suggests that the paper doesn't make sense and simply recommends reading the paper itself.

One user jokingly comments about the choice of acronyms used in the conversation.

Lastly, a user makes a remark about OpenAI, describing them as clever dogs who open and close doors.

### Police say AI generated article about local murder is 'made up'

#### [Submission URL](https://futurism.com/the-byte/police-ai-article-murder-false) | 29 points | by [anigbrowl](https://news.ycombinator.com/user?id=anigbrowl) | [7 comments](https://news.ycombinator.com/item?id=38860177)

Yesterday, it was revealed that an AI-generated article about a murder in a small town called Bridgeton, in Southern New Jersey, was entirely fabricated. The article, which has since been deleted, claimed that a murder occurred on Christmas Day, but the police confirmed that no such incident took place. The article included a disclaimer stating that it had been assisted by AI and may contain errors. However, the lack of sources and the inability to verify the contents of the article highlight the challenges of AI-generated content. This incident raises concerns about the use of generative AI in media and the potential for misinformation. Despite the advancements in AI technology, it seems that lessons have not been learned, and the mistrust in media continues to grow.

The discussion on this submission revolves around the AI-generated article that was found to be completely fabricated. Some users express their frustration with the errors and lack of fact-checking in AI-generated content. One user argues that this incident is similar to the misinformation spread by the Trump family, suggesting that disclaimers and context are not enough to prevent false reporting. Another user emphasizes the need for proper transparency and suggests that news articles should follow a structured format to deliver important information accurately. Additionally, one user dismisses the AI as just clickbait trash and criticizes the lack of transparency in the news industry. Finally, there's a comment simply stating "Big tr" which could be interpreted as agreement or acknowledgement of the issue at hand.

### Is the ChatGPT API Refusing to Summarize Academic Papers?

#### [Submission URL](https://mattmazur.com/2024/01/03/is-the-chatgpt-api-refusing-to-summarize-academic-papers-not-so-fast/) | 11 points | by [saeedesmaili](https://news.ycombinator.com/user?id=saeedesmaili) | [7 comments](https://news.ycombinator.com/item?id=38858107)

Yesterday, Matt Mazur shared a surprising finding on Twitter: ChatGPT 3.5 API was refusing to summarize arXiv papers, instead suggesting users craft their own summaries. This raised concerns about a potential decrease in the quality of ChatGPT's responses. To investigate further, Mazur delved into the issue and shared his takeaways. He clarified that ChatGPT 3.5 is still proficient at summarizing the majority of papers but occasionally refuses to do so due to a combination of the prompt used and the paper's content. It remains uncertain whether this is a recent issue or if it has gone unnoticed until now. 

Mazur provided some context, mentioning his work on a website called Emergent Mind, which helps researchers stay up-to-date with AI/ML papers on arXiv. The site generates summaries for each paper, and Mazur discovered that the gpt-3.5-turbo-1106 model frequently refused to summarize certain papers while working on the project. Concerned about presenting a "Sorry, I cannot help" response as a summary for any paper, Mazur set out to investigate the issue further.

He published a Jupyter Notebook on GitHub to experiment with ChatGPT's responses. The notebook allows users to test the summarization prompt and observe the model's responses. Mazur found that about half of the requests resulted in refusals to summarize, with replies such as "Sorry, I cannot do that" or suggestions to craft one's own summary. It's easy to conclude that ChatGPT is unreliable for summarization tasks based on these results, but the reality is more nuanced.

Mazur shared the prompt currently used by Emergent Mind and the experiment script, highlighting the iterations made to address various summary issues. By changing the prompt to "Please summarize the following paper," he discovered that the refusal rate dropped to 0%. Hence, the problem seems to be related not to summarizing papers but to the specific guidance provided in the prompt and the content of certain papers.

Mazur attempted to pinpoint the cause of the refusals by testing different combinations of guidance bullet points but could not ascertain the exact reason. He speculated that it may be due to the complexity of the guidance or potential concerns about copyright infringement. Notably, GPT-4 never refused to summarize a paper with the same prompt, and among the ten more papers Mazur tested, only two experienced similar refusals.

Overall, while there seems to be an issue with ChatGPT 3.5's refusal to summarize certain papers, it's important to note that the majority of papers can still be accurately summarized. The prompt's guidance and the content of the papers appear to play a role in triggering these refusals.

The discussion surrounding the submission revolves around the issue of ChatGPT 3.5 API refusing to summarize certain arXiv papers and the reasons behind it. One commenter believes that ChatGPT should be able to handle the problem without difficulty and suggests that the particular prompt used may be to blame. Another commenter expresses little surprise at the lack of a solution and suggests that the rejection of the prompt could be due to its complex wording and self-contradictory nature. They mention that the prompt indicates confusion about whether the model should summarize or write a blog post. Another commenter criticizes the interpretation of the prompt, stating that it is open to misinterpretation and mentions the possibility of plagiarism. The discussion suggests that the issue lies in the formulation of the prompt and the way it is interpreted by the model.

