## AI Submissions for Sat Sep 30 2023 {{ 'date': '2023-09-30T17:10:41.888Z' }}

### Optical Circuit Switching for ML Systems

#### [Submission URL](https://dl.acm.org/doi/10.1145/3603269.3604836) | 59 points | by [tim_sw](https://news.ycombinator.com/user?id=tim_sw) | [11 comments](https://news.ycombinator.com/item?id=37718368)


Google researchers have developed and deployed large-scale lightwave fabrics, using optical circuit switches (OCSes) and optical transceivers, for both datacenter networking and machine learning applications. By employing a hardware and software codesign approach, the researchers integrated these fabrics into their network and computing infrastructure. The design includes a high degree of multiplexing enabled by new wavelength-division-multiplexing (WDM) and optical circulator technologies. The result is a synchronous lightwave fabric that is reconfigurable, low-latency, rate agnostic, and highly available. The researchers report impressive results, including up to 3 times better system availability and performance improvements of up to 3.3 times compared to a static fabric. Lightwave fabrics constituted less than 6% of the total system cost.

Key Points:
- Google researchers have developed and deployed large-scale lightwave fabrics for datacenter networking and machine learning applications.
- The fabrics utilize optical circuit switches (OCSes) and optical transceivers for high-performance networking.
- A hardware and software codesign approach was used to integrate the fabrics into Google's network and computing infrastructure.
- The fabrics incorporate new wavelength-division-multiplexing (WDM) and optical circulator technologies to achieve high-bandwidth bidirectional traffic on a single strand of optical fiber.
- The result is a highly available, low-latency, and rate-agnostic synchronous lightwave fabric.
- These fabrics have provided substantial benefits for long-lived traffic patterns in datacenter networks and predictable traffic patterns in machine learning clusters.
- The researchers report impressive results, including up to 3 times better system availability and performance improvements of up to 3.3 times compared to a static fabric.
- Lightwave fabrics constituted less than 6% of the total system cost.

The discussion on this submission revolves around the use of optical circuit switching (OCS) in datacenter networking and its potential applications in machine learning workloads. 

One user points out that MEMS cross-connects can also be used to address issues related to congestion in localized network areas. Another user highlights the relevance of OCS in machine learning tasks and shares a link to a related paper discussing the approach. There is a discussion about the potential benefits of OCS in large-scale datacenters and the challenges of implementing OCSes in such environments.

Another user emphasizes the limitations of traditional electrical packet switching networks in large datacenters, such as packet blocking and difficulty in achieving disaggregated storage-compute architectures. They suggest that OCS networks can address these challenges by providing higher speeds and simplifying network topologies.

A user mentions that implementing network functions, routers, and switches in software-defined networks (SDNs) can be complex and expensive, and suggests that OCS can be a cost-effective solution for smaller players in the industry, especially in AI/ML workloads.

One user comments on the potential of optical switching in improving the efficiency and latency of high-performance computing and machine learning applications. They suggest that optical switching can replace classical physical networking systems, resulting in more efficient processing and reduced latency.

There is also a discussion about the cost and scalability challenges of implementing OCS, with one user mentioning the high cost of individual MEMS mirrors and the expensive nature of high-speed switches. Another user highlights the cost of 800Gbps switches and their potential impact on deployment.

### DoNotPay – Your AI Consumer Champion

#### [Submission URL](https://donotpay.com/) | 17 points | by [belter](https://news.ycombinator.com/user?id=belter) | [5 comments](https://news.ycombinator.com/item?id=37714743)

Introducing DoNotPay, your AI consumer champion! Tired of dealing with big corporations, bureaucracy, and hidden fees? DoNotPay is here to help. With the power of artificial intelligence, this highly motivated team builds tools to fight back and level the playing field for consumers. Whether you need to cancel subscriptions, get refunds, fight spam, or find hidden money, DoNotPay has got your back. They offer a range of features, from fighting bank fees and workplace discrimination to finding unclaimed money and securing compensation for victims of crime. Don't let big corporations take advantage of you—sign up for DoNotPay today and take charge of your consumer rights!

The discussion on this submission focuses on various concerns and experiences related to DoNotPay. One user points out that the Terms of Service (ToS) of the service seem to have concerning indemnification standards. Another user raises questions about the licensing and reproduction of text generated by the AI. One comment mentions that it is common practice to license and reproduce such AI-generated text. However, inconsistencies in the Terms of Service are pointed out, suggesting that additional terms prevail over others. Another user shares their experience of being mentally exhausted by constant clickbait articles and expresses skepticism about the legitimacy of the service. In response, another user suggests signing up for Bill.com instead. Another comment highlights concerns about the AI-generated help articles and the scraping of significant amounts of personal data, while another comment shares a link to a negative article about DoNotPay. One user jokes about their experience with upsells and difficulties with cancelling subscriptions, specifically mentioning AT&T.

### Cloudflare launches new AI tools to help customers deploy and run models

#### [Submission URL](https://techcrunch.com/2023/09/27/cloudflare-launches-new-ai-tools-to-help-customers-deploy-and-run-models/) | 127 points | by [malavwarke](https://news.ycombinator.com/user?id=malavwarke) | [18 comments](https://news.ycombinator.com/item?id=37713222)

Cloudflare has launched a new suite of products and apps dedicated to helping customers build, deploy, and run AI models at the network edge. The suite includes Workers AI, which allows customers to access GPUs hosted by Cloudflare partners to run AI models on a pay-as-you-go basis. Another offering, Vectorize, provides a vector database to store vector embeddings generated by models from Workers AI. The third product, AI Gateway, provides metrics to help customers manage the costs of running AI apps. Cloudflare CEO Matthew Prince said the launch was motivated by a desire from customers for a simpler, cost-saving AI management solution.

The discussion starts with a comment questioning the expected pricing for Cloudflare's AI products, as the pricing for different responses seems to vary significantly. Another commenter mentions that the pricing can be expensive, while another suggests that cheaper versions may not offer the same quality. One user expresses satisfaction with Cloudflare's approach, as it can lead to lower prices compared to competitors. 

Another user expresses skepticism and views Cloudflare's announcement as a marketing move. A user mentions a previous security product, and another discusses the use of Cloudflare Workers for server notifications in iOS apps.

One user highlights the fragmented pricing structure and suggests that customers currently pay less for unused virtual machines and GPUs. They also express interest in low-latency machine learning AI services and models. Another user mentions the high prices of existing Python vendors and suggests that Cloudflare's solution could be cost-effective and provide powerful quality and low latency.

A user remarks on the potential of Cloudflare AI to replicate trade and suggests trying it out. Another user discusses the current latency issue with large language models (LLMs) and mentions that even with a 10-second response time, the latency remains a challenge. They suggest that Cloudflare AI can help improve this issue.

The discussion then shifts to the specific experience of running certain models and the perceived slowness of LLMs. A user mentions that Cloudflare AI does not currently offer worker-to-worker communication and another adds that they haven't seen fast responses with LLMs. However, one user suggests that Cloudflare's offering could be promising and worth exploring. Another user asks for proof of Cloudflare's claims.

The conversation concludes with a user discussing the latency problem of LLM models and how it affects response times. They mention that Cloudflare doesn't currently offer dedicated worker-to-worker communication. Another user shares their experience with different models, suggesting that Claude 2 is faster but still slower than desired. They mention trying Etsy's solution for faster response times.

### Mistral releases ‘unmoderated’ chatbot via torrent

#### [Submission URL](https://www.404media.co/260-million-ai-company-releases-chatbot-that-gives-detailed-instructions-on-murder-ethnic-cleansing/) | 173 points | by [cainxinth](https://news.ycombinator.com/user?id=cainxinth) | [269 comments](https://news.ycombinator.com/item?id=37714703)

In a controversial move, Mistral, a $260 million AI company founded by former Google and Meta employees, has released an "unmoderated" chatbot that provides detailed instructions on murder, ethnic cleansing, and other harmful activities. The company tweeted a magnet link to a torrent file containing its publicly released language model, which can be freely downloaded and modified. The model, named Mistral-7B-v0.1, has raised concerns about safety and the lack of moderation. Mistral's approach stands in contrast to companies like OpenAI, which emphasize safeguards and moderation in their AI models. The release of Mistral's model has also ignited ideological debates within the AI community, with some praising the open approach while others advocate for stricter controls.

The discussion on this submission revolves around the controversial release of Mistral's unmoderated chatbot that provides instructions on harmful activities. Some users argue that the chatbot's content is a paraphrasing of a Wikipedia article on ethnic cleansing, suggesting that it is not necessarily producing censored prompts. Others express concerns about the potential misuse of AI for harmful purposes and the need for stricter controls. There is also a discussion on the legal implications of such a release and the responsibility of individuals who engage with the content produced by the chatbot. Some users debate the significance of personal responsibility versus the need for censorship, while others question the effectiveness of online censorship in preventing crimes. The discussion touches upon topics such as privacy, the limitations of AI models, and the potential consequences of unrestricted access to AI-generated content.

### ChatGPT-4 significantly increased performance of business consultants

#### [Submission URL](https://d3.harvard.edu/navigating-the-jagged-technological-frontier/) | 291 points | by [bx376](https://news.ycombinator.com/user?id=bx376) | [267 comments](https://news.ycombinator.com/item?id=37714343)

Harvard researchers, in collaboration with Boston Consulting Group, have conducted field experiments to study the impact of AI on knowledge worker productivity and quality. The research involved evaluating the performance of 758 consultants across various tasks, such as creativity, analytical thinking, writing proficiency, and persuasiveness. The findings revealed that the use of ChatGPT-4, an AI model, significantly improved performance in tasks within the AI frontier. Specifically, it increased speed by over 25%, human-rated performance by over 40%, and task completion by over 12%. The study identified two distinct patterns of AI use: "Centaurs," who divided and delegated tasks between themselves and the AI, and "Cyborgs," who integrated their workflow with the AI. The paper suggests that the focus should shift from a binary decision of adopting or not adopting AI, to evaluating the value of different combinations of humans and AI for various tasks in the knowledge workflow.

The discussion on Hacker News about the submission mainly revolves around the effectiveness and value of management consultants, as well as the role of AI in knowledge work and the music industry. Here are some notable points from the discussion:

- Some users expressed skepticism about the value of management consultants, suggesting that their contributions may not be worth the high fees they charge. They argue that many large consulting firms generate significant revenues while providing relatively small benefits.
- Others mentioned that the worth of management consultants depends on their frameworks and methodologies, as well as the specific insights they provide to clients. They also noted that management consulting can be a lucrative career path, with top firms like McKinsey, BCG, and Bain generating billions of dollars in annual revenue.
- There was a discussion about the music industry and the role of AI in music creation. Some users argued that the quality of music and its popularity are influenced by factors such as the quantity of songs, publicity, and connections with industry professionals. They mentioned notable producers like Max Martin and Serban Ghenea as examples of individuals who have had a significant impact on the success of artists.
- The importance of quality in music was also discussed, with some users suggesting that quality songs make people popular, while others highlighted the importance of publicity and marketing.
- The discussion touched on the general perception of management consultants, with some users expressing skepticism about their value, while others noted that they can provide valuable guidance and insights to businesses.
- The potential limitations and biases of management consultants were also mentioned, with users suggesting that excessive reliance on consultants can lead to groupthink and a lack of critical thinking within organizations.
- There were some comments about the significant amount of writing and reporting involved in management consulting, with users discussing the high fees consultants charge for producing reports that support their recommendations.

Overall, the discussion encompassed various perspectives on the value and impact of management consultants and the role of AI in knowledge work and the music industry.

### Palantir’s Reputation Stalks Its Bid for the UK’s National Health Data

#### [Submission URL](https://www.wired.com/story/palantir-nhs-data/) | 24 points | by [benjvi](https://news.ycombinator.com/user?id=benjvi) | [6 comments](https://news.ycombinator.com/item?id=37713497)

The UK's National Health Service (NHS) is reportedly planning to build a central operating system called the federated data platform that will allow patient data to move more freely within the healthcare system. The aim is to improve patient care by connecting different systems in a secure environment. However, there are concerns about the front-runner bidding to build the system, US tech company Palantir. Doctors, privacy campaigners, and politicians have expressed reservations due to Palantir's alleged involvement in controversial projects such as detaining migrants in the US and directing drone strikes in Afghanistan. Critics question whether Palantir can be trusted with the sensitive data held by the NHS.


The discussion surrounding the submission on Hacker News revolves around concerns regarding the UK government handing over critical national citizen health data to a foreign company. One user points out that this is a matter of national security and questions the decision to compromise the data of UK citizens. Another user mentions that Palantir, the frontrunner bidding to build the system, was founded with investment from the CIA, which raises further concerns. 

In response, another user shares a link to a saved version of the article that provides additional information on the topic. Another user expresses worries about privacy violations and believes that there may be potential ethical violations happening. Lastly, a user states that circumstances allow for access to the data, implying that there may be a justifiable reason for it.

### Fake News Detectors Are Biased Against Texts Generated by Large Language Models

#### [Submission URL](https://arxiv.org/abs/2309.08674) | 16 points | by [PaulHoule](https://news.ycombinator.com/user?id=PaulHoule) | [12 comments](https://news.ycombinator.com/item?id=37712713)

A new study has found that fake news detectors are biased against texts generated by large language models (LLMs). The spread of fake news has become a significant challenge, and LLMs have only intensified this issue by being able to generate highly believable fake content. The study revealed that existing detectors are more likely to flag LLM-generated content as fake news while misclassifying human-written fake news as genuine. This unexpected bias is due to distinct linguistic patterns in LLM outputs. To address this bias, the researchers introduced a mitigation strategy using adversarial training with LLM-paraphrased genuine news. This approach significantly improved the detection accuracy for both human and LLM-generated news. To encourage further research in this area, the researchers released two comprehensive datasets that combine human-validated articles with LLM-generated fake and real news.

The discussion begins with a user named Hendrikto expressing their interest in large language models (LLMs) and how they can generate both truthful statements and plausible fake news. They find it interesting that LLMs tend to flag LLM-generated content as fake news but misclassify human-written fake news as genuine. They mention that the researchers addressed this bias by using adversarial training with LLM-paraphrased genuine news, resulting in improved detection accuracy for both human and LLM-generated news. Another user, flr, suggests that this sounds like a "rabbit myth" and compares it to classifiers that mistake cloudy days for sunny ones.

User ttctcyf raises a point about the specificity of fake news sources and mentions a paper that lists different news sites categorized as either reliable or unreliable. They provide some examples of unreliable news websites labeled as conspiracy, pseudo-science, and misinformation. They also note that reliable news websites consist of mainstream and center-right sources. Another user, hlt, comments that LLMs are supposed to determine the truthfulness of things.

Then, user fllngknf states that humans can detect fake news and suggests that training LLMs to detect fake news is pointless. In response, user marginalia_nu points out the contradiction in the statement, mentioning that while humans cannot fly, machines can. Another user, ben_w, highlights the difficulty of determining truthfulness and mentions the concept of Munchausen Trilemma, where establishing perfect certainty about truth is impossible. They explain that all tests mimic system 2 thinking, which is slower than the decision-making done by humans. User marginalia_nu adds that generating labeled data holds challenges, as it may have undesired properties and can lead to different types of generated fake news. Another user, smbz, mentions the importance of human verification.

Finally, Grimblewald argues that humans have good detection of fake news through critical analysis and suggests that relying on technology alone is not sufficient for spotting fake news.

