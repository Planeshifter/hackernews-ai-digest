## AI Submissions for Sun Dec 17 2023 {{ 'date': '2023-12-17T17:10:18.549Z' }}

### BrainGPT turns thoughts into text

#### [Submission URL](https://www.iflscience.com/new-mind-reading-braingpt-turns-thoughts-into-text-on-screen-72054) | 328 points | by [11thEarlOfMar](https://news.ycombinator.com/user?id=11thEarlOfMar) | [183 comments](https://news.ycombinator.com/item?id=38673854)

Researchers at the University of Technology Sydney have developed a breakthrough mind-reading technology that can transform thoughts into words on a screen. The technology, called BrainGPT, uses electroencephalogram (EEG) signals recorded from a cap worn by users to decode their brain activity and convert it into language. Unlike previous methods that require brain implants or access to an MRI machine, BrainGPT only requires the use of an EEG cap, making it more practical and convenient. The technology has shown promising results in trials, with an accuracy score of about 0.4 according to the BLEU algorithm. This innovation could have significant implications for neuroscience and AI.

The discussion around the submission revolves around several points. One comment points out that previous research in brain-computer interfaces (BCIs) focused on helping paralyzed individuals communicate, but EEG signals are not strong enough to support fast communication speeds. Another comment mentions Neuralink, Elon Musk's project, and questions if the article's findings are similar to other BCI research. There is also discussion about the signal-to-noise ratio of EEG and the challenges it presents. Another commenter brings up a recent development in communication using brain signals and mentions that it has shown promising results. Some commenters question the validity and replicability of the technology mentioned in the article, while others discuss the limitations and potential applications of BCIs. Some comments also mention previous advancements in voice-to-text systems and compare them to the potential of this mind-reading technology. The discussion touches on topics such as the use of AI to filter unwanted thoughts, the challenges of handling electrical noise in mobile devices, and the interplay between signal processing and brain waves. There is some skepticism about the claims made in the article and the credibility of the sources used. The conversation also branches out into discussions about brain scanning devices, the potential implications for privacy and surveillance, and the use of technology for tracking thoughts or detecting criminal activity.

### Intel proposes XeGPU dialect for LLVM MLIR

#### [Submission URL](https://discourse.llvm.org/t/rfc-add-xegpu-dialect-for-intel-gpus/75723) | 82 points | by [artagnon](https://news.ycombinator.com/user?id=artagnon) | [12 comments](https://news.ycombinator.com/item?id=38675503)

Intel has proposed the addition of a new XeGPU dialect to MLIR (Multi-Level Intermediate Representation), aiming to support high-performance GEMM (General Matrix Multiply) code generation on Intel GPUs. The XeGPU dialect provides an abstraction that closely models Xe instructions and introduces XeGPU operations for cases where a special Xe instruction cannot be expressed by LLVM/SPIR-V dialect. This new dialect complements existing MLIR dialects like Arith, Math, Vector, and Memref, allowing for a smooth integration of XeGPU-based MLIR GEMM implementation with other operations. The proposal includes an example code snippet showcasing the usage of the XeGPU dialect. Intel has already implemented XeGPU in its Extension to MLIR repository and has also developed a high-performance XeGPU-based GEMM implementation, which demonstrated close-to-peak performance on Intel Max series.

The discussion on this submission includes various comments from different users. Some users express confusion regarding the technical details, such as the strange characters used in the proposed MLIR dialect and its connection to GitHub. Others discuss the potential implications of the XeGPU dialect and how it models Xe instructions. There is also a comment asking for an explanation of the work done by engineers on system programming from a higher-level perspective. Some users discuss the generalizability of compiler infrastructure and the importance of hardware-software compatibility. One user shares a link to the proposal for the XeGPU dialect, while another user asks for a correction to an incorrect topic. Additionally, there is a discussion about a common middle layer for accelerators, and users make comparisons between AMD, Nvidia, and Intel.

### Pearl: A Production-Ready Reinforcement Learning Agent

#### [Submission URL](https://github.com/facebookresearch/Pearl) | 72 points | by [da4id](https://news.ycombinator.com/user?id=da4id) | [8 comments](https://news.ycombinator.com/item?id=38671030)

Introducing Pearl: A Production-ready Reinforcement Learning AI Agent Library

The Applied Reinforcement Learning team at Meta has released a new open-source library called Pearl. This library enables researchers and practitioners to develop production-ready reinforcement learning AI agents. Pearl prioritizes cumulative long-term feedback and can adapt to environments with limited observability, sparse feedback, and high stochasticity.

With Pearl, you can build state-of-the-art reinforcement learning AI agents that can adapt to a wide range of complex production environments. The library offers various features such as dynamic action spaces, offline learning, intelligent neural exploration, safe decision making, history summarization, and data augmentation.

To get started with Pearl, simply clone the repository and install it using pip. The library provides an example of how to kick off a Pearl agent with a classic reinforcement learning environment. More detailed tutorials will be presented at NeurIPS 2023 EXPO.

If you're interested in reinforcement learning and AI, Pearl is definitely worth checking out. It's a powerful tool for building intelligent agents that can thrive in diverse and challenging environments.

The discussion about the submission "Introducing Pearl: A Production-ready Reinforcement Learning AI Agent Library" on Hacker News includes several comments. 

One user named DennisP highlights the importance of cumulative long-term feedback and how Pearl can handle environments with limited observability, sparse feedback, and high stochasticity. Another user, syngrog66, simply expresses that it is good news. 

A user named catlover76 apologizes for their question but asks for a simplified explanation of the library and its purpose. This prompts a response from adastra22, who suggests that Pearl's AI might be able to understand perceptions, actions, and goals. They explain that reinforcement learning focuses on learning tasks based on databases associated with contextual feedback. 

Another user named bwnb comments on the applications of reinforcement learning, mentioning industrial applications like walking robots, playing games like AlphaGo or AlphaChess, and more. 

B1FF_PSUVM comments on the misspelling of "Pearl" in the title. This leads to a conversation between gll and kvndmm, with gll guessing that the library may be named after Judea Pearl, and kvndmm confirming that it is indeed named "Pearl".

### WyGPT: Minimal mature GPT model in C++

#### [Submission URL](https://github.com/wangyi-fudan/wyGPT) | 62 points | by [wangyi_fudan](https://news.ycombinator.com/user?id=wangyi_fudan) | [15 comments](https://news.ycombinator.com/item?id=38670358)

Introducing wyGPT, Wang Yi's GPT (Generative Pre-trained Transformer) solution! This project represents Wang Yi's 2.5 years of hard work and optimization to create a mature and highly optimized GPT model that works exceptionally well on a single GPU. The usage is straightforward: just execute the command `make` to train on a `text_file.txt`, and then you can use the GPU or CPU to generate text based on a given prompt. 

Wang Yi has also shared a working version of the model trained on PubMed and Chinese datasets, along with the respective download links. For finetuning purposes, there is an option to iterate the model over 12 hours using the `./train` command. 

The sample text provided demonstrates the analysis of EGFR gene mutation status in NSCLC patients, highlighting the impact of the EGFR mutation on prognosis and the potential for targeted treatment. The findings suggest that EGFR gene mutations should be considered as an independent risk factor in the management of advanced NSCLC. 

Overall, wyGPT offers a powerful GPT solution that has been meticulously developed and optimized by Wang Yi.

The discussion starts with a comment from user "bt1a" who finds the text in the submission cryptic and random. Another user "nmthkd" expresses surprise at the "ccrt cmmnt." User "ie21" jokingly suggests throwing a party. User "dh" adds a playful comment saying it's a "sexy part" and mentions Neanderthals. User "LoganDark" mentions that they have been reading code for 25 years and it seems to be working properly.

User "lxs" finds the project interesting but criticizes the source code as being "gross" and not properly formatted. They suggest that compilers don't care about these things. "hllplnts" agrees, saying the code release is crass and doesn't provide much information.

User "kgst" discusses the hashing algorithm used in the project and asks what information is missing from the project's description. "hskln" comments on their efforts to find a small GPU-friendly model and mentions their ongoing training.

The conversation continues with "ttrvrs" commenting on the self-contradictory naming convention used in the code. User "shvrdnn" mentions that they love mini databases but find the formatting in this case to be uncommon. "acheong08" finds the code wizardry confusing with convoluted JavaScript. "tt567x" adds a comment as well.

### Augmenting long-term memory (2018)

#### [Submission URL](https://augmentingcognition.com/ltm.html) | 73 points | by [MovingTheLimit](https://news.ycombinator.com/user?id=MovingTheLimit) | [25 comments](https://news.ycombinator.com/item?id=38669928)

Michael Nielsen, a researcher at Y Combinator, delves into the concept of augmenting long-term memory in his essay. He begins by recounting the story of Solomon Shereshevsky, a man with an exceptional memory who could recall lengthy strings of words and numbers with ease. Nielsen explores the idea of utilizing computers as tools to enhance our memory, referencing historical proposals like Vannevar Bush's memex and Tim Berners-Lee's conception of the World Wide Web. He then delves into his personal experience with a memory system called Anki, discussing its potential for remembering a wide range of information, including research papers and books. Nielsen also emphasizes the importance of memory in problem-solving and creativity, arguing against the view that rote memory is inferior. He concludes by stating that the essay serves as a guide for developing virtuoso skills with personal memory systems.

The discussion on this submission revolves around the effectiveness and usefulness of memory systems like Anki, as well as the broader concept of augmenting long-term memory. Some users express their positive experiences with Anki and highlight its benefits in helping them remember and retain information. Others discuss the differences between Anki and SuperMemo, another memory system, and share their preferences and experiences with both. The discussion also touches on the importance of structuring and processing information for effective learning, as well as the potential of using Anki for various subjects and problem-solving. Some users mention their own projects or tools inspired by Anki, such as Reasonote and table2anki. Overall, the discussion showcases the diverse perspectives and experiences related to memory systems and their impact on learning and knowledge retention.

### AI is owned by Big Tech

#### [Submission URL](https://www.technologyreview.com/2023/12/05/1084393/make-no-mistake-ai-is-owned-by-big-tech/) | 21 points | by [srbhr](https://news.ycombinator.com/user?id=srbhr) | [5 comments](https://news.ycombinator.com/item?id=38672156)

The AI industry, particularly in generative AI, is heavily dependent on Big Tech companies like Microsoft, Amazon, and Google. Startups and research labs rely on these tech giants for computing infrastructure and market reach. Many startups even license and rebrand AI models created by these companies. This concentration of power and reliance on a few corporate actors raises concerns about democracy, culture, and security. The recent OpenAI board breakdown, where Microsoft exerted its dominance, highlights the control that Big Tech has over AI development. OpenAI made a deal to exclusively license its GPT-4 system to Microsoft in exchange for access to their computing infrastructure. This leaves few alternatives for companies wishing to build their own AI models. Regulatory challenges and concentrated chipmaking markets further limit options. While open-source AI offers some benefits, it alone cannot escape the industry concentration caused by Big Tech. Without intervention, the AI market will continue to reward these companies and deepen the divide between them and the public.

The discussion on this submission is relatively brief. Here are the main points made by the commenters:

1. One commenter suggests that the development and use of AI are heavily biased towards big tech companies, and that this concentration of power is problematic.
2. Another commenter argues that the high cost of AI technologies limits access to a select few, thus creating a disparity of opportunities.
3. Efforts to decentralize AI development and make it more accessible are mentioned, with one person recommending using crowd-sourced datasets.
4. The idea of using social media platforms as an alternative to centralized AI is briefly mentioned.
5. Finally, one comment simply says "sht," indicating dissatisfaction with the submission or the discussion.

Overall, there isn't a robust or in-depth conversation happening in this particular thread.

### First autonomous, AI-powered restaurant

#### [Submission URL](https://abc7.com/ai-restaurant-pasadena-robots/14190130/) | 11 points | by [lxm](https://news.ycombinator.com/user?id=lxm) | [10 comments](https://news.ycombinator.com/item?id=38669314)

In a landmark development for the fast food industry, a Pasadena-based company called Miso Robotics has unveiled what it claims to be the world's first fully autonomous, AI-powered restaurant. Located in Pasadena's Old Town, the CaliExpress restaurant features robots that handle the cooking process, including burger making and French fry preparation. The business utilizes artificial intelligence to ensure smooth operations. Despite the robotic automation, human employees will still be present to pack the food and provide a friendly face for customers. This innovative venture represents years of research, development, and investment in a family of companies dedicated to revolutionizing the restaurant industry. The aim is to create a restaurant experience that combines the efficiency of automation with the convenience and personal touch of human interaction.

The discussion on this submission revolves around the cost and feasibility of implementing robotic systems in restaurants. Some commenters point out that the cost of the robots, such as the $4,000 Flippy robot, along with the additional components, can add up to around $20,000. Others compare this cost to the monthly wage of a minimum-wage human employee.

There is also discussion about the potential benefits of automation, with one commenter mentioning that robots can perform tasks simultaneously and may be more cost-effective in the long term. However, another commenter argues that there are hidden costs associated with human employees, such as healthcare benefits and payroll overhead, that should be taken into consideration.

One commenter shares a video of a robotic arm flipping burgers to highlight the advancements in automation technology. Another commenter speculates about a future where robots replace all cooking processes in restaurants.

There is also a mention of McDonald's implementing automation in their restaurants, optimizing workflows and potentially reducing the need for human production. Someone else adds that McDonald's in Australia has already automated their drink filling process.

Overall, the discussion explores the cost-effectiveness, practicality, and potential impact of implementing robotics in the restaurant industry.

