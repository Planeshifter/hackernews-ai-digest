## AI Submissions for Sun Feb 04 2024 {{ 'date': '2024-02-04T17:10:12.756Z' }}

### Beyond self-attention: How a small language model predicts the next token

#### [Submission URL](https://shyam.blog/posts/beyond-self-attention/) | 411 points | by [tplrbv](https://news.ycombinator.com/user?id=tplrbv) | [72 comments](https://news.ycombinator.com/item?id=39251909)

In a recent blog post, the author shares their findings after diving deep into understanding how a small language model predicts the next token. They trained a small transformer model and wanted to explore what was happening internally to produce the model's results. The author discovered that each transformer block learns weights that associate a given prompt with a class of strings found in the training corpus. The distribution of tokens that follow those strings in the training corpus is what the block outputs as its predictions for the next token. Each block may associate the same prompt with a different class of training corpus strings, resulting in different predictions. The final transformer output is a combination of each block's predictions. The author implemented code that replicates this process and found that it produces outputs similar to the transformer model. The blog post provides a detailed walkthrough of the code implementation and presents supporting evidence for the proposed explanation. Overall, this research provides insights into how transformers make predictions and offers a unique perspective on the inner workings of language models.

The discussion on this submission covers various topics related to the article's content. Here are some key points mentioned:

- Some commenters discuss the implications of the research and the relevance of neural networks in understanding the phenomenon.
- The conversation touches on the use of language models in compression, copyright issues, and the challenges of training models with scrap internet content.
- There are also discussions about the limitations and possibilities of using discrete algorithms in optimizing models.
- The question of whether AI should be developed with energy-efficient gradient descent or specialized coprocessors is raised.
- Some commenters express confusion or lack of understanding about Shannon's treatment of systems and the relevance of mathematics in explaining neural networks.
- The topic of scraping web content for training data and the ethical implications of such actions is also explored.
- The discussion briefly delves into copyright laws and the concept of fair use in the context of AI training.
- Other comments touch on the technical aspects of implementing language models and the differences between GPT and transformers.

Overall, the discussion provides a mix of insights, questions, and debates around the research and its implications in areas such as AI development, copyright, and machine learning techniques.

### Finance worker pays out $25M after video call call with deepfake CFO

#### [Submission URL](https://edition.cnn.com/2024/02/04/asia/deepfake-cfo-scam-hong-kong-intl-hnk/index.html) | 442 points | by [bsdz](https://news.ycombinator.com/user?id=bsdz) | [309 comments](https://news.ycombinator.com/item?id=39248649)

A finance worker in Hong Kong was tricked into paying out $25 million to fraudsters using deepfake technology, according to local police. The scam involved the worker attending a video call with what he believed were colleagues, but were actually deepfake recreations. The worker's suspicions were initially aroused by a message supposedly from the company's chief financial officer, but he disregarded them after seeing seemingly familiar faces on the call. The fraudsters used AI deepfakes to trick facial recognition programs and imitate the people pictured on stolen identity cards. This incident highlights the growing concern over the sophistication and misuse of deepfake technology.

The discussion on this submission covers various aspects of the article and raises some interesting points. Here are some key points from the comments:

- One user shares an article about the similarities between this situation and the Heaven's Gate cult, where people blindly followed their leader despite being suspicious of the situation.
- Another user mentions an incident in South Korea where people were scammed by sushi restaurants pretending to be Japanese.
- Some users discuss cultural differences and how certain cultures may be more susceptible to scams due to different cultural norms.
- The discussion also touches on the practices in Japanese management culture and the use of digital signatures and stamping in official documents.
- There are debates about the role of corporate culture and individual responsibility in preventing scams.
- Some users suggest that the issue lies in the lack of verification and thorough checking in companies and transactions.
- Others point out that scammers can easily manipulate their appearance and present themselves convincingly, highlighting the need for better verification processes.
- The discussion dives into topics like communication, trust, paperwork, and different approaches to security measures in various countries.

Overall, the comments highlight the complexity of the issue and the need for improved awareness, vigilance, and security measures to combat scams facilitated by deepfake technology.

### ByteGraph: A High-Performance Distributed Graph Database in ByteDance [pdf]

#### [Submission URL](https://www.vldb.org/pvldb/vol15/p3306-li.pdf) | 17 points | by [belter](https://news.ycombinator.com/user?id=belter) | [3 comments](https://news.ycombinator.com/item?id=39254295)

Apologies, but it seems like the input you provided is a PDF document. As an AI text-based model, I am unable to process or interpret PDF files directly. If you have any text-based content or specific stories from Hacker News that you'd like me to summarize for you, please provide the relevant information, and I'll be happy to assist you.

The discussion revolves around a submission titled "ByteKV: A ByteDance Infrastructure Song," which talks about ByteKV, a metadata storage technology developed by ByteDance. However, the post highlights that the information technology landscape in China is heavily influenced by the Chinese government and that the development of ByteKV is based on FoundationDB. 

In the comments, one user shares their experience of interviewing for a position related to database technologies in China, where they mention that many developments are written in Chinese. Another user reminds everyone of similar projects like TiKV, TiDB, and SurrealDB.

### Apple fixes zero-day bug in Apple Vision Pro that 'may have been exploited'

#### [Submission URL](https://techcrunch.com/2024/01/31/apple-vision-pro-zero-day-security-bug-exploited/) | 111 points | by [arkadiyt](https://news.ycombinator.com/user?id=arkadiyt) | [78 comments](https://news.ycombinator.com/item?id=39252321)

Yesterday, Apple released a security patch for its Vision Pro mixed reality headset to address a vulnerability that may have been exploited by hackers. The vulnerability was found in WebKit, the browser engine running Safari and other web apps. If exploited, malicious code could run on the device. This is the same vulnerability that Apple patched last week for iOS devices. However, no patches were released for Apple Watch. The company did not disclose whether hackers specifically targeted the Vision Pro or who they were. Hackers often focus on WebKit vulnerabilities to gain access to users' personal data. The Vision Pro headset is set to launch on Friday.

The discussion on this submission covers various topics related to Apple's Vision Pro mixed reality headset and the vulnerability found in WebKit. Some users discuss the technical aspects of the kernel exploit and the potential safety concerns of using AR devices while driving. Others point out similarities to other AR headsets like Valve Index and share information about hypervisors and the capabilities of Apple's M2 chip. The discussion also touches on the theory of visionOS being based on iPadOS and the idea of AR devices being used to distract people while driving. Overall, the discussion provides a mix of technical and speculative perspectives on the topic.

