## AI Submissions for Fri May 16 2025 {{ 'date': '2025-05-16T17:12:50.077Z' }}

### Getting AI to write good SQL

#### [Submission URL](https://cloud.google.com/blog/products/databases/techniques-for-improving-text-to-sql) | 434 points | by [richards](https://news.ycombinator.com/user?id=richards) | [299 comments](https://news.ycombinator.com/item?id=44009848)

In the ever-evolving world of data access and retrieval, the fusion of natural language processing with SQL queries is revolutionizing the way organizations interact with their databases. Google Cloud's introduction of text-to-SQL capabilities, powered by its advanced Gemini model, marks a significant leap forward in enabling non-technical users to extract meaningful insights from complex datasets.

This pioneering technology allows users to generate SQL queries from natural language prompts, dramatically boosting productivity for developers and analysts and democratizing data access across various Google Cloud platforms like BigQuery, Cloud SQL Studio, and AlloyDB. At the helm of this development is the robust and intelligent Gemini 2.5 model, now accessible through Vertex AI, which converts human language into precise SQL queries.

In a detailed exploration, Google's Principal Software Engineer, Per Jacobsson, sheds light on the technical intricacies behind Gemini's text-to-SQL agents. The cutting-edge approaches discussed include advanced context building, table retrieval, and innovative LLM evaluation techniques to ensure high-quality SQL generation. Critical challenges are highlighted, such as the necessity for business-specific context and understanding user intent beyond the literal text.

While the capabilities of systems like Gemini are impressive, they also face hurdles. A primary challenge is adapting to various SQL dialects and ensuring precise query generation, especially as natural language can be inherently ambiguous. Furthermore, while Gemini can translate intricate questions into operational SQL, ensuring the correctness of these queries amidst different business contexts remains complex.

The technology isn't just about translating text to SQL; it's a move towards integrating practical business logic with data queries. Google's initiatives in this space don't just end at increasing accessibility but extend towards refining the technology's ability to comprehend nuanced user requests and adjust SQL outputs accordingly, paving the way for more intuitive data interactions in the future.

**Summary of Hacker News Discussion:**

The discussion revolves around Google’s Gemini AI, particularly its text-to-SQL capabilities and broader coding/writing applications. Here are the key themes and reactions:

### **1. Praise for Gemini’s Technical Capabilities**
- Users highlight Gemini’s superiority over competitors like Claude and ChatGPT in coding tasks, emphasizing its speed, context-handling, and ability to generate complex SQL queries or code snippets.  
- Some note its utility in creative writing workflows, where Gemini assists with outlining, refining drafts, and filling gaps in narratives (e.g., generating story structures or dialogue).  

### **2. Skepticism and Criticism**
- **Cost and Accessibility:** Frustration arises over Google’s pricing models, with users criticizing opaque billing and high costs for API access. Alternatives like OpenRouter are suggested for cheaper, flexible access to multiple models.  
- **Overhyped Claims:** Some dismiss Gemini’s advancements as marketing hype, arguing that incremental improvements don’t justify the fanfare. Others question its practicality in replacing human developers or writers.  
- **Ethical/Legal Concerns:** Copyright issues surface around AI-generated content and code. Users debate ownership and whether AI-assisted work qualifies for legal protection.  

### **3. Workflow Integration**
- Developers share mixed experiences: While Gemini excels in generating TypeScript/Python code or simplifying boilerplate, inconsistencies in quality (e.g., hallucinated plugins or nonsensical responses) are noted.  
- Writers describe hybrid workflows, using Gemini for brainstorming and rough drafts but relying on manual refinement for polish.  

### **4. Broader Industry Implications**
- **Competition:** Comparisons with Claude and OpenAI highlight a fragmented ecosystem, with users switching models based on task-specific performance.  
- **Corporate Priorities:** Critics argue Google prioritizes profit over developer needs, citing past product discontinuations and opaque data usage policies. Others defend Gemini as democratizing access to advanced tools.  

### **5. Humor and Meta-Comments**
- Lighthearted jokes include references to a “Canadian LLM” (polite AI) and sarcastic quips about AI’s role in replacing humans or generating “nonsense.”  

### **Conclusion**
The thread reflects polarized views: enthusiasm for Gemini’s technical prowess clashes with skepticism about its cost, reliability, and ethical implications. While many acknowledge its potential, concerns about over-reliance, corporate control, and practical limitations dominate the discourse.

### MIT asks arXiv to withdraw preprint of paper on AI and scientific discovery

#### [Submission URL](https://economics.mit.edu/news/assuring-accurate-research-record) | 354 points | by [carabiner](https://news.ycombinator.com/user?id=carabiner) | [180 comments](https://news.ycombinator.com/item?id=44006426)

In an effort to maintain academic integrity and ensure accurate research records, MIT has taken significant steps to distance itself from a controversial preprint paper titled "Artificial Intelligence, Scientific Discovery, and Product Innovation." Originally posted on arXiv in November 2024, the paper quickly raised eyebrows regarding its research integrity.

MIT conducted a confidential internal review, which concluded with a lack of confidence in the paper's data reliability and overall research veracity. Consequently, the institution has formally requested arXiv to withdraw the paper, as it might breach their Code of Conduct. However, it's important to note that typically only the paper's authors can request withdrawal, and the author has not made such a request. 

Despite the lack of peer review typical for preprints, the paper's discussion within the research community necessitated this intervention to avoid misleading academic and public discourse. Highlighting the seriousness of this issue, prominent MIT Professors Daron Acemoglu and David Autor, who were mentioned in the paper's footnotes, expressed their doubts about the paper's validity and emphasized the importance of an accurate research record. By taking these actions, MIT reiterates its commitment to research integrity and transparency.

**Summary of Discussion:**

The discussion revolves around MIT's decision to distance itself from a controversial arXiv preprint and the broader implications for academic integrity. Key points include:

1. **MIT's Actions & arXiv's Role**:  
   - Users note that arXiv typically allows only authors to withdraw papers, raising questions about MIT’s direct intervention. Some speculate the paper’s withdrawal request might involve procedural or legal nuances (e.g., impersonation via email addresses).  
   - Concerns are raised about arXiv’s policies and whether institutions can pressure platforms to retract work without author consent.  

2. **Skepticism About the Paper**:  
   - The paper’s claims (e.g., AI-driven 44% improvement in materials discovery) are criticized as implausible. Commenters with materials science expertise argue such leaps are unrealistic, suggesting fabricated data.  
   - Comparisons are made to the **LaCour scandal**, where fraudulent data in social science studies went undetected initially, highlighting the need for stricter verification in AI/ML research.  

3. **Technical Critiques**:  
   - The paper’s methodology (e.g., using GANs/diffusion models for materials innovation) is questioned for lacking technical depth. Appendices with "clean" data plots are flagged as potential red flags for fabrication.  
   - A linked **WIPO complaint** involving Corning and a suspicious domain registration (*corningresearch.com*) hints at possible corporate espionage or fraud.  

4. **Institutional Accountability**:  
   - MIT’s handling of the situation is scrutinized. Some argue the institution is deflecting blame onto a junior PhD student while protecting senior faculty (Acemoglu, Autor) mentioned in the paper.  
   - Debates arise about academic labs collaborating with private companies, with concerns over secrecy, data ownership, and conflicts of interest.  

5. **Broader Academic Culture**:  
   - Participants critique the pressure on early-career researchers to produce groundbreaking results, which may incentivize fraud.  
   - The role of seminars and conferences is discussed, with some noting that polished presentations often mask methodological flaws, perpetuating a "publish-or-perish" culture.  

6. **Email Security & Legal Concerns**:  
   - A tangential thread explores how email impersonation (e.g., using MIT-affiliated addresses) could facilitate fraudulent submissions, though most agree this is unlikely in this case.  

**Takeaway**: The discussion underscores tensions in academia around preprint credibility, institutional transparency, and the ethical challenges of AI-driven research. MIT’s response is seen as both a necessary defense of integrity and a potential overreach, reflecting broader debates about accountability in fast-moving, high-stakes fields.

### Will AI systems perform poorly due to AI-generated material in training data?

#### [Submission URL](https://cacm.acm.org/news/the-collapse-of-gpt/) | 105 points | by [pseudolus](https://news.ycombinator.com/user?id=pseudolus) | [126 comments](https://news.ycombinator.com/item?id=44010705)

Since November 2022, when ChatGPT was unleashed on the world, it has been filling the internet with AI-generated text, from mundane emails to awkward poetry. However, the rapid proliferation of such machine-made prose might be stirring trouble in the AI world, a phenomenon known as "model collapse." As these language models (LLMs) like OpenAI’s GPT 3.5 continuously evolve, they often scrape new training data from the web, including content penned by previous iterations of LLMs themselves. This mix leads to a scenario where the model's training data drifts away from the extensive, diverse dataset that's closer to real human communication, resulting in a sort of digital echo chamber of nonsense—a case of "garbage in, garbage out," reimagined for the AI age.

Stanford University's Sanmi Koyejo explains that this statistical imbalance occurs because machine-generated content deviates from the natural token distribution reflected in human discourse. This skewed dataset means subsequent AI models may be trained on text that misses the richness and diversity of human language, amplifying inaccuracies and reinforcing errors over time. Oxford's Yarin Gal highlights that with each pass of AI-generated data, rare linguistic "events" become more elusive, compounding the issue further.

But fear not, this pitfall isn't exclusive to text-generating models. Algorithms creating everything from images to predictive analyses are susceptible if they repeatedly train on AI-born data. Yet, models using synthetic data sparingly or for augmentation—like cancer-detection algorithms—remain unaffected by collapse, says Gal.

The solution may lie in thoughtful data curation. Rather than indiscriminately mixing human and machine-generated content, enhancing datasets with carefully vetted AI text can improve quality. Yet, identifying AI text on the internet remains challenging, and measures to automatically spot and filter it are under active development.

New York University's Yunzhen Feng warns that simply adding more data won't curb collapse without potentially steep computational costs. The key lies in selecting high-quality synthetic data, potentially evaluated by other AI systems to ensure accuracy and coherence.

In essence, the future of AI's learning curve may depend on our ability to sift through and refine both human and machine-made content to maintain the integrity and utility of next-generation language models.

**Summary of Hacker News Discussion on AI Model Collapse and Synthetic Data Concerns:**

1. **Model Collapse & Feedback Loops**:  
   Participants highlighted the risk of AI models (like ChatGPT) entering a downward spiral by training on their own outputs, leading to "model collapse." This occurs when synthetic data amplifies errors and reduces linguistic diversity, creating a "garbage in, garbage out" cycle. Users compared this to a digital echo chamber where AI-generated content drifts further from human language patterns.

2. **Human vs. Synthetic Content**:  
   Debate centered on whether human-created content inherently reflects "real-world" data, as humans interpret and represent reality. Some argued that even flawed human content is preferable to synthetic data, which lacks genuine reasoning. Others noted that AI-generated content is already pervasive (e.g., spam, SEO-driven articles), blurring lines between human and machine contributions.

3. **Data Quality Challenges**:  
   - **Filtering Noise**: Users discussed the difficulty of curating high-quality training data, especially as low-quality AI-generated conversations and generic human responses (e.g., "BS," "fck off") flood datasets.  
   - **Wikipedia Decline**: Concerns were raised about declining human contributions to platforms like Wikipedia, where "vandalism" and superficial edits now outweigh thoughtful, expert input.  

4. **Ethics & Privacy in Data Collection**:  
   Critiques targeted OpenAI and other companies for opaque data practices, including scraping user interactions without clear consent. Some shared steps to opt out of data collection, but skepticism remained about enforcement. Comparisons were drawn to past privacy lawsuits (e.g., Apple, LinkedIn) to underscore distrust in corporate data stewardship.

5. **Societal Impacts**:  
   - **Children and AI**: Fears were voiced about children growing up with AI companions (e.g., Alexa, ChatGPT), potentially harming social development and mental health.  
   - **Enshittification of Communication**: Users imagined a future where human-AI interactions dominate, leading to hollow, formulaic conversations that erode authenticity.  

6. **Historical Parallels**:  
   Participants likened the current AI-driven content pollution to past internet quality declines, such as Eternal September (1993) or the rise of SEO-spam. The broader cultural shift toward mass-produced, low-effort content was seen as a persistent challenge.  

7. **Proposed Solutions**:  
   - **Curated Synthetic Data**: Selectively using high-quality AI-generated text, vetted by other models, to augment training.  
   - **Human Oversight**: Prioritizing human-crafted content that captures nuanced thought processes over "finished" outputs.  
   - **Transparency**: Demanding clearer opt-out mechanisms and ethical data practices from AI companies.  

**Key Takeaway**: The discussion underscored a tension between AI's potential and its pitfalls. While synthetic data offers scalability, participants emphasized the irreplaceable value of human creativity and the urgent need for responsible data curation to prevent irreversible degradation of AI systems and societal discourse.

### Show HN: Workflow Use – Deterministic, self-healing browser automation (RPA 2.0)

#### [Submission URL](https://github.com/browser-use/workflow-use) | 65 points | by [gregpr07](https://news.ycombinator.com/user?id=gregpr07) | [21 comments](https://news.ycombinator.com/item?id=44007065)

There's exciting buzz in the world of robotic process automation (RPA) with the launch of "Workflow Use," a game-changing tool hailed as RPA 2.0. This newly introduced platform by browser-use.com aims to revolutionize the way we automate browser tasks by allowing users to create deterministic and self-healing workflows effortlessly. The project, currently in its early development stages, promises a future where you simply demonstrate tasks to the system once, and it handles the rest autonomously.

Equipped with intuitive features like recording and reusing browser interactions, the tool is also built on an enterprise-ready foundation. This provides scalability challenges like self-repairing workflows and efficient handling of variables extracted from forms - a nod to the growing demand for more reliable automation solutions.

Despite its promising outset, the developers have issued a word of caution: the project is not yet ready for production use and significant changes are on the horizon. However, tech enthusiasts and developers can still dive into the code, contribute, or test its functionalities by following the setup process outlined in the repository.

This groundbreaking tool not only captures the repetitive tasks seamlessly but also boasts future enhancements such as better large language model (LLM) fallback steps and workflow diffs, enhancing its robustness and appeal. With a modest yet vital group of contributors and an AGPL-3.0 license, Workflow Use is set to redefine browser automation's landscape. Keep an eye out as it matures and stands to potentially make monotonous browser tasks a thing of the past!

The discussion around Workflow Use highlights a mix of excitement, technical feedback, and comparisons to existing tools. Here's a concise breakdown:

### Key Themes:
1. **Positives & Potential**:
   - Users praised Workflow Use's vision of "self-healing" workflows and integration of deterministic automation with LLM fallbacks.
   - Comparisons to tools like Playwright highlight its unique focus on robustness and adaptability for E2E testing/critical workflows.
   - Chrome extension support (e.g., `nnbrwsr`) and TypeScript compatibility were noted as promising for community adoption.

2. **Challenges & Feedback**:
   - **Brittleness**: Users noted struggles with dynamic UI elements (checkboxes, multi-choice inputs), timing issues, and non-deterministic behavior in earlier prototypes. Some compared it to older RPA tools that fail on DOM changes.
   - **Clarity**: Requests for better documentation around input types, default values, and workflow error recovery.
   - **LLM Integration**: Suggestions to improve LLM prompting for contextual understanding (e.g., form fields for birthdays/countries) to reduce manual clarifications.

3. **Comparisons & Alternatives**:
   - **Healenium** and **Lavague** were cited as tools offering similar self-healing capabilities for test automation, with Workflow Use positioned as a broader automation solution.
   - **Playwright** users acknowledged its script-generation utility but highlighted Workflow Use’s potential edge in handling broken selectors via self-repair logic.

4. **Technical Contributions**:
   - Open-source contributions (e.g., PRs improving Playwright compatibility and TypeScript support) were welcomed.
   - Users proposed ideas like caching strategies, trajectory analysis, and centralizing element locators (POM/CSVs) to enhance stability.

5. **Future Directions**:
   - Interest in AI-driven element detection (e.g., Gemini’s vision models) for dynamic apps.
   - Enthusiasm for merging deterministic automation with LLMs to auto-correct workflows when layouts change (inspired by [Healenium’s approach](https://www.loom.com/share/1af87d78d6814512b17a8f949c28ef13)).

### Notable Quotes:
- *"Workflow Use's self-healing *could* relegate brittle XPath/CSS selectors to the past"*  
- *"The real winner will combine browser trajectories with LLM interpretation, not just screenshots."*  
- *"We’ve tried Playwright’s scripting 10x—Workflow Use might finally solve our maintenance headaches."*

### Conclusion:
The community sees Workflow Use as a promising evolution in RPA but stresses the need to refine self-healing logic, expand LLM context-awareness, and streamline integration with existing testing frameworks. Its open-source model and focus on adaptability position it as one to watch, especially if early adopters’ pain points are addressed.

### Sci-Net

#### [Submission URL](https://sci-hub.se/sci-net) | 268 points | by [greyface-](https://news.ycombinator.com/user?id=greyface-) | [120 comments](https://news.ycombinator.com/item?id=44004625)

In the academic world, accessing or sharing research papers can sometimes be a real challenge, especially with the recent hiccups in Sci-Hub's database updates. Enter Sci-Net, a burgeoning platform aimed at making the world of research more open and collaborative. Users can request and share unavailable research articles and access them without glitches. With its simplistic interface, users just need to drop a paper's DOI to kick off the process. If available, it will direct you to the paper; if not, you can create a new request for others to help with.

One of the standout features of Sci-Net is its unique approach to rewarding contributors with "Sci-Hub meme coins" on the Solana network, incentivizing the sharing of knowledge. This gives researchers the power to direct funds straight to those who assist, contrasting sharply with traditional publishers who maintain high-access paywalls that benefit few. While this decentralized system might seem daunting for newbies, it represents a fresh wave of democratized academic access.

This site is in active development, promising more exciting features soon. The portal not only facilitates access but promises enduring access to shared papers, making them freely available once uploaded. By fostering a spirit of shared learning and token-driven rewards, Sci-Net seeks to underline the significance of open access and community-driven academic sharing. This platform holds potential for reimagining how knowledge is accessed and valued.

**Summary of Hacker News Discussion on Sci-Net:**

The discussion around Sci-Net revolved largely around its use of cryptocurrency incentives and broader ethical, legal, and practical implications. Key points include:

1. **Cryptocurrency Debate**:  
   - Supporters highlighted **Monero** as a privacy-focused alternative to Bitcoin due to its untraceability, while critics noted Bitcoin's limitations (e.g., traceability, transaction fees, and price volatility).  
   - Skepticism arose about using "meme coins" (like Sci-Net's Solana-based tokens) for rewards, with concerns over price stability and practicality in microtransactions. Some users argued that crypto’s primary use case aligns with decentralized, pseudonymous systems like Sci-Hub.

2. **Legal and Ethical Concerns**:  
   - Users compared Sci-Net’s model to **Sci-Hub’s legal risks**, referencing Alexandra Elbakyan’s criminal charges. Discussions warned of potential copyright infringement penalties for users sharing papers.  
   - The **Aaron Swartz case** was cited as a cautionary tale, emphasizing how publishers and institutions might aggressively litigate against open-access efforts.

3. **Incentives and Effectiveness**:  
   - While some praised incentivizing knowledge sharing with crypto, others questioned whether minimal rewards would effectively motivate scientists. Critics noted existing academic pressures (e.g., publish-or-perish dynamics) and suggested researchers already share work freely when possible.  
   - Concerns were raised about Sci-Net’s ability to compete with established "gray market" systems like Sci-Hub without substantial funding.

4. **System Design Critiques**:  
   - Privacy mechanisms (e.g., watermark removal) were deemed insufficient to protect users’ identities. Critics argued that centralized platforms, even with crypto elements, could still expose contributors to legal action.  
   - Some dismissed the token model as a "pump-and-dump scheme," while others highlighted challenges like ensuring reliable, long-term access to uploaded papers.

5. **Brotherhood with Sci-Hub**:  
   - Many saw Sci-Net as complementary to Sci-Hub but stressed the need for non-commercial, community-driven funding. Users debated whether Sci-Hub’s survival hinges on donations or illicit revenue streams.

In summary, while the community largely supports open-access ideals, skepticism persists around Sci-Net’s reliance on cryptocurrency, legal sustainability, and practical impact compared to existing solutions. The discussion underscored the tension between democratizing knowledge and navigating entrenched academic publishing power structures.

### Beyond Text: On-Demand UI Generation for Better Conversational Experiences

#### [Submission URL](https://blog.fka.dev/blog/2025-05-16-beyond-text-only-ai-on-demand-ui-generation-for-better-conversational-experiences/) | 75 points | by [fka](https://news.ycombinator.com/user?id=fka) | [38 comments](https://news.ycombinator.com/item?id=44003347)

In the fast-evolving realm of AI, a fascinating development is unfolding as highlighted in Fatih Kadir Akın's recent blog post. The exploration delves into AI's potential to transcend its traditional text-only interactions through dynamically generated user interfaces (UI). The concept revolves around Large Language Models (LLMs) that can craft interactive UI components on demand, making AI interactions more efficient and accessible.

Traditionally, interacting with AI has been a text-heavy affair, posing challenges such as cognitive overload and accessibility issues. For instance, in customer service scenarios like shipping companies, relying solely on text to gather information like addresses can be inefficient. Akın's prototype tackles this by enabling AI to generate intuitive UI elements—like forms—based on the conversation's context, effectively combining conversational ease with structured data input.

The underlying mechanism involves AI interpreting the user's request, identifying the necessary tasks, and generating appropriate UI components in a structured JSON format, which the application then renders. This approach significantly enhances user experience by reducing errors, providing clarity, and supporting accessibility through familiar interface patterns.

Moreover, this AI-generated UI strategy synergizes well with Managed Content Provider (MCP) services, ensuring seamless interaction via standardized methods. Such integration promises a reduction in cognitive load, robust data validation, and a natural user experience.

Identifying key UI components—like forms, which are pivotal for collecting structured data—Akın's work opens a gateway to a future where AI not only understands but also communicates and interacts with users in visually intuitive and efficient ways, marking a significant leap forward in human-AI collaboration.

The Hacker News discussion on AI-generated dynamic user interfaces (UIs) highlights enthusiasm for the potential of LLMs to reduce cognitive overload and improve accessibility, alongside debates about practicality and implementation challenges. Key points include:

1. **Dynamic UI Benefits**:  
   - Users praise the idea of AI generating context-aware UI elements (e.g., forms, maps, dropdowns) to streamline interactions, especially in scenarios like address verification for shipping. This could replace rigid forms with adaptive interfaces that blend conversational and structured input.

2. **Technical Considerations**:  
   - Some suggest server-side logic to validate inputs and determine appropriate UI components, ensuring data accuracy while maintaining flexibility. Others reference research (e.g., Microsoft’s work on prompt-driven UX refinement) as progress toward context-aware AI interfaces.

3. **Cultural and Historical Context**:  
   - Comparisons to *Star Trek*’s LCARS interface highlight the appeal of futuristic, minimalist designs, though users note real-world usability challenges (e.g., "pressing the wrong button" in high-stakes scenarios).

4. **Skepticism and Challenges**:  
   - Concerns include the cost of developing LLMs capable of generating UIs, potential over-reliance on text prompts, and the difficulty of translating natural language into structured data. Some argue traditional buttons/forms remain more reliable for now.

5. **Future Directions**:  
   - Optimists envision AI creating "on-demand" UIs that adapt to user needs in real time, integrating multimodal interactions (voice, text, visuals). Others stress the importance of balancing AI flexibility with user familiarity and semantic clarity.

Overall, the discussion reflects excitement about AI’s potential to revolutionize human-computer interaction but acknowledges hurdles in usability, cost, and technical execution.

### Ollama's new engine for multimodal models

#### [Submission URL](https://ollama.com/blog/multimodal-models) | 348 points | by [LorenDB](https://news.ycombinator.com/user?id=LorenDB) | [83 comments](https://news.ycombinator.com/item?id=44001087)

Exciting news from Ollama as they unveil a potent new engine for multimodal models, enhancing their platform with robust capabilities for processing vision data. This release kicks off with several cutting-edge vision models including Meta's Llama 4, Google's Gemma 3, and Qwen 2.5 VL, which are set to deliver more accurate general multimodal understanding and reasoning. One standout model, Llama 4 Scout, employs a mixture-of-experts approach with a whopping 109 billion parameters, demonstrating its prowess by addressing location-based queries about images such as identifying landmarks, estimating distances, and suggesting travel routes.

In a captivating demonstration, Ollama's system showcased its ability to analyze multiple images at once with the Gemma 3 model. Users can input several pictures and receive insights about relationships between them, such as identifying common subjects—like a playful instance of a boxing match between a llama and a dolphin, where the llama stands as the surprise victor thanks to its depicted punching prowess!

To bolster these capabilities, the new engine enhances model modularity and reliability, allowing models to operate independently without cross-dependencies, thus simplifying the integration process for developers. This modular design elegantly isolates model functions, making it easier for creators to innovate without entangling systems. Supported by the GGML tensor library, Ollama lays the foundation for future expansions into other modalities like speech, image, and video generation, positioning itself at the cutting edge of AI.

This update marks a significant leap forward for Ollama, affirming their commitment to advancing the field of AI with powerful, user-friendly multimodal models, while engaging their partners and the open-source community to contribute to a future-ready platform.

The Hacker News discussion around Ollama's new multimodal engine highlights technical debates, concerns about open-source collaboration, and interest in expanded capabilities:

1. **Integration & Credit Controversy**:  
   Users debate whether Ollama meaningfully contributes back to the **llama.cpp** project (which it builds upon) or merely copies code. Critics argue Ollama’s Go-based server binary appears to reuse **llama.cpp** components without clear upstream contributions, sparking frustration about transparency and credit. Some note historical friction, like unresolved issues around proper attribution.

2. **Technical Implementation Challenges**:  
   - **Gemma 3’s sliding window attention** is flagged as a point of divergence: Ollama supports it, but **llama.cpp** currently does not.  
   - Contributors highlight the complexity of adding multimodal features (e.g., image processing) to C++-based **llama.cpp**, requiring deep expertise. Skepticism arises about Ollama’s ability to upstream changes due to language barriers (Go vs. C++) and modular design choices.  

3. **Community Dynamics**:  
   - Some defend Ollama’s UX improvements for local LLM deployment, acknowledging its value despite dependency concerns.  
   - Others criticize rushed PRs and “silly templates” in open-source AI projects, advocating for systematic, collaborative development.  

4. **Interest in Video & Advanced Multimodal Features**:  
   Users express curiosity about **Qwen2.5-Omni**’s video input support and potential for tasks like object segmentation. However, early tests suggest limited functionality in Ollama’s current implementation.  

5. **Broader Ecosystem Tensions**:  
   The thread reflects wider open-source challenges: balancing innovation with upstream contributions, navigating technical debt, and ensuring credit for foundational projects like **llama.cpp** and **GGML**.  

In summary, while excitement exists for Ollama’s vision capabilities, the discussion underscores friction around collaboration practices and technical hurdles in evolving multimodal AI ecosystems.

### After months of coding with LLMs, I'm going back to using my brain

#### [Submission URL](https://albertofortin.com/writing/coding-with-ai) | 343 points | by [a7fort](https://news.ycombinator.com/user?id=a7fort) | [207 comments](https://news.ycombinator.com/item?id=44003700)

In a thought-provoking personal reflection titled "After months of coding with LLMs, I'm going back to using my brain," one developer recounts their journey of initially embracing AI for software development, only to find themselves reconnecting with more traditional approaches. The allure of rapid coding, fueled by advanced language model tools like Claude and Gemini, initially seemed like the perfect solution for rebuilding the infrastructure of a burgeoning SaaS business. However, as time went on, the developer realized the chaos and inconsistency introduced by the over-reliance on AI-generated code.

By diving deeper into Go and Clickhouse, and taking a meticulous audit of the AI-written code, the developer discovered a disjointed mess reminiscent of uncoordinated teamwork by junior developers. This awareness prompted a shift back to hands-on coding, prioritizing personal expertise and understanding over speed. Through this transition, debugging and code comprehension have improved, easing the project's progress.

The piece touches on a larger concern about the potential downsides of AI reliance—diminishing human problem-solving skills and mental sharpness. This led the developer to adopt a balanced approach, using AI for mundane tasks like renaming parameters or converting pseudo code, while leaning on personal knowledge and pen-and-paper methods for creativity and strategy.

The narrative highlights a cautionary tale about technology's place in development, advocating for a harmonious blend where AI acts as a helpful assistant rather than a substitute for human ingenuity. By becoming the senior developer of their own project, with AI as a capable assistant, the developer has found a happy medium, free from the frustration experienced before.

The discussion surrounding the developer's shift away from LLMs highlights nuanced views on AI's role in coding:

1. **Experience Matters**: Junior developers may find LLMs (like Cursor, Copilot) transformative, accelerating tasks such as scaffolding code or exploring new frameworks, particularly in environments tolerant of bugs. However, seasoned developers with strict standards often find LLMs hinder productivity due to the time spent correcting AI-generated code and navigating vague or flawed outputs.

2. **Tool, Not Replacement**: Participants emphasize LLMs as unreliable "assistants"—useful for boilerplate, pseudo-code conversion, or navigating legacy systems, but insufficient for critical design decisions. They require human oversight, akin to how compilers or linters are used cautiously.

3. **Learning Trade-offs**: Concerns arise that juniors over-relying on LLMs might stunt their problem-solving skills, as AI shortcuts deprive them of deeper understanding. Conversely, veterans leverage LLMs for tedious tasks, freeing mental bandwidth for architecture and creativity.

4. **Code Quality vs. Speed**: LLMs can generate code quickly, but it often lacks cohesion, leading to technical debt. Experienced developers stress the irreplaceable value of hands-on coding for maintainability and strategic alignment, especially in complex systems.

5. **Workflow Integration**: Effective use hinges on context. While some tout LLMs as revolutionary, others argue their value depends on integration into existing workflows—e.g., generating draft code for later refinement, not end-to-end solutions.

The consensus: LLMs are double-edged swords. Their productivity boosts are real but context-dependent, demanding discernment to avoid sacrificing code quality and learning opportunities. Balancing AI assistance with human expertise remains key.
