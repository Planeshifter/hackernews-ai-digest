## AI Submissions for Wed Aug 23 2023 {{ 'date': '2023-08-23T17:10:42.967Z' }}

### Show HN: Dataherald AI – Natural Language to SQL Engine

#### [Submission URL](https://github.com/Dataherald/dataherald) | 184 points | by [aazo11](https://news.ycombinator.com/user?id=aazo11) | [94 comments](https://news.ycombinator.com/item?id=37240363)

Dataherald is an open-source project that aims to make querying structured data easier and more user-friendly. Built on the latest language models, Dataherald allows users to ask questions in plain English and receive SQL queries as a response. The project, which is currently under development, offers several key features. It is designed to be modular, allowing different implementations of core components to be easily plugged in. It also includes best-in-class implementations for components like text-to-SQL conversion and evaluation. Additionally, Dataherald is easy to set up and use with major data warehouses, and it is designed to improve over time as it gains more usage.

Dataherald can be used in various scenarios, such as enabling business users to access insights from data warehouses without the need for a data analyst, integrating question-answering capabilities into SaaS applications, or creating chatbot plugins based on proprietary data. To get started with Dataherald, users can either sign up for the hosted version or self-host the engine locally using Docker. The engine uses MongoDB to store application data by default, and the setup process involves configuring environment variables and generating an encryption key. Overall, Dataherald aims to make querying structured data more accessible and intuitive, bringing the power of natural language processing to the realm of data analysis and exploration.

The comments on Hacker News regarding the Dataherald project mainly discussed the benefits and limitations of natural language-to-SQL engines 
One user commented that the value of such a product is in question because it may generate incorrect or irrelevant SQL queries. They suggested that Bob, the product manager, may not fully understand the data model or the language model being used. Another user expressed skepticism about using large language models for practical applications like consulting work. They mentioned that CEOs and PMs often lack knowledge in SQL and struggle with technical literacy.

In response, another user pointed out that using natural language processing can help non-technical users understand and interact with databases. They also mentioned using an API to generate SQL queries in real-time. The discussion also touched on other similar projects, such as Olympe and Ada, which are natural language-to-SQL solutions. Some users shared their experiences and difficulties with these projects. A few users mentioned alternative tools and frameworks that handle queries to databases, such as Hasura, PostgREST, and SQLize. There was also a discussion about the limitations and challenges of applying natural language processing to SQL queries. Security concerns were raised, as well as the importance of providing examples and clear documentation. Overall, the discussion highlighted the potential benefits of natural language-to-SQL engines for non-technical users but also raised concerns about accuracy, security, and the need for proper education and documentation.

### ChatGPT turned generative AI into an “anything tool”

#### [Submission URL](https://arstechnica.com/ai/2023/08/how-chatgpt-turned-generative-ai-into-an-anything-tool/) | 197 points | by [CharlesW](https://news.ycombinator.com/user?id=CharlesW) | [206 comments](https://news.ycombinator.com/item?id=37236027)

The chief technology officer of a robotics startup recently discovered that they could use a pre-trained AI model, ChatGPT, to control their robots without the need for specialized training. This highlights a shift towards general-purpose AI models that can be used across various applications, rather than being limited to specific domains. The key to this development is making large language models (LLMs) more responsive to human interaction. For example, OpenAI's work on ChatGPT involved modifying LLMs like GPT3 to improve their conversational capabilities. As a result, newer versions like GPT3.5 and GPT4 can be utilized as powerful, general-purpose information-processing tools that don't rely on the original training data or applications. This shift opens up opportunities for AI to become a "multiplying tool" that enhances human productivity. However, it's important to recognize that AI is not all-powerful and should be subject to appropriate processes and procedures.

The discussion on this submission mainly revolves around concerns and debates related to the use of AI models like ChatGPT and the impact they may have on different fields and industries. Here are some key points highlighted in the comments:
- There are concerns about the long-term sustainability of relying on AI models like ChatGPT. Some users mention that using large models like GPT3 for extended periods of time can be resource-intensive and energy-consuming. They raise the question of whether AI models can be cost-effective in the long run.
- Others express worries about the potential negative consequences of relying too heavily on AI. They argue that as AI takes over certain tasks, it may result in the loss of skills and jobs, leading to a decline in overall productivity.
- Some users discuss the ethics of using AI models and the potential for theft of intellectual property. They raise concerns about the unauthorized use of copyrighted material and the need for appropriate legal frameworks to protect artists and creators.
- The discussion also touches on the limitations and capabilities of AI models compared to human creativity. Some users highlight that AI models can generate vast amounts of content but lack the depth and originality of human artists. They emphasize the importance of human inspiration and the irreplaceability of human creativity in the arts.
- There are discussions about the role of AI in compensating creators and artists. Users question whether AI models should be used to generate copyrighted works without proper financial rewards for content creators.
- The debate expands to discuss broader topics like the current copyright system and the need for laws that benefit humans rather than favoring corporations. Some argue that existing laws and systems need to evolve to address the challenges and opportunities presented by AI models.

Overall, the discussion reflects a mix of enthusiasm for the potential of AI models like ChatGPT and concerns about their implications for various industries and intellectual property rights.

### AI Nutrition Fact Labels

#### [Submission URL](https://nutrition-facts.ai) | 90 points | by [maxwell](https://news.ycombinator.com/user?id=maxwell) | [22 comments](https://news.ycombinator.com/item?id=37239455)

Twilio, a communications platform, has outlined its principles for building AI products. These principles include transparency, responsibility, and accountability. The company believes in being transparent about the usage of AI and giving customers control. They also prioritize selecting responsible AI vendors with a focus on privacy, data security, and bias. Additionally, Twilio monitors the use of AI to address any potential harms and ensure fitness for purpose. To help customers understand the AI features they offer, Twilio has created "AI Nutrition Fact" labels. These labels provide details about the features, such as the model type, data privacy, data deletion, human involvement, compliance, and other resources. By providing this information, Twilio aims to empower customers to make informed decisions about adopting AI-powered capabilities.

The discussion on this submission covers various aspects of the topic. Here are the key points:

- Some users compare Twilio's AI Nutrition Fact labels to Apple's privacy labels, emphasizing the importance of objective information and government regulations.
- A few users debate whether the term "nutrition labels" is fitting for AI products, with some questioning its relevance.
- One user mentions a similarity to the AI FactSheets project, which aims to increase transparency and understanding of AI.
- The concept of using metaphors, such as food or nutrition, to explain AI is discussed, with a mention of a metaphor involving receipts.
- Some users express concerns about the effectiveness and comprehension of AI labels and suggest alternative approaches.
- Users highlight the autonomy and control provided to customers through Twilio's API, praising the flexibility and building blocks it offers.
- The need for strict regulatory standards and punishments for false information is mentioned.
- A user suggests that the topic of AI nutrition labels is gaining attention due to the potential financial stakes involved for Silicon Valley companies.
- There are mentions of the importance of privacy in the context of AI and the application of nutrition labeling to AI products.

Overall, the discussion covers a range of opinions on the effectiveness, relevance, and regulation of AI nutrition labels, as well as alternative approaches and concerns related to AI transparency and privacy.

### Princeton ‘AI Snake Oil’ authors say GenAI hype has ‘spiraled out of control’

#### [Submission URL](https://venturebeat.com/ai/princeton-university-ai-snake-oil-professors-say-generative-ai-hype-has-spiraled-out-of-control/) | 103 points | by [CharlesW](https://news.ycombinator.com/user?id=CharlesW) | [93 comments](https://news.ycombinator.com/item?id=37243354)

Princeton University professor Arvind Narayanan and his Ph.D. student Sayash Kapoor are working on a book that tackles the hype and misconceptions surrounding generative AI. They initially started the "AI Snake Oil" project to address the issues of predictive AI, but the rapid progress and consumer adoption of generative AI has prompted them to refocus their efforts. While they acknowledge the power and usefulness of generative AI, they also highlight the risks, the harmful consequences, and the need for responsible development practices. They urge people to be mindful of the hype and to use their collective power to bring about positive change. The project has received positive feedback from the academic and entrepreneurial communities, with some criticisms helping shape their arguments and refine their thinking. Despite concerns over labor exploitation, the authors advocate for policy changes rather than complete avoidance of generative AI.

The discussion on this submission revolves around various aspects of the topic, including criticisms of the authors' claims and arguments, comparisons with other technologies like blockchain and cryptocurrency, and different perspectives on the impact and potential of generative AI. One commenter challenges the claims made in the article, questioning the evidence presented and suggesting that the authors may have reached conclusions beyond what the article supports. Another commenter suggests that the comparison between generative AI and other technologies like blockchain is not valid, as the hype surrounding them is driven by marketing and buzzwords. There is also a discussion about the extent to which generative AI can solve problems and whether it is worth the investment. Some commenters argue that certain technologies, like blockchain, have not lived up to their promises, while others defend the potential of generative AI and its ability to transform various industries. The discussion also touches on the popularity of platforms like Roblox among kids, the potential of cryptocurrencies for investment purposes, and the complexities of copyright protection in the context of AI-generated content. Overall, the discussion highlights the diverse perspectives and debates surrounding generative AI, with commenters expressing both skepticism and optimism about its capabilities and implications.

### AI predicts certain esophageal and stomach cancers three years before diagnosis

#### [Submission URL](https://www.michiganmedicine.org/health-lab/ai-can-predict-certain-forms-esophageal-and-stomach-cancer) | 130 points | by [thunderbong](https://news.ycombinator.com/user?id=thunderbong) | [39 comments](https://news.ycombinator.com/item?id=37235578)

A team of researchers led by Joel Rubenstein, M.D., M.S., has developed an artificial intelligence tool called K-ECAN that can predict certain forms of esophageal and stomach cancer at least three years prior to a diagnosis. The tool uses basic information already available in electronic health records, such as patient demographics, weight, previous diagnoses, and routine laboratory results, to determine an individual's risk of developing esophageal adenocarcinoma (EAC) and gastric cardia adenocarcinoma (GCA). By identifying people at an elevated risk, the tool can help facilitate early detection and preventative measures. The researchers hope to integrate K-ECAN into electronic health records to alert providers about patients who are at an increased risk of developing these types of cancer. The study was funded by the Department of Defense and the National Institutes of Health.

Discussion Summary:
- One user points out that the study seems to be more about statistics and machine learning than actual breakthroughs in cancer diagnosis. They mention that the study analyzes risk factors and correlates with cancer, but it does not offer much new insight.
- Another user agrees, stating that machine learning and AI are often used as buzzwords without significant results.
- A discussion arises about the accuracy of the tool and the importance of accuracy as a metric. One user argues that accuracy can be a misleading metric and emphasizes the importance of other factors such as the area under the ROC curve (AUC) to evaluate performance.
- Another user adds that accuracy is meaningless without understanding the context and suggests that there is strong evidence for specific data points being associated with stomach cancer.
- Some users express skepticism about the AI tool's ability to accurately predict cancer diagnoses. They mention that accuracy rates reported for AI diagnosis are often low and caution against overestimating the capabilities of AI in this area.
- One user suggests that early detection of cancer may not always lead to better outcomes and cites examples of personal experiences where cancer was diagnosed at later stages despite symptoms being present earlier.
- Several users discuss the importance of lifestyle changes and other risk factors for cancer prevention, such as smoking and specific diet choices.
- A few users mention that obesity is a major risk factor for certain types of cancer and criticize the emphasis on technology as a solution rather than addressing the underlying problems.
- The discussion ends with users sharing videos and articles about esophageal and stomach cancer and discussing the relationship between obesity, metabolic syndrome, and cancer.
Overall, the discussion revolves around the usefulness and accuracy of the AI tool for predicting cancer, the limitations of relying solely on technology for diagnosis, and the importance of considering lifestyle factors in cancer prevention.

### Denuvo security now on Switch, including new tech to block PC Switch emulation

#### [Submission URL](https://www.videogameschronicle.com/news/denuvo-security-is-now-on-switch-including-new-tech-to-block-pc-switch-emulation/) | 25 points | by [kotaKat](https://news.ycombinator.com/user?id=kotaKat) | [14 comments](https://news.ycombinator.com/item?id=37242491)

Security software company Denuvo has become the first security partner to join the Nintendo Developer Portal, offering its protection technology to Switch developers. The first tool being offered to Switch developers is the Nintendo Switch Emulator Protection, which will block the ability to play Switch games on PC emulators. Nintendo has been trying to prevent Switch emulation on PC for some time, issuing DMCA takedown requests to remove homebrew tools designed to play Switch games on an emulator. Denuvo's technology can integrate seamlessly into the game development process and block gameplay on emulators. This move aims to increase revenue for studios during the game launch window by ensuring that players have to buy legitimate copies of the game.

The discussion on this submission is quite varied. One user, jstrfsh, made a satirical comment about Bowser, the villain from the Mario series, being sued and paying damages for copyright infringement and kidnapping. Another user, SOLAR_FIELDS, responded with a comment about corporate responsibility and the need to hold individuals accountable.

Another user, brucethemoose2, wonders if Nintendo trusts Denuvo's security measures, considering the prevalence of cracks for Denuvo-protected games. User dprctv adds that it would be ideal if people stopped buying low-quality products from companies like Sega, but there is still demand due to millennial nostalgia.

The conversation shifts to the performance capabilities of the Nintendo Switch. Some users, like trhls, comment that the Switch's CPU is not as powerful as other modern gaming consoles, and therefore, demanding games may not run as well. Denuvo's DRM is also criticized for potentially exacerbating the performance issues. Another user, snkr, brings up the fact that some games released on the Switch are already playable on PC through workarounds that bypass the console's protections, questioning the necessity of Denuvo's solution. SOLAR_FIELDS mentions the limitations of the Switch platform, especially in terms of character restrictions and the extra work it requires for game development.

### The False Promises of Tesla’s Full Self Driving

#### [Submission URL](https://www.theverge.com/2023/8/23/23837598/tesla-elon-musk-self-driving-false-promises-land-of-the-giants) | 32 points | by [ra7](https://news.ycombinator.com/user?id=ra7) | [14 comments](https://news.ycombinator.com/item?id=37242435)

In the latest episode of Land of the Giants: The Tesla Shock Wave, The Verge explores the false promises of Tesla's Full Self-Driving (FSD) feature. Back in 2016, Elon Musk announced that all Tesla vehicles would come equipped with the hardware necessary for full self-driving capabilities. However, the reality is that Tesla has yet to deliver on that promise. While they have released an advanced driver-assist system called FSD beta, the idea of a Tesla owner being able to nap in their car while it drives itself is far from becoming a reality. In fact, there have been hundreds of crashes involving Tesla vehicles using FSD and Autopilot, as well as multiple deaths. Government agencies are investigating Tesla's claims around self-driving, and a major recall could be on the horizon. The episode features firsthand testing of FSD and other autonomous vehicles, interviews with experts and former Tesla employees, and insights from competitors like General Motors-backed Cruise. The Verge's investigation raises questions about whether Tesla will be able to fulfill its promises without rethinking its hardware strategy.

The discussion revolves around the skepticism surrounding Tesla's Full Self-Driving (FSD) feature and the safety concerns associated with it. One commenter highlights that many people, including technical experts, wrongly believe that Tesla is leading the self-driving industry when in reality, they have not delivered on their promises. Another commenter points out that Waymo and Cruise have significantly more miles logged with fewer incidents compared to Tesla. They argue that Tesla's FSD system requires constant monitoring and is not as advanced as Waymo and Cruise. However, another commenter disagrees and mentions that many people enjoy using FSD and understand its limitations. The discussion also covers the issue of Tesla's safety reports, with one commenter asserting that they consist of unverified crash rates and misleading statistics. They suggest that Tesla's marketing department manipulates the data to present an incomplete safety profile. In contrast, Waymo's reports are said to provide detailed analysis of crash differences. Another commenter brings up multiple instances of Tesla's safety defects, such as failing to recognize road signs and critical deficiencies in its Driver Monitoring System. They argue that these issues persist despite several months of demonstrations and highlight the importance of addressing safety concerns in a self-driving product. One commenter expresses appreciation for the Dawn Project's comprehensive coverage, pointing out that their videos showcase the discrepancies between Tesla's claims and the reality of the FSD system.

