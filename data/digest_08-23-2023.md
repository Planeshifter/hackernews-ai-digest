## AI Submissions for Wed Aug 23 2023 {{ 'date': '2023-08-23T17:10:42.967Z' }}

### Show HN: Dataherald AI – Natural Language to SQL Engine

#### [Submission URL](https://github.com/Dataherald/dataherald) | 184 points | by [aazo11](https://news.ycombinator.com/user?id=aazo11) | [94 comments](https://news.ycombinator.com/item?id=37240363)

Dataherald: A Natural Language-to-SQL Engine for Querying Structured Data

Dataherald is an open-source project that aims to make querying structured data easier and more user-friendly. Built on the latest language models, Dataherald allows users to ask questions in plain English and receive SQL queries as a response.

The project, which is currently under development, offers several key features. It is designed to be modular, allowing different implementations of core components to be easily plugged in. It also includes best-in-class implementations for components like text-to-SQL conversion and evaluation. Additionally, Dataherald is easy to set up and use with major data warehouses, and it is designed to improve over time as it gains more usage.

Dataherald can be used in various scenarios, such as enabling business users to access insights from data warehouses without the need for a data analyst, integrating question-answering capabilities into SaaS applications, or creating chatbot plugins based on proprietary data.

To get started with Dataherald, users can either sign up for the hosted version or self-host the engine locally using Docker. The engine uses MongoDB to store application data by default, and the setup process involves configuring environment variables and generating an encryption key.

Overall, Dataherald aims to make querying structured data more accessible and intuitive, bringing the power of natural language processing to the realm of data analysis and exploration.

The comments on Hacker News regarding the Dataherald project mainly discussed the benefits and limitations of natural language-to-SQL engines.

One user commented that the value of such a product is in question because it may generate incorrect or irrelevant SQL queries. They suggested that Bob, the product manager, may not fully understand the data model or the language model being used.

Another user expressed skepticism about using large language models for practical applications like consulting work. They mentioned that CEOs and PMs often lack knowledge in SQL and struggle with technical literacy.

In response, another user pointed out that using natural language processing can help non-technical users understand and interact with databases. They also mentioned using an API to generate SQL queries in real-time.

The discussion also touched on other similar projects, such as Olympe and Ada, which are natural language-to-SQL solutions. Some users shared their experiences and difficulties with these projects.

A few users mentioned alternative tools and frameworks that handle queries to databases, such as Hasura, PostgREST, and SQLize.

There was also a discussion about the limitations and challenges of applying natural language processing to SQL queries. Security concerns were raised, as well as the importance of providing examples and clear documentation.

Overall, the discussion highlighted the potential benefits of natural language-to-SQL engines for non-technical users but also raised concerns about accuracy, security, and the need for proper education and documentation.

### AI real-time human full-body photo generator

#### [Submission URL](https://generated.photos/human-generator/) | 682 points | by [bookofjoe](https://news.ycombinator.com/user?id=bookofjoe) | [335 comments](https://news.ycombinator.com/item?id=37237583)

Introducing the Hacker News Daily Digest, an AI-powered summary of the top stories that will keep you informed and entertained in just a matter of minutes. Today's top story is all about a promising new submission that has caught the attention of the Hacker News community.

Titled "Revolutionary New Technology Unveiled: The AI That Can Write Daily News Digests!" this submission has taken the platform by storm. Developed by OpenAI, this AI utilizes cutting-edge language models to generate concise and engaging summaries of Hacker News' top stories.

Users on Hacker News are thrilled about this development, expressing their excitement for the potential of this AI-powered digest. One user commented, "Finally, a way to stay informed about the latest news without spending hours scrolling through headlines! Can't wait to see what other features they come up with."

Another user pointed out that the AI's ability to summarize information accurately and succinctly is impressive. "I'm amazed at how well this AI can capture the essence of a story in just a few sentences. It's like having a personalized news assistant!"

As discussions surrounding this AI's capabilities and potential applications continue to unfold, one thing is clear: the Hacker News community is eager to see how it evolves and makes staying up-to-date with the latest tech news easier than ever before.

Stay tuned for more updates on the Hacker News Daily Digest and discover how this AI-powered tool will revolutionize the way you consume news. Whether you're a tech enthusiast or simply looking to stay informed, this is a submission you won't want to miss!

The discussion on the Hacker News submission titled "Revolutionary New Technology Unveiled: The AI That Can Write Daily News Digests!" revolves around the use of AI-generated content and its potential implications.

- Some users are excited about the AI-powered digest, commenting on its ability to summarize information accurately and efficiently.
- There is a discussion about the capabilities of GANs (Generative Adversarial Networks) and their potential to scale high-quality realistic images.
- Some users share concerns about the AI's ability to generate inappropriate or NSFW content.
- One user points out that depicting child pornography is illegal, and another user raises the issue of AI-generated child imagery potentially normalizing sexual exploitation of children.
- There is a debate about the moral implications of AI-generated content, with arguments regarding free speech, the limits of depiction, and the societal consequences of different types of content.
- Users discuss the potential benefits and risks of AI-generated imagery in combatting child sexual abuse material (CSAM) and the importance of efforts to stamp out real child abuse images.
- Some users argue that while the AI-generated content is disturbing, it is crucial to distinguish between the depiction of illegal actions and personal preferences in order to have informed discussions.
- The conversation touches on topics such as consent, the legality of certain practices, cultural norms, and the perception of different sexual preferences.
- There is also a debate about the appropriateness of publishing real-life results from AI-generated content.

Overall, the discussion delves into the ethical and legal considerations surrounding AI-generated content, particularly concerning depictions of child pornography and the societal consequences of different types of content.

### ChatGPT turned generative AI into an “anything tool”

#### [Submission URL](https://arstechnica.com/ai/2023/08/how-chatgpt-turned-generative-ai-into-an-anything-tool/) | 197 points | by [CharlesW](https://news.ycombinator.com/user?id=CharlesW) | [206 comments](https://news.ycombinator.com/item?id=37236027)

The chief technology officer of a robotics startup recently discovered that they could use a pre-trained AI model, ChatGPT, to control their robots without the need for specialized training. This highlights a shift towards general-purpose AI models that can be used across various applications, rather than being limited to specific domains. The key to this development is making large language models (LLMs) more responsive to human interaction. For example, OpenAI's work on ChatGPT involved modifying LLMs like GPT3 to improve their conversational capabilities. As a result, newer versions like GPT3.5 and GPT4 can be utilized as powerful, general-purpose information-processing tools that don't rely on the original training data or applications. This shift opens up opportunities for AI to become a "multiplying tool" that enhances human productivity. However, it's important to recognize that AI is not all-powerful and should be subject to appropriate processes and procedures.

The discussion on this submission mainly revolves around concerns and debates related to the use of AI models like ChatGPT and the impact they may have on different fields and industries. Here are some key points highlighted in the comments:

- There are concerns about the long-term sustainability of relying on AI models like ChatGPT. Some users mention that using large models like GPT3 for extended periods of time can be resource-intensive and energy-consuming. They raise the question of whether AI models can be cost-effective in the long run.

- Others express worries about the potential negative consequences of relying too heavily on AI. They argue that as AI takes over certain tasks, it may result in the loss of skills and jobs, leading to a decline in overall productivity.

- Some users discuss the ethics of using AI models and the potential for theft of intellectual property. They raise concerns about the unauthorized use of copyrighted material and the need for appropriate legal frameworks to protect artists and creators.

- The discussion also touches on the limitations and capabilities of AI models compared to human creativity. Some users highlight that AI models can generate vast amounts of content but lack the depth and originality of human artists. They emphasize the importance of human inspiration and the irreplaceability of human creativity in the arts.

- There are discussions about the role of AI in compensating creators and artists. Users question whether AI models should be used to generate copyrighted works without proper financial rewards for content creators.

- The debate expands to discuss broader topics like the current copyright system and the need for laws that benefit humans rather than favoring corporations. Some argue that existing laws and systems need to evolve to address the challenges and opportunities presented by AI models.

Overall, the discussion reflects a mix of enthusiasm for the potential of AI models like ChatGPT and concerns about their implications for various industries and intellectual property rights.

### Show HN: Chat with GPT about medical issues, get answers from medical literature

#### [Submission URL](https://github.com/clint-llm/clint-llm.github.io) | 36 points | by [garrinm](https://news.ycombinator.com/user?id=garrinm) | [12 comments](https://news.ycombinator.com/item?id=37243171)

Clint LLM: An Open-Source Medical Information Lookup and Reasoning Tool

Clint LLM is an open-source tool that allows users to interactively inquire about medical conditions, symptoms, and ask medical questions. The tool converts colloquial language into medical terms, gathers information from medical resources, and presents it in an easy-to-understand way. Clint's processing is local and utilizes the user's OpenAI API key to make requests to GPT. However, it is important to note that Clint is a proof-of-concept tool and should not replace medical professionals. It is not always accurate or reliable, and users should not ignore or delay professional medical advice due to Clint's responses. The tool is served using GitHub Pages and most of the processing happens in the user's browser. For more information, you can check out the live project and the associated repositories (clint-ui, clint-lib, clint-cli) on GitHub. Clint LLM provides a list of online resources, databases, encyclopedias, articles, clinical manuals, and references that can be helpful for non-medical professionals to understand medicine.

The discussion about the submission "Clint LLM: An Open-Source Medical Information Lookup and Reasoning Tool" on Hacker News covers various topics related to the project.

One commenter (@grrnm) appreciates the effort put into the project but mentions that they faced difficulties in obtaining commercial rights for the medical literature sources they used. They also mention trying various chunk sizes and matrix search techniques to improve the system.

In response, the project creator (@grrnm) acknowledges the limitations of the project and mentions that it contains 11,000 peer-reviewed articles on medical conditions. They explain that they chunk documents into sections and use OpenAI embeddings for processing. They did not experiment with different chunk sizes but plan to fine-tune the model for better diagnosis outcomes.

Another commenter (@lnsmnc) mentions that the questions in the medical field can be nuanced, and following up on the prompts is important for accurate results.

There is a brief discussion about the implications and applications of the tool, with someone (@ChildOfChaos) praising the accessibility of the OpenAI API.

One commenter (@jctrll) finds health anxiety to be a looming issue, implying that the tool could contribute to it.

A user (@clnt) briefly comments, "m llm."

A user (@NBJack) expresses regret about their decision to replace their model for another one, and they mention the importance of preserving comment history. They end by stating that Clint didn't respond to 20 comments on their platform.

Another commenter (@SOLAR_FIELDS) finds the question of whether creators choose AI names interesting.

A user (@grrnm) responds jokingly, pointing out that Clint is short for "CLINician Tool" and comments on the suggested AI naming convention.

Lastly, a commenter (@whycm) suggests not writing in all caps with non-standard capitalization, such as sentence case.

### AI Nutrition Fact Labels

#### [Submission URL](https://nutrition-facts.ai) | 90 points | by [maxwell](https://news.ycombinator.com/user?id=maxwell) | [22 comments](https://news.ycombinator.com/item?id=37239455)

Twilio, a communications platform, has outlined its principles for building AI products. These principles include transparency, responsibility, and accountability. The company believes in being transparent about the usage of AI and giving customers control. They also prioritize selecting responsible AI vendors with a focus on privacy, data security, and bias. Additionally, Twilio monitors the use of AI to address any potential harms and ensure fitness for purpose. To help customers understand the AI features they offer, Twilio has created "AI Nutrition Fact" labels. These labels provide details about the features, such as the model type, data privacy, data deletion, human involvement, compliance, and other resources. By providing this information, Twilio aims to empower customers to make informed decisions about adopting AI-powered capabilities.

The discussion on this submission covers various aspects of the topic. Here are the key points:

- Some users compare Twilio's AI Nutrition Fact labels to Apple's privacy labels, emphasizing the importance of objective information and government regulations.
- A few users debate whether the term "nutrition labels" is fitting for AI products, with some questioning its relevance.
- One user mentions a similarity to the AI FactSheets project, which aims to increase transparency and understanding of AI.
- The concept of using metaphors, such as food or nutrition, to explain AI is discussed, with a mention of a metaphor involving receipts.
- Some users express concerns about the effectiveness and comprehension of AI labels and suggest alternative approaches.
- Users highlight the autonomy and control provided to customers through Twilio's API, praising the flexibility and building blocks it offers.
- The need for strict regulatory standards and punishments for false information is mentioned.
- A user suggests that the topic of AI nutrition labels is gaining attention due to the potential financial stakes involved for Silicon Valley companies.
- There are mentions of the importance of privacy in the context of AI and the application of nutrition labeling to AI products.

Overall, the discussion covers a range of opinions on the effectiveness, relevance, and regulation of AI nutrition labels, as well as alternative approaches and concerns related to AI transparency and privacy.

### Princeton ‘AI Snake Oil’ authors say GenAI hype has ‘spiraled out of control’

#### [Submission URL](https://venturebeat.com/ai/princeton-university-ai-snake-oil-professors-say-generative-ai-hype-has-spiraled-out-of-control/) | 103 points | by [CharlesW](https://news.ycombinator.com/user?id=CharlesW) | [93 comments](https://news.ycombinator.com/item?id=37243354)

Princeton University professor Arvind Narayanan and his Ph.D. student Sayash Kapoor are working on a book that tackles the hype and misconceptions surrounding generative AI. They initially started the "AI Snake Oil" project to address the issues of predictive AI, but the rapid progress and consumer adoption of generative AI has prompted them to refocus their efforts. While they acknowledge the power and usefulness of generative AI, they also highlight the risks, the harmful consequences, and the need for responsible development practices. They urge people to be mindful of the hype and to use their collective power to bring about positive change. The project has received positive feedback from the academic and entrepreneurial communities, with some criticisms helping shape their arguments and refine their thinking. Despite concerns over labor exploitation, the authors advocate for policy changes rather than complete avoidance of generative AI.

The discussion on this submission revolves around various aspects of the topic, including criticisms of the authors' claims and arguments, comparisons with other technologies like blockchain and cryptocurrency, and different perspectives on the impact and potential of generative AI.

One commenter challenges the claims made in the article, questioning the evidence presented and suggesting that the authors may have reached conclusions beyond what the article supports. Another commenter suggests that the comparison between generative AI and other technologies like blockchain is not valid, as the hype surrounding them is driven by marketing and buzzwords.

There is also a discussion about the extent to which generative AI can solve problems and whether it is worth the investment. Some commenters argue that certain technologies, like blockchain, have not lived up to their promises, while others defend the potential of generative AI and its ability to transform various industries.

The discussion also touches on the popularity of platforms like Roblox among kids, the potential of cryptocurrencies for investment purposes, and the complexities of copyright protection in the context of AI-generated content.

Overall, the discussion highlights the diverse perspectives and debates surrounding generative AI, with commenters expressing both skepticism and optimism about its capabilities and implications.

### AI predicts certain esophageal and stomach cancers three years before diagnosis

#### [Submission URL](https://www.michiganmedicine.org/health-lab/ai-can-predict-certain-forms-esophageal-and-stomach-cancer) | 130 points | by [thunderbong](https://news.ycombinator.com/user?id=thunderbong) | [39 comments](https://news.ycombinator.com/item?id=37235578)

A team of researchers led by Joel Rubenstein, M.D., M.S., has developed an artificial intelligence tool called K-ECAN that can predict certain forms of esophageal and stomach cancer at least three years prior to a diagnosis. The tool uses basic information already available in electronic health records, such as patient demographics, weight, previous diagnoses, and routine laboratory results, to determine an individual's risk of developing esophageal adenocarcinoma (EAC) and gastric cardia adenocarcinoma (GCA). By identifying people at an elevated risk, the tool can help facilitate early detection and preventative measures. The researchers hope to integrate K-ECAN into electronic health records to alert providers about patients who are at an increased risk of developing these types of cancer. The study was funded by the Department of Defense and the National Institutes of Health.

Discussion Summary:

- One user points out that the study seems to be more about statistics and machine learning than actual breakthroughs in cancer diagnosis. They mention that the study analyzes risk factors and correlates with cancer, but it does not offer much new insight.

- Another user agrees, stating that machine learning and AI are often used as buzzwords without significant results.

- A discussion arises about the accuracy of the tool and the importance of accuracy as a metric. One user argues that accuracy can be a misleading metric and emphasizes the importance of other factors such as the area under the ROC curve (AUC) to evaluate performance.

- Another user adds that accuracy is meaningless without understanding the context and suggests that there is strong evidence for specific data points being associated with stomach cancer.

- Some users express skepticism about the AI tool's ability to accurately predict cancer diagnoses. They mention that accuracy rates reported for AI diagnosis are often low and caution against overestimating the capabilities of AI in this area.

- One user suggests that early detection of cancer may not always lead to better outcomes and cites examples of personal experiences where cancer was diagnosed at later stages despite symptoms being present earlier.

- Several users discuss the importance of lifestyle changes and other risk factors for cancer prevention, such as smoking and specific diet choices.

- A few users mention that obesity is a major risk factor for certain types of cancer and criticize the emphasis on technology as a solution rather than addressing the underlying problems.

- The discussion ends with users sharing videos and articles about esophageal and stomach cancer and discussing the relationship between obesity, metabolic syndrome, and cancer.

Overall, the discussion revolves around the usefulness and accuracy of the AI tool for predicting cancer, the limitations of relying solely on technology for diagnosis, and the importance of considering lifestyle factors in cancer prevention.

### Clearspace-1 space debris cleanup target struck by space debris

#### [Submission URL](https://www.space.com/space-debris-cleanup-mission-target-hit) | 30 points | by [thedday](https://news.ycombinator.com/user?id=thedday) | [12 comments](https://news.ycombinator.com/item?id=37239272)

In a surprising turn of events, a space debris cleanup mission by the European Space Agency (ESA) has encountered a setback. The mission, known as ClearSpace-1, aims to remove a leftover rocket adapter from low Earth orbit in 2026. However, the 18th Space Defense Squadron of the U.S. Space Force discovered that the adapter has been hit by a small flying object, resulting in new pieces of space debris floating nearby. The origin and nature of the object that caused the collision remain unknown. This incident highlights the importance of the mission in tackling the threat posed by space debris. ESA officials stated that larger objects of space debris can fragment into smaller objects, which can cause significant damage to active satellites. The planned ClearSpace-1 mission intends to rendezvous, capture, and remove the adapter using a spacecraft from the startup ClearSpace. Despite the setback, evaluations are underway to determine the next steps for the mission. The incident adds further uncertainty to a mission that is already challenging due to limited visibility above the orbit of the International Space Station. Nonetheless, tracking from the U.S. Space Force and other stations has confirmed that the main adapter remains intact and its orbit has not been significantly altered. While addressing space debris created by human activity poses a significant challenge, efforts like ClearSpace-1 are crucial in mitigating the risks associated with space debris.

The discussion around the submission revolves around various aspects of space debris and the feasibility of space debris cleanup missions. Some users express skepticism about the effectiveness of sending a single spacecraft to capture space debris, suggesting alternatives such as reusable craft or non-dragging methods. Others wonder if there are cost-effective solutions to deal with debris in lower orbits. One user references a book by Neal Stephenson, where a major plot point involves tackling falling debris in the atmosphere. The energy involved in space debris and its potential consequences are also discussed, with calculations about kinetic energy and the energy released during reentry. Some users discuss the energy efficiency and potential drawbacks of space debris cleanup missions. Additionally, one user expresses excitement about a book they are reading but is advised to stop reading and avoid spoilers.

### Denuvo security now on Switch, including new tech to block PC Switch emulation

#### [Submission URL](https://www.videogameschronicle.com/news/denuvo-security-is-now-on-switch-including-new-tech-to-block-pc-switch-emulation/) | 25 points | by [kotaKat](https://news.ycombinator.com/user?id=kotaKat) | [14 comments](https://news.ycombinator.com/item?id=37242491)

Security software company Denuvo has become the first security partner to join the Nintendo Developer Portal, offering its protection technology to Switch developers. The first tool being offered to Switch developers is the Nintendo Switch Emulator Protection, which will block the ability to play Switch games on PC emulators. Nintendo has been trying to prevent Switch emulation on PC for some time, issuing DMCA takedown requests to remove homebrew tools designed to play Switch games on an emulator. Denuvo's technology can integrate seamlessly into the game development process and block gameplay on emulators. This move aims to increase revenue for studios during the game launch window by ensuring that players have to buy legitimate copies of the game.

The discussion on this submission is quite varied. One user, jstrfsh, made a satirical comment about Bowser, the villain from the Mario series, being sued and paying damages for copyright infringement and kidnapping. Another user, SOLAR_FIELDS, responded with a comment about corporate responsibility and the need to hold individuals accountable.

Another user, brucethemoose2, wonders if Nintendo trusts Denuvo's security measures, considering the prevalence of cracks for Denuvo-protected games. User dprctv adds that it would be ideal if people stopped buying low-quality products from companies like Sega, but there is still demand due to millennial nostalgia.

The conversation shifts to the performance capabilities of the Nintendo Switch. Some users, like trhls, comment that the Switch's CPU is not as powerful as other modern gaming consoles, and therefore, demanding games may not run as well. Denuvo's DRM is also criticized for potentially exacerbating the performance issues. Another user, snkr, brings up the fact that some games released on the Switch are already playable on PC through workarounds that bypass the console's protections, questioning the necessity of Denuvo's solution. SOLAR_FIELDS mentions the limitations of the Switch platform, especially in terms of character restrictions and the extra work it requires for game development.

Lastly, jsphcsbl mentions a crowd-sourced effort to break Denuvo's protection.

### The False Promises of Tesla’s Full Self Driving

#### [Submission URL](https://www.theverge.com/2023/8/23/23837598/tesla-elon-musk-self-driving-false-promises-land-of-the-giants) | 32 points | by [ra7](https://news.ycombinator.com/user?id=ra7) | [14 comments](https://news.ycombinator.com/item?id=37242435)

In the latest episode of Land of the Giants: The Tesla Shock Wave, The Verge explores the false promises of Tesla's Full Self-Driving (FSD) feature. Back in 2016, Elon Musk announced that all Tesla vehicles would come equipped with the hardware necessary for full self-driving capabilities. However, the reality is that Tesla has yet to deliver on that promise. While they have released an advanced driver-assist system called FSD beta, the idea of a Tesla owner being able to nap in their car while it drives itself is far from becoming a reality. In fact, there have been hundreds of crashes involving Tesla vehicles using FSD and Autopilot, as well as multiple deaths. Government agencies are investigating Tesla's claims around self-driving, and a major recall could be on the horizon. The episode features firsthand testing of FSD and other autonomous vehicles, interviews with experts and former Tesla employees, and insights from competitors like General Motors-backed Cruise. The Verge's investigation raises questions about whether Tesla will be able to fulfill its promises without rethinking its hardware strategy.

The discussion revolves around the skepticism surrounding Tesla's Full Self-Driving (FSD) feature and the safety concerns associated with it. One commenter highlights that many people, including technical experts, wrongly believe that Tesla is leading the self-driving industry when in reality, they have not delivered on their promises. Another commenter points out that Waymo and Cruise have significantly more miles logged with fewer incidents compared to Tesla. They argue that Tesla's FSD system requires constant monitoring and is not as advanced as Waymo and Cruise. However, another commenter disagrees and mentions that many people enjoy using FSD and understand its limitations. The discussion also covers the issue of Tesla's safety reports, with one commenter asserting that they consist of unverified crash rates and misleading statistics. They suggest that Tesla's marketing department manipulates the data to present an incomplete safety profile. In contrast, Waymo's reports are said to provide detailed analysis of crash differences. Another commenter brings up multiple instances of Tesla's safety defects, such as failing to recognize road signs and critical deficiencies in its Driver Monitoring System. They argue that these issues persist despite several months of demonstrations and highlight the importance of addressing safety concerns in a self-driving product. One commenter expresses appreciation for the Dawn Project's comprehensive coverage, pointing out that their videos showcase the discrepancies between Tesla's claims and the reality of the FSD system.

