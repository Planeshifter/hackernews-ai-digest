## AI Submissions for Wed Sep 17 2025 {{ 'date': '2025-09-17T17:15:30.753Z' }}

### Tau² benchmark: How a prompt rewrite boosted GPT-5-mini by 22%

#### [Submission URL](https://quesma.com/blog/tau2-benchmark-improving-results-smaller-models/) | 190 points | by [blndrt](https://news.ycombinator.com/user?id=blndrt) | [60 comments](https://news.ycombinator.com/item?id=45275354)

- What’s new: Using the Tau² benchmark for agent tasks, the author found that rewriting domain policies into checklist-style prompts lifted GPT-5-mini’s pass@1 from 55% to 67.5% (+22.7%) and pass@2 from 40% to 50% (+25%). The number of tasks the agent failed on every attempt dropped by about half.

- Setup: They ran 40 simulations on Tau²’s telecom_small (20 scenarios, 2 trials each) with both the “stock” and “optimized” agent prompts, using GPT-5-mini for both agent and user roles.

- The hack: Offload prompt engineering to a stronger model (Claude). Claude rewrote telecom agent policies into AI-friendly SOPs:
  - Decision trees and numbered steps
  - Explicit tool calls with exact function names/params
  - Binary yes/no gates, prerequisites, and error-handling paths
  - Recheck/verify after each fix
  - Reference tables and “common mistakes” callouts
  - Imperative, minimal language: “Check X → If Y, do Z”

- Why it matters: For agentic tasks, small/faster/cheaper models can close much of the gap with better instructions and tool schemas. Prompt/policy design is a first-class performance lever, not an afterthought.

- Context (per the post): 
  - GPT-5-mini is roughly 2× lower latency, higher throughput, and 5× cheaper than the flagship, while achieving 85–95% of its performance on some tasks.
  - Reported benchmark comparators: GPT-5 ~97%, o3 ~58%, GPT-4.1 ~34% on this domain; the optimized GPT-5-mini outperformed o3.

- Caveats:
  - Narrow scope: only the telecom domain (and a 20-scenario subset), 2 trials each.
  - Improvements may reflect better alignment to this benchmark’s tools and policies; generalization to other domains/tasks remains to be shown.

- Takeaways for practitioners:
  - Turn long policy docs into decision trees and checklists with explicit preconditions, tool args, error branches, and verification.
  - Measure reliability with pass@k, not just single-shot accuracy.
  - Use a stronger model to rewrite domain SOPs for a smaller, cheaper production model.

Now on HN’s front page; discussion focuses on how much of “model quality” is actually prompt and tooling design.

The Hacker News discussion revolves around the implications of prompt engineering on LLM performance, drawing parallels to programming and debating benchmark validity. Key points:

1. **Prompt Engineering as Programming**  
   - Users liken structured prompts (checklists, decision trees) to coding, with some arguing it’s an extension of programming principles. Others debate whether it’s a new skill or a natural evolution of technical writing.  
   - Comparisons are made to logical languages (Lojban) and mathematical proofs, emphasizing the need for unambiguous instructions.

2. **Benchmark Skepticism**  
   - Critics (**tdsndrs**) question the telecom-focused benchmark, suggesting cherry-picking and overfitting. They argue domains like Retail or Airline may not see similar gains.  
   - Concerns arise about “ground truth” validity and whether models are graded fairly against rigid reference solutions.

3. **Transparency and Reproducibility**  
   - Multiple users (**dljdc**, **qnncm**) request the exact prompts used, highlighting the importance of open methodology. The author (**blndrt**) commits to sharing details, with others emphasizing reproducibility for credibility.

4. **Manual vs. Automated Optimization**  
   - While structured prompts boosted performance, some (**sblmfr**) note the trial-and-error process is time-consuming. Mentions of frameworks like **DSPy** suggest interest in algorithmic prompt optimization over manual tweaks.

5. **Cognitive Load and Model Limitations**  
   - Commenters highlight how smaller models (like GPT-5-mini) benefit from reduced cognitive load via clear instructions, though challenges remain in handling complex, domain-specific rules.

6. **Skepticism and Praise**  
   - Some dismiss benchmarks as inflated, while others applaud the progress. The telecom domain’s high scores (97% for GPT-5) are contrasted with lower performance in other areas, sparking debate about real-world applicability.

**Takeaway**: The discussion underscores prompt engineering’s growing role in LLM performance but stresses the need for domain-agnostic benchmarks, transparency, and automated tools to scale these optimizations. The line between “prompt design” and “programming” continues to blur, reshaping how practitioners approach LLM workflows.

### Anthropic irks White House with limits on models’ use

#### [Submission URL](https://www.semafor.com/article/09/17/2025/anthropic-irks-white-house-with-limits-on-models-uswhite-house-with-limits-on-models-use) | 241 points | by [mindingnever](https://news.ycombinator.com/user?id=mindingnever) | [124 comments](https://news.ycombinator.com/item?id=45279143)

Semafor reports that Anthropic has declined requests from contractors working with federal law enforcement, citing a long-standing policy that bars “domestic surveillance” use of its models. Officials in the Trump administration say the policy—applied to agencies like the FBI, Secret Service, and ICE—is too broad and amounts to a moral judgment on how agencies do their jobs. Anthropic didn’t comment.

Why it matters:
- Policy friction meets procurement reality: Claude models on AWS GovCloud are among the few top-tier systems cleared for certain classified contexts, so the restrictions are creating headaches for contractors—even as Anthropic offers a $1 access deal to government and markets a national security service.
- Different from peers: Other providers restrict surveillance but often carve out lawful law-enforcement use; officials argue Anthropic’s undefined “domestic surveillance” ban leaves wide room for interpretation.
- Bigger debate: The clash spotlights how much control AI vendors should retain over end uses—unlike traditional software—and reflects the broader rift between AI “safety” advocates and a Republican administration pushing to move faster.
- Business risk vs. performance buffer: Claude’s strong performance helps Anthropic today, but its policies could limit future government business.

The Hacker News discussion about Anthropic’s refusal to allow law enforcement use of its AI models revolves around several key themes:

### 1. **Policy and Enforcement Concerns**  
   - Users criticize Anthropic’s broad “domestic surveillance” ban as overly restrictive compared to competitors like Microsoft, which allow lawful law-enforcement exceptions. Some argue the policy reflects moral posturing rather than practical constraints, especially since Anthropic’s models are already FedRAMP-certified for government use.  
   - Comparisons are made to **Java’s licensing disclaimers** (e.g., prohibiting use in life-support systems), which are often ignored but legally unenforceable. Skepticism arises about how Anthropic would enforce its terms, particularly in classified or government contexts.

### 2. **Contractual and Legal Ambiguities**  
   - Government contractors express frustration with SaaS models (e.g., AWS GovCloud) that let vendors update Terms of Service (ToS) unilaterally, creating uncertainty for long-term agreements. Some note U.S. contracts often **reference ToS dynamically**, leading to disputes over whether terms apply retroactively.  
   - Debates emerge about the validity of “clickwrap” agreements in government procurement and whether vendors can alter terms post-signing. One user cites the **Uniform Commercial Code (UCC)** as a framework for resolving such ambiguities.

### 3. **Political and Business Implications**  
   - Critics speculate Anthropic’s stance may backfire, limiting its government business despite current performance advantages. Comparisons are drawn to Apple’s historic restrictions (e.g., banning iTunes for “missile production”), highlighting how tech firms often set symbolic usage boundaries.  
   - Some suggest the Biden administration’s AI safety focus clashes with Republican desires for rapid deployment, positioning Anthropic as a political actor rather than a neutral vendor.

### 4. **Calls for Alternatives**  
   - Users advocate for **self-hosted or open-source AI** to bypass vendor restrictions. Others mock the impracticality of “ethics theater,” given AI’s reliance on centralized infrastructure.  

### 5. **Skepticism About Media Coverage**  
   - Semafor’s reporting is dismissed by some as sensationalized, with speculation about ulterior motives behind highlighting Anthropic’s policy.  

In summary, the debate underscores tensions between corporate ethics, government procurement realities, and the unique challenges of regulating AI compared to traditional software. While some applaud Anthropic’s stance, others warn it may alienate a lucrative market segment.

### Bringing fully autonomous rides to Nashville, in partnership with Lyft

#### [Submission URL](https://waymo.com/blog/2025/09/waymo-is-coming-to-nashville-in-partnership-with-lyft) | 134 points | by [ra7](https://news.ycombinator.com/user?id=ra7) | [206 comments](https://news.ycombinator.com/item?id=45275415)

Waymo is bringing its fully driverless ride-hailing to Nashville, teaming up with Lyft’s Flexdrive for fleet management. The company says it will begin fully autonomous operations in the coming months and open to the public in 2026. Riders will start with the Waymo app, with Lyft app integration to follow as the service scales.

Key details:
- Partnership: Waymo tech + Lyft’s Flexdrive for fleet operations and customer experience
- Hailing: Waymo app at launch; Lyft app support added over time
- Scale claim: “Hundreds of thousands” of fully autonomous rides per week across five U.S. cities
- Safety claim: 100M+ fully autonomous miles; “significantly safer than human drivers” in serviced areas
- Local backing: Tennessee Gov. Bill Lee voiced support, citing innovation and economic growth

Why it matters:
- Lyft returns to AV via partnership (after selling its AV unit in 2021), potentially boosting Waymo’s rider funnel and ops efficiency.
- Another non–Sun Belt launch signals Waymo’s confidence in generalizing to new cities and conditions.

Timeline: driverless ops in Nashville “in the coming months”; public access in 2026. Sign-ups: waymo.com/updates.

The Hacker News discussion around Waymo’s Nashville expansion with Lyft highlights several key debates and perspectives:

### 1. **Partnership Strategy & Competition**
   - **Vertical Integration vs. Partnerships**: Users compare Waymo’s approach to Apple’s vertical integration model, debating whether owning hardware/software (like Waymo) or relying on partnerships (e.g., Lyft’s Flexdrive) is more sustainable. Some argue vertical integration offers control but risks commoditization, while partnerships reduce costs but squeeze margins.
   - **Lyft/Uber’s Role**: Skeptics question Lyft’s long-term benefit, noting its 15-30% platform fees and lack of vehicle ownership. Others see the partnership as a smart way for Waymo to leverage Lyft’s user base without heavy marketing, especially as Waymo focuses on scaling technology.

### 2. **Profitability Concerns**
   - **High Costs**: Doubts persist about Waymo’s path to profitability due to expensive hardware (LIDAR, vehicles) and operational costs (remote operators, fleet maintenance). One user estimates $200k/vehicle, though others counter that LIDAR costs are dropping rapidly.
   - **Labor & Remote Operators**: Critics highlight Alphabet’s “reckless spending” on remote operators (working 24/7 shifts) and support staff, questioning scalability. Comparisons to Cruise’s 2023 struggles add skepticism, though some note Waymo’s headcount growth is slower than fleet expansion.

### 3. **Operational Challenges**
   - **Scaling Infrastructure**: Users stress the difficulty of building parking, maintenance, and charging infrastructure in new cities. Licensing fees and partnerships (e.g., Avis, Moove) are seen as workarounds but not long-term solutions.
   - **Regulatory Hurdles**: Mentions of San Francisco allowing remote-controlled vehicles underscore the regulatory variability Waymo must navigate. Nashville’s launch is seen as a test of Waymo’s ability to generalize beyond Sun Belt cities.

### 4. **Market Optimism**
   - **Positive Signals**: Some cite a Forbes interview where Waymo’s CEO hinted at improving economics, with riders paying “more than drivers cost.” Others draw parallels to SpaceX’s Starlink, where early losses preceded profitability.
   - **Ride Demand**: Optimists argue Waymo’s safety record and convenience (no surge pricing, 24/7 availability) could attract users despite higher upfront costs. Partnerships with Uber/Lyft are seen as critical for funneling demand during scaling.

### 5. **Long-Term Bets**
   - **Lyft’s Survival**: Lyft’s pivot to fleet management (after selling its AV unit) is viewed as a lifeline, but users question its viability against Uber’s dominance. Waymo’s success could hinge on Lyft’s ability to retain market share.
   - **Autonomy vs. Labor**: A recurring theme is whether driverless tech will ultimately reduce labor costs or simply shift expenses to remote operators and support staff, with no clear consensus.

### Conclusion
The discussion reflects cautious optimism about Waymo’s expansion but underscores skepticism about profitability, scalability, and the sustainability of partnerships. While some see Nashville as a stepping stone to broader adoption, others warn of capital intensity and operational hurdles reminiscent of past AV failures (e.g., Cruise). The success of Waymo’s model may depend on balancing tech innovation with cost-efficient scaling and regulatory navigation.

### Show HN: Pgmcp, an MCP server to query any Postgres database in natural language

#### [Submission URL](https://github.com/subnetmarco/pgmcp) | 13 points | by [fosk](https://news.ycombinator.com/user?id=fosk) | [5 comments](https://news.ycombinator.com/item?id=45280980)

PGMCP: Natural‑language, read‑only access to any Postgres via the Model Context Protocol

What it is
- An MCP server (subnetmarco/pgmcp) that lets AI assistants query your PostgreSQL database in plain English and returns structured SQL results. It’s designed to be safe (read‑only), fast, and drop‑in for any schema.

Why it matters
- Bridges chat-based assistants (Cursor, Claude Desktop, VS Code MCP clients, custom apps) to real company data without building bespoke APIs or changing your DB. Non‑technical users can ask questions; the server handles SQL generation, execution, and streaming results with guardrails.

How it works
- Uses OpenAI to translate natural language into SQL (mentions support for other LLMs like Anthropic/local via MCP ecosystem).
- Connects to Postgres via pgx/v5 with pooling; communicates over HTTP Server‑Sent Events for streaming/pagination.
- Caches schema for context, auto‑paginates large result sets, and logs/audits queries.

Notable features
- Read‑only safety: blocks INSERT/UPDATE/DELETE; input sanitization and SQL guardrails.
- Robust error handling: detects and recovers from bad AI‑generated SQL, provides helpful feedback.
- Performance protections: simplifies expensive queries, connection limits, memory management.
- Text search across all text columns; multiple output formats (table/JSON/CSV).
- Optional bearer‑token auth; graceful shutdown; extensive config validation and tests.

Example use cases
- Ad‑hoc analytics (“Top 5 customers by spend?”), support dashboards, quick audits across arbitrary schemas without ETL or schema changes.

Tech stack
- Go server, pgx/v5, OpenAI integration, MCP-compatible with clients like Cursor/Claude Desktop/VS Code.

Repo: github.com/subnetmarco/pgmcp (includes README, schema caching, SSE transport, and server/client folders)

The discussion around PGMCP includes several key points:

1. **Implementation Feedback**: User chy highlights the tool's simplicity for Postgres MCP integration using npx commands, but raises concerns about LLM-generated SQL efficiency. A reply from oulipo2 emphasizes potential resource issues, advocating for memory monitoring and query cancellation safeguards for long-running operations.

2. **Competitor Mention**: frkynt shares a "shameless plug" for their own desktop app ([znqry.app](https://znqry.app)), which offers similar natural-language query capabilities with CSV/JSON/Excel/Parquet support and LLM integration.

3. **Related Project**: mistrial9 links to another recent HN post (ID 43520953) about a comparable project, acknowledged by fsk with a brief "project" reply.

The thread reflects interest in AI-powered database interfaces while highlighting resource management concerns and alternative implementations in the space.

### Is AI a Bubble?

#### [Submission URL](https://www.exponentialview.co/p/is-ai-a-bubble) | 10 points | by [witch-king](https://news.ycombinator.com/user?id=witch-king) | [4 comments](https://news.ycombinator.com/item?id=45281070)

Is AI a bubble? Exponential View’s Azeem Azhar lays out a practical way to judge, not just vibe. Drawing on Carlota Perez and Bill Janeway (and having lived through dot‑com and the GFC), he proposes a five‑gauge dashboard to compare today’s genAI cycle with past manias.

Key ideas:
- Two systems to watch: financial markets and real‑economy investment. A bubble isn’t just soaring stocks; it’s also a surge (and later collapse) in productive capital.
- Working definition: a sustained 50% equity drawdown lasting 5+ years, paired with roughly a 50% drop from peak in productive capital deployment (capex/VC). For reference, the dot‑com trough lasted ~5 years and took ~15 years to fully recover; US housing recovered in ~10.
- Boom vs bubble: both start with rising prices and investment; in a boom, fundamentals eventually catch up (cash flows, productivity, real demand).
- Historical context: tulip mania’s damage is overstated; 1840s railways overbuilt “veins” beyond sustainable commerce; 1990s telecom left ~70 million miles of dark fiber; narratives are powerful but can detach from earnings reality.
- Today’s debate: from Gary Marcus’s “peak bubble” claim to The Atlantic’s warning and The Economist’s alarm, sentiment is split—hence the need for measurable gauges.

Azhar says the full methodology and data will be published for Exponential View members soon; this overview is free, with a PDF available and limited consult slots for investors/executives. The promise: a repeatable, evidence-based way to track whether genAI is a boom that fundamentals can meet—or a bubble that can’t.

The Hacker News discussion revolves around whether the current AI boom is a speculative bubble akin to historical manias like tulips or railways, with a focus on GPUs and their role in training large language models (LLMs). Key points:

1. **Tulip Mania Comparison**:  
   Some users liken investing in GPUs to tulip mania, arguing that GPUs could become obsolete if advancements in chip efficiency render them "worthless" over time. Skeptics note parallels to past bubbles where infrastructure (e.g., tulips, dark fiber) lost value once demand waned or technology improved.

2. **Counterarguments**:  
   Others push back, emphasizing that GPUs are not inherently valueless like tulips. They highlight practical business applications of LLMs, such as accelerating workflows (e.g., reducing processing times from days to hours), which provide tangible ROI. The issue, they argue, lies in speculative ventures (e.g., "selling tokens") rather than AI’s utility.

3. **Long-Term vs. Short-Term**:  
   A user predicts AI could become a "trillion-dollar business in 5 years," suggesting long-term potential despite short-term hype. Concerns about depreciation and scaling costs (e.g., chip obsolescence, infrastructure demands) are raised, referencing an article on AI labs bracing for financial challenges.

4. **Hardware Evolution**:  
   Debates touch on competition among GPU manufacturers and the indirect pricing impact of modern chips. Some warn that current hardware could be outdated soon, while others stress that efficiency gains and real-world use cases (e.g., streamlining business processes) justify ongoing investment.

**In short**: The thread reflects a split between those viewing AI as a speculative bubble fueled by transient hardware hype and those advocating for its sustainable value based on practical, productivity-boosting applications. The role of GPUs—as either a fleeting asset or a foundational tool—anchors the debate.

### AI fares better than doctors at predicting deadly complications after surgery

#### [Submission URL](https://hub.jhu.edu/2025/09/17/artificial-intelligence-predicts-post-surgery-complications/) | 25 points | by [Improvement](https://news.ycombinator.com/user?id=Improvement) | [19 comments](https://news.ycombinator.com/item?id=45273355)

- What’s new: Johns Hopkins researchers trained deep learning models on pre-op ECGs to predict 30-day post-surgical complications (heart attack, stroke, or death). A “fusion” model that combined ECG data with basic chart info (age, comorbidities, etc.) hit 85% accuracy, beating commonly used clinical risk scores (~60% accuracy per the authors).

- Why it matters: ECGs are cheap, fast, and already collected before major surgery. Turning a 10-second trace into a personalized risk estimate could change who gets flagged for extra monitoring, optimization, or alternative care plans—without new hardware or tests.

- How they did it: Analyzed 37,000 patients’ pre-op ECGs from Beth Israel Deaconess (Boston). Trained two models:
  - ECG-only: surpassed standard risk tools.
  - Fusion (ECG + EHR features): performed best.
  They also built a method to highlight ECG features associated with adverse outcomes, nodding toward explainability.

- Caveats and next steps: Results are retrospective from a single health system; external validation and prospective trials are planned. The paper (British Journal of Anaesthesia) doesn’t clarify how “85% accuracy” maps to metrics like AUC, calibration, or PPV at various thresholds—key for clinical deployment and fairness across subgroups.

- Big picture: If validated broadly, this could upgrade pre-op risk stratification using data hospitals already capture, offering an inexpensive path to better outcomes and resource targeting.

**Summary of Hacker News Discussion:**

1. **ML vs. Human Judgment**:  
   - Users debate whether ML models, trained on vast datasets, can outperform human clinicians in surgical risk assessment. Proponents argue ML avoids human biases (e.g., underdiagnosing marginalized groups) and processes complex data more thoroughly. Critics caution against overreliance on "black-box" models lacking transparency.

2. **Explainability Concerns**:  
   - Several commenters emphasize the need for interpretability (*"nodding toward explainability"* in the study). A key tension arises: should clinicians prioritize accuracy (even via opaque models) or understanding? Some argue explainability is critical for trust and ethical deployment.

3. **Augmentation, Not Replacement**:  
   - Many reject the idea of AI replacing doctors, framing it as a tool to *augment* clinical judgment. For example, models could flag high-risk patients for closer monitoring, while surgeons retain decision-making authority.

4. **Data and Bias Skepticism**:  
   - Skeptics question the study’s retrospective design and single-institution data, highlighting risks of systemic bias (e.g., racial disparities in training data). Others note that "85% accuracy" lacks context—without metrics like AUC or PPV, real-world performance is unclear.

5. **AI vs. Traditional Statistics**:  
   - Some dismiss the hype, arguing ML is merely advanced statistics. Others counter that modern AI’s ability to uncover latent patterns in raw ECG data represents a meaningful leap over conventional risk scores (e.g., RCRI).

6. **Ethical and Practical Implications**:  
   - Concerns include financial incentives driving adoption (*"bld fnncl bs"*) and patient anxiety if high-risk predictions lead to overtreatment. Optimists highlight AI’s potential to democratize care by reducing reliance on subjective clinician experience.

**Key Takeaway**:  
The discussion reflects cautious optimism about AI’s role in improving surgical risk prediction but stresses the need for rigorous validation, transparency, and ethical integration into clinical workflows. Most agree AI should enhance—not replace—human expertise.

