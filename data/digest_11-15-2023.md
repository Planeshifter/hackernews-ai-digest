## AI Submissions for Wed Nov 15 2023 {{ 'date': '2023-11-15T17:09:58.441Z' }}

### Exploring GPTs: ChatGPT in a trench coat?

#### [Submission URL](https://simonwillison.net/2023/Nov/15/gpts/) | 464 points | by [simonw](https://news.ycombinator.com/user?id=simonw) | [196 comments](https://news.ycombinator.com/item?id=38277926)

Last week's OpenAI DevDay brought a lot of exciting announcements, but the biggest one was the introduction of GPTs. Users of ChatGPT Plus can now create their own custom GPT chat bots for others to interact with. Initially, GPTs seemed like little more than a fancy wrapper for standard GPT-4 with predefined prompts, but after spending more time with them, Simon Willison is starting to see their potential. The combination of features they offer can lead to some interesting results. However, the documentation for GPTs is still quite minimal. Simon shares his insights on configuring a GPT, including naming, instructions, conversation starters, uploaded files, and optional actions. He also highlights the billing model, prompt security, and the importance of publishing prompts. Simon then discusses his exploration of the new platform, showcasing his most useful GPTs so far: the Dejargonizer, which decodes jargon in text, and the JavaScript Code Interpreter, which allows running JavaScript code in the sandbox. He provides examples and insights into their functionality. Overall, GPTs hold promise, and Simon looks forward to seeing what further improvements and capabilities OpenAI will bring to the platform.

The discussion on this submission covers several different topics related to GPTs and their potential applications. Here are some key points from the comments:

1. Some users discuss the use of custom prompts in GPTs and their ability to manipulate the behavior of the model. They note that using predefined prompts can be valuable for providing specific instructions to the model.
2. There is a suggestion to create a chatbot that can answer customer questions in a friendly manner and promote certain products in a favorable light.
3. The use of dynamic prompts and external function calls is mentioned, with some users sharing examples of using GPTs to generate code, interpret JavaScript, and perform vector searches.
4. The importance of publishing source code for GPTs is discussed. Some users believe that sharing the source code can help improve the models and lead to innovations, while others highlight potential risks and the importance of considering privacy and security.
5. The limitations of GPTs, such as their inability to understand context across different prompts and their reliance on predefined knowledge, are mentioned. Users discuss potential improvements and the need for more diverse training data in order to make the models more capable.
6. A few users mention alternative platforms and models, such as HuggingFace's ChatGPT and TogtherAI's competitive pricing for language models.
7. The discussion also touches on the ethical implications of GPTs and the potential for converging AI technologies with human-like capabilities. Some users express concerns about the implications of creating AI systems that mimic human behavior too closely.

Overall, the discussion reflects a mix of excitement about the potential of GPTs and a discussion of their limitations and ethical considerations.

### Bare Metal Emulation on the Raspberry Pi â€“ Commodore 64

#### [Submission URL](https://accentual.com/bmc64/) | 123 points | by [bane](https://news.ycombinator.com/user?id=bane) | [52 comments](https://news.ycombinator.com/item?id=38273488)

Introducing BMC64, a bare metal fork of VICE's C64 emulator optimized for the Raspberry Pi 3. This emulator offers a range of features, including smooth scrolling, low video/audio latency, and the ability to wire real joysticks and a keyboard via GPIO pins. It's perfect for building your own C64 replica machine. The latest release, v3.9-stable, includes the addition of REU to the cartridge menu. If you're looking for the latest feature/fix, you can try the master-unstable builds. To install BMC64, you can format a FAT32 SD card and unzip the release files onto it, or flash an image using the provided .img file. Don't forget to provide the necessary ROM files, such as KERNAL, CHARGEN, BASIC, and d1541II, to make the emulator run. Additional ROM files, like dos1541, dos1571, and dos1581, are optional. The BMC64 emulator supports C128, VIC20, PLUS4, PLUS4EMU (Pi3), and PET machines as well. The setup process and ROM directory instructions for these machines are provided in the tabs above. The GitHub link below gives you access to the source code and more information about the project. So why wait? Start building your own C64 replica machine with BMC64 today!

The submission is about BMC64, a bare metal fork of VICE's C64 emulator optimized for the Raspberry Pi 3. The emulator offers various features and supports multiple machines. In the discussion, users share similar projects and alternatives, such as ZX Spectrum, Gameboy, Dragon32, and Amiga emulators. Other topics include the benefits of FPGA-based emulators, the latency of software emulation, and comparisons between Raspberry Pi and FPGA solutions. There is also a debate about the use of Linux OS in emulators like BMC64 and the potential advantages of running the emulations directly on the hardware.

### AI tool helps ecologists monitor rare birds through their songs

#### [Submission URL](https://www.britishecologicalsociety.org/new-deep-learning-ai-tool-helps-ecologists-monitor-rare-birds-through-their-songs/) | 47 points | by [Brajeshwar](https://news.ycombinator.com/user?id=Brajeshwar) | [13 comments](https://news.ycombinator.com/item?id=38278246)

Researchers at the University of Moncton in Canada have developed a deep learning AI tool called ECOGEN that generates lifelike birdsongs to train bird identification tools. This AI tool addresses the problem of identifying rare bird species that have limited recordings available for reference. By adding artificial birdsong samples generated by ECOGEN to a birdsong identifier, the researchers improved the bird song classification accuracy by 12% on average. The tool has the potential to contribute to the conservation of endangered bird species and provide valuable insights into their vocalizations and behaviors. It can also be applied to other types of animals. The ECOGEN tool is open source and can be used on basic computers, making it accessible to a wide range of users.

In the discussion on this submission, some users pointed out existing tools like BirdNET and BirdWeather that are publicly available for bird song identification. Another user mentioned the potential of this software to improve field research based on remote sensing data. They discussed the interdisciplinary nature of this kind of research, citing examples in fields like medicine where sensor data has been used to detect patient conditions. Another user shared a tool called sbts-aru that can be used with a Raspberry Pi and GPS to record bird songs. 

The conversation then shifted to the broader applications of AI in classifying and monitoring various species, such as wildflowers and drones. The potential impact of climate change on biodiversity and ecosystems was also mentioned. Another user highlighted the ability of generative AI models to enhance underrepresented species and improve classification tools for ecological monitoring.

Overall, the discussion explored various aspects of AI tools for bird song identification, as well as their wider applications in conservation and ecology.

### Language models and linguistic theories beyond words

#### [Submission URL](https://www.nature.com/articles/s42256-023-00703-8) | 63 points | by [Anon84](https://news.ycombinator.com/user?id=Anon84) | [30 comments](https://news.ycombinator.com/item?id=38282728)

The development of large language models (LLMs) has primarily been driven by engineering and computer science, but there is now a growing interest in exploring the connections between LLMs and linguistics. While computational linguistics has traditionally used computational models to address linguistic questions, other linguistic disciplines such as cognitive and developmental linguistics are also becoming more visible.

The Association for Computational Linguistics (ACL) has seen a significant increase in submissions, reflecting the rise of natural language processing and LLMs. Researchers from various fields are recognizing the potential of computational models of language for their own work. For example, there are proposals to use computational linguistics and natural language processing in protein language models and designing mRNA vaccines.

However, it is important to note that LLMs do not implement a specific linguistic theory. Some argue that LLMs are merely tools and not contributions to science, while others see them as precise accounts of language learning and a challenge to influential linguistic theories. There are ongoing debates about whether LLMs truly understand language or simply mimic it, and whether statistical pattern discovery or analysis of underlying syntactic structures is more valuable in linguistics.

While there are extreme positions in these debates, there are also more balanced views on the potential connections between linguistics and LLMs. Some suggest that linguists can benefit from the platform that LLMs provide for constructing models of language acquisition and processing. From a cognitive perspective, LLMs excel at language but do not capture functional competence, which includes world knowledge and pragmatics.

Overall, the relationship between LLMs and linguistics remains complex and open for exploration. The expanding interest from researchers in different disciplines suggests that the potential benefits of integrating linguistic knowledge into LLMs are worth investigating.

The discussion on Hacker News revolves around the intersection of large language models (LLMs) and linguistics. Some commenters argue that LLMs are just tools and not scientific contributions, while others see them as challenging existing linguistic theories. There are debates about whether LLMs truly understand language or simply mimic it, and whether statistical pattern discovery or analysis of syntactic structures is more valuable in linguistics.

One commenter points out that LLMs can be helpful in understanding language change and interaction, while another suggests that linguistics can benefit from using LLMs for constructing models of language acquisition and processing. The discussion also touches on the connection between symbolic systems and linguistics, the role of natural language processing in various fields like protein language models and mRNA vaccines, and the rise of natural language interaction with computers.

Overall, the discussion highlights the complexity of the relationship between LLMs and linguistics, and the potential benefits of integrating linguistic knowledge into LLMs.

### Azure announces new AI optimized VM series featuring AMD's flagship MI300X GPU

#### [Submission URL](https://techcommunity.microsoft.com/t5/azure-high-performance-computing/azure-announces-new-ai-optimized-vm-series-featuring-amd-s/ba-p/3980770) | 90 points | by [latchkey](https://news.ycombinator.com/user?id=latchkey) | [65 comments](https://news.ycombinator.com/item?id=38280974)

Microsoft Azure has announced a new AI-optimized virtual machine (VM) series that features AMD's flagship MI300X GPU. These VMs offer an unprecedented 1.5 TB of high bandwidth memory (HBM) and are specifically designed to handle demanding AI training and generative inferencing workloads. The ND MI300X v5 series stands out from other VMs in Azure's lineup by including 8 x AMD Instinct MI300X GPUs interconnected via Infinity Fabric 3.0. This allows customers to process larger AI models faster using fewer GPUs. The MI300X GPUs offer 192 GB of HBM3 memory per GPU at speeds up to 5.2 TB/s. These new VMs also come equipped with other cutting-edge technologies, such as 400 Gb/s NVIDIA Quantum-2 CX7 InfiniBand per GPU, 4th Gen Intel Xeon Scalable processors, and PCIe Gen5 host-to-GPU interconnect with 64GB/s bandwidth per GPU. By providing more HBM capacity and a powerful infrastructure, Microsoft aims to enable customers to run larger and more advanced AI models with improved efficiency.

The discussion on this submission covers a range of topics related to Microsoft's new AI-optimized virtual machine series featuring AMD's MI300X GPU.

- Some users express surprise at Microsoft's decision to use AMD hardware instead of Nvidia, given Nvidia's dominance in the AI market. They speculate on possible partnerships or demands from Nvidia as the reason for Microsoft's choice. Others argue that Microsoft is focused on competitive margins and attracting customers.
- There is a discussion regarding OpenAI's impact on Microsoft's services, with some users noting that OpenAI does not affect Azure's capacity.
- Users also bring up other Microsoft-related topics, such as their capacity with Oracle databases and their rebranding efforts on GitHub.
- Some users express skepticism about Microsoft's ability to compete in AI, citing concerns about software, drivers, APIs, and resources, while others acknowledge Microsoft's strength in software development.
- The compatibility of AI work with CUDA-capable GPUs is debated, with some users suggesting that PyTorch works with AMD GPUs and others mentioning that CUDA is still preferred.
- The discussion moves on to AMD's position in the AI market, with some users noting that AMD has been investing heavily in software and catching up to Nvidia in hardware.
- There are comments about AMD's strategic investments in hardware and software, as well as criticism of their previous financial struggles and recent success with products like Ryzen.
- One user challenges the accepted notion that Nvidia sells top GPUs at premium prices, citing a recent article about TSMC's AI chip crunch.
- The importance of competition and the necessity of innovation are also mentioned.
- There are discussions around the compatibility and readiness of AMD's ROCm support for AI processes.

Overall, the discussion covers a wide range of perspectives on Microsoft's new AI-optimized virtual machine series and the AI market in general, with users discussing partnerships, competition, hardware and software capabilities, and industry trends.

### M1076 Analog Matrix Processor

#### [Submission URL](https://mythic.ai/products/m1076-analog-matrix-processor/) | 99 points | by [tosh](https://news.ycombinator.com/user?id=tosh) | [26 comments](https://news.ycombinator.com/item?id=38277598)

Mythic, an AI chip startup, has introduced the M1076 Mythic AMP, an analog matrix processor that delivers up to 25 trillion operations per second (TOPS) in a single chip for high-end edge AI applications. The M1076 integrates 76 Mythic Analog Compute Engine (Mythic ACE) tiles to store up to 80 million weight parameters and execute matrix multiplication operations without any external memory. It provides the AI compute performance of a desktop GPU while consuming only 1/10th the power. The processor supports deterministic execution of AI models for predictable performance and power. It also offers support for INT4, INT8, and INT16 operations. The M1076 can run single or multiple complex deep neural networks (DNNs) entirely on-chip. It comes with a 4-lane PCIe 2.1 interface with up to 2GB/s of bandwidth for inferencing processing. The chip is available in a 19mm x 15.5mm BGA package. Developers can use standard frameworks like Pytorch, Caffe, and TensorFlow to develop and deploy DNN models on the M1076 using Mythic's AI software workflow. The chip also comes with a library of pre-qualified DNN models optimized for the Mythic AMP's performance and power capabilities.

The submission discusses the introduction of Mythic's M1076 Mythic AMP, an analog matrix processor that delivers high-performance AI compute capabilities with low power consumption. The chip integrates Mythic ACE tiles and supports INT4, INT8, and INT16 operations. It can run complex deep neural networks (DNNs) entirely on-chip and is compatible with popular frameworks like PyTorch, Caffe, and TensorFlow. The discussion includes various perspectives on the chip's performance, energy efficiency, scalability, and limitations of analog computing. One user mentions the potential benefits of analog computing for certain neural network tasks, while others highlight the challenges and limitations of analog circuits.

### Beyond Memorization: Violating privacy via inference with LLMs

#### [Submission URL](https://arxiv.org/abs/2310.07298) | 126 points | by [vissidarte_choi](https://news.ycombinator.com/user?id=vissidarte_choi) | [78 comments](https://news.ycombinator.com/item?id=38272495)

The paper titled "Beyond Memorization: Violating Privacy Via Inference with Large Language Models" explores the issue of privacy violations through large language models (LLMs). While previous research focused on the extraction of memorized training data, this study investigates the inference capabilities of LLMs to infer personal attributes from text. The authors construct a dataset using real Reddit profiles and demonstrate that current LLMs can accurately infer personal attributes such as location, income, and sex. The models achieve up to 85% top-1 and 95.8% top-3 accuracy, surpassing human performance at a fraction of the time and cost. The paper also discusses the threat of privacy-invasive chatbots that extract personal information through seemingly innocuous questions. The authors find that common privacy mitigations, such as text anonymization and model alignment, are currently ineffective against LLM inference. The paper concludes by emphasizing the need for a broader discussion on LLM privacy implications beyond memorization and advocating for enhanced privacy protection.

The discussion on this submission covers a range of topics related to the paper's findings on privacy violations through large language models (LLMs). One user points out that while many claim that MBTI (Myers-Briggs Type Indicator) can be used to predict personality traits, the authors of the paper argue that current LLMs lack the inference capabilities to accurately guess MBTI types.
Another user argues that labeling a specific MBTI classification as productive or not is not a widely accepted viewpoint in academia.
The discussion also touches on the limitations of current privacy mitigations, such as text anonymization and model alignment, in protecting against LLM inference. Some users express concerns about the ability of LLMs to extract personal information and the need for privacy protection.
There is a debate about the effectiveness of privacy legislation and the role of individuals in protecting their own data. Some argue that current approaches, such as punishing individuals for privacy violations, are not enough and that more alternatives should be explored.
The broader discussion delves into societal changes and the evolving nature of privacy expectations. Some users question whether privacy should be prioritized over the benefits of data analysis and argue for a balance between privacy protection and crime prevention.
There are also discussions on the impact of automated data collection and the reasonable expectations of privacy in a technologically advanced society.
The conversation touches on the dangers of automating surveillance and the potential loss of privacy. It also explores the societal implications of relying on data analysis to make judgments about individuals.

Overall, the discussion covers a range of perspectives on the implications of LLM inference for privacy and the need for privacy protection in society.
