## AI Submissions for Sun Dec 21 2025 {{ 'date': '2025-12-21T17:10:20.277Z' }}

### A guide to local coding models

#### [Submission URL](https://www.aiforswes.com/p/you-dont-need-to-spend-100mo-on-claude) | 540 points | by [mpweiher](https://news.ycombinator.com/user?id=mpweiher) | [301 comments](https://news.ycombinator.com/item?id=46348329)

You don’t need $100/mo for Claude Code? Author tests, then retracts. Logan Thorneloe bought a 128 GB RAM MacBook Pro to see if beefy local models could replace paid coding copilots. After weeks of tinkering, he revises his claim: local LLMs are surprisingly strong (even some 7B models) and can cover ~90% of routine dev work, but the last 10%—reliability, edge cases, and production-critical tasks—still favors frontier cloud tools. Practical hurdles bit hardest: tool-use is flaky, thinking traces get stuck, dev workloads (Docker, etc.) eat RAM forcing smaller, weaker models, and latency/throughput trade-offs matter. His new stance: local models are excellent supplements that can lower costs or handle side projects, but they’re not a full replacement for professional coding assistants; free/cheap options like Google’s Gemini CLI further blunt the ROI. The post also includes a step-by-step local setup guide, memory-sizing advice, quantization tips, and model recommendations—plus a candid correction noting the original hypothesis overstated the case.

The discussion broadens the submission's hardware analysis into a debate on the economics, privacy, and workflows of cloud versus local AI assistants.

**Key themes include:**

*   **TCO and Latency vs. Cost:** Commenters argue that the "savings" of local models are often illusory. One user calculates the Total Cost of Ownership for a 4090 rig (depreciation + electricity) at roughly $733/year, noting that you only break even if you were spending over $61/month on cloud APIs. Furthermore, local models suffer from slower turnaround and smaller context windows (32k local vs. 200k+ cloud), making "whole codebase" reasoning difficult compared to the instant responses of models like Claude.
*   **Subscription Tier Strategies ($20 vs. $200):** Users debate the necessity of high-tier plans.
    *   **The "Pro" Regret:** Some users who purchased $100-$200/mo enterprise/team tiers reported using only ~10% of their quota, suggesting it is more cost-effective to "stack" multiple cheaper subscriptions (e.g., Gemini Advanced + ChatGPT Plus) rather than relying on a single expensive tier.
    *   **The "Vibecoding" Trap:** Conversely, others argue that $20/mo plans are insufficient for intense "vibecoding" sessions (iterative, whole-app generation), claiming they hit rate limits within 10–20 minutes when handling complex prompts or large contexts.
*   **Privacy and Google:** While Google's $20/mo Gemini plan is praised for its high token limits and value, users caution that its privacy policy regarding training on user data is less clear or protective compared to OpenAI and Anthropic's paid API/Team tiers.
*   **Micro-Optimizations & CLI Tools:** A technical sub-thread focuses on low-cost workflows, specifically piping `man` pages or documentation into simpler LLMs (using tools like `llm`, `pandoc`, or the OP’s tool `Pal`) to solve specific command-line problems without burning expensive "reasoning" tokens or dealing with cloud latency.

### How I protect my Forgejo instance from AI web crawlers

#### [Submission URL](https://her.esy.fun/posts/0031-how-i-protect-my-forgejo-instance-from-ai-web-crawlers/index.html) | 103 points | by [todsacerdoti](https://news.ycombinator.com/user?id=todsacerdoti) | [56 comments](https://news.ycombinator.com/item?id=46345205)

A tiny Nginx “cookie gate” to fend off crawlers killing a Forgejo instance

- The author’s self-hosted Forgejo was repeatedly taken down by bots hammering the web UI and API (ignoring robots.txt). Heavy solutions like Anubis felt overkill, so they built a minimalist Nginx gate.
- How it works: on first visit, if the request isn’t from git/git‑lfs and a specific cookie isn’t present, Nginx returns a 418 page with inline JS that sets a cookie and reloads. After that, traffic passes normally. Git clones over HTTP/HTTPS work via a user‑agent allowlist.
- Why it’s effective: most unsophisticated crawlers don’t execute JS, so they never get the cookie and move on. Real users barely notice a single reload. A 302 with Set-Cookie didn’t help; JS did.
- Trade-offs: requires JavaScript; trivial to bypass once targeted; not a security control. But it’s cheap, fast to deploy, and good enough to shed abusive bot load for now. The author suggests renaming the cookie for uniqueness and acknowledges that widespread use would prompt crawler adaptation.
- Backstory/refs: inspired by a Caddy-based “You don’t need Anubis” approach; this is the Nginx variant tailored for Forgejo. Potential future escalation paths include Anubis or iocaine if bots adapt.

Why it matters: Self-hosters need lightweight, user-friendly defenses. This pattern offers a pragmatic middle ground between robots.txt (ignored) and heavyweight bot walls—especially for sites where git clients must remain unaffected.

Here is a summary of the discussion:

**The "Infinite Graph" Problem**
Commenters pointed out that Git forges are uniquely vulnerable to crawlers not just due to volume, but due to structure. Users like *marginalia_nu* and *pdrzg* explained that naive crawlers often get trapped in the "infinite graph" of git history (commits, diffs, blame views, and archives). A simple breadth-first search by a bot can explode into millions of unique URLs, effectively DDoSing the server by forcing it to reconstruct historical file views.

**Alternative Defense Strategies**
While the Nginx cookie gate was praised for simplicity, users discussed several other approaches to bot management:
*   **ASN Blocking:** *nclrdg* shared a strategy of tracking request rates by subnet and blocking entire ASNs (specifically targeting hosting providers and budget VPSs rather than consumer ISPs) when thresholds are crossed.
*   **Honeypots & Port Blocking:** *BLKNSLVR* suggested using "UninvitedActivity," a method that bans IPs attempting to connect to closed ports or non-Cloudflare whitelisted IPs attempting to access port 443.
*   **Poisoning:** Discussion drifted toward hostile responses, with *jkwl* mentioning "Iocaine," a project designed to serve garbage data to poison AI training sets, and others joking about serving Markov chains to scrapers.

**The Nature of the Traffic**
There was debate regarding the source of the abuse. Some users reported aggressive traffic from specific regions (e.g., Singapore) or specific bots (Amazonbot ignoring `robots.txt`). Others, like *m0llusk*, suggested that the "hammering" might not be sophisticated AI companies, but rather students and "script kiddies" running basic scrapers with no rate limiting as they learn to code.

**Technological Trade-offs**
*srbntr* noted the obvious downside: this strictly requires JavaScript, breaking access for privacy-focused users or non-standard browsers. *flxgn*, the author of the original "You don't need Anubis" article that inspired the submission, appeared in the thread to express support for the Nginx logic implementation.

### Waymo halts service during S.F. blackout after causing traffic jams

#### [Submission URL](https://missionlocal.org/2025/12/sf-waymo-halts-service-blackout/) | 278 points | by [rwoll](https://news.ycombinator.com/user?id=rwoll) | [382 comments](https://news.ycombinator.com/item?id=46342412)

Waymo pauses SF robotaxi service after blackout strands cars at dark intersections; resumes Sunday

- A citywide power outage in San Francisco on Dec. 20 knocked out traffic lights and reportedly spotty cell service, leaving clusters of Waymo autonomous vehicles stalled at intersections and blocking lanes across multiple neighborhoods. Videos showed lines of AVs blinking hazard lights while human drivers crept around them.
- PG&E’s rolling blackouts affected roughly a third of the city (about 125,000 customers). In response, Waymo suspended ride-hailing Saturday evening, saying it was focused on rider safety and keeping routes clear for emergency crews.
- Observers speculated that dark signals confused the cars’ decision-making and that flaky connectivity may have hampered remote assistance.
- The incident underscores a key AV challenge: resilience during infrastructure failures (power, comms) and how robotaxis handle all-way-stop scenarios at scale.
- Update: Waymo said Sunday afternoon its fleet was back on the roads.

Here is a summary of the discussion on Hacker News:

**Intersection Behavior and Local Driving "Culture"**
A significant portion of the discussion analyzed why the vehicles failed, with an eyewitness (`cjsplt`) noting that while a dead traffic light legally becomes a 4-way stop, the practical reality in San Francisco is very different.
*   **The Problem:** While human drivers in SF handle these scenarios with "slow rolls" and aggressive "interleaving" (zipper merging), Waymo vehicles appeared to wait for massive, clear slots to move.
*   **The Deadlock:** As the AVs hesitated, human drivers eventually navigated around them to fill the gaps. This behavior seemingly confused the AVs further or triggered a failsafe mode, causing them to freeze entirely and lose their "turn" in the rotation.
*   **Laws vs. Norms:** Users debated the strict interpretation of traffic laws versus local customs. While the law requires a full stop and yield-to-right, users noted that in high-traffic urban environments, strict adherence effectively brings traffic to a halt, whereas the flexible human approach maintains flow.

**Resilience and "The Montreal Test"**
The incident sparked skepticism regarding the readiness of AVs for conditions outside of fair-weather California bubbles.
*   User `b112` argued that if the fleet cannot handle a simple blackout, they are nowhere near ready for winter climates like Montreal, where snow obscures lane markings, ice changes breaking distances, and slush covers sensors.
*   The critique highlights that current AV tech relies on "perfect conditions" and struggles when visual indications are obscured or when "rules" become ambiguous.

**Emergency Response and "The Big One"**
The failure raised alarm regarding disaster scenarios, specifically a major earthquake.
*   Users worried that if a simple power outage causes a mass "bricking" of AV fleets, a major disaster could see these vehicles blocking evacuation routes and impeding emergency services.
*   Some questioned why the "failsafe" seems to be freezing in place rather than pulling to the curb, noting that in a true emergency, this behavior could be catastrophic.

**General Sentiment**
*   **Post-Mortem:** Commenters speculated on the internal turmoil at Waymo, guessing that the company will have to tweak software to handle "self-induced congestion" better.
*   **Reputation:** While some viewed this as a necessary learning step for the AI, others felt it was a significant reputation hit, dispelling the illusion that the cars are currently "smarter" than human drivers in complex, unscripted scenarios.

### Measuring AI Ability to Complete Long Tasks

#### [Submission URL](https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/) | 235 points | by [spicypete](https://news.ycombinator.com/user?id=spicypete) | [186 comments](https://news.ycombinator.com/item?id=46342166)

METR: Measuring AI ability to finish long tasks is doubling every ~7 months

- What’s new: METR proposes a simple, comparable metric for agentic LLMs—the task-completion time horizon: the human time length of tasks a model can complete with a given success rate (e.g., 50% or 80%). Code and raw data are released; full paper linked in the post.

- Key results:
  - Over the past 6 years, the 50% success horizon has grown exponentially with a ~7‑month doubling time (roughly 1–4 doublings/year).
  - Today’s best generalist agents nearly always solve tasks humans do in <4 minutes, but succeed <10% on tasks taking humans >~4 hours.
  - Example: Claude 3.7 Sonnet’s 50% horizon is about one hour.
  - Extrapolation: If the trend holds for 2–4 more years, general agents could autonomously handle many week‑long software tasks; a 10× calibration error would shift that arrival by only ~2 years.

- Method in brief: Use diverse multi-step software and reasoning tasks; measure completion time by qualified humans; fit a logistic curve for model success vs human time; read off horizons at fixed success rates. Uncertainty is shown via hierarchical bootstrap confidence intervals.

- Why it matters: This helps reconcile superhuman benchmark scores with models’ inconsistent usefulness on real projects—current limitations stem less from knowledge gaps and more from long-horizon reliability and sequencing of actions.

- Caveats: Results depend on task mix and human baselines; trends persist across subsets but get noisier. “Autonomous” capability here is defined by the evaluated agent setup; details are in the paper.

**Discussion Summary:**

The discussion focuses on the trade-off between productivity and pedagogical depth when using AI agents for development.

*   **The "Struggle" vs. Efficiency:** The thread began with user *sbdvs* noting they implemented vector search in 15 minutes using AI (down from 4+ hours) but felt they "learned essentially nothing." This sparked a debate on the role of friction in learning. *throwaway613745* and *bhy* argued that "grunt work," debugging, and failed assumptions are necessary to build implicit "working knowledge"—comparing AI usage to "paint by numbers" versus learning to draw.
*   **High-Value vs. Low-Value Friction:** *smnw* (Simon Willison) countered that not all struggle is educational. He argued that AI removes "useless struggle" (like fighting syntax errors or YAML configurations) which often causes beginners to quit, allowing learners to focus on higher-level concepts. He cited the **Worked-example effect** (Wikipedia), suggesting that studying completed solutions can be more effective than solving problems from scratch.
*   **Compression of Experience:** *mms* described LLMs as "compression algorithms for experience," prompting a discussion on whether using agents prevents developers from gaining the intuition required to maintain systems.
*   **Technical Debt and "Vibe Coding":** *vsrg* and *Applejinx* warned that agentic coding leads to "vibe coding" (running code until it looks right) without understanding the underlying logic. They argued this generates massive, soul-crushing technical debt that future maintainers—lacking the "struggle" history of the code's creation—will be unable to fix.
*   **Skill Rot:** Several users voiced concerns that if entry-level "grunt work" is automated, junior practitioners will never acquire the deep expertise necessary to become senior architects (*dns_snek*).

### Get an AI code review in 10 seconds

#### [Submission URL](https://oldmanrahul.com/2025/12/19/ai-code-review-trick/) | 135 points | by [oldmanrahul](https://news.ycombinator.com/user?id=oldmanrahul) | [61 comments](https://news.ycombinator.com/item?id=46346391)

Turn any GitHub PR into an instant LLM code review with “.diff”

A simple trick is making the rounds: append .diff to any GitHub pull request URL, copy the raw diff, and paste it into an LLM (Claude, ChatGPT, etc.) with a short prompt like “please review.” No Copilot Enterprise, extensions, or special tooling required.

Why it matters:
- Fast first pass: catches obvious bugs, missing tests, and edge cases before involving teammates.
- Shorter cycle times: you show up to human review with a cleaner PR and fewer nits.
- Works anywhere: it’s just the diff text, so it’s tool-agnostic.

How to try:
- Take your PR URL, e.g., https://github.com/USER/REPO/pull/123
- Add .diff: https://github.com/USER/REPO/pull/123.diff
- Copy the raw diff, paste into your LLM, and ask for a review.

Caveats:
- Not a replacement for peer review.
- Be mindful of org policy and IP—avoid pasting private code into external LLMs unless approved or use an enterprise/on-prem model.
- Very large diffs may need chunking or summarization.

Here is a summary of the discussion:

While the submission proposes a lightweight hack for AI code reviews using GitHub's `.diff` endpoint, the discussion reveals that most developers find simple diffs insufficient for high-quality feedback. The conversation evolved into a debate on context management, the role of AI in the review lifecycle, and preventing notification fatigue.

**Context and Workflow**
*   **The context bottleneck:** Several users pointed out that raw diffs often lack the context required for an LLM to provide meaningful insights. **Smaug123** and **infl8ed** argued that checking out the full branch or concatenating touched files yields significantly better results than just the diff. **fwmr** suggested using `-U99999` to provide full-file context, as LLMs often struggle to differentiate between unchanged context and actual modifications in standard diffs.
*   **Tools vs. Tricks:** While the `.diff` trick is free, many users prefer integrated tooling like Cursor or bespoke scripts that fetch repository context. **dnlstr** noted that workflows utilizing tools like Claude inside Cursor can spot mismatched coding styles, missing docs, and bugs effectively, serving as a "pre-pass" before human eyes.
*   **Model selection:** Opinions varied on model necessity. **Smaug123** argued that high-end models (like "GPT-5/High") are competent reviewers worth the cost, while **vrdvrm** advocated for faster, cheaper models like Gemini-1.5-Flash for routine tasks.

**Philosophy: Augmentation vs. Replacement**
*   **Low-hanging fruit:** There is strong consensus that AI excels at catching "dumb mistakes," missing edge cases, and enforcing lint-like rules, allowing humans to focus on the big picture. **smnw** and **bllq** highlighted AI's ability to spot logical errors and unused variables that humans miss due to fatigue.
*   **Knowledge transfer:** **mvnbk** strongly opposed using AI as a *replacement*, arguing that code review is the primary mechanism for knowledge sharing and architectural alignment within a team—something an LLM cannot replicate.
*   **The "Linter" Approach:** **mvnbk** and others suggested that AI feedback should be treated as a local pre-commit check or a strict linter rather than a conversational PR participant.

**Friction and Fatigue**
*   **Notification spam:** **mrldd** complained about automated AI reviewers being too "chatty," flooding teams with notifications for nitpicky or incorrect suggestions (1 useful comment per 20).
*   **The "Lazy Reviewer" Anti-pattern:** **fssl** shared a horror story of a CTO who copy-pasted raw AI outputs into PRs, creating massive amounts of useless noise and eventually getting AI reviews banned at the company. However, they noted AI is still useful for helping juniors *draft* reviews to learn the tone and structure of good feedback.

### Clair Obscur having its Indie Game Game Of The Year award stripped due to AI use

#### [Submission URL](https://www.thegamer.com/clair-obscur-expedition-33-indie-game-awards-goty-stripped-ai-use/) | 176 points | by [anigbrowl](https://news.ycombinator.com/user?id=anigbrowl) | [395 comments](https://news.ycombinator.com/item?id=46342902)

Indie Game Awards yank Clair Obscur’s GOTY over gen‑AI use; Blue Prince promoted

- What happened: After airing a pre-recorded show naming Clair Obscur: Expedition 33 Game of the Year and Debut Game, the Indie Game Awards rescinded both wins hours later, citing the game’s confirmed use of generative AI art during production.
- Why the reversal: Sandfall Interactive had attested no gen‑AI was used when submitting. On the day of the IGA 2025 premiere, the studio confirmed gen‑AI art had been used (since patched out), which violates the IGAs’ “hard stance” against gen‑AI in nominations and ceremony.
- New winners: Blue Prince is now Game of the Year; Sorry We’re Closed takes Debut Game. Their acceptance speeches will be recorded and published after the holiday break, likely early 2026.
- Context: A wider industry debate is brewing over acceptable AI tooling. Larian’s Swen Vincke recently said no generative AI content for its next Divinity, but endorsed AI-assisted pipelines (e.g., mocap cleanup). A resurfaced July 2025 interview had Sandfall’s COO acknowledging “some AI” in production.
- Why it matters: This is a rare post-broadcast disqualification that sets a strict precedent on gen‑AI for at least one awards body. It also spotlights a gray zone between banned “gen‑AI art” and permitted non-generative production tools—and raises questions about verification and transparency in awards submissions.

Based on the discussion, the reaction is divided between those blaming the studio for dishonesty and those criticizing the awards body for technological zealotry.

**Key themes include:**

*   **Dishonesty vs. Technicality:** Several users argue the disqualification is justified not just because of the art, but because Sandfall representatives explicitly answered "No" to questions regarding AI use during development. These commenters feel that using AI for placeholders technically constitutes "development," treating the check-box attestation as a lie.
*   **The "Digital Amish" Critique:** A significant contingent finds the "blanket ban" on AI unreasonable, labeling the awards body as "Digital Amish." They argue that using AI for internal placeholders (like brick textures) or tedious tasks (like lip-syncing) is standard efficiency ("work-saving") and distinct from replacing creative human connection.
*   **The "Speed Limit" Analogy:** A sub-thread uses speeding tickets as a metaphor. Users debate whether the "leftover" AI texture was a punishable offense (strict liability) or a negligible margin of error (like driving 10% over the limit) that should be tolerated in a massive project.
*   **Defining the Gray Area:** Commenters highlight the difficulty in drawing a moral line between "Generative AI" and other algorithmic tools. Users point out that tools for smoothing lines, noise reduction, or procedural animation effectively "generate" data, asking why those are acceptable while placeholder textures are not.
*   **"Witch Hunt" Accusations:** Some users describe the investigation as a "witch hunt" targeting a legitimate game over a minor oversight, though others push back, arguing that "witch hunt" is a loaded term used to discredit valid enforcement of stated rules.

