## AI Submissions for Wed Jun 25 2025 {{ 'date': '2025-06-25T17:15:07.242Z' }}

### -2000 Lines of code

#### [Submission URL](https://www.folklore.org/Negative_2000_Lines_Of_Code.html) | 466 points | by [xeonmc](https://news.ycombinator.com/user?id=xeonmc) | [191 comments](https://news.ycombinator.com/item?id=44381252)

In February 1982, the Lisa software team at Apple was under pressure to ship their software within six months, leading management to adopt a controversial productivity tracking method based on lines of code. Each engineer had to report their weekly code output, but Bill Atkinson, the brains behind Quickdraw and a pivotal player in user interface design, viewed this metric as counterproductive. Atkinson, who prioritized concise and efficient code, faced this challenge head-on when he innovatively slimmed down Quickdraw's region calculation, eliminating around 2,000 lines of code while significantly boosting performance.

When asked to submit his progress, Atkinson cheekily noted "-2000" lines for the week, underlining his belief that fewer, more effective lines of code were far more valuable than a bloated output. The anecdote underscores the silliness of equating productivity with sheer quantity, and after some time, management seemingly agreed, ceasing their demands for Atkinson's weekly reports. This story, shared on the folklore website, resonates widely with developers and managers, who praise Atkinson's classic lesson in quality over quantity. The comments reflect a universal understanding among programmers: the value lies within code efficiency, not volume—a timeless reminder for IT leadership everywhere.

The discussion revolves around the challenges and insights related to code efficiency, algorithm design, and management practices in software development. Key points include:

1. **Algorithmic Efficiency**: Participants shared experiences where leveraging graph theory (e.g., directed cyclic graphs, DFS/BFS traversal) and data structures like Tries drastically simplified code and improved performance. One user reduced API response times from ~500ms to 10ms by replacing XML/JSON bloat with streamlined logic.

2. **Code Quality Over Quantity**: Many echoed Bill Atkinson’s lesson, citing cases where removing code (e.g., 60k lines in a legacy server, 34k Turbo Pascal lines) or refactoring led to better outcomes. Some criticized management metrics that prioritize lines of code, highlighting how this incentivizes bloat over elegance.

3. **Learning and Tools**: Users debated the value of deeply understanding algorithms vs. rote LeetCode preparation, with recommendations to study foundational books and real-world problem-solving. Others emphasized visualizing problems (e.g., drawing graphs) over memorization.

4. **Management Pitfalls**: Stories of misguided practices included redundant code duplication to meet personal metrics, legacy systems ballooning to millions of lines, and the difficulty of convincing stakeholders to delete unused or inefficient code.

5. **Skepticism and Humor**: A sub-thread critiqued possible AI-generated comments, reflecting the community’s vigilance against low-effort content. Jokes about "inventing" solutions like CSV-based SQL workarounds underscored the iterative, often humorous nature of problem-solving.

Overall, the discussion reinforces that good software hinges on thoughtful design, algorithmic clarity, and resistance to superficial productivity metrics—lessons as relevant today as in Atkinson’s era.

### Build and Host AI-Powered Apps with Claude – No Deployment Needed

#### [Submission URL](https://www.anthropic.com/news/claude-powered-artifacts) | 285 points | by [davidbarker](https://news.ycombinator.com/user?id=davidbarker) | [120 comments](https://news.ycombinator.com/item?id=44379673)

Exciting news for developers and AI enthusiasts! Claude is rolling out a new feature that lets you build, host, and share AI-powered apps right from its platform. This means creators can now develop apps that interact with Claude via API, turning ideas into fully functional, interactive applications without having to worry about backend complexities or costs.

With this new capability, developers tap into their existing Claude accounts and API subscriptions, meaning usage doesn't hit your wallet—it counts against the user's subscription instead. Plus, no need to hassle with managing API keys. Claude can generate real code that you can tweak and freely distribute, opening up a world of possibilities for dynamic and responsive applications.

Early adopters have already crafted a host of exciting apps, from AI-enhanced games featuring NPCs with memory, to adaptable learning tools and smart data analysis solutions. Users have also reported successful creations of writing assistants and complex workflows deftly handled by multiple Claude interactions.

Getting started is a breeze: describe your app idea in Claude, and it handles everything from writing the initial code to debugging and improving based on your feedback. Sharing your creation is as simple as sending a link; no complicated deployment required. Claude even takes care of the technical nitty-gritty, letting you zero in on your creativity.

While the feature is in beta and carries some limitations—such as no external API calls or persistent storage yet—it already offers powerful capabilities. And if you're a Free, Pro, or Max plan user, you can jump right in and start exploring the limitless potential for creating innovative, custom AI solutions with Claude.

The Hacker News discussion on Claude's new AI app-building feature reveals a mix of enthusiasm, skepticism, and practical concerns:

### **Key Themes**
1. **Potential & Excitement**:  
   - Users highlight possibilities like AI-enhanced games with memory-retaining NPCs, learning tools, and custom productivity apps.  
   - The democratization of app creation (no backend hassles, instant sharing) is praised as a step toward an "AI future."  

2. **Technical Challenges**:  
   - **Unpredictable LLM Behavior**: Debugging prompts and ensuring consistent outputs is called "janky" and critical for reliability.  
   - **Cost & Scalability**: Fears of exorbitant token costs if apps go viral (e.g., half a million users could drain budgets rapidly). Suggestions include on-device models (like Firebase’s experimental APIs) to reduce expenses.  
   - **Limited Features**: Lack of persistent storage and external API access curtails app complexity, though workarounds like `localStorage` are proposed.  

3. **Ethical & Moderation Concerns**:  
   - Users stress the need for content controls to prevent harmful outputs (e.g., Holocaust denial, extremist ideologies).  
   - Trust issues arise around abrupt service changes, poor customer support, and accountability for user data.  

4. **Monetization & Business Models**:  
   - Skepticism about Anthropic’s potential revenue strategies, such as pushing users toward premium plans or taking a revenue cut from popular apps.  
   - Ideas for hybrid models include per-project fees, API call charges, or even an "AI App Store" (hypothetically by NVIDIA) taking a 30% cut.  

5. **User Experience Hurdles**:  
   - Downloading/installing apps vs. web-based interactions significantly impacts user adoption.  
   - Critiques of "fragile" app durability due to prompt brittleness and lack of context awareness.  

### **Notable Quotes & Insights**  
- **"AI hype vs. reality"**: While rapid prototyping is celebrated, many note that LLMs remain unreliable for mission-critical tasks without conventional logic layers.  
- **"Financial responsibility"**: Concerns over who bears costs for viral apps, with some speculating Anthropic might push users to higher-tier plans.  
- **"Ethical guardrails"**: Calls for strict content moderation to prevent misuse, with references to Claude’s role in filtering harmful ideologies.  

### **Conclusion**  
The discussion balances optimism about democratizing AI development with pragmatic warnings about costs, scalability, and ethical risks. While users see potential for innovation, they emphasize the need for robust tooling, transparent pricing, and safeguards to ensure Claude’s platform matures responsibly.

### Define policy forbidding use of AI code generators

#### [Submission URL](https://github.com/qemu/qemu/commit/3d40db0efc22520fa6c399cf73960dced423b048) | 476 points | by [todsacerdoti](https://news.ycombinator.com/user?id=todsacerdoti) | [330 comments](https://news.ycombinator.com/item?id=44382752)

In a significant policy update, the QEMU project has decided to decline any code contributions that are believed to be generated or derived from AI content generators. This includes tools like ChatGPT, Claude, Copilot, Llama, and others alike. The increasing use of AI in software development has raised legal concerns, primarily related to the ambiguous copyright and license status of AI-generated content.

Contributors to QEMU are required to certify that their patches comply with the Developer's Certificate of Origin (DCO), which entails a clear understanding of the copyright and licensing conditions of their contributions. Due to the uncertain legal standing of AI-generated content, especially when it comes from large language models with potentially restrictive or incompatible training data, the project is erring on the side of caution.

The policy excludes other AI uses like API research, static analysis, and debugging, as long as their outputs do not form part of contributions. As AI tools evolve and the legal framework becomes clearer, the policy may also change. Meanwhile, exceptions can be made if contributors can convincingly demonstrate that the AI tool's output meets the required licensing and copyright standards. This decision underscores QEMU’s commitment to maintaining legal compliance and clarity in its codebase.

**Summary of Discussion:**

The discussion on QEMU's ban of AI-generated code contributions revolves around **legal uncertainties**, **open-source sustainability**, and the **practical implications** of AI tools in software development. Key points include:

1. **Legal and Licensing Concerns**:  
   - Participants highlight vulnerabilities in open-source projects using AI-generated code, particularly around unclear copyright status and derivative work implications. The requirement for human contributors to certify code ownership (via DCO) clashes with AI’s opaque training data origins, risking license non-compliance.  
   - Debates emerge on whether AI could render traditional copyleft licensing obsolete, as proprietary entities exploit AI to bypass open-source obligations. Some argue this threatens the collaborative ethos of OSS by enabling corporations to monetize community efforts without reciprocation.

2. **Skepticism vs. Adoption of AI Tools**:  
   - While QEMU’s policy aims to preempt legal risks, skeptics question whether banning AI-driven contributions stifles innovation. Others counter that 100% human-authored code ensures legal clarity, especially for critical projects.  
   - Examples surface of developers using AI for rapid prototyping (e.g., generating QR code tools, browser scripts) and enhancing productivity locally, even if such code isn’t submitted to projects like QEMU. However, doubts linger about AI-generated projects overtaking traditional ones in quality or market competitiveness.

3. **Corporate and Market Dynamics**:  
   - Concerns arise over businesses quietly integrating AI to reduce costs and accelerate workflows without transparency, potentially marginalizing smaller developers. Critics warn of a future where AI-driven tools flood the market with low-quality, derivative software, eroding trust and support ecosystems.  
   - The tension between maintaining open-source principles and adapting to AI’s efficiency gains is palpable, with some predicting a bifurcation: “clean” human-led projects vs. forks embracing AI, each catering to different legal and ethical standards.

4. **Practical Enforcement Challenges**:  
   - Enforcing the AI ban is seen as difficult, given the indistinguishability of AI-generated code and its potential utility in non-submitted contexts (e.g., debugging, prototyping). Tools like GitHub Copilot already blur the lines, prompting calls for clearer legal frameworks.  

**Conclusion**:  
QEMU’s policy reflects caution amid unresolved legal gray areas, prioritizing compliance over innovation. However, the discussion underscores a divide: while some advocate for preserving human-centric, legally verifiable code, others view AI integration as inevitable, highlighting its utility in accelerating development despite associated risks. The path forward may hinge on evolving legislation and the OSS community’s ability to reconcile transparency with technological progress.

### Bot or human? Creating an invisible Turing test for the internet

#### [Submission URL](https://research.roundtable.ai/proof-of-human/) | 127 points | by [timshell](https://news.ycombinator.com/user?id=timshell) | [158 comments](https://news.ycombinator.com/item?id=44378127)

In the ongoing battle against bots, a new player has emerged: Roundtable's "Proof-of-Human" API, a stealthy guardian that verifies human presence online without the clunky interruptions of traditional CAPTCHAs. The innovative API taps into the distinct behavioral signatures that differentiate humans from AI—delving into the nuanced world of keystrokes and mouse movements.

Despite the dominance of systems like Google's reCAPTCHA v3, which scrutinizes user behavior to catch bots, there's a chink in its armor. Recent tests reveal AI agents can bypass these measures with unnaturally precise actions, highlighting a pressing gap in current defenses.

As AI continues to master traditional Turing Tests, behavioral analysis emerges as a promising frontier. Humans display unique quirks in typing and cursor navigation, while bots lack these idiosyncrasies, gliding through tasks with robotic efficiency. Curious minds can explore these disparities firsthand with interactive demos that juxtapose human and bot behaviors.

But what about cognitive tests? Enter the Stroop task—a psychological staple that confounds humans with color-word mismatches, causing delays in response. Bots, free from such human interference, breeze through unscathed, yet another demonstration of their non-human nature.

Amidst continuing research, the consensus is optimistic: while AI might mimic human actions, perfectly replicating cognitive psychology with its intricate processes remains a tall order. Studies suggest these behavioral biometrics offer a sturdy line of defense, economically challenging for fraudsters to overcome.

In this high-stakes game of digital cat and mouse, Roundtable's innovative methods promise a critical edge—transforming our everyday clicks and keystrokes into secure proof of human life online. For those eager to engage with these innovations, interactive tools offer a hands-on glimpse into the future of bot detection.

The discussion around Roundtable's "Proof-of-Human" API and bot detection explores several key themes:

### 1. **Challenges with Existing Systems**  
   - Traditional CAPTCHAs are disliked for disrupting user experience, while proof-of-work systems can be costly. Despite advancements like Google’s reCAPTCHA v3, AI agents increasingly bypass these defenses with unnatural precision.  

### 2. **Decentralized & Government IDs**  
   - **Decentralized Identifiers (DIDs)** are proposed as a long-term solution, allowing users to verify identity without revealing personal details. However, adoption hurdles and trust issues persist.  
   - **Government-issued IDs** (e.g., passports, Worldcoin’s biometric scanning) raise privacy concerns. Critics argue governments might misuse data, citing examples like NSA surveillance.  

### 3. **Trust & Privacy Concerns**  
   - Users debate whether centralized entities (governments or corporations) can be trusted with identity verification. Decentralized systems aim to mitigate this but face challenges in scalability and practicality.  
   - Behavioral biometrics (keystrokes, mouse movements) are seen as promising but risk enabling tracking via "fingerprinting," potentially compromising anonymity.  

### 4. **Economic Deterrents & Spam Mitigation**  
   - Making spam/bot attacks economically unviable (e.g., charging for email) is suggested, though skeptics note attackers adapt quickly.  
   - Analogies to postal systems highlight that increasing costs for bulk actions could deter bots but might harm legitimate users.  

### 5. **AI vs. Human Nuances**  
   - Bots lack human cognitive delays (e.g., Stroop test) and behavioral quirks, but AI’s rapid evolution threatens current detection methods. Striking a balance between security and user experience remains critical.  

### 6. **Real-World Tradeoffs**  
   - Solutions like forced user registration or stringent ID checks risk alienating users and stifling open platforms (e.g., Twitter’s struggles with spam).  
   - Critics warn dystopian outcomes if privacy is sacrificed for security, urging systems that protect rights without invasive tracking.  

### Final Takeaways  
While Roundtable’s behavioral analysis offers innovation, the broader conversation underscores the complexity of bot detection: **no solution is foolproof**, and balancing security, privacy, and usability remains a moving target. Decentralized frameworks and cognitive tests hold potential but require careful design to avoid unintended consequences.

### Anthropic wins fair use victory for AI – but still in trouble for stealing books

#### [Submission URL](https://simonwillison.net/2025/Jun/24/anthropic-training/) | 42 points | by [taubek](https://news.ycombinator.com/user?id=taubek) | [11 comments](https://news.ycombinator.com/item?id=44381639)

In a landmark case for the AI industry, Anthropic has scored a significant legal win regarding the incorporation of copyrighted texts into AI training data under the doctrine of fair use. The decision, handed down by Judge William Alsup, allows Anthropic to continue using millions of print books, which they scanned into digital form for internal research. This was deemed transformative and thus qualifying as fair use. However, the company still faces a jury trial concerning their unauthorized acquisition of millions of pirated ebooks, which do not qualify for fair use protection.

Anthropic, founded by former OpenAI researchers in 2021, initially relied on pirated libraries such as Books3 and Library Genesis to build their data resources. The recent ruling details how they later shifted strategies, investing heavily in purchasing and scanning print books to replace illicit copies. This case places a spotlight on the contentious issue of whether training Language Learning Models (LLMs) with unlicensed data falls under fair use. Judge Alsup's nuanced perspective equated the process to how humans read and internalize information, asserting that charging them for each act of using a book's information would be impractical.

The decision is particularly pivotal given the judge's reputation; Alsup, known for his tech-savvy approach in cases like Oracle v. Google, harnesses his programming hobbyist background in his legal reasoning. As this case unfolds, it highlights ongoing debates about intellectual property rights in an AI-driven world. Meanwhile, Anthropic's actions signal their resolve to create a vast, legally sound data library for AI development.

The Hacker News discussion on Anthropic's legal victory highlights several key themes and critiques:  

1. **Copyright Law vs. AI Scale**: Users argue that existing copyright frameworks are ill-equipped for AI's capabilities. While humans reading/using books is manageable, AI processing millions of texts simultaneously disrupts traditional economic models meant to incentivize creativity. One commenter likened it to charging humans for "breathing" CO₂ emissions, underscoring the impracticality of applying old rules to AI's scale.  

2. **Ethical and Legal Gray Areas**: Concerns were raised about corporations exploiting abstract rights and the lack of clear liability frameworks for AI systems. Comparisons were drawn to self-driving car liability, questioning who bears responsibility (e.g., developers vs. users) in cases of AI misuse or errors.  

3. **Cultural References**: A comment referenced Vernor Vinge’s *Rainbows End*, where digitized books from discarded copies fuel AI, mirroring Anthropic’s scanning strategy. Others critiqued the sourcing of data as akin to "stealing" or "burglary," highlighting ethical discomfort with AI’s data acquisition methods.  

4. **Industry Implications**: Speculation arose about future legal battles (e.g., Disney/Universal vs. OpenAI) and whether large media corporations might challenge AI’s use of copyrighted content, similar to past tech copyright disputes (e.g., Oracle v. Google).  

Overall, the discussion reflects skepticism about current laws keeping pace with AI’s transformative impact, ethical concerns over data sourcing, and the need for updated regulatory frameworks to address these novel challenges.

### DeepSpeech Is Discontinued (2020)

#### [Submission URL](https://github.com/mozilla/DeepSpeech) | 48 points | by [LorenDB](https://news.ycombinator.com/user?id=LorenDB) | [35 comments](https://news.ycombinator.com/item?id=44379688)

In a significant update on Hacker News, Mozilla's DeepSpeech repository has been archived as of June 19, 2025, marking the end of this open-source project's development. DeepSpeech, a pioneering speech-to-text engine, was lauded for its ability to operate offline and in real-time across a broad spectrum of devices—from the Raspberry Pi 4 to powerful GPU servers. Inspired by Baidu’s Deep Speech research, it leveraged Google's TensorFlow to simplify its implementation. Despite discontinuation, the project accrued an impressive 26.5k stars and 4.1k forks on GitHub, attesting to its wide adoption and community support.

For those interested in exploring the archives, documentation for installation and training models, along with the latest releases and pre-trained models, remain accessible on their GitHub page. While the project's active development has ceased, its extensive library of resources, including contribution guidelines and support information, provide lasting value. DeepSpeech's legacy lies in its contribution to making speech recognition more accessible and decentralized, empowering a generation of developers with the tools to innovate in the machine learning and speech recognition space.

**Summary of Submission:**  
Mozilla's DeepSpeech, a pioneering open-source speech-to-text engine, has been archived, ending active development. Known for offline, real-time transcription across devices (Raspberry Pi to GPU servers), it garnered 26.5k GitHub stars and 4.1k forks. Despite discontinuation, its legacy includes democratizing speech recognition and fostering decentralized AI innovation. Resources remain accessible for archival use.

**Discussion Highlights:**  
1. **Conspiracy & Organizational Criticism:**  
   - Users speculated whether Google’s financial ties to Mozilla influenced priorities, but others countered that Mozilla’s struggles stem from management missteps (e.g., pivoting to VR/metaverse/AIA and underinvesting in Firefox).  
   - Comparisons to Netscape’s decline and Firefox’s marketing challenges versus Chrome/Brave surfaced.  

2. **Technical Alternatives:**  
   - **Whisper (OpenAI/NVIDIA):** Praised for accuracy, even on Raspberry Pi. Users highlighted Whisper’s edge over Parakeet in multilingual transcription.  
   - **Piper TTS:** Noted for Raspberry Pi compatibility and real-time synthesis, though quality trails macOS’s built-in tools.  
   - **Migration Tools:** Projects like `parakeet-mlx` and `cq-STT` were suggested for transitioning from DeepSpeech.  

3. **Community Sentiment:**  
   - Disappointment over archiving, with some sharing personal efforts to maintain forks.  
   - Debates on Mozilla’s prioritization of experimental projects (Web3, AI) over core browser development.  

4. **Hardware Considerations:**  
   - GPU vs. CPU tradeoffs for real-time transcription, with Raspberry Pi users favoring lightweight models like Whisper’s distilled versions.  

**Key Takeaway:**  
While DeepSpeech’s discontinuation sparks critique of Mozilla’s strategy, the community is pivoting to robust alternatives like Whisper. Raspberry Pi users remain active in lightweight, offline-friendly solutions, emphasizing practicality over corporate dependencies.

### Web Translator API

#### [Submission URL](https://developer.mozilla.org/en-US/docs/Web/API/Translator) | 95 points | by [kozika](https://news.ycombinator.com/user?id=kozika) | [60 comments](https://news.ycombinator.com/item?id=44374748)

In this week's dive into developer tools on Hacker News, we've stumbled upon a fascinating update regarding the experimental Browser APIs for translation. These Translator and Language Detector APIs are packed with functionalities, offering developers a cutting-edge way to integrate translation capabilities directly into their applications. This suite includes the ability to check the availability of AI models, initialize a new Translator instance, and manage translation operations, all while keeping an eye on input quotas. Key methods include generating translation strings or even streams, promising seamless integration with various applications.

Given its experimental status, developers are advised to approach with caution and thoroughly consult the browser compatibility table before deploying in live environments. The APIs provide both synchronous translations and a streaming option, offering flexibility in how text can be translated. For a practical deep-dive into these features, the community is encouraged to refer to the complete examples provided in the documentation.

As web technologies rapidly evolve, tools like these push the envelope of multilingual web applications, paving the way for more inclusive and accessible software on a global scale. Keep contributing and discussing to help refine these capabilities and support the development community at large. For those intrigued by its potential or eager to contribute, feedback can be provided via the MDN documentation page, ensuring collaborative improvement and innovation.

**Summary of Hacker News Discussion:**

The discussion revolves around experimental **Browser Translation APIs** and their implications. Here’s a breakdown of key points and debates:

1. **Model Size & Resource Use**:
   - Google’s Chrome-based API reportedly requires **22 GB of disk space** for offline models, raising concerns about practicality, especially on mobile devices. In contrast, Firefox’s implementation uses **20-70 MB per language pair**, prioritizing efficiency. Critics question why Chrome’s models are so large.  
   - Some speculate that Chrome might push users toward paid Google APIs if local models are unmanageable.  

2. **Chrome vs. Firefox Approaches**:
   - Firefox’s lightweight models and explicit user consent for downloads (e.g., via API-triggered prompts) are praised. Chrome’s automatic model downloads without clear permissions draw skepticism.
   - Developers highlight potentials for **client-side extensions** (e.g., replacing Twitter’s broken translation button) and offline use in Firefox, though Chrome’s API remains more feature-rich.

3. **Standardization & Privacy Concerns**:
   - Mozilla and WebKit criticize Chrome’s API design for exposing sensitive data (e.g., model availability, download progress), risking **browser fingerprinting**. Advocates argue minimal information exposure is safer.
   - Debate arises over whether Chrome’s API should be standardized via **W3C**. Critics argue Chrome promotes its proprietary features as “standards,” while others defend community-driven standardization processes.

4. **Alternative Solutions**:
   - Developers mention **Hugging Face models** (e.g., `nllb-200`) or JavaScript/WASM-based translators, though these lag behind Google’s speed. One user reported translating 3k characters took 10 minutes with alternative tools vs. seconds via Google.
   - Suggestions include browser-prompted selective model downloads to save storage.

5. **Translation Quality**:
   - Complaints about `t-translate` (client-side tools) degrading text quality, with users abandoning certain tools. Some prefer server-side APIs but acknowledge trade-offs in privacy and cost.

6. **Adoption & Impact**:
   - Excitement exists for **local translation APIs enabling multilingual apps** (e.g., travel tools, comment translation). Concerns persist about Google’s dominance and whether smaller browsers can realistically adopt large models.

**Theme**: The community balances enthusiasm for modern translation capabilities with skepticism around resource demands, privacy, and vendor lock-in. Firefox’s privacy-centric model and Mozilla’s cautious stance contrast with Chrome’s ambitious but heavyweight approach. Standardization debates highlight tensions between innovation and interoperability.

### Things that AI cannot do

#### [Submission URL](https://www.mcsweeneys.net/articles/artificial-intelligence-cannot) | 35 points | by [sgt101](https://news.ycombinator.com/user?id=sgt101) | [9 comments](https://news.ycombinator.com/item?id=44380807)

In the latest edition of McSweeney’s Quarterly, Jason Gudasz delivers a whimsical and poignant piece titled "Artificial Intelligence Cannot," exploring the charmingly human experiences that remain elusive to AI. From experiencing existential yearning while gardening, to the uniquely awkward encounters with love interests—cue Doreen—a character named throughout, Gudasz takes readers on a delightful journey through the idiosyncrasies of human life still beyond the reach of artificial minds.

Subscribers to McSweeney's Quarterly can dive into Jason's work, and even get $5 off with the code TENDENCY. In a world lining up trends with life’s chaos, there’s also mention of Google Maps introducing walking speed accuracy filters, and a cheekily controversial piece declaring, "Congrats, Dipshit, You're a Dad Now" by Carlos Greaves.

McSweeney’s calls for support to keep their literary adventures ad-free, offering everything from a flamboyant 12-hour flash sale celebrating "The Believer" to inviting readers to become patrons. So, if you're yearning for literary surprises and poignant storytelling, it's time you became a part of McSweeney's world.

The Hacker News discussion on Jason Gudasz’s *McSweeney’s* piece, "Artificial Intelligence Cannot," blends critique, meta-debate, and appreciation:  

1. **Skepticism Toward AI’s Capabilities**: User `psygn89` critiques AI’s limitations, suggesting that even after a decade of development, it struggles with visually simple tasks despite advances in complexity. They appear disappointed by AI’s inability to match human nuance.  

2. **Debate Over the Article’s Intent**: A subthread involving `sgt101` and `thrwwyld` questions whether the piece should be taken seriously as a critique of AI or embraced as humor. While `thrwwyld` emphasizes the article’s satirical tone, others (e.g., `0_____0`) question if commenters misread the original submission, sparking a meta-debate about engagement and reading comprehension.  

3. **Appreciation for the Humor**: User `hypf` succinctly calls the piece “refreshing,” highlighting that some readers enjoyed its whimsical take on human experiences beyond AI’s reach.  

The discussion reflects a mix of analytic scrutiny of AI’s shortcomings, playful arguments over interpreting satire, and praise for the article’s creativity.

### Anthropic destroyed millions of print books to build its AI models

#### [Submission URL](https://arstechnica.com/ai/2025/06/anthropic-destroyed-millions-of-print-books-to-build-its-ai-models/) | 26 points | by [bayindirh](https://news.ycombinator.com/user?id=bayindirh) | [32 comments](https://news.ycombinator.com/item?id=44381838)

In a groundbreaking yet controversial move, AI company Anthropic has invested millions in physically scanning books to build Claude, an AI assistant akin to ChatGPT. This process, revealed through court documents, involved the massive destruction of print books, cutting them from bindings, and scanning them into digital formats—all to train their AI systems. Unlike the non-destructive scanning methods like those used by Google Books, which returned borrowed library books, Anthropic's approach opted for speed and cost-efficiency, sacrificing physical copies for digital ones.

Court rulings have deemed this method as falling under "fair use," mainly because Anthropic systematically purchased and subsequently destroyed its physical book copies, retaining the digital versions strictly for internal use. These tactics underscore the AI industry's ceaseless quest for high-quality data to feed vast language models, directly impacting their ability to generate accurate and cohesive outputs. The urgency in obtaining professionally edited texts without lengthy negotiations saw Anthropic bypass initial reliance on pirated ebooks for the legal safety of purchased books, albeit at the expense of their physical form.

While no rare books were claimed to have been harmed, this method starkly contrasts with initiatives like The Internet Archive's non-destructive methods or OpenAI's partnerships with institutions like Harvard, which preserve historic manuscripts while digitizing them.

Ultimately, Claude, the AI born from this transformation process, reflects on its creation from the "ashes" of discarded books, offering a narrative as intricate as the ethical and legal debates its existence stirs.

**Summary of Discussion:**

The discussion around Anthropic's book-scanning method to train AI (Claude) revolved around several key themes:

1. **Cultural and Fictional Comparisons**:  
   Commenters drew parallels to sci-fi narratives like Vernor Vinge’s *Rainbows End*, where books are shredded for digitization, and real-world historical efforts (e.g., reconstructing shredded Stasi files). These references framed the debate as both dystopian and pragmatic.

2. **Legal and Ethical Debates**:  
   - While a court ruled destructive scanning lawful under *fair use* (as Anthropic purchased books and retained digital copies for internal use), critics argued that legality doesn’t equate to ethicality.  
   - Concerns were raised about the “slippery slope” of normalizing destructive practices for corporate AI training, with some users condemning the wastefulness and disrespect for physical books.

3. **Environmental and Practical Concerns**:  
   - Critics highlighted the environmental impact of discarding physical books, though others countered that bulk recycling might mitigate waste.  
   - Non-destructive methods (e.g., Google Books, Internet Archive) were praised for preserving originals, while DRM restrictions on e-books were noted as a barrier, making physical book scanning a cheaper, legally safer alternative.

4. **Industry Practices and Criticism**:  
   - AI companies were accused of prioritizing cost-efficiency and data quality over ethical considerations. Some users dismissed Anthropic’s marketing framing, arguing that purchasing commodity books for destruction is neither novel nor noble.  
   - Rare books were reportedly spared, but critics emphasized the symbolic harm of treating books as disposable data sources.

5. **Broader Implications**:  
   - The case was seen as a microcosm of wider battles over copyright, transformative use, and corporate power in the AI era.  
   - Skepticism lingered about AI’s societal impact, with one user likening the race for AI dominance to a “Mile Island” scenario, hinting at unchecked risks.

In essence, the debate balanced technical necessity against ethical and cultural preservation, reflecting tensions between innovation and tradition in the digital age.
