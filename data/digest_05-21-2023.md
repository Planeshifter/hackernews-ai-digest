## AI Submissions for Sun May 21 2023 {{ 'date': '2023-05-21T22:22:11.209Z' }}

### Deep neural networks as computational graphs (2018)

#### [Submission URL](https://medium.com/tebs-lab/deep-neural-networks-as-computational-graphs-867fcaa56c9) | 79 points | by [softwaredoug](https://news.ycombinator.com/user?id=softwaredoug) | [28 comments](https://news.ycombinator.com/item?id=36020520)

Neural networks have often been referred to as "black boxes" due to their complex structure. However, understanding the mathematics behind neural networks and how they arrive at their predictions can provide valuable insight into their use. At the core of every neural network is a single mathematical function represented by a computational graph. A computational graph is a way of representing a mathematical function in the language of graph theory, where nodes represent input values or combining functions and edges receive their weights as data flows through the graph. While math notation can also be used to represent the same functions, computational graphs are preferable for more complex neural networks with hundreds of thousands of nodes and edges.

The discussion delves deeper into various topics. Users stress the fundamental importance of understanding mathematical expressions and graphs in analyzing and manipulating graphs, as well as the advantages of computational graphs over mathematical notation for complex neural networks. Additionally, there is a conversation on distinguishing different types of structures in computational graphs such as backpropagation. The discussion also explores the significance of graphs and compilers in math, with a focus on their benefits for deep learning systems.

Furthermore, the comments touch upon the relationship between neural networks and computational graphs and their significance in AI methods. There is also a conversation on the application of computing risk metrics in financial models, how neural networks are incorporated in these models and how analyzing computational graphs can allow for sensitivity analysis. One user also recommends watching a YouTube video which breaks down how to read math expressions in neural networks. Another user brings up the topic of Theano and TensorFlow and how PyTorch is increasingly being used partly because it enables control of the computational graphs used by the neural network.

### Perfectly secure steganography using minimum entropy coupling

#### [Submission URL](https://arxiv.org/abs/2210.14889) | 45 points | by [Topolomancer](https://news.ycombinator.com/user?id=Topolomancer) | [8 comments](https://news.ycombinator.com/item?id=36022598)

Researchers have developed what they claim is the first steganography algorithm to offer perfect security guarantees with non-trivial efficiency, according to a paper submitted to the Computer Science > Cryptography and Security arXiv. The team, from Carnegie Mellon University in Pittsburgh, used minimum entropy coupling to develop a scalable steganography procedure that is resistant to detection by adversaries. The researchers also found evidence of coupling in natural language and images, which should stimulate new approaches to steganalysis, the detection of hidden messages.

The discussion in the comments covers various aspects of steganography, its limitations, and its potential impact. One user notes the potential application of AI-generated steganography in transmitting confidential information, while another user points out the use of steganography in creating unbreakable watermarks for downstream content. Another user notes that steganography is different from encryption, and the two serve different purposes. A user mentions their interest in the references cited in the paper, while another user mentions their understanding of steganography and explains the process of decoding it.

### PrivateGPT

#### [Submission URL](https://github.com/imartinez/privateGPT) | 444 points | by [antouank](https://news.ycombinator.com/user?id=antouank) | [127 comments](https://news.ycombinator.com/item?id=36024503)

PrivateGPT is a new tool designed to interact with your documents using the power of GPT and provide 100% private communication without any data leaks. You can ingest your own documents, ask questions without an internet connection, and rely on LangChain, GPT4All, LlamaCpp, Chroma, and SentenceTransformers for the functionalities this tool provides. The instructions for setting up an environment, ingesting a test dataset, and asking questions locally are all available in the README file.

The submission is about PrivateGPT, a tool that allows users to interact with their documents using GPT for private communication. The Reddit discussion revolves around issues with compatibility and dependencies, such as the conflicts between different versions of Python or between Pyenv and Homebrew installation methods. Some contributors recommend using Docker or virtual environments to manage dependencies and isolate environments. The thread also discusses the advantages and challenges of using self-hosted and self-trained language models and machine learning tools for enterprise and consumer applications.

### The DRAKON Language

#### [Submission URL](https://drakonhub.com/en/drakon) | 216 points | by [brudgers](https://news.ycombinator.com/user?id=brudgers) | [38 comments](https://news.ycombinator.com/item?id=36021495)

DRAKON is a visual language used in the aerospace industry to represent algorithms, processes, and procedures. Its goal is to make procedures easy to understand, and it has gained recognition outside of aerospace among developers, project managers, and in medical and business fields. DRAKON is based on best practices for flowchart drawing, but also has unique features such as the skewer, silhouette, and common fate. While it is possible to draw DRAKON flowcharts in a general-purpose diagram editor, using a specialized tool such as DrakonHub provides a faster and smoother experience.

People in the discussion mention the existence of other fascinating languages such as Analitik, CAS Matlab, and Dragon. Despite its popularity, some people in the discussion prefer local data understanding and text-based interface over using DRAKON. There is an ongoing debate about the efficiency of using DRAKON compared to other programming languages/tools, and some people question the Soviet engineers' preference for graphic languages over traditional languages. Furthermore, some people mentioned the existence of previous discussions about DRAKON and its specifications, and a few provided relevant links. At the same time, others shared their practical experience in using DRAKON for non-trivial projects.

### AI boom could expose investors’ natural stupidity

#### [Submission URL](https://www.reuters.com/breakingviews/ai-boom-could-expose-investors-natural-stupidity-2023-05-19/) | 180 points | by [mirthlessend](https://news.ycombinator.com/user?id=mirthlessend) | [183 comments](https://news.ycombinator.com/item?id=36022768)

Behavioural economics has some important lessons for investors hoping to cash in on artificial intelligence (AI). The first lesson to beware of bubbles since the rush of investments in AI-related capital since OpenAI released its ChatGPT chatbot in November. Orthodox asset pricing models suggest that changing but rational assessments of future profitability drive the wild gyrations of the stock market. The second lesson is that natural stupidity can drive stock market valuations to unrealistic and ultimately unprofitable extremes. Finally, investors should question whether AI is able to replicate its extraordinary predictive ability in areas such as commercial, financial, and political life where the rules can be fuzzier.

The commenters in the following discussion thread explore topics such as the flaws in training language models, the potential of AI causing job loss, the limitations of AI in replicating predictive ability in certain domains, and the benefits and limitations of using AI in business operations. Additionally, there is discussion around the modeling of the market and the impact of human behavior on investment. Some commenters express concern for the speculation in the AI market leading to a potential bubble effect.

### GPT detectors are biased against non-native English writers

#### [Submission URL](https://arxiv.org/abs/2304.02819) | 325 points | by [giuliomagnifico](https://news.ycombinator.com/user?id=giuliomagnifico) | [258 comments](https://news.ycombinator.com/item?id=36019580)

A new study shows that widely-used GPT detectors for differentiating between AI and human-generated content are biased against non-native English writers. The study found that GPT detectors consistently misclassify non-native English writing samples as AI-generated while accurately identifying native writing samples. The authors caution against the use of these detectors in evaluative or educational settings, particularly when they inadvertently penalize or exclude non-native English speakers from the global discourse. The study suggests that these detectors may unintentionally penalize writers with constrained linguistic expressions and prompts for bypassing them.

The comments include several discussions on the nature of writing styles, with some users arguing that GPT detectors might penalize writers who use constrained linguistic expressions and prompts for bypassing these detectors. Other users discuss plagiarism in academic settings, where some argue that cheating is prevalent and can result in students passing their courses without actually learning. Finally, there are discussions about cultural perspectives on cheating and the effectiveness of plagiarism detection tools.

### Tarteel – AI-powered Quran companion

#### [Submission URL](https://www.tarteel.ai/) | 88 points | by [nraf](https://news.ycombinator.com/user?id=nraf) | [31 comments](https://news.ycombinator.com/item?id=36018332)

Tarteel, an AI-powered Quran companion, has advanced Quran memorization by interacting with recitation and notifying users of mistakes in real-time. The app enforces correct sentence structure and detects missed words or incorrect verses, presents similar verses when mistakes are made, and supports voice search and follow-along reading. Tarteel also allows users to test their memorization by hiding unrehearsed words, choose from over 112 Quran translations, and set custom challenges or track their progress with streaks. With over 20,000 ratings on the Google Play Store and the Apple App Store, the app has been discovered by over 4 million Muslims worldwide, making the Quran a daily habit for many.

The discussion around the Tarteel app is varied, with some people congratulating the team and discussing the app's functionalities while others discuss Islamic topics and question certain interpretations. One user recommends alternative applications for religious text memorization and expresses doubts about the helpfulness of AI in certain areas. There is also a discussion around the lack of a privacy policy on the Tarteel website. In addition, there is a debate about the use of AI in interpreting religious texts and its compatibility with various Islamic perspectives. One user shares their experience being forced to read the Quran in school in Iran, while another user argues against child marriage and defends marital relativism.

### Typical: Data interchange with algebraic data types

#### [Submission URL](https://github.com/stepchowfun/typical) | 122 points | by [g0xA52A2A](https://news.ycombinator.com/user?id=g0xA52A2A) | [48 comments](https://news.ycombinator.com/item?id=36019005)

Typical is a data serialization framework that generates efficient serialization and deserialization code for various languages, including Rust, TypeScript, and JavaScript. It allows for forward and backward compatibility between different schema versions and employs modern type systems based on algebraic data types for safer programming with non-nullable types and exhaustive pattern matching. Typical offers a solution to safely add or remove fields in record types without breaking compatibility, via its concept of asymmetric fields. Supported by the experiences of using Protocol Buffers and Apache Thrift, Typical is a modern solution for API developers who seek uncompromising type safety and binary compatibility between schema versions.

In the comments, the discussion covers topics such as human-readable encoding, the asymmetry of some fields, and how Typical compares to similar serialization frameworks. Some commenters also raised questions about the safety rules and fundamental changes, while others suggested using expressive type systems and integrating with existing systems. It is worth noting that Typical aims to offer a modern solution for developers seeking uncompromising type safety and binary compatibility.

### DarkBERT: A Language Model for the Dark Side of the Internet

#### [Submission URL](https://arxiv.org/abs/2305.08596) | 138 points | by [rajtilakjee](https://news.ycombinator.com/user?id=rajtilakjee) | [59 comments](https://news.ycombinator.com/item?id=36018657)

Researchers have introduced DarkBERT, a language model that is specifically trained on Dark Web data, which they argue offers valuable insights and benefits for researchers studying the Dark Web. The research suggests that the language used on the Dark Web is significantly different from that used on the Surface Web, and aims to combat the extreme lexical and structural diversity of the Dark Web in order to build a proper representation of the domain. The researchers evaluated DarkBERT against other widely used language models and found that it outperformed them in various use cases.

The comments section delves into the legality and ethics of accessing and analyzing Dark Web data, with some users expressing concern about sensitive and potentially illegal content such as child pornography and illicit drug sales. Others argue that law enforcement should monitor the Dark Web to prevent and prosecute criminal activity, while some express worry about the erosion of privacy and freedoms. The discussion also touches on issues such as copyright infringement, consumption of illegal material, and the ethics of demand for exploitative content. Overall, the comments section raises important questions about the use and limitations of language models like DarkBERT and the need for responsible and ethical analysis of the Dark Web.

### The Bitter Lesson – Rich Sutton (2019)

#### [Submission URL](http://incompleteideas.net/IncIdeas/BitterLesson.html?dup) | 28 points | by [hyperthesis](https://news.ycombinator.com/user?id=hyperthesis) | [7 comments](https://news.ycombinator.com/item?id=36017857)

In a thought-provoking post on Hacker News, AI pioneer Rich Sutton contends that the history of AI research shows that general purpose methods leveraging computation are ultimately the most effective. Rather than relying on human knowledge, breakthroughs in AI have come about through scaling up computation using search and learning. Sutton argues that researchers' psychological commitments to investment in their chosen approach can inhibit further progress, and reiterates the importance of recognizing the power of general purpose methods.

The top commenters on the submission discuss the relationship between energy efficiency and general purpose methods in AI research. One comment argues that general purpose methods and increased computation lead to greater efficiency, while another points out that there are limits to available energy and suggests that there may be other factors, such as stability, worth investing in. Another commenter provides a different perspective, suggesting that modern AI is built on interpretable models and fundamental knowledge. A fourth commenter adds that Sutton's thoughts are not particularly innovative, as he has written multiple books on the topic of reinforcement learning.
