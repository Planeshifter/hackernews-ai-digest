## AI Submissions for Thu Dec 25 2025 {{ 'date': '2025-12-25T17:09:10.820Z' }}

### Critical vulnerability in LangChain – CVE-2025-68664

#### [Submission URL](https://cyata.ai/blog/langgrinch-langchain-core-cve-2025-68664/) | 117 points | by [shahartal](https://news.ycombinator.com/user?id=shahartal) | [82 comments](https://news.ycombinator.com/item?id=46386009)

What happened
- A core serialization bug in langchain-core allowed attacker-shaped data to be treated as trusted structure. If a user-controlled dictionary included the reserved key lc, LangChain’s dumps()/dumpd() failed to escape it, so later deserialization could instantiate arbitrary LangChain objects.
- Because LLM outputs often populate fields like additional_kwargs or response_metadata, a single prompt can flow through logging, streaming, traces, memory, or caches and trigger the bug when that data is deserialized.

Why it’s a big deal
- It’s in Core, not a fringe tool or integration. LangChain has massive reach (hundreds of millions of installs; ~98M last month).
- Impact per the advisory (12 common flows affected): 
  - Secret exfiltration from environment variables, especially when deserializing with secrets_from_env=True (which was the default until the fix).
  - Object instantiation within pre-approved namespaces (langchain_core, langchain_openai, langchain_aws, langchain_anthropic, etc.), enabling side effects in constructors (network/file I/O).
  - Under certain conditions, it can lead to arbitrary code execution.
- Classified as CWE-502 (Deserialization of Untrusted Data), CVSS 9.3 (Critical).

Patches
- Fixed in versions 1.2.5 and 0.3.81. Update ASAP.

Who’s at risk
- Any app that serializes LLM/tool outputs or retrieved docs and later deserializes them via “normal” framework features (event streaming, logging, message history/memory, caches). You don’t need to accept explicit serialized blobs; a prompt alone can set this in motion.

What to do now
- Upgrade to 1.2.5 / 0.3.81 immediately.
- If available in your setup, disable secrets_from_env and rotate any secrets that could have been exposed.
- Treat LLM-influenced metadata as untrusted. Strip/validate unexpected lc keys before persistence or rehydration.
- Audit logs, traces, caches, and memory stores that may hold serialized data; assume they’re tainted until proven otherwise.
- Reduce blast radius: run with least privilege, restrict egress, and monitor for unusual constructor-time side effects.

Backstory
- Found by Yarden Porat (Cyata Research). The flaw wasn’t a bad load()—it was missing escaping in dumps()/dumpd(). The research underscores a recurring AI-security theme: when trust boundaries blur and attacker-controlled structure crosses them, “one prompt” can cascade into deep framework machinery.

**The LangChain Fatigue**
The vulnerability disclosure sparked a broader critique of the framework itself, with a significant segment of commenters treating LangChain as a negative signal in engineering culture.

*   **The "Hiring Filter":** Multiple users, including *prdgycrp* and *pb*, argued that relying on LangChain indicates a lack of fundamental understanding. Some described it as a "filter" for filtering *out* candidates; they prefer engineers who discuss orchestration and harnessing rather than "LangChain experts."
*   **Abstraction Bloat:** The consensus among critics (*int_19h*, *XCSme*) is that LangChain complicates simple tasks—specifically string concatenation and templates—into unnecessary enterprise abstractions. Users noted that features LangChain originally "solved," like Structured Output (JSON), are now natively supported by model providers (OpenAI, Gemini), rendering the framework’s "glue" redundant or brittle.
*   **Preferred Stacks:** The most recommended alternative is simply "hand-rolling" code using native SDKs (OpenAI/Anthropic) combined with **Pydantic** for validation (*pb*, *smtkmr*). Other mentions included BAML, Spring AI (for Java users), and avoiding vendor lock-in from tools like CrewAI.
*   **Language Wars:** A sub-thread debated whether Python is fit for complex agents. *vr* argued that Python’s lack of a static type system makes it poor for "stochastic" agent inputs, preferring strictly typed languages to handle the inherent randomness of LLM I/O, though *stngrychrls* defended Python's role as the industry standard.
*   **Meta-Commentary:** Several readers (*fn-mt*, *crmr*) suspected the vulnerability write-up itself was LLM-generated, citing phrases like "attacker-shaped data" and "blast radius" as hallmarks of AI prose.

### A framework for technical writing in the age of LLMs

#### [Submission URL](https://thedataquarry.com/blog/a-framework-for-reading-and-writing/) | 17 points | by [sebg](https://news.ycombinator.com/user?id=sebg) | [3 comments](https://news.ycombinator.com/item?id=46387034)

The author lays out a simple, reader-driven framework for better technical writing, built from how we actually consume long-form content online. They argue most good pieces operate on three layers:
- Outline (coarse): structure and vision — why should I care?
- Ideas (medium): core concepts and arguments — what are the important themes?
- Details (fine): examples, anecdotes, data, references — how does it work?

Good flow lets readers move smoothly between these layers without getting lost. When posts have fuzzy outlines, generic ideas, or thin details, they feel empty—even if the prose is polished.

That diagnosis doubles as a critique of AI-generated content. Citing Shreya Shankar and Ted Chiang (“blurry JPEGs of the web”), the post argues LLMs often fail at the outline layer (they don’t “care,” and prompt-to-output is lossy), then remix common themes, and finally skimp on grounded specifics—yielding slop that reads fine but says little. The author notes they used an LLM only at the end for light proofreading/paraphrasing; the ideas, outline, and flow were human.

Takeaways for writers in 2026:
- Start with an explicit outline and a clear promise to the reader.
- Make the ideas non-generic; if it feels like a mashup, it probably is.
- Invest in details: your own anecdotes, data points, and references.
- Obsess over transitions and flow so readers can glide between layers.
- Use LLMs as tools for polish, not substitutes for thinking.

Why it matters: As feeds fill with video fluff and AI-written long-form, this is a practical compass for producing human, substantive technical writing that stands out.

**Discussion Summary:**

Commenters largely reinforced the author's critique, arguing that the true value of long-form writing lies in human perspective, specific tastes, and the "desire to share"—elements that statistically generated text cannot replicate. The discussion zeroed in on three main points:

*   **Redundancy:** Several users noted that publishing generic, LLM-generated posts is fundamentally useless because readers can simply query an LLM directly if they want generic info. The utility of a blog post is the specific human experience that an LLM cannot hallucinate.
*   **The "Copy Editor" Role:** While rejecting AI as an author, commenters accepted its role as a copy editor for improving grammar and expression, provided there is transparency about its usage.
*   **Format Preferences:** Highlighting the critique of "fluff," one user suggested that for technical topics, readers often prefer raw bullet points and specs rather than AI attempting to weave them into a narrative.

### Claude Code changed my life

#### [Submission URL](https://spader.zone/xmas/) | 23 points | by [dboon](https://news.ycombinator.com/user?id=dboon) | [8 comments](https://news.ycombinator.com/item?id=46383266)

Thesis: The author argues LLM coding agents aren’t revolutionary at generating code—they’re “dear-god-how-did-we-ever-work-before-this” good at reading it. Treat them as a metal detector on an infinite beach (software), not a dredge that spits out new sand (code). The joy here is intrinsic: software as a fractal playground, not just a money machine.

Why this works:
- Grounded to a repo, agents cite files and line numbers, drastically reducing hallucinations.
- Read-only use preserves style and architecture.
- Low-stakes automation: it’s safe to let the agent search again; no messy rollbacks from bad writes.

What that unlocked for one “regular” programmer:
- Shipped “Claude Wrapped” (usage analytics) with a real DB and static hosting.
- Built a 3D raymarching ASCII renderer with multiple lighting models and a winter scene.
- Serialized internal build graphs to Mermaid; implemented a version resolver after surveying package managers.
- Wrote ~500 solid tests for a C standard library; implemented UTF-8 encode/decode/validate/iterate.
- Studied burntsushi’s glob library and built a similar one; parsed/wrote minimal ELF object files.
- Hooked a global Wayland hotkey to record Zoom audio, transcribe via LLM, and copy cleaned text.
- Indexed a decade of personal docs in a vector DB with an HTMX frontend; reconstructed chat logs from provider JSON.
- Built a TUI coding agent with a context compiler using the opencode SDK + OpenTUI; even made goofy Unicode animations.

Bottom line: LLMs as ultra-fast code readers/comprehenders are transformative. Point them at a specific corpus, demand citations, and let them winnow and explain—your velocity (and joy) will spike.

**The Art of Verification vs. "Vibe Coding"**

While the submission extols LLMs as superior readers, the comment section debates whether they foster actual skill or just create a dependency on "vibe coding."

*   **The Hype Cycle:** Skeptics argue that relying on AI creates an illusion of competence—"vibe coding"—that collapses when a project inevitably hits non-trivial bugs or scales beyond a basic proof-of-concept. They fear users are skipping the foundational understanding required to solve problems without an assistant.
*   **Code as a Commodity:** A 40-year industry veteran (assembly to K8s) pushes back, suggesting that writing code is now a commodity. From this perspective, the "art" of programming has shifted entirely to verification, TDD (Test Driven Development), and specification.
*   **The "Magazine" Analogy:** An interesting parallel emerged comparing LLM drawbacks to 1980s computing. Commenters noted that debugging subtly broken AI code offers a similar learning curve to typing in code from magazines and hunting for typos—serving as a frustrating but effective springboard for understanding how the system actually works.

### Show HN: Why many AI-generated websites don't show up on Google

#### [Submission URL](https://pagesmith.ai/seo-for-ai-generated-sites) | 12 points | by [manu_trustdom](https://news.ycombinator.com/user?id=manu_trustdom) | [5 comments](https://news.ycombinator.com/item?id=46385424)

HN summary: AI site builders vs. SEO, and why SSG wins

The post argues that many AI website/app builders ship marketing sites as client-side rendered SPAs, which quietly kneecaps discovery and performance. The “view source” test is the giveaway: if the initial HTML is just a shell with a root div, crawlers see an empty page.

Key points:
- Google’s two-wave indexing bites CSR: HTML is parsed immediately, but JS rendering is deferred to a render queue that can take hours or weeks. Fresh pages lag, crawl budget suffers, and you get “Discovered – currently not indexed.”
- Metadata duplication in SPAs: a static head means identical titles/descriptions across routes, creating duplicate-content signals and bad social previews.
- Core Web Vitals degrade under CSR: slower LCP (waiting on bundles + data) and more CLS (elements popping in), even if TTFB is fast.
- The fix is architectural, not “add an SEO plugin”: pre-render HTML (SSG), serve from the edge, ship zero-JS by default, and hydrate only interactive islands. Ensure unique per-page metadata.

Why it matters: For marketing sites that depend on organic discovery, using “app” builders optimized for dashboards/prototypes can cost rankings and traffic. The author pitches Pagesmith’s SSG-first, zero-JS-by-default approach as the remedy.

**Discussion Summary:**

The conversation shifted from the technical SEO arguments to the fundamental value of AI-generated websites. Several users expressed skepticism about the existence of "good" AI-generated pages, with some preferring that search engines ignore such content entirely to prevent spam. Conversely, it was argued that search algorithms prioritize quality over origin, implying that high-quality AI sites should still rank. On a practical level, commenters critiqued the visual output of these tools, noting that some examples suffered from broken layouts (like overlapping text) and resembled "spam-injected" contact forms.

### Silicon Valley's tone-deaf take on the AI backlash will matter in 2026

#### [Submission URL](https://fortune.com/2025/12/23/silicon-valleys-tone-deaf-take-on-the-ai-backlash-will-matter-in-2026/) | 83 points | by [howToTestFE](https://news.ycombinator.com/user?id=howToTestFE) | [31 comments](https://news.ycombinator.com/item?id=46380730)

Silicon Valley’s tone-deaf take on AI backlash will matter in 2026 (Fortune, Sharon Goldman)

- The gist: Goldman argues that builders’ “look what my model can do” excitement clashes with how most people actually experience AI—through job anxiety, rising costs, local data-center fights, and a sense that benefits flow to a narrow few. That disconnect, she says, is poised to harden into broader political backlash in 2026.

- Why it matters: VC pitches about “competition with China” and miracle productivity don’t land when housing and healthcare dominate daily life. Ordinary users don’t want demos; they want answers on jobs, prices, winners/losers, and who’s accountable. Without that, skepticism is rational—not ignorance.

- Snapshot of the gap: Insiders thrill to “a computer friend that never takes a day off”; outsiders hear “a tireless rival coming for my livelihood” (and a bigger power bill in my town). The framing problem is cultural, economic, and local—not just technical.

- A warning from within tech: 8VC’s Sebastian Caliri says the country is “polarized against tech” and urges a clearer story with tangible benefits people can believe in—fast.

- 2026 risk: If AI leaders keep leading with awe instead of answers, expect more organized pushback—policy, permitting, and public-opinion headwinds that slow rollouts and raise costs.

- Also in the roundup:
  - Faith-based critiques gain volume: Christian leaders across denominations are pressing for caution on AI’s effects on family life, labor, children, and religion (Time).
  - Energy is the battleground: Google Cloud’s long game centers on custom silicon and power constraints; even underground salt caverns for grid storage are in play.
  - AI coding consolidation: Cursor acquired code-review startup Graphite as competition intensifies.
  - Pricing optics matter: Instacart ended AI-driven pricing tests that increased costs for some shoppers—another sign consumers won’t tolerate opaque AI-driven price shifts.

- Takeaway for builders: Stop trying to impress; start de-risking the everyday. Show net-new jobs, lower bills, community benefits for data centers, and clear guardrails. Otherwise, 2026 will be the year the backlash hardens.

Based on the discussion, here is a summary of the comments on Hacker News:

**The Rationality of Backlash**
Users largely validated the article's central thesis: the pushback against AI is logical rather than ignorant. The most prominent thread argued that the hostility stems from a realization that, unlike previous tech cycles where the PC acted as an equalizing force, current AI advances centralize power. Commenters noted that productivity gains are flowing exclusively to capital owners while white-collar workers are now facing the same displacement anxiety blue-collar workers have felt for decades.

**Corporate "Salivation" and Gaslighting**
There was significant resentment regarding the "optics" of corporate leadership. Users expressed disgust at CEOs "openly salivating" over the prospect of firing humans to replace them with AI. However, skepticism remains about the reality of these claims; some commenters argued that companies (referencing Amazon specifically) are using "AI productivity" as a narrative shield to conduct standard layoffs and boost stock prices, even when the technology isn't actually doing the work yet.

**The "Marie Antoinette" Disconnect**
The cultural gap mentioned in the article was starkly highlighted by users reacting to the roundup's mention of an OpenAI researcher finding "relief" in a "computer friend." Commenters viewed this not as a triumph, but as a symptom of deep mental disconnect, reinforcing the idea that tech insiders live in a "Marie Antoinette bubble," oblivious to how their "computer friend" narrative sounds to people worried about bills and employment.

**Existential Philosophy vs. Real World**
Illustrating the disconnect, a controversial sub-thread debated an abstract defense of tech accelerationism (citing Peter Thiel). One user argued that humanity must endure suffering and prioritize advancements to survive hypothetical future threats (like alien invasions). This logic was widely ridiculed by others as absurd "destroy the village to save it" thinking, further proving the article's point that tech rhetoric often fails to address the immediate, tangible reality of the average person.

