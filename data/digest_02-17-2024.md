## AI Submissions for Sat Feb 17 2024 {{ 'date': '2024-02-17T17:10:38.171Z' }}

### Code for the Byte Pair Encoding algorithm, commonly used in LLM tokenization

#### [Submission URL](https://github.com/karpathy/minbpe) | 66 points | by [magoghm](https://news.ycombinator.com/user?id=magoghm) | [21 comments](https://news.ycombinator.com/item?id=39407407)

The repository "minbpe" by karpathy provides minimal and clean code for the Byte Pair Encoding (BPE) algorithm, commonly used in Large Language Models (LLMs) tokenization. The BPE algorithm operates at the byte level on UTF-8 encoded strings and has been popularized by papers like GPT-2 and GPT-4 for training tokenizers. 

The repository includes two tokenizers: BasicTokenizer and RegexTokenizer, with RegexTokenizer extending the text splitting by categories before tokenization approach. There's also a GPT4Tokenizer that replicates tokenization in GPT-4. The provided `train.py` script demonstrates training the tokenizers on input text and saving the vocabulary for visualization. 

Usage examples are given in the code files, showcasing how to train, encode, and decode text using the implemented tokenizers. There are future plans to optimize the Python version for handling large files, potentially create a C or Rust version, and explore adding support for GPT-2 with a renamed tokenizer. Additionally, there is a plan to create a LlamaTokenizer inspired by SentencePiece and handle special tokens.

This repository is licensed under the MIT license and has garnered attention with 1.9k stars and 78 forks on GitHub.

### Automated Unit Test Improvement Using Large Language Models at Meta

#### [Submission URL](https://arxiv.org/abs/2402.09171) | 287 points | by [mfiguiere](https://news.ycombinator.com/user?id=mfiguiere) | [180 comments](https://news.ycombinator.com/item?id=39405996)

The paper titled "Automated Unit Test Improvement using Large Language Models at Meta" introduces Meta's TestGen-LLM tool, showcasing how it leverages LLMs to enhance existing human-written tests. This innovative approach ensures that the generated test classes surpass specific filters, enhancing the original test suite without falling into LLM hallucination pitfalls. The successful deployment of TestGen-LLM at Meta test-a-thons for Instagram and Facebook platforms highlights significant improvements in test case building, reliability, and coverage. With an 11.5% enhancement rate and 73% recommendation acceptance for production deployment by Meta software engineers, this marks a pioneering industrial-scale implementation of LLM-generated code for code enhancement.

The discussion on the Hacker News submission centers around various aspects of software testing and quality assurance. 
- One user talks about the challenges faced when trying to improve test coverage in legacy codebases and the importance of having experienced programmers handle test-related tasks effectively.
- Another user discusses the efforts being made in trying to make computers intelligent and the limitations in achieving true artificial intelligence.
- The conversation delves into the topic of business apathy towards focusing on complex metrics rather than solving real-world problems.
- The debate about the significance of magic numbers in code arises, with some users emphasizing the importance of clear and meaningful naming conventions for constants.
- Users share different perspectives on the use of magic numbers in code and the potential confusion they can cause, especially in mathematical contexts.
- The concept of mutation testing, its benefits in detecting faults, and the challenges of implementing it in large Java projects are also discussed.
- There is mention of the struggles faced when dealing with automated quality tools like Sonar and the tension between developers and management regarding code quality priorities and adherence to best practices.
- Additionally, the conversation touches upon the strategies and tools available for mutation testing in software development and ways to improve test efficiency and speed. 
Overall, the discussion showcases a diverse range of opinions and insights into the complexities of modern software development practices.

### I worry our Copilot is leaving some passengers behind

#### [Submission URL](https://joshcollinsworth.com/blog/copilot) | 232 points | by [headalgorithm](https://news.ycombinator.com/user?id=headalgorithm) | [130 comments](https://news.ycombinator.com/item?id=39411912)

In a recent blog post, the author expresses concerns about AI coding tools like GitHub Copilot and their impact on code quality and accessibility, especially in web development. While acknowledging the time-saving benefits of Copilot, the author worries that the tool might inadvertently worsen accessibility on the web by generating code that is not properly optimized for all users.

The post highlights instances where Copilot's suggestions were far from ideal, including the comical recommendation of starting components with excessive nested divs. Despite the humor in these situations, the author raises a more serious issue regarding the potential negative effects of relying too heavily on AI-generated code.

By sharing a personal experience of creating a simple footnote component in Svelte, the author demonstrates how Copilot's suggestions may not always align with best practices or accessibility standards. This example serves as a cautionary tale about the importance of critically evaluating AI-generated code and considering its broader implications for inclusive web development.

The discussion on the Hacker News submission revolves around the concerns raised by the author regarding AI coding tools like GitHub Copilot and their potential impact on code quality and accessibility in web development. 

Several users engage in a nuanced conversation about the efficiency of code writing versus time required, with references to the "ninety-ninety" rule and discussions about the challenges and benefits of using AI-generated code. Furthermore, there are reflections on the implications of relying heavily on AI assistants during coding interviews and the need for developers to critically evaluate the code suggestions provided by such tools. 

The conversation also touches on issues related to software development practices, the importance of understanding underlying concepts rather than blindly copying code, and the potential risks of overreliance on AI tools in coding tasks. Users share experiences and insights on the balance between efficiency and understanding in coding, emphasizing the significance of mindful coding practices and critical thinking in inclusive web development.

### New Google Chrome feature blocks attacks against home networks

#### [Submission URL](https://www.bleepingcomputer.com/news/google/new-google-chrome-feature-blocks-attacks-against-home-networks/) | 55 points | by [Wasserpuncher](https://news.ycombinator.com/user?id=Wasserpuncher) | [25 comments](https://news.ycombinator.com/item?id=39411976)

Google is stepping up its security game with a new Chrome feature aimed at protecting home networks. This innovative tool blocks malicious websites from hijacking devices or services on internal networks, like printers and routers, by leveraging the browser as a gateway. By conducting preflight checks and seeking permission from the target device, Chrome aims to prevent attacks originating from the web. Developers will receive warnings in the DevTools console during the testing phase, allowing adjustments before full enforcement kicks in. This enhancement seeks to safeguard against unauthorized access to local devices and routers, addressing concerns around web interface vulnerabilities. The implications are far-reaching, as Google pushes the boundaries of browser security to combat internet-based threats effectively.

The discussion on Hacker News regarding Google's new Chrome feature aimed at protecting home networks revolves around various viewpoints and considerations. Some users express concerns about the potential conflict of interest due to Google being a major tech company with advertising interests, suggesting that the blocking of ads might interfere with Google's revenue model. Others argue that blocking ads does not necessarily harm the web ecosystem and that Chrome's actions can be seen as intrusive and against Google's interests. There are also discussions about the impact on web standards, privacy, and the potential interference with Google's business model if Chrome blocks ads. Furthermore, the conversation delves into the technical aspects of the feature, such as the ability to block outsider intrusions on local area networks and the implications for IoT devices. Overall, the discussion covers a range of perspectives on the implications and technical aspects of Google's new security feature in Chrome.

### Experimenting with GPTs custom actions, an example written in Rust

#### [Submission URL](https://danielegarbagnati.com/articles/neuro-rs) | 30 points | by [danigrb](https://news.ycombinator.com/user?id=danigrb) | [3 comments](https://news.ycombinator.com/item?id=39411380)

The big news today is about ChatGPT Custom Actions, a feature that allows GPT to perform specific tasks within a conversation, like generating images or querying databases. Custom actions take this a step further, enabling GPT to perform specialized tasks defined by developers, such as checking the weather or ordering a pizza. The potential of custom actions is huge, as they could lead to a new way of interacting with apps where users can simply ask their app to do things instead of clicking through menus. This feature is currently available in the GPT store to subscribers, sparking thoughts about the evolution of software development towards defining essential rules and API specifications for specialized UI like GPT to create personalized experiences.

To showcase this concept, a practical example using Rust shows how an app built around ChatGPT interacts with OpenAPI Specs, a Custom Backend REST API, and an OIDC Provider for authentication. The implementation details, including using Axum as the web framework in Rust and creating CRUD REST API endpoints for todos, demonstrate how custom actions can be integrated into an app effectively. Overall, the rise of ChatGPT Custom Actions signals a shift towards more conversational and intuitive interactions with technology, potentially reshaping the future of software development.

The comments on the submission mainly discuss the experimentation and potential of using ChatGPT Custom Actions. One user highlighted the potential to try incorporating player functionality in RPGs using this approach, and another expressed appreciation for the implementation of a proof of concept involving embedding vectors and databases into applications using the GPT platform. Additionally, there was mention of exploring custom actions as an alternative to traditional software interactions in a blog post shared on GitHub, emphasizing practical implementations in Rust with OIDC authentication. Overall, the discussion reflects a positive outlook on the possibilities and applications of ChatGPT Custom Actions in software development contexts.

### O(zero)

#### [Submission URL](https://koliber.com/articles/o-zero) | 18 points | by [koliber](https://news.ycombinator.com/user?id=koliber) | [5 comments](https://news.ycombinator.com/item?id=39409121)

Today on Hacker News, a post by Krystian Cybulski delves into the fascinating world of algorithmic complexity. The author takes us on a journey from the familiar realms of O(n^2) and O(n) to the more optimized O(log(n)) and the coveted O(1). But just when you think you've reached the pinnacle of efficiency, along comes the concept of O(zero). Yes, zero time! 

O(zero) challenges the notion that constant time is the best we can achieve in algorithmic efficiency. It's about questioning whether a task even needs to be done in the first place. The fastest code, after all, is the one that doesn't need to run at all. By eliminating unnecessary steps or processes, engineers can achieve remarkable gains in efficiency.

The key takeaway? Sometimes doing nothing at all is the most efficient solution. It's a powerful concept that can elevate your problem-solving skills to new heights. So next time you're optimizing code, remember to ask yourself: is there a way to accomplish this task without doing anything? The answer might just surprise you.

In the discussion on the submission about algorithmic complexity, users had varied reactions. 

- "cmrx64" mentioned the Richard Clarkson Open Source Institute identifying algorithms that may question the time spent on computing an answer, mentioning the possibility of an O(0) algorithm where no answer sharing is needed. They noted that documenting examples offline could be problematic and questioned what classic algorithms inherently provide a better answer.
- "drts" expressed that it seemed like an empty read.
- "czzyd" made a comment about a Baikal, and then there was a nested reply by "mnhtp".
- "az09mugen" simply commented "Nice pen." 

Overall, the discussions touched on the complexity and nuances of algorithmic efficiency, with some users raising interesting points and reflections related to the topic.

### Sierra Says Conversational AI Will Kill Apps and Websites

#### [Submission URL](https://www.wired.com/story/plaintext-sierra-conversational-ai-will-kill-apps-and-websites/) | 12 points | by [bookofjoe](https://news.ycombinator.com/user?id=bookofjoe) | [15 comments](https://news.ycombinator.com/item?id=39413575)

Two Silicon Valley leaders, Bret Taylor and Clay Bavor, have set out to revolutionize customer experiences with their AI startup, Sierra. The company aims to enhance interactions between big corporations and their customers using AI-powered agents. Sierra's approach, focusing on AI advancements for mainstream companies, challenges the traditional pursuit of superintelligence in the tech industry.

By implementing innovative AI models and safeguards against misinformation, Sierra's agents are designed to understand and represent a company as effectively as a human employee. This human-like touch includes providing empathy at scale, a concept that WeightWatchers embraced when Sierra promised genuine and relatable AI interactions. Despite skepticism about robots displaying empathy, WeightWatchers found value in the emotional support their AI agents could offer to customers in need.

The potential impact of Sierra's work extends beyond just improving customer interactions; it could redefine how companies engage with their audience in the digital realm. Through their advanced AI technology, Sierra is striving to shift automated customer interactions from frustrating experiences to positive ones, ultimately transforming how businesses exist in the digital landscape.

- **Zetobal** commented on the submission, but the content seems to be encoded in a way that is not understandable.
- **lxgr** mentioned the challenges of conversational UI in terms of solving concrete problems while also highlighting the difficulty in generating various responses. They hinted at the possibility of more sophisticated AI that can read minds to have more natural conversations.
- **yzzk** sarcastically suggested that companies making chatbots believe that chatbots can replace actual human employees.
- **vvzkstrl** brought up the recall of chatbots killing websites and apps in 2016, referencing Pepperidge Farm's meme.
- **llamaLord** discussed the limitations of chat-based interfaces due to their high focus on providing precise answers, which can lead to narrow and boring interactions. They highlighted the inefficiency of text-based communication in discovering information compared to other methods.
- **sfk** expressed that people prefer human-like conversational UIs.
- **pxmpxm** explained the limitations in transferring vehicle information through conversations and suggested separating conversational knowledge content from structured content.
- **wslh** questioned if conversational AI is the future of interface communication and mentioned that search engines like Google dominate the market due to their efficiency in providing quick results in a single interface.
- **dvngnt_** pointed out the potential problem where chatbots might kill digital assistants.
- **nprtm** shared insights on the training data required for systems like chatbots and mentioned the differences in training systems for web content versus search engines.
- **throwaway6733** discussed Clay's background at Google and their involvement in creating chat interfaces and visual searches. They highlighted the rapid growth of the enterprise chat service market.
- **DrNosferatu** commented on the thought-provoking nature of the conversation.
- **bkfj** shared an archive link.
- **j45** suggested making conversational approaches more approachable and expressed the need for transparency in information presentation on screens.
- **RecycledEle** threw in a disclaimer about resisting links starting with "https" and warned about potential security risks online.

### New chip opens door to AI computing at light speed

#### [Submission URL](https://phys.org/news/2024-02-chip-door-ai.html) | 36 points | by [thunderbong](https://news.ycombinator.com/user?id=thunderbong) | [10 comments](https://news.ycombinator.com/item?id=39407525)

The University of Pennsylvania has made a groundbreaking advancement in AI technology with the development of a new chip that operates at the speed of light, using light waves instead of electricity for complex mathematical computations. This innovation has the potential to revolutionize computer processing speed and energy efficiency. The chip, known as a silicon-photonic (SiPh) chip, combines Nanoscale manipulation techniques with silicon technology to perform calculations at unprecedented speeds. This technology could be a game-changer for AI applications, offering faster speeds, lower energy consumption, and enhanced privacy features. The implications of this new chip are vast, with applications ranging from neural networks to graphics processing units. Read more about this exciting development in AI computing at light speed in the full article provided by the University of Pennsylvania.

Discussion Summary:
- **crtsf** shared a link to a paper published in Nature Photonics and arXiv discussing the University of Pennsylvania's breakthrough in photonics chips. The paper describes the chip's ability to solve matrix-vector and matrix-matrix products efficiently, demonstrating the potential for larger-scale wave-based analog computing platforms.
- **mrnq** commented that this technology may not work well for machine learning tasks involving matrices with billions of parameters, as light-based calculations may have limitations when dealing with large matrices.
- **xsctt** pointed out that NVIDIA's tensor cores can handle large matrix calculations efficiently, even with matrix sizes as large as 32000x32000.
- **fnrdpglt** highlighted the potential for large-scale parallel processing using 2x2 to 10x10 matrix sizes demonstrated in the proof-of-concept.
- **mchlb** mentioned reading about light chips being a futuristic concept akin to the mythical battery technologies of the 1990s, but expressed skepticism about tangible products resulting from this technology.
- **DrNosferatu** shared a link related to neuromorphic engineering and integration as a point of novelty.
- **tmkld** raised concerns about the vulnerability of virtually unhackable technology in terms of memory management and potential security risks.
- **JieJie** shared a link related to buffer overflow vulnerabilities.
- **crnbrrytrky** made a short enthusiastic comment about the topic.

Overall, the discussion touched upon the potential limitations and applications of the University of Pennsylvania's light-based chip technology for AI computing, raised concerns about security vulnerabilities, and discussed related concepts in neuromorphic engineering and memory management.

### AI hiring tools may be filtering out the best job applicants

#### [Submission URL](https://www.bbc.com/worklife/article/20240214-ai-recruiting-hiring-software-bias-discrimination) | 63 points | by [rntn](https://news.ycombinator.com/user?id=rntn) | [75 comments](https://news.ycombinator.com/item?id=39412283)

AI hiring tools have been a hot topic on Hacker News today. Companies are increasingly turning to artificial intelligence-driven platforms to screen job applicants, with tools like body-language analysis, vocal assessments, and CV scanners being used to filter candidates. However, concerns are rising that these AI tools may be excluding highly qualified candidates, leading to potentially harmful repercussions. 

These tools are supposed to help eliminate biases in the hiring process but some experts argue that they could actually be exacerbating the problem. One example highlighted in the article is a make-up artist who lost her job after an AI tool scored her body language poorly. Similarly, biases in the algorithms can lead to qualified candidates being unfairly rejected based on factors like hobbies or educational background.

There are fears that marginalized groups could be disproportionately affected by these tools, as well as concerns about the lack of transparency in how candidates are evaluated. Additionally, there are worries that companies may not have the incentive to address biases in these systems, especially as they have replaced human HR staff with AI to save time and money.

Efforts are being made to address these issues, with initiatives like the Conditional Demographic Disparity test being developed to help companies identify biases in their AI algorithms. Ultimately, the goal is to have AI tools that are fair, unbiased, and capable of making merit-based decisions to benefit both the candidates and the companies.

The discussion on the topic of AI hiring tools includes various perspectives and concerns. 

1. One user pointed out that non-verbal interviews, body language analysis, and vocal assessments are highly discriminatory towards individuals who may not fit the standard criteria set by these tools.
2. Another user highlighted the importance of considering demographic variance in these tools and the potential biases they may carry.
3. There was a discussion about personal projects listed on resumes and the significance of effectively showcasing skills during the interview process.
4. The debate extended to the idea that self-promotion and interpersonal skills are essential for a successful hiring process, and some users emphasized the importance of these skills in the engineering field.
5. The discussion touched upon concerns about AI tools potentially leading to discrimination, especially in the case of large companies handling a high volume of applications.
6. The conversation veered towards how the use of AI tools could inadvertently create biases and harm the recruitment process, potentially affecting the diversity of candidates being considered.
7. The role of HR professionals and their knowledge in addressing discrimination was also scrutinized, with some users emphasizing the need for proper training in handling discriminatory issues.
8. Additionally, the conversation delved into the potential age discrimination that could occur in the hiring process due to AI tools that tweak applications to make candidates appear younger.
9. There was a discussion about the implications of these tools on disabled candidates and how they may face challenges due to the standardized criteria these tools use for evaluation.

Overall, the discussion highlights various concerns regarding the use of AI hiring tools and their potential impact on the recruitment process and candidate diversity.

