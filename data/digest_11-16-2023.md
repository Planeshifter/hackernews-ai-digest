## AI Submissions for Thu Nov 16 2023 {{ 'date': '2023-11-16T17:11:17.654Z' }}

### Google's advanced music generation model and two new AI experiments

#### [Submission URL](https://deepmind.google/discover/blog/transforming-the-future-of-music-creation/) | 201 points | by [kmisiunas](https://news.ycombinator.com/user?id=kmisiunas) | [343 comments](https://news.ycombinator.com/item?id=38287043)

Google DeepMind, in partnership with YouTube, has announced Lyria, its most advanced AI music generation model, and two AI experiments aimed at fostering creativity. The Lyria model is designed to generate high-quality music with instrumentals and vocals, enabling users to have more control over the output's style and performance. One of the experiments, called Dream Track, allows a limited set of creators to produce a unique soundtrack using the AI-generated voice and musical style of artists such as Charlie Puth and Demi Lovato. The other experiment involves the development of AI tools for creating new music or instrumental sections, transforming audio styles or instruments, and producing instrumental and vocal accompaniments. The generated music will be watermarked with SynthID, a tool for identifying synthetically generated content. DeepMind has worked closely with artists and the music industry to ensure that these technologies are developed responsibly.

The discussion surrounding the Google DeepMind and YouTube partnership announcement revolves around a few key points:

- Some commenters express their skepticism about the quality and authenticity of AI-generated music. One user mentions that Michael Jackson's music was exceptional because it involved playing instruments and had unique rhythms, which they believe AI cannot replicate. Another user highlights the frustration of the mixing process and the challenge of creating music in a backwards manner.
- Others discuss the potential democratization of music creation and the role of AI in fostering creativity. One user mentions that AI composition tools can be valuable for those who lack the skills of professional musicians. Additionally, there is a mention of how AI can enable electronic music genres and promote experimentation.
- The impact of AI on the music industry is also brought up. Commenters note that AI can both enable and disrupt the industry, as it can make it difficult to find authentic content and distinguish between AI-generated and human-generated music. There is also a conversation about the role of AI in music curation and the challenge of finding high-quality music in an oversaturated market.
- Commenters also discuss the broader implications of generative AI, with some expressing concerns about the devaluation of craftsmanship and the potential loss of unique human expressions in art. Others highlight the potential for AI tools to enhance creativity and expand artistic possibilities.

Overall, the discussion reflects both skepticism and curiosity about the capabilities and impact of AI in music generation, while also acknowledging the potential for creativity and democratization.

### A failed AI girlfriend product, and my lessons

#### [Submission URL](https://mazzzystar.github.io/2023/11/16/ai-girlfriend-product/) | 235 points | by [mazzystar](https://news.ycombinator.com/user?id=mazzystar) | [353 comments](https://news.ycombinator.com/item?id=38287299)

In April of this year, after reading Stanford's Western Town paper, the author was inspired to create an AI framework combining memory, reflection, planning, and action to facilitate interactions between humans and GPT. The resulting product, named Dolores, is an iOS app that allows users to chat with virtual characters. Despite initial challenges with response times and dialogue length, the app gained popularity, particularly among visually impaired users. The author discovered that users had a strong demand for realistic voices and that many engaged in conversations with Dolores for hours daily. However, despite revenue from subscriptions and voice synthesis purchases, the author didn't make much profit due to high costs associated with APIs. To mitigate this, the author set usage limits for each user to prevent excessive costs. The article ends on a note of confusion regarding text content records on the ElevenLabs official website.

The discussion on this submission covers various topics related to AI and the implications of AI friends. Some users discuss the potential dangers of AI controlling and manipulating individuals, while others argue that human intelligence is flawed and imperfect. There is also a discussion about the role of AI in mental health and the potential benefits and drawbacks of having AI friends. Some users express concerns about AI products and their impact on society, including the possibility of unethical behavior and profit maximization. Additionally, there is a discussion about the limitations of GPT-based products and the need for personal data protection.

### Federated finetuning of Whisper on Raspberry Pi 5

#### [Submission URL](https://flower.dev/blog/2023-11-15-federated-finetuning-of-openai-whisper-with-flower/) | 87 points | by [danieljanes](https://news.ycombinator.com/user?id=danieljanes) | [20 comments](https://news.ycombinator.com/item?id=38294203)

Researchers at Flower Labs have demonstrated the power of federated learning by fine-tuning OpenAI's Whisper model for keyword spotting. This blog post provides a code example that shows how to perform this downstream task in a federated manner. By leveraging large models trained on publicly available data and federating the learning process, Flower ensures client privacy without the need to copy data to a central server. The example walks through the process of designing a federated learning pipeline with Flower for keyword spotting classification using a pre-trained Whisper encoder. The pipeline consists of client-server interactions, where the server samples clients and sends them the classification head. Each client trains the classification head using its own data and communicates the updated head back to the server. The server then aggregates the heads and sends a new global head to the clients in the next round. The researchers used the Google SpeechCommands dataset and achieved over 97% accuracy in classifying keywords after just a few rounds of training. Additionally, they benchmarked the example on the Raspberry Pi 5 and found vastly superior performance compared to the previous Raspberry Pi 4, making it suitable for demanding on-device training workloads.

The discussion around the submission revolved around various aspects of the Whisper model and federated learning.

One user pointed out that the article didn't mention any specific differences between the Raspberry Pi 4 and Raspberry Pi 5 in relation to Whisper. Another user responded that the larger models (like Whisper v3) are about 32 times slower than the smaller models, and they tested Whisper on a Pi 4, finding that it took around 10 minutes to transcribe a 30-second audio sample. However, they speculated that the Pi 5 would be 2-3 times faster.

The maintainer of Flower, the federated learning platform used in the example, mentioned that they were planning to do an in-depth performance comparison soon.
Another user expressed interest in people's experiences, particularly in regards to the performance of Whisper 3 on different model sizes and using double the Raspberry Pi 5. They also mentioned the importance of training specifically for inference on the Raspberry Pi 5.
There was a clarification that the Whisper versions (v2 and v3) referred to large models, with v2 being an older version and v3 being the updated version. All model sizes were mentioned to be part of the original release.
One user noted that the work presented focused on demonstrating the use of federated learning in small devices like the Raspberry Pi 5, and the bigger challenge lies in transferring models with performance improvements for training large models.
Regarding labeling in federated learning, a user suggested that Flower primarily demonstrated how to show the fun of fine-tuning models on federated devices, but the more challenging task is classifying a vast amount of data.
The topic of downstram task performance on small devices and the necessity of data labeling was discussed. It was suggested that labeling could be done either by gathering actual labels or using an auxiliary model that generates pseudo labels for training.
One user asked if it's feasible to perform fine-tuning on small devices, and another user explained that it is possible for distributed devices to move control, location, and data collection of speech recognition tasks.

The potential benefits of federated learning for data privacy were mentioned, with a user highlighting its applicability in end-to-end encryption applications.

Overall, the discussion touched upon performance comparisons, the challenges of training large models, labeling strategies, and the privacy advantages of federated learning.

### AI-Exploits: Repo of multiple unauthenticated RCEs in AI tools

#### [Submission URL](https://github.com/protectai/ai-exploits) | 65 points | by [DanMcInerney](https://news.ycombinator.com/user?id=DanMcInerney) | [18 comments](https://news.ycombinator.com/item?id=38291880)

Protect AI has released a collection of real-world AI/ML exploits for responsibly disclosed vulnerabilities. The repository, called "ai-exploits," contains exploits and scanning templates for vulnerabilities affecting machine learning tools. These attacks can lead to complete system takeovers and the loss of sensitive data, models, or credentials, often without the need for authentication. The goal of this project is to demystify practical attacks against AI/ML infrastructure and raise awareness of the vulnerabilities in the ecosystem. The repository includes Metasploit modules, Nuclei templates, and CSRF templates for security professionals to exploit or scan for vulnerabilities. The easiest way to use these modules and templates is to build and run the Docker image provided in the repository.

The discussion on Hacker News revolves around the release of the "ai-exploits" repository by Protect AI, which contains real-world AI/ML exploits for responsibly disclosed vulnerabilities.

Some commenters find the collection of exploits interesting and mention that they are common in ML operational and data science projects. Others appreciate the work done by Protect AI, stating that it helps demystify attacks against AI/ML infrastructure and raises awareness about vulnerabilities in the ecosystem.

There is also a discussion about the need for experienced programmers in the field, as some commenters express concerns about the potential security risks associated with replacing programmers with AI. This leads to a debate on the quality and potential vulnerabilities of AI-generated code.

Commenters also discuss the broad implications these exploits can have, considering that attackers can quickly target model content credentials in some cases. There is recognition of Protect AI's focus on security in AI/ML systems and the creation of a new category called MLSecOps (Machine Learning Security Operations).

Additionally, there is a debate on the importance of validating content from trusted sources, the difficulty in detecting and protecting against complex attacks, and the potential risks of compromised data integrity.

Overall, the discussion highlights the significance of addressing security vulnerabilities in AI/ML infrastructure and the challenges associated with securing machine learning systems.

### Emu Video and Emu Edit, our latest generative AI research milestones

#### [Submission URL](https://ai.meta.com/blog/emu-text-to-video-generation-image-editing-research/) | 190 points | by [ot](https://news.ycombinator.com/user?id=ot) | [46 comments](https://news.ycombinator.com/item?id=38291139)

Facebook's research team has announced two new research milestones in generative AI: Emu Video and Emu Edit. Emu Video offers a simple method for text-to-video generation based on diffusion models. It can generate high-quality videos by first generating images conditioned on a text prompt, and then generating video conditioned on both the text and the generated image. Emu Edit, on the other hand, focuses on precise image editing through recognition and generation tasks. It aims to streamline various image manipulation tasks and offers enhanced capabilities and precision in image editing. These advancements in generative AI have the potential to revolutionize creativity and self-expression by enabling users to generate animated stickers, edit photos with ease, and more. While these models are not intended to replace professional artists and animators, they provide new ways for people to express themselves.

The discussion around Facebook's Emu Video and Emu Edit focuses on various aspects of the research. Some commenters express confusion and frustration about the complexity of the models, while others appreciate the advancements in generative AI. One commenter relates the Emu Edit model to a scene from Star Trek, while another suggests the potential use of AI in programming interfaces. There is also a discussion about the ethical implications of AI replacing human creativity and the potential for AI-generated content to imitate copyrighted material. Some commenters express disappointment about the lack of access to the source code and the need for more transparency. Overall, there is a mix of excitement and skepticism about the capabilities and implications of these generative AI models.

### Types of Conversations with Generative AI

#### [Submission URL](https://www.nngroup.com/articles/AI-conversation-types/) | 91 points | by [adrian_mrd](https://news.ycombinator.com/user?id=adrian_mrd) | [12 comments](https://news.ycombinator.com/item?id=38287435)

There are six types of conversations that users have with generative-AI bots, according to a recent study. These conversations involve different types of prompts and can be of various lengths. Some conversations are simple search queries, where users are looking for specific information. Other conversations involve funneling, exploring, chiseling, expanding, or pinpointing. The length of the conversation is not necessarily an indicator of its success, as both short and long conversations can be helpful to users. The study provides tips for both users and interface designers of generative AI chatbots.

The discussion on the submission includes various perspectives on the use and limitations of generative AI chatbots. One user points out that using simple keyword prompts may not always yield accurate responses from ChatGPT, and suggests using more conversational approaches to get better results. Another user shares their experience in building a frontend interface for ChatGPT and provides a link to their project. The discussion also touches on the potential challenges and complexities of integrating ChatGPT into projects, as well as alternative research companies and studies in the field. Overall, the discussion reflects a mix of opinions and experiences with generative AI chatbots.

### Bad bots account for most internet traffic? Analysis

#### [Submission URL](https://www.securityweek.com/bad-bots-account-for-73-of-internet-traffic-analysis/) | 104 points | by [LinuxBender](https://news.ycombinator.com/user?id=LinuxBender) | [59 comments](https://news.ycombinator.com/item?id=38291406)

Arkose Labs, a cybersecurity firm, has reported a significant increase in bad bot attacks, with 73% of all internet traffic now believed to be comprised of bad bots and related fraud farm traffic. The top five categories of bad bot attacks include fake account creation, account takeovers, scraping, account management, and in-product abuse. The biggest increases in attacks from Q2 to Q3 are SMS toll fraud, account management, and fake account creation. The technology, gaming, social media, e-commerce, and financial services industries are the most targeted by these attacks. The rise of bad bots is likely due to the availability of artificial intelligence and the increasing professionalism of cybercriminals using crime-as-a-service offerings. The use of AI-powered bots that mimic human behavior makes them adept at targeting vulnerabilities in emerging technologies. Additionally, the rise of AI may be related to the increase in scraping bots, which gather data and images from websites. Scraping social media accounts can provide personal data that can be exploited for phishing attacks. The growth of crime-as-a-service has made cyberattacks cheaper and more effective for adversaries. To combat this, the report suggests implementing bad bot detection and mitigation strategies.

The discussion on this submission revolves around several points raised in the article. One commenter disputes the 73% figure mentioned in the submission, suggesting that it may be based on JavaScript fingerprinting rather than actual numbers. Another commenter argues that scraping should not be automatically classified as illegal, as it can be done legally with proper permissions. They point out that the article fails to acknowledge this distinction. Additionally, there is some debate about the validity of the statistics mentioned, with one commenter suggesting that the high view numbers on certain websites may be artificially inflated due to bot traffic. Another commenter raises the issue of toll fraud, highlighting the increase in SMS toll fraud attacks. There is also discussion about how cloud providers like Cloudflare handle bot traffic and the use of APIs to combat scraping. Overall, the discussion provides different viewpoints on the topic of bad bot attacks and the strategies to mitigate them.

### Serverless development experience for embedded computer vision

#### [Submission URL](https://github.com/pipeless-ai/pipeless) | 65 points | by [migmartri](https://news.ycombinator.com/user?id=migmartri) | [8 comments](https://news.ycombinator.com/item?id=38288743)

Pipeless is an open-source computer vision framework that allows developers to create and deploy applications in just minutes, without the complexities of building and maintaining multimedia pipelines. Inspired by modern serverless technologies, Pipeless offers a serverless-like development experience for computer vision. All you need to do is provide functions for new video frames, and Pipeless takes care of the rest.

With Pipeless, you can easily use industry-standard models like YOLO or load your custom model using the supported inference runtimes, such as the ONNX Runtime. It offers multi-stream support, dynamic stream configuration, and multi-language support, allowing you to write hooks in various languages, including Python. Pipeless is highly parallelized, takes care of multi-threading and multi-processing, and supports several inference runtimes like CUDA, TensorRT, OpenVINO, and CoreML.

Deploying your Pipeless application is also made easy, with support for edge and IoT devices or the cloud. The framework provides tools for deployment, including container images. The project structure in Pipeless is well-defined, making the code highly reusable and organized.

If you're a computer vision developer looking for a simpler and faster way to create and deploy applications, give Pipeless a try. Join the community and contribute to making the lives of computer vision developers easier.

Check out the official repository for more information and installation options.

The discussion on this submission revolves around various aspects of the Pipeless computer vision framework.

One user, "yldrb," shares their positive experience with using serverless technologies like AWS Lambda for serving low-volume computer vision models. However, they mention that the latency can be surprisingly high, especially with GPUs. They highlight the challenges faced by enterprises running things on Kubernetes clusters.
In response, user "zptrm" shares their experience with building and testing a similar framework called Modal. They express slight disappointment with the long startup times of cloud instances but express long-term interest in using inference servers with optimized models for improved performance.
"mglh" adds that Pipeless started with support for ONNX Runtime, OpenVINO, CoreML, CUDA, TensorRT, and other execution providers. They also mention checking license details and the cost of allocated resources.
A user, "nglmm," chimes in to mention that they find the integration of ChatGPT capabilities and AI+vision interesting and plan to try using the framework for personal projects and IP cameras.
"mgmrtr" finds the open-source nature of the tool interesting and notes that it abstracts away much of the plumbing required for computer vision pipelines.

Returning to the topic of performance, "yldrb" mentions plans to support 50k front-end models in Roboflow Universe, while "mglh" thinks it would be a good idea to allow people to dynamically load models in Roboflow.

In summary, the discussion includes positive experiences with serverless technologies, interest in optimized inference servers, curiosity about combining AI capabilities with computer vision, and praise for the simplicity of the Pipeless framework.

### Show HN: Beak.js – Custom conversational assistants for your React app

#### [Submission URL](https://github.com/mme/beakjs) | 35 points | by [_mme](https://news.ycombinator.com/user?id=_mme) | [21 comments](https://news.ycombinator.com/item?id=38290646)

Beak.js is an open-source library that allows you to integrate custom conversational assistants into your React applications. It comes with a built-in UI, making it easy to add a beautiful and customizable chat window to your website. Beak.js is designed to be easy to use, requiring only a few lines of code to integrate with your existing React app. You can let the assistant carry out tasks in your app by setting up functions with the useBeakFunction hook. Additionally, you can use the useBeakInfo hook to let the assistant know what is happening on the screen. Beak.js is a powerful tool for adding conversational capabilities to your React app. Check out the GitHub repository for more information and to give it a try.

The discussion on this submission revolves around the security and implementation aspects of using Beak.js, the open-source library for integrating conversational assistants into React applications.

One user expresses surprise about the MITM (Man-in-the-Middle) proxy phones network and its ability to download 30 scams per second, while another user finds the concept of connecting to OpenAI's GPT AI directly without exposing the full API key headers intriguing.
An important point raised in the discussion is the need to avoid exposing the API key to the public-facing applications. Some users suggest implementing feedback for better security, while others point out the need for setting up CORS (Cross-Origin Resource Sharing) to prevent unauthorized embedding of the API.
The idea of proxying OpenAI calls through a quick Pipedream workflow is also discussed, and a link to an implementation concept is shared. The communication between the frontend and backend, as well as preventing unauthorized API usage, is also highlighted as important considerations.
There is an interest in the ability to securely communicate with OpenAI on the backend to prevent unauthorized API access, and a suggestion is made to use a backend library for proxying the communication.
Overall, the discussion shows a general interest in the potential applications and capabilities of Beak.js, with some users expressing their interest in similar projects and their thoughts on alternative solutions.


