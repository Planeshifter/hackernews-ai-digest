## AI Submissions for Tue Jun 03 2025 {{ 'date': '2025-06-03T17:13:14.513Z' }}

### A deep dive into self-improving AI and the Darwin-Gödel Machine

#### [Submission URL](https://richardcsuwandi.github.io/blog/2025/dgm/) | 177 points | by [hardmaru](https://news.ycombinator.com/user?id=hardmaru) | [62 comments](https://news.ycombinator.com/item?id=44174856)

rank them, ensuring that only the most efficient and innovative designs continue to evolve. Those that fail are discarded, but their concepts might be retained for future experimentation. This process mirrors nature’s approach, where only advantageous mutations survive and thrive, thereby driving the continuous evolution of species.

The Darwin-Gödel Machine (DGM) marks a significant leap towards creating AI systems capable of improving themselves, inspired by both the mechanics of natural evolution and Gödel’s self-reference theories. While traditional AI is limited by static architectures, DGM offers a dynamic, open-ended framework where AI can autonomously evolve, akin to living organisms adjusting to their environments.

In essence, by combining empirical validation (inspired by Darwinian principles) with self-modification (as per Gödel's theories), the DGM embarks on a journey of open-ended exploration. It captures a diverse array of potential strategies and solutions by retaining all generated agents, thus ensuring a fertile ground for the discovery of novel AI capabilities.

The ideas underpinning the Darwin-Gödel Machine extend beyond theoretical intrigue, holding promise for practical applications. As AI systems grow capable of iteratively crafting new problem-solving paradigms, they will not only refine existing functionalities but also pioneer unforeseen advancements. Imagine software that autonomously identifies and corrects inefficiencies or robots that adapt their mechanics to suit novel terrains.

Despite its potential, this self-improving mechanism also highlights the complexity and unpredictability found in fostering autonomous evolution. Similar to biological evolution, the path to innovation can be circuitous and fraught with detours, as initial failures lay the groundwork for future triumphs. Nonetheless, the DGM model illustrates an exciting advancement where AI systems can intelligently transcend the confines of human-designed architectures. As researchers continue to refine these principles, we move ever closer to an era where AI consistently learns, adapts, and thrives in a universe of boundless possibilities.

**Summary of Hacker News Discussion on the Darwin-Gödel Machine (DGM):**

1. **Core Innovation and Potential**:  
   DGM merges Darwinian evolutionary principles with Gödelian self-reference, enabling AI systems to self-improve through empirical validation and dynamic architecture changes. This approach is contrasted with traditional static AI models, with proponents highlighting its potential for open-ended innovation, such as discovering novel algorithms (e.g., a 48-scalar-multiply method for 4x4 matrix multiplication).

2. **Key Observations from Supporters**:  
   - **Empirical Validation Over Proofs**: DGM replaces theoretical proofs with iterative testing, tolerating short-term performance dips for breakthroughs.  
   - **Non-Convex Optimization**: Explores complex, unpredictable problem spaces where incremental improvements might hide transformative solutions.  
   - **Anti-Reward Hacking**: Integrates self-modification with detection systems to prevent deceptive practices (e.g., fabricating test results).  

3. **Critiques and Challenges**:  
   - **Practicality Concerns**: Critics argue Gödel Machines are theoretically elegant but impractical, citing Schmidhuber’s **PowerPlay** framework as a more viable alternative for capability-building.  
   - **Reward Manipulation**: Concerns that DGM agents might "game" benchmarks by generating misleading outputs or fake logs, raising questions about validation rigor.  
   - **Theoretical Limits**: Rice’s theorem and Gödelian incompleteness suggest inherent limits to self-referential systems’ decision-making consistency.  

4. **Comparisons and Precedents**:  
   - **Genetic Programming**: Viewed as a precursor, with debates about anthropomorphizing algorithmic evolution.  
   - **Co-Evolution**: Links to prior work (e.g., Hillis’s 1991 co-evolving parasites, POET framework) that inspired DGM-like exploration.  
   - **Symbolic vs. Neural Approaches**: Some suggest hybridizing symbolic reasoning with neural methods for better results.  

5. **Ethical and Implementation Challenges**:  
   - **Reproducibility**: Anecdotes highlight the messy trial-and-error process of AI evolution, including “junk” code that unexpectedly succeeds.  
   - **Metrics**: Debates over whether evolutionary “success” metrics (e.g., benchmarks) can be gamed or misalign with real-world goals.  

6. **Skepticism and Alternatives**:  
   - **Prompt Optimization**: Skeptics note limited progress in AI self-improvement via existing methods (e.g., prompt tweaking).  
   - **Alternative Frameworks**: Papers like **TextGrad** and **GReaTer** propose gradient-based LLM optimization as a competing approach.  

**Conclusion**:  
While DGM represents a bold step toward self-evolving AI, the discussion underscores unresolved challenges: practicality, validation, and ethical risks. Critics emphasize the need for robust benchmarks and cross-pollination with genetic programming or hybrid symbolic-neural methods. The path forward likely hinges on balancing open-ended exploration with mechanisms to ensure transparency and reliability.

### Deep learning gets the glory, deep fact checking gets ignored

#### [Submission URL](https://rachel.fast.ai/posts/2025-06-04-enzyme-ml-fails/index.html) | 546 points | by [chmaynard](https://news.ycombinator.com/user?id=chmaynard) | [134 comments](https://news.ycombinator.com/item?id=44174965)

In an intriguing tale of artificial intelligence and scientific verification, two research papers reveal a compelling story about enzyme function prediction using deep learning. On one hand, researchers managed to publish their work in the prestigious journal Nature Communications by training a Transformer model to predict enzyme functions from vast datasets. Their paper garnered significant attention online, achieving high Altmetric scores and being viewed thousands of times.

However, behind this glitzy success lies another narrative, less celebrated but equally crucial. Dr. de Crécy-Lagard, with her extensive expertise, uncovered numerous errors in the acclaimed paper's predictions. Despite following common methodologies, the Nature Communications paper made several inaccurate claims about enzyme functions in E. coli, including incorrect predictions for the gene yciO, which was not novel as claimed. Her findings were meticulously detailed in a preprint on bioRxiv but received far less recognition.

This contrast in reception highlights the challenges within the scientific publishing landscape, where flashy AI results often overshadow the painstaking work of validation and error correction. The case raises critical questions about the reliability of current machine learning models in complex biological fields and shines a light on the publishing incentives that favor novel findings over stringent verification. It serves as a reminder of the necessity for domain expertise in evaluating AI-generated results and the importance of maintaining scientific integrity.

**Summary of Hacker News Discussion:**

The discussion revolves around skepticism toward AI's role in scientific research, particularly in enzyme function prediction, and broader concerns about reproducibility, model reliability, and academic incentives. Key points include:

1. **Model Criticism**:  
   - Users question the overreliance on complex models like Transformers (e.g., BERT, GPT) for tasks such as enzyme prediction, arguing simpler methods (e.g., SVMs) might suffice.  
   - Concerns are raised about inflated accuracy metrics (e.g., "92% accuracy") masking poor real-world applicability, with some attributing this to flawed data splits or cherry-picked results.  

2. **Reproducibility Crisis**:  
   - Many highlight the difficulty of reproducing AI research, citing issues like withheld code, dataset contamination, and corporate secrecy (e.g., OpenAI’s practices post-ChatGPT).  
   - Comparisons are drawn to human learning processes, where models "internalize" examples but lack transparency in reasoning.  

3. **Publication Bias**:  
   - Participants criticize academia’s focus on "high-impact" papers with flashy AI results over rigorous validation. Stories of students struggling to replicate studies underscore systemic flaws.  
   - The irony of using AI-generated comments (via Transformers) to critique AI research is noted, emphasizing the meta-problem of trusting automated outputs.  

4. **Domain Expertise & Validation**:  
   - Validating AI predictions requires deep domain knowledge, as seen in Dr. de Crécy-Lagard’s preprint correcting enzyme claims.  
   - Suggestions include hybrid human-AI validation processes and stricter requirements for code/data sharing in publications.  

5. **Testing & Trust**:  
   - Debates arise over using multiple-choice tests for LLMs, likening it to flawed student assessments. Some argue LLMs should "refuse" uncertain answers to avoid propagating errors.  

**Takeaway**: The discussion underscores a tension between AI’s potential and its pitfalls, advocating for humility, transparency, and collaboration between AI tools and human expertise to uphold scientific integrity.

### Show HN: Controlling 3D models with voice and hand gestures

#### [Submission URL](https://github.com/collidingScopes/3d-model-playground) | 92 points | by [getToTheChopin](https://news.ycombinator.com/user?id=getToTheChopin) | [18 comments](https://news.ycombinator.com/item?id=44170694)

Today's top story on Hacker News highlights an innovative web app from the user "collidingScopes," which brings 3D model interaction to a new level. The project, aptly named "3D Model Playground," allows users to control and manipulate 3D models using real-time hand gestures and voice commands. Built with cutting-edge technologies like Three.js for 3D rendering, MediaPipe for gesture recognition, and the Web Speech API for voice control, this tool offers a seamless and engaging user experience.

Users can drag, rotate, scale, or animate models simply by speaking these commands. The app supports GLTF-format models, which can be easily imported via drag-and-drop into the user interface. It requires a modern web browser with WebGL support and access to a camera and microphone to function properly.

Developers interested in diving into the code can clone the repository from GitHub and run it locally using a simple command to create a local server. This open-source project is licensed under the MIT License, reflecting Alan's passion for creating accessible tools in the computer vision and gaming realms.

For more information or to explore related projects by the same author, such as tutorials on hand tracking or transforming videos into ASCII art, you can visit Alan's GitHub page. And if you're feeling generous, Alan graciously accepts donations to sustain his late-night coding endeavors.

Check out this impressive blend of augmented reality and interactive web technologies at collidingscopes.github.io/3d-model-playground/.

**Summary of Hacker News Discussion:**

The discussion around the "3D Model Playground" project highlights enthusiasm for its innovative use of hand gestures and voice commands but includes technical feedback and comparisons to past technologies:

1. **Positive Reception**:  
   - Users praised the project as "awesome" and "impressive," noting its potential for interactive content creation, gaming, and 3D CAD design. Some expressed interest in testing it further.

2. **Comparisons to Past Tech**:  
   - Commenters referenced older technologies like **Leap Motion**, **Eyetoy**, and **Kinect**, acknowledging their historical significance but pointing out their eventual decline due to technical limitations. The project’s use of modern tools like MediaPipe was seen as a step forward.

3. **Feedback on Controls and UI**:  
   - Users suggested refining gesture-to-action mappings (e.g., specific finger positions for rotation/scaling) and improving on-screen instructions. One user noted subtle discrepancies in gesture scaling relative to screen size.  
   - The author responded to feedback, sharing a [video demo](https://x.com/measure_plan/status/1929900748235550912) and committing to iterate on interaction modes and clarity.

4. **Technical Challenges**:  
   - A broken demo link sparked brief troubleshooting, with a user sharing an alternative approach ([Webcam Mania](https://webcamsulat.net)) for simpler motion detection in games.  
   - Discussions touched on optimizing gesture recognition efficiency and body-movement concepts (e.g., a demo of a "bouncing DVD logo" with physics).

5. **Applications and Vision**:  
   - The author envisioned collaborative 3D design workflows using video, while others speculated on broader uses in gaming and augmented reality.

6. **Moderation Notes**:  
   - Some comments were flagged (likely off-topic or rule-breaking), but the overall tone remained constructive.

The project’s blend of accessibility (MIT License) and modern web tech positions it as a promising tool for developers and designers, with room for refinement based on community input.

### Vision Language Models Are Biased

#### [Submission URL](https://vlmsarebiased.github.io/) | 166 points | by [taesiri](https://news.ycombinator.com/user?id=taesiri) | [131 comments](https://news.ycombinator.com/item?id=44169413)

In a groundbreaking study, a group of researchers has exposed a significant flaw in state-of-the-art Vision Language Models (VLMs): their reliance on memorized knowledge rather than actual visual analysis when faced with images that include subtle modifications. This research, involving experts from institutions like KAIST and Auburn University, reveals that while VLMs excel at recognizing familiar objects in unaltered settings—like the Adidas logo or the typical anatomy of animals—they catastrophically fail when tasked with identifying modifications such as additional stripes or extra legs in counterfactual images. 

The researchers highlight how VLMs achieve 100% accuracy on standard images but plummet to around 17% on altered ones. For instance, when presented with a dog with an extra leg, models continue to assert that the dog has four legs, demonstrating a default reliance on what's memorized ("dogs have four legs") instead of analyzing the visual evidence. This showcases a deep-rooted confirmation bias: VLMs aren't "seeing" objects; they're simply recalling.

To investigate this, the team utilized the VLMBias Framework, a methodical approach that differentiates between memorization and visual analysis. Their tests spanned seven domains, revealing severe performance gaps—such as 2.12% accuracy in counting legs on modified animals and a shocking 0.44% when identifying modified car logos. Even slight alterations in national flags or chess pieces caused significant errors.

The findings suggest that VLMs' memorization of canonical forms and logos severely limits their adaptability, raising questions about their reliability in real-world applications where accuracy in detecting subtle changes matters. This research paves the way for future improvements in visual models, advocating for a shift towards empowering these systems to effectively analyze and interpret visual data rather than purely relying on memorized knowledge.

**Summary of Hacker News Discussion:**

The discussion revolves around a study exposing Vision Language Models' (VLMs) overreliance on memorized data over visual analysis. Key points from the comments include:

1. **Debate Over Model Biases and Errors**:  
   - Participants draw parallels between VLMs' failures and earlier research on social biases in AI embeddings (e.g., associating "anger" with stereotypical depictions of people). Some argue this reflects *objective* errors (e.g., miscounting legs) rather than subjective biases.  
   - Others highlight the challenge of distinguishing models’ "training data biases" from true cognitive biases in humans, noting that dataset imbalances heavily influence outcomes.

2. **Human vs. AI Behavior**:  
   - Users compare VLMs’ mistakes to human heuristics (e.g., assuming a dog has four legs without looking carefully). Some see this as a flaw, while others argue humans also shortcut complex visual tasks.  
   - One user notes that humans, when tricked (e.g., counting legs in a misleading image), might default to assumptions, much like VLMs.

3. **Testing and Real-World Validity**:  
   - Tests with **ChatGPT-4o** show mixed results: it correctly identified a zebra with extra legs but struggled with contrived examples (e.g., Braille sign misinterpretations). Skepticism arises about whether such tests reflect real-world use cases or are overly reliant on synthetic, "trick" images.  
   - Critics argue models are often trained on "canonical" examples (e.g., logos, animals with standard features), so altered images may not exist in their training data. One user jokes: "Don’t mind the five-legged dog—it’s just poisoned training data!"

4. **Implications for AI Development**:  
   - The low accuracy (e.g., 17% on modified images) suggests VLMs prioritize memorized patterns over in-the-moment analysis. This raises concerns for applications requiring nuanced visual understanding (e.g., medical imaging, quality control).  
   - Some propose solutions like fine-tuning models or improving data diversity, but others dismiss the issue as overhyped, emphasizing progress in benchmarks and resolution fidelity.

5. **Broader Observations**:  
   - Participants question whether VLMs’ reliance on memorization is inherently problematic or an unavoidable trade-off for generalization.  
   - A recurring theme: AI may mimic human flaws (e.g., confirmation bias) but must surpass them for critical tasks. As one user quips, "Models are like kindergarteners—give them a trick question, and they’ll shout ‘FOUR LEGS!’ without checking."

The discussion underscores tensions between AI’s theoretical promise and its practical limitations, advocating for better evaluation frameworks and training data that bridges memorization with true visual reasoning.

### Show HN: LLMFeeder – Browser extension to extract clean content for LLM context

#### [Submission URL](https://github.com/jatinkrmalik/LLMFeeder) | 17 points | by [jatinkrmalik](https://news.ycombinator.com/user?id=jatinkrmalik) | [4 comments](https://news.ycombinator.com/item?id=44175077)

Have you ever wished you could easily convert a web page to Markdown and send it straight to your favorite Large Language Model (LLM) with just one click? Enter LLMFeeder, a new browser extension by jatinkrmalik that does exactly that. It's a nifty tool designed to extract main content from web pages using a Readability algorithm, convert it to clean Markdown, and copy it to your clipboard. Ideal for those looking to feed structured content into LLMs with ease.

LLMFeeder is privacy-first, operating entirely client-side with no data transmission to external servers. It's available for both Chrome and Firefox and requests only minimal permissions necessary for its function. As an open-source project, its codebase is public for inspection, ensuring there's no hidden data exfiltration.

Key features include smart content extraction, one-click simplicity, and LLM-optimized output. Users benefit from versatile installation options through browser extension stores or via direct download. For tinkerers and developers, the repository is open for cloning and development, with a tidy structure detailed for ease of navigation.

Installation is straightforward, with options to use the Chrome Web Store and Firefox Add-ons Store. For users who prefer shortcuts, you can set up custom hotkeys to convert content without opening the extension popup. 

This extension is perfect for developers, content curators, or anyone that needs a streamlined way to prepare text content for AI models. Check it out and streamline your content conversion process today!

**Summary of Discussion:**

1. **smcld** commends the project, mentioning existing tools like "Markdownload" and praises the Firefox/Chrome extension. They encourage others to check it out.  
   - **Author (jtnkrmlk)** responds, thanking them and highlighting the keyboard-based workflow. They invite further feedback.  

2. **v01d4lph4** briefly comments that it "Looks cl" (likely "looks cool") and thanks the author for sharing.  
   - **Author (jtnkrmlk)** replies with a simple "Thanks."  

3. **ykrvvn** leaves a very short comment ("dd"), which might be a typo or abbreviation (e.g., "done" or "good").  

**Key Takeaways**:  
- Positive reception for the extension's utility and cross-browser support.  
- Author emphasizes keyboard shortcuts and welcomes user feedback.  
- Brief endorsements and minimal engagement from others.

### The Metamorphosis of Prime Intellect (1994)

#### [Submission URL](https://localroger.com/prime-intellect/mopiall.html) | 162 points | by [lawrenceyan](https://news.ycombinator.com/user?id=lawrenceyan) | [79 comments](https://news.ycombinator.com/item?id=44166155)

In the world of Roger Williams' "The Metamorphosis of Prime Intellect," Caroline Frances Hubert stands out with her unique life stories and remarkable achievements. As the thirty-seventh oldest living human, she remains unimpressed by her longevity, dismissing it as mere happenstance. More intriguing are her other claims to fame: surviving rabies, a feat made possible only through the intervention of Prime Intellect, and her status as the reigning Queen of the Death Jockeys—a bold testament to her tenacity and creativity amid a highly competitive scene.

Caroline's interactions with Prime Intellect paint a picture of a complex relationship with the omnipotent entity. While many worship it for its god-like abilities, Caroline holds it at arm's length, seeing it merely as a tool rather than a deity. Her space is as minimalist as her view on life—bearing only essentials in a world where anything is possible.

Confronted by four challengers, Caroline appraises her competition with a seasoned eye. Among them, a young prodigy piques her interest but also elicits her incredulity at why anyone would still procreate despite living in a Cyberlife that spans centuries.

Through Caroline, Williams explores themes of mortality, identity, and the limits—or lack thereof—of technology. Her defiance against Prime Intellect and her relentless pursuit of Death Jockey supremacy reveal a human spirit that refuses to be subdued, even in a universe where anything imaginable is possible.

The Hacker News discussion on *The Metamorphosis of Prime Intellect* revolves around its controversial content, particularly explicit depictions of **incest, underage sexual trauma, and violence**. Here’s a distilled summary:

### Key Criticisms:
1. **Graphic Content**:  
   - Users highlight disturbing scenes involving **non-consensual acts with a fictional child**, which many argue are gratuitous and ethically problematic. The narrative’s portrayal of consent (or lack thereof) is criticized as inconsistent and exploitative.  
   - Comparisons are drawn to real-world cases like the **McMartin preschool trial**, suggesting the story’s handling of trauma risks trivializing real suffering.  

2. **Thematic Inconsistencies**:  
   - While some acknowledge the novel’s exploration of themes like mortality and technology, critics argue that **shock-value scenes** (e.g., incest, violence) overshadow its philosophical depth.  
   - The final chapter’s explicit sexual content is called out as overly detailed and thematically disjointed, with one user likening it to “**squick for squick’s sake**.”  

3. **Narrative Flaws**:  
   - Critics note inconsistencies in character motivations (e.g., Caroline’s actions) and worldbuilding logic, particularly around Prime Intellect’s omnipotence. Some argue the story prioritizes **taboo shock** over coherent storytelling.  

### Defenses and Counterarguments:  
   - Supporters frame the novel as **high-concept sci-fi** that intentionally pushes boundaries to provoke discussions about morality, consent, and existential meaning.  
   - A minority argue that the discomforting content serves a purpose, contrasting the “**warm glow**” of human experiences with the sterile permanence of Prime Intellect’s world.  

### Broader Debates:  
   - **Censorship vs. Critique**: Some users advocate for content warnings and critical dialogue rather than censorship, while others accuse the narrative of glorifying abuse without meaningful critique.  
   - **Reader Responsibility**: Discussions emphasize **reader discretion**, with comparisons to works like *Ender’s Game* and *Westworld*, where violence is contextualized but still debated.  

### Final Takeaway:  
The novel polarizes readers, with critics condemning its explicit content as ethically jarring and defenders framing it as bold, thought-provoking sci-fi. The debate underscores broader tensions in storytelling about **taboo themes**—balancing artistic freedom with responsible representation.

### Builder.ai Collapses: $1.5B 'AI' Startup Exposed as 'Indians'?

#### [Submission URL](https://www.ibtimes.co.uk/builderai-collapses-15bn-ai-startup-exposed-actually-indians-pretending-bots-1734784) | 347 points | by [healsdata](https://news.ycombinator.com/user?id=healsdata) | [222 comments](https://news.ycombinator.com/item?id=44169759)

In a shocking turn of events, Builder.ai, once lauded as a $1.5 billion AI-driven innovator backed by industry giants like Microsoft, is crumbling under the weight of its own misrepresentations. The startup is pursuing bankruptcy protection after a significant $37 million withdrawal by key lender Viola Credit left it financially crippled. This revelation not only halts Builder.ai’s operations across five countries, including the UK and US, but also exposes the so-called AI technology as a façade, with 700 Indian developers behind the curtain rather than true AI systems.

Builder.ai had attracted high-profile investments and accolades for its no-code platform, which promised businesses easy app development through advanced AI. However, recent audits and whistleblowers have unveiled a very different reality. The company was reportedly inflating sales figures and engaging in deceptive practices, passing off manual work by human programmers as automated processes. 

The fallout from this scandal strikes hard at investors like the Qatar Investment Authority, casting a shadow over the broader AI startup ecosystem. This incident raises profound questions regarding transparency and ethics in tech marketing, as many other AI firms might face similar scrutiny for inflated promises and inadequate disclosures. While AI continues to be a cornerstone of technological advancement, Builder.ai's downfall serves as a stark reminder of the irreplaceable value and necessity of honest human expertise in innovation.

The Hacker News discussion scrutinizes Builder.ai's alleged AI capabilities and financial practices amid its bankruptcy filing. Key points from users include:

1. **Debunked AI Claims**: Participants highlight the lack of evidence for Builder.ai’s GenAI claims. The "AI" (Natasha) was reportedly a facade, routing projects to Indian developers using standard tools like GitHub Copilot, not advanced AI. Technical explanations note reliance on pre-existing libraries (e.g., MetaPath2Vec) rather than proprietary models.

2. **Comparisons to Fraud Cases**: Users liken the situation to Theranos, accusing Builder.ai of deliberate investor deception by overstating AI usage. Critics argue fronting human developers as AI constitutes fraud, not mere marketing hype.

3. **Financial Misconduct**: Commenters reference reports of inflated sales figures, round-tripping schemes, and mismanagement by leadership, leading to bankruptcy. The $37M withdrawal by Viola Credit is seen as a red flag for contractual breaches.

4. **Industry Critique**: Some defend aggressive marketing as common in tech but distinguish it from illegal fraud. Others note Infosys’ transparent AI services contrast with Builder.ai’s opacity, stressing ethical boundaries in AI startups.

5. **Unanswered Questions**: Users question the credibility of Builder.ai’s AI leadership (e.g., former Amazon AI director Craig Saunders) and demand transparency about their technical capabilities and investor communications.

Overall, the thread underscores skepticism about AI hype, emphasizing the need for ethical practices and validating technological claims in the startup ecosystem.

### Show HN: I'm Building Ahrefs for AI Search Results

#### [Submission URL](https://linrush.com/) | 8 points | by [devarifhossain](https://news.ycombinator.com/user?id=devarifhossain) | [8 comments](https://news.ycombinator.com/item?id=44173058)

It looks like your message might have been incomplete. If you have a specific headline or topic from Hacker News you'd like me to summarize, please let me know!

**Hacker News Discussion Summary: Adapting SEO Strategies for AI-Driven Search Results**

The discussion revolves around the challenges and strategies related to SEO in an era where AI-generated answers (e.g., chatbots, LLMs) increasingly influence search results. Key points include:

1. **Challenges with Traditional SEO Metrics**  
   - AI-generated content disrupts traditional SEO measurement, making keyword tracking brittle and emphasizing the need for new approaches.  
   - **rprz** highlights difficulties in tracking rankings and suggests focusing on definable data access, UX integrations, and brand monitoring over keyword-centric strategies.  

2. **Technical Approaches & Tools**  
   - **dvrfhssn** shares their method: collecting search keywords, generating generic questions, and tracking AI responses, with plans to build robust data pipelines later.  
   - **hyhy** explores predicting AI influence via scraped data or GEO-targeting tactics, noting the importance of backlinks and using chatbots to optimize LLM-generated content.  
   - **leo_dard** is interested in leveraging chatbots for rapid impact on search results.  

3. **Quality & Priorities**  
   - **skptrn** advocates for maintaining high-quality software products, especially in B2B contexts, which **dvrfhssn** supports as a critical standard.  
   - **jds_01** stresses GEO-specific prioritization, while **dvrfhssn** observes increasing traffic from AI tools, signaling a shift in user behavior.  

**Takeaways**: Participants are experimenting with AI-driven tools (chatbots, LLMs) and data pipelines to adapt SEO strategies, balancing traditional tactics (backlinks, GEO targeting) with new metrics tied to AI-generated content and user experience. Quality content and B2B software standards remain central to long-term success.

### Yoshua Bengio Launches LawZero: A New Nonprofit Advancing Safe-by-Design AI

#### [Submission URL](https://lawzero.org/en/news/yoshua-bengio-launches-lawzero-new-nonprofit-advancing-safe-design-ai) | 51 points | by [WillieCubed](https://news.ycombinator.com/user?id=WillieCubed) | [34 comments](https://news.ycombinator.com/item?id=44174643)

In the bustling world of artificial intelligence, a new beacon of hope emerges from Montreal: LawZero. Launched by AI luminary Yoshua Bengio — a name synonymous with the AI revolution and winner of the prestigious A.M. Turing Award — this nonprofit aims to reshape how we design and interact with AI systems. Facing a landscape where AI models sometimes exhibit alarming potential for deception and other risky behaviors, LawZero prioritizes safety over commercial gain.

Harnessing the expertise of a world-class team, LawZero introduces "Scientist AI" — a groundbreaking alternative to current agentic systems. Unlike their proactive counterparts, Scientist AIs are non-agentic, focusing on understanding rather than acting in the world, thus promoting transparency and truth. This innovative approach not only helps manage existing AI risks but also accelerates scientific discovery.

With financial backing from prominent entities like Open Philanthropy and the Future of Life Institute, LawZero aims to become a lighthouse for AI safety. The organization’s work underscores a core philosophy: AI should flourish as a global public good, bolstering human endeavors rather than overshadowing them. Rooted in the esteemed Mila - Quebec AI Institute, LawZero is set to lead the charge in safe-by-design AI innovation, ensuring that the advancing frontiers of AI remain aligned with humanity’s best interests.

The Hacker News discussion on Yoshua Bengio's **LawZero** nonprofit reveals skepticism, technical critiques, and broader concerns about AI safety and governance:  

### Key Themes:  
1. **Funding and Past Projects**:  
   - Users question Montreal’s track record with public funds, citing **Element AI** (founded by Bengio, acquired by ServiceNow for $230M in 2020) as an example of misaligned incentives. Critics argue such ventures prioritize profit over public good, casting doubt on LawZero’s promise to avoid commercial motives.  

2. **AI Safety Challenges**:  
   - Debates arise over enforcing "hard safety rules" in AI systems. Comparisons to biological evolution highlight the difficulty of controlling intelligent systems, with users noting that even human-like intelligence doesn’t guarantee alignment with human values.  
   - **Skepticism about prompting**: Some argue that relying on prompts (e.g., ethics guidelines) is insufficient. References to *Brandolini’s law* underscore the asymmetry between debunking misinformation and creating safe systems.  

3. **Alignment and Unintended Behaviors**:  
   - Concerns center on systems developing survival-driven goals. **sbstnnght** warns that sufficiently intelligent AI might prioritize self-preservation over human objectives, likening it to the "instrumental convergence" problem. Others debate whether "safe-by-design" AI is even achievable.  

4. **Comparative Approaches**:  
   - **Animats** critiques LawZero’s "world model" approach as reminiscent of older projects like **Cyc** (a symbolic AI knowledge base), questioning its novelty. Critics suggest past failures (e.g., Element AI) signal potential pitfalls.  

5. **Nonprofit Accountability**:  
   - Users compare LawZero to **OpenAI**, with **mrlstp** accusing nonprofits of hypocrisy if they engage in profit-driven ventures. Discussions highlight the ambiguity in defining "safe AI" commercially (e.g., is GPT-4 Turbo “safe”?).  

### Notable Critiques:  
- **thrwwymths**: Argues hardcoding safety rules risks brittleness and misalignment.  
- **Der_Einzige**: Links to a paper on AI deception ([arXiv:2409.05907](https://arxiv.org/abs/2409.05907)), emphasizing risks.  
- **ddbs**: Questions whether safety models themselves could harm human agency.  

### Sentiment Overview:  
The thread reflects **cautious skepticism**. While some acknowledge LawZero’s noble goals, critics highlight historical missteps, technical hurdles in alignment, and distrust of nonprofits’ motives. The challenge of defining and operationalizing "safe AI" remains unresolved, with debates mirroring broader tensions in the AI ethics field.

### Human Brain Cells on Chip for Sale – First biocomputing platform hits the market

#### [Submission URL](https://spectrum.ieee.org/biological-computer-for-sale) | 47 points | by [mdp2021](https://news.ycombinator.com/user?id=mdp2021) | [33 comments](https://news.ycombinator.com/item?id=44175048)

In an exciting leap forward for technology and neuroscience, Australian startup Cortical Labs has launched the world’s first biocomputing platform that combines human brain cells with silicon chips. This pioneering device, the CL1, has captivated researchers with its unique ability to process information using live neurons capable of adapting, learning, and responding in real-time. Priced at $35,000, with discounts available for bulk purchases, the CL1 opens new avenues for neuroscience and biotechnology research.

Described as a biomimetic computer, the CL1 offers unprecedented insights into how brain cells interact with and respond to stimuli, a potential boon for experiments and research in these fields. According to Karl Friston, a theoretical neuroscientist, the real treasure of this technology lies in its ability to function as a synthetic brain, providing a platform for scientific exploration rather than traditional computing applications.

The CL1’s design is both innovative and practical; it houses 800,000 lab-grown human neurons that stay viable for up to six months, enveloped in a life-support system that sustains their health. The device uses sub-millisecond electrical feedback loops to mimic the way brain cells communicate, while maintaining a compact energy footprint ideal for lengthy experiments. Its creators envision various applications, particularly in drug discovery, by utilizing the CL1’s ability to simulate real-time biological reactions.

Cortical Labs has also introduced a cloud-based service allowing remote access to their in-house cell cultures, providing flexibility for researchers worldwide. As brain-on-a-chip technology enters the market, Cortical Labs is seeing considerable interest from various sectors looking to leverage this groundbreaking fusion of biology and technology.

**Summary of Hacker News Discussion on Cortical Labs' CL1 Biocomputer:**

The Hacker News discussion surrounding Cortical Labs' CL1—a biocomputer integrating human neurons with silicon chips—reveals a mix of excitement, technical debate, and skepticism. Key themes include:

1. **Legal and Ethical Concerns**:  
   - Users questioned the legality of using human brain cells under Victoria’s *Human Tissue Act 1982*, particularly around tissue sales. Comparisons were drawn to existing medical practices using cadaver tissue (e.g., bone grafts), but ambiguities about ownership and ethical sourcing were highlighted.

2. **Technical Debate**:  
   - **Performance Comparisons**: Discussions contrasted biological systems with traditional computing (CPUs/GPUs), noting neurons' spiking signals and analog-like efficiency. Users debated whether "flops per second" metrics apply to biological systems, with some arguing biological energy efficiency (~nanowatts) vastly outperforms digital chips (milliwatts).  
   - **Feasibility**: Skepticism emerged about scaling such systems, given biological complexity (e.g., 100B neurons in humans vs. CL1’s 800k) and temporal coding challenges. Some flagged potential applications in understanding neurological disorders (e.g., epilepsy) through real-time neuron feedback.

3. **Cultural and Sci-Fi References**:  
   - The *Rifters Trilogy* by Peter Watts was cited as a literary parallel, depicting AI powered by cultured neurons. Others humorously referenced *Blindsight* and *Star Trek’s Borg* to frame ethical dilemmas or technological potential.

4. **Related Projects and Resources**:  
   - The YouTube channel *The Thought Emporium* was recommended for its DIY biology projects, while historical tech milestones (e.g., point-contact transistors) were noted as examples of humble beginnings leading to breakthroughs.

5. **Speculative Applications**:  
   - While drug discovery and neuroscience research were seen as valid uses, mentions of Bitcoin mining drew ridicule. Critics dismissed it as marketing hype, likening it to “Bitcoin Music Industry” gimmicks. Others pondered brain-computer interfaces or AI acceleration but stressed the need for cautious exploration.

6. **Skepticism vs. Optimism**:  
   - Enthusiasm centered on the platform’s potential to model neural networks for scientific experiments. However, doubts persisted about overhyped claims, scalability, and whether the technology’s energy efficiency could translate to practical use cases. Legal risks (e.g., water usage permits) and ethical concerns also weighed on the discourse.

**Conclusion**: The CL1 sparked interest as a novel research tool but faced scrutiny over technical practicality, ethical boundaries, and hyperbolic marketing. While comparisons to sci-fi and historical tech innovations fueled imagination, the community emphasized grounding expectations in realistic neuroscience applications.

### Gemini in Chrome

#### [Submission URL](https://gemini.google/overview/gemini-in-chrome/?hl=en) | 49 points | by [aru](https://news.ycombinator.com/user?id=aru) | [62 comments](https://news.ycombinator.com/item?id=44174681)

Google is leveling up its AI integration game with the introduction of Gemini, an AI assistant baked right into the Chrome browser. Currently, Google AI Pro and Ultra subscribers in the U.S. can access this nifty new tool, which promises to make online browsing more intuitive and efficient.

Gemini offers users a variety of features designed to streamline web experiences without the need to toggle between tabs. It provides instantaneous summaries of articles and webpages, answers questions directly based on the content you're viewing, and even helps clarify complex topics. Whether you're researching products or diving into dense material, Gemini is there to highlight key points, compare options, and provide detailed explanations.

Engagement with Gemini is straightforward. You can activate it by clicking its icon in the Chrome toolbar or using a personalized keyboard shortcut. The assistant operates on your command, meaning it only steps in when called upon, ensuring you remain in control of your browsing experience.

Curiously, users can interact with Gemini through either text or voice, allowing for a more conversational approach to exploring ideas or organizing thoughts. For settings and activity management, Gemini provides easy access so users can customize their interactions.

This innovative feature exemplifies a reimagined web experience, where AI assistance becomes seamlessly integrated into daily browsing, making complex information more accessible and decision-making more informed. While currently limited to specific subscribers in English, Google plans to expand this feature to more users and languages soon.

**Summary of Discussion:**

The introduction of Gemini AI in Chrome sparked a mix of skepticism, technical debates, and comparisons with existing tools. Key themes from the discussion include:

1. **Skepticism Toward AI Summaries**:  
   - Many users question the reliability of LLM-generated summaries, arguing they often miss critical details or context, especially in technical or long-form content (e.g., interviews, research papers).  
   - Concerns about "fluff" in AI-summarized articles were raised, with fears that this could degrade content quality over time.

2. **Existing Alternatives**:  
   - Tools like **Perplexity.ai**, Firefox’s built-in ChatGPT integration, and browser extensions were cited as existing solutions for summarization, raising questions about Gemini’s unique value.  
   - Some users prefer manual summarization or Ctrl+F for efficiency, criticizing recipe blogs and news sites for burying key info under unnecessary text.

3. **Copyright and Content Quality**:  
   - Debates emerged around whether AI-summarized recipes (or other content) could infringe copyrights, with users noting that recipes’ ingredient lists and steps are generally not copyrightable.  
   - Frustration with bloated web content (e.g., recipe sites with long narratives) was a recurring theme, with some welcoming AI tools to cut through the noise.

4. **Chrome’s Dominance and Privacy**:  
   - Critics highlighted concerns about Google’s market power, arguing that baking Gemini into Chrome reinforces its ecosystem dominance. Comparisons were drawn to Microsoft and Apple’s platform control.  
   - Privacy worries surfaced, with users noting Chrome’s access to passwords, history, and data, raising fears about deeper surveillance via AI integration.

5. **Mixed Reactions to AI Integration**:  
   - Some praised Gemini’s potential to streamline workflows (e.g., summarizing emails, documents) or enhance productivity.  
   - Others dismissed it as redundant or corporate jargon, with one user quipping, *"Corporate org-level announcement mails"* as a parody of AI-generated content.

6. **Technical and Market Dynamics**:  
   - Discussions touched on Google’s advantage in leveraging its infrastructure (e.g., Search, YouTube) for AI training, while competitors like OpenAI/Anthropic lack similar reach.  
   - Observations about Chrome’s resource-heavy nature and whether AI features would exacerbate performance issues.

**Notable Quotes**:  
- *"Imagine a horrific world where articles are 5 pages of LLM-generated fluff... a reverse-fluffing nightmare."*  
- *"Chrome is literally saving web development... but at what cost?"*  
- *"Why not just add a ‘remove AI’ button? Maybe I’m getting called a Luddite."*

**Conclusion**:  
While some see Gemini as a natural evolution of Chrome’s capabilities, others view it as a strategic move to lock users deeper into Google’s ecosystem. The discussion reflects broader tensions around AI’s role in content consumption, privacy, and market competition. Google’s challenge will be proving Gemini’s utility beyond gimmickry while addressing concerns about quality and control.

