## AI Submissions for Tue Jun 03 2025 {{ 'date': '2025-06-03T17:13:14.513Z' }}

### Deep learning gets the glory, deep fact checking gets ignored

#### [Submission URL](https://rachel.fast.ai/posts/2025-06-04-enzyme-ml-fails/index.html) | 546 points | by [chmaynard](https://news.ycombinator.com/user?id=chmaynard) | [134 comments](https://news.ycombinator.com/item?id=44174965)

In an intriguing tale of artificial intelligence and scientific verification, two research papers reveal a compelling story about enzyme function prediction using deep learning. On one hand, researchers managed to publish their work in the prestigious journal Nature Communications by training a Transformer model to predict enzyme functions from vast datasets. Their paper garnered significant attention online, achieving high Altmetric scores and being viewed thousands of times.

However, behind this glitzy success lies another narrative, less celebrated but equally crucial. Dr. de Crécy-Lagard, with her extensive expertise, uncovered numerous errors in the acclaimed paper's predictions. Despite following common methodologies, the Nature Communications paper made several inaccurate claims about enzyme functions in E. coli, including incorrect predictions for the gene yciO, which was not novel as claimed. Her findings were meticulously detailed in a preprint on bioRxiv but received far less recognition.

This contrast in reception highlights the challenges within the scientific publishing landscape, where flashy AI results often overshadow the painstaking work of validation and error correction. The case raises critical questions about the reliability of current machine learning models in complex biological fields and shines a light on the publishing incentives that favor novel findings over stringent verification. It serves as a reminder of the necessity for domain expertise in evaluating AI-generated results and the importance of maintaining scientific integrity.

**Summary of Hacker News Discussion:**

The discussion revolves around skepticism toward AI's role in scientific research, particularly in enzyme function prediction, and broader concerns about reproducibility, model reliability, and academic incentives. Key points include:

1. **Model Criticism**:  
   - Users question the overreliance on complex models like Transformers (e.g., BERT, GPT) for tasks such as enzyme prediction, arguing simpler methods (e.g., SVMs) might suffice.  
   - Concerns are raised about inflated accuracy metrics (e.g., "92% accuracy") masking poor real-world applicability, with some attributing this to flawed data splits or cherry-picked results.  

2. **Reproducibility Crisis**:  
   - Many highlight the difficulty of reproducing AI research, citing issues like withheld code, dataset contamination, and corporate secrecy (e.g., OpenAI’s practices post-ChatGPT).  
   - Comparisons are drawn to human learning processes, where models "internalize" examples but lack transparency in reasoning.  

3. **Publication Bias**:  
   - Participants criticize academia’s focus on "high-impact" papers with flashy AI results over rigorous validation. Stories of students struggling to replicate studies underscore systemic flaws.  
   - The irony of using AI-generated comments (via Transformers) to critique AI research is noted, emphasizing the meta-problem of trusting automated outputs.  

4. **Domain Expertise & Validation**:  
   - Validating AI predictions requires deep domain knowledge, as seen in Dr. de Crécy-Lagard’s preprint correcting enzyme claims.  
   - Suggestions include hybrid human-AI validation processes and stricter requirements for code/data sharing in publications.  

5. **Testing & Trust**:  
   - Debates arise over using multiple-choice tests for LLMs, likening it to flawed student assessments. Some argue LLMs should "refuse" uncertain answers to avoid propagating errors.  

**Takeaway**: The discussion underscores a tension between AI’s potential and its pitfalls, advocating for humility, transparency, and collaboration between AI tools and human expertise to uphold scientific integrity.

### Vision Language Models Are Biased

#### [Submission URL](https://vlmsarebiased.github.io/) | 166 points | by [taesiri](https://news.ycombinator.com/user?id=taesiri) | [131 comments](https://news.ycombinator.com/item?id=44169413)

In a groundbreaking study, a group of researchers has exposed a significant flaw in state-of-the-art Vision Language Models (VLMs): their reliance on memorized knowledge rather than actual visual analysis when faced with images that include subtle modifications. This research, involving experts from institutions like KAIST and Auburn University, reveals that while VLMs excel at recognizing familiar objects in unaltered settings—like the Adidas logo or the typical anatomy of animals—they catastrophically fail when tasked with identifying modifications such as additional stripes or extra legs in counterfactual images. 

The researchers highlight how VLMs achieve 100% accuracy on standard images but plummet to around 17% on altered ones. For instance, when presented with a dog with an extra leg, models continue to assert that the dog has four legs, demonstrating a default reliance on what's memorized ("dogs have four legs") instead of analyzing the visual evidence. This showcases a deep-rooted confirmation bias: VLMs aren't "seeing" objects; they're simply recalling.

To investigate this, the team utilized the VLMBias Framework, a methodical approach that differentiates between memorization and visual analysis. Their tests spanned seven domains, revealing severe performance gaps—such as 2.12% accuracy in counting legs on modified animals and a shocking 0.44% when identifying modified car logos. Even slight alterations in national flags or chess pieces caused significant errors.

The findings suggest that VLMs' memorization of canonical forms and logos severely limits their adaptability, raising questions about their reliability in real-world applications where accuracy in detecting subtle changes matters. This research paves the way for future improvements in visual models, advocating for a shift towards empowering these systems to effectively analyze and interpret visual data rather than purely relying on memorized knowledge.

**Summary of Hacker News Discussion:**

The discussion revolves around a study exposing Vision Language Models' (VLMs) overreliance on memorized data over visual analysis. Key points from the comments include:

1. **Debate Over Model Biases and Errors**:  
   - Participants draw parallels between VLMs' failures and earlier research on social biases in AI embeddings (e.g., associating "anger" with stereotypical depictions of people). Some argue this reflects *objective* errors (e.g., miscounting legs) rather than subjective biases.  
   - Others highlight the challenge of distinguishing models’ "training data biases" from true cognitive biases in humans, noting that dataset imbalances heavily influence outcomes.

2. **Human vs. AI Behavior**:  
   - Users compare VLMs’ mistakes to human heuristics (e.g., assuming a dog has four legs without looking carefully). Some see this as a flaw, while others argue humans also shortcut complex visual tasks.  
   - One user notes that humans, when tricked (e.g., counting legs in a misleading image), might default to assumptions, much like VLMs.

3. **Testing and Real-World Validity**:  
   - Tests with **ChatGPT-4o** show mixed results: it correctly identified a zebra with extra legs but struggled with contrived examples (e.g., Braille sign misinterpretations). Skepticism arises about whether such tests reflect real-world use cases or are overly reliant on synthetic, "trick" images.  
   - Critics argue models are often trained on "canonical" examples (e.g., logos, animals with standard features), so altered images may not exist in their training data. One user jokes: "Don’t mind the five-legged dog—it’s just poisoned training data!"

4. **Implications for AI Development**:  
   - The low accuracy (e.g., 17% on modified images) suggests VLMs prioritize memorized patterns over in-the-moment analysis. This raises concerns for applications requiring nuanced visual understanding (e.g., medical imaging, quality control).  
   - Some propose solutions like fine-tuning models or improving data diversity, but others dismiss the issue as overhyped, emphasizing progress in benchmarks and resolution fidelity.

5. **Broader Observations**:  
   - Participants question whether VLMs’ reliance on memorization is inherently problematic or an unavoidable trade-off for generalization.  
   - A recurring theme: AI may mimic human flaws (e.g., confirmation bias) but must surpass them for critical tasks. As one user quips, "Models are like kindergarteners—give them a trick question, and they’ll shout ‘FOUR LEGS!’ without checking."

The discussion underscores tensions between AI’s theoretical promise and its practical limitations, advocating for better evaluation frameworks and training data that bridges memorization with true visual reasoning.

### The Metamorphosis of Prime Intellect (1994)

#### [Submission URL](https://localroger.com/prime-intellect/mopiall.html) | 162 points | by [lawrenceyan](https://news.ycombinator.com/user?id=lawrenceyan) | [79 comments](https://news.ycombinator.com/item?id=44166155)

In the world of Roger Williams' "The Metamorphosis of Prime Intellect," Caroline Frances Hubert stands out with her unique life stories and remarkable achievements. As the thirty-seventh oldest living human, she remains unimpressed by her longevity, dismissing it as mere happenstance. More intriguing are her other claims to fame: surviving rabies, a feat made possible only through the intervention of Prime Intellect, and her status as the reigning Queen of the Death Jockeys—a bold testament to her tenacity and creativity amid a highly competitive scene.

Caroline's interactions with Prime Intellect paint a picture of a complex relationship with the omnipotent entity. While many worship it for its god-like abilities, Caroline holds it at arm's length, seeing it merely as a tool rather than a deity. Her space is as minimalist as her view on life—bearing only essentials in a world where anything is possible.

Confronted by four challengers, Caroline appraises her competition with a seasoned eye. Among them, a young prodigy piques her interest but also elicits her incredulity at why anyone would still procreate despite living in a Cyberlife that spans centuries.

Through Caroline, Williams explores themes of mortality, identity, and the limits—or lack thereof—of technology. Her defiance against Prime Intellect and her relentless pursuit of Death Jockey supremacy reveal a human spirit that refuses to be subdued, even in a universe where anything imaginable is possible.

The Hacker News discussion on *The Metamorphosis of Prime Intellect* revolves around its controversial content, particularly explicit depictions of **incest, underage sexual trauma, and violence**. Here’s a distilled summary:

### Key Criticisms:
1. **Graphic Content**:  
   - Users highlight disturbing scenes involving **non-consensual acts with a fictional child**, which many argue are gratuitous and ethically problematic. The narrative’s portrayal of consent (or lack thereof) is criticized as inconsistent and exploitative.  
   - Comparisons are drawn to real-world cases like the **McMartin preschool trial**, suggesting the story’s handling of trauma risks trivializing real suffering.  

2. **Thematic Inconsistencies**:  
   - While some acknowledge the novel’s exploration of themes like mortality and technology, critics argue that **shock-value scenes** (e.g., incest, violence) overshadow its philosophical depth.  
   - The final chapter’s explicit sexual content is called out as overly detailed and thematically disjointed, with one user likening it to “**squick for squick’s sake**.”  

3. **Narrative Flaws**:  
   - Critics note inconsistencies in character motivations (e.g., Caroline’s actions) and worldbuilding logic, particularly around Prime Intellect’s omnipotence. Some argue the story prioritizes **taboo shock** over coherent storytelling.  

### Defenses and Counterarguments:  
   - Supporters frame the novel as **high-concept sci-fi** that intentionally pushes boundaries to provoke discussions about morality, consent, and existential meaning.  
   - A minority argue that the discomforting content serves a purpose, contrasting the “**warm glow**” of human experiences with the sterile permanence of Prime Intellect’s world.  

### Broader Debates:  
   - **Censorship vs. Critique**: Some users advocate for content warnings and critical dialogue rather than censorship, while others accuse the narrative of glorifying abuse without meaningful critique.  
   - **Reader Responsibility**: Discussions emphasize **reader discretion**, with comparisons to works like *Ender’s Game* and *Westworld*, where violence is contextualized but still debated.  

### Final Takeaway:  
The novel polarizes readers, with critics condemning its explicit content as ethically jarring and defenders framing it as bold, thought-provoking sci-fi. The debate underscores broader tensions in storytelling about **taboo themes**—balancing artistic freedom with responsible representation.

### Builder.ai Collapses: $1.5B 'AI' Startup Exposed as 'Indians'?

#### [Submission URL](https://www.ibtimes.co.uk/builderai-collapses-15bn-ai-startup-exposed-actually-indians-pretending-bots-1734784) | 347 points | by [healsdata](https://news.ycombinator.com/user?id=healsdata) | [222 comments](https://news.ycombinator.com/item?id=44169759)

In a shocking turn of events, Builder.ai, once lauded as a $1.5 billion AI-driven innovator backed by industry giants like Microsoft, is crumbling under the weight of its own misrepresentations. The startup is pursuing bankruptcy protection after a significant $37 million withdrawal by key lender Viola Credit left it financially crippled. This revelation not only halts Builder.ai’s operations across five countries, including the UK and US, but also exposes the so-called AI technology as a façade, with 700 Indian developers behind the curtain rather than true AI systems.

Builder.ai had attracted high-profile investments and accolades for its no-code platform, which promised businesses easy app development through advanced AI. However, recent audits and whistleblowers have unveiled a very different reality. The company was reportedly inflating sales figures and engaging in deceptive practices, passing off manual work by human programmers as automated processes. 

The fallout from this scandal strikes hard at investors like the Qatar Investment Authority, casting a shadow over the broader AI startup ecosystem. This incident raises profound questions regarding transparency and ethics in tech marketing, as many other AI firms might face similar scrutiny for inflated promises and inadequate disclosures. While AI continues to be a cornerstone of technological advancement, Builder.ai's downfall serves as a stark reminder of the irreplaceable value and necessity of honest human expertise in innovation.

The Hacker News discussion scrutinizes Builder.ai's alleged AI capabilities and financial practices amid its bankruptcy filing. Key points from users include:

1. **Debunked AI Claims**: Participants highlight the lack of evidence for Builder.ai’s GenAI claims. The "AI" (Natasha) was reportedly a facade, routing projects to Indian developers using standard tools like GitHub Copilot, not advanced AI. Technical explanations note reliance on pre-existing libraries (e.g., MetaPath2Vec) rather than proprietary models.

2. **Comparisons to Fraud Cases**: Users liken the situation to Theranos, accusing Builder.ai of deliberate investor deception by overstating AI usage. Critics argue fronting human developers as AI constitutes fraud, not mere marketing hype.

3. **Financial Misconduct**: Commenters reference reports of inflated sales figures, round-tripping schemes, and mismanagement by leadership, leading to bankruptcy. The $37M withdrawal by Viola Credit is seen as a red flag for contractual breaches.

4. **Industry Critique**: Some defend aggressive marketing as common in tech but distinguish it from illegal fraud. Others note Infosys’ transparent AI services contrast with Builder.ai’s opacity, stressing ethical boundaries in AI startups.

5. **Unanswered Questions**: Users question the credibility of Builder.ai’s AI leadership (e.g., former Amazon AI director Craig Saunders) and demand transparency about their technical capabilities and investor communications.

Overall, the thread underscores skepticism about AI hype, emphasizing the need for ethical practices and validating technological claims in the startup ecosystem.

### Yoshua Bengio Launches LawZero: A New Nonprofit Advancing Safe-by-Design AI

#### [Submission URL](https://lawzero.org/en/news/yoshua-bengio-launches-lawzero-new-nonprofit-advancing-safe-design-ai) | 51 points | by [WillieCubed](https://news.ycombinator.com/user?id=WillieCubed) | [34 comments](https://news.ycombinator.com/item?id=44174643)

In the bustling world of artificial intelligence, a new beacon of hope emerges from Montreal: LawZero. Launched by AI luminary Yoshua Bengio — a name synonymous with the AI revolution and winner of the prestigious A.M. Turing Award — this nonprofit aims to reshape how we design and interact with AI systems. Facing a landscape where AI models sometimes exhibit alarming potential for deception and other risky behaviors, LawZero prioritizes safety over commercial gain.

Harnessing the expertise of a world-class team, LawZero introduces "Scientist AI" — a groundbreaking alternative to current agentic systems. Unlike their proactive counterparts, Scientist AIs are non-agentic, focusing on understanding rather than acting in the world, thus promoting transparency and truth. This innovative approach not only helps manage existing AI risks but also accelerates scientific discovery.

With financial backing from prominent entities like Open Philanthropy and the Future of Life Institute, LawZero aims to become a lighthouse for AI safety. The organization’s work underscores a core philosophy: AI should flourish as a global public good, bolstering human endeavors rather than overshadowing them. Rooted in the esteemed Mila - Quebec AI Institute, LawZero is set to lead the charge in safe-by-design AI innovation, ensuring that the advancing frontiers of AI remain aligned with humanity’s best interests.

The Hacker News discussion on Yoshua Bengio's **LawZero** nonprofit reveals skepticism, technical critiques, and broader concerns about AI safety and governance:

### Key Themes:  
1. **Funding and Past Projects**:  
   - Users question Montreal’s track record with public funds, citing **Element AI** (founded by Bengio, acquired by ServiceNow for $230M in 2020) as an example of misaligned incentives. Critics argue such ventures prioritize profit over public good, casting doubt on LawZero’s promise to avoid commercial motives.  

2. **AI Safety Challenges**:  
   - Debates arise over enforcing "hard safety rules" in AI systems. Comparisons to biological evolution highlight the difficulty of controlling intelligent systems, with users noting that even human-like intelligence doesn’t guarantee alignment with human values.  
   - **Skepticism about prompting**: Some argue that relying on prompts (e.g., ethics guidelines) is insufficient. References to *Brandolini’s law* underscore the asymmetry between debunking misinformation and creating safe systems.  

3. **Alignment and Unintended Behaviors**:  
   - Concerns center on systems developing survival-driven goals. **sbstnnght** warns that sufficiently intelligent AI might prioritize self-preservation over human objectives, likening it to the "instrumental convergence" problem. Others debate whether "safe-by-design" AI is even achievable.  

4. **Comparative Approaches**:  
   - **Animats** critiques LawZero’s "world model" approach as reminiscent of older projects like **Cyc** (a symbolic AI knowledge base), questioning its novelty. Critics suggest past failures (e.g., Element AI) signal potential pitfalls.  

5. **Nonprofit Accountability**:  
   - Users compare LawZero to **OpenAI**, with **mrlstp** accusing nonprofits of hypocrisy if they engage in profit-driven ventures. Discussions highlight the ambiguity in defining "safe AI" commercially (e.g., is GPT-4 Turbo “safe”?).  

### Notable Critiques:  
- **thrwwymths**: Argues hardcoding safety rules risks brittleness and misalignment.  
- **Der_Einzige**: Links to a paper on AI deception ([arXiv:2409.05907](https://arxiv.org/abs/2409.05907)), emphasizing risks.  
- **ddbs**: Questions whether safety models themselves could harm human agency.  

### Sentiment Overview:  
The thread reflects **cautious skepticism**. While some acknowledge LawZero’s noble goals, critics highlight historical missteps, technical hurdles in alignment, and distrust of nonprofits’ motives. The challenge of defining and operationalizing "safe AI" remains unresolved, with debates mirroring broader tensions in the AI ethics field.

### Gemini in Chrome

#### [Submission URL](https://gemini.google/overview/gemini-in-chrome/?hl=en) | 49 points | by [aru](https://news.ycombinator.com/user?id=aru) | [62 comments](https://news.ycombinator.com/item?id=44174681)

Google is leveling up its AI integration game with the introduction of Gemini, an AI assistant baked right into the Chrome browser. Currently, Google AI Pro and Ultra subscribers in the U.S. can access this nifty new tool, which promises to make online browsing more intuitive and efficient.

Gemini offers users a variety of features designed to streamline web experiences without the need to toggle between tabs. It provides instantaneous summaries of articles and webpages, answers questions directly based on the content you're viewing, and even helps clarify complex topics. Whether you're researching products or diving into dense material, Gemini is there to highlight key points, compare options, and provide detailed explanations.

Engagement with Gemini is straightforward. You can activate it by clicking its icon in the Chrome toolbar or using a personalized keyboard shortcut. The assistant operates on your command, meaning it only steps in when called upon, ensuring you remain in control of your browsing experience.

Curiously, users can interact with Gemini through either text or voice, allowing for a more conversational approach to exploring ideas or organizing thoughts. For settings and activity management, Gemini provides easy access so users can customize their interactions.

This innovative feature exemplifies a reimagined web experience, where AI assistance becomes seamlessly integrated into daily browsing, making complex information more accessible and decision-making more informed. While currently limited to specific subscribers in English, Google plans to expand this feature to more users and languages soon.

**Summary of Discussion:**

The introduction of Gemini AI in Chrome sparked a mix of skepticism, technical debates, and comparisons with existing tools. Key themes from the discussion include:

1. **Skepticism Toward AI Summaries**:  
   - Many users question the reliability of LLM-generated summaries, arguing they often miss critical details or context, especially in technical or long-form content (e.g., interviews, research papers).  
   - Concerns about "fluff" in AI-summarized articles were raised, with fears that this could degrade content quality over time.

2. **Existing Alternatives**:  
   - Tools like **Perplexity.ai**, Firefox’s built-in ChatGPT integration, and browser extensions were cited as existing solutions for summarization, raising questions about Gemini’s unique value.  
   - Some users prefer manual summarization or Ctrl+F for efficiency, criticizing recipe blogs and news sites for burying key info under unnecessary text.

3. **Copyright and Content Quality**:  
   - Debates emerged around whether AI-summarized recipes (or other content) could infringe copyrights, with users noting that recipes’ ingredient lists and steps are generally not copyrightable.  
   - Frustration with bloated web content (e.g., recipe sites with long narratives) was a recurring theme, with some welcoming AI tools to cut through the noise.

4. **Chrome’s Dominance and Privacy**:  
   - Critics highlighted concerns about Google’s market power, arguing that baking Gemini into Chrome reinforces its ecosystem dominance. Comparisons were drawn to Microsoft and Apple’s platform control.  
   - Privacy worries surfaced, with users noting Chrome’s access to passwords, history, and data, raising fears about deeper surveillance via AI integration.

5. **Mixed Reactions to AI Integration**:  
   - Some praised Gemini’s potential to streamline workflows (e.g., summarizing emails, documents) or enhance productivity.  
   - Others dismissed it as redundant or corporate jargon, with one user quipping, *"Corporate org-level announcement mails"* as a parody of AI-generated content.

6. **Technical and Market Dynamics**:  
   - Discussions touched on Google’s advantage in leveraging its infrastructure (e.g., Search, YouTube) for AI training, while competitors like OpenAI/Anthropic lack similar reach.  
   - Observations about Chrome’s resource-heavy nature and whether AI features would exacerbate performance issues.

**Notable Quotes**:  
- *"Imagine a horrific world where articles are 5 pages of LLM-generated fluff... a reverse-fluffing nightmare."*  
- *"Chrome is literally saving web development... but at what cost?"*  
- *"Why not just add a ‘remove AI’ button? Maybe I’m getting called a Luddite."*

**Conclusion**:  
While some see Gemini as a natural evolution of Chrome’s capabilities, others view it as a strategic move to lock users deeper into Google’s ecosystem. The discussion reflects broader tensions around AI’s role in content consumption, privacy, and market competition. Google’s challenge will be proving Gemini’s utility beyond gimmickry while addressing concerns about quality and control.

