## AI Submissions for Tue Jul 18 2023 {{ 'date': '2023-07-18T17:10:26.153Z' }}

### Voder Speech Synthesizer

#### [Submission URL](https://griffin.moe/voder/) | 247 points | by [CyborgCabbage](https://news.ycombinator.com/user?id=CyborgCabbage) | [40 comments](https://news.ycombinator.com/item?id=36771149)

In a fascinating throwback to the 1939-40 New York World's Fair, an application has been created that allows users to experience what it was like to operate the Voder, an early speech synthesis device. The Voder required complex button sequences to form each syllable, and it took about a year of practice to produce fluent speech. Helen Harper, one of the first people to master the Voder, went on to teach women how to use it through a year-long course. Now, this application puts users in the shoes of these skilled operators, allowing them to create vowel formants by pressing specific button combinations. While the results may not sound exactly like the original video, due to subtle articulations and dynamics, it provides a unique glimpse into the past.

### Generative AI space and the mental imagery of alien minds

#### [Submission URL](https://writings.stephenwolfram.com/2023/07/generative-ai-space-and-the-mental-imagery-of-alien-minds/) | 244 points | by [RafelMri](https://news.ycombinator.com/user?id=RafelMri) | [106 comments](https://news.ycombinator.com/item?id=36767837)

In a recent post on his blog, Stephen Wolfram explores the concept of alien minds and how artificial intelligence (AI) can help us understand them. Wolfram explains that AIs are essentially accessible forms of alien minds because they are not aligned with human thought processes. To capture the "mental imagery" of an alien AI, Wolfram suggests modifying a generative AI by resetting weights in its neural net. This "alien" neural net still produces images, but they become increasingly different from human perception as the neural net is modified. By studying these progressively alien images, Wolfram believes that we can gain insight into the worlds of different and alien minds. Additionally, Wolfram explains how AIs are trained to generate images by capturing the regularities found in billions of images from the web. These "random images" exhibit the statistical patterns of the training data and can show recognizable objects or scenes. Overall, Wolfram's exploration of alien minds and generative AI provides a fascinating perspective on perception and cognition.

The discussion on the submission revolves around the concept of AI-generated images and how they relate to human perception. Some comments touch on the similarities and differences between AI dream-like images and human dreams. Other comments discuss the technical aspects of AI-generated images and how they can be seen as artistic expression. Some users also express concerns about the role of AI in generating content and its impact on traditional art forms. Overall, the discussion explores the potential of AI to produce novel and intriguing imagery, while also raising questions about its limitations and implications.

### A Theory on Adam Instability in Large-Scale Machine Learning

#### [Submission URL](https://arxiv.org/abs/2304.09871) | 135 points | by [vov_or](https://news.ycombinator.com/user?id=vov_or) | [51 comments](https://news.ycombinator.com/item?id=36771484)

A new theory has emerged in the field of machine learning that explains the previously unexplained divergent behavior observed in the training of large language models. The theory suggests that the phenomenon is due to the optimization algorithm used for training, called Adam. The researchers argue that Adam can reach a state where the parameter update vector has a large norm and is uncorrelated with the direction of descent on the training loss landscape, leading to divergence. This behavior is more likely to occur in the training of deep models with large batch sizes, which is common in large-scale language model training. The theory is supported by observations from training runs of language models with varying scales. The paper, titled "A Theory on Adam Instability in Large-Scale Machine Learning," is authored by Igor Molybog and 16 other researchers. It is available for download on arXiv.

The discussion on this submission revolves around the new theory proposed by the researchers regarding the divergence in training large language models due to the use of the Adam optimization algorithm. Some users suggest trying different techniques such as controlling the term of the gradient or restarting the training process to mitigate the issue. Others discuss the potential of using CMA-ES (Covariance Matrix Adaptation Evolution Strategy) and other optimization methods for better results. There are also discussions on the limitations of gradient-based optimization methods and the challenges faced in finding global minima. Some users mention the possibility of using swarm optimization and genetic algorithms to improve the search process. One user points out the importance of local minima and the difference between biological neural networks and machine learning models. The energy costs of training models and the efficiency of neural networks compared to humans are also discussed in the comments.

### G/O media will make more AI-generated stories despite critics

#### [Submission URL](https://www.vox.com/technology/2023/7/18/23798164/gizmodo-ai-g-o-bot-stories-jalopnik-av-club-peter-kafka-media-column) | 101 points | by [Analemma_](https://news.ycombinator.com/user?id=Analemma_) | [100 comments](https://news.ycombinator.com/item?id=36773363)

G/O Media, the digital publisher behind sites like Gizmodo and the Onion, sparked controversy when it published four stories generated by AI engines without input from its editors or writers. Despite the backlash, G/O executives have expressed plans to create more AI-written stories as part of an ongoing experiment with the technology. The move sets G/O apart from most conventional publishers, who are interested in using AI to assist in content creation but are not yet interested in fully machine-made content. G/O Media CEO Jim Spanfeller believes that AI will be transformative for the media industry and should not be ignored. However, G/O employees are concerned about the impact on employee morale and fear that AI will eventually replace human journalists. G/O executives insist that they will not replace staff with AI, but the skepticism among employees remains.

### A Latent Space Theory for Emergent Abilities in Large Language Models

#### [Submission URL](https://arxiv.org/abs/2304.09960) | 14 points | by [rileyphone](https://news.ycombinator.com/user?id=rileyphone) | [4 comments](https://news.ycombinator.com/item?id=36776920)

The paper titled "A Latent Space Theory for Emergent Abilities in Large Language Models" by Hui Jiang explores the relationship between languages and their underlying meanings. The study categorizes languages as unambiguous or epsilon-ambiguous and demonstrates that large language models (LLMs) can exhibit emergent abilities, such as language understanding, in-context learning, chain-of-thought prompting, and effective instruction fine-tuning, through Bayesian inference on the sparse joint distribution of languages. This paper presents quantitative results and sheds light on the capabilities of LLMs in language processing.

The discussion around the submission focuses on the structure and analysis of languages. One user argues that languages are created randomly for specific purposes and convey information through distinct and independent units such as sentences or programming languages. Another user adds that language messages represent intended meanings in a space, where different intentions constitute distinct regions and simple intentions are represented by elements in a finite set. They suggest that the study of language should consider discrete countable assumptions.  In response, another user provides research that supports the effectiveness of transformer-based language models. They share two articles that discuss the reasonable effectiveness of these models.  However, one user disagrees with the assumption that language messages contain intentions recursively in a fundamental space of meaning. They argue that this approach to studying language is oversimplified and may not fully capture the complexity of the field.  Finally, a user points out that the discussion seems to lack an interdisciplinary perspective and suggests that siloing of perspectives may prevent a comprehensive understanding of the topic.

### Qualcomm works with Meta to enable on-device AI applications using Llama 2

#### [Submission URL](https://www.qualcomm.com/news/releases/2023/07/qualcomm-works-with-meta-to-enable-on-device-ai-applications-usi) | 99 points | by [ahiknsr](https://news.ycombinator.com/user?id=ahiknsr) | [82 comments](https://news.ycombinator.com/item?id=36778730)

Introducing the Hacker News Daily Digest, your one-stop destination for a quick and engaging summary of the top stories on Hacker News. We've got our virtual pens and minds ready to deliver the latest and most interesting news from the developer and tech community straight to you.

Whether you're a seasoned coder, a tech enthusiast, or simply curious about the buzzing tech world, our daily digest will keep you informed and entertained. So sit back, relax, and let us handle the heavy lifting of filtering through the vast sea of Hacker News submissions.

From software development breakthroughs to the latest tech gossip, we'll cover it all. Expect insights into the hottest programming languages, discussion threads on frameworks, debates over the merits of different tools, and updates on the most interesting startups.

But that's not all. We'll also dive into thought-provoking and mind-bending articles on artificial intelligence, blockchain technology, cybersecurity, and more. Our aim is to provide you with the most captivating and relevant stories that will ultimately spark your curiosity and keep you ahead of the game.

You can count on us to deliver daily summaries that capture the essence of the Hacker News community. Say goodbye to endlessly scrolling through countless submissions; we'll bring you only the best and most interesting conversations that are shaping the tech landscape.

So whether you're a seasoned Hacker News regular or a newcomer to the platform, join us for the Hacker News Daily Digest and stay tuned for an engaging and informative daily dose of the tech world's most captivating stories.

The discussion on the submission revolves around various aspects of Apple's approach to artificial intelligence (AI). Some commenters express skepticism about Apple's AI capabilities and suggest that the company is focusing more on hardware rather than AI integration. Others mention Apple's past failures, such as Apple Maps, and criticize the company's product planning and recognition of flaws. 

There is a discussion about the usability of Apple Maps compared to Google Maps, with some users pointing out the shortcomings of Apple Maps and praising Google Maps for its UI and search capabilities. 

On the topic of Apple's AI advancements, some users mention the potential integration of Siri with local machine learning models (LLMs) and the possibility of Siri becoming a more advanced personal assistant. Others discuss Apple's track record of integrating AI features into iOS, such as text and photo recognition, and the company's ability to apply innovative models. 

There is a debate about the advantages of Apple's AI strategy compared to other companies, with some highlighting the company's focus on user experience and unique features, while others argue that Apple is not as competitive in AI. 

The conversation also touches on Apple's investment in LLMs, possible future developments in AI, and the capabilities of chatbots and machine learning models.

### Meta and Qualcomm team up to run Llama 2 on phones

#### [Submission URL](https://www.cnbc.com/2023/07/18/meta-and-qualcomm-team-up-to-run-big-ai-models-on-phones.html) | 17 points | by [tzm](https://news.ycombinator.com/user?id=tzm) | [3 comments](https://news.ycombinator.com/item?id=36775645)

Qualcomm and Meta have announced a partnership to enable the social networking company's new large language model, Llama 2, to run on Qualcomm chips on phones and PCs starting in 2024. Large language models like Llama 2 have traditionally run on large server farms with powerful Nvidia graphics processors. This announcement by Qualcomm suggests that it wants to position its processors as well-suited for AI "on the edge," or on a device, rather than in the cloud. Running large language models on phones could significantly reduce the cost of running AI models and lead to better and faster voice assistants and other applications. Qualcomm's chips include a tensor processor unit (TPU) that is well-suited for AI calculations, although the processing power available on a mobile device is much less than that of a data center with cutting-edge GPUs. Meta's Llama 2 is unique because Meta published its "weights," which govern how the AI model works, making it open-source and accessible to researchers and commercial enterprises without permission or payment. Qualcomm and Meta have previously collaborated on chips for virtual reality devices.

The discussion on Hacker News revolves around the announcement of the partnership between Qualcomm and Meta to enable the running of Meta's large language model, Llama 2, on Qualcomm chips on devices starting in 2024.  One user questions whether Llama 2 will be relevant by 2024 and suggests that there might be a newer version of the model. 
 Another user mentions that Qualcomm has previously collaborated with Stable Diffusion to run ML models on phones and that they published a demo with card recognition. They speculate that Qualcomm might have built an extensive feature set, possibly for user interface implementation.  Another user highlights the importance of this announcement, specifically in terms of enabling low-latency implementation of large language models on devices, which could greatly impact the user experience of AI voice assistants and other applications. They compare it to Google's PaLM version for Android devices and suggest that Apple might also jump on this trend early by introducing an on-device Siri powered by ML chips.

### From Dating to Vector Search – “Stable Marriages” on a Global Scale

#### [Submission URL](https://ashvardanian.com/posts/searching-stable-marriages/) | 35 points | by [vov_or](https://news.ycombinator.com/user?id=vov_or) | [31 comments](https://news.ycombinator.com/item?id=36772545)

In this article, the author explores the concept of stable marriages and its implications for the future of databases. Stable marriages are computed using preference lists, but this approach requires a large amount of memory when dealing with a large number of candidates. To overcome this, the author suggests using a scalable Vector Search engine to dynamically recalculate candidate lists. However, the quality of representations in a shared Vector Space, particularly for Multi-Modal data, is a challenge. The author highlights the need for improving space-alignment techniques to enhance the performance of upcoming Generative Vision-Language models. The article also shares the author's personal journey of applying these algorithms to a dating app and the cost implications of using the classic stable marriage algorithm for a billion candidates. The author explains how to balance compute requirements by recalculation and discusses the implementation details of a Vector Search engine.

The discussion on this article covers a wide range of topics related to arranged marriages, divorce rates, personal freedom, societal stability, and cultural differences. Some users argue that arranged marriages can lead to stability and happiness, while others believe that individual happiness should take precedence. There is also a discussion about the cultural contexts in which arranged marriages are common and the social pressures that can be associated with them. Additionally, there are comments about the need for structured content in databases and the trade-offs between personal freedom and societal benefits.

### AI System Helped Cops ID a Drug Trafficker by Analyzing His Driving Patterns

#### [Submission URL](https://gizmodo.com/rekor-ai-system-analyzes-driving-patterns-criminals-1850647270) | 30 points | by [HiroProtagonist](https://news.ycombinator.com/user?id=HiroProtagonist) | [19 comments](https://news.ycombinator.com/item?id=36772253)

In a recent case in New York, police were able to identify and arrest a drug trafficker with the help of artificial intelligence (AI). The police used the services of a company called Rekor, which analyzes traffic patterns and identifies suspicious behavior. Rekor's software sifts through a large database of information collected from regional roadways by the county's automatic license plate recognition system. By recording and analyzing vehicle trajectories, the software can determine whether certain routes are suspicious or not. In this case, the AI algorithm determined that the driver's routes were consistent with those of a drug trafficker, leading to his arrest. This highlights how AI is being used to enhance surveillance systems and aid law enforcement. However, there are concerns about the potential misuse of this technology and the need for appropriate regulations to prevent abuse.

The discussion on this submission revolves around the pros and cons of AI-assisted surveillance systems and their potential misuse.  One user points out that the AI algorithm used in this case reminds them of the controversy surrounding the analysis of phone data, where innocent activities were mistakenly flagged as suspicious. Another user jokingly suggests that the investigation could have been prompted by Tupperware parties, emphasizing that false positives can occur. Some users express concerns about the potential for targeted searches in minority neighborhoods and the erosion of privacy. Others argue that AI-based investigations can be helpful in fighting crime, but there needs to be clear regulations in place to prevent abuse. The discussion also touches on the limitations of AI systems and their potential for false positives. One user brings up the issue of law enforcement accessing location records without a warrant, while another user notes the historical use of AI systems to combat terrorism and drug dealers. There is a debate about the balance between privacy and security, with some arguing that surveillance technology is necessary to target criminals while others feel it infringes on civil liberties. One user points out the decline in public trust in AI and the surveillance state, while another user suggests that the focus should be on legislation to address the problem. Finally, there is a mention of the shift in public attention towards other pressing issues such as domestic terrorism, child trafficking, and party affiliations.

