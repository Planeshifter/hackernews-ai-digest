## AI Submissions for Sat Sep 16 2023 {{ 'date': '2023-09-16T17:10:10.057Z' }}

### Generative Image Dynamics

#### [Submission URL](https://generative-dynamics.github.io/) | 310 points | by [hughes](https://news.ycombinator.com/user?id=hughes) | [26 comments](https://news.ycombinator.com/item?id=37536016)

Google Research has released a paper and demo showcasing their latest project called Generative Image Dynamics. This approach models an image-space prior on scene dynamics, allowing for the transformation of a single image into a seamless looping video or an interactive dynamic scene. The model learns from motion trajectories in real video sequences, such as trees swaying in the wind or clothes billowing. Using a frequency-coordinated diffusion sampling process, the model predicts per-pixel long-term motion representations in the Fourier domain, which are called neural stochastic motion textures. These textures can then be converted into dense motion trajectories that span an entire video. The project also includes an image-based rendering module, which can be used to turn still images into dynamic videos, or to allow users to interact with objects in photos. A demo is available where users can click and drag a point on an image to see how the scene moves. The project enables the simulation of object dynamics in response to user excitation and the generation of slow-motion videos by interpolating predicted motion textures.

The discussion on this submission revolves around various aspects of the project. Some users are discussing the potential of using generative images and cinemagraphs for marketing purposes, noting that they can have a bigger impact on viewers than regular still photos. Others are sharing examples of subtle movement in cinemagraphs and suggesting ways to qualify and describe these types of images. There is also a discussion about the feasibility of implementing the technology in video games, with some users noting that realistic physics and dynamic movements are already being handled in games like Red Dead Redemption 2. One user shares examples of games that have impressive grass and physics simulations. Another user mentions the limitations of the gaming industry in adopting deep learning and AI models due to performance and complexity issues. They also discuss the potential negative effects of randomly breaking immersion in games by generating non-continuous movements for characters.

The discussion also touches on related topics, such as the stability of video game physics and interacting with floors in games, the potential of combining photogrammetry and physics for realistic effects, and the use of low-vector movement requirements in EbSynth. One user appreciates Google's research efforts and shares excitement about the possibilities of combining machine learning and gaming. Another user expresses anticipation for stable diffusion GPT models in video games and notes the challenges in implementing them without extensive computational resources. Overall, the discussion showcases different perspectives on the applications, limitations, and potential of generative image dynamics in various domains, including marketing and gaming.

### Adobe will charge “credits” for generative AI

#### [Submission URL](https://helpx.adobe.com/firefly/using/generative-credits-faq.html) | 130 points | by [tambourine_man](https://news.ycombinator.com/user?id=tambourine_man) | [145 comments](https://news.ycombinator.com/item?id=37538878)

Generative credits are a new feature introduced by Adobe that provide priority processing of generative AI content. These credits are used when performing actions such as generating text effects, loading more images in Text to Image, using generative recolor in Adobe Illustrator, using text effects in Adobe Express, and using generative fill in Adobe Photoshop. The consumption of generative credits depends on the computational cost of the generated output and the value of the generative AI feature used. However, certain actions, such as using generative AI features defined as "0" in the rate table or trying a prompt in the Firefly gallery without refreshing, do not consume generative credits. 

The number of generative credits you have depends on your plan, and they reset each month. Different plans offer different numbers of generative credits, with higher-tier plans including more credits. For example, the Creative Cloud All Apps plan includes 1,000 generative credits per month, while lower-tier plans may include 250 or 100 generative credits. Adobe Stock paid subscriptions also include 500 generative credits per month. 

It's important to note that until November 1, 2023, subscribers of Creative Cloud, Adobe Firefly, Adobe Express, and Adobe Stock won't be subject to generative credit limits. However, starting from November 1, 2023, credit limits will apply. Adobe plans to expand generative AI features to include higher-resolution images, animation, video, and 3D in the future, and the number of generative credits consumed for those features may be greater. Overall, generative credits aim to enhance creative possibilities and empower users to create extraordinary content using AI technology.

The discussion on this submission covers various topics related to the use of generative AI, Adobe's plans, hardware requirements, and the implications for users. Some users express concerns about charging for AI-powered features and the need for compliance with regulations. Others discuss the possibility of Adobe moving towards local deployment of AI models and the trustworthiness of Adobe products. There are also discussions about the capabilities of consumer GPUs, the cost of hardware, and the potential energy consumption of running AI models. Users analyze the performance of different GPUs and compare them to Adobe's offerings. The discussion also touches on the future of AI models, the limitations of hardware, and the impact of energy constraints. Some users mention the availability of open-source alternatives and the flexibility of locally-run models. There are also discussions about licensing and the commercial use of AI-generated content.

### Unity's Self-Combustion Engine

#### [Submission URL](https://www.gamesindustry.biz/unitys-self-combustion-engine-this-week-in-business) | 148 points | by [erickhill](https://news.ycombinator.com/user?id=erickhill) | [144 comments](https://news.ycombinator.com/item?id=37535910)

Unity, the popular game development platform, faced backlash after introducing a new Runtime Fee. The fee applies to Unity developers of a certain size and requires them to pay a fee every time their game is installed on a new device after January 1, 2024. The fee is based on game installs, not sales, which has created confusion among developers. Unity initially stated that demos would count as installs, but later clarified that demos, trials, game bundles, and giveaways would not be included. However, subscription services like Game Pass would count as an install. In response to the fee, a collective of studios pulled Unity and IronSource ads from their titles and called upon others to do the same. Developers of popular games like Among Us and Slay the Spire have expressed their inclination to switch engines if the changes go through, citing trust as a crucial factor for developers using a commercial game engine. Unity, having recognized the importance of supporting developers in the long term, now faces the challenge of addressing concerns and restoring trust within its community.

The discussion on this topic revolves around several key points. Some users express skepticism about Unity's decision and suggest that the company is trying to gain a market advantage. Others raise concerns about the impact of the fee on developers and question Unity's handling of the situation. There is also a discussion about the similarities and differences between Unity and other game development engines like Unreal and Cryengine. In addition, some users discuss the broader implications of market dominance and the impact on industries such as taxis and ride-sharing. The discussion also touches on issues such as the safety of ride-sharing services and the impact of technology on traditional businesses. Some users also discuss the challenges faced by new generations of entrepreneurs and the changing dynamics of the business world. Finally, there is a debate about the pricing and accessibility of software in general, with some users arguing that the current market dynamics favor larger businesses and hinder smaller ones.

### Show HN: Superflows – open-source AI Copilot for SaaS products

#### [Submission URL](https://github.com/Superflows-AI/superflows) | 24 points | by [henry_pulver](https://news.ycombinator.com/user?id=henry_pulver) | [5 comments](https://news.ycombinator.com/item?id=37533503)

Superflows is an open-source toolkit that allows you to build an AI assistant for SaaS products. This AI assistant can understand natural language queries and make calls to the software's API to provide answers. For example, a CRM user could ask about the status of a deal or ask for recommendations on how to get deals back on track. The toolkit also provides a developer dashboard to configure and test the assistant, as well as pre-built UI components for easy integration into your product. You can try out the cloud version for free or self-host it. Superflows aims to make it easier for users to interact with software products and get the information they need.

The discussion on the submission started with a comment from user "SaarasM" who mentioned that they had recently tried to build a similar tool for their SaaS product but were not satisfied with its reliability. They were interested in trying out Superflows and asked if the tool had good reliability. User "henry_pulver" responded that they have had issues with reliability in similar tools, with only 80% of the tasks working smoothly and the remaining 20% being a challenge. However, they mentioned that they would like to try Superflows and provided their contact information for further discussion. User "RobertVDB" chimed in to mention that they have seen support for open-source models and that it is possible to self-host the models. In response to this, "henry_pulver" mentioned that they are currently working on a project called Base Llama 2, which aims to improve reliability by prompting users to provide feedback on the model's performance. They mentioned that they are also working on fine-tuning the model's prompts for better accuracy and will release it soon for others to self-host. Lastly, user "jmrmblw" expressed their appreciation for the developer dashboard and mentioned that open-source tools like Superflows are helpful for debugging purposes. Overall, the discussion mainly revolved around the reliability of similar tools, the possibility of self-hosting models, and positive feedback on Superflow's developer dashboard and open-source nature.

### GPT-4 is not getting worse

#### [Submission URL](https://coagulopath.com/gpt-4-is-not-getting-worse/) | 141 points | by [COAGULOPATH](https://news.ycombinator.com/user?id=COAGULOPATH) | [164 comments](https://news.ycombinator.com/item?id=37532522)

In a recent post on Hacker News, the author reflects on their initial criticism of GPT-4, OpenAI's state-of-the-art AI text generation model. They admit that their previous tests were flawed and biased, leading to an inaccurate assessment of the model's performance. The author acknowledges their personal dislike for GPT-4's tone and the hype surrounding AI, which may have influenced their desire for the model to fail. However, they have since reconsidered their stance and find themselves defending AI against unfounded criticisms. The author highlights a study that shows a decline in GPT-4's performance in identifying prime numbers but argues that mistakes are a part of learning and progress. Overall, the author's perspective has shifted, and they now recognize the need for a more balanced and objective approach when evaluating AI models.

The discussion on this Hacker News submission revolves around various aspects of GPT-4 and OpenAI's AI text generation models. Here are some key points from the conversation:

- One user mentioned encountering a bug in OpenAI's API that causes the response to stop streaming after 5 minutes of debugging prompt lines. They also mentioned that skipping sections in the generated output seems to be a common issue in information extraction tasks.
- Another user expressed annoyance at how frequently things change in AI models, making it challenging to keep up with updates and causing issues in their coding work.
- Some users discussed the limitations of GPT-4, such as its difficulty in reliably multiplying large numbers. However, others argued that mistakes in AI models are to be expected and should be seen as an opportunity for learning and improvement.
- There were discussions about the limitations of OpenAI's API in terms of response times and resource allocation. Some users pointed out that the 5-minute time limit for generating responses is insufficient and that OpenAI should provide better support.
- The topic of unintended behavior in AI models was raised, with users suggesting that people should not expect perfect results and should be aware of the limitations and potential issues.
- There were also discussions about server configuration, network connectivity, and potential streaming problems related to the OpenAI API.

Overall, the conversation highlighted the challenges and limitations of AI models like GPT-4 and the need for better documentation, support, and understanding of AI technologies.

### Mesa-optimization algorithms in Transformers

#### [Submission URL](https://arxiv.org/abs/2309.05858) | 23 points | by [kelseyfrog](https://news.ycombinator.com/user?id=kelseyfrog) | [5 comments](https://news.ycombinator.com/item?id=37531815)

Researchers from various institutions have released a paper titled "Uncovering mesa-optimization algorithms in Transformers," aiming to understand the superior performance of Transformers in deep learning. The study suggests that Transformers possess an architectural bias towards mesa-optimization, a learned process within the forward pass of a model. The research team reverse-engineered autoregressive Transformers trained on simple sequence modeling tasks to uncover underlying gradient-based mesa-optimization algorithms. They also demonstrated that the learned forward-pass optimization algorithm could be applied to solve supervised few-shot tasks. The researchers propose a novel self-attention layer called the mesa-layer, which can efficiently solve optimization problems specified in context and potentially improve the performance of Transformers. Overall, this work sheds light on the presence of mesa-optimization as a crucial but hidden operation within trained Transformers.

The discussion on this submission revolves around the significance and implications of the research paper.  One commenter finds the research fascinating and mentions that it explores the optimization process in Transformers, which can potentially lead to significant improvements in performance. Another commenter appreciates the in-depth analysis and difficulty of the paper, stating that it tackles complex concepts and demonstrates sophisticated methodologies. They also note that the paper is internationally significant. Another commenter points out that the paper primarily focuses on natural language processing tasks and how Transformers can be applied to different digital domains. They mention that the paper offers valuable applications, but they do not provide much feedback beyond that.

Further discussion delves into a detailed breakdown of the paper's content. It includes sections on the hypothesis of mesa-optimization in Transformers, reverse-engineering Transformers to uncover the internal optimization process, the few-shot learning capabilities of Transformers, the introduction of the mesa-layer, and the generalization of previous work. The commenters analyze the theoretical connections, such as linear self-attention gradient descent, and the two-stage mesa-optimizer. They also discuss the empirical analysis, where the paper concludes that Transformers implicitly perform optimization steps and propose the mesa-layer to enhance model performance. Overall, the discussion appreciates the importance of understanding the optimization process in Transformers and the potential impact of the proposed mesa-layer. Commenters delve into the technical aspects of the research and provide insights into its significance within the field of deep learning.

