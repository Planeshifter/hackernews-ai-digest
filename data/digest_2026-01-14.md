## AI Submissions for Wed Jan 14 2026 {{ 'date': '2026-01-14T17:18:34.975Z' }}

### Scaling long-running autonomous coding

#### [Submission URL](https://cursor.com/blog/scaling-agents) | 251 points | by [samwillis](https://news.ycombinator.com/user?id=samwillis) | [159 comments](https://news.ycombinator.com/item?id=46624541)

Hundreds of autonomous coding agents, one codebase: what actually worked

- Flat, self-coordinating swarms stalled. Letting equal-status agents “claim” tasks via a shared state file led to lock thrashing, deadlocks, and risk-averse behavior. Even optimistic concurrency reduced brittleness but not the deeper issue: no one took ownership of hard, end-to-end work.

- A planner/worker split unlocked scale. Persistent planner agents map the codebase and spawn sub-planners; workers focus only on execution, pushing changes without coordinating laterally. A simple judge gates iterations. This hierarchy preserved momentum and avoided tunnel vision while scaling to hundreds of concurrent workers.

- Real-world stress tests were big. The system ran for about a week to build a web browser from scratch: >1M lines across ~1,000 files, with many workers pushing to the same branch and few conflicts. Other long runs included:
  - In-place Solid→React migration in the Cursor codebase (3+ weeks, +266K/−193K edits), now testable and likely mergeable.
  - A Rust rewrite that made video rendering 25× faster, with smooth zoom/pan and motion blur, headed to production.
  - Ongoing moonshots: Java LSP (7.4K commits, 550K LoC), a Windows 7 emulator (14.6K commits, 1.2M LoC), and “Excel” (12K commits, 1.6M LoC).

- Model choice and role fit mattered. Newer GPT-5.2 variants sustained long-horizon focus and fidelity; Opus 4.5 yielded control sooner. Planners and coders benefited from different models, so they mix models by role rather than standardize on one.

- Simpler beats clever. Removing an “integrator” bottleneck improved throughput; workers handled conflicts fine. The sweet spot is between chaos and rigidity, and prompts shaped behavior more than orchestration code.

Bottom line: Multi-agent coding can push through multi-week, million-LOC projects, but coordination is the hard part. Lightweight hierarchy, role-specialized models, and careful prompting drove most of the wins; there’s still plenty of headroom to make planners and long runs more reliable.

Based on the discussion, users were highly skeptical of the submission's claims of success, with several commentators auditing the repository and characterizing the output as "AI slop" rather than a functional application.

**Key points from the discussion:**

*   **Codebase Quality and Maintainability:** Users describing the codebase found it nearly impossible to navigate or maintain. One critic noted the code consisted of hundreds of thousands of tiny files buried in deeply nested subdirectories, making it difficult to search or understand the architecture. The consensus was that while an agent can "compile" a codebase, the result is unmaintainable "spaghetti code" that no human could reasonably take over.
*   **Failed Constraints and Faked Functionality:** Upon closer inspection, the "browser" implementation relied on crude shortcuts rather than actual browser logic. One user pointed out that the font rendering didn't calculate real metrics, instead using a hardcoded estimate (`0.5 * font_size`), and that basic layout engines and directionality handling were fundamentally incorrect.
*   **Broken Build State:** Despite the article claiming a successful "judge" system, users reported that the repository was filled with errors and warnings. One user noted that the Continuous Integration (CI) pipeline was completely broken, yet Pull Requests were still being merged and "verified" as successful, casting doubt on the reliability of the autonomous evaluation process.
*   **Complexity vs. Memorization:** Several commenters questioned whether the agents were actually solving problems or simply regurgitating training data from open-source projects like Chromium or online tutorials. They argued that building a browser requires handling thousands of edge cases and performance optimizations that this "toy" implementation ignored entirely.
*   **Predictions vs. Reality:** The discussion referenced Simon Willison’s prediction that an AI would build a browser by 2029. While this project attempts that, users argued it failed the "functioning" part of the criteria. They suggested that without a human in the loop to correct quality spirals, completely autonomous coding agents currently produce large-scale broken software rather than functional products.

### ChromaDB Explorer

#### [Submission URL](https://www.chroma-explorer.com/) | 55 points | by [arsentjev](https://news.ycombinator.com/user?id=arsentjev) | [4 comments](https://news.ycombinator.com/item?id=46624731)

Chroma Explorer is a native macOS client for ChromaDB that gives RAG builders and LLM app developers a GUI for exploring and managing vector stores. It supports multiple connection profiles (local, remote, and Chroma Cloud) with secure API key storage, plus full collection management including custom embedding functions and HNSW tuning. You can run natural-language semantic searches, browse and edit documents, and perform batch operations for bulk updates. It integrates with 13+ embedding providers out of the box (OpenAI, Cohere, Gemini, Mistral, Jina, Voyage AI, Ollama, and more). Open-source on GitHub, designed with a slick glassmorphic UI, but macOS-only (11.0+). The pitch: less SDK/CLI wrangling, faster debugging and iteration on vector pipelines.

The discussion focused on the technical implementation of the client, specifically the use of the term "native." One user expressed hope that the application relied on OS-native code rather than a wrapper, but a respondent examined the repository and confirmed it is built using Electron. Additionally, the conversation offered praise for Chroma's broader research efforts, specifically a study highlighting how complex reasoning tasks in LLMs degrade faster than standard "needle-in-a-haystack" benchmarks suggest.

### The Influentists: AI hype without proof

#### [Submission URL](https://carette.xyz/posts/influentists/) | 242 points | by [LucidLynx](https://news.ycombinator.com/user?id=LucidLynx) | [165 comments](https://news.ycombinator.com/item?id=46623195)

The Influentists: hype-first AI demos and the “technical debt of expectations”

A widely shared tweet from Jaana “Rakyll” Dogan claimed Claude Code recreated Google’s distributed agent orchestrator work “in an hour.” After a wave of doom-posting, her follow-up thread added crucial context: the “hour” result was a proof-of-concept guided by hard-won architectural decisions from prior iterations, not a production system conjured from scratch. In other words, AI amplified expert intent; it didn’t replace it.

The post uses this episode to examine a broader pattern the author dubs “The Influentists”—technically credible voices who drive hype via:
- Trust-me-bro anecdotes framed as universal truths (e.g., “I’ve never felt this much behind as a programmer”).
- Lack of reproducible artifacts (code, data, prompts, methodology).
- Strategic ambiguity that allows later “clarifications.”

Examples span big AI labs and platform companies: a Microsoft leader touting AI-assisted Rust rewrites of massive C/C++ codebases before dialing it back as “research,” and staff at frontier labs teasing AGI-adjacent breakthroughs ahead of more modest releases.

Why it matters
- Expectations gap: Viral demos can imply “a year of work in an hour,” discouraging juniors who don’t see the hidden scaffolding of expertise, curation, and constraints.
- Misallocation of attention: Teams may chase vibes over verifiable results, accumulating a “technical debt of expectations.”
- Practical takeaway: LLMs can be powerful force multipliers when steered by deep domain knowledge, but claims should be judged by reproducible evidence, not rhetoric.

**The Influentists: Hype-First AI Demos and the "Technical Debt of Expectations"**

**The Story:**
A recent viral post by Jaana Dogan proposing that Claude Code recreated Google’s distributed agent orchestrator "in an hour" sparked a new debate on the gap between AI demos and engineering reality. Dogan later clarified that the result was a proof-of-concept heavily reliant on pre-existing architectural decisions, but the incident highlighted a growing trend: "The Influentists."

The author argues that these "technically credible" voices drive hype through unverifiable anecdotes and strategic ambiguity, accumulating a "technical debt of expectations." By framing proof-of-concepts as production-ready breakthroughs, they discourage junior developers and mislead companies into chasing "vibes" over verifiable results. The piece concludes that while LLMs are powerful force multipliers for experts, claims must be judged by reproducible artifacts, not rhetoric.

**The Discussion:**
The discussion on Hacker News was highly skeptical, focusing on the mechanics of social media grift, the reality of "AI-assisted" coding, and the flooding of technical spaces with marketing noise.

*   **The "Hype Translator":** Users humorously translated common viral AI claims into reality. "I built a brick-and-mortar business with AI" was translated to "I asked Claude to Google local lawyers." "I managed 10 Product Managers" became "I drafted some PRDs." "I launched a product in a weekend" became "I made a single HTML landing page."
*   **The New Grift Economy:** Many commenters noted that AI hype threads have replaced previous trends like drop-shipping, crypto, and Amazon FBA as the primary method for engagement farming. The consensus is that generic "I feel so behind" posts aren't genuine reflections of the industry, but top-of-funnel marketing for courses or newsletters.
*   **Astroturfing and Dead Internet:** Several users lamented the state of online communities (including Reddit and X), suggesting that a significant portion of "I built this with Claude" content feels like astroturfing or bot-generated marketing, making it difficult to find genuine technical discussions.
*   **The "Last 10%" Problem:** Experienced engineers argued that while LLMs are excellent for reaching the "70% done" mark (prototypes, scripts, throwaway code), the remaining 30%—security, reliability, maintainability—takes the vast majority of the effort. They warned that relying on AI for complex architecture introduces "technical debt of the future," where subtle bugs and verbose code create maintenance nightmares.
*   **Historical Optimism:** A counter-narrative emerged comparing the current frenzy to the Dotcom bubble. Users pointed out that while businesses like Pets.com failed (akin to today's unusable AI wrappers), the internet itself succeeded. Just because the hype is loud and often foolish doesn't mean the underlying technology won't eventually facilitate real utility, such as buying furniture online—or coding complex systems—becoming mundane.

### Eigent: An open source Claude Cowork alternative

#### [Submission URL](https://github.com/eigent-ai/eigent) | 23 points | by [WorldPeas](https://news.ycombinator.com/user?id=WorldPeas) | [3 comments](https://news.ycombinator.com/item?id=46618954)

Eigent: open-source multi-agent “cowork” desktop app (local-first, MCP-enabled)

What it is
- A desktop app for orchestrating a team of AI agents to automate complex workflows, with privacy-friendly, fully local deployment as the default.
- Built on CAMEL-AI; Apache-2.0 licensed.

Why it matters
- Pushes the local-first, multi-agent trend: parallel task execution, human-in-the-loop checkpoints, and deep tool integration without sending data to a third-party service.
- MCP (Model Context Protocol) support means agents can safely use a wide ecosystem of tools (web browsing, code exec, Notion, Google suite, Slack, and custom/internal APIs).

Key features
- Multi-agent workforce: prebuilt Developer, Search, Document, and Multi-Modal agents; parallel task breakdown and coordination.
- Local model support: vLLM, Ollama, LM Studio, and custom models.
- Human-in-the-loop: escalates for input when uncertain or stuck.
- Enterprise: SSO/access control, customizable deployments.
- 100% open source.

How to try it
- Local deployment (recommended): full isolation, local backend + models; guide in repo.
- Quick start (cloud-connected): git clone, npm install, npm run dev (requires Eigent account).
- Also available as a hosted cloud option.

Tech notes
- Electron desktop app with a local/server backend; repo shows a modern TS/JS stack.
- License: Apache-2.0. Stars: ~5.5k.

Links
- Repo: https://github.com/eigent-ai/eigent
- Site: https://www.eigent.ai

Take
Eigent aims to be a “Cursor-for-agents” style desktop, but local-first and tool-rich via MCP. If you’ve been evaluating CrewAI/LangGraph/AutoGPT-style orchestration or internal agent platforms, this offers a packaged, GUI-driven path with privacy and enterprise knobs baked in.

**Discussion Summary:**

Discussion focused on validating the project's "open source" label. One user was initially skeptical after noticing a pricing page and credit system, questioning if the term was being applied too loosely to a commercial wrapper. A respondent clarified that the tool supports a fully local deployment—including the backend server and integrations with local model runners like Ollama—with zero external dependencies, validating its open-source status despite the parallel existence of a hosted cloud tier. On the practical side, a tester noted that using high-end models like Claude Opus for multi-agent workflows could quickly become cost-prohibitive due to API fees.

### Show HN: OSS AI agent that indexes and searches the Epstein files

#### [Submission URL](https://epstein.trynia.ai/) | 204 points | by [jellyotsiro](https://news.ycombinator.com/user?id=jellyotsiro) | [95 comments](https://news.ycombinator.com/item?id=46611348)

Nozomio Labs launches a searchable “Epstein Files” archive
- What it is: A unified, AI-powered index of materials from the Epstein archive—emails, messages, flight logs, court documents, and related records—aimed at making primary sources easier to explore.
- How it works: Built on Nia with Claude Sonnet 4.5 for semantic search and Q&A. You can ask natural-language queries like “Most frequently mentioned people?”, “Flight records from 2002–2005?”, or “Emails about specific locations?”, and check whether specific names appear.
- Why it matters: Centralizes scattered, often unwieldy documents into a single interface for journalists, researchers, and the public to investigate timelines, connections, and mentions.
- Caveats: Content is sensitive; OCR/entity extraction can mislabel names or contexts. The site encourages verifying AI answers against the original documents to avoid misinterpretation.

Based on the discussion, here is the summary of the comments regarding the Nozomio Labs archive:

**Technical Viability and Accuracy**
*   **LLMs vs. Traditional Search:** Users debated the best architecture for this type of archive. While some argued that LLMs often underperform compared to traditional NLP and regex for hard data extraction, the creator (*jllytsr*) clarified the site uses a hybrid approach: traditional regex for exact matches (names/dates), vector search for semantic queries, and an LLM only as an orchestration layer to ground answers.
*   **Hallucinations:** Skepticism remains high regarding "grounding." Users questioned whether any model is truly constrained enough to distinct source documents without "hallucinating" details, with one user calling the potential for errors "spicy hallucinations."
*   **Feature Requests:** There was interest in making search results and conversation threads shareable via unique URLs (UUIDs) to help researchers cite their findings.

**Censorship and Model Bias**
*   **Guardrails:** A significant portion of the conversation focused on whether an AI model would refuse to answer controversial questions due to safety training (censorship). Users compared "uncensored" local models against commercial models (like OpenAI’s), theorizing that commercial models might refuse to answer queries about specific political figures or provocative keywords.
*   **Testing Limits:** Users discussed testing models with questions about election eligibility and specific politicians to gauge bias, though results were mixed regarding which models (Grok, Claude, GPT) are actually structurally "uncensored" versus simply prompted differently.

**Content and Key Figures**
*   **High-Profile Mentions:** Commenters discussed findings related to specific figures, including Bill Gates (emails, social interactions) and Donald Trump (flight logs, quotes from 2002 regarding their friendship).
*   **Cynicism regarding Justice:** Despite the accessibility of the files, the prevailing sentiment was cynical. Users expressed doubt that "analyzing files" would lead to consequences for powerful figures like Clinton, Trump, or Gates, suggesting the wealthy are insulated from prosecution regardless of what the archive reveals.

**Ethical Concerns regarding AI Imagery**
*   **Unblurring Images:** A distinct sub-thread emerged from a user wondering if AI could "un-blur" redacted faces of minors in the files. This sparked a debate on the ethics of AI-generated inputs; users argued that generating realistic images of children—even to reconstruct reality—is dangerous, could normalize abuse material, and creates psychological disconnects regarding consent and harm.

### Show HN: Nori CLI, a better interface for Claude Code (no flicker)

#### [Submission URL](https://github.com/tilework-tech/nori-cli) | 35 points | by [csressel](https://news.ycombinator.com/user?id=csressel) | [8 comments](https://news.ycombinator.com/item?id=46616562)

Nori CLI: one terminal UI for coding with Claude, Gemini, and OpenAI. Nori is a Rust-based, local CLI that unifies multiple AI coding agents behind a single, fast TUI. You can switch providers on the fly with /agent, chat about your codebase, and run commands directly from the interface. It piggybacks on existing logins from each vendor’s CLI (Claude Code, Gemini CLI, or OpenAI/Codex), so setup is quick if you’ve used those before.

Why it’s interesting:
- Multi-provider workflow without context-switching tools
- Snappy terminal experience (Ratatui, incremental renders, double-buffered scrollback), built in Rust
- Open-source (Apache-2.0), actively releasing (v0.2.7 as of Jan 14, 2026)

Roadmap highlights:
- Sandboxed command execution
- Model Context Protocol (MCP) integration for external tools (aligned with Zed’s agent protocol)
- Session persistence (resume), multi-agent orchestration

How to try:
- npm install -g nori-ai-cli, then run nori
- Authenticate per provider:
  - Claude: npx @anthropic-ai/claude-code then /login in Nori
  - Gemini: npx @google/gemini-cli then /auth
  - OpenAI: switch /agent to Codex, then /login in Nori

Repo: github.com/tilework-tech/nori-cli (Rust-heavy codebase; credits OpenAI Codex CLI as a foundation) Note: “local” here refers to the CLI/TUI; actual model inference still happens via the cloud providers you authenticate.

**Discussion Summary:**

In the comments, the author (`csrssl`) engages with users regarding feature requests and comparisons to existing tools:

*   **Feature Wishlist:** Users prioritized "must-have" features such as conversation history, the ability to rewind and restore code states, and a "dangerous mode" to skip permission prompts. Future support for MCP (Model Context Protocol) servers was also requested.
*   **Similar Tools:** Commenters pointed to "Toad," a similar Python-based TUI. The author expressed admiration for the Python terminal ecosystem (specifically Textual) but noted Nori was built to align with the performance and style of Rust tools like `fzf` and `helix`.
*   **Differentiation:** When asked about the downsides of using Nori versus using `Claude Code` directly, the author admitted that maintaining 1:1 feature parity with vendor CLIs is the main challenge. However, they highlighted that Nori aims to provide superior session management features, such as session resuming, rewinds, and branching.

### LLMs are a 400-year-long confidence trick

#### [Submission URL](https://tomrenner.com/posts/400-year-confidence-trick/) | 110 points | by [Growtika](https://news.ycombinator.com/user?id=Growtika) | [173 comments](https://news.ycombinator.com/item?id=46613997)

Tom Renner: LLMs as a classic confidence game

- Core thesis: The AI industry is running a playbook straight out of a con—build trust, exploit emotions, then create urgency—pushing people to over‑rely on LLMs.
- Build trust: For 400 years, calculators and machines have trained us to see machine outputs as the gold standard. That societal habit of trusting “the machine” is now being leveraged for LLMs, despite their very different reliability profile.
- Exploit emotions—fear: Vendors hype existential risk and job-loss narratives to make non-adoption feel dangerous, while still selling the tools they claim to fear.
- Exploit emotions—sympathy: RLHF bakes in relentless positivity and flattery, encouraging parasocial bonds. Renner cites reports of harmful effects and even claims OpenAI over-tuned “positivity” and rolled it back in 2025—evidence, he argues, that this friendliness is a manipulation layer, not intelligence.
- Create urgency: With constant promises that “next year” the tech will replace swaths of jobs, users and orgs are pushed into hasty decisions that suspend skepticism and due diligence.

Why it matters: We’re primed to trust machines because calculators earned it; LLMs haven’t. Treat their outputs as untrusted by default, resist fear/flattery pressure, and slow down procurement and workflow changes until validation and safeguards catch up.

The discussion around Renner’s thesis pits developer anecdotes of "10x productivity" against skepticism regarding the definition of productivity and the gap between marketing claims and technical reality.

**The "Con" is Marketing, Not Utility**
While user **krystf** pushed back strongly—claiming legitimate productivity gains, describing LLMs as a "Gutenberg press" moment, and citing 10x gains in reading/writing enterprise Django code—others argued this misses the article's point. **vanderZwan** and **rms** countered that the "con" isn’t that the technology doesn't work, but that it is marketed deceptively (e.g., selling "intelligence" and "existential risk" rather than a probability engine). **ltxr** offered the analogy of selling regular apples as a magical cancer cure: the product (apples/LLMs) is fine, but the sales pitch is a fraud.

**Productivity: Perception vs. Reality**
A significant debate emerged regarding actual vs. perceived efficiency:
*   **The "Power Tool" Defense:** **ntndd** argued LLMs are "power tools," not autonomous robots; they are highly effective in the hands of a skilled carpenter but deceptively marketed as able to build a house on their own.
*   **The "Productivity Paradox":** **stx** provided a counter-anecdote to the hype, citing internal company data where overall developer productivity *plummeted* using Claude Code, despite developers reporting they *felt* more productive.
*   **The Cause of the Drop:** **bndrm** and **JacoboJacobi** theorized that while generating code feels fast (twiddling switches), the resulting "uncanny valley" of code requires more effort to review, debug, and understand than code written by a human.

**Scope of Problem Solving**
Users debted *where* the utility lies. **kyl** argued LLMs only truly help with "solved problems" (like standard Django patterns) rather than novel engineering. However, **dnlbln** and **jason_oster** disagreed, arguing that LLMs can handle novelty and obscure tasks (like writing compilers for custom DSLs) by synthesizing context rather than just regurgitating training data, provided the user supplies sufficient context or "specs."

### Show HN: Harmony – AI notetaker for Discord

#### [Submission URL](https://harmonynotetaker.ai/) | 28 points | by [SeanDorje](https://news.ycombinator.com/user?id=SeanDorje) | [8 comments](https://news.ycombinator.com/item?id=46622139)

Harmony: AI notetaker for Discord calls

What it is: A Discord bot that joins voice channels to record, transcribe, and auto-summarize conversations. It targets teams and communities that run meetings on Discord and miss Zoom-style transcription.

Key features:
- One-command recording (/record, /stop) with automated joining
- AI transcripts and summaries, speaker analytics, multi-channel support
- “AskHarmony” chat to query meeting content
- Smart search over past calls; 57+ languages

Setup: Invite the bot → start/stop recording → review summaries, analytics, and transcripts.

Claims and social proof: “Trusted by 6,000 users,” with testimonials from team leads and gaming community admins; several testimonials repeat and include some generic productivity quotes.

Why it matters: Discord lacks native meeting notes/transcripts; this fills a gap for standups, officer meetings, and distributed teams living in Discord.

What HN will ask:
- Consent and compliance: how it handles recording notices and Discord TOS
- Privacy/security: data retention, encryption, access controls, SOC2/GDPR
- Accuracy: diarization, multilingual performance, latency to summaries
- Pricing/limits: free tier caps, storage, export options and webhooks
- Admin controls: per-channel permissions, auto-join rules, and deletion

Getting started: “Start for free” and “Book demo”; invite the bot and try it in a test channel.

Here is a summary of the discussion on Hacker News:

**Discord vs. Enterprise Viability**
Much of the discussion focuses on whether Discord is a suitable platform for professional "serious" work. Several users argue that established companies prioritize privacy, security, and access control, typically opting for Slack or self-hosted solutions over Discord. Consequently, commenters question if Harmony plans to expand support to Zoom, Google Meet, or Slack to capture the enterprise market.

**Open Source Alternatives**
One user suggests a DIY approach using open-source tools—specifically connecting audio streams to the Whisper API for transcription and then feeding that into an LLM. They argue this method offers better control over security/data privacy and is more cost-effective for bulk processing than a dedicated SaaS bot.

**Demographics and Use Cases**
There is a debate regarding *who* actually uses Discord for work. While some dismiss it as a tool for "Gen Z" and early-stage startups that can't afford Slack, others defend its utility. Specifically, users in the gaming industry note that Discord’s screen-sharing controls (balancing resolution vs. frame rate) make it superior to Slack for pair programming, even if their organizations still use Slack for text communication.

**Platform Uncertainty**
Rumors of a near-term Discord IPO have sparked anxiety, with some users expressing disappointment in the current state of self-hosted alternatives and looking to migrate away from the platform entirely.

### Tesla moving Full Self-Driving to a monthly subscription

#### [Submission URL](https://www.cnbc.com/2026/01/14/musk-tesla-full-self-driving-subscription-fsd.html) | 53 points | by [leopoldj](https://news.ycombinator.com/user?id=leopoldj) | [35 comments](https://news.ycombinator.com/item?id=46618435)

Tesla makes Full Self-Driving subscription-only, ending the $8,000 one-time purchase

- What happened: Elon Musk said Tesla will stop selling Full Self-Driving (Supervised) as a flat-fee add-on after Feb 14; it will only be offered as a monthly subscription (starts at $99/month). Tesla shares closed down 1.8% on the news.
- Context: FSD still requires an attentive human driver and does not make Teslas autonomous. CFO Vaibhav Taneja said in October that paid FSD users were about 12% of the fleet.
- Strategy shift: Moving to subscriptions emphasizes recurring revenue and lowers the upfront barrier, but may irk owners who paid thousands upfront. It also pushes usage data collection as Tesla chases autonomy.
- Competitive/regs: Tesla is far behind Waymo, which hit 450,000 weekly paid rides in December and operates in multiple cities (SF Bay Area, Austin, Phoenix, Atlanta, LA) with international expansion targeted for 2026. Tesla’s limited “robotaxi” in Austin still uses human safety supervisors; in California, Tesla lacks driverless testing/operating permits and recently faced a DMV false-advertising finding and a related class action on appeal.
- Company backdrop: Tesla reported Q4 deliveries of 418,227 (down ~16% YoY) and production down 5.5% YoY, marking a second straight annual decline. Earnings are due Jan 28.

Why it matters
- Signals Tesla’s bet that autonomy is a software subscription business.
- Could boost near-term software margins and usage, but highlights the gap with fully driverless services and ongoing regulatory hurdles.

**Hacker News Discussion Summary**

**Skepticism regarding naming and delivery**
Much of the discussion centers on the perceived dishonesty of the name "Full Self-Driving" when paired with the modifier "(Supervised)." Users mock this contradiction with analogies like selling "Fully Edible Cakes (Poisonous)" and label it as deceptive marketing. Commenters also scoff at Elon Musk's history of missed timelines, joking about "LLM calendars" and noting that promises made in 2017 regarding autonomous capabilities still haven't materialized, leading to accusations that the pivot to subscriptions is simply a way to monetize "vaporware" now that the pool of buyers willing to pay a lump sum has dried up.

**Economics and ROI**
Users crunch the numbers on the switch from flat-fee to subscription. One commenter notes that at the peak price of $15,000, the breakeven point would have been 12.5 years of subscriptions, implying the flat fee was a poor financial decision for consumers. The consensus suggests Tesla "ran out of suckers" willing to pay the high upfront cost, necessitating a pivot to a lower barrier of entry to maintain revenue flow.

**Market commoditization and competition**
A debate emerged regarding the long-term viability of charging for autonomy. Some speculate that competitors like BYD will eventually make self-driving features "table stakes" (standard inclusions), forcing Tesla to abandon the subscription model. Others argue that car manufacturers will attempt to maintain "vendor lock-in," acting like oligopolies to keep subscription revenue flowing even as hardware prices drop (likened to Smart TVs selling at a loss to monetize user data).

**Grandfathering and liability**
There are open questions about how early adopters who paid the full price will be treated long-term, with sarcastic suggestions that they might eventually receive "ad-supported" versions. On the operational side, users highlighted the legal distinction between Tesla’s system (which places liability on the human driver) versus Waymo (where the system remains liable). Dark humor also surfaced regarding the subscription model, with users imagining the car demanding payment mid-drive or careening off a cliff because the monthly plan expired.

### Microsoft keeps reinstalling Copilot, so I found a way to rip it out for good

#### [Submission URL](https://www.howtogeek.com/how-to-rip-out-copilot-from-windows-11/) | 37 points | by [rolph](https://news.ycombinator.com/user?id=rolph) | [16 comments](https://news.ycombinator.com/item?id=46622526)

Headline: How to truly remove Windows Copilot (and keep it from coming back)

The piece walks through what it actually takes to purge Copilot from Windows 11—and why a simple uninstall isn’t enough.

Key points:
- Uninstall the app: Copilot won’t appear in Control Panel, but you can remove it via Settings > Apps > Installed Apps. For multi-user systems, PowerShell can remove it for all users and from provisioned packages:
  - Remove for all users: Get-AppxPackage -AllUsers -Name "Microsoft.Windows.Copilot" | Remove-AppxPackage -AllUsers
  - Remove provisioned package: Get-AppxProvisionedPackage -Online | Where-Object { $_.DisplayName -like "*Microsoft.Copilot*" } | Remove-AppxProvisionedPackage -Online
- Kill leftovers and autostart: Disable Copilot in Task Manager’s Startup tab. UI remnants (buttons in Paint, Notepad, and Microsoft 365 apps) can linger even after uninstall.
- Edge integration: Turn off the Copilot/Sidebar button in Edge (Settings > Appearance > Copilot and Sidebar). This hides it but doesn’t fully remove the feature.
- Block reinstalls via Registry (backup first): 
  - HKLM\SOFTWARE\Policies\Microsoft\Windows\WindowsCopilot -> TurnOffWindowsCopilot (DWORD) = 1
  - HKCU\Software\Microsoft\Windows\CurrentVersion\ContentDeliveryManager -> SilentInstalledAppsEnabled (DWORD) = 0
- Expect persistence: Windows updates may try to reintroduce Copilot; the policy/registry changes help, but aren’t foolproof. Windows Search may still show shortcuts.
- Nuclear option: Third-party scripts on GitHub can strip Copilot and AI components more aggressively—use only if you understand and trust the code, and create a restore point first.

Bottom line: Removing Copilot fully is possible but takes multiple steps—uninstalling, disabling autostart and Edge hooks, and setting policies to prevent it from quietly reinstalling.

**Discussion Summary:**

The conversation focused on the erosion of user control within the Windows ecosystem, with commenters expressing frustration that "uninstalling" features is often temporary due to Microsoft’s aggressive update cycles and "KPI-driven" darker patterns.

Key themes included:

*   **Loss of Agency:** Users lamented that Windows is increasingly behaving like a "walled garden" (comparable to iOS), where user preferences are routinely overwritten to serve Microsoft's interests. There is skepticism that any manual removal will last beyond the next system update.
*   **The Linux Migration:** A significant portion of the discussion advocated for switching to Linux to escape "Microsoft spyware." While some users hesitated due to gaming and driver concerns, others highlighted Valve’s Proton and user-friendly distros like Pop!_OS as viable bridges for modern gaming.
*   **Technical Blocking Limits:** Participants discussed network-level blocks, noting that modifying the `hosts` file is ineffective because Windows often bypasses it for internal Microsoft domains. External DNS solutions (like Pi-hole) were suggested as better alternatives, though they risk breaking other OS functionalities.