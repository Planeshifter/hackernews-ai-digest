## AI Submissions for Sat Nov 08 2025 {{ 'date': '2025-11-08T17:12:27.233Z' }}

### Study identifies weaknesses in how AI systems are evaluated

#### [Submission URL](https://www.oii.ox.ac.uk/news-events/study-identifies-weaknesses-in-how-ai-systems-are-evaluated/) | 385 points | by [pseudolus](https://news.ycombinator.com/user?id=pseudolus) | [181 comments](https://news.ycombinator.com/item?id=45856804)

Oxford-led review: most LLM benchmarks don’t measure what they claim

- What’s new: A 42‑researcher team led by Oxford Internet Institute reviewed 445 LLM benchmarks and found widespread issues with construct validity—the basic question of whether tests measure what they say they do. The paper, Measuring What Matters, is accepted for NeurIPS 2025.
- Key stats: Only 16% of studies used statistical methods when comparing models; ~50% tried to assess abstract traits (e.g., “reasoning,” “harmlessness”) without clear definitions.
- Why it matters: Benchmarks drive research priorities, leaderboards, and are referenced by regulators (e.g., EU AI Act). Weak tests risk overstating progress and safety.
- Examples of problems:
  - Formatting confounds: models penalized for output style rather than task competence.
  - Brittleness: small wording/number changes flip correct answers to failures.
  - Overclaims: exam multiple-choice scores miscast as “doctor-level” ability.
- Recommendations: Define constructs precisely and isolate them from confounders; build representative, real‑world test sets; report uncertainty and use proper statistical comparisons; perform error analysis; justify why a benchmark is valid for its intended use.
- Tooling: A Construct Validity Checklist is available for researchers, developers, and regulators: https://oxrml.com/measuring-what-matters/
- Who’s involved: Contributors span OII, EPFL, Stanford, TUM, UC Berkeley, UK AI Security Institute, Weizenbaum Institute, and Yale. Paper to appear at NeurIPS 2025 (San Diego, Dec 2–7).

The Hacker News discussion on the Oxford-led review of LLM benchmarks reflects widespread skepticism about current evaluation practices, with several recurring themes:

1. **Statistical Rigor Concerns**:  
   Users highlighted the lack of proper statistical methods in benchmarks (e.g., only 16% of studies use statistical comparisons). Many criticized the reliance on "bullshit" metrics like p-values without context, emphasizing the need for uncertainty reporting and causal inference techniques. Some noted that even academic programs often fail to teach applied statistics effectively.

2. **Economic Incentives & Corruption**:  
   Commenters argued that hyperscale platforms and corporations prioritize marketable benchmark scores over genuine progress, leading to "contaminated" or gamed results. Examples included GPT-4o’s sycophantic outputs and companies using secret internal benchmarks to inflate perceptions of performance.

3. **Brittleness & Overclaiming**:  
   Participants pointed out that minor changes in input phrasing or numbers can cause models to fail catastrophically, exposing a lack of true understanding. Overclaims like equating multiple-choice test scores to "doctor-level" competence were widely mocked as misleading.

4. **Transparency & Real-World Validity**:  
   Many criticized the opacity of benchmarks, with companies cherryicking favorable metrics. Some argued that benchmarks rarely predict real-world performance, advocating for user-driven validation (e.g., analyzing customer query patterns) instead of abstract lab tests.

5. **Proposed Solutions**:  
   Suggestions included:  
   - **Counterfactual benchmarking** to isolate specific capabilities.  
   - **Terminal Bench 2.0**-style stress tests with human-crafted challenges.  
   - Prioritizing metrics tied to user retention and engagement over artificial lab scores.  
   - Integrating privacy-friendly, large-scale user feedback into evaluations.

6. **Cynicism vs. Pragmatism**:  
   While some dismissed benchmarks entirely as "Wild West" marketing tools, others acknowledged their necessity despite flaws. A recurring sentiment was that benchmarks must evolve alongside models, with stricter statistical rigor and alignment with practical use cases.

Overall, the discussion underscores a crisis of trust in LLM evaluation, driven by methodological shortcomings and corporate incentives, but also highlights emerging efforts to create more robust, real-world-focused assessment frameworks.

### Firefox Forcing LLM Features

#### [Submission URL](https://equk.co.uk/2025/10/28/firefox-forcing-llm-features/) | 114 points | by [birdculture](https://news.ycombinator.com/user?id=birdculture) | [111 comments](https://news.ycombinator.com/item?id=45858959)

A Firefox user argues Mozilla has been rolling out AI/LLM features by default without a clear GUI off switch, leading to privacy discomfort and reports of high CPU/RAM usage. They also point to confusing ToS wording around user data that, in their view, dovetails uncomfortably with the AI push.

Key points:
- No obvious toggle: Even with some ml prefs disabled, the user still saw “Ask an AI chatbot (z)” in the context menu.
- Hidden behind about:config: The post shares a larger blocklist of prefs to kill AI-related features, e.g. browser.ml.enable, browser.ml.chat.enabled, browser.ml.pageAssist.enabled, browser.ml.linkPreview.enabled, browser.tabs.groups.smart.enabled, and extensions.ml.enabled.
- Automation: They provide scripts and a default prefs.js on GitHub to apply these settings across profiles.
- Alternatives: Suggests non-technical users may prefer Firefox forks that strip AI features.
- Market context: Cites “September 2025” browser share figures putting Firefox around 2.17%, framing the concern that shipping AI by default could further alienate users.

Takeaway: If you’re seeing unwanted AI features in Firefox, you’ll likely need to visit about:config and disable multiple ml/* and related prefs, or use a prebuilt prefs.js/script. The post’s broader critique is about consent, performance, and trust—expect a lively debate on how Mozilla should ship (and let users opt out of) AI.

**Summary of Discussion:**

The Hacker News discussion reflects polarized views on Mozilla’s integration of AI/LLM features in Firefox, centering on **user autonomy, performance, and trust**:

1. **Criticism of Default Integration**:
   - Many users express frustration over AI features (e.g., context-menu chatbots, tab grouping) being enabled by default without clear opt-out options. Critics argue this undermines Firefox’s reputation as a privacy-focused browser.
   - Concerns about **resource usage** (CPU/RAM) are raised, particularly for low-end devices. Some claim disabling AI via `about:config` is cumbersome for non-technical users.

2. **Debate Over AI Utility**:
   - **Pro-AI**: Supporters highlight practical uses like translation (Mozilla’s Transformer-based Marian NMT) and summarization, arguing these enhance accessibility. 
   - **Anti-AI**: Opponents dismiss AI as bloat, questioning its value in core browsing. Some view it as a market-driven gimmick ("AI-powered browsers") that complicates the UI.

3. **Technical Nuances**:
   - Discussions clarify distinctions between LLMs and other ML models (e.g., translation tools). Critics contest Mozilla’s labeling of features as "AI," arguing it conflates technical definitions.
   - Workarounds like prefs.js scripts, forks (Waterfox), or alternative browsers (Ladybird, Servo) are suggested to avoid AI entirely.

4. **Trust and Mozilla’s Direction**:
   - Long-term users lament Mozilla’s shift toward "forced" features, contrasting with its earlier ethos. Pocket integration and telemetry are cited as past red flags.
   - Defenders argue local AI (e.g., on-device translation) aligns with privacy goals, but skeptics fear data-handling ambiguities in ToS.

5. **Market Realities**:
   - Some acknowledge Mozilla’s need to compete with Chromium-based browsers adopting AI, though critics see this as pandering to trends rather than user needs.

**Takeaway**: The debate underscores tension between innovation and user consent. While AI features have defenders, their opt-out complexity and perceived intrusiveness fuel distrust. Mozilla faces pressure to balance modern tooling with its privacy-centric identity.

### Cerebras Code now supports GLM 4.6 at 1000 tokens/sec

#### [Submission URL](https://www.cerebras.ai/code) | 181 points | by [nathabonfim59](https://news.ycombinator.com/user?id=nathabonfim59) | [123 comments](https://news.ycombinator.com/item?id=45852751)

Cerebras is pitching a faster coding assistant: its Code Pro service now runs GLM‑4.6 and claims 1,000+ tokens/sec generation. The company touts the model as a top open coding model—#1 for tool calling on the Berkeley Function Calling Leaderboard and on par with Sonnet 4.5 for web-dev tasks.

Highlights
- Speed and model: GLM‑4.6, marketed for low-latency coding workflows with high throughput.
- Integrations: Works via API key with AI-friendly editors/agents (Cline, RooCode, OpenCode, Crush, etc.), so you can “bring your own editor.”
- Plans: Free tier (limited). Pro at $50 with up to 24M tokens/day (pitched as 3–4 hours of continuous coding). Max at $200 with up to 120M tokens/day for heavier IDE and multi‑agent use.
- Context: The update lands alongside Cerebras’ $1.1B Series G at an $8.1B valuation.

Why it matters
- Emphasis on raw throughput and tool-calling strength targets agentic and refactoring-heavy workflows.
- Pricing by large daily token quotas could appeal to power users who hit rate limits elsewhere.

Caveats
- Benchmarks and speed are vendor-reported; context length, per-request caps, and SLAs aren’t detailed on the page.

Here's a concise summary of the Hacker News discussion about Cerebras' Code Pro and AI coding tools:

### Key Discussion Points
1. **Model Comparisons**
   - GLM-4.6 praised for speed (1,000 tokens/sec) and web development parity with Sonnet 4.5
   - Limitations: Lacks web search/image recognition features that Claude/Gemini offer
   - Mixed opinions on code quality: Some find Sonnet more predictable, others prefer GLM for simplicity

2. **Pricing & Workflows**
   - $50/month Pro plan seen as competitive for power users hitting Claude/Gemini rate limits
   - Embedded developers report using 300+ daily prompts, value Zed subscriptions
   - Concerns about per-request caps and SLA transparency

3. **Embedded Development Experiences**
   - Effective for boilerplate code (Rust/TypeScript CRUD APIs)
   - Struggles with low-level tasks: UEFI bindings, DMA configurations, ESP32 firmware
   - Testing challenges: LLMs can't execute hardware-specific code, requiring manual verification

4. **Testing Practices**
   - Emphasis on static typing (Rust/TypeScript) to catch errors early
   - Users report 25-50% productivity gains but stress need for:
     - Comprehensive test harnesses
     - Documentation maintenance
     - Careful prompt engineering for complex systems

5. **Terminology Debate**
   - Skepticism about "vibing coding" (blind code generation without understanding)
   - Defense of LLM-assisted development as iterative process requiring review

6. **Skepticism**
   - Concerns about LLMs' ability to handle novel system design combinations
   - Historical comparison to Bell Labs' rigorous engineering methods
   - Observations that LLMs excel at common patterns but struggle with true innovation

The discussion reflects cautious optimism about coding assistants accelerating workflows, tempered by recognition of current limitations in reliability and system-level understanding.

### GPT-OSS 120B Runs at 3000 tokens/sec on Cerebras

#### [Submission URL](https://www.cerebras.ai/blog/openai-gpt-oss-120b-runs-fastest-on-cerebras) | 45 points | by [samspenc](https://news.ycombinator.com/user?id=samspenc) | [28 comments](https://news.ycombinator.com/item?id=45853849)

OpenAI’s first open‑weight reasoning model, GPT OSS 120B, is now live on Cerebras — and the company is leaning hard into speed and cost claims.

Highlights
- Model: 120B-parameter MoE, open weights under Apache 2.0; near-parity with o4‑mini on core reasoning benchmarks, strong on chain-of-thought tasks across coding, math, and health.
- Speed: Measured up to ~3,045 tokens/sec on OpenRouter; time to first token ~280 ms; Cerebras claims ~15–16x faster than leading/median GPU clouds and single‑second latency.
- Price/perf: Priced at $0.25 per million input tokens and $0.69 per million output tokens with 131K context; Cerebras touts an 8.4x price‑performance advantage (tokens/sec per dollar).
- Accuracy: Artificial Analysis reports Cerebras as equal-first on AIME 2025 accuracy among OSS 120B providers.
- Availability: Cerebras Cloud, plus Hugging Face, OpenRouter, and Vercel; can also run on-prem on the Wafer Scale Engine.

Why it matters
- Reasoning models are often too slow for production agents and coding tools; if these latency and throughput numbers hold up, OSS 120B on Cerebras could make open-weight reasoning practical at interactive speeds.

Caveat
- Most figures are vendor/third‑party benchmarks; real‑world performance can vary with prompts, numerics, and quantization settings.

The Hacker News discussion about Cerebras’s GPT OSS 120B model highlights mixed reactions, blending enthusiasm for its technical performance with skepticism about business claims and usability:

### **Positive Reactions**
- **Speed/Cost Praise**: Users like `ptsrgnt` and `snpzd` applaud the model’s speed (~3,045 tokens/sec) and affordability ($0.25/M input tokens), calling it a "lightning-fast" alternative to GPU-based providers like Groq and OpenRouter.
- **Technical Potential**: Some see the model as a practical breakthrough for interactive coding/math tasks if latency claims hold up.

### **Criticisms & Skepticism**
- **Business Viability**: Users debate Cerebras’s financial sustainability, citing its $1.1B Series funding and $8.1B valuation as potential hype. `rajman187` notes recent $60M+ losses and a stalled IPO, questioning long-term viability.
- **Hardware Economics**: Concerns arise about SRAM costs and scalability. `jshrd` argues Cerebras/Groq may struggle with expensive hardware bottlenecks despite speed gains.
- **User Experience**: Frustration with sign-up processes (`freak42` compares it to "dark patterns") and skepticism about real-world performance versus benchmarks. `anonym29` mocks claims as akin to a "Ferrari dealership offering test drives for a million dollars."

### **Other Notes**
- **Geopolitical Angle**: `ptsrgnt` defends UAE investments in Cerebras as "smart money" aligned with long-term AI strategy.
- **Comparisons**: Some users contrast Cerebras with Nvidia’s dominance, questioning if specialized hardware can disrupt GPU ecosystems.

### **Summary**
While the model’s speed and cost metrics excite developers, doubts linger about Cerebras’s business model, hardware economics, and real-world usability. The thread reflects cautious optimism tempered by scrutiny of vendor claims and financial transparency.

### GPT-5-Codex-Mini – A more compact and cost-efficient version of GPT-5-Codex

#### [Submission URL](https://github.com/openai/codex/releases/tag/rust-v0.56.0) | 50 points | by [wahnfrieden](https://news.ycombinator.com/user?id=wahnfrieden) | [49 comments](https://news.ycombinator.com/item?id=45861329)

OpenAI’s Codex 0.56.0 lands with a new, cheaper code model and a sweep of v2 APIs and stability fixes. The headline is GPT-5-Codex-Mini, a compact, cost-efficient model aimed at faster, lower-cost coding tasks. On the platform side, the app-server gains v2 Thread and Turn APIs plus a revamped v2 login flow (start/completed/cancel), laying groundwork for cleaner session management and notifications. The TypeScript SDK adds a modelReasoningEffort option, and the runtime sees reliability boosts: better token refresh (fixing “Re-connecting”), nix/build fixes, CI flake reductions, and sandbox tweaks (including Windows warnings and broader cert ops when networking is enabled). UX touches include TUI refinements and a “model nudge” for queries, while contributor docs clarify that gpt-5-codex shouldn’t amend commits unless explicitly asked. Overall: cheaper model, API modernization, and a lot of polish aimed at smoother dev and user workflows.

**Hacker News Discussion Summary:**  

The release of OpenAI's GPT-5-Codex-Mini sparked mixed reactions. Supporters praised its cost-efficiency and coding improvements, with users like *RestartKernel* noting its "impressive" advancements over previous models. However, skeptics like *hnidiots3* dismissed Codex as "not a good model," though others countered that practical experience (e.g., *k4rli* using Sonnet45) showed solid results.  

**Technical Debates:**  
- A detailed exchange between *jswn* and *nwgz* highlighted challenges with TypeScript generics and runtime behavior, illustrating frustrations with GPT-5-Codex’s handling of complex code patterns (e.g., `List<User>` vs. `List_1<User>`).  
- *lstms* shared struggles integrating GPT-5-Codex into .NET projects, citing issues with context length and architectural redesigns, though acknowledging partial success in test cases.  

**Business & Pricing Concerns:**  
- *bgwltr* criticized AI providers for unsustainable pricing models, citing Grok-4’s errors and Claude Code’s high costs. Others countered that Anthropic’s Claude Code reportedly generates $1B annually, with developers paying $20–$200/month for premium plans.  
- *crzylggr* noted anecdotal "10x" usage costs beyond subscriptions, sparking debates about long-term viability amid competition from cheaper open-weight models (e.g., DeepSeek, Kimi).  

**Humor & Meta-Comments:**  
- Users joked about AI-generated product names ("Groq") and mocked verbose API documentation.  
- *smnw* humorously referenced "EF Hutton" ads to highlight the hype around AI announcements.  

**Takeaway:** While GPT-5-Codex-Mini’s efficiency and API updates impressed many, debates persist over its practicality for complex tasks, cost sustainability, and competition in the rapidly evolving AI landscape.

### The AI Ick

#### [Submission URL](https://stackoverflow.blog/2025/11/05/the-ai-ick/) | 29 points | by [Wowfunhappy](https://news.ycombinator.com/user?id=Wowfunhappy) | [3 comments](https://news.ycombinator.com/item?id=45859566)

Stack Overflow blog essay: When your own writing gets mistaken for “AI slop”
- A Stack Overflow writer recounts a colleague assuming their em-dash-heavy, neatly structured draft was ChatGPT output—triggering a broader look at why AI text often feels hollow.
- Cites Wikipedia’s “field guide” to AI tells (overused em dashes, zhuzhed headings, rule of three, weasel words, superficial analysis), with the ironic caveat that LLMs learned many of these habits from human professional/academic prose.
- Core argument: AI outputs are statistically plausible but uncomprehending—“just product, no struggle”—the “stochastic parrot” problem that leaves readers sensing a lack of intent, friction, and insight.
- Visceral reaction parallels AI art’s uncanny valley: meme-y “count the fingers/teeth” vibes and a fear of something essential being borrowed or stolen.
- Takeaway: Stylistic tells are unreliable; authenticity comes from substance, specificity, and human context—so don’t conflate house style (yes, including em dashes) with machine-made writing.

The Hacker News discussion reflects skepticism and discomfort with AI-generated content:  
1. **Skepticism Toward Normalization**: One user questions whether the growing prevalence of AI-generated content risks becoming normalized, despite its flaws, as technology improves revision and generation capabilities.  
2. **"Stochastic Parrot" Critique**: Another commenter invokes the "stochastic parrot" concept (mimicry without comprehension), expressing disappointment that AI-generated content is being uncritically adopted. They liken it to a "digested battery" of human writing, lacking authenticity.  
3. **Resistance to AI in Creativity**: A user reacts viscerally ("Yuck") to a friend’s request for an AI-generated character, dismissing it as inorganic and undesirable compared to human creativity (referencing *Tilly Norwood TV* as a "hard pass").  
4. **Subcomment on Improvement**: A reply humorously critiques the idea of AI "improvement," suggesting it might result in shallow or performative outputs ("improv titled").  

The thread underscores concerns about AI’s hollow mimicry and resistance to its encroachment into creative domains, echoing the submission’s themes of authenticity and intent.

### Oddest ChatGPT leaks yet: Cringey chat logs found in Google Analytics tool

#### [Submission URL](https://arstechnica.com/tech-policy/2025/11/oddest-chatgpt-leaks-yet-cringey-chat-logs-found-in-google-analytics-tool/) | 71 points | by [vlod](https://news.ycombinator.com/user?id=vlod) | [20 comments](https://news.ycombinator.com/item?id=45853995)

ChatGPT prompts leaked into Google Search Console; OpenAI says a “glitch” is fixed, won’t confirm scraping

Developers began spotting long, deeply personal ChatGPT queries showing up in Google Search Console starting in September—sometimes 300+ characters—exposing user prompts about relationships and workplace plans. Quantable’s Jason Packer and SEO consultant Slobodan Manić traced the issue to a ChatGPT URL that included a hints=search parameter, which they say forced web lookups and, due to a bug, prepended that URL to user prompts. Because the queries appeared in GSC (which wouldn’t capture API traffic), they argue this is evidence OpenAI was directly scraping Google and sending actual user prompts along—leaking them to Google and to site owners who rank for tokenized bits of that URL. OpenAI declined to confirm scraping but said it had “resolved” a routing glitch affecting “a small number” of queries; Google declined comment. The scope is unclear: Packer reviewed ~200 strange queries on one site alone and worries any prompt that triggered Google Search over the past two months may have been exposed. He calls it “weirder, if not as serious” as past ChatGPT indexing leaks—and a reminder that prompts aren’t as private as many assume.

Tip: Site owners reported seeing leaked strings starting with “https://openai.com/index/chatgpt/” in GSC.

**Summary of Discussion:**

The Hacker News discussion highlights concerns over privacy and technical oversights after ChatGPT user prompts were leaked via Google Search Console (GSC). Key points include:

1. **Exposure of Sensitive Queries**: Users shared examples of leaked prompts (e.g., personal trip planning, relationship proposals) that appeared in GSC due to a bug involving a ChatGPT URL parameter (`hints=search`). These prompts were indexed by Google, raising alarms about unintended data exposure.

2. **Filter Failures**: Participants noted that GSC’s privacy filters, designed to suppress low-volume or sensitive queries, failed to block these leaks. This suggests OpenAI’s glitch bypassed standard protections, exposing raw user prompts to site owners via search analytics.

3. **Scraping Speculation**: Some commenters theorized OpenAI might be scraping Google Search results directly, using user prompts as search queries. While OpenAI fixed the "routing glitch," they did not clarify whether scraping occurred, fueling skepticism about transparency.

4. **Broader Privacy Implications**: The incident underscores risks of using AI tools for sensitive topics. Critics compared the leak to accidental code vulnerabilities, emphasizing that user prompts are less private than assumed. Others warned product managers to consider privacy implications when integrating AI with analytics tools.

5. **Community Reaction**: Reactions ranged from shock (“wtf”) to frustration over corporate accountability. Many called for stricter safeguards, while others lamented the normalization of privacy trade-offs in AI development.

Overall, the discussion reflects distrust in tech companies’ handling of user data and demands clearer safeguards for AI interactions.

