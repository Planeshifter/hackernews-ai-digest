## AI Submissions for Sun Feb 01 2026 {{ 'date': '2026-02-01T17:22:32.885Z' }}

### My iPhone 16 Pro Max produces garbage output when running MLX LLMs

#### [Submission URL](https://journal.rafaelcosta.me/my-thousand-dollar-iphone-cant-do-math/) | 389 points | by [rafaelcosta](https://news.ycombinator.com/user?id=rafaelcosta) | [179 comments](https://news.ycombinator.com/item?id=46849258)

A developer’s “simple” expense-tracker spiraled into a wild on-device AI bug hunt: MLX LLMs produced pure gibberish on his iPhone 16 Pro Max while the same code ran flawlessly on an iPhone 15 Pro and a MacBook Pro. After Apple Intelligence refused to download its on-device model, he fell back to bundling models with MLX; the 16 Pro Max pegged the CPU, never emitted a stop token, and generated noise like “Applied.....*_dAK[...].” Instrumenting Gemma’s forward pass with breakpoints showed tensor values on the 16 were off by about an order of magnitude compared to the 15—strongly suggesting a faulty Neural Engine (or ML-related hardware) on that particular unit rather than an OS or code issue. The episode doubles as a cautionary tale about Apple’s fragmented ML paths (Apple Intelligence vs MLX) and the pain of debugging on-device LLMs: if MLX outputs look deranged, try another device before rewriting your stack.

Here is the daily digest summary for this story and its discussion.

**Story: iPhone 16 Pro Max AI "Gibberish" Bug Tracked to Hardware-Software Mismatch**
A developer discovered a critical issue where MLX-based LLMs produced incomprehensible noise on the iPhone 16 Pro Max, despite the same code running perfectly on older devices like the iPhone 15 Pro. While the author initially suspected a faulty Neural Engine in their specific unit due to tensor values being off by an order of magnitude, the issue highlights the fragility of debugging on-device AI and the fragmentation between Apple’s native Intelligence features and open-source libraries like MLX.

**Discussion Summary**
The Hacker News discussion identified the actual root cause and debated the timing of the fix:

*   **Root Cause Identified:** Commenters debunked the author's theory of a defective specific unit. Users pointed to a specific pull request in the MLX repository (PR #3083) that fixed the issue just a day after the blog post. The problem was a software-hardware mismatch: the iPhone 16 Pro's SKU and verify-new "Neural Accelerator" support were misdetected by the library, causing "silently wrong results" in the GPU tensor cores.
*   **The "Blog Post Effect":** There was significant debate regarding the timing of the fix. Since the patch appeared one day after the blog post, some skeptics argued the publicity forced Apple's hand. Others countered that critical engineering bugs often have short turnaround times or that the fix was likely already in the QA pipeline before the post went viral.
*   **Prompt Philosophy:** A humorous sub-thread focused on the developer’s debug prompt: "What is moon plus sun?" Answers ranged from "eclipse" and the Chinese character for bright (明), to the catastrophic physics of a star colliding with a moon.
*   **Apple Silicon Complexity:** Technical users discussed the opacity of Apple’s naming conventions (Neural Engine vs. Neural Accelerator) and noted that MLX primarily runs on the GPU because the ANE (Apple Neural Engine) remains accessible only via closed-source APIs.
*   **Lack of Testing:** Several users lamented that this error suggests a lack of Continuous Integration (CI) testing on actual new hardware (iPhone 16 Pro) for these libraries.

### Two kinds of AI users are emerging

#### [Submission URL](https://martinalderson.com/posts/two-kinds-of-ai-users-are-emerging/) | 299 points | by [martinald](https://news.ycombinator.com/user?id=martinald) | [280 comments](https://news.ycombinator.com/item?id=46850588)

Power users vs. chatters: why enterprise AI is falling behind

The author sees a sharp split in AI adoption. On one side are “power users” (often non-technical) who run Claude Code, agents, and Python locally to supercharge real work—finance teams in particular are leapfrogging Excel’s limits. On the other are users stuck “just chatting” with tools like ChatGPT or, in enterprises, Microsoft 365 Copilot.

They argue Copilot’s UX and agent capabilities lag badly, yet it dominates corporate environments due to bundling and policy. Locked-down laptops, legacy systems without internal APIs, and siloed/outsourced engineering mean employees can’t run scripts, connect agents to core workflows, or get safe sandboxes—so leaders try Copilot, get weak results, and conclude AI underdelivers. Meanwhile, smaller, less-encumbered companies are “flying”: one example converted a sprawling 30‑sheet Excel model to Python almost in one shot with Claude Code, then layered on simulations, data pulls, and dashboards.

Why it matters
- The productivity gap is widening in favor of smaller, bottom‑up, tool‑agnostic teams.
- Enterprise risk isn’t just security—it’s stagnation and misjudging AI’s potential based on subpar tools.

What the author says to do
- Empower domain teams to build AI-assisted workflows organically, not via top‑down “digital transformation.”
- Provide safe sandboxes and read‑only data warehouses; expose internal APIs so agents have something to connect to.
- Don’t restrict staff to one bundled assistant; evaluate tools that actually execute code and integrate with systems.

**User Taxonomy and the Definition of "Thinking"**
The discussion focused heavily on categorizing AI users, with **PunchyHamster** proposing a taxonomy that framed the debate:
*   **Group 1 (The Executor):** Uses AI as a junior intern or boilerplate generator to speed up tasks while remaining aware of limitations.
*   **Group 2 (The Outsourcer):** Outsources the entire skillset and "thinking" process, interested only in the result rather than honing the craft.
*   **Group 3 (The Delusional):** Believes a talking chatbot can replace senior developers.

**Burnout and Corporate Cynicism**
A significant thread emerged regarding *why* engineers might choose to "outsource thinking" (Group 2). **svnzr** admitted to leaning on AI to deliver minimum viable work because their organization prioritizes speed over quality engineering. They described a "B2B SaaS trap" where Product Managers demand features immediately to sign contracts, ignoring long-term quality or user feedback. This resonated with others like **tlly**, who noted a "seismic shift" over the last 5–6 years where professional pride has eroded in the face of corporate dysfunction, pushing workers to use AI simply to keep up with the grind.

**The "Result vs. Ritual" Debate**
**3D30497420** offered a counterpoint to the negative perception of outsourcing skills. They described using Claude Code to build a custom German language learning app; while they "outsourced" the software development (which they don't care to learn), they did so to double down on learning German (which they do care about). This suggests that "outsourcing thinking" is valid if it clears the path for a different, preferred intellectual pursuit.

**Capabilities and Context**
Other commenters added nuance to the rigid grouping:
*   **GrinningFool** argued that users fluidly switch between groups depending on the task (e.g., caring about details for one project but just wanting the result for another).
*   **ck** and **mttmg** pointed out a missing group: users who treat AI as a "rubber duck" or virtual teammate to bounce ideas off of (ping-ponging).
*   **safety1st** distinguished between the "supply side" generations of tools, noting that while "Gen 1" (chatbots) is often just "vibes" and entertainment, "Gen 2" (RAG, agents, coding tools) offers the distinct capabilities—like scanning docs or deploying test suites—that separate power users from chatters.

### Towards a science of scaling agent systems: When and why agent systems work

#### [Submission URL](https://research.google/blog/towards-a-science-of-scaling-agent-systems-when-and-why-agent-systems-work/) | 97 points | by [gmays](https://news.ycombinator.com/user?id=gmays) | [34 comments](https://news.ycombinator.com/item?id=46847958)

Google Research: When multi-agent systems help—and when they hurt

What’s new: Google Research ran a controlled study of 180 agent configurations across five architectures and four benchmarks to derive quantitative “scaling principles” for AI agent systems. Core finding: more agents isn’t automatically better—gains depend on whether the task is parallelizable or inherently sequential.

Key results
- Scope: 5 architectures (Single-Agent, Independent, Centralized, Decentralized, Hybrid), 4 benchmarks (Finance-Agent, BrowseComp-Plus, PlanCraft, Workbench), 3 model families (OpenAI GPT, Google Gemini, Anthropic Claude).
- Parallelizable tasks: Multi-agent coordination—especially centralized—can deliver big wins. Example: Finance-Agent saw +80.9% over a single agent.
- Sequential tasks: Multi-agent setups often degrade performance due to coordination overhead and error cascades. Example: PlanCraft dropped by ~70%.
- Predictive selector: A model that picks the optimal architecture for 87% of unseen tasks.
- “Agentic” tasks are defined by sustained multi-step interaction, partial observability with iterative info gathering, and adaptive strategy refinement—conditions where architecture choices matter most.

Why it matters
- Challenges the popular “more agents = better” heuristic and prior collaboration-scaling claims.
- Introduces a task-architecture alignment principle: choose coordination structures that match task decomposability and communication needs.

Practical takeaways for builders
- Start single-agent; add agents only if the task cleanly decomposes into parallel subtasks.
- Prefer centralized coordination for parallel workloads; avoid heavy coordination for sequential pipelines.
- Watch communication rounds and memory sharing—overhead can erase gains.
- Consider using (or emulating) a selector to predict the best architecture before scaling out.

Paper and details: From Google Research (Jan 28, 2026); includes controlled comparisons across GPT, Gemini, and Claude families.

Here is a summary of the discussion on Hacker News:

**The Complexity Penalty in Sequential Tasks**
Commenters largely validated the paper's findings regarding sequential tasks, noting that multi-agent systems (MAS) often degrade performance due to "fragmented reasoning." One user argued that splitting a fixed cognitive budget (e.g., the paper’s ~4,800 token limit) across multiple agents wastes tokens on coordination "chatter," leaving insufficient capacity for the actual problem-solving. Others pointed out that in sequential pipelines, error rates compound aggressively—a 1% error rate in a single agent can render a deterministic multi-step flow unacceptable, and independent agents can amplify error rates by orders of magnitude (one user noted a 17x error amplification in independent setups versus 4x in centralized ones).

**The Case for Centralized Orchestration**
Builders discussed practical alternatives to "swarm" architectures, with a strong preference for Centralized or "Driver/Worker" patterns.
*   **The Orchestrator:** Several developers championed an architecture where a core "Orchestrator" or "Manager" agent breaks down tasks and delegates to specialized workers, rather than letting agents negotiate amongst themselves.
*   **Dynamic Selection:** Validating the paper’s "predictive selector" concept, users reported success using an agent specifically to plan and recommend the orchestration strategy *before* execution begins.
*   **Model Specialization:** Participants shared anecdotal "squad" compositions, such as using Google models for document extraction, Anthropic’s Claude for coding, and OpenAI models for management and orchestration.

**Is Multi-Agent Just a Band-Aid?**
A thread of skepticism questioned whether complex MAS architectures are simply artifacts of current limitations. Some argued that as context windows become larger and more reliable, the need to decompose tasks across multiple agents will diminish. It was suggested that developers currently use MAS to circumvent context limits or hallucination issues that a sufficiently advanced single model (ideally with symbolic recursion capabilities) could famously handle alone.

**Tooling and Protocols**
Discussion touched on emerging standards like the Model Context Protocol (MCP). Users debated whether stacking MCP servers is a viable form of coordination or if it introduces unnecessary overhead compared to simple CLI tool equivalents. The consensus leaned toward software design principles: align coordination structures with the task's natural decomposability—high cohesion and low coupling applied to AI agents.

### What I learned building an opinionated and minimal coding agent

#### [Submission URL](https://mariozechner.at/posts/2025-11-30-pi-coding-agent/) | 393 points | by [SatvikBeri](https://news.ycombinator.com/user?id=SatvikBeri) | [166 comments](https://news.ycombinator.com/item?id=46844822)

What I learned building an opinionated and minimal coding agent

Why he built it
- Frustrated by feature-bloated coding agents (e.g., Claude Code) that keep changing prompts/tools, break workflows, and flicker.
- Wants strict control over context, full transparency into what the model sees, a documented session format, and easy self-hosting—all hard with current harnesses.

What he built
- pi-ai: Unified LLM API with multi-provider support (Anthropic, OpenAI, Google, xAI, Groq, Cerebras, OpenRouter, and OpenAI-compatible), streaming, tool calling via TypeBox schemas, reasoning/“thinking” traces, cross-provider context handoff, token and cost tracking.
- pi-agent-core: Minimal agent loop handling tool execution, validation, and event streaming.
- pi-tui: Terminal UI toolkit with retained-mode design, differential rendering, and synchronized output to minimize flicker; includes editor components (autocomplete) and markdown rendering.
- pi-coding-agent: CLI wiring it all together with session management, custom tools, themes, and project context files.

Design philosophy
- If he doesn’t need it, it’s not built.
- Minimal system prompt, minimal toolset.
- YOLO by default (no permission prompts).
- No built-in to-dos, no “plan mode,” no MCP, no background bash, no sub-agents.

Multi-model reality and context control
- Embraces a multi-model workflow; supports seamless context handoff across providers/models.
- Heavy focus on “context engineering” to improve code quality and predictability.
- Clean, inspectable sessions to enable post-processing and alternative UIs.

API unification: the gritty bits
- Four APIs cover almost everything: OpenAI Completions, OpenAI Responses, Anthropic Messages, Google Generative AI.
- Normalizing quirks across providers and engines (Ollama, vLLM, llama.cpp, LM Studio), e.g.:
  - Different fields for token limits (max_tokens vs max_completion_tokens).
  - Missing system “developer” role on some providers.
  - Reasoning traces appear in different fields (reasoning_content vs reasoning).
  - Certain params not accepted by specific models (e.g., Grok and reasoning_effort).
- Backed by a cross-provider test suite for images, tools, reasoning, etc. to catch breakage as models change.

TUI details
- Two TUI modes considered; landed on retained-mode UI with differential rendering to avoid the “flicker” common in agent UIs.
- Synchronized output for near flicker-free updates.

Why it matters
- A counterpoint to ever-growing, opaque coding agents: deterministic, inspectable, self-host-friendly.
- Useful reference for anyone normalizing LLM APIs across providers and dealing with tool-calling/trace differences.
- Shows how far you can get with a small, purpose-built stack when you prioritize context control and transparency over features.

Based on the discussion, here is a summary of the comments:

**Security and Sandboxing**
A major portion of the discussion criticized the current state of security in coding agents, with several users describing manual permission approvals as "security theater."
*   Commenters noted that while manual approval prevents immediate disasters, the friction eventually leads users to approve actions blindly or use flags like `dangerously-skip-permissions` to make the tool usable.
*   The consensus was that true security requires OS-level sandboxing (VMs, containers, Unix jails, or macOS Seatbelt) rather than relying on the agent or CLI harness to police itself.
*   There is significant concern regarding agents having direct access to sensitive environments (like email or unrestricted file systems) without proper isolation.

**Context Engineering and Workflow**
Users responded positively to the author's focus on "context engineering," expressing frustration with the default linear conversational flow of most LLMs.
*   Several users discussed the need for non-linear history, such as "mind maps" or tree structures that allow a user to "save" a good context state, go on a "side quest" (debugging), and then return to the clean state without polluting the context window.
*   Users shared technical approaches to this, such as using specific Markdown files (`MIND_MAP.md`) or graph formats to maintain a persistent project memory separate from the chat logs.

**Economics: API vs. Subscription**
There was a debate regarding the financial sustainability of "bring your own key" (pay-per-token) agents versus subscription models like Claude Code.
*   Some users hope that API prices will decrease, making self-hosted agents significantly cheaper and more precise than bundled subscriptions.
*   Others argued that inference costs remain high and R&D requires massive funding, suggesting that subscription models will persist to subsidize the underlying compute, and that prices may not drop as quickly as hoped.

**Google API Quirks**
Commenters validated the author's complaints about Google's developer experience. Specific grievances included the lack of streaming support for tool calls and the inefficiency of Google's token counting methods (which require API calls or cause 100% CPU usage in AI Studio just to count tokens while typing).

**Naming**
There was lighthearted criticism of the name "pi-agent" for being un-Google-able and generic. Users expressed a preference for the author's original internal project name, "Shitty Coding Agent," proposing backronyms like SCA (Software Component Architecture) to make it acceptable.

### Show HN: Zuckerman – minimalist personal AI agent that self-edits its own code

#### [Submission URL](https://github.com/zuckermanai/zuckerman) | 70 points | by [ddaniel10](https://news.ycombinator.com/user?id=ddaniel10) | [49 comments](https://news.ycombinator.com/item?id=46846210)

Zuckerman: an ultra‑minimal, self‑modifying personal AI agent

What it is
- A TypeScript, AGPL‑3.0 project that starts as a tiny agent and then edits its own files (config, tools, prompts, even core logic) to add features on the fly. Changes hot‑reload instantly—no rebuilds or restarts.

Why it’s different
- Moves away from heavyweight agent stacks: the agent grows by rewriting itself, and improvements can be shared across agents via a contribution site. The README positions it as a simpler, more approachable alternative to complex, fast‑moving frameworks.

Notable features
- Self-editing runtime: modifies its own code/behavior and instantly reloads.
- Feature versioning and collaborative ecosystem for sharing capabilities.
- Multi‑channel I/O: Discord, Slack, Telegram, WhatsApp, Web chat.
- Voice support (TTS/STT), calendar/scheduling, and an activity timeline of runs/tool calls.
- Multiple agents with distinct personalities/tools.
- Dual interfaces: CLI and an Electron app for a visual chat/inspector/settings experience.
- Security foundations: auth, policy engine, Docker sandboxing, secret management.

Architecture (3 layers)
- World: communication, execution, runtime factory, config loader, voice, system utils.
- Agents: each agent lives in its own folder with core modules, tools, sessions, personality.
- Interfaces: clients including CLI and Electron/React app.

Getting started
- Clone the repo, install with pnpm, and run the Electron app: pnpm run dev.
- Repo: github.com/zuckermanai/zuckerman

Caveats
- Marked as WIP; self‑modifying behavior heightens security considerations despite sandboxing.
- AGPL‑3.0 license may limit closed‑source/commercial use without open‑sourcing derivatives.

Here is a summary of the discussion:

**Branding and Imagery**
A significant portion of the discussion focused on the project's name ("Zuckerman") and its avatar. While some users interpreted it as a "genius" or playful reference to memes about Mark Zuckerberg being a robot, others found it "creepy," distasteful, or distracting due to negative associations with Facebook/Meta. The author addressed these comments, noting the humor was intended but acknowledging the feedback regarding the mixed reception.

**Language Choice and Architecture**
The project’s "hot-reloading" and self-modifying capabilities led to a debate about programming languages. Several users questioned why the project wasn't built in Lisp or Erlang, which are historically renowned for hot-code reloading and "code as data" properties. Others clarified that while Lisp has deep roots in AI history, it is unrelated to the current wave of LLM-based architecture.

**Cost, Security, and Bugs**
Users raised practical concerns about the cost of running a self-navigating, self-editing agent that relies on paid API calls. The author responded that optimizations are planned, while other commenters discussed the feasibility of using local models on consumer hardware to mitigate costs. Security was also highlighted, specifically the risk of prompt injection if users download shared "skills" or agent capabilities from a community ecosystem. Additionally, users reported technical issues, including hardcoded file paths and installation hurdles, which the author acknowledged.

### Show HN: OpenJuris – AI legal research with citations from primary sources

#### [Submission URL](https://openjuris.org/) | 18 points | by [Zachzhao](https://news.ycombinator.com/user?id=Zachzhao) | [8 comments](https://news.ycombinator.com/item?id=46843162)

I’m ready—please share the Hacker News submission you want summarized.

Send one of:
- The HN item URL (news.ycombinator.com/item?id=…) or item ID
- The article URL/text (if you want the linked post summarized)
- Both, if you want highlights from the HN discussion plus the article

Also let me know:
- Length: ultra-brief (3 bullets), short (1–2 paragraphs), or deeper (5–7 bullets + takeaways)
- Whether to include comment highlights (top arguments, consensus, notable dissent)
- Any audience focus (technical, product, general)

Based on the discussion provided, here is a summary of the Hacker News conversation regarding a new legal database/AI tool.

### **Topic: Building an AI-Powered Case Law Database**

**Summary of Discussion:**
The discussion centers on a "Show HN" style post where the author is building a legal database and AI tool. Commenters are generally skeptical, focusing on the immense difficulty of competing with incumbents (Lexis, Westlaw) and the specific dangers of using Large Language Models (LLMs) in the legal field.

**Key Arguments & Insights:**

*   **The Data Moat:** Implementing a legal database is described as a "massive undertaking." Users point out that raw court data is fragmented, often requires physical access to courthouses, suffers from bad formats, and lacks the necessary metadata. Incumbents provide value not just through data access, but through "Shepardizing" (tracking if a case has been overturned or affirmed), which is difficult for a disrupted startup to replicate.
*   **The Hallucination Problem:** A major criticism involves the tendency of AI to hallucinate non-existent cases or citations.
    *   *Counter-point:* Some argue that proficient lawyers use AI for retrieval but verify everything by "reading the underlying case" (e.g., *Brady v. Maryland*), distinguishing between harmless typos and fatal hallucinations.
    *   *Technical Solution:* Others suggest "grounding" (RAG) to force the AI to link to appropriate, existing sources rather than generating text from a vacuum.
*   **"Wrapper" Accusations:** There is notable pushback against the tool being a "crappy wrapper" around a general-purpose LLM. One user criticizes the business model, accusing the poster of utilizing a confusing "non-profit/open" structure for a for-profit entity (similar to criticisms of OpenAI), calling it potentially unethical or misleading.

**Consensus:** building a legal tech startup requires solving deep infrastructure and data acquisition problems, not just applying an LLM to existing text.

