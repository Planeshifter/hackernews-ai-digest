## AI Submissions for Fri Jan 30 2026 {{ 'date': '2026-01-30T17:12:01.499Z' }}

### Self Driving Car Insurance

#### [Submission URL](https://www.lemonade.com/car/explained/self-driving-car-insurance/) | 135 points | by [KellyCriterion](https://news.ycombinator.com/user?id=KellyCriterion) | [307 comments](https://news.ycombinator.com/item?id=46825828)

Lemonade is launching what it calls the first car insurance priced specifically for self‑driving use: Tesla owners get 50% off every mile driven in Full Self‑Driving (FSD), with normal pricing for manual miles.

Key details
- How it works: With your permission, Lemonade connects to your car via Tesla’s Fleet API and automatically separates FSD miles from manual miles—no dongles or self‑reporting. Billing is usage‑based: pay normal rates for manual miles, 50% off for FSD miles.
- Why they price this way: Lemonade cites Tesla-reported safety data (e.g., 52% overall crash reduction on FSD) and says safer miles should cost less.
- Availability: Live in Arizona; Oregon launches Feb 26, 2026; more states “coming soon.”
- Requirements: Tesla Hardware 4.0+ and firmware 2025.44.25.5 or newer.
- Integration: No special endorsements; works like a normal auto policy, and you can bundle with Lemonade’s home/renters/pet/life for extra discounts.
- Admin: Setup is through the Lemonade app; it shows tracked FSD miles and savings.

Why it matters
- First mainstream attempt to price autonomous vs. manual miles separately, rather than giving a generic “safety features” discount.
- Points to a future where insurers tie premiums directly to OEM telemetry and automation usage (Lemonade says other automakers will follow as their systems mature).

HN‑style open questions
- The discount hinges on Tesla’s safety stats—how will regulators and actuaries validate them?
- Privacy/data: what exactly is shared via the Fleet API, and can users granularly limit it?
- Liability edge cases when FSD is engaged vs. driver responsibility, and how claims are adjudicated.
- Limited rollout: HW4+ and specific firmware only; AZ and OR to start.

**Discussion Summary**

The Hacker News community reacted with high skepticism regarding the safety data underpinning Lemonade’s business model, balanced by a debate on the predictive power of insurance markets.

*   **Statistical validity of "Safer Miles":** The dominant critique focuses on selection bias. Users argue that FSD is primarily used in "easy" environments (highway cruising), while humans take over for complex, high-risk situations (parking lots, tricky intersections). Critics claim that comparing FSD miles to average manual miles is an apples-to-oranges comparison that artificially inflates FSD's safety record.
*   **Market as a Truth Mechanism:** A counter-narrative suggests that legal or marketing claims are cheap, but insurance premiums represent "money where the mouth is." Some users view Lemonade’s willingness to underwrite this risk as a strong validation of the technology, similar to how health insurers assess BMI or smoking risks. Others dismiss it as a standard marketing customer-acquisition cost (CAC) disguised as a "tech" discount, noting similar programs from GEICO and Progressive.
*   **The "Supervised" Experience:** Anecdotal reports on FSD reliability are polarized.
    *   **Hardware disparity:** Several users note a distinct performance gap between Hardware 3 (HW3) and Hardware 4 (HW4), with the latter being significantly more capable.
    *   **Stress levels:** Detractors describe "supervised FSD" as more stressful than driving manually—likening it to supervising a teenager who might suddenly swerve into oncoming traffic or fail to yield at blind intersections.
    *   **Edge cases:** Users discussed the specific difficulty of "random" behavior (like children running into the street), debating whether AI or distracted humans respond better to sudden, chaotic events.
*   **Liability and Automation:** There is discussion regarding the legal definition of the driver. As long as Tesla requires supervision, the human remains the insurer's liability target. Users questioned when (or if) liability will shift to the manufacturer, suggesting that true autonomy should result in the OEM insuring the vehicle, not the owner.

### Show HN: Amla Sandbox – WASM bash shell sandbox for AI agents

#### [Submission URL](https://github.com/amlalabs/amla-sandbox) | 139 points | by [souvik1997](https://news.ycombinator.com/user?id=souvik1997) | [73 comments](https://news.ycombinator.com/item?id=46824877)

What it is
- A WebAssembly (WASM + WASI) sandbox for LLM agents that lets models write short scripts (JavaScript or shell) to orchestrate multiple tool calls, without giving them arbitrary code execution on your host.
- Ships as a Python package and one binary. No Docker, no VM.

Why it matters
- Today, many agent frameworks execute model-generated code with exec()/subprocess, which is risky; the README cites real-world issues (e.g., CVE-2025-68664).
- Pure tool-calling is safe but pricey: every tool call is a round trip through the model. “Code mode” collapses many calls into one script, cutting latency and token cost—if you can make it safe. This project aims to do that.

How it works
- Isolation: Code runs inside WASM with WASI via wasmtime (memory isolation, minimal syscalls). No network, no shell escape, VFS constrained (read/write only under /workspace and /tmp).
- Capabilities: You explicitly grant which tools can be called, with constraints and quotas. Examples include:
  - Method patterns (e.g., stripe/charges/*)
  - Parameter constraints (e.g., amount <= 10,000; currency in [USD, EUR])
  - Call limits (e.g., max_calls=100)
- Tooling model: The agent writes one JS/shell script that calls your whitelisted tools; sandbox enforces capabilities at each call boundary.
- Languages: JavaScript (async/await, object-style args) and shell pipelines. Output via return or console.log.
- Integration: Exposes a LangChain/LangGraph-compatible tool so an LLM can “write code” as a single action in a graph/agent.

Dev notes and quick start highlights
- Install: uv pip install "git+https://github.com/amlalabs/amla-sandbox"
- Provide Python functions as tools; the sandbox exposes them with enforced constraints.
- JS tools take object arguments only (no positional args).
- Filesystem: Only /workspace and /tmp are writable.

What’s interesting for HN
- A practical middle path between unsafe exec() and costly tool-chaining.
- Capability-based security mindset (no ambient authority) applied to agent code execution.
- Simpler ops than container isolation while still providing strong blast-radius reduction.
- Potential discussion: WASI surface area and escape risks, wasmtime hardening, side channels, performance overhead vs Docker, how to manage complex capability policies, and real-world integration stories.

Caveats
- No network access in the sandbox; all external effects must go through your vetted tools.
- You must design and maintain capability constraints; mistakes there can still cause misuse.
- Limited language/runtime surface (JS + shell via WASM) by design.

Repo: amlalabs/amla-sandbox (GitHub)

**Licensing and Open Source**
Discussion opened with **smnw** (Simon Willison) noting that while the Python usage code is MIT licensed, the underlying WASM binary is proprietary. He argued this effectively blocks the project from being used as a dependency in other open-source tools. The author (**souvik1997**) acknowledged this feedback and committed to prioritizing open-sourcing the WASM source code to allow for redistribution.

**Technical Architecture & Comparisons**
*   **vs. Pyodide:** When asked how this compares to Pyodide, the creators explained that Pyodide behaves like CPython compiled to Emscripten (for browsers/Node), whereas `amla-sandbox` is CPython compiled to WASI. This allows it to run on standard WASM runtimes (like Wasmtime), enabling server-side features like precise instruction counting, memory limits, and syscall interception.
*   **vs. LocalSandbox:** User **vmt** shared `LocalSandbox` (a Deno-based alternative), leading to a side discussion on implementation details like SQLite-backed virtual filesystems for agent "resume" funtionality.

**Security Model**
A user questioned the safety of tools running *outside* the sandbox. The author clarified their architectural philosophy: the sandbox acts as a **policy enforcement layer**. While the agent generates code inside the sandbox, the actual tools (like Stripe or API calls) execute on the host to access the network or credentials. The sandbox's job is ensuring the agent only invokes those tools with specific, whitelisted parameters and quotas.

**Ecosystem and Standards**
*   **Missing Modules:** In early testing, `smnw` noted that the Python environment currently lacks standard libraries like `sqlite3`.
*   **Typed Interfaces:** User **rllfy** argued that sandboxing is only half the battle; the ecosystem needs standardized component interfaces (like WIT) rather than raw shell scripting to ensure agents are traceable and safe at a build-time composition level.

### Claude Code's GitHub page auto closes issues after 60 days

#### [Submission URL](https://github.com/anthropics/claude-code/issues/16497) | 26 points | by [dcreater](https://news.ycombinator.com/user?id=dcreater) | [15 comments](https://news.ycombinator.com/item?id=46830179)

Anthropic’s claude-code repo is grappling with a stale-bot misfire: a new issue reports that GitHub Actions auto-closed numerous tickets for “60 days of inactivity” even after users replied to the 30‑day warning with “still relevant.” The report lists affected issues (e.g., #3006, #3030, #7742, #7743), has 100+ thumbs-up, and is marked as a regression and “external,” implying an upstream action/config bug. The ask: stop auto-closing or at least allow reporters to reopen. It’s a fresh example of how aggressive stale workflows can bury legitimate, slow-moving bugs—especially painful in a large repo with heavy community usage.

**The Irony of Auto-Closing**
The discussion reflects a broad disdain for "stale bot" workflows, with users arguing that auto-closing tickets prioritizes clean metrics over software quality.
*   **dashboard-driven development:** User `kngstnp` described the practice as the "worst kind" of management, citing Goodhart's Law to argue that closing evergreen issues merely to clear queues constitutes a process failure rather than a solution.
*   **Bad Logic:** `ssbttbttss` analyzed the likely cause of the "misfire," suggesting the bot checks the date of its *own* last comment while ignoring subsequent human activity—logic they found difficult to believe passed code review at an AI company.
*   **Perverse Incentives:** `SahAssar` and others noted that this policy forces users to spam "bump" or "+1" comments solely to keep valid bug reports prone to "unintended" closure alive, reminiscent of old internet forums.
*   **The "AI" Solution:** Several users, including `dcrtr`, found the situation ironic, asking why Anthropic doesn't simply use Claude to triage the issues rather than relying on a rigid, buggy script.

### How AI assistance impacts the formation of coding skills

#### [Submission URL](https://www.anthropic.com/research/AI-assistance-coding-skills) | 445 points | by [vismit2000](https://news.ycombinator.com/user?id=vismit2000) | [333 comments](https://news.ycombinator.com/item?id=46820924)

Headline: RCT finds AI coding help boosts speed little, but cuts short‑term mastery—especially debugging

- What’s new: Anthropic ran a randomized controlled trial with 52 mostly junior Python devs learning a new async library (Trio). Some used an in‑sidebar AI assistant; others coded unaided, then all took an immediate quiz.

- Key results:
  - Mastery dropped with AI: AI group scored 50% vs 67% without AI (≈17% gap; Cohen’s d=0.74; p=0.01).
  - Biggest hit was debugging—spotting and explaining errors in code.
  - Speed gain was small and not statistically significant (≈2 minutes faster on average).
  - Many participants spent substantial time crafting prompts—up to 11 minutes and 15 queries—eating into expected speedups.

- How AI was used mattered: Developers who asked the AI for explanations, follow‑ups, and conceptual clarifications retained more than those who mainly solicited code.

- Why it matters: As AI automates more low‑level coding, humans still need to catch errors and provide oversight. This study suggests naive reliance on AI can erode near‑term understanding—especially the very skills (reading, debugging, conceptual grasp) needed to supervise AI‑generated code.

- Caveats:
  - Short‑term learning only (quiz minutes after task), single library (Trio), small N, mostly junior devs.
  - Observational data elsewhere shows large productivity boosts (up to ~80%) on some tasks; this RCT isolates the trade‑off with immediate mastery.

- Practical takeaways:
  - Use AI as a tutor, not just a code printer: ask “why,” request step‑by‑step explanations, and probe concepts.
  - Interleave manual coding, code reading, and debugging; verify before running.
  - Teams should not equate AI‑boosted throughput with skill growth; consider policies and tools that scaffold explanation and require user reasoning.

**Corporate Motives and Study Validity**
The discussion opened with skepticism regarding Anthropic’s motives for publishing results that highlight the downsides of their own technology. While some users praised the transparency of publishing negative data on mastery, others viewed it as a "psy-op" similar to historical tactics used by tobacco companies—admitting to minor flaws to build trust while maintaining a conflict of interest. A few commenters also criticized the study itself for a small sample size and potential errors in the included figures.

**The Illusion of Speed vs. Competence**
Commenters largely validated the study's findings, noting that AI often makes developers *feel* faster while masking a lack of genuine progress. There was a consensus that the "path of least resistance" leads most users to use AI as a code printer rather than a tutor. Users highlighted that while the "tutor mode" (asking conceptual questions) preserves skill, the natural tendency is to bypass learning, potentially creating a generation of "1-3 year juniors" who never develop the deep problem-solving skills necessary to become senior engineers.

**Shifting Bottlenecks: Coding vs. Specifying**
A significant thread explored how AI changes the value of different engineering skills. Several users argued that as code generation becomes easier, the bottleneck shifts to Product Management skills—specifically the ability to write clear requirements and specifications. While some anecdotes claimed AI has already improved team outputs like Jira tickets and documentation "for free," skeptics countered that this likely results in "low signal" bloat that looks impressive but adds little value.

**Dependency and "Midnight" Risks**
Finally, participants discussed the operational risks of reliance. If developers lose the ability to code unaided, they become helpless when tools are unavailable (e.g., license issues or outages). One commenter posed a worst-case scenario: if an AI-generated system breaks in production at midnight, the human "gatekeepers" who merely prompted the code may lack the fundamental understanding required to fix it.

### Mamdani to kill the NYC AI chatbot caught telling businesses to break the law

#### [Submission URL](https://themarkup.org/artificial-intelligence/2026/01/30/mamdani-to-kill-the-nyc-ai-chatbot-we-caught-telling-businesses-to-break-the-law) | 171 points | by [jyunwai](https://news.ycombinator.com/user?id=jyunwai) | [59 comments](https://news.ycombinator.com/item?id=46827665)

NYC to scrap Adams-era AI business chatbot after it steered users toward illegal practices

- What’s new: New York City Mayor Zohran Mamdani plans to shut down the city’s Microsoft-powered business rules chatbot, calling it “functionally unusable.” The move comes amid efforts to close a $12 billion budget gap and follows investigations by The Markup and THE CITY showing the bot gave incorrect—and sometimes illegal—guidance.

- Why it’s getting axed: Reported failures included telling employers they could take a cut of workers’ tips, suggesting landlords could discriminate against tenants with Section 8 vouchers, misidentifying the minimum wage, and saying businesses could refuse cash despite a 2020 city law.

- Cost and context: Mamdani cited roughly half a million dollars in costs; prior reporting put foundational build costs near $600,000. The bot was part of the Adams administration’s MyCity digital overhaul, criticized for heavy reliance on outside contractors.

- What happened after the backlash: The Adams team defended the tool and added disclaimers warning users not to treat responses as legal advice, while narrowing the kinds of questions it would answer. The new administration has not set a takedown date yet.

- Bigger picture: The episode underscores the risks of deploying general-purpose AI for high-stakes compliance in government services—disclaimers can’t compensate for authoritative but wrong answers, and procurement-driven rollouts face heightened scrutiny in a budget crunch.

Source: The Markup/THE CITY (Jan 30, 2026)
Link: https://themarkup.org/artificial-intelligence/2026/01/30/mamdani-to-kill-the-nyc-ai-chatbot-we-caught-telling-businesses-to-break-the-law

Based on the discussion, here is a summary of the Hacker News comments:

**QA difficulties and non-determinism**
A significant portion of the technical discussion focused on the difficulty of performing Quality Assurance (QA) on "black box," non-deterministic systems like LLMs.
*   **The "Happy Path" Problem:** Users speculated that the city likely only tested "happy path" scenarios (standard, easy queries) where the bot performs well, failing to test edge cases or specific legal nuance.
*   **Whack-a-mole:** Commenter *lgn* described QA efforts for such tools as "whack-a-mole," noting that because LLMs are "inherent generators of plausible-sounding text," they are fundamentally mismatched for domains where exact correctness is required.
*   **Testing Limitations:** Discussions emerged regarding how to test these systems—whether through sampling user interactions or checking training data—with the consensus being that standard software engineering practices (testing for deterministic outputs) do not apply, making government deployment risky.

**LLMs are the wrong tool for compliance**
Commenters argued that general-purpose AI is fundamentally unsuited for government regulations and legal advice.
*   **Need for verification:** One user noted the "dirty little secret" of LLMs: if the output requires an expert to verify it to ensure it isn't illegal, the tool "defeats its own purpose."
*   **Static vs. Dynamic:** Users pointed out that NYC governance and laws change daily, while LLM training data is static ("lossy compression"), making it impossible for the model to remain current without a robust RAG (Retrieval-Augmented Generation) layer, which seemingly failed here.
*   **Search vs. Chat:** Several users suggested that better search functionality (citing Kagi or Google’s citation attempts) would be preferable to a conversational agent that "hallucinates" answers.

**Critique of the Adams Administration**
The failure was viewed by many as a symptom of former Mayor Eric Adams’ leadership style.
*   **Tech Hype:** Commenters drew parallels between this chatbot and Adams’ previous enthusiasm for cryptocurrency, characterizing his administration as one that chased "shiny promises" and tech buzzwords without understanding the underlying utility or risks.
*   **Bureaucratic Incompetence:** References were made to Louis Rossmann’s videos documenting NYC bureaucracy to illustrate a culture of incompetence (zero accountability/QA) regarding city services. The $600k price tag for a "barely working" wrapper was criticized as a waste of taxpayer money driven by enterprise sales and procurement dynamics rather than technical merit.

**Microsoft and Procurement**
A sidebar discussion emerged regarding the specific mention of Microsoft in the reporting. While some questioned if the provider mattered, others noted that Microsoft’s cloud division focuses heavily on government and large non-tech corporate sales, often resulting in expensive implementation contracts for software that is "sold by sales teams" rather than vetted by engineers.

### Show HN: I built an AI conversation partner to practice speaking languages

#### [Submission URL](https://apps.apple.com/us/app/talkbits-speak-naturally/id6756824177) | 64 points | by [omarisbuilding](https://news.ycombinator.com/user?id=omarisbuilding) | [58 comments](https://news.ycombinator.com/item?id=46830698)

TalkBits: voice-first, pressure‑free language practice on iPhone

What it is: A new iPhone app that drills real, short, spoken exchanges with an AI conversation partner. You press and hold to speak; it replies instantly with voice, using casual, everyday language and gently correcting mistakes in‑line.

Why it’s interesting:
- Conversation over lessons: prioritizes realistic, short responses and common expressions (multiple English accents plus German, French, Spanish, Italian, Dutch, Portuguese, Arabic, and more).
- Low friction: built for 30‑second to 5‑minute sessions; feels like quick, real‑world practice rather than study blocks.
- Private by design: no profiles or public ratings; App Store privacy label says “Data Not Collected” (as reported by the developer).

Details:
- iPhone only (iOS 15.1+), 28 MB; initial release Jan 2025, v1.0.2 updated 5 days ago.
- Free download with in‑app purchases: tiers listed at $9.99, $14.99, $19.99, $29.99, $49.99.
- Solo developer: Omar Muhammad Omar.

Caveats and open questions:
- Few/no ratings yet; real‑world quality of corrections and speech recognition is unproven.
- Offline mode and accessibility support aren’t specified.
- Privacy label is developer‑provided; Apple notes it’s not independently verified.

Here is a summary of the discussion:

**The "Wrapper" Debate vs. Utility**
The discussion opened with immediate observations that the app appears to be a ChatGPT wrapper. While some commenters found the proliferation of "wrappers" depressing or low-effort, others argued that the underlying technology matters less than the user experience; if the app solves a problem better than the raw model, the "provenance" is irrelevant. When users asked why they shouldn't just use ChatGPT’s native Voice Mode, the developer and other commenters noted that the raw models can be repetitive ("dull," "tends to repeat"), effectively forcing the user to do the prompting work, whereas this app offers a refined UI/UX and specific prompt engineering for language learning.

**Differentiation in a Crowded Market**
Critiques regarding the app's value proposition were prominent. Commenters pointed out that "focusing on conversation over vocabulary" is no longer a unique selling point, listing numerous competitors already filling this niche (Univerbal, Malan Chat, EnglishCall, TongueFu, etc.). Feedback suggested the develop needs to find a stronger "hook" or differentiator than just conversation practice to survive in a saturated market.

**Technical Feedback & Bugs**
Users reported several specific issues:
*   **Audio Loops:** A significant bug was identified where the microphone picks up the AI's speech, creating an endless feedback loop when using headphones (the developer confirmed they are fixing this).
*   **Localization Quality:** Users testing Portuguese noted a mix of European and Brazilian dialects, and German users noted jarring switches between formal (*Sie*) and informal (*Du*) address, as well as robotic accents.
*   **App Store Search:** Searching for "TalkBits" on the App Store triggers autocorrect to "Talbots," making the app difficult to find.
*   **Website Assets:** The landing page screenshots were described as blurry and unreadable, with font choices that needed an overhaul.

**Feature Requests**
The discussion included constructive suggestions for the product roadmap:
*   **Correction Toggles:** Users suggested a toggle between "immersion mode" (natural conversation without interruption) and "correction mode" (explicit feedback on grammar/pronunciation), as constant corrections can break flow.
*   **Latency Handling:** Several comments emphasized that latency is the "killer" for voice apps; managing Voice Activity Detection (VAD) to distinguish between a user pausing to think versus finishing a sentence is crucial for immersion.

### The Cost of AI Art

#### [Submission URL](https://www.brandonsanderson.com/blogs/blog/ai-art-brandon-sanderson-keynote) | 5 points | by [jplusequalt](https://news.ycombinator.com/user?id=jplusequalt) | [6 comments](https://news.ycombinator.com/item?id=46829452)

Brandon Sanderson: What is art, and why he rebels against AI-made art

- The setup: Sanderson opens by revisiting Roger Ebert’s 2010 claim that “video games can never be art,” arguing that games’ mechanics can themselves be artistic—and using that frame to examine today’s generative AI.

- Why now: He cites two signals that force the question:
  - An AI-generated track (“Walk My Walk”) topping Billboard’s Digital Country Songs chart, with Billboard acknowledging multiple recent AI chart-toppers.
  - Mark Lawrence’s blind test where readers struggled to distinguish short AI-written passages from ones by well-known novelists (AI still falters at long-form).

- The dilemma: Sanderson worries about becoming the next Ebert—reflexively dismissing a new medium. He notes how critics once derided prose (vs. poetry), photography, and film as “not art,” and wonders if AI is just another evolution.

- His stance: Even setting aside economics, environmental cost, and ethically messy training data, he says he would still oppose AI-made art. The piece tees up a deeper, philosophical argument about what art is and why we make it—implying the crux isn’t surface-level quality but something about intent, authorship, and the human act of creation.

Why it matters to HN: This is a prominent fantasy author engaging the “can AI be art?” debate without handwaving the tech’s capabilities. It’s a useful lens for product, policy, and culture: if audiences can’t tell the difference in outputs, does authorship matter? If mechanics can be art in games, can human-guided AI tools be part of that mechanics? Where should lines be drawn between tool, collaboration, and replacement?

**Process Over Product:** The discussion centers on the distinction between the technical quality of the output and the internal journey of the creator.
*   **The Utility vs. The Growth:** User **lckr** initially argues that AI art fails simply because the current quality is low—specifically in novels—viewing it as a technical hurdle rather than a philosophical one. User **jplsqlt**, clarifying Sanderson's thesis, counters that the objection isn’t about the quality of the final product (which may eventually become indistinguishable from human work) but about the "transformative process." Sanderson argues that struggling through creation is what improves the artist ("humans are the art"), and using AI generates a result while robbing the creator of that necessary growth.
*   **The Human Quotient:** The thread examines why audiences consume art in the first place. User **lbjcksn**, identifying as a writer/developer, argues that readers crave human connection and a reflection of current times—attributes an AI cannot authentically provide. Even with a "human-in-the-loop" acting as an editor, the consensus suggests that without the "human quotient," the work loses its appeal.
*   **Romanticism vs. Reality:** User **prmsfbns** notes that defining art by the creator's effort rather than the output is a "slightly romantic notion," though they concede that audiences do value context and backstory (evidenced by guided museum tours). The discussion concludes with the sentiment that while AI serves as a utility for tasks like summarizing meetings, applying it to creative endeavors discards the personal development inherent in the artistic struggle.

### Tesla’s autonomous vehicles are crashing at a rate much higher tha human drivers

#### [Submission URL](https://electrek.co/2026/01/29/teslas-own-robotaxi-data-confirms-crash-rate-3x-worse-than-humans-even-with-monitor/) | 471 points | by [breve](https://news.ycombinator.com/user?id=breve) | [257 comments](https://news.ycombinator.com/item?id=46822632)

Tesla’s Robotaxi crash rate looks far worse than humans — even with safety drivers, per Electrek’s read of new data

- The numbers: NHTSA’s Standing General Order shows 9 Tesla Robotaxi crashes in Austin from July–November 2025 (right-turn collisions, a cyclist strike, fixed-object hits, a construction-zone crash, an animal strike, and a low-speed backing collision). Tesla’s Q4 2025 earnings chart pegs cumulative robotaxi miles at ~500,000 by November, implying ~1 crash every 55,000 miles.

- How that compares: Human drivers average ~1 police-reported crash every ~500,000 miles (NHTSA), or roughly ~1 every ~200,000 miles when you include non-police-reported incidents. That puts Tesla’s supervised robotaxis at ~9x worse than police-reported human rates, or about ~3–4x worse using the broader estimate.

- Safety drivers didn’t save the stats: Every Tesla Robotaxi mile cited had a safety monitor onboard who could intervene. The top HN-style critique: any accidents averted by those monitors don’t show up, meaning an unsupervised rate could be worse.

- Transparency gap: Tesla’s crash narratives in the NHTSA database are fully redacted (“[REDACTED, MAY CONTAIN CONFIDENTIAL BUSINESS INFORMATION]”). By contrast, Waymo and others publish detailed narratives for each incident.

- Waymo contrast: Operating fully driverless, Waymo reports 125M+ autonomous miles with crash rates below human averages and incident-level transparency, including a recent school-zone collision where it released specifics showing rapid braking and reduced impact speed.

- Trajectory and caveats: Electrek notes only one crash in October and one in November (possible improvement), but the overall rate is still far from “robotaxi-ready.” The dataset is small and limited to one city, but the lack of disclosures makes independent assessment harder.

- Big takeaway: Electrek argues Tesla needs both a dramatically better safety record and real transparency about incident causes to be credible as a robotaxi operator.

Here is a summary of the discussion:

**Statistical Validity and Comparisons**
The primary debate centered on whether the comparison between Tesla’s Robotaxi and human drivers was "like-for-like." Critics argued that NHTSA SGO reports force AV companies to report minor low-speed contact events (rubbing a curb, minor bumps), whereas human baselines typically rely on police reports or insurance claims which exclude the vast majority of minor incidents.
*   **Counter-argument:** Others noted the article attempted to control for this by using a stricter "unreported incident" baseline for humans (~1 crash every 200k miles), but Tesla’s rate (1 every ~55k miles) was still roughly 3-4x worse.
*   **Denominator Issues:** Some users questioned the mileage figures, noting potential discrepancies between the cumulative mileage reported and the specific July–November window where the crashes occurred.

**Sample Size and Safety Drivers**
Commenters debated the significance of a dataset containing only 9 crashes. While some dismissed the sample size as too small to draw firm conclusions, others argued that statistically, incurring 9 incidents in such short mileage is highly improbable if the system were actually safe (calculated by one user as a 0.4% chance if the system matched human averages).
*   **The Safety Net:** A recurring point was that these statistics occurred with safety drivers behind the wheel. Users emphasized that since humans presumably intervened to prevent other potential accidents, the "unsupervised" crash rate would theoretically be much worse than the data suggests.

**Severity and Reporting**
Discussions broke down the specific types of crashes listed (hitting fixed objects vs. other vehicles). While some argued that hitting a static object shouldn't count as heavily as hitting a car, others retorted that "hitting a wall" is still a failure, and that humans rarely report such incidents unless there is significant damage.
*   **Transparency:** There was broad consensus that Tesla's redaction of crash narratives prevents independent verification. Users contrasted this with Waymo’s open reporting, suggesting that if Tesla's detailed data proved the system was safe, the company would likely release it. The prevailing sentiment was that obfuscation suggests the raw data looks bad.
