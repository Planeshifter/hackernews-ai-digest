## AI Submissions for Tue Oct 10 2023 {{ 'date': '2023-10-10T17:10:55.429Z' }}

### AI hype is built on flawed test scores

#### [Submission URL](https://www.technologyreview.com/2023/08/30/1078670/large-language-models-arent-people-lets-stop-testing-them-like-they-were/) | 193 points | by [antondd](https://news.ycombinator.com/user?id=antondd) | [224 comments](https://news.ycombinator.com/item?id=37830011)

A recent article in MIT Technology Review discusses the challenges and limitations of evaluating large language models like GPT-3 and GPT-4. While these models have showcased impressive abilities, such as passing human tests and demonstrating cognitive skills, there is a lack of consensus on what these results truly mean. Some researchers argue for more rigorous evaluation methods, while others suggest that scoring machines on human tests is flawed from the start. The article highlights the need for a better understanding of the capabilities and limitations of these language models as their impact on various industries becomes more pronounced.

The discussion in the comments revolves around the topic of whether language models like GPT-3 and GPT-4 are capable of true reasoning or if their performance is limited to surface-level pattern matching. Some commenters argue that these models lack the capacity for genuine reasoning and are merely sophisticated pattern recognition systems. Others highlight the limitations of current evaluation methods and the need for better techniques to assess the reasoning capabilities of these models. There is also a debate about the definition of "bullshit" and "reasoning," with some commenters arguing that these terms are subjective and context-dependent. Additionally, the discussion touches on the potential future developments in language models and the challenges of incorporating true reasoning abilities into artificial intelligence systems.

### Thought Propagation: An analogical approach to complex reasoning with LLMs

#### [Submission URL](https://paperswithcode.com/paper/thought-propagation-an-analogical-approach-to) | 55 points | by [amilios](https://news.ycombinator.com/user?id=amilios) | [13 comments](https://news.ycombinator.com/item?id=37836668)

ðŸ“° **Top Story**: "Thought Propagation: An Analogical Approach to Complex Reasoning with Large Language Models" by Junchi Yu, Ran He, and Rex Ying. Large Language Models (LLMs) have made great strides in reasoning tasks, but existing prompting methods prompt LLMs to reason from scratch, which can lead to accumulated errors in multi-step reasoning. To address this issue, the authors propose "Thought Propagation" (TP), a method that explores analogous problems and leverages their solutions to enhance the complex reasoning ability of LLMs. TP prompts LLMs to propose and solve a set of analogous problems related to the input problem, and then reuses the results of these problems to improve the initial solution obtained from scratch. Experimental results show that TP improves the performance of LLMs across three challenging tasks, achieving an average increase of 12% in finding optimal solutions in Shortest-path Reasoning, 13% improvement in human preference in Creative Writing, and 15% enhancement in the task completion rate of LLM-Agent Planning. [**Read more**](https://www.paperswithcode.com/paper/thought-propagation-an-analogical-approach-to)

The discussion on this submission revolves around various aspects of the proposed "Thought Propagation" (TP) method and its implications for large language models (LLMs).

One commenter, "mls," highlights the remarkable success of LLMs in reasoning tasks and notes that existing prompting methods often prompt LLMs to reason from scratch, leading to accumulated errors in multi-step reasoning. They believe that TP, by exploring analogous problems and leveraging their solutions, can enhance the complex reasoning ability of LLMs. They mention experimental results showing improvements in tasks like shortest-path reasoning, creative writing, and LLM-agent planning.

Another commenter, "hc," shares a personal anecdote and expresses skepticism about the effectiveness of TP in LLMs like ChatGPT. They believe that LLMs based on prompt extensions and pre-configured responses may not truly understand or reason through analogous problems.

"krmm" appreciates the cleverness of the proposed approach and expresses a desire for more detailed examples of how TP works in decomposing and recomposing problems.

"bpch" raises the possibility of humans historically employing similar methods of thought propagation and suggests that these innovations highlight how humans tackle hard problems.

In response, "mls" clarifies that abstract paper questions do not imply posting abstract texts directly. They mention a recent trend of posting abstracts and provide a link to an example.

"mjhy" expresses hope that this research signals a shift away from single giant neural networks towards models that incorporate multiple smaller, layered models. They suggest that breaking out of the "hallucination" of a single unified model could lead to better performance and potential advancements with more layers and networks.

### HyperAttention: Long-Context Attention in Near-Linear Time

#### [Submission URL](https://arxiv.org/abs/2310.05869) | 67 points | by [kelseyfrog](https://news.ycombinator.com/user?id=kelseyfrog) | [13 comments](https://news.ycombinator.com/item?id=37832599)

Researchers Insu Han, Rajesh Jarayam, Amin Karbasi, Vahab Mirrokni, David P. Woodruff, and Amir Zandieh have proposed a new attention mechanism called HyperAttention, which aims to address the computational challenges faced by large language models that use long contexts. In their paper titled "HyperAttention: Long-context Attention in Near-Linear Time," the authors introduce two parameters that measure the difficulty of the problem and use them to achieve a linear time sampling algorithm. HyperAttention features a modular design that can easily integrate other fast low-level implementations, such as FlashAttention. The researchers empirically demonstrate that HyperAttention outperforms existing methods, including FlashAttention, offering significant speed improvements without sacrificing performance. This new attention mechanism is expected to make inference time faster and improve the efficiency of large language models.

The discussion on Hacker News revolves around the proposed HyperAttention mechanism and its potential benefits for large language models. Some users highlight the performance improvements achieved by HyperAttention compared to existing methods like FlashAttention. They point out that HyperAttention makes inference time faster without sacrificing performance, leading to significant speed improvements on tasks with long contexts.

Other users discuss the trade-offs and considerations in using smaller models with 32k context windows. They mention that smaller models with limited memory can still perform well on certain tasks, and optimizing backends for specific context lengths can be beneficial.

There is also a discussion about how machine learning researchers tweak parameters in large language models. Some users express the opinion that researchers often publish papers that tweak metrics to make the improvements seem more significant than they really are. They highlight the importance of being transparent about the metrics and mentioning negative results as well.

The topic of publishing negative or non-significant results also emerges in the conversation. Some users argue that researchers publish papers that only mention the positive results, while others argue that publishing negative results is important to advance the field.

There is a brief discussion about the publication process, with users expressing different opinions on formal results and peer review.

Lastly, some users mention the need for better comparisons and benchmarking in research papers, suggesting that researchers should compare their models with existing popular frameworks. Others point out that it is hard to gauge the value of papers without industry or academic consensus.

### Building a collaborative pixel art editor with CRDTs

#### [Submission URL](https://jakelazaroff.com/words/building-a-collaborative-pixel-art-editor-with-crdts/) | 151 points | by [jakelazaroff](https://news.ycombinator.com/user?id=jakelazaroff) | [22 comments](https://news.ycombinator.com/item?id=37832432)

In today's post "Building a Collaborative Pixel Art Editor with CRDTs" on jakelazaroff.com, the author takes us through the process of using Conflict-free Replicated Data Types (CRDTs) to build a collaborative pixel art editor. The post assumes no prior knowledge about CRDTs and provides a basic introduction to the topic.

The author starts by explaining that they will be using JavaScript and graphics programming to demonstrate how CRDTs can be implemented in a real-world application. They provide the necessary code to build the CRDT, which is a class called PixelData that acts as a wrapper over a Last Write Wins (LWW) Map. The PixelData class maps pixel coordinates to colors, with each key representing a single pixel.

The author then provides a visualization of how the keys and values interact when drawing on the canvas. They show how painting a pixel sets the key value to the selected RGB color, and how pixels that haven't been set default to white. When painting over a pixel, the value is overwritten and the timestamp is incremented.

Moving on to the UI, the author shares the HTML and CSS code for setting up the canvas and color input elements. They then provide JavaScript code to instantiate two editors, Alice and Bob, which are instances of the PixelEditor class. The states of Alice and Bob are merged whenever a change is made in either editor, and the color is set based on user input.

Overall, this post serves as a practical example of how CRDTs can be used in a collaborative application, specifically a pixel art editor. By following the author's explanations and code samples, readers can gain a deeper understanding of CRDTs and how they can be implemented in their own projects.

The discussion revolves around several aspects of the post. 

One user points out that training models to resolve conflicts in collaborative dating can be interesting, and they mention a specific case on GitHub where conflict resolution was handled manually. They also suggest that having non-interactive resolution as the default in developer tools could lead to the loss of work in certain situations, like large refactorings.

Another user mentions that conflicts in CRDTs can be resolved at a semantic layer and provides an example of how conflicts in a canvas can be resolved by prioritizing the most recent drawing. They emphasize the importance of understanding semantics to resolve conflicts effectively.

There is a discussion about whether or not text convergence is guaranteed with CRDTs. One user argues that CRDTs do not guarantee semantic content preservation, while another user explains how they handle nested data in a CRDT to ensure content convergence.

The discussion also touches on the benefits of CRDTs and how they can handle conflict resolution in a self-driving manner, making it easier to work with conflicts in simpler domains.

Another user raises the challenge of preserving the intent of content, particularly when it involves making human-like judgment calls, and mentions that CRDTs provide consistency but may not handle judgment calls effectively.

There is also dialogue about the convergence of state in CRDTs, with one user pointing out that while CRDTs technically converge resulting in the same state, the neural network approach ensures that the final state matches the human's intended state more reliably.

A user shares a link to a related discussion on Our World Pixels, mentioning the date and number of comments.

Finally, there are a few comments about the syntax, font, and highlighting of code in the post, with one user suggesting a specific font family for code.

The discussion is informative and includes various perspectives and insights related to the topic of CRDTs and collaborative pixel art editing.

### Apple patents suggest future AirPods could monitor biosignals and brain activity

#### [Submission URL](https://applemagazine.com/apple-patents-suggest-future-airpods-could-monitor-biosignals-and-brain-activity/59510) | 124 points | by [sundarurfriend](https://news.ycombinator.com/user?id=sundarurfriend) | [95 comments](https://news.ycombinator.com/item?id=37836603)

Apple has been granted a patent by the US Patent & Trademark Office for next-generation AirPods that could measure various biosignals such as electrooculography (EOG), electromyography (EMG), and electroencephalography (EEG). The patent application reveals that the AirPods would contain strategically placed electrodes to capture these measurements, which can monitor brain activity when attached to the user's scalp. The patent suggests that the future AirPods may be customizable to accurately measure ear-EEG, taking into account the variations in size and shape of individuals' ears. There are also reports that Apple is exploring features to measure body temperature through the ear canal and a hearing test feature to assess a user's hearing abilities.

The discussion on this submission covered a wide range of topics:

- user "karim79" expressed excitement about the potential for AirPods to measure brain activity levels and how it could lead to innovative software and personalized experiences.
- Some users made jokes and light-hearted comments.
- User "cmiller1" mentioned that this patent is tangentially related to a dream they had about a music player that could sense their physical activity.
- There was a discussion about mental health and therapy services such as BetterHelp and their potential impact.
- Some users commented on the use of music for various activities and mentioned music-streaming service Spotify.
- User "jrckwy" discussed the impact of technology on activities like cycling and mentioned headphones that allow for situational awareness.
- There was a conversation about the potential applications of biofeedback and brainwave-sensing devices.
- User "rtsdx" wondered about the possibility of tracking REM sleep stages and adjusting alarms accordingly.
- Other topics brought up included fitness devices, patent validity, and the quality of life improvements that small things can bring.

Overall, the discussion covered a wide range of ideas and opinions related to the patent for next-generation AirPods.

### Artificial General Intelligence Is Already Here

#### [Submission URL](https://www.noemamag.com/artificial-general-intelligence-is-already-here/) | 25 points | by [falava](https://news.ycombinator.com/user?id=falava) | [10 comments](https://news.ycombinator.com/item?id=37836957)

Artificial General Intelligence (AGI), which refers to AI systems that exhibit human-level intelligence across a wide range of tasks, is already here, according to Blaise AgÃ¼era y Arcas, a vice president at Google Research, and Peter Norvig, a computer scientist at Stanford. They argue that although today's advanced AI models have many flaws, they have already achieved the key properties of generality that define AGI. These "frontier models" can perform a variety of tasks, operate on different modalities like text, images, and audio, handle multiple languages, and learn from prompts rather than just training data. While these models still have limitations, the authors believe they will be recognized as the first true examples of AGI in the future. They compare this to the ENIAC, the first general-purpose electronic computer, which paved the way for today's computers. The authors emphasize that AGI should be seen as a multidimensional scorecard rather than a simple yes/no proposition.

The discussion on this submission revolves around several key points. 

One user suggests that instead of focusing on cognitive reach as a criteria for AGI, attention should be given to cognitive architecture based on sensors and stimuli. They argue that AGI should involve a high level of cognitive interactions, including signals and behavioral modulation.

Another user points out that Google's language models (LLMs) may be considered AGI, while another user questions this assertion.

Another user brings up the idea that current AI systems lack certain qualities that define AGI, such as the ability to understand complex reasoning and assimilation. They argue that AGI requires testing in real-world situations, and that current LLM-based applications are limited to specific contexts.

A user expresses the belief that AGI is already here, thanks to advanced LLM models.

There is a debate regarding the definition and criteria of AGI, with some arguing that current AI models should not be considered AGI due to their limitations, while others believe they already exhibit human-level intelligence.

One user agrees with the notion that AGI is already here, while another user dismisses the claim as "Total BS."

Another user jokingly suggests that GPT-4 is 100% AGI.

The discussion also delves into the differences between instinctive intelligence and general intelligence, with some arguing that current AI models lack the instincts and knowledge-based intelligence exhibited by humans.

One user highlights the role of consciousness in general intelligence, arguing that intelligence is not solely a product of consciousness.

The discussion ends with a user suggesting that the metric for artificial consciousness should be maximizing models' predictive abilities while minimizing predictability. They argue that consciousness is the foundation for general intelligence.

### The rent is too damn algorithmic

#### [Submission URL](https://washingtoncitypaper.com/article/631461/the-rent-is-too-damn-algorithmic/) | 111 points | by [thunderbong](https://news.ycombinator.com/user?id=thunderbong) | [143 comments](https://news.ycombinator.com/item?id=37829575)

Attorney General Brian Schwalb is investigating a company called RealPage for potential antitrust violations in the housing industry. RealPage provides software that recommends rent prices for apartments based on market data. Critics argue that the company's algorithm effectively enables collusion among big landlords to fix prices. Schwalb has proposed hiring a law firm to assist with the investigation and potential litigation against RealPage and other targets in the housing industry. The Texas-based company is already facing multiple lawsuits and an antitrust investigation by the Department of Justice. This could be one of the first challenges to RealPage by state attorneys general.

The discussion on Hacker News about the submission focuses on various aspects of the antitrust investigation against RealPage and the implications of their software on the housing market. Here are the main points discussed:

- Some users argue that RealPage's software enables collusion among landlords and effectively fixes rent prices. They point out that the software allows landlords to coordinate and set prices based on market data, potentially leading to higher rents.
- Others suggest that the problem lies with the shortage of housing supply, rather than RealPage's software. They argue that if there were enough available housing units, the market forces would naturally adjust prices. They criticize the company for taking advantage of the scarcity of housing.
- There is a debate about whether the actions of RealPage constitute a market manipulation or just a reflection of the supply and demand dynamics. Some users argue that market forces are at play, while others believe that there is a manipulation of prices by large landlords.
- One user suggests that one possible solution to address the issue would be to remove class entirely from residential property. They argue that housing should not be treated as an investment, as it contributes to pricing out many people and exacerbates the shortage of affordable housing.
- The discussion also touches on the idea of induced demand and its impact on traffic congestion. Some users argue that increasing housing supply could lead to more people moving to the city and worsening congestion, while others believe that the lack of affordable housing forces people to live far away from their workplaces, causing traffic jams.
- A user mentions the housing market in Germany, where the construction of housing is more focused on short-term rentals and leads to a shortage of affordable housing. They argue that construction should prioritize long-term housing solutions and consider the environmental impact.

Overall, the discussion highlights the complexities and different perspectives surrounding the investigation into RealPage and the broader issues of affordability and market dynamics in the housing industry.

### Lit 3.0

#### [Submission URL](https://lit.dev/blog/2023-10-10-lit-3.0/) | 116 points | by [meiraleal](https://news.ycombinator.com/user?id=meiraleal) | [88 comments](https://news.ycombinator.com/item?id=37834927)

Today is an exciting day for the Lit project and the web components community. The Lit team has officially released Lit 3.0, marking their first major version since Lit 2.0 in early 2021. In addition to Lit 3.0, they also announced the graduation of the first class of Lit Labs packages, which include @lit/react, @lit/task, and @lit/context. But that's not all! The team also released two bonus packages: @lit-labs/compiler and @lit-labs/preact-signals.

One of the main highlights of Lit 3.0 is the removal of support for IE11. After surveying the developer community, the team felt that now is the right time to say goodbye to IE11. This release also introduces some additional breaking changes that remove technical debt and pave the way for new features planned for future releases.

Despite being a breaking change release, Lit 3.0 does include one new feature: support for the TC39 standard decorators specification. With the arrival of standard decorators, Lit can begin transitioning to a decorator implementation that doesn't require a compiler to use. The team has made efforts to ensure a smooth upgrade path from experimental decorators to the standard spec.

Another noteworthy release is the new @lit-labs/compiler package. This Labs package provides a TypeScript Transformer that can be used during build-time preparation of Lit templates, resulting in faster rendering performance. According to benchmarks, the new compiler can improve first render speed by 46% and update speed by 21%.

For those interested in upgrading to Lit 3.0, the process should be seamless for most users. Simply update the npm dependency version to the latest release of Lit.

Overall, these releases demonstrate the Lit team's commitment to stability, performance, and adherence to standards. The Lit project continues to evolve and improve, offering developers powerful tools for building web components.

The discussion on this submission covers various topics related to Lit 3.0 and web components. Here are some key points:

- Some users express their excitement for the Lit project and the new releases, while others mention potential benefits like improved performance and better integration with other frameworks.
- There is a discussion about the difficulty of getting started with Lit and the benefits of using pre-existing design systems like Shoelace.
- Some users discuss the advantages of web components and their potential integration with frameworks like React and Vue.
- There is a debate about the relevance of web components and their adoption in the industry. Some users mention the limitations of React and the benefits of using Lit as a smaller and more efficient library.
- A user highlights the progress made in the Lit project, comparing it to jQuery in terms of simplifying web development.
- There is a discussion about the similarities and differences between Lit and other web frameworks.
- Some users mention the importance of standardization and the potential of web components to facilitate component integration across different platforms.
- Other topics include the performance improvements in Lit's compiled templates and the advantages of using compiled templates for optimization.

Overall, the discussion reflects different perspectives on the use of Lit and web components, with users discussing their experiences, concerns, and potential use cases.

### Polymathic AI

#### [Submission URL](https://polymathic-ai.org/blog/announcement/) | 30 points | by [jasondavies](https://news.ycombinator.com/user?id=jasondavies) | [5 comments](https://news.ycombinator.com/item?id=37838943)

Introducing the Polymathic AI initiative, a research project aiming to accelerate the development of versatile foundation models for scientific machine learning tasks. While there have been significant advances in machine learning for vision and natural language processing, the same paradigm shift has not yet occurred for scientific datasets. The goal of Polymathic AI is to build AI models that can leverage information from heterogeneous datasets and across different scientific fields. By providing off-the-shelf models with strong priors for scientific concepts, the initiative aims to democratize AI in science. The project brings together a team of machine learning researchers and domain scientists and is guided by a scientific advisory group. Preliminary research has already been published on key architectural components, and the initiative holds great potential to redefine the landscape of scientific machine learning.

The discussion about the Polymathic AI initiative on Hacker News includes a few comments. 

One user, "wrsh07," finds the announcement paper interesting and shares a link to it. 

Another user, "xlxbr," expresses difficulties in finding information about the organization on the Polymathic AI website. They wonder about the participating institutions and the skills required for participation. 

A user called "jsndvs" suspects that this might be a collaborative scientific effort supported by commercial enterprises. They mention the Flatiron Institute and the Simons Foundation as potential supporters based on a tweet from the Polymathic AI account. They also provide a link to the Simons Foundation's website, which they believe provides informative press releases about the initiative. 

Another user, "hltst," finds the work interesting and highlights the discovery of promising concepts related to dynamics in static and dynamic systems. 

One user, "lypnv," flags the discussion. 

Finally, a user called "bgwg" comments with just "nm ll," which is not clear in terms of meaning or relevance to the discussion.

