## AI Submissions for Fri Nov 07 2025 {{ 'date': '2025-11-07T17:13:42.247Z' }}

### Leaving Meta and PyTorch

#### [Submission URL](https://soumith.ch/blog/2025-11-06-leaving-meta-and-pytorch.md.html) | 699 points | by [saikatsg](https://news.ycombinator.com/user?id=saikatsg) | [168 comments](https://news.ycombinator.com/item?id=45843948)

Soumith Chintala is leaving Meta and stepping away from leading PyTorch after 11 years at the company and nearly eight years shepherding PyTorch from a scrappy research project to the default stack for modern AI.

Highlights
- Impact: PyTorch now supports exascale training, powers foundation models across the industry, runs in production at most major AI companies, and is taught globally—fulfilling his original goals of accessibility and lowering the barrier to entry.
- Why leave now: After the birth of his daughter last year, he planned a deliberate transition. He wants to do “something small, new, and uncomfortable” outside Meta, trading one of the industry’s most leveraged roles for curiosity.
- Continuity: He says PyTorch no longer needs him day-to-day. A deep bench—Edward Yang, Suo, Alban Desmaison, Greg Chanan, John, Joe, Jana, and others—has been steering hard people/product/technical decisions and delivered a coherent 2025 product story. He expects the “flavor” may change without his taste on top, but values and execution to stay strong.
- He’ll still be around: “Keep making AI delicious and accessible. I’ll be watching. Probably filing issues. Definitely staying involved.”

Context and gratitude
- Soumith reminisces about FAIR’s early, wildly productive years (GANs, StarCraft bots, the first FAIR cluster) and credits mentors and contributors who helped turn PyTorch from a library into a movement.
- He explicitly didn’t want to be tied to a single thing for decades (name-checking Guido and Linus), and previously stepped back 2020–2022 before returning.

Why it matters
- Leadership transitions in foundational AI infrastructure are rare and consequential. Soumith’s note is both a reassurance of PyTorch’s stability and a signal that its next chapter will be led by a broader, seasoned team.
- No new venture announced—just an intent to start small and explore.

**Summary of Discussion:**

1. **Impact and Legacy of Soumith Chintala:**  
   Participants universally praised Soumith’s role in transforming PyTorch into the dominant AI framework, emphasizing its accessibility, Pythonic design, and community-driven growth. Users highlighted Andrej Karpathy’s 2017 endorsement of PyTorch as a pivotal moment, crediting Soumith for lowering barriers in AI research and enabling widespread adoption.

2. **Technical Predecessors and Evolution:**  
   Discussions noted PyTorch’s roots in earlier projects like **Chainer** (a pioneer in define-by-run models) and Python’s **Autograd** library. While Chainer influenced PyTorch’s design, contributors argued PyTorch’s comprehensive toolkit (e.g., `torch.nn`) and Meta’s resources propelled it past competitors. JAX, seen as Autograd’s successor, was acknowledged but deemed less intuitive for researchers.

3. **Community and Transition Concerns:**  
   Commenters expressed confidence in PyTorch’s future under its "deep bench" of maintainers (e.g., Edward Yang, Alban Desmaison). Some speculated whether Meta’s shift toward LLMs influenced Soumith’s departure, though others countered that PyTorch remains integral to Meta’s traditional ML workflows (e.g., recommendation algorithms).

4. **Deployment and Tooling Debates:**  
   Users debated PyTorch’s dominance in training versus deployment tools like **Candle** or **llama.cpp**, which offer smaller footprints. While PyTorch’s `libtorch` was called the de facto standard, some noted emerging alternatives for edge/inference use cases.

5. **Military Tech Tangent:**  
   A subthread compared military and civilian tech development, critiquing military projects for inefficiency, bureaucracy, and reliability challenges. This diverged from the main discussion but underscored broader themes of collaboration vs. secrecy in tech.

**Key Sentiments:**  
- Gratitude for Soumith’s vision and execution.  
- Confidence in PyTorch’s continuity post-transition.  
- Recognition of open-source community dynamics driving innovation.  
- Minor speculation about Meta’s strategic focus and Soumith’s next steps.  

**Controversies:**  
- Disagreement over whether Meta’s LLM focus impacts PyTorch’s roadmap.  
- Debate on PyTorch’s suitability for deployment vs. training.  

**Conclusion:**  
The discussion reflects admiration for Soumith’s legacy, optimism about PyTorch’s future, and nuanced technical debates about its ecosystem. The military tech tangent, while off-topic, highlighted broader industry contrasts in development practices.

### From Memorization to Reasoning in the Spectrum of Loss Curvature

#### [Submission URL](https://arxiv.org/abs/2510.24256) | 59 points | by [andy12_](https://news.ycombinator.com/user?id=andy12_) | [14 comments](https://news.ycombinator.com/item?id=45845800)

TL;DR: The authors show that you can separate and edit out a model’s “memorization” behavior by looking at the loss landscape’s curvature, cutting untargeted regurgitation more effectively than a recent unlearning method (BalancedSubnet) while keeping perplexity low. The catch: closed‑book fact recall and arithmetic get worse, but open‑book fact use and general logical reasoning largely survive—evidence that some skills live in narrow, specialized weight directions.

What’s new
- Curvature as a lens: Prior work links memorized examples to sharper loss curvature. The authors exploit this by ordering weight components along a curvature spectrum, revealing memorization without labels.
- Weight editing, not retraining: They perform a targeted edit in weight space guided by curvature, suppressing recitation of memorized data in both LMs and ViTs.
- Beats a strong baseline: Compared to BalancedSubnet unlearning, their edit reduces untargeted memorization more and maintains lower perplexity.

What they found
- Tradeoffs: After the edit, language models lose performance on closed‑book fact retrieval and arithmetic, but keep open‑book fact use and general reasoning intact.
- Interpretation: The harmed skills seem to rely on specialized, narrowly used weight directions (aligned with the components the edit suppresses), while broader reasoning mechanisms are more distributed and robust.
- Evidence: Task activation strength correlates with the curvature-identified components they remove, matching the observed post‑edit performance drops on those tasks.

Why it matters
- Safety and compliance: A lightweight, model‑intrinsic way to curb regurgitation without costly unlearning pipelines.
- Mechanistic insight: Supports a picture where “memorization” and parts of “skill” (e.g., arithmetic, fact recall) occupy sharper, specialized directions, whereas general reasoning is more diffuse.
- Practical caution: Stripping memorization may also strip useful specialized capabilities; evaluations should distinguish closed‑book vs open‑book and reasoning vs recall.

Baseline/benchmarks
- Outperforms BalancedSubnet on reducing unintended memorization while keeping perplexity lower.
- Side effects concentrated in fact recall and arithmetic; open‑book and general reasoning preserved.

Link: arXiv:2510.24256 (v2), Oct 31, 2025.

The discussion around the paper highlights several key points and connections:

### **Technical Breakdown & Clarifications**
1. **Methodology**: Users dissect the paper's approach:
   - Compute loss curvature via MLP weight matrices and K-FAC gradient covariance approximations.
   - Decompose weights into curvature-ordered components, linking low-curvature directions to memorization and high-curvature to shared generalization mechanisms.
   - Edit models by dropping low-curvature subspaces to suppress memorization.

2. **Curvature vs. Memorization**: Clarifications emerge:
   - High-curvature directions correlate with memorized training points (sharp minima), while flatter directions correspond to generalizable features.
   - Averaging curvature across multiple examples reveals broader reasoning mechanisms in high-curvature components.

3. **Connections to Prior Work**:
   - **SAM (Sharpness-Aware Minimization)**: Users note similarities to optimizing for flat minima to improve generalization.
   - **Generalization Theory**: Smoother loss landscapes are tied to regularization and robust reasoning, aligning with classical ML intuition.

---

### **Implications & Interpretations**
- **Trade-offs**: Removing memorization harms closed-book tasks (fact recall, arithmetic) but preserves open-book reasoning, suggesting specialized vs. distributed skill encoding.
- **Mechanistic Insights**: 
  - Memorization relies on narrow, high-curvature weight directions.
  - Reasoning is more diffuse and resilient to curvature-based pruning.
- **Practical Concerns**: 
  - Editing might strip useful skills (e.g., arithmetic), necessitating careful evaluation.
  - Lightweight, intrinsic edits could enhance safety/compliance without costly retraining.

---

### **Community Reactions**
- **Support**: Users praise the paper’s novel lens (curvature) and practical editing method, calling it "super interesting" and a potential tool for model safety.
- **Skepticism/Caution**: Some highlight risks (losing critical skills) and note prior observations of similar loss-curve "kinks" (e.g., in recent talks).
- **Related Work**: Mentions of Karpathy’s podcast on targeted models and reasoning vs. memorization, reinforcing the paper’s relevance.

---

### **Key Takeaways**
The discussion underscores the paper’s contribution to **mechanistic interpretability** and **model editing**, while cautioning against oversimplification. The community sees promise in curvature-based approaches but emphasizes balancing memorization removal with preserving essential capabilities. Links to SAM and generalization theory enrich the context, positioning the work within broader ML research trends.

### Gmail AI gets more intrusive

#### [Submission URL](https://daveverse.org/2025/11/07/gmail-ai-gets-even-more-intrusive/) | 234 points | by [speckx](https://news.ycombinator.com/user?id=speckx) | [148 comments](https://news.ycombinator.com/item?id=45848504)

Dave Winer claims Gmail has crossed a line from “offering to help” to auto-inserting AI-written text into your draft—leaving it to you to delete if you don’t want it. He calls the behavior intrusive and “reeking of desperation,” and notes it’s hard to screenshot without exposing personal info. This reads as a shift from opt-in assistance (Smart Compose, “Help me write”) to an opinionated default that can steer or clog your messages.

Why it matters
- User agency: Pre-filled drafts flip the burden from opt-in to opt-out, a classic dark-pattern concern.
- Trust and safety: AI-inserted content risks hallucinations or tone misfires being sent under your name.
- Enterprise/compliance: Unexpected generated text in regulated comms could create audit and policy headaches.

What to check/try
- This may be a limited rollout, experiment, or misfire—HN readers will likely try to reproduce.
- To disable: Gmail Settings → General → Smart Compose (off), Smart Reply (off); Settings → Smart features & personalization (off); turn off “Help me write”/Gemini features and Workspace Labs. On mobile: Settings → your account → Smart Compose/Smart Reply toggles. Workspace admins can disable AI assist features org-wide.

Discussion prompts
- Should generative assistants ever default to auto-drafting without an explicit action?
- What’s the right UX cue (ghost text vs. inline text vs. side-panel) to preserve agency?
- How should admins and users audit when AI contributed to a sent message?

**Summary of Hacker News Discussion:**

The discussion revolves around growing frustration with Google’s recent product decisions, particularly intrusive AI-driven features and UX changes across services like Gmail and YouTube. Key themes include:

### **1. Critiques of Google’s UX and AI Integration**
- **Intrusive Defaults**: Users criticize Google for forcing AI-generated content (e.g., auto-translated video titles, AI-drafted emails) without clear opt-out options. Examples include YouTube’s aggressive auto-translation of titles/descriptions, often resulting in nonsensical or misleading text, and Gmail’s AI inserting draft text by default.
- **Dark Patterns**: Many label these changes as “dark patterns” designed to prioritize engagement metrics (watch time, clicks) over user experience. YouTube Shorts’ forced vertical cropping and auto-conversion of videos are cited as disrespectful to creators and viewers.
- **Decline in Trust**: Users argue Google’s focus on AI-driven engagement optimization erodes trust, with some claiming the company now serves advertisers and metrics over end-users.

### **2. Workarounds and Solutions**
- **Extensions and Tools**: Recommendations include browser extensions like [Hide YouTube Shorts](https://github.com/conifer215/hide-ytb-shorts) and [YouTube No Translation](https://github.com/YouG-o/YouTube-No-Translation) to disable unwanted features. Others suggest ad-blockers (uBlock Origin) or paying for YouTube Premium to avoid intrusive ads.
- **Disabling Features**: Steps to turn off Gmail’s AI features (Smart Compose, Smart Reply) are shared, though users note these settings are buried and vary by region (e.g., EU/UK users have stricter defaults).

### **3. Broader Criticisms of Google’s Strategy**
- **Product Management Failures**: Commentators blame “desperate” product managers and marketers for prioritizing quarterly metrics over user needs. Comparisons are drawn to Reddit’s poorly received AI-translated comments and WhatsApp’s controversial changes.
- **Enterprise Concerns**: Self-hosted email and alternatives are suggested to avoid reliance on Google’s shifting policies, especially for compliance-sensitive organizations.
- **Cultural Missteps**: Non-English users highlight YouTube’s flawed auto-translations (e.g., Japanese titles mangled into gibberish), arguing the platform undervalues non-Western content.

### **4. User Sentiment**
- **Frustration with Engagement-Driven Design**: Many feel trapped in a cycle of “addictive” algorithms (e.g., Shorts, AI-recommended content) designed to maximize screen time, not utility.
- **Nostalgia for Simpler Times**: Some reminisce about earlier Google products that prioritized functionality over monetization, contrasting today’s “bottom-of-the-barrel” content.

### **Key Quotes**:
- *“Google literally couldn’t care less about users… they optimize for whatever KPIs their managers are chasing.”*  
- *“YouTube’s AI dubbing is like replacing a village artisan’s craft with a polished, soulless factory product.”*  
- *“It’s sad when the only fix is paying for Premium or installing 10 extensions to make the platform usable.”*

**Conclusion**: The discussion reflects widespread disillusionment with Google’s shift toward aggressive AI integration and engagement-driven design, with users advocating for transparency, user control, and third-party tools to reclaim their experience.

### AI Capabilities May Be Overhyped on Bogus Benchmarks, Study Finds

#### [Submission URL](https://gizmodo.com/ai-capabilities-may-be-overhyped-on-bogus-benchmarks-study-finds-2000682577) | 43 points | by [Cynddl](https://news.ycombinator.com/user?id=Cynddl) | [17 comments](https://news.ycombinator.com/item?id=45852240)

Oxford study: popular AI benchmarks may be overstating model abilities

- What happened: Researchers at the Oxford Internet Institute reviewed 445 AI benchmarks and say many don’t validly measure what they claim. Vague target definitions and poor statistical disclosure make cross-model comparisons shaky. Lead author Adam Mahdi argues that headline claims like “reasoning” are often inferred from tasks that don’t actually test it.

- Example: GSM8K (grade-school word problems) is marketed as probing multi-step reasoning. Scores have climbed, but the study suggests that could reflect contamination or memorization. When models were tested on a fresh set of similar problems, performance “dropped significantly.”

- Why it matters: Those splashy “passed the bar/PhD-level” narratives may rest on leaky or mis-specified tests. The paper echoes earlier Stanford work finding benchmark quality varies widely and often degrades from design to implementation.

- Takeaways for builders and readers:
  - Treat benchmark wins as narrow signals, not proof of general reasoning.
  - Demand clearer construct definitions, leakage checks, and transparent stats (variance, significance, sample sizes).
  - Prefer dynamic, held-out, and adversarial test sets over static leaderboards.
  - Be wary when benchmarks double as marketing.

Bottom line: AI might be getting better at benchmarks, not necessarily at the underlying skills those benchmarks are supposed to measure.

**Summary of Discussion:**

The Hacker News discussion reflects skepticism about AI benchmarks' ability to measure true reasoning, focusing on claims that models like GPT-5 excel at tasks such as math problems. Key points include:

1. **Memorization vs. Reasoning Debate**:  
   - Users argue that high performance on benchmarks like GSM8K (grade-school math) may stem from memorization or exposure to training data rather than genuine reasoning. Critics note that models often struggle with **new, unseen problems** or ambiguous prompts, suggesting overfitting.  
   - Example: When tested on harder benchmarks like AIME (designed for top high school math students), performance gaps emerge, raising doubts about generalizability.  

2. **Defending Model Capabilities**:  
   - Proponents claim GPT-5 rarely makes errors in high school or undergraduate-level math unless prompts are unclear. They challenge skeptics to find concrete examples of mistakes, asserting that correct answers imply functional reasoning.  

3. **Benchmark Limitations**:  
   - Critics highlight that benchmarks prioritize **correct answers** over the reasoning process, conflating task completion with understanding. Some argue benchmarks like GSM8K have become "solved" through optimization, rendering them ineffective for differentiating state-of-the-art models.  
   - Dynamic, adversarial, or held-out test sets (e.g., AIME) are proposed as better alternatives to static leaderboards.  

4. **Meta-Critique of Benchmark Culture**:  
   - Comments dismiss benchmarks as marketing tools, with users likening AI optimization for synthetic tests to "GPUs optimized for synthetic benchmarks." Others mock the hype-driven narratives around AI progress.  

**Takeaways**:  
- The discussion underscores a divide between those impressed by benchmark scores and skeptics demanding proof of underlying reasoning.  
- There’s consensus that benchmarks need rigor (e.g., leakage checks, clearer metrics) to avoid conflating memorization with intelligence.  
- The exchange reflects broader tensions in AI evaluation, balancing practical utility with scientific validity.  

**Bottom Line**: While models like GPT-5 excel at specific tasks, the debate questions whether benchmarks capture true reasoning or merely reward pattern recognition—a tension central to AI’s evolving narrative.

### Jensen Huang's Stark Warning: China's 1M AI Workers vs. America's 20k

#### [Submission URL](https://entropytown.com/articles/2025-11-06-nvidia-jensen-taipei/) | 36 points | by [chaosprint](https://news.ycombinator.com/user?id=chaosprint) | [26 comments](https://news.ycombinator.com/item?id=45852376)

What happened
- Behind closed doors in Taipei, Nvidia CEO Jensen Huang allegedly told Taiwan tech leaders that US export controls are “accelerating China,” not stopping it. He contrasted China’s AI push (“one million people working on this 24/7”) with Silicon Valley’s much smaller talent pool.
- Attendees say Huang argued Nvidia has lost roughly $15B in sales due to controls; China revenue for its most advanced products has cratered, and Nvidia took billions in charges on halted H20 shipments.
- He reportedly warned that by 2027 China could have more AI compute than the rest of the world combined, citing domestic capacity growth and aggressive localization mandates.
- Two days later, after public remarks sparked jitters, Huang softened the message, saying China is “nanoseconds behind” and the US must win by “racing ahead.”

China’s buildup, by the numbers (as reported)
- Workforce: China’s AI workforce reportedly grew to ~52k by 2024, with 30k+ active AI researchers—about double the US research base—spread across 150+ institutions publishing at scale.
- Chips: Huawei’s Ascend 910C is variously claimed at 60% of Nvidia H100 performance on inference (independent testing) versus near-parity in private chatter. SemiAnalysis projects ~805k Ascend units in 2025, mostly 910C.
- Supply chain: After tapping a TSMC “die bank,” Huawei is leaning on SMIC’s enhanced 7nm. The key bottleneck is high-bandwidth memory: CXMT’s current output supports only ~250k–300k high-end AI chips annually, but capacity is ramping.
- Policy push: Domestic AI chip share in China reportedly rose from 28% (2022) to ~65% (2025). Beijing targets 100% self-developed intelligent computing infrastructure by 2027, with subsidies flowing and local mandates (e.g., Shanghai) enforcing domestic content.
- Compute: Official projections put China’s intelligent compute at 260 EFLOPS (2022) to 1,117 EFLOPS (2027), implying rapid growth—but methodologies vary.

Why it matters
- If export controls are fueling a “national mobilization,” the US may be trading short-term denial for long-term substitution—eroding Nvidia’s China business while accelerating a domestic Chinese stack across chips, memory, and software.
- Huawei’s steady progress plus policy-backed demand could make memory, not logic, the limiting factor—shifting leverage to HBM suppliers and packaging capacity.
- A 2027 compute inflection, if realized, would redraw the global AI map and complicate allied industrial strategies.

Caveats and open questions
- The dinner remarks are leaked; some claims conflict with independent testing and public data.
- “Compute” metrics are not standardized; comparing EFLOPS across systems and workloads can mislead.
- SMIC yields, Huawei software ecosystem maturity, and HBM ramp timelines will determine how fast China can translate chips into usable AI capacity.
- US policy could tighten further (memory, EDA, cloud routing), or broaden allied coordination, altering trajectories.

What to watch next
- Nvidia’s China-tailored products and any new compliance chips.
- Huawei Ascend deployments at scale and software/toolchain adoption.
- HBM output growth (CXMT and packaging) and any new chokepoints.
- US/EU/Japan policy moves targeting memory, advanced packaging, or services.

**Summary of Hacker News Discussion on Jensen Huang’s Leaked Comments About China’s AI Progress:**

1. **Workforce Dynamics:**  
   - Skepticism arose over claims of China’s "1 million AI workers," with users noting discrepancies in metrics (e.g., China reports ~52k AI researchers vs. US’s ~30k). Some argue raw population numbers (1.4B in China vs. 340M in the U.S.) don’t equate to proportional talent, emphasizing quality, collaboration, and immigration policies (e.g., H1B visa caps) as critical factors. Others counter that China’s centralized mobilization could offset these gaps through sheer scale.

2. **Chip Performance & Supply Chain:**  
   - Huawei’s Ascend 910C performance claims (60% vs. 90% of Nvidia’s H100) were debated, with users highlighting conflicting reports and the need for independent verification. High-bandwidth memory (HBM) production emerged as a bottleneck, with CXMT’s current capacity (~300k GPUs/year) seen as insufficient to meet China’s projected 2025 targets (~805k units). SMIC’s 7nm advancements were noted, but yield and packaging challenges remain unresolved.

3. **Policy & Economic Factors:**  
   - China’s state-driven subsidies, mandates (e.g., 100% domestic AI infrastructure by 2027), and aggressive localization were seen as accelerants for self-sufficiency. Conversely, U.S. export controls were criticized as potentially counterproductive, pushing China to develop alternatives faster. Nvidia’s financial motives were questioned, with users suggesting Huang’s warnings aimed to lobby against stricter controls to protect market share.

4. **Geopolitical Strategies:**  
   - India was proposed as a potential counterbalance, leveraging its non-aligned history and growing tech sector. Others doubted this, citing India’s recent alignment with U.S. interests. The U.S.’s reliance on integrated supply chains was framed as a vulnerability, while China’s centralized model was seen as enabling rapid, large-scale deployment despite inefficiencies.

5. **Skepticism vs. Optimism:**  
   - Optimists pointed to China’s rapid STEM education growth and policy focus, predicting a 2027 compute inflection. Skeptics highlighted systemic issues: software ecosystem immaturity, housing market crises (as a distraction), and potential overestimation of chip capabilities. Some dismissed Huang’s warnings as fearmongering to justify Nvidia’s market position.

6. **Future Projections & AGI Concerns:**  
   - Discussions speculated on AI’s exponential compute demands and China’s potential to lead if it overcomes HBM/logic bottlenecks. AGI development was flagged as a strategic wildcard, with users debating whether China’s state-driven approach could outpace U.S. innovation. The 2027 timeline was contested, with critics citing methodological flaws in compute metrics (EFLOPS comparisons).

**Notable Subplots:**  
- Humor/sarcasm: Quips about AI workers “checking calculators” for basic math, or China’s housing “glut” overshadowing tech progress.  
- Cultural contrasts: Centralized policy vs. Silicon Valley’s “efficient markets,” and debates over whether quantity (China) or quality (U.S.) will dominate.  
- Meta-concerns: Whether leaked comments reflect genuine strategic insights or corporate lobbying.  

**Conclusion:** The discussion underscores deep divisions over China’s AI trajectory, balancing skepticism of its technical capabilities against admiration for its scale and policy focus. The U.S.-China rivalry is framed as a clash of systems, with export controls and talent dynamics as pivotal battlegrounds.
