## AI Submissions for Fri Jul 14 2023 {{ 'date': '2023-07-14T17:09:55.792Z' }}

### Tinygrad and rusticl and aco: why not?

#### [Submission URL](https://airlied.blogspot.com/2023/07/tinygrad-rusticl-aco-why-not.html) | 34 points | by [pantalaimon](https://news.ycombinator.com/user?id=pantalaimon) | [25 comments](https://news.ycombinator.com/item?id=36722158)

In a recent blog post, a developer shared their experience working with tinygrad, rusticl, and ACO. They started by running tinygrad on their Radeon 6700XT using rusticl with the LLVM backend and found that it could successfully run an LLM model. The developer then decided to experiment with the Mesa ACO compiler backend and compared the performance to LLVM. They found that ACO was about four times faster to compile but produced less optimized binaries. The benchmark results showed that the LLVM backend had better performance in terms of runtime and GFLOPS. The developer mentioned that they plan to investigate ROCm in the future but are currently dealing with a cold/flu.

The discussion on the submission revolves around various topics related to NixOS, tinygrad, ACO, MLIR, and Rusticl.

- Users discuss the pros and cons of using NixOS for machine learning environments, with some praising its declarative configuration and others highlighting potential challenges with managing dependencies and complexity.

- Regarding tinygrad, there is a debate about its validity and whether it is a worthwhile project. Some express skepticism and question its benchmarks, while others appreciate its simplicity and ease of installation.

- MLIR and Rusticl are also discussed. Some users comment on the increasing popularity of MLIR-based middle-layer frameworks. Rusticl's hidden positive points are mentioned, and there is interest in exploring distributions with better hardware support.

- The ACO (AMD Compiler) and Mesa OpenCL driver are mentioned, with discussions about their compatibility and performance on different hardware and Linux distributions. There are some questions about specific APUs and their OpenCL support.

Overall, the discussion includes a mix of technical insights, opinions, and experiences related to the various technologies and projects being discussed.

### GaryOS

#### [Submission URL](https://github.com/garybgenett/gary-os) | 37 points | by [skibz](https://news.ycombinator.com/user?id=skibz) | [4 comments](https://news.ycombinator.com/item?id=36730392)

Introducing GaryOS: An Entire GNU/Linux System in a Single Bootable File

GaryOS is a unique operating system that packs an entire GNU/Linux system into a single bootable file. Developed by Gary Bennett, this system is designed to be fast and flexible, making it ideal for system rescue and recovery, as well as providing an anonymous and secure workstation environment.

One of the key features of GaryOS is its optimized source-based Gentoo system with a GNU toolchain. This allows for a highly customizable and powerful operating system, as source-based distributions offer more flexibility than binary ones.

Another standout feature of GaryOS is its in-memory filesystem, which does not require a physical device. This means that the operating system can be booted from anywhere a Linux Kernel can be run, making it extremely portable.

But what really sets GaryOS apart is its ability to build new or updated versions of itself from within itself. This self-building capability allows for easy upgrades and maintenance, as replacing the existing file with a new version is all that's needed.

To get started with GaryOS, simply download the latest Kernel file and select a boot method. You can use a virtual environment for quick testing and experimentation, follow the instructions for Linux or Windows installation, configure an existing EFI bootloader, or even host it on a PXE server.

Once booted, you can log in as root with the password "p@ssw0rd!". And because GaryOS is designed to be stored on media as a resident, rather than being the purpose of it, you can detach any boot media without any issues.

GaryOS is a truly unique and innovative operating system that brings a fresh approach to GNU/Linux. Whether you're looking for a powerful and customizable system or need a reliable rescue and recovery tool, GaryOS has you covered.

The discussion on Hacker News about the submission titled "Introducing GaryOS: An Entire GNU/Linux System in a Single Bootable File" touched on a few different points. One user, with the username "hmrp," found the licensing section of the project interesting and shared a link with more information. Another user, "dsqrd," mentioned that Gary Genett is the developer and maintainer of GaryOS, and the project is a personal endeavor that has gained recognition. Finally, a user named "hypercube33" requested screenshots of the operating system, and another user, "lckhs," suggested scrolling through the page to find them.

### Building a safer FIDO2 key with privilege separation and WebAssembly

#### [Submission URL](https://benkettle.xyz/posts/plat/) | 88 points | by [bkettle](https://news.ycombinator.com/user?id=bkettle) | [14 comments](https://news.ycombinator.com/item?id=36726077)

limited set of actions. In the context of Plat, the components are the different parts of the security key, such as the USB driver and the cryptographic implementation.

By separating these components and enforcing strict access controls, Plat prevents bugs in one component from compromising the security of the entire key. For example, if there is a bug in the USB driver that allows arbitrary messages to be sent, Plat's privilege separation ensures that the driver doesn't have access to the cryptographic implementation or the secret key. This means that even if an attacker can interact with the authenticator over USB, they won't be able to access the secret key or forge cryptographic signatures without explicit user approval.

To implement privilege separation, Plat uses a WebAssembly-based toolchain that creates isolation domains on an embedded ARM platform. This toolchain allows Plat to isolate software-only libraries and control access to individual hardware peripherals. By retrofitting privilege separation onto an existing codebase, Plat minimizes the need to write new code while still achieving a significant increase in security.

Plat's approach is especially important given the growing popularity of FIDO2 and the increased attention on security keys as an attractive target for attackers. While physical security keys like the YubiKey already provide strong security, bugs in their code can still compromise the security of user accounts. Plat's privilege separation adds an extra layer of protection, ensuring that bugs in one part of the key don't lead to complete account compromise.

The implementation of Plat was part of the author's Master's thesis, which focused on creating a safer FIDO2 security key. By addressing the potential vulnerabilities of existing security keys through privilege separation, Plat offers an alternative that enhances the security and usability of FIDO2 authentication.

The discussion on this submission revolves around the concept of privilege separation and the security implications of Plat, a new approach to FIDO2 security keys. 

One commenter agrees with the benefits of this separation and suggests other approaches, such as using hardware MPUs for privilege domains. They also mention the complexity of implementing these mechanisms and suggest exploring other possibilities like hardware sandboxing.

The author of the submission joins the discussion, stating that their goal was to prevent side-channel attacks and power draw measurements. They also mention the possibility of adding additional sandboxed applications to the key to enhance security.

Further comments discuss the technical aspects of Plat, including the use of WebAssembly-based sandboxing and the control of hardware peripherals. There is also a clarification regarding the USB driver's access to memory and the complexity of managing DMA controllers.

The discussion then shifts to the potential security risks and threat models of Pla+. There is a discussion about the attacker compromising the host PC through interactive messages over USB and extracting sensitive data from the browser or logging keystrokes. The commenter suggests that in FIDO2, the private key is stored securely and highlights the benefits of reducing reliance on passwords.

Another participant wonders about account recovery with FIDO2 if they lose their hardware key and mentions the use of multiple linked accounts for validation and control.

The topic then changes to the importance of strong authentication and potential security measures for account recovery. The discussion focuses on preventing cross-site scripting attacks and resource injection and mentions the need for strong mechanisms and multi-factor authentication for account recovery.

Overall, the discussion delves into the technical aspects of Plat and explores various security considerations related to FIDO2 security keys, highlighting the importance of privilege separation and account recovery mechanisms.

### Meta to release open-source commercial AI model

#### [Submission URL](https://www.zdnet.com/article/meta-to-release-open-source-commercial-ai-model-to-compete-with-openai-and-google/) | 169 points | by [maskil](https://news.ycombinator.com/user?id=maskil) | [157 comments](https://news.ycombinator.com/item?id=36724739)

Meta, formerly known as Facebook, is gearing up to release a commercial version of its open-source large language model (LLM), LLaMA. LLaMA can generate text, images, and code using artificial intelligence (AI). The commercial release of LLaMA will enable developers and businesses to build applications using the foundational model, leading to accelerated technological innovation across various sectors. Meta's LLaMA comes in different sizes, ranging from 7 billion parameters to 65 billion parameters, surpassing OpenAI's GPT-3.5, which has 175 billion parameters. OpenAI and Google are Meta's main competitors in the AI space, and with the release of LLaMA, Meta hopes to make significant advancements in the field while addressing concerns about transparency and security associated with closed or proprietary software.

The discussion around the submission revolves around various aspects of Meta's release of LLaMA and the implications it might have. Here are some key points from the comments:

- Some users question whether the training data used for LLaMA includes illegally obtained books, which raises concerns about copyright infringement.
- Others argue that the production of knowledge is often funded through copyright licensing models, and the quality of current web content has declined, making a content-finding model like LLaMA potentially valuable.
- There is debate about the role of copyright in incentivizing content creators and the power dynamics between publishers and creators.
- Some users raise concerns about the potential misuse of AI models like LLaMA, such as generating deceptive or harmful content.
- There are discussions about the ethical implications of LLaMA, including potential abuse in generating harmful narratives or facilitating unethical practices.
- The issue of copyright and the legality of scraping internet content for training models like LLaMA is also discussed, with some arguing that it may violate copyright laws.
- There is also a discussion about the technical aspects of removing specific sources or data from the LLaMA model and the challenges associated with it.
- The potential impact of LLaMA on various industries, including journalism and content creation, is debated, with some expressing concerns about job displacement.

Overall, the discussion touches on a wide range of topics, including copyright, ethics, technical challenges, and the societal impact of AI language models like LLaMA.

### Pulling my site from Google over AI training

#### [Submission URL](https://tracydurnell.com/2023/07/11/pulling-my-site-from-google-over-ai-training/) | 46 points | by [headalgorithm](https://news.ycombinator.com/user?id=headalgorithm) | [90 comments](https://news.ycombinator.com/item?id=36727384)

Tracy Durnell, a writer and designer in Seattle, has decided to de-index her website from Google in protest against the company using the content posted on the internet to train their generative AI models. She was influenced by posts from Jeremy Keith and Vasilis van Gemert. Although Tracy admits that she doesn't know how much search traffic her website receives, she's willing to sacrifice it for her beliefs. Tracy plans to start by pulling her websites out of Google search and then work on adding her sites to directories. She has added a noindex meta tag to her WordPress header and created a robots.txt file to block bots that collect training data for AI models. Tracy's decision highlights the ethical concerns surrounding AI training with user-generated content.

The discussion surrounding Tracy Durnell's decision to de-index her website from Google in protest against the company's use of user-generated content for training AI models is varied. Some users argue that reproducing content without permission is plagiarism and a breach of copyright, while others point out that machine learning and human learning operate under different principles and assumptions. There is a debate about the subjective nature of plagiarism and copyright law, and the potential consequences of AI training. Some users argue that AI cannot perform tasks at the speed and volume of humans and will never fully replicate human capabilities, while others believe that AI will continue to evolve and may have significant consequences. There are discussions about alternative search engines, the effectiveness of blocking bots with a robots.txt file, and the role of directories and webrings in web search. Some users highlight the challenges of determining what is legal or illegal, while others argue that the legality of certain actions does not make them right or wrong.

### Meta introduces CM3leon, a more efficient image generation model

#### [Submission URL](https://ai.meta.com/blog/generative-ai-text-images-cm3leon/) | 28 points | by [envy2](https://news.ycombinator.com/user?id=envy2) | [3 comments](https://news.ycombinator.com/item?id=36723886)

CM3leon is a state-of-the-art generative AI model that can generate both text and images. It is the first multimodal model trained with a recipe adapted from text-only language models, using a combination of retrieval-augmented pre-training and multitask supervised fine-tuning. Despite being trained with five times less compute, CM3leon achieves state-of-the-art performance in text-to-image generation.

One of the key advantages of CM3leon is its versatility. Unlike previous models that could only generate either text or images, CM3leon can generate sequences of text and images conditioned on other image and text content. This expands its functionality and makes it more powerful.

CM3leon also excels in tasks such as image caption generation, visual question answering, text-based editing, and conditional image generation. It outperforms Google's text-to-image model and achieves an FID score of 4.88 on the widely used image generation benchmark, establishing a new state of the art. The model demonstrates an impressive ability to generate complex compositional objects and performs well across a variety of vision-language tasks.

With CM3leon's capabilities, image generation becomes more coherent and follows input prompts more accurately. The model can generate images that include complex objects and can edit images based on text instructions. It can also generate captions and answer questions about images.

Overall, CM3leon represents a significant advancement in generative AI models and showcases the potential of retrieval augmentation and scaling strategies in improving the performance of autoregressive models.

GaggiX commented that the Midjourney Dall-E model is pretty good at generating consistent and interesting images, especially for custom style transfer models.

In response to GaggiX's comment, lksh questioned why the submission has received 27 upvotes, suggesting that it might be due to the presence of the GitHub link.

