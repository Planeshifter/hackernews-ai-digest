## AI Submissions for Thu Jul 20 2023 {{ 'date': '2023-07-20T17:09:30.842Z' }}

### Project Aria 'Digital Twin' Dataset by Meta

#### [Submission URL](https://www.projectaria.com/datasets/adt/) | 173 points | by [socratic1](https://news.ycombinator.com/user?id=socratic1) | [89 comments](https://news.ycombinator.com/item?id=36800041)

Meta, the parent company of Facebook, has introduced the Aria Digital Twin Dataset, which aims to accelerate research in egocentric machine perception. The dataset, captured using Aria glasses, includes extensive ground-truth annotations for devices, objects, and environments. It consists of 200 sequences (~400 minutes) captured in two different locations, with annotations such as 6DoF device trajectory, 3D object pose, 3D human skeleton, and 3D eye gaze. The dataset also includes tools for loading and visualizing the data, as well as participating in object detection challenges. The Aria Digital Twin Dataset is designed to promote responsible innovation and has been captured in controlled environments with fully consented researchers. Researchers can access the dataset by providing their email address.

The discussion on Hacker News revolves around the issue of cookie consent banners and their compliance with privacy regulations such as the GDPR. Some commenters argue that the language used in the banners could be deceptive or misleading, potentially violating the GDPR. Others point out that the banners are necessary for tracking purposes and to improve content and services. There is also a discussion about the implementation of the "Do Not Track" (DNT) feature and whether it is effective or relevant in protecting privacy.  Another topic discussed is the release of the Aria Digital Twin Dataset by Meta (parent company of Facebook) and its potential impact on research in egocentric machine perception. The dataset includes annotated sequences captured using Aria glasses, providing researchers with valuable data for their experiments.

### Foundational AI models do not violate copyright

#### [Submission URL](http://marble.onl/posts/general_technology_doesnt_violate_copyright.html) | 22 points | by [andy99](https://news.ycombinator.com/user?id=andy99) | [15 comments](https://news.ycombinator.com/item?id=36807408)

In a recent article, Andrew Marble discusses the contention that AI models violate copyright because they are capable of generating copyrighted content. Marble argues that while it is possible to build an AI model that violates copyright and use broad foundation models like chatGPT to generate copyrighted content, this does not mean that the training or existence of the models themselves violate copyright. Marble equates the situation to other technologies like photocopiers or VCRs, stating that just because a model can do something doesn't mean people are necessarily utilizing it for nefarious purposes. Rather than limiting the capabilities of AI models, Marble suggests focusing on regulating the use of the technology and holding individuals accountable for any copyright infringements or other misuses. Marble also mentions the importance of distinguishing between broadly capable AI models and narrowly trained models with malicious intent. Overall, the article argues against legislating limitations on AI models at the capability level and emphasizes the need to consider how people use the technology instead.

The discussion on the submission centers around the argument made in the article and the comparison between AI models and technologies like photocopiers and VCRs. Some users agree with the article's perspective, stating that regulating the use of AI technology and holding individuals accountable for copyright infringements is more important than limiting the capabilities of AI models. Others disagree, pointing out that comparing AI to humans in terms of copyright is disingenuous, as AI does not have the same consciousness or decision-making abilities. The discussion also touches on the need to make works publicly available for training AI models and the complexities of copyright law in relation to AI-generated content. Overall, there is a division of opinions on how AI models should be regulated in terms of copyright infringement.

### Decoding the ACL Paper: Gzip and KNN Rival Bert in Text Classification

#### [Submission URL](https://codeconfessions.substack.com/p/decoding-the-acl-paper-gzip-and-knn) | 30 points | by [abhi9u](https://news.ycombinator.com/user?id=abhi9u) | [5 comments](https://news.ycombinator.com/item?id=36806577)

A recent paper presented at the ACL conference for natural language processing (NLP) has gained attention for its innovative approach to text classification. The paper shows that combining the use of gzip and K-nearest neighbour (KNN) can achieve performance comparable to state-of-the-art models like BERT. This is a refreshing perspective at a time when most research focuses on large language models. The paper's findings are explained in layman's terms in this article, with a follow-up article planned to delve deeper into the approach and its implications for NLP research. The traditional approaches to text classification, such as linear regression or neural networks, often require training on large datasets and can be computationally expensive. In contrast, the gzip and KNN approach offers a simpler and more cost-effective solution. The key steps involve compressing the input text using gzip, computing the Normalized Compression Distance (NCD) between the compressed texts, and then finding the k-nearest neighbors based on these distances. The majority class of the neighbors is then chosen as the target label. The NCD measures the amount of shared information between two texts, and if they have similar content, their concatenation will achieve higher compression. This approach leverages compression algorithms and information theory to achieve accurate text classification without the need for massive language models.

The discussion around the submission primarily revolves around the potential of the gzip and KNN approach for text classification. One user points out that the technique may overestimate performance relative to BERT, and recommends evaluating its accuracy against expected KNN compressed data. Another user suggests that the paper has the potential to start an interesting discussion on finding synonymous word classes, to which another user responds that such an approach could work using semantic modeling and embeddings. The second user also mentions that they are writing an article to explain how gzip can be helpful in text classification. One user jokingly adds that the approach wouldn't find synonyms that are closer to random strings.

### AI That Teaches Other AI

#### [Submission URL](https://viterbischool.usc.edu/news/2023/07/teaching-robots-to-teach-other-robots/) | 97 points | by [geox](https://news.ycombinator.com/user?id=geox) | [40 comments](https://news.ycombinator.com/item?id=36799073)

Researchers from the University of Southern California (USC) have developed a tool called SKILL (Shared Knowledge Lifelong Learning) that allows robots to teach each other how to learn. In a paper published in Transactions on Machine Learning Research, the team describes how AI agents learned 102 different tasks, such as categorizing images of cars or flowers, and shared their knowledge over a decentralized communication network. The robots were able to master all 102 tasks by teaching each other, reducing the time needed for learning by a significant factor. The researchers believe that this approach could be scaled up to thousands or millions of tasks, potentially transforming various industries and creating a more connected and efficient global community. They envision applications in healthcare, where AI systems could specialize in different areas of medicine and provide doctors with the most up-to-date information, as well as in tourism, where every smartphone user could become a local tour guide by sharing photos and details about different landmarks and local cuisine. The researchers see potential in using SKILL technology in any profession that requires vast knowledge or deals with complex systems.

The discussion on this submission revolves around several topics. One user highlights the challenges of individuality in AI and the potential risks associated with the development of artificial general intelligence (AGI). Another user discusses the limitations and diminishing returns of scaling up AI models. Some users express confusion about the terminology used and seek clarification on the concept of Mixture of Experts (MoE) models. The potential implications of SKILL technology in various industries, such as healthcare and tourism, are also discussed. Additionally, there is conversation about the integration of AI and human knowledge-sharing, as well as debates on the risks and benefits of AI advancements for society.

### TSMC delays Arizona factory that will eventually build chips for iPhones and AI

#### [Submission URL](https://www.theverge.com/2023/7/20/23802107/tsmc-arizona-chip-factory-delay-q2-earnings-report) | 29 points | by [rntn](https://news.ycombinator.com/user?id=rntn) | [10 comments](https://news.ycombinator.com/item?id=36806863)

Taiwan Semiconductor Manufacturing Company (TSMC) has announced that the start of chip production at its new facility in Arizona, which is intended to manufacture chips for iPhones and AI, will be delayed to 2025 due to labor shortages. The company's first fab in Phoenix was initially scheduled to begin producing 4nm chips next year, but TSMC is now facing challenges in finding skilled workers for equipment installation. To make up for lost time, TSMC plans to send experienced technicians from Taiwan to train local workers at the plant. TSMC's Q2 earnings report also showed a 10% decline in revenue and a 23% decline in profits compared to the same period last year, with a projected 10% revenue drop for the full year. The company expects the capacity shortage caused by high demand for AI-capable chips to persist until next year. TSMC is working with the US government to maximize subsidies and tax credits available to cover the increased costs of fabricating in the US.

The discussion on this submission revolves around various aspects of the delay in chip production at TSMC's new facility in Arizona, along with the company's financial situation. Here are some key points from the comments:

- Some users suggest that TSMC could have started training programs to address the labor shortage earlier.
- A user mentions that skilled friends who used to work in chip fabs have moved to software jobs, as the latter is considered to have higher salaries and fewer hazards.
- Another user points out that certain fields require high skills, yet their compensation is little compared to software-related work.
- There is a discussion about TSMC's profitability, with some users emphasizing the company's success and others highlighting the significant profits it generates and the higher salaries it pays to employees.
- A user shares a link to an article discussing TSMC's financials, including its $33 billion profit.
- One user mentions that while the delay in chip production is unfortunate, TSMC should have started comprehensive training programs for workers earlier.
- There is a request for further discussion on this topic, with a link to an article on The Verge being shared.

Note: The conversation contains a few replies marked as "dd" or "dp", without further context or content.

### 9k authors say AI firms exploited books to train chatbots

#### [Submission URL](https://www.latimes.com/entertainment-arts/story/2023-07-19/artificial-intelligence-9000-authors-sign-letter-rebuking-ai-companies-books) | 28 points | by [webmaven](https://news.ycombinator.com/user?id=webmaven) | [12 comments](https://news.ycombinator.com/item?id=36807296)

More than 9,000 authors have signed an open letter calling out tech companies behind generative AI for exploiting copyright-protected works without consent, credit, or compensation. The authors argue that these companies are using their writings to train chatbots, such as GPT-4 and ChatGPT, to summarize and imitate their works. The open letter specifically calls out tech giants like OpenAI, Alphabet, Meta, IBM, and Microsoft, urging them to obtain permission and fairly compensate authors for the use of their works in generative AI programs and outputs. The Authors Guild highlights the threat that generative AI poses to writers' professions, as it floods the market with machine-written content and contributes to a decline in authors' income. This comes shortly after bestselling authors Mona Awad and Paul Tremblay filed a lawsuit against OpenAI, claiming that ChatGPT was trained using portions of their novels without their consent. Both the open letter and the lawsuit shed light on the ethical and legal questions surrounding the use of copyrighted material in AI technology.

The discussion on this submission revolves around various aspects of copyright law, fair use, and the impact of generative AI on the distribution of content.  One commenter raises the point that purchasing a book does not necessarily grant the purchaser the right to modify or use the content in AI systems. They argue that there is a distinction between creating a parody or satire, which may be protected as fair use, and using copyrighted works without permission in generative AI. Another commenter questions whether embedding AI crops into AI systems through modification would qualify as fair use. They further discuss the complexity of copyright claims and the applicability of fair use factors in determining the legality of using copyrighted works. A reply to this comment emphasizes the importance of the four factors considered in fair use cases: the purpose and character of the use, the nature of the copyrighted work, the amount and substantiality of the portion used, and the effect on the potential market for the copyrighted work. They also mention that the commercial nature of the AI applications and the potential harm to the market for original works could be significant factors to consider. In response to a comparison made between modifying the Mona Lisa and using AI-generated crops, another commenter mentions a recent Supreme Court decision involving Warhol and Goldsmith. They note that the decision clarified the high level of commerciality necessary to define copying as illegal for derivative works. The discussion then delves into the proprietary nature of AI algorithms and the commercial rush to capitalize on generative AI. The commenter suggests that the AI community is protecting their proprietary interests and refusing to engage with AI regulation. One commenter expresses skepticism about prevailing legal arguments in the court proceedings and hopes that people will learn to synthesize new content based on existing works.

