## AI Submissions for Wed Aug 30 2023 {{ 'date': '2023-08-30T17:10:19.632Z' }}

### Show HN: I automated half of my typing

#### [Submission URL](https://github.com/eschluntz/compress) | 738 points | by [eschluntz](https://news.ycombinator.com/user?id=eschluntz) | [298 comments](https://news.ycombinator.com/item?id=37326870)

"Compress" is a tool that automates the process of creating keyboard shortcuts from a corpus of your own writing. It analyzes the text to find common phrases that can be replaced with shorter abbreviations, and then generates configuration files for Autokey, a linux program that implements keyboard shortcuts. The tool also includes an optional feature to parse a Slack Data Export of your messages and create a corpus from it. The suggested abbreviations are ranked by the amount of characters saved multiplied by the frequency of the phrase. The tool aims to generate memorable abbreviations using heuristics and preferences. It provides instructions on how to install and use the tool, and offers customization options for adding or editing shortcuts. Overall, "Compress" simplifies typing by reducing the need to type out frequently used phrases.

The discussion around the submission "Compress" includes various comments and suggestions from the Hacker News community. One user mentions that there is already a built-in Chinese language typing system called Shuangpin, which allows users to type English characters that correspond to whole words in Chinese. The user also suggests that there are other typing systems and shorthand methods available for different languages. Another user raises concerns about the tool's longevity and suggests that a universal version could be more useful. They propose that the tool should be able to process books, emails, text messages, and other sources of diverse backgrounds and contexts to benefit a wider range of people.

A discussion thread also explores the limitations of shorthand systems and abbreviations. One user points out that common shorthand systems, like Evans Basic English Code or Phillips Code, are often insufficient in practice. Another user mentions that certain abbreviations, like dictionary abbreviations or abbreviations for frequently used words, can be considered. There are also comments discussing the practicality of using the tool on Android phones, with some users mentioning the difficulties they face while typing and suggesting alternative approaches to improve typing speed. Some users share their experiences with typing speed and discuss various factors that can impact typing efficiency. The discussion touches on factors such as keyboard ergonomics, typing techniques, and the benefits of specialized keyboards or typing systems. The topic of stenography is also brought up, with users recommending the Open Steno Project as a resource for those interested in shorthand and high-speed typing.

Overall, the discussion revolves around the potential benefits and challenges of the "Compress" tool, as well as broader topics related to typing efficiency and alternative typing methods.

### High-Speed AI Drone Overtakes World-Champion Drone Racers

#### [Submission URL](https://www.news.uzh.ch/en/articles/media/2023/Drone-race.html) | 275 points | by [geox](https://news.ycombinator.com/user?id=geox) | [167 comments](https://news.ycombinator.com/item?id=37323834)

Researchers from the University of Zurich and Intel have achieved a major milestone in artificial intelligence by creating an autonomous system capable of beating human champions at a physical sport: drone racing. The AI system, called Swift, won multiple races against three world-class drone racing champions. Swift was trained using reinforcement learning in a simulated environment, where it taught itself to fly by trial and error. The drone flies autonomously in real time, reacting to data from an onboard camera, just like human racers. Swift achieved the fastest lap overall, half a second ahead of the best lap by a human pilot. However, human pilots proved to be more adaptable to changing conditions. The researchers believe that pushing the envelope in autonomous flight is important not only for drone racing, but also for applications such as forest monitoring, space exploration, and rescue missions.

The discussion on this submission covers various aspects of the achievement of the AI system Swift beating human champions in drone racing. Some commenters discuss the technical aspects of training the AI using reinforcement learning and the complexity of modeling the dynamics of drones. Others highlight the challenges of simulating real-world physics accurately and the importance of learning from real-world data. There is also a mention of past achievements in robotics and AI in drone racing, as well as some humor about AI developing winning AI pilots. The limitations of the AI system are also mentioned, such as localization and external system mapping. Overall, the discussion shows excitement about the progress made in autonomous drone racing.

### ChatLZMA â€“ text generation from data compression

#### [Submission URL](http://pepijndevos.nl/2023/07/15/chatlmza.html) | 117 points | by [bschne](https://news.ycombinator.com/user?id=bschne) | [18 comments](https://news.ycombinator.com/item?id=37318810)

A programmer on Hacker News shared an interesting idea on building ChatGPT using data compression techniques. They proposed compressing a large corpus of text to create an encoding table, compressing a prompt along with some random data, and then decompressing the random data to generate a response. While the approach may seem unconventional, the initial results were surprisingly amusing, producing coherent-ish words in mere seconds of "training." The programmer shared some code snippets in Python showcasing their progress so far. Although they are unsure whether the generated words are accidental prompts or just the result of flushing the buffer, the output is intriguing nonetheless. This experiment highlights the potential for leveraging compression algorithms in natural language processing tasks.

The discussion on the Hacker News submission revolves around the idea of using data compression techniques to build ChatGPT, a conversational AI model. Some comments express skepticism about compressing text and the effectiveness of the approach. Others mention similar compression-based approaches in different domains such as gaming FAQs and network data.  One commenter suggests building a context-token probability table using SQLite and suggests the use of prediction and learning algorithms to improve efficiency. Another commenter introduces the concept of Weighted Finite State Transducers for speech recognition. There is also a mention of GPT-40, a model that uses base64 compression as a potential starting point for implementing compression in ChatGPT. Some commenters share related resources, including a paper on Parameter-Free Classification Method Compressors and a YouTube video demonstrating simple implementations. Others express interest in exploring the idea further, with one commenter mentioning the possibility of applying the approach to Gulliver's Travels or Finnegans Wake. Overall, the discussion explores the potential benefits and challenges of incorporating data compression techniques into ChatGPT, with varying levels of enthusiasm and skepticism.

### Designing deep networks to process other deep networks

#### [Submission URL](https://developer.nvidia.com/blog/designing-deep-networks-to-process-other-deep-networks/) | 69 points | by [weird_science](https://news.ycombinator.com/user?id=weird_science) | [9 comments](https://news.ycombinator.com/item?id=37328609)

Researchers have explored the idea of designing deep neural networks (DNNs) that can process the weights of other neural networks, enabling them to perform operations on pretrained models. This concept has practical applications, such as editing 3D objects represented using Implicit Neural Representations or adapting image classifiers to different domains. Previous work has attempted to process deep weight spaces by vectorizing the parameters and applying a fully connected network. However, this approach overlooks the structural properties of neural networks and hurts generalization. To address this, researchers propose using deep architectures that are insensitive to transformations of model weights, similar to how convolutional neural networks are insensitive to image shifts. The study draws upon the field of Geometric Deep Learning, which focuses on learning objects while being invariant to transformations, to develop equivariant architectures. These architectures enable neural networks to effectively process the weights of other neural networks, creating opportunities for more efficient and flexible model operations.

The discussion on this submission begins with a comment by "Culonavirus" stating that the idea of designing deep neural networks that can process the weights of other neural networks is interesting. They mention that Nvidia's recent projects are pushing the boundaries of reconstructing 3D objects using deep learning.

In response, "gbrsr" expresses their surprise at Nvidia's hardware and software collaboration, suggesting that Nvidia is leveraging its strong chips for efficient computation, which they find impressive.

Another user, "rvz," comments that while the post explains the lack of technical questions in the responses, it takes experiments and reimplementation of the findings in papers to fully understand the research. They note that posting non-code explanations generates little interest.

"Pzz" finds the perspective interesting and connects it to a method called "Dynamic Dispatch Type Overloading" in programming. They provide an example to illustrate their point and mention their contribution in implementing a library for optimizing overall distortion in approximation.

"Lbbt" suggests that Nvidia's management is doubling down on AI chips, leading to exponentially increasing demand.

A user with the handle "wht-n-tsts" references the novel "Neuromancer" and its antagonist "Wintermute" in their comment.

"Blammar" responds to "wht-n-tsts" by suggesting that controlling chips that can self-assemble, like those described in the book, has been a long-standing desire for humans. They mention the challenges of self-manufacturing in vacuum tube systems and their resilience in space environments.

Overall, the discussion touches on Nvidia's projects, the implementation challenges, and references to sci-fi novels.

### ThirdAI Uses Ray for Parallel Training of Billion-Parameter NN on Commodity CPUs

#### [Submission URL](https://www.anyscale.com/blog/how-thirdai-uses-ray-for-parallel-training-of-billion-parameter-neural-networks-on-commodity-cpus) | 77 points | by [thirdailab](https://news.ycombinator.com/user?id=thirdailab) | [15 comments](https://news.ycombinator.com/item?id=37325173)

In a blog post by ThirdAI, they share how they utilize the Ray framework for distributed training of deep learning and foundational models using only CPUs. ThirdAI is an early-stage startup dedicated to democratizing AI models by training and deploying large neural networks on commodity CPU hardware. Their proprietary BOLT engine, a new deep learning framework, allows their sparse deep learning models to outperform dense architectures on GPUs in both training time and inference latency. To scale their models to terabyte-scale datasets and billion-parameter models, they have built a distributed data parallel engine powered by Ray. They discuss how Ray enables them to build an industry-grade distributed training solution with features such as fault-tolerance, multiple modes of communication, and seamless scalability. They also highlight their recent migration from Ray Core to Ray Trainer, which provides a simplified developer experience and enhanced fault tolerance and automatic scaling. With Ray, ThirdAI is able to achieve near-linear scaling for distributed training on a popular terabyte-scale benchmark dataset.

