## AI Submissions for Mon Jun 16 2025 {{ 'date': '2025-06-16T17:12:43.745Z' }}

### OpenAI wins $200M U.S. defense contract

#### [Submission URL](https://www.cnbc.com/2025/06/16/openai-wins-200-million-us-defense-contract.html) | 278 points | by [erikrit](https://news.ycombinator.com/user?id=erikrit) | [240 comments](https://news.ycombinator.com/item?id=44293988)

In a significant move for AI and defense collaboration, the U.S. Defense Department has awarded OpenAI a $200 million contract to enhance national security with frontier artificial intelligence models. This partnership, named "OpenAI for Government," marks a new milestone for the company as it ventures into national defense arenas, aiming to prototype AI capabilities that could revolutionize warfighting and administrative operations for the military.

The contract, announced on Monday, will see most of the work taking place around Washington, D.C., and involves collaboration with Anduril, a defense tech startup that received its own $100 million contract last December. This aligns with recent industry trends where AI and tech companies are increasingly engaging with national defense projects. For instance, Anthropic is working with Palantir and Amazon for similar purposes.

OpenAI's involvement will focus on developing AI tools that streamline military operations and improve service delivery, such as healthcare for service members and proactive cyber defenses—all while adhering to OpenAI's usage policies.

OpenAI's co-founder and CEO, Sam Altman, emphasized the importance of their role in national security during a discussion at Vanderbilt University. The initiative underscores OpenAI's commitment to providing the government with cutting-edge AI solutions while expanding its influence and capabilities in the public sector.

Moreover, as part of a broader effort to bolster AI infrastructure within the U.S., OpenAI is also involved in the $500 billion Stargate project, aiming to expand computing power domestically. With the company already generating over $10 billion in annual sales, this contract represents a strategic but smaller fraction of its overall operations.

As this contract kicks off, the collaboration between OpenAI and the U.S. Defense Department could set a precedent for future public-private partnerships in the burgeoning field of artificial intelligence and defense technology.

**Summary of Hacker News Discussion:**

1. **Tolkien-Inspired Defense Companies:**  
   Users mock defense tech firms like **Anduril** and **Palantir** (both named after elements from Tolkien’s works), suggesting their involvement in military projects reflects a disconnect between whimsical branding and the grim reality of warfare. Critics argue that such partnerships often prioritize corporate profits over ethical considerations or public benefit.

2. **Debate Over $200M Contract’s Significance:**  
   - Many dismiss the $200M contract as a trivial expense within the **$1 trillion annual military budget** (just 0.02%), comparing it to “30 cents in a $3 transaction.”  
   - Others counter that even small amounts reflect systemic waste and “theater,” arguing contracts like these funnel taxpayer money into private hands with minimal accountability. The **Stargate project** ($500B for AI infrastructure) is cited as a larger example of unchecked spending.

3. **Government Inefficiency & Corruption Concerns:**  
   - Users criticize the DoD’s reliance on outdated software (e.g., Excel, PowerPoint) and question the wisdom of funding AI projects before addressing basic inefficiencies.  
   - Some allege widespread corruption, with contracts serving as tools for “rent-seeking” by politically connected firms. References to **Peter Thiel** (co-founder of Palantir) and the “military-industrial complex” underscore fears of a self-perpetuating system.

4. **Broader Military Spending Critiques:**  
   - Comments highlight a paradox: despite massive budgets, projects often fail to deliver proportional value. The **Broken Window Fallacy** is invoked to critique spending that generates activity without meaningful returns.  
   - Comparisons to India’s cost-effective space program ($1B annually) contrast with perceived U.S. bloat, though defenders argue cutting-edge tech is necessary for national security.

5. **Mixed Perspectives on AI’s Role:**  
   - While some view AI as essential for modern defense (e.g., cyber warfare, logistics), others see OpenAI’s involvement as symbolic of Silicon Valley’s complicity in “privatizing gains while socializing risks.”  
   - A vocal minority defends the contract as pragmatic, emphasizing that even small investments in AI could yield strategic advantages.

**Overall Sentiment:**  
The discussion leans skeptical, with users questioning the ethics, efficacy, and oversight of military-tech partnerships. However, there is acknowledgment of the complex trade-offs between innovation, national security, and fiscal responsibility. Memes and shorthand (e.g., “Lt. Col PowerPoint commissions”) reflect frustration with bureaucracy, while broader critiques of corporate influence in government recur throughout.

### Breaking Quadratic Barriers: A Non-Attention LLM for Ultra-Long Context Horizons

#### [Submission URL](https://arxiv.org/abs/2506.01963) | 64 points | by [PaulHoule](https://news.ycombinator.com/user?id=PaulHoule) | [23 comments](https://news.ycombinator.com/item?id=44292601)

A groundbreaking paper has just emerged from the world of machine learning, authored by Andrew Kiruluta, Preethi Raju, and Priscilla Burity, tackling one of the biggest hurdles in large language models (LLMs). Traditional Transformers, which have been the backbone of recent AI developments, suffer from quadratic memory and computational constraints due to their reliance on self-attention mechanisms. This new research, intriguingly titled "Breaking Quadratic Barriers: A Non-Attention LLM for Ultra-Long Context Horizons," proposes a fresh architecture that breaks free from these limitations.

Their approach completely skips token-to-token attention, instead using state space blocks inspired by S4 to learn continuous time convolution kernels, enabling near-linear scaling with sequence length. The model also uses multi-resolution convolution layers to capture local context at varying dilation levels, alongside a lightweight recurrent supervisor that maintains a global hidden state. Complementing these are retrieval-augmented external memory mechanisms that store and recall high-level chunk embeddings without backtracking into quadratic operations.

This innovative model provides a remarkable breakthrough for LLMs, potentially handling context windows comprising hundreds of thousands to millions of tokens efficiently. It's a significant leap forward for applications that demand processing stunningly vast swaths of text, pushing the boundaries of what's possible in natural language processing. This work, now accessible on arXiv, could shape the next generation of language models and inspire further research into non-attention-based architectures.

Here’s a concise summary of the Hacker News discussion about the non-attention-based LLM paper:

### Key Discussion Points:
1. **Cautious Optimism**:  
   Users acknowledge the **potential** of replacing quadratic self-attention with linear-time alternatives (e.g., state-space models, FFT-based methods) to handle **million-token contexts**. However, many stress the need for **large-scale experiments** to validate real-world performance, as current results show only **minor improvements** in metrics like perplexity.

2. **Practical Bottlenecks**:  
   Critics argue that **memory and compute** during inference (e.g., MLP layers, KV-caching) often dominate costs more than self-attention. Even with linear scaling, handling ultra-long sequences may remain impractical due to **fixed batch costs** or hardware constraints.

3. **Attention vs. Non-Attention Tradeoffs**:  
   While avoiding token-to-token attention reduces complexity, some speculate the model may still **implicitly use attention** in hybrid architectures (e.g., DeepSeek’s design). Others note that preserving attention-like mechanisms might be crucial for **reasoning tasks** (e.g., QA, summarization), where existing attention models still excel.

4. **Comparisons & Alternatives**:  
   Users cite similar efforts like **Mamba, MesaNet, and Gated DeltaNet**, suggesting non-attention architectures are a growing trend. However, commercial models (Gemini, GPT-4) already support million-token windows, while open-source alternatives (Llama 3/4) lag behind.

5. **Paper Criticisms**:  
   Skepticism arose about the paper’s **vagueness**, lack of technical detail, and possible reliance on **AI-generated text**. Critics demand clearer explanations of the architecture, ablation studies, and comparisons to baseline models.

---

### Notable Technical Debates:
- **KV-Caching**: Linear-time key-value caching (O(N)) vs. FFT-based attention (O(N log N)) as alternatives to quadratic scaling.  
- **Hybrid Models**: Whether "attention-free" architectures secretly reintroduce attention in specific layers (e.g., chunk-boundary mechanisms).  
- **Cost Dynamics**: Debates on whether ultra-long contexts will remain prohibitively expensive even with linear scaling, due to **hardware or memory bandwidth limits**.

### Final Takeaway:  
While the work is seen as a **promising research direction**, the community urges more empirical validation, transparency, and comparisons to state-of-the-art attention variants before declaring a breakthrough.

### Nanonets-OCR-s – OCR model that transforms documents into structured markdown

#### [Submission URL](https://huggingface.co/nanonets/Nanonets-OCR-s) | 338 points | by [PixelPanda](https://news.ycombinator.com/user?id=PixelPanda) | [75 comments](https://news.ycombinator.com/item?id=44287043)

Hacker News enthusiasts, brace yourselves for an innovative leap in OCR technology! Meet Nanonets-OCR-s, a cutting-edge image-to-markdown optical character recognition model that's redefining how documents are processed. Unlike traditional OCR systems that merely extract text, this state-of-the-art model delves deeper, transforming documents into structured markdown with sophisticated content recognition and semantic tagging.

**Key Features:**
- **LaTeX Equation Recognition**: Automatically converts complex mathematical expressions into cleanly formatted LaTeX syntax, distinguishing between inline and display equations.
- **Intelligent Image Description**: Collates structured `<img>` tags to describe various images, ensuring seamless processing for Large Language Models (LLMs).
- **Signature and Watermark Handling**: Precisely identifies and isolates signatures, while watermark text is discerned and tagged for definitive handling of legal documents.
- **Smart Checkbox Management**: Transforms form checkboxes into standardized Unicode symbols to streamline data handling.
- **Complex Table Parsing**: Extracts detailed tables with precision, rendering them into markdown and HTML formats for versatile applications.

The model, available on Hugging Face, is optimized for downstream processing by LLMs and comes packed in a hefty, yet efficient, 3.75 billion parameter structure. It’s designed with user-friendly interfaces that support vLLM and docext integrations, ensuring seamless adoption into various workflows.

Importantly, this tool isn't just about technological prowess; it's about facilitating human-computer interactions by enabling LLMs to comprehend complex document structures more naturally. With last month seeing over 18,000 downloads, the demand for this tool is clear, underscoring the value of advanced document processing in our data-driven world.

Curious to explore further? Their full announcement and demo usage are live, offering developers a hands-on experience of this powerful tool. As the field of AI continues to evolve, models like Nanonets-OCR-s are paving the way for richer, more nuanced interactions between technology and human users.

**Summary of Discussion:**

The Hacker News community showed enthusiasm for **Nanonets-OCR-s**, particularly its ability to convert complex documents into structured Markdown with features like LaTeX equation recognition and table parsing. However, several themes emerged:

1. **Technical Insights & Limitations**:  
   - Users debated the model's **accuracy and scalability**—whether larger models would improve performance.  
   - Concerns about **hallucinations in OCR output** were raised, with examples like nonsensical page numbers (e.g., "1000000000000") and odd formatting.  
   - The **3B parameter model's capabilities** (based on Qwen25-VL-3B) were noted, but some highlighted inherent hallucination limitations.  

2. **Practical Applications**:  
   - Highlighted use cases included processing **legal documents** (signature/watermark extraction), **academic papers** (equations), and **magazine layouts** with varying text angles.  
   - A user working on translating a **Shipibo indigenous language dictionary** shared struggles with formatting, suggesting potential utility for Nanonets-OCR-s.  

3. **Integration & Alternatives**:  
   - Discussions emphasized integration with **LLMs** and structured formats (e.g., JSON, XML/TEI for academic publishing).  
   - Comparisons with tools like **DatalabMarker**, **Marker**, and **dclng** surfaced, with users sharing test results (e.g., successfully parsed LaTeX equations).  
   - **MyST Markdown** was recommended for handling footnotes and structured content in academic contexts.  

4. **User Experiments & Feedback**:  
   - A PowerShell script for local PDF conversion was shared, though noted as slow on older GPUs.  
   - Users tested the model on complex tables and multilingual content (e.g., Latin phrases), with mixed but promising results.  

5. **Requests & Challenges**:  
   - Non-English text handling (e.g., Cyrillic, Asian scripts) was flagged as an area needing improvement.  
   - Some desired better handling of legacy formats (Word, PowerPoint) beyond basic OCR.  

Overall, the discussion reflects optimism about Nanonets-OCR-s’s potential but underscores the need for robust performance in edge cases and broader language support. The blend of technical scrutiny, real-world applications, and tool comparisons highlights the community’s push for practical, reliable document-processing AI.

### Jokes and Humour in the Public Android API

#### [Submission URL](https://voxelmanip.se/2025/06/14/jokes-and-humour-in-the-public-android-api/) | 283 points | by [todsacerdoti](https://news.ycombinator.com/user?id=todsacerdoti) | [166 comments](https://news.ycombinator.com/item?id=44285781)

In the intricate dance of Android development, sometimes humor shines through in unexpected places. A recent dive into the public APIs has uncovered a treasure trove of whimsical touches, whispered inside jokes, and Easter eggs that nudge developers with a wink and a nod. Here’s a peek into some of the quirkiest elements that remind us that even tech giants like Google know how to have fun.

**Monkeying Around with `isUserAMonkey()`**  
Originally sounding like a line from a comedic script, `ActivityManager.isUserAMonkey()` isn't entirely a joke. This method returns true if an UI test tool, the Monkey, is currently active. It originated from a mishap during Android's development when errant inputs from the Monkey dialed 911. This light-hearted but useful feature was incorporated to prevent similar incidents by restricting certain actions when testing.

**A Goat-y Reference in `isUserAGoat()`**  
On the more frivolous side, this method playfully checks whether the user is, well, a goat. Originally it always returned false until Android Lollipop, aligning its behavior with the craze around the Goat Simulator game. It would detect the game’s presence and remarkably return true, preserving goat dignity even further. As app privacy became stricter in Android 11, this method reverted to a constant false, hinting at serious data protection—or goat privacy, as it were.

**Fun is Disallowed with UserManager.DISALLOW_FUN**  
Adding to the comedic flair, there's a policy constant, `UserManager.DISALLOW_FUN`, hilariously described in documentation as preventing the user from experiencing joy on the device. Although a legitimate policy to control device usage, the tongue-in-cheek description suggests that Android developers occasionally take a page from the playbook of GLaDOS, the snarky AI from Portal.

**It's the Final Countdown**  
One of the hidden gems in Android Oreo’s API was a method `Chronometer.isTheFinalCountdown()`, audaciously opening YouTube to play Europe’s classic hit "The Final Countdown." This playful intrusion of rock nostalgia into programming underscores a delightful irreverence, embracing both technology and culture narratives.

**Jazz Hands and Multitouch Jazz**  
Devices capable of tracking five touch inputs are referenced by the charmingly named constant `PackageManager.FEATURE_TOUCHSCREEN_MULTITOUCH_JAZZHAND`. This humorous nod to Jazz hands dances its way back to the Gingerbread days, illustrating Android's long history of lighthearted creativity.

**Logging Woes with `Log.wtf()`**  
To express utter disbelief or unexpected failure, there's `Log.wtf()`, cheekily expanded to "What a Terrible Failure." It logs anomalies so catastrophic that the developers might as well say "what the … you know the rest."

**Hosts and KThx**  
Finally, the method `AdapterViewFlipper.fyiWillBeAdvancedByHostKThx()` comes with an eye-catching casual identifier, reminiscent of informal internet exchanges. Although its function is quite straightforward, the naming choice remains a delightful Easter egg for techies who enjoy a little chuckle in their code.

These delightful snippets of humor hidden within Android's APIs offer a rare insight into the personalities of its creators. As changes in technology and privacy continue to drive evolution, these amusing elements reflect a balance between utility and a shared joy found in the ever-evolving tech landscape.

**Summary of Discussion:**

The discussion expands on the original post's theme of humor in APIs, sharing examples from various systems and debating the balance between levity and professionalism:

1. **Humorous Naming Across Tech:**  
   - React's `__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED` and extensions like `..._INTO_THE_SUN` illustrate playful warnings.  
   - Haiku OS’s `is_computer_on` and `is_computer_on_fire`, along with Delphi’s `EProgrammerNotFound` exception, showcase dry humor in documentation.  
   - Legacy references like X11’s `party_like_its_1989` variable and Android’s `runWithScissors()` method highlight historical whimsy.  

2. **Security Concerns:**  
   - Hidden Easter eggs in public APIs or undocumented code can introduce risks. One user shared a story where a hidden music-playing feature (triggered by a key combo) was mistaken for a backdoor, highlighting the fine line between fun and vulnerabilities.  
   - Security teams often discourage such elements to avoid expanding the attack surface.  

3. **Anecdotes & Cultural Impact:**  
   - A contractor’s tale recalled a project where a crude template filename (`MOOL.bs`) led to client confusion and a security audit scare.  
   - Chrome’s task manager once listed "teleported goats" in its columns, amusing users until removed.  

4. **Balancing Humor and Professionalism:**  
   - Some argue lighthearted names (e.g., `DISALLOW_FUN`) humanize code, while others note they risk confusion or maintenance issues if overly cryptic.  
   - Interview banter asked whether humor belongs in professional settings, with replies split between "fun engages" and "security trumps all."  

5. **Historical Context:**  
   - The origin of Android’s `isUserAMonkey()` traces back to Mac’s "Monkey" testing tool, emphasizing legacy tech humor.  

**Takeaway:** While playful elements foster camaraderie and creativity, they require careful consideration for maintainability and security—proving that even in code, humor walks a tightrope between delight and diligence.

### Accumulation of cognitive debt when using an AI assistant for essay writing task

#### [Submission URL](https://arxiv.org/abs/2506.08872) | 294 points | by [stephen_g](https://news.ycombinator.com/user?id=stephen_g) | [258 comments](https://news.ycombinator.com/item?id=44286277)

A groundbreaking study featured on arXiv examines the neural and behavioral impacts of using large language models (LLMs) like ChatGPT in essay writing. Conducted by Nataliya Kosmyna and her team, the research highlights a potential downside to relying on AI assistants: the accumulation of cognitive debt. Participants were divided into LLM, Search Engine, and Brain-only groups for three sessions, with some swapping roles in a fourth session. EEG analyses revealed that LLM users demonstrated the weakest brain connectivity, while Brain-only participants showed the strongest. Interestingly, when LLM users were switched to the Brain-only task, they exhibited reduced cognitive engagement, suggesting a concerning decline in mental agility. This study uncovers the cognitive costs of LLM convenience, raising serious questions about the educational implications of becoming too dependent on AI tools. It emphasizes the need for a careful reevaluation of AI's role in learning environments, as consistent reliance on LLMs might impair neural, linguistic, and behavioral performance over time. This study prompts a more profound investigation into how AI shapes our learning processes and cognitive health.

The discussion surrounding the study on LLMs' cognitive impacts reveals a nuanced debate about the role of AI in communication, education, and critical thinking. Key points include:

1. **Cognitive Decline Concerns**:  
   Critics argue that over-reliance on LLMs risks fostering shallow thinking, reduced problem-solving depth, and "cognitive debt." Users like **mjbrgss** warn that substituting human reasoning with algorithms erodes adaptability and decision-making skills, particularly in educational settings where students may bypass critical analysis.

2. **Business Communication Dynamics**:  
   Participants debate the value of human vs. AI-generated communication. **cddck** and **bonoboTP** highlight underrated skills like crafting persuasive narratives, tailoring arguments for technical/non-technical audiences, and navigating social dynamics. They note that even talented engineers often struggle with clear communication, emphasizing the irreplaceable role of human nuance.

3. **LLMs as Tools, Not Replacements**:  
   While **je42** praises LLMs for improving writing efficiency (e.g., grammar corrections, structural suggestions), others caution against overuse. **byndrh** warns that tools like Grammarly can strip writing of personal style, and **bsnftnr** stresses that AI should augment—not replace—critical thinking or Socratic learning methods.

4. **Institutional Complacency**:  
   **mjbrgss** and **Al-Khwarizmi** discuss how institutions might misuse LLMs for bureaucratic tasks, prioritizing speed over depth. This could entrench superficial processes, reducing incentives for rigorous analysis unless external pressures demand it.

5. **Social and Educational Trade-offs**:  
   **hnsmyr** critiques real-time LLM use in conversations, noting it leads to formulaic, "bullet-point" interactions that lack depth. Meanwhile, **Ekaros** hints at broader societal implications of outsourcing communication to AI.

**Conclusion**: The discussion underscores a tension between efficiency gains from LLMs and the potential erosion of human cognitive and communicative depth. While AI tools offer practical benefits, participants stress the need to preserve critical thinking, creativity, and the uniquely human ability to navigate complex social and intellectual landscapes.

### Salesforce study finds LLM agents flunk CRM and confidentiality tests

#### [Submission URL](https://www.theregister.com/2025/06/16/salesforce_llm_agents_benchmark/) | 142 points | by [rntn](https://news.ycombinator.com/user?id=rntn) | [84 comments](https://news.ycombinator.com/item?id=44289554)

A recent study led by Salesforce's AI researcher Kung-Hsiang Huang has unveiled notable shortcomings in Large Language Model (LLM) agents when it comes to performing standard CRM tasks and safeguarding customer confidentiality. Despite the potential high-margin opportunities AI agents represent for CRM vendors like Salesforce, the research highlighted through a new benchmark tool, CRMArena-Pro, that LLM agents have about a 58% success rate for single-step tasks and a dismal 35% for complex, multi-step ones. Moreover, these AI agents struggle with understanding the importance of confidential data, which dampens their task performance despite improvements through targeted prompting.

This revelation underscores a gap between AI capabilities and real-world enterprise demands, suggesting that organizations should tread carefully before fully banking on AI enhancements. While efficiency-driven entities like the UK government aim to leverage AI for substantial savings, proving tangible benefits remains crucial. As the landscape develops, stakeholders are urged to measure AI's potential realistically, not just as an efficiency booster but also concerning its readiness for handling sensitive data responsibly. Stay tuned as AI integration continues to evolve, promising yet unexplored territories for both vendors and users.

The discussion revolves around the practical challenges and ethical concerns of deploying LLMs in enterprise CRM tasks, as highlighted by Salesforce's study. Key points include:

1. **Technical Limitations**:  
   - LLMs show modest success rates (58% for single-step tasks, 35% for multi-step), sparking debates about whether these metrics reflect a "coin-flip" reliability.  
   - Participants question benchmarks, with analogies like drawing marbles from a jar to explain statistical success probabilities. Skepticism arises about whether current performance justifies enterprise reliance.  

2. **Data Privacy & Ethics**:  
   - Confidentiality risks are noted, with suggestions like RAG layers to mitigate data exposure.  
   - A heated sub-thread debates data scraping legality, contrasting terms like “stealing” vs. “unlicensed use.” Critics argue LLM training on public data may infringe copyrights, while others dismiss this as semantic pedantry.  

3. **Model Behavior**:  
   - Concerns about LLMs’ verbosity (e.g., Gemini) and tendency to “hallucinate” or persist in conversations, leading to false promises or misleading answers. Examples include chatbots offering incorrect discounts.  

4. **Industry Realism vs. Hype**:  
   - Participants push back against AI hype, emphasizing the gap between marketing claims and real-world performance. Competitors like HubSpot’s ChatGPT integration are cited as examples of rushed, overhyped solutions.  

5. **Communication Clarity**:  
   - Users stress the need for brevity and precision in LLM responses, noting that unclear outputs burden human oversight and erode trust.  

Overall, the discussion underscores cautious optimism, urging realistic expectations, improved transparency, and ethical frameworks as AI evolves in enterprise contexts.

