## AI Submissions for Mon Oct 16 2023 {{ 'date': '2023-10-16T17:10:35.045Z' }}

### MemGPT â€“ LLMs with self-editing memory for unbounded context

#### [Submission URL](https://github.com/cpacker/MemGPT) | 330 points | by [shishirpatil](https://news.ycombinator.com/user?id=shishirpatil) | [78 comments](https://news.ycombinator.com/item?id=37901902)

MemGPT is a system that allows you to create perpetual chatbots with self-editing memory. It intelligently manages different memory tiers within the model to provide extended context and enable continuous conversations. The system knows when to push critical information to a vector database and when to retrieve it later in the chat. It supports various types of data sources, including SQL databases, local files, and documents.

You can try out MemGPT on Discord by messaging the MemGPT bot in the #memgpt channel. To run MemGPT locally, you need to install the dependencies and add your OpenAI API key to the environment variables. Then you can run the main.py file. You can also create new starter users or personas by adding .txt files in the appropriate folders.

MemGPT CLI provides various commands for interacting with the chatbot, such as saving and loading checkpoints, viewing message logs, and managing memory. Additionally, MemGPT's archival memory feature allows you to load your SQL database and have conversations with it. The system includes a toy example using a test database.

MemGPT is a powerful tool for creating chatbots that can have meaningful and ongoing conversations by leveraging memory management techniques. Check out the repository for more information and examples.

The discussion on this submission covers a range of topics related to MemGPT and memory management in chatbots.

- One user mentions that they have experimented with a similar approach of managing memory in a limited context window, where the chatbot generates memories and retrieves them later. They also mention that there are multiple ways to handle memory in chatbots, including implicit and explicit memory management.
- Another user finds the approach interesting and mentions that they are working on a similar feedback loop for rewriting history and transactional data in a conversational context. They discuss the potential of using structured data to extract context and generate embeddings for building vector databases.
- Someone points out that multi-agent systems could be implemented with confidence levels and entropy to make conversations more worthwhile.
- Another user suggests that the same approach could be applied to ChatGPT, a chatbot that they have used which degrades in performance when long chat histories are included. They speculate that recursive summarization could be a fundamental feature to solve this issue.
- The limitations of Llama (a similar project) are brought up, including difficulties in generating correct function calls and grammatically correct sampling.
- In response, it is mentioned that grammar-based sampling is not a perfect fit for MemGPT experiments, as the main impact is with incorrect function parameters, not the function call itself.
- The discussion also touches on the potential of using total chat change as a prompt and how conversations with context windows could retain important information.
- Some users discuss the potential of a middle language model and the vanishing gradient problem in long-context models. Resources related to this topic are shared, including papers on long-context language models and the vanishing gradient problem.
- Finally, the discussion briefly mentions regularization techniques for mitigating the vanishing gradient problem in neural networks.

Overall, the discussion provides insights and ideas related to memory management, chatbot design, and the challenges associated with long-context models.

### Actively exploited Cisco 0day with maximum severity gives full network control

#### [Submission URL](https://arstechnica.com/security/2023/10/actively-exploited-cisco-0-day-with-maximum-10-severity-gives-full-network-control/) | 114 points | by [AdmiralAsshat](https://news.ycombinator.com/user?id=AdmiralAsshat) | [6 comments](https://news.ycombinator.com/item?id=37906156)

Cisco has warned its customers about a critical zero-day vulnerability that is actively being exploited by threat actors. The vulnerability, tracked as CVE-2023-20198, allows attackers to gain full administrative control over Cisco devices, including switches, routers, and wireless LAN controllers running IOS XE software with the HTTP or HTTPS Server feature enabled and exposed to the Internet. Cisco's Talos security team discovered that an unknown threat actor has been exploiting the zero-day since September 18, creating an authorized user account and deploying a malicious implant that allows for the execution of arbitrary commands. Cisco has advised affected entities to implement the necessary steps outlined in its advisory to protect their devices.

The discussion around the submission revolves around various points:

1. User "jpc0" mentions that Cisco's advisory is related to the vulnerability being accessible through the HTTP or HTTPS server feature enabled on Internet-facing systems. They also highlight the importance of following established practices for securing critical hardware and management access.
2. User "cdh" criticizes Cisco, suggesting that they compromise software in a sneaky manner and fail to contact victims. They also make negative remarks about CEOs, network teams, and inexperienced internet entry-level staff.
3. User "jcqsm" finds it interesting how the headline implies that Cisco's response to the situation is different from their previous actions in similar incidents.
4. User "crs" simply states that it is noteworthy that Cisco has a zero-day vulnerability.
5. User "mltynss" contributes to the discussion by mentioning Security Technical Implementation Guides (STIGs), which provide guidelines for secure configuration and operation of various systems. They provide links to STIGs related to Cisco IOS XE switch and router configurations. Another user, "ThePowerOfFuet," identifies the acronym STIG and confirms its meaning.

Overall, the discussion contains mixed sentiments, ranging from technical insights to criticisms of Cisco's handling of the situation.

### PaLI-3 Vision Language Models

#### [Submission URL](https://arxiv.org/abs/2310.09199) | 173 points | by [maccaw](https://news.ycombinator.com/user?id=maccaw) | [22 comments](https://news.ycombinator.com/item?id=37895601)

A team of researchers from various institutions has introduced a smaller, faster, and stronger vision language model (VLM) called PaLI-3. Compared to larger models, PaLI-3 achieves comparable performance while using only a fraction of the parameters. The researchers achieved this by comparing pretrained models using classification objectives to contrastively pretrained ones. While PaLI-3 slightly underperforms on image classification benchmarks, it outperforms on various multimodal benchmarks, particularly in localization and visually-situated text understanding. The team scaled up the model's image encoder to 2 billion parameters, setting a new state-of-the-art on multilingual cross-modal retrieval. With PaLI-3, the researchers hope to encourage further research on fundamental aspects of complex VLMs and pave the way for scaled-up models in the future.

The discussion on this submission covers various aspects of the research and its implications. Some users express skepticism about the realism of the training data used for the model and argue that it may not reflect real-world scenarios. Others point out that benchmarking is a common practice in machine learning but note that the benchmarks may not fully represent complex real-world tasks. One user shares a link to another paper for further comparison. 

There is also a discussion about the technical details of the model, including the visual tokens and the projection of visual tokens in the PaLI-3 model. Users provide explanations and comparisons with other implementations such as ViT.

Some users raise concerns about the limitations of models like PaLI-3 in handling tasks such as pixel-wise segmentation masks. Others discuss potential applications of vision-language models for tasks like OCR and image categorization.

A few comments comment on the rivalry between different companies in the field of AI research, particularly mentioning Google, OpenAI, and Facebook. There is a debate surrounding the capabilities and performance of their respective models, with some users criticizing the boastful nature of their submissions.

The discussion also touches on copyright infringement and the potential misuse of AI models.

### AI pioneers LeCun, Bengio clash in intense online AI safety, governance debate

#### [Submission URL](https://venturebeat.com/ai/ai-pioneers-yann-lecun-and-yoshua-bengio-clash-in-an-intense-online-debate-over-ai-safety-and-governance/) | 31 points | by [rntn](https://news.ycombinator.com/user?id=rntn) | [17 comments](https://news.ycombinator.com/item?id=37902248)

Two prominent figures in the field of artificial intelligence (AI), Yann LeCun and Yoshua Bengio, engaged in a fiery debate over the weekend regarding the potential risks and safety concerns associated with AI. LeCun, Meta's chief AI scientist, initiated the debate on Facebook, calling on AI scientists and engineers who believe in the power of AI to voice their opinions. Bengio, founder of Element AI and a professor at the University of Montreal, responded by challenging LeCun's perspective on AI safety and raising concerns about the risks of open-source AI platforms. The debate also involved input from Jason Eisner, director of research at Microsoft Semantic Machines and professor at Johns Hopkins University. Despite their past collaboration and shared recognition for contributions to the field, the debate highlights the considerable disagreement among esteemed researchers regarding the risks and safety measures associated with AI. This ongoing debate reflects the growing concern surrounding the implications of AI as it becomes increasingly embedded in daily life. As AI technology continues to advance, the need for informed discussions on its implications becomes more urgent.

The discussion on the submission revolves around different perspectives on AI safety and its potential risks. Some commenters argue that AI should be designed with strong safety measures in place to avoid dangerous consequences, comparing it to the responsible design of weapons. Others disagree with this comparison, stating that AI is intended to enhance human intelligence and not cause harm. 

One commenter brings up the topic of the American Medical Association (AMA) and its control over the supply of medical providers, arguing that restrictions on the number of doctors allowed to practice exacerbate the medical crisis. Another commenter responds by emphasizing that the commenter's comment is condescending and that people facing the medical crisis deserve better understanding.

The debate continues with discussions on the affordability of medical care, the role of AI in replacing certain professions, and the potential economic impact of such changes. There are also comments suggesting alternate solutions and expressing skepticism about the role of radiologists.

Some commenters argue that AI should be prioritized for applications that can achieve positive results and ensure safety, rather than wasting time on potentially harmful endeavors. Others argue that the responsibility for any negative consequences lies with society, not solely with AI researchers or developers.

Overall, the discussion reflects differing opinions on the importance of AI safety measures and the potential risks associated with AI technology.

### Ultra-efficient machine learning transistor cuts AI energy use by 99%

#### [Submission URL](https://newatlas.com/technology/ai-machine-learning-transistor/) | 27 points | by [0xa2](https://news.ycombinator.com/user?id=0xa2) | [9 comments](https://news.ycombinator.com/item?id=37899129)

Researchers at Northwestern University have developed a microtransistor that could significantly reduce the energy consumption of AI machine learning tasks. The microtransistor, built from two-dimensional sheets of molybdenum disulfide and carbon nanotubes, is 100 times more efficient than current technology, and can perform classification tasks at just 1% of the energy consumption. This breakthrough could enable the deployment of AI directly in wearable electronics, leading to real-time detection and data processing in health emergencies. The new transistor's high tunability and low energy consumption make it ideal for sophisticated classification algorithms with a small footprint. The researchers demonstrated its capabilities by correctly classifying abnormal heartbeats with 95% accuracy using just two of these microtransistors. Once this technology is brought to production, mobile devices will be able to run machine learning AI on their own sensor data, providing quicker results and keeping personal data local and secure. It remains to be seen if this technology can extend beyond portable devices and be applied to larger AI equipment, potentially revolutionizing large model training by drastically reducing energy consumption.

The discussion surrounding the submission revolves around the concept of weight valuation in AI neural networks and the potential impact of the new microtransistor technology. One user suggests that understanding how neural networks work, particularly backpropagation and forward pass, can help optimize weight valuation and save energy. They also mention that analog computation could introduce errors from noise and thermal variations. Another user acknowledges the importance of energy-efficient training but mentions that inferences on low-power devices with pre-trained models are more common in practice. Another user suggests that it might be possible to create AI chips with programmable architectures that can handle both training and inference tasks with lower power consumption. A different perspective is brought up, stating that regardless of the advancements in energy efficiency, digital neural networks still require high bandwidth due to data correction and machine learning algorithms that are not tolerant to noise. Overall, the discussion focuses on various aspects of AI, neural networks, and the potential benefits and challenges of the new microtransistor technology.

