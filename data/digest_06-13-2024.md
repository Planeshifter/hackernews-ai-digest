## AI Submissions for Thu Jun 13 2024 {{ 'date': '2024-06-13T17:10:57.693Z' }}

### MLow: Meta's low bitrate audio codec

#### [Submission URL](https://engineering.fb.com/2024/06/13/web/mlow-metas-low-bitrate-audio-codec/) | 552 points | by [mikece](https://news.ycombinator.com/user?id=mikece) | [189 comments](https://news.ycombinator.com/item?id=40670612)

Meta has unveiled a new groundbreaking technology named Meta Low Bitrate (MLow) codec, enhancing audio quality for users on slower connections. The codec, designed for real-time communication products like WhatsApp and Instagram, aims to ensure a high-quality experience for all users, regardless of their device or connection speed.

Traditionally, audio/video codecs compress data for efficient transmission over the internet, balancing quality, bitrate, and complexity. The development of MLow addresses the need for high audio quality at low bitrates, particularly in poor network conditions where existing codecs struggle to maintain clear audio.

Unlike previous AI/ML-based codecs that require high computational power, MLow focuses on delivering top-notch audio quality with minimal computational requirements. This approach ensures that even users with low-end devices can benefit from improved audio experiences during calls.

After two years of development, MLow has achieved superior audio quality compared to the widely used Opus codec, while maintaining lower computational complexity. By rolling out MLow in Instagram, Messenger, and WhatsApp calls, Meta has already witnessed a significant boost in user engagement due to the enhanced audio experience.

In addition to enhancing audio quality, MLow enables more effective Forward Error Correction (FEC) strategies at lower bitrates, improving audio quality in scenarios where packet loss occurs. This innovation underscores Meta's commitment to providing a seamless and high-quality communication experience for all users across its platforms.

The discussion on the submission about Meta's MLow codec primarily focuses on technical aspects such as latency, bandwidth, packet distribution, and the impact on voice communication systems. Users delve into the details of packet transmission, codec efficiency, and the optimization of voice communication systems. There are also discussions on the practical implications of the MLow codec rollout in popular Meta platforms like WhatsApp and Instagram. Some users highlight the potential enhancements in user engagement and audio quality brought about by MLow. Additionally, there are comments on the challenges and considerations related to network conditions, packet loss, and bandwidth management in real-time communication applications. The conversation expands to include comparisons with other video calling solutions like Skype and FaceTime, as well as considerations for different network technologies like 3G and 5G. Suggestions are made regarding further improvements in codec optimization for better call quality and network performance.

### Show HN: Paramount – Human Evals of AI Customer Support

#### [Submission URL](https://github.com/ask-fini/paramount) | 68 points | by [hakimk](https://news.ycombinator.com/user?id=hakimk) | [26 comments](https://news.ycombinator.com/item?id=40672843)

Today on Hacker News, a project called Paramount caught the attention of developers. Paramount enables expert agents to evaluate AI chats, offering features like quality assurance, ground truth capturing, and automated regression testing. Users can install the package, decorate their AI functions, and then launch the Paramount UI to evaluate results. The tool runs offline in a private environment, ensuring data security. Additionally, the project provides detailed configuration options for setting up the chat parameters, making it versatile for various AI implementations. With 144 stars and 4 forks on GitHub, Paramount seems to be gaining traction in the AI and developer community.

The discussion around the Paramount project on Hacker News delves into the topic of customer support in the context of AI and automation. Some users express concerns about the current state of customer support, highlighting issues with outsourced support in non-English-speaking countries, the challenges faced by growing companies in providing quality support, and the need for scalable, hassle-free customer support. The conversation also touches on the role of AI bots in answering customer calls and the potential drawbacks of relying too heavily on automated responses. Additionally, there is a debate about the licensing of the Paramount project, with suggestions for potential adjustments to the license to ensure compatibility with different company sizes and usage scenarios. Overall, the discussion explores various perspectives on the evolving landscape of customer support, the balance between automation and human touch, and the implications of these advancements on customer experiences and business operations.

### Luma AI Dream Machine

#### [Submission URL](https://lumalabs.ai/dream-machine) | 200 points | by [Anon84](https://news.ycombinator.com/user?id=Anon84) | [55 comments](https://news.ycombinator.com/item?id=40670096)

Dream Machine, a groundbreaking AI model, is revolutionizing video creation by generating high-quality, realistic videos fast from text and images. This highly scalable and efficient transformer model trained directly on videos can produce physically accurate, consistent, and dynamic shots. This innovation marks a significant step towards developing a universal imagination engine, now available to everyone.

One of the standout features of Dream Machine is its exceptional speed, capable of generating 120 frames in just 120 seconds. This rapid video generation allows users to iterate quickly, explore diverse ideas, and dream big.

In terms of video quality, Dream Machine excels at creating action-packed shots with smooth motion, cinematic flair, and compelling drama. It can transform static images into lively, engaging stories, offering endless creative possibilities.

Moreover, Dream Machine ensures consistency in character depiction and realistic physics interactions in the generated videos. By understanding how people, animals, and objects interact with the physical world, this AI tool enables users to maintain character integrity throughout their video projects.

Furthermore, Dream Machine facilitates experimentation with a wide range of camera motions, from fluid to naturalistic, enhancing the emotional impact and content of each scene. Its capabilities in creating breathtaking camera moves capture viewers' attention and elevate the visual storytelling experience.

The team behind Dream Machine consists of a talented group of individuals working on model development and systems design, showcasing a collective effort in pushing the boundaries of video creation technology. With these advancements, Dream Machine opens up new possibilities for content creators to unleash their creativity and bring their imaginative visions to life.

The discussion on Hacker News regarding the submission about Dream Machine covers various points. Users discuss topics such as issues with trying the product as potential customers, the speed and cost of the service, considerations about alternate service providers, and the implications for user privacy. Additionally, there are comparisons to other AI models like OpenAI's SORA and Pollinations. There is also a mention of a related project called DREAMACHINE. Other users raise questions about the limitations and features of these AI video generation models, with some comparisons to natural AI-generated videos and concerns about potential consequences of these technologies.

### New algorithm discovers language just by watching videos

#### [Submission URL](https://www.csail.mit.edu/news/new-algorithm-discovers-language-just-watching-videos) | 28 points | by [geox](https://news.ycombinator.com/user?id=geox) | [3 comments](https://news.ycombinator.com/item?id=40673751)

Mark Hamilton, an MIT PhD student, has developed an innovative algorithm, DenseAV, that learns language from scratch by watching videos. Inspired by a penguin groaning in a movie, Hamilton's algorithm can predict what it's seeing based on what it's hearing and vice versa. By training on a matching game where audio and visual signals are paired, DenseAV can distinguish between words and sounds without human intervention.

This breakthrough has many potential applications, from analyzing instructional videos to understanding animal communications like dolphin language. DenseAV's unique approach of contrastive learning sets it apart from previous algorithms by focusing on detailed connections between audio and visual signals. The team's efforts have resulted in a highly effective model that outperforms others in tasks like object identification and sound localization.

Despite the challenges faced in transitioning to a large transformer architecture and encouraging focus on fine-grained details, the team's year-long project has been a success. Moving forward, they aim to develop systems capable of learning from vast amounts of video and audio data, opening up exciting possibilities for language discovery and pattern recognition.

1. User "jw" found the submission interesting and noted the potential application of the algorithm in universal translators and deciphering the Voynich manuscript.
   
2. User "kskhkd" mentioned linguists and Chomsky in the context of the algorithm, possibly pointing to the significance of linguistic theories and proofs in the field of study.
   
3. User "jls" shared a link to the paper related to the submission, directing others to the source for further information and details.

### Uncensor any LLM with abliteration

#### [Submission URL](https://huggingface.co/blog/mlabonne/abliteration) | 529 points | by [mizzao](https://news.ycombinator.com/user?id=mizzao) | [256 comments](https://news.ycombinator.com/item?id=40665721)

The latest article by Maxime Labonne delves into a fascinating technique called "abliteration," which aims to uncensor any LLM without the need for retraining. Modern LLMs, while designed for safety and instruction-following, often come with a built-in refusal mechanism to prevent harmful requests. By identifying and removing the "refusal direction" within the model using abliteration, it becomes possible for the model to respond to all types of prompts.

The process involves data collection, calculating mean differences, and selecting the best refusal direction for each layer of the model. Ablation can be achieved through inference-time intervention or weight orthogonalization, where the refusal direction is either subtracted from the model's output or directly modified in the model weights.

The article provides detailed insights into the implementation of abliteration with weight orthogonalization, with code snippets and references to related libraries like TransformerLens. By following the step-by-step guide and utilizing the provided resources, readers can experiment with abliteration on their own LLM models.

The discussion in the comments on Hacker News is quite diverse and covers various aspects related to the submission on abliteration and LLMs. Here are some key points discussed:

1. **Technical Details and Implementation of Abliteration**:
   - There is a mix of opinions on the novelty and practicality of the technique of abliteration in uncensoring LLMs without retraining. Some find the idea refreshing and interesting, while others have reservations about its real-world applications.

2. **Related Topics and References**:
   - The conversation touches on a wide range of topics, including GPU capabilities, potential applications of LLMs like scenarios involving cryptocurrency mining and creating simulated worlds, and references to popular blogs like "What If" by Randall Munroe.

3. **Ethical Considerations and Use Cases**:
   - Discussions also delve into ethical considerations, such as the responsibility of LLMs in providing information, potential dangers of misinformation, and the need for safeguards in place to prevent harmful or misleading outputs.

4. **Critiques and Debates**:
   - Some users express concerns about the implications of modifying LLMs to remove refusal mechanisms, debating the balance between safety mechanisms and unrestricted prompt responses. There are also discussions on the boundaries of content creation and the role of LLMs in censoring certain content.

5. **Practical Applications and Limitations**:
   - The conversation extends to practical applications of LLMs in various scenarios like gaming, fantasy role-playing, content moderation for sensitive topics, and ensuring the safety and ethical use of AI technologies.

6. **Freedom of Speech and Expression**:
   - There are discussions about the balance between safety mechanisms and freedom of speech in AI models, including considerations about how to implement safety measures without infringing on expression rights.

Overall, the discussion reflects a mix of technical curiosity, ethical concerns, and debates on the implications of implementing advanced AI techniques like abliteration in LLMs.

### What If We Recaption Billions of Web Images with LLaMA-3?

#### [Submission URL](https://arxiv.org/abs/2406.08478) | 87 points | by [Jimmc414](https://news.ycombinator.com/user?id=Jimmc414) | [40 comments](https://news.ycombinator.com/item?id=40665734)

A recent paper titled "What If We Recaption Billions of Web Images with LLaMA-3?" delves into the realm of enhancing model training for various vision-language tasks, especially text-to-image generation. The authors leverage the open-sourced LLaMA-3, a powerful language model, to recaption 1.3 billion web images. The results show significant benefits in training advanced vision-language models, with improvements in both discriminative and generative models. The project aims to bridge the gap in large-scale investigations in this area by offering an enhanced dataset, Recap-DataComp-1B. The recaptioning pipeline involves fine-tuning a LLaMA-3-8B model, leading to enhanced zero-shot performance in cross-modal retrieval tasks and improved alignment in text-to-image generation, especially with complex queries. This project holds promise for advancing vision-language tasks and model training.

- User mcnnr mentions that the paper's Table 3 shows zero-shot image classification works well but struggles with recaptioned labels. They find the improvement in evaluations surprising and note the correlation downstream task performance. User stbnk agrees and suggests improving captions' quality to enhance the performance of the LLMs.
  
- User slm discusses the importance of query rewrite in cross-visual tasks, highlighting the need for accuracy and specificity. They provide examples of model errors in describing images, emphasizing the importance of context for accurate image captions.
  
- User vssns criticizes the quality of captions generated by LLama-3 and LLava-15, calling them "crappy" and possibly wrong compared to human captions. They raise concerns about model training data and note discrepancies in results shown in different tables.
  
- User ntrlx expresses surprise at the original captions containing extra context for image descriptions. They speculate on the purpose of this extra information and suggest ways to improve captioning for images with detailed content.
  
- User mchlt comments on the use of fine-tuning LLaMA-3-8B with LLaVA-15 for recaptioning 1.3 billion images, highlighting the surprising aspect of the reported results. Another user, mjbrgss, points out the potential issues with the quality of labels used for training.
  
- User bastien2 discusses the decision to rely on generated captions for images, suggesting they might be inconsistent or of questionable value. This prompts a discussion on living wages and the benefits of utilizing human reviewers in such processes.
  
- User grndl expresses confusion over LLMs generating training data for other LLMs and questions the efficacy of feedback loops in producing coherent outputs. This leads to a discussion on how humans bootstrap training for LLMs and the limitations of synthetic data generation.
  
- User RecycledEle acknowledges the potential benefits of recaptioning images for making image retrieval easier. They discuss AI challenges in image labeling and classifier misclassifications. Additionally, they question the censorship of large public models and the handling of offensive content by AI models.
  
- User grnhrth simply comments with "don't."

### IntelliJ GitHub Plugin leaking credentials

#### [Submission URL](https://blog.jetbrains.com/security/2024/06/updates-for-security-issue-affecting-intellij-based-ides-2023-1-and-github-plugin/) | 144 points | by [jonahss](https://news.ycombinator.com/user?id=jonahss) | [21 comments](https://news.ycombinator.com/item?id=40671738)

A new security issue has been discovered that affects IntelliJ-based IDEs and the JetBrains GitHub Plugin, potentially exposing access tokens to third-party sites. The issue, marked as CVE-2024-37051, has been resolved, and fixes are available for affected IDE versions. It is strongly advised to update to the latest versions and revoke any GitHub tokens used by the plugin for security measures. JetBrains also informs about changes in the Security Bulletin and shares insights regarding the SOC 2 audit report they received. Stay updated and stay secure in the coding world!

- User Merad pointed out that GitHub is rejecting requests from affected versions of IDEs due to issues with the GitHub integration in IntelliJ, advising users to update their IDEs to the latest versions to resolve the problem.

- User albert_e mentioned a topic related to JetBrains' website displaying text in a styled format within IntelliJ, hinting at IntelliJ CSS magic and developer tools for text styling. This led to a discussion about the font used - Jetbrain Sans - by other users (yen223, rncl, mnngs) who shared insights about fonts, characters, and symbols.

- User rf highlighted a vulnerability in the past related to detailed token requests by plugins and third-party sites receiving tokens mentioned by user lstms.

- User stuff4ben emphasized the importance of rotating tokens regularly for security measures and received responses from thnd and WorldMaker regarding token expiration practices and the necessity of regular rotation to mitigate vulnerabilities.

- User that_guy_iain mentioned seeing a notice about the 10th text length increase without providing further details. This sparked a conversation with mnqr, rflgnts, and bdhcdbb discussing the frequency of notices, blog posts, and recent trends in sharing knowledge among developers, with some users expressing surprise at the increasing frequency of such updates.

- User mttjyns highlighted a security vulnerability in JetBrains' TeamCity CI product that allows attackers to access internal information, recommending critical CVEs be addressed immediately and suggesting reinstalling the server if necessary. This sparked discussions about plugins mixing products, the compatibility of different JetBrains products, and the challenges of securing software products, particularly in a complex environment like Android Studio.

- Users rf15, mnqr, and lyu07282 further discussed the mixed bag of plugins and products, the importance of addressing critical vulnerabilities promptly, the challenges faced by Android developers using JetBrains products, and the difficulty in managing software products in a diverse environment.

### Minuimus is a file optimiser utility script

#### [Submission URL](https://birds-are-nice.me/software/minuimus.html) | 39 points | by [UltimateEdge](https://news.ycombinator.com/user?id=UltimateEdge) | [4 comments](https://news.ycombinator.com/item?id=40664541)

Today's top story on Hacker News is about Minuimus, a powerful file optimiser utility script that can make files smaller without compromising their contents by using various file-specific optimizations. The latest version, Minuimus 4.1, automates the optimization process by calling upon multiple utilities and specialized methods. It can optimize a wide range of file types, including images, documents, archives, and more. 

Minuimus ensures transparent optimizations by preserving the quality of images, audio, video, and metadata. It also supports lossy optimization options if explicitly enabled. The process of optimizing files is straightforward - install the prerequisites, point Minuimus at the files to be optimized, and it will handle the rest, skipping over files it cannot optimize. 

This versatile tool is especially effective for reducing the size of e-books, office documents, and compressed files like ZIP, epub, or docx. It can make a significant difference in storage and transmission efficiency with minimal impact on the file contents. Minuimus employs a variety of techniques for different file types, such as JPEG optimization, PNG processing, PDF optimization, and more, making it a comprehensive solution for maximizing file efficiency. 

With its ability to achieve substantial space savings depending on the file being optimized, Minuimus is a valuable tool for improving website performance, distributing games, or managing large collections of files. Its support for a wide range of file types and thorough optimization processes make it a must-have for anyone looking to streamline their digital content.

- lfthrsr shared information about Minuimus's compatibility with DEFLATE-based file formats such as PNG, ZIP, JPEG, and ECT. They also mentioned that the tool is available for knowledgeable users in the form of Pareto frontier or better Zopfli ADVPNG output. They suggested checking out the Efficient Compression Tool on GitHub for further details.
  
- srbntr added that Minuimus can delete unnecessary files like Thumbdb and .ds_store from ZIP files, which leads to longer-lasting and more lossless compression.

- In response to srbntr's comment, ttyprntk pointed out that it should not be assumed that removing useless files equates to metadata loss, especially concerning PDFs.

- gndlv complimented the nice plugin for GenAI interfaces that were generated.

### The Qualcomm Snapdragon X Architecture Deep Dive: Getting to Know Oryon

#### [Submission URL](https://www.anandtech.com/show/21445/qualcomm-snapdragon-x-architecture-deep-dive) | 47 points | by [msh](https://news.ycombinator.com/user?id=msh) | [12 comments](https://news.ycombinator.com/item?id=40669827)

Qualcomm is all set to unveil its latest Snapdragon X SoC, featuring the highly-anticipated Oryon CPU core and Adreno X1 GPU architecture. The Snapdragon X Elite and Snapdragon X Plus are about to hit the market after months of anticipation and performance teasers. These chips have already been dispatched to Qualcomm's laptop partners, with the first laptops expected to ship soon.

Qualcomm is going all out with the Snapdragon X, not just cobbling together existing IP blocks but designing novel technology from scratch. The spotlight is on the Oryon CPU core, a groundbreaking design that will power Qualcomm's future Windows-on-Arm SoCs and mobile devices. This marks a significant move away from Arm's reference designs, setting Qualcomm apart in the PC and mobile space.

The Snapdragon X lineup includes Elite and Plus SKUs, with varying core configurations tailored for different power levels. Qualcomm is gearing up for a strong launch with an array of device manufacturers on board, indicating significant industry interest. Stay tuned for more updates as Qualcomm's Snapdragon X SoC takes the market by storm.

1. User "krckrs" wonders if Apple's M1 series essentially outshines other companies in designing high-performance SoCs, indicating excitement about Qualcomm's Snapdragon X release. User "chlv" agrees, mentioning the talent of chip designers and the competitive nature of the field. User "AtlasBarfed" sarcastically comments on the hype around these discussions.

2. User "ngdlss" expresses interest in upcoming reviews and questions Microsoft's handling of translation. User "ndddy" mentions a Reddit post with feedback on the limitations of a laptop potentially related to battery life and thermal issues compared to Qualcomm's claims of performance.

3. User "brknmchn" shares their experience of working with Linux and dreaming of a Linux laptop that works as efficiently as a MacBook in terms of sleep modes and trackpad functionality. User "sangel21" points to a high likelihood of this dream becoming reality.

4. User "brcmthrwwy" announces a shift to Microsoft, while user "WhereIsTheTruth" questions Microsoft's transition process and the challenges faced by game developers in making games compatible. User "kysn" provides insights into Microsoft's transition process, mentioning software translation, testing, validation, and Qualcomm's efforts to support different platforms. User "SushiHippie" shares a link to presentation slides discussing Qualcomm's efforts in this space.

5. User "mshckwv" criticizes Microsoft for lackluster execution and praises Qualcomm's initiative in the industry despite Microsoft's slower adaptation.

### New Stable Diffusion 3 unable to generate human bodies due to nudity filter

#### [Submission URL](https://arstechnica.com/information-technology/2024/06/ridiculed-stable-diffusion-3-release-excels-at-ai-generated-body-horror/) | 43 points | by [isaacfrond](https://news.ycombinator.com/user?id=isaacfrond) | [12 comments](https://news.ycombinator.com/item?id=40667014)

The latest release by Stability AI, the Stable Diffusion 3 Medium, has stirred up quite a storm in the AI image-synthesis community. While this model promises to turn text prompts into AI-generated images, users have been left disappointed by the strange and often anatomically incorrect outputs it produces, especially when it comes to rendering human figures.

Reddit threads have popped up showcasing the comical yet flawed images generated by SD3 Medium, with users pointing out the model's struggles with details like hands and feet. The community, once impressed by Stability AI's advancements, now views this release as a regression compared to previous models like Midjourney or DALL-E 3.

The root of the issue seems to lie in Stability's decision to filter out adult content from the training data, inadvertently resulting in a lack of diverse human anatomy examples. This has led to bizarre interpretations of user prompts, producing images that range from amusing to downright terrifying.

As users experiment with the model on platforms like Hugging Face, they continue to encounter similar issues, reinforcing the disappointment surrounding Stable Diffusion 3 Medium. These struggles with the model's output have only exacerbated concerns about Stability AI's internal challenges, including recent leadership changes and financial troubles.

For now, enthusiasts are left questioning the future of Stability AI and its image-synthesis capabilities, as the release of SD3 Medium fails to meet the high expectations set by previous models.

The discussion on this submission covers various perspectives on Stability AI's release of the Stable Diffusion 3 Medium model. 

- **gnbgb**: The user mentions there are 9 comments on the Reddit thread discussing the topic.
- **mrndsh**: Expresses concern about the decision to filter adult content from the training data and criticizes the model's output. They also talk about the need to pay attention to the current levels of concern in the industry.
- **smsmshh**: Comments on the challenges faced by the model in generating human bodies accurately due to the lack of not safe for work (NSFW) filters.
- **jrm4**: Finds the Stability AI's Stable Diffusion model fascinating in terms of image generation capabilities.
- **__loam & jrm4**: Discuss potential implications of the large sums of money being spent on these technologies, including concerns about privacy and the impact on society.
- **wkwkwk**: Comments on the quality of the generated images by the model and questions the purpose behind its release.
- **pplchmst & twtwtwtw**: Talk about the challenges faced by the model in generating human-like images and suggest trying to generate different types of content to test its capabilities. 

Overall, the comments reflect a mix of disappointment, concern, and curiosity regarding Stability AI's latest release and its implications for the future of image generation technology.

### How Amazon blew Alexa's shot to dominate AI, according to employees worked on it

#### [Submission URL](https://fortune.com/2024/06/12/amazon-insiders-why-new-alexa-llm-generative-ai-conversational-chatbot-missing-in-action/) | 20 points | by [firstSpeaker](https://news.ycombinator.com/user?id=firstSpeaker) | [4 comments](https://news.ycombinator.com/item?id=40674104)

In a bid to revolutionize Amazon's Alexa, a new generative AI-powered version was unveiled in September 2023, promising a more natural and conversational interaction. However, according to insiders, the launch has been marred by delays and organizational challenges, with the new Alexa still not ready for prime time. Former employees paint a picture of Amazon struggling to keep up with Big Tech rivals in the AI race, citing structural dysfunction and technological hurdles.

Meanwhile, Apple has made strides in the generative AI space with recent announcements at its WWDC conference, showcasing upgrades for Siri and a partnership with OpenAI. This puts pressure on Amazon to deliver its revamped Alexa. Despite high expectations, it seems Amazon is facing setbacks in transitioning its digital assistant to compete in the evolving AI landscape.

There are various opinions shared by Hacker News users about the submission regarding Amazon's Alexa and generative AI technology:

1. User "rghthnd" implies that current AI companies are taking profitability factors for granted, resulting in a revenue stream path. They mention dumping data, Long-Short Term Memory (LLMs), clever statistics, and reference Amazon's business stock landing roles regarding technology products and hardware.

2. User "rdtsc" jokes about the need to work harder and talks about the message magic of LLMDoes Prasad, making a comparison to Michael Scott from The Office TV show.

3. User "hlssn" shares a link, but it appears to be paywalled. They suggest searching Google with the provided title to find information related to Amazon's Alexa and the dominance of AI according to a dozen employees who worked at Amazon.

4. User "fkjslt" simply comments "dd," which could indicate agreement or acknowledgment of the points discussed in the thread.

### Can LLMs invent better ways to train LLMs?

#### [Submission URL](https://sakana.ai/llm-squared/) | 59 points | by [hardmaru](https://news.ycombinator.com/user?id=hardmaru) | [36 comments](https://news.ycombinator.com/item?id=40665194)

At Sakana AI, they are pushing the boundaries of AI research by leveraging evolutionary optimization and Large Language Models (LLMs) to automate the process of developing cutting-edge foundation models. In their recent paper, they introduce a novel approach called LLM² (‘LLM-squared’) where LLMs are used for self-referential improvement in discovering preference optimization algorithms. By using LLMs to propose and implement new preference optimization loss functions, they have discovered a state-of-the-art algorithm named DiscoPOP that outperforms existing methods. This breakthrough not only minimizes human intervention in AI research but also opens up new possibilities for enhancing LLM capabilities in various applications. Collaborating with prestigious universities, Sakana AI has released their report and open-sourced the discovery process code, signaling a significant leap in AI innovation that promises to revolutionize the field.

The discussion on the Sakana AI submission on Hacker News covered a range of topics related to Large Language Models (LLMs) and their applications in AI research. Here are some key points highlighted in the discussion:

- There was an exchange about the accuracy of the model name used in the submission, where the author corrected the naming from "gpt_model = gpt4_20231230_1106preview" to "gpt-4".
- Some users discussed the potential of LLMs to optimize training processes and invent new ways of fine-tuning existing models.
- One user emphasized the importance of self-consciousness in AI systems.
- The discussion also touched upon the limitations of LLMs in inventing completely new concepts and the distinction between building on existing systems and generating truly novel ideas.
- One user raised a question about the ability of LLMs to invent new things, highlighting the difference between analyzing existing systems and truly creating original solutions.
- The discussion also delved into the concept of combining existing patterns to create new innovations, with comparisons made between human cognitive processes and computational models.
- Another user pointed out that the question of efficient invention may be related to the fundamental forces in nature, suggesting a deep philosophical angle to innovation.
- Lastly, a user commented that the highlighted work could be construed as clickbait.

Overall, the discussion explored the capabilities and limitations of LLMs in AI research and delved into the nuances of invention and creativity in computational systems.

### An Empirical Study of Mamba-Based Language Models

#### [Submission URL](https://arxiv.org/abs/2406.07887) | 42 points | by [panabee](https://news.ycombinator.com/user?id=panabee) | [3 comments](https://news.ycombinator.com/item?id=40672606)

A recent arXiv submission titled "An Empirical Study of Mamba-based Language Models" by Roger Waleffe and a team of 15 other authors delves into the realm of Selective State-Space Models (SSMs) like Mamba, comparing them with Transformers in language modeling. The study explores the performance of 8B-parameter Mamba, Mamba-2, Transformer models, and a hybrid architecture on various tasks and datasets up to 3.5T tokens. The results indicate that while pure SSMs can outperform Transformers on many tasks, they struggle with tasks requiring strong copying abilities or long-context reasoning. Interestingly, the 8B Mamba-2-Hybrid model surpasses the 8B Transformer on all evaluated tasks and is predicted to be significantly faster in token generation at inference time. The study also includes experiments on long-context tasks, showing the hybrid model matching or exceeding the Transformer on average. The research aims to shed light on the strengths and weaknesses of different architectures, providing insights for further exploration in the field of machine learning.

The discussion revolves around the comparison between Mamba models and Transformers based on their sizes and parameter counts. One user wonders about the largest Mamba model trained so far and notes that Mamba models appear to scale better than Transformers based on their experiments. Another user shares their experience with training large language models, stating that rapid improvements were seen when training higher-capacity models. They express regret at not investing more resources into training larger models sooner. Additionally, there is a mention of the difficulty in long-context tasks and the potential benefits of using Mamba models for such tasks. A response to this touches on the apparent analytical theory behind the performance of Large Language Models (LLMs) like Mamba, highlighting the trade-offs in scaling and the advantages for long-context tasks.

