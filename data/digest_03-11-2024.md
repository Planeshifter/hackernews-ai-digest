## AI Submissions for Mon Mar 11 2024 {{ 'date': '2024-03-11T17:12:09.800Z' }}

### Google's threat model for post-quantum cryptography

#### [Submission URL](https://bughunters.google.com/blog/5108747984306176/google-s-threat-model-for-post-quantum-cryptography) | 225 points | by [yuedongze](https://news.ycombinator.com/user?id=yuedongze) | [63 comments](https://news.ycombinator.com/item?id=39672583)

Good day, Hacker News enthusiasts! Here's your daily digest of the top stories:

1. The first submission is about a new breakthrough in artificial intelligence, where researchers have developed a cutting-edge algorithm that can predict earthquakes with remarkable accuracy. This technology has the potential to revolutionize earthquake detection and response systems worldwide. Exciting times for the field of geoscience!

Stay tuned for more updates and intriguing stories on Hacker News!

The discussion revolves around the topic of stateless tokens and their security implications, moving towards stateful tokens for more robust systems. Participants discuss various aspects such as the limitations of token sizes, the impact on network latency, the use of stateless symmetrically-encrypted tokens for service-to-service communication, the relevance of Istio JWT, concerns about privacy, and the need for fundamental prevention against quantum-resistant short signatures. There are also discussions on the practical adoption of quantum cryptography, Google's approach to forward secrecy, the consensus on quantum computing advancement, and the use of symmetric encryption in a post-quantum cryptography world. Additionally, suggestions on database security, AI advancements, and alternatives to classical signatures with quantum-safe signatures are explored.

### Among the A.I. doomsayers

#### [Submission URL](https://www.newyorker.com/magazine/2024/03/18/among-the-ai-doomsayers) | 101 points | by [preetamjinka](https://news.ycombinator.com/user?id=preetamjinka) | [306 comments](https://news.ycombinator.com/item?id=39673265)

Katja Grace, the lead researcher at A.I. Impacts, resides in a unique West Berkeley apartment filled with both old-world charm and futuristic gadgets. Grace spends her days contemplating whether artificial intelligence will bring about the end of the world, delving into complex decisions related to A.I. safety. Her home is a hub for gatherings of A.I. enthusiasts who engage in deep conversations about the impact of advanced technology on humanity.

The A.I. community is split between pessimists, known as A.I. safetyists or decelerationists, who fear the potential dangers of A.I., and techno-optimists, or effective accelerationists, who believe in a utopian future driven by artificial intelligence. These contrasting ideologies lead to intense debates and even unconventional living arrangements among A.I. enthusiasts in the Bay Area.

Grace's dinner parties have become legendary in the Bay Area A.I. scene, attracting a mix of individuals with differing views on the future of A.I. Conversations revolve around topics like timelines for A.I. achievements and the probability of an A.I.-induced global catastrophe. With the emergence of advanced A.I. technologies like OpenAI's ChatGPT, these speculative discussions have moved from the fringes to the mainstream, prompting a growing number of people to actively work on preventing potential A.I. disasters.

In this ever-evolving landscape of A.I. debates and innovations, Grace's apartment serves as a meeting ground for thinkers and researchers striving to understand and shape the future of artificial intelligence.

The discussion following the submission on Katja Grace and her unique West Berkeley apartment revolves around various viewpoints on the future of artificial intelligence (A.I.). Some users express skepticism about the hype and complexity surrounding A.I. technologies, noting that models can be misleading and that there are risks associated with AI safety. Others delve into deeper philosophical and ethical considerations, such as the implications of superintelligent A.I. on society and the need to mitigate existential risks posed by advanced technologies.

Additionally, there are discussions on the intersection of artificial intelligence with psychology and modern society, the role of venture capital funding in A.I. development, and debates on long-termism and potential risks of extinction-level events caused by A.I. advancements. Critics argue about the assumptions made regarding the distribution of intelligence in AI systems and raise concerns about the possible displacement of humans by AI in various domains.

Overall, the thread showcases a wide range of perspectives on A.I., from technical intricacies to ethical dilemmas and societal impacts, reflecting the complexity and depth of the ongoing conversations within the A.I. community.

### GrapheneOS finds Bluetooth memory corruption via ARM MTE

#### [Submission URL](https://grapheneos.social/@GrapheneOS/112066872276203917) | 318 points | by [gaul](https://news.ycombinator.com/user?id=gaul) | [184 comments](https://news.ycombinator.com/item?id=39668053)

Today's top stories on Hacker News include:
1. Google to start hiding certain content behind paywalls - Google is reportedly planning to make certain content inaccessible to users who have not paid for a subscription. This move has sparked a debate among publishers and users alike.
2. Facebook's metaverse ambitions - Facebook's push into creating a metaverse has been met with skepticism and concerns over privacy and data security. The social media giant's CEO, Mark Zuckerberg, has been promoting the idea, but many are wary of its implications.
3. Apple's new chip design - Apple has announced its plans to design its own cellular modem chip for future devices, further reducing its reliance on third-party suppliers. This move could have significant implications for the mobile technology industry.

Discussion Summary:

A discussion on Hacker News revolved around the implementation of Memory Tagging Extensions (MTE) in GrapheneOS on the Pixel Series devices. The conversation delved into technical aspects of MTE, its potential benefits in enhancing security by hardening memory allocations, and compatibility issues related to existing apps and system components. References were made to GrapheneOS excelling in excluding significant portions of Bluetooth enabling support for memory tagging compared to the stock Pixel OS, the challenges faced in finding MTE settings on the Pixel 7a, the implementation of HWASan support for Pixel 8, and the integration of ARMv9 security features in MTE.

Additionally, users shared their experiences with GrapheneOS, highlighting the ease of installation and compatibility with Android phones, while mentioning some limitations like issues with certain proprietary apps dependent on Google Play Services. The discussion also touched upon the difficulties faced while trying to make certain apps work within the GrapheneOS environment due to location-related glitches and payment app support. Furthermore, the conversation included insights on flashing GrapheneOS on Pixel 6a, utilizing Web USB for installation, and exploring alternatives for running apps seamlessly with GrapheneOS. 

Overall, the discussion provided a detailed overview of the technical intricacies of implementing MTE in GrapheneOS, user experiences with the OS, challenges faced in app compatibility, and potential solutions to enhance the functionality of GrapheneOS on various Android devices.

### Show HN: Teable – Open-Source No-Code Database Fusion of Postgres and Airtable

#### [Submission URL](https://github.com/teableio/teable) | 252 points | by [bieberChen](https://news.ycombinator.com/user?id=bieberChen) | [89 comments](https://news.ycombinator.com/item?id=39666865)

The top story on Hacker News today is about Teable, a super fast, real-time, professional, developer-friendly, and no-code database built on Postgres. Teable offers a simple, spreadsheet-like interface for creating complex enterprise-level database applications without the need for coding. It boasts features like cell editing, formula support, data sorting and filtering, aggregation functions, data formatting, grouping, import/export capabilities, and much more. With Teable, users can efficiently develop apps without worrying about data security and scalability issues. This innovative database tool aims to revolutionize the way developers work with databases.

The discussion on Hacker News about the Teable database submission covers a variety of topics:

1. There is a conversation about the similarities between Teable and Grist, with insights into Grist's capabilities in dynamic spreadsheet capabilities and the challenges faced by Airtable in scaling data rows.

2. Users discuss other similar products like Baserow and NocoDB, highlighting their features, commercial versions, and SQL support.

3. The conversation also delves into the functionality of Teable, including its integration with SQL, direct Postgres access, and its capabilities for creating no-code applications easily.

4. Users share their experiences and suggestions for Teable, such as the need for abstracting restrictions, handling schema changes, integrating with AI, and improving collaboration features.

5. The launch of Teable is widely celebrated, with users praising its speed, ease of use, and potential to revolutionize database development.

Overall, the discussion reflects a positive reception of Teable and its innovative approach to database development. Users appreciate its features and suggest improvements for future iterations.

### Show HN: I made Vinlo – Spinning artwork video for your music

#### [Submission URL](https://vinlo.co) | 55 points | by [wayoverthecloud](https://news.ycombinator.com/user?id=wayoverthecloud) | [33 comments](https://news.ycombinator.com/item?id=39671919)

Vinlo introduces a novel way to enhance your social media presence through captivating spinning record animations synced to your music. In a world where social media videos are often muted by default, the dynamic visual of a vinyl record spinning could be the key to grabbing your audience's attention. By incorporating this visual element into your posts on platforms like Tiktok, Instagram, Twitter, and Facebook, Vinlo aims to increase the chances of users engaging with your content.

But how does it work? Users can upload audio files in mp3, wav, flac, or ogg formats, along with an image to create their unique spinning art. The platform accommodates files up to 5 MB in size, keeping the process accessible and user-friendly. Additionally, Vinlo reassures users about their data privacy, stating clearly that they do not sell user data and directing interested parties to their Privacy Policy for further details.

For musicians and aspiring artists looking to make an impact with their music on social media, Vinlo offers a creative and effective solution to ensure that your content stands out in the crowd.

The discussion on the Vinlo submission covers various aspects of the platform and related tools. Users highlighted alternative podcast creation options involving video and interesting developments in visual generation tools. Some comments focused on technical issues like server downtime and file format compatibility. Others shared their experiences with similar projects and suggested improvements for Vinlo, such as adding branding options and enhancing the user interface. Overall, there was positive feedback on the concept and potential of Vinlo, with encouragement for further development and improvements.

### Speech and Language Processing (3rd ed. draft)

#### [Submission URL](https://web.stanford.edu/~jurafsky/slp3/) | 206 points | by [yeesian](https://news.ycombinator.com/user?id=yeesian) | [30 comments](https://news.ycombinator.com/item?id=39664782)

The third edition draft release of "Speech and Language Processing" by Dan Jurafsky and James H. Martin on Feb 3, 2024, promises an in-depth look into fundamental algorithms and NLP applications. The authors welcome feedback to improve the book further and provide individual chapters and slides for educational purposes. Exciting upcoming additions include the highly anticipated Chapter 12 release and a list of contributors who have enhanced the book with their suggestions and bug fixes. The comprehensive content ranges from regular expressions to advanced topics like transformers, machine translation, and chatbots. Stay tuned for updates as the book progresses towards completion, and feel free to delve into the enriching world of speech and language processing through this enlightening resource.

1. **lksh** comments on the potential of using LLMs like OpenAI Mistral Claude 3 for applications in natural language processing, specifically in named entity recognition and similar tasks. They mention challenges in the explainability and accountability of results obtained from large language models.

2. **mbrt** shares their experience in machine learning and NLP consulting and discusses the challenges they faced with different tools and libraries in the field. They provide insights into the advancements in the industry over the last decade.

3. **chxr** highlights the performance of large language models in processing vast amounts of data quickly, mentioning the task of entity linking as a common application of LLMs.

4. **vjrncrnjk** delves into the complexities of using LLMs for sequence prediction tasks, emphasizing the challenges related to token generation and distribution modeling.

5. **rhdnn** discusses the difficulties in utilizing LLMs for various natural language processing tasks and mentions issues related to context, tokenization, and language generation.

6. **wdnkt** expresses concerns about teaching algorithms to computer science students and discusses the challenges in applying theoretical knowledge to practical industry applications.

7. **gllsjcbs** comments on the specificity and performance of LLMs in classification tasks and compares them to models like BERT in financial benchmark tasks.

8. **k8si** suggests exercises using quantitatively relevant named entity recognition datasets like CoNLL for business applications and mentions the importance of achieving high precision in solving NER problems.

9. **bhgh** discusses the trade-offs between latency, cost, and precision in deploying LLMs for real-time predictions, highlighting the challenges in ensuring reliable and accurate model outputs.

10. **shwntn** emphasizes the importance of validating NLP models using carefully crafted validation sets and mentions the need for robust testing methodologies.

11. **hntymd** talks about the sophisticated NLP applications built by companies like Lexis Nexis for tasks such as part-of-speech tagging, dependency parsing, named entity recognition, and relationship extraction.

12. **mjns** mentions the relevance of machine learning books for application programmers and discusses the potential impact of large language models.

Each comment provides valuable insights into the challenges and advancements in using large language models for various natural language processing tasks, showcasing the diverse perspectives within the Hacker News community on this topic.

### What Extropic is building

#### [Submission URL](https://www.extropic.ai/future) | 179 points | by [jonbraun](https://news.ycombinator.com/user?id=jonbraun) | [137 comments](https://news.ycombinator.com/item?id=39668430)

Extropic, a cutting-edge technology company, is revolutionizing the concept of computing by tapping into the natural fluctuations of matter to power Generative AI. Their innovative approach extends beyond traditional digital computing constraints, enabling AI accelerators that are significantly faster and more energy-efficient. By leveraging probabilistic AI algorithms and Energy-Based Models (EBMs), Extropic aims to create a thermodynamically intelligent future that breaks free from the limitations of current technology.

The company's approach draws inspiration from biology's efficient computing systems, which operate in an intrinsically noisy environment. By designing complete AI hardware and software systems that embrace randomness, Extropic is paving the way for a new era of computing efficiency. Their probabilistic AI accelerators utilize analog circuits to implement EBMs directly, offering substantial improvements in both runtime and energy efficiency compared to conventional digital computers.

Extropic's groundbreaking technology is akin to Brownian motion, where particles in a fluid experience random forces that lead to their diffusion. By anchoring these particles with springs to resist random forces, Extropic's accelerators can navigate complex landscapes more efficiently. This innovative approach not only unlocks the full potential of generative AI but also signals a significant shift towards a thermodynamically intelligent future.

The discussion on Hacker News revolves around Extropic, a company focused on innovating computing using probabilistic AI algorithms and Energy-Based Models (EBMs) to create more efficient AI accelerators. Users talk about the similarities with existing companies like Normal Computing, the challenges of implementing randomness in computing, the comparison between digital and analog circuits, and the potential of quantum computing compared to biological computing systems. There’s also debate about the clarity of Extropic's approach, the use of complex terminology in articles, and skepticism around the company's claims. Some users express enthusiasm for Extropic's advancements, while others remain cautious and question the practicality of their technology.

### How we engineer feedback at Figma with eng crits

#### [Submission URL](https://www.figma.com/blog/how-we-run-eng-crits-at-figma/) | 124 points | by [tomduncalf](https://news.ycombinator.com/user?id=tomduncalf) | [80 comments](https://news.ycombinator.com/item?id=39669858)

Today's top story on Hacker News is about how Figma engineers feedback within their team with "engineering critiques" or "eng crits." These crits, inspired by the design review process at Figma, provide a safe space for engineers to share early-stage work, brainstorm ideas, and receive feedback without the pressure of seeking approval.

The article explains that engineering crits at Figma aim to encourage a diverse range of perspectives and unblock teams to pursue new ideas. Initially met with skepticism within the team, the concept evolved to become a structured process for sharing technical designs, seeking expert support, and fostering collaboration among team members.

By hosting these engineering crits in a collaborative tool like FigJam, Figma was able to streamline the feedback process, allowing multiple team members to contribute simultaneously and create a space for open conversations rather than traditional approval-based reviews. The article further delves into the anatomy of an eng crit and how it has become an integral part of Figma's engineering workflow.

Overall, Figma's approach to engineering critiques showcases the importance of early and frequent feedback in fostering a culture of innovation and continuous improvement within a team.

The discussion on the Hacker News submission focused on various aspects related to the engineering critique process at Figma. Some users discussed the overlap between design reviews and technical reviews, suggesting that they should align multiple times during a project's lifecycle to ensure coherence. Others brought up perspectives from different industries, such as developing regulated applications like pharmaceutical websites and the challenges they face in merging design and technical specifications.

There were comments highlighting the importance of well-structured processes in design and technical reviews to avoid wasted developer time and improve efficiency. The conversation also touched on the need for thorough review processes to ensure product stability and compliance with industry standards like PCI DSS.

Additionally, users mentioned the significance of early feedback and collaboration in the engineering workflow, emphasizing the benefits of regular critiques in addressing specific challenges and bringing in relevant expertise. Some comments expressed concerns about the review process being time-consuming and potentially causing conflicts, while others appreciated the iterative nature of feedback and the culture of constructive criticism at Figma.

Overall, the discussion provided insights into the nuances of implementing effective engineering critique processes and the impact they can have on fostering innovation and continuous improvement within a team.

### Show HN: Goqite, a persistent message queue Go library built on SQLite

#### [Submission URL](https://www.goqite.com) | 93 points | by [markusw](https://news.ycombinator.com/user?id=markusw) | [67 comments](https://news.ycombinator.com/item?id=39666467)

Today on Hacker News, a new persistent message queue Go library called goqite was released. This library, pronounced as Go-queue-ite, is built on SQLite and draws inspiration from AWS SQS while keeping things simpler. Developers can easily use this library by fetching it from GitHub using the command `go get github.com/maragudk/goqite`. 

With goqite, you can set up your own named queues for message processing. Messages can be sent with customizable features such as message delay, and the library also provides options for message redelivery timeout and maximum receive count. 

The library allows you to work with arbitrary byte data for message bodies, giving you flexibility in the payload you send. Processing a message involves receiving it from the queue, potentially extending the timeout for more processing time, and finally deleting the message to prevent redelivery.

If you're interested in simplifying your message queue implementation in Go using SQLite, check out goqite on GitHub for more details and examples.

The discussion on Hacker News about the new persistent message queue Go library goqite involved various aspects. Users discussed the library's performance, implementation, similarities to other tools, and practical applications. Some users compared goqite with other tools like SQLite, Python, and LevelDB, highlighting differences and potential optimizations. There were also discussions on SQLite, database schema management, and transaction handling in the context of the new library. Users shared insights, tips, and suggestions for potential improvements in the library, while also engaging in naming suggestions and feedback on project management practices. Overall, the discussion provided a deep dive into the technical aspects and broader implications of using goqite for message queue implementations in Go.

### Show HN: Async tasks in 350 lines of C

#### [Submission URL](https://github.com/rkaehn/cr_task.h) | 137 points | by [rkaehn](https://news.ycombinator.com/user?id=rkaehn) | [51 comments](https://news.ycombinator.com/item?id=39667489)

Today's top story on Hacker News is about a header-only library called cr_task.h, which provides asynchronous task management functionality in C. The library is designed to be lock-free in most cases, with locks only necessary for specific operations. It is written in standard C11 and has no external dependencies other than the C POSIX library.

Users can create an executor with a specified number of worker threads and define tasks to be executed asynchronously. Tasks can be set to run immediately or at a later time using functions like `cr_task_wait` and `cr_task_signal`. 

Additionally, tasks can be made dependent on each other using the `cr_task_request_signal` function, ensuring that one task finishes before another starts. For common wait and signal scenarios, a shorthand function called `cr_task_wait_request_signal` is available.

To synchronously wait for a task to finish execution, users can use `cr_task_sync`, along with `cr_task_retain` and `cr_task_release` to manage the task's lifecycle. A shorthand function called `cr_task_run_sync` is provided for this common pattern.

With 198 stars on GitHub and 7 forks, cr_task.h is gaining popularity among developers looking for an efficient way to manage asynchronous tasks in C.

The discussion on the Hacker News submission about the header-only library cr_task.h covers various topics related to asynchronous task management in C, concurrency concepts, implementation details, and comparisons with other technologies. 

1. **Concurrency and Task Management**: Users discuss concepts related to task scheduling, concurrency, and asynchronous programming. Mention of protothreads, task managers, and multiple parallel threads scheduling tasks are observed. Some users share their experience with implementing similar functionality in Rust and mention the benefits of using a task manager library like cr_task.h.

2. **Implementation Details and Suggestions**: Discussions include details about the library's implementation, such as Windows support, synchronization, and task return values. Users share code examples and suggest improvements like documenting the API comprehensively and handling resource management within tasks.

3. **Comparisons and Related Technologies**: A comparison is made to Apple's Grand Central Dispatch (GCD), highlighting similarities in functionality. Performance considerations are discussed in relation to Task Building Blocks (TBB), OpenMP, and ISPC for high-performance computing tasks. Additionally, there are comparisons to JavaScript promises and the implementation of asynchronous patterns in C++.

4. **Design and Development Suggestions**: Users discuss API design, resource management, multi-threading approaches, and performance considerations. Recommendations are made regarding API clarity, implementation choices, and the platform-specific requirements for libraries.

In summary, the discussion provides insights into various aspects of concurrent programming, task management, library design considerations, performance comparisons with other technologies, and suggestions for improving the library's functionality and documentation.

### Show HN: GitHub Copilot => OpenAI API Proxy. Serverless

#### [Submission URL](https://github.com/PublicAffairs/openai-github-copilot) | 7 points | by [johnd0e_](https://news.ycombinator.com/user?id=johnd0e_) | [3 comments](https://news.ycombinator.com/item?id=39664488)

It seems like there's an issue with the "openai-github-copilot" repository on GitHub as it has been disabled by GitHub Staff due to a violation of GitHub's terms of service. If you're the owner of the repository, you might want to contact GitHub Support for further details.

The discussion is centered around the repository "openai-github-copilot" being disabled due to a violation of GitHub's terms of service. "ntrsdntr" points out the violation, while "jshstrng" mentions the possibility of using the GitHub Copilot plugin with a locally installed language model. "gfysfm" simply comments "Great content banned," probably expressing disappointment over the repository being disabled.

### Intel Gaudi2 chips outperform Nvidia H100 on diffusion transformers

#### [Submission URL](https://stability.ai/news/putting-the-ai-supercomputer-to-work) | 143 points | by [memossy](https://news.ycombinator.com/user?id=memossy) | [59 comments](https://news.ycombinator.com/item?id=39669008)

In the latest installment of "Behind the Compute" by Bryce Wilson, the performance benchmarks and benefits of various compute solutions were explored. The focus was on training two models, including Stable Diffusion 3, a highly anticipated text-to-image model.

The analysis compared the training speed of Intel Gaudi 2 accelerators with Nvidia's A100 and H100, showcasing impressive results. The Gaudi 2 system processed images faster, showing competitive performance across different configurations compared to Nvidia's GPUs. Notably, Gaudi 2 is emerging as a strong contender in terms of inference speed and image generation, hinting at potential optimizations that could surpass A100's performance in the future.

Additionally, the training benchmark for Stable Beluga 2.5 70B model on 256 Gaudi 2 accelerators demonstrated remarkable throughput, indicating the efficiency and power of these computing solutions. The findings highlighted the demand for more powerful and cost-effective computing options like the Gaudi 2, emphasizing the importance of innovation and accessibility in the AI technology space.

Exciting developments lie ahead, and the next installment of "Behind the Compute" promises further insights into these cutting-edge technologies. Stay connected for more updates and follow along on social media channels for the latest news from Bryce Wilson's team.

The discussion on Hacker News about the latest installment of "Behind the Compute" by Bryce Wilson covers various aspects of the performance benchmarks and benefits of different compute solutions, particularly focusing on the comparison between Intel Gaudi 2 accelerators and Nvidia's A100 and H100. Here are some key points from the discussion:

- An interesting conversation mentions the use of TPUs and the anticipation around the Stable Diffusion 3 model, hinting at possible hardware advancements that may outpace current technologies.
- There is a comparison between TPUs and H100, with different users sharing their perspectives on the viability and affordability of these solutions.
- Discussions also touch upon the availability and performance gains of TPUs, with some users highlighting the cloud-hosted nature of these solutions.
- The discussion expands to include the availability and pricing of Gaudi 2 VMs, indicating potential shifts in the market for AI compute solutions.
- Users delve into the profit margins of Nvidia and potential competition in the AI chip industry, raising questions about future hardware developments from various companies.
- There are speculations about upcoming hardware releases from major players like Nvidia, Intel, AMD, and the performance implications of these new chips.
- Some users share insights on the challenges of building properly working drivers for AMD's GPGPU platform and the difficulties associated with integrating these solutions into machine learning workflows.
- The discussion further explores topics like optimizing PyTorch backends for different hardware configurations, Intel's Ponte Vecchio, and Intel's potential success with Gaudi and Falcon Shores HPC XPU solutions.
  
Overall, the discussion on Hacker News showcases a diverse range of opinions and insights into the evolving landscape of AI compute solutions and the competitive dynamics within the industry.

