## AI Submissions for Mon Jun 02 2025 {{ 'date': '2025-06-02T17:15:39.199Z' }}

### My AI skeptic friends are all nuts

#### [Submission URL](https://fly.io/blog/youre-all-nuts/) | 1928 points | by [tabletcorry](https://news.ycombinator.com/user?id=tabletcorry) | [2314 comments](https://news.ycombinator.com/item?id=44163063)

In a thought-provoking piece on AI-assisted programming, Thomas Ptacek takes a deep dive into the controversial adoption of Large Language Models (LLMs) by tech executives, questioning the skepticism often seen among some of the smartest individuals he knows. With more than 25 years of software development under his belt, Ptacek argues that LLMs, contrary to being a mere fad akin to NFTs, have significantly impacted software development and will continue to do so even if progress halts.

Ptacek points out that many critics might not fully grasp the latest AI tools, perhaps because they are merely dabbling with ChatGPT or similar models in outdated ways. Serious AI coders today use smarter, more autonomous agents that can navigate, test, and refactor codebases with surprising effectiveness. He argues that LLMs might not write perfect code, but they accelerate tasks by handling tedious, repetitive efforts—allowing developers to focus on refining the essential parts of projects.

For him, the adoption of LLMs ushers a new era of coding that minimizes tedious groundwork, potentially reigniting a developer’s passion for building and iterating on projects. He acknowledges that AI-generated code still requires manual adjustments, but this interaction doesn't doom AI's practicality; it only underscores the importance of human oversight in polishing AI’s drafts.

Addressing common critiques like AI hallucinations, Ptacek humorously suggests it's less about the flaws in AI and more about the adaptability of programming languages. With agents able to automatically identify and rectify invented errors, skepticism seems to stem from a misunderstanding of how LLMs integrate into modern coding practices.

Ultimately, Ptacek encourages developers to embrace these tools—highlighting that while LLMs might not replace the need to read and understand code, they reduce the preliminary legwork. In essence, LLM adoption isn’t about relinquishing creative control but augmenting human expertise with a potent ally that can handle the grunt work. For those hesitant to evolve, Ptacek suggests it might be time to shift perspectives and accept that AI may just be the next big leap in programming evolution.

**Summary of Discussion:**

The Hacker News discussion on Thomas Ptacek’s article about AI-assisted programming reveals a mix of enthusiasm, skepticism, and pragmatic adaptation. Key themes include:

1. **Personal Experiences with LLMs**:  
   - Users like **mtthwsnclr** shared their journey from skepticism to adoption, noting tools like **Claude Code** became effective when paired with detailed documentation and iterative refinement. They likened AI tools to Photoshop for artists—transformative but requiring skill.  
   - **spaceman_2020** highlighted rapid advancements, citing tools like **Cursor** that now handle complex tasks unimaginable months ago.  

2. **Evolution of LLM Utility**:  
   - Many agreed LLMs have evolved from generating "garbage" to becoming practical aids. **wptr** referenced the *Stone Soup* analogy, suggesting initial skepticism is natural, but tools stabilize and prove value over time.  
   - **kd** emphasized that LLMs’ effectiveness depends on the user’s background, enabling non-experts to code while requiring experts to adapt workflows.  

3. **Challenges and Limitations**:  
   - **rxxrrxr** and others noted difficulties in prompting LLMs for complex tasks, stressing the need for clear, structured input. **Cthulhu_** compared this to design thinking, arguing that conveying abstract concepts via prompts remains a hurdle.  
   - **algorithmsRcool** pointed out AI’s disruption of traditional design processes, though some found iterative prompting useful for scaffolding ideas.  

4. **Human Oversight and Skill**:  
   - **vmr** likened managing LLMs to training interns—requiring effort to guide and refine outputs. Others highlighted the need for developers to develop *new skills* (e.g., systematic prompting) to leverage AI effectively.  
   - **xp** argued perceptions of AI are shaped by roles and experience, with developers historically resisting new tools until they’re forced to adapt.  

5. **Hype vs. Reality**:  
   - While some dismissed AI as hype, others countered that skepticism often stems from outdated experiences. **wptr** cautioned against over-optimism but acknowledged LLMs’ incremental value.  
   - Comparisons to past shifts (e.g., TDD, open-source adoption) underscored that AI’s impact may unfold gradually, blending into workflows rather than replacing them.  

**Conclusion**: The community remains divided but leans toward cautious integration of AI tools. While LLMs are seen as powerful allies for reducing grunt work, their effectiveness hinges on human expertise, structured input, and iterative refinement. The discussion reflects a broader tension between excitement for AI’s potential and the pragmatic recognition of its current limitations.

### Japanese scientists develop artificial blood compatible with all blood types

#### [Submission URL](https://www.tokyoweekender.com/entertainment/tech-trends/japanese-scientists-develop-artificial-blood/) | 243 points | by [Geekette](https://news.ycombinator.com/user?id=Geekette) | [50 comments](https://news.ycombinator.com/item?id=44163428)

In an exhilarating breakthrough for global healthcare, Japanese scientists at Nara Medical University have made a leap forward in blood transfusion technology. Led by Hiromi Sakai, the team has developed an innovative type of artificial blood that defies traditional compatibility issues, making it universal for all blood types. This synthetic marvel is crafted from expired donor blood by extracting and re-engineering hemoglobin into virus-free artificial red blood cells, which come with an extended shelf life—up to two years at room temperature or five years when refrigerated. Such longevity is a game-changer compared to the 42-day limit for stored donated blood.

Following promising early trials that began in 2022, where volunteers received gradual doses with minimal mild side effects, the project is moving swiftly toward larger-scale trials. These new trials aim to establish the efficacy and safety of this groundbreaking blood substitute, with a hopeful eye on practical use by 2030.

In parallel, Professor Teruyuki Komatsu of Chuo University is exploring albumin-encased hemoglobin to target conditions like hemorrhage and strokes, with animal studies showing encouraging results. As researchers eagerly prepare for human trials, the development of artificial blood and oxygen carriers appears poised to revolutionize transfusion medicine and medical treatment worldwide, particularly in areas where blood supply is limited.

**Summary of Hacker News Discussion:**

The discussion around the Japanese artificial blood breakthrough highlights both excitement and skepticism, drawing parallels to past efforts and addressing technical, ethical, and commercial challenges:

1. **Historical Precedents & Challenges:**
   - Users referenced **Biopure**, a 2000s-era company that developed a cow hemoglobin-based oxygen therapeutic (Oxyglobin). Despite FDA approval for veterinary use, it faced legal issues, mismanagement, and failed human trials. A senior executive was even sentenced for fraud, underscoring the risks of corporate misconduct.
   - **PolyHeme**, another blood substitute, was criticized for unethical trials where trauma patients received it without explicit consent, raising concerns about research ethics.

2. **Technical Considerations:**
   - Some noted that hemoglobin-based substitutes (from expired human blood, cow blood, or **plant-based sources**, like leghemoglobin) face challenges in stability, scalability, and safety. Recombinant human hemoglobin production remains technically demanding.
   - **Perfluorocarbons** (PFCs), fully synthetic oxygen carriers used in Mexico and Russia, were mentioned as alternatives, though "liquid breathing" with PFCs was described as unsettling.

3. **Regulatory and Commercial Hurdles:**
   - Past failures were attributed to poor business models, patent expirations, and regulatory roadblocks. Users speculated whether the Japanese team’s approach could avoid these pitfalls, especially given the long timeline (targeting 2030 for deployment).
   - **Kalocyte**, a U.S. company partnering with DARPA on shelf-stable artificial blood, was cited as a parallel effort.

4. **Ethical and Practical Implications:**
   - The potential to aid **Jehovah’s Witnesses** (who refuse blood transfusions) was highlighted as a key application.
   - Concerns were raised about **blood doping** in sports, as hemoglobin-based products could be misused to enhance athletic performance, similar to past scandals (e.g., Tour de France).

5. **Skepticism and Optimism:**
   - While some praised the Japanese team’s progress (noting successful rabbit trials and early human safety data), others questioned scalability and whether the technology would face the same fate as earlier attempts.
   - The extended shelf life (2–5 years vs. 42 days for blood) was seen as transformative, especially for disaster response and regions with limited blood supplies.

**Conclusion:** The discussion reflects cautious optimism, balancing enthusiasm for a potential medical breakthrough with lessons from past failures. Technical innovation, ethical oversight, and sustainable business models will likely determine its success.

### Show HN: Penny-1.7B Irish Penny Journal style transfer

#### [Submission URL](https://huggingface.co/dleemiller/Penny-1.7B) | 144 points | by [deepsquirrelnet](https://news.ycombinator.com/user?id=deepsquirrelnet) | [71 comments](https://news.ycombinator.com/item?id=44160073)

In the spirit of the 19th century, the newly unveiled Penny-1.7B model is making waves in the world of AI with its exquisite flair for the Victorian-era prose of the Irish Penny Journal. This marvel of machine learning, fine-tuned through Group Relative Policy Optimization (GRPO), undertakes the stylistic transformation of ordinary text, transporting readers back to 1840 with its ornate diction and rhythmic cadence.

Crafted upon the sophisticated SmolLM2 backbone, Penny-1.7B boasts an impressive 1.7 billion parameters, each meticulously adjusted across 6,800 policy steps. The reward model, a MiniLM2 L6 384H classifier, ensures outputs echo the quaint yet elegant spirit of yesteryear’s journals. Whether for creative writing, educational endeavors, or a literary trip through time, this model deftly blends historical charm with modern-day reasoning.

However, users are cautioned against relying on Penny-1.7B for contemporary facts or in situations where clarity is paramount, as its dedication to archaic style might obscure current veracity. Moreover, the model's Victorian influences may inadvertently reflect outdated societal norms, necessitating vigilance when reviewing its outputs.

For those eager to explore, the model can be accessed via Hugging Face under the Apache 2.0 license, ready to infuse narratives with a nostalgic touch or inspire research in the dynamic field of style transfer. As an ode to the power of AI and the elegance of language's past, Penny-1.7B invites users to savor the prose of a bygone era.

The Hacker News discussion on the **Penny-1.7B** model explores its potential applications in gaming (particularly RPGs) while debating the challenges of integrating AI-generated text into interactive storytelling. Here’s a summary of key points:

### Key Themes:
1. **Dynamic NPC Dialogue**  
   - Users envision AI like Penny-1.7B reducing repetitive NPC interactions in games like *Skyrim* by generating context-aware, Victorian-styled dialogue. However, concerns arise about maintaining coherence, avoiding immersion-breaking responses, and ensuring relevance to in-game events.  
   - Suggestions include **hybrid systems** (e.g., pre-scripted prompts paired with AI-generated variations) to balance creativity and consistency. *Disco Elysium*’s dialogue system is cited as inspiration for rewarding role-playing and character-driven interactions.

2. **Technical Challenges**  
   - Small models may struggle with context retention, factual accuracy, and "hallucinations." Methods like **LoRA adapters** or **prefix tuning** are proposed to optimize efficiency.  
   - A "journaling system" could track key narrative beats, ensuring NPCs reference prior player actions or world events without excessive repetition.

3. **Design Considerations**  
   - Scripted dialogue remains critical for plot advancement, while AI could handle ambient "small talk" (e.g., villagers discussing local rumors).  
   - UI cues (e.g., text color/syle) might help players distinguish AI-generated vs. scripted dialogue, as seen in *Baldur’s Gate 3*.  

4. **Historical Comparisons**  
   - Older systems like *AI Dungeon* (2019) highlighted pitfalls: erratic outputs, limited character knowledge, and off-topic responses. Users stress the need for strict narrative "rails" to avoid similar issues.

5. **Creative Potential**  
   - Despite challenges, participants express excitement for AI’s role in enriching open-world immersion (e.g., generating lore-friendly gossip or reactive dialogue) and inspiring experimental storytelling.  

### Criticisms & Cautions:  
- Over-reliance on AI risks diluting narrative focus or alienating players with verbose, irrelevant text. Smaller models, while faster, may lack nuance.  
- Ethical concerns include inadvertent reinforcement of outdated norms through stylized language.  

Ultimately, the discussion reflects cautious optimism: Penny-1.7B and similar models could revolutionize in-game storytelling but require careful design to complement, not replace, traditional scriptwriting.

### ReasoningGym: Reasoning Environments for RL with Verifiable Rewards

#### [Submission URL](https://arxiv.org/abs/2505.24760) | 97 points | by [t55](https://news.ycombinator.com/user?id=t55) | [27 comments](https://news.ycombinator.com/item?id=44157077)

In an exciting development for the reinforcement learning community, a group of researchers has unveiled "Reasoning Gym" (RG), a dynamic new library for challenging AI with verifiable rewards. Highlighted in their paper recently submitted to arXiv, authors Zafir Stojanovski, Oliver Stanley, Joe Sharratt, Richard Jones, Abdulhakeem Adefioye, Jean Kaddour, and Andreas Köpf have introduced a suite featuring over 100 data generators and verifiers. These span an impressive array of domains including algebra, cognition, geometry, and even common games.

The standout feature of RG is its capability to produce virtually limitless training data while toggling complexity levels, setting it apart from prior static datasets. This approach enables continuous evaluation and adaptive learning, potentially revolutionizing how reasoning models are trained and assessed. The team's experimental results emphasize RG's effectiveness, showcasing its pertinence in the exploration of complex reasoning tasks within reinforcement learning frameworks.

To access the full findings, you can view the paper on arXiv, where intrigued developers can also explore the associated code to integrate RG's promising features into their own projects.

The Hacker News discussion surrounding the "Reasoning Gym" (RG) paper reflects a mix of enthusiasm for its potential and critical technical debates:

1. **Excitement and Comparisons**:  
   Users highlight RG’s promise for dynamic data generation and adaptive learning, with comparisons to models like Gemini 1.5 Pro. Debates arise over whether Gemini’s performance stems from its long-context training (100K+ tokens) or architectural innovations. Some speculate Google DeepMind’s RL focus drives Gemini’s capabilities, while others note RL’s long-standing roots (e.g., Q-learning since 1989).

2. **Novelty Challenges**:  
   Skepticism emerges around claims of RG enabling "novel reasoning strategies." A user argues observed improvements might not reflect true novelty but instead better execution of pre-existing strategies. Experiments showing high success rates (e.g., 99%) are questioned, with alternative explanations proposed, such as probability mass shifts toward favorable outcomes during training.

3. **Reinforcement Learning Dynamics**:  
   Discussions diverge into broader RL themes. For example, a comment highlights a separate paper ("Spurious Rewards") where rewarding incorrect or random outputs paradoxically boosts benchmark performance, likening this to regularization or GAN-like adversarial training. Users debate whether RG’s RL approach merely amplifies existing good behaviors rather than fostering new strategies.

4. **Benchmarks and Contributions**:  
   RG’s adjustable difficulty and non-repetitive validation tasks are praised as valuable benchmarks. However, GSM8K and MATH benchmarks are noted as tougher challenges. Contributors express interest in the project’s open-source potential, and the authors respond positively to collaboration.

5. **Technical Quibbles**:  
   Some comments are flagged or nonsensical, while others question the verification methods used in RG, emphasizing the need for rigorous, unbiased evaluation. 

The thread underscores cautious optimism: RG is seen as a promising tool for advancing RL and reasoning tasks, but its claims are met with calls for clearer evidence distinguishing *novel strategies* from refined execution of known methods.

### Show HN: I built an AI Agent that uses the iPhone

#### [Submission URL](https://github.com/rounak/PhoneAgent) | 48 points | by [rounak](https://news.ycombinator.com/user?id=rounak) | [13 comments](https://news.ycombinator.com/item?id=44155426)

In today's tech round-up from Hacker News, we dive into an exciting open-source project called PhoneAgent by Rounak, which cleverly integrates OpenAI models with iPhones to function like a personal assistant within your device's apps. Created during an OpenAI hackathon, PhoneAgent is an innovative tool that leverages the GPT-4.1 model to automate tasks such as sending messages, snapping selfies, booking rides, and more. Users provide commands either via text or voice, and the app acts like a human user interacting with your phone.

What makes PhoneAgent stand out is its use of Xcode's UI testing framework, allowing it to inspect and perform actions across different apps without requiring a jailbreak. It taps into an app's accessibility tree, enabling it to pinpoint and interact with elements just like you would. The app's capabilities include executing commands through a TCP server and persisting your OpenAI API key securely on your device. Plus, it supports an "Always On" feature that listens for the wake word, adding to its convenience even when running in the background.

However, it’s worth noting that PhoneAgent has its share of limitations, such as challenges with keyboard inputs and occasional misinterpretation of tasks during animations. It's still experimental, so the developers recommend running it in an isolated environment. Since app content is transmitted to OpenAI's APIs, privacy and security are important considerations for users.

These compelling features, combined with its open-source nature under the MIT license, have attracted attention, amassing 354 stars and 45 forks on GitHub. Whether you're a developer or just tech-curious, PhoneAgent is an intriguing project worth exploring.

**Summary of Discussion:**

1. **Security & Privacy Concerns:**  
   Users raised significant concerns about PhoneAgent's access to sensitive data (credit cards, calendars, Signal messages) and its reliance on off-device processing via OpenAI. Meredith Whittaker's critique highlights potential risks, as transmitting app content to external servers could undermine privacy, especially for encrypted services like Signal.

2. **AI Ethics & Sci-Fi Parallels:**  
   Comments humorously referenced sci-fi scenarios (e.g., *Terminator*’s John Connor, Asimov’s robotics laws) to discuss ethical implications. Debates emerged around designing AI agents that avoid harm, with nods to *Horizon* games and *Futurama*’s "Robosexuals" as cultural touchstones for synthetic life dilemmas.

3. **Apple’s AI Integration Speculation:**  
   Speculation arose about Apple potentially adopting similar AI agent technology, with skepticism around whether Apple Intelligence would materialize at WWDC. Some users doubted Apple’s commitment due to security vulnerabilities highlighted in recent reports.

4. **Technical Limitations & Feasibility:**  
   Questions about PhoneAgent’s practical limitations included challenges with iOS sandboxing and App Store restrictions. A linked technical explanation clarified its use of Xcode’s UI testing framework, avoiding jailbreaking but requiring local device execution.

**Key Themes:** Privacy risks of cloud-dependent AI, ethical AI design, corporate AI adoption skepticism, and technical hurdles in app integration. Humorous sci-fi analogies underscored broader societal anxieties about autonomous agents.

### Show HN: Agno – A full-stack framework for building Multi-Agent Systems

#### [Submission URL](https://github.com/agno-agi/agno) | 72 points | by [bediashpreet](https://news.ycombinator.com/user?id=bediashpreet) | [19 comments](https://news.ycombinator.com/item?id=44155074)

Today on Hacker News, the spotlight shines on Agno—a powerful new framework for developers eager to harness the power of Multi-Agent Systems (MAS) with memory, knowledge, and reasoning. True to its name—a nod to AGI (Artificial General Intelligence)—Agno offers a robust full-stack framework that simplifies building complex, intelligent systems.

Agno is designed to craft agents across five levels of complexity, starting from basic tool-using entities to sophisticated teams and workflows capable of reasoning, collaboration, and maintaining determinism. An eye-catching feature for developers is its model-agnostic nature; it provides a single interface for over 23 model providers, liberating users from vendor lock-ins while ensuring highly performant agent instantiation—averaging a swift ~3μs startup time with a memory footprint of ~6.5KiB.

Another highlight of Agno is its focus on reasoning, regarded as a cornerstone in developing reliable, autonomous agents. To this end, Agno champions structured reasoning models and provides tools and custom methodologies for enhanced cognitive capabilities. Notably, the agents are natively multi-modal, meaning they can interpret and output diverse media types, including text, images, audio, and video.

Developers will appreciate Agno’s advanced multi-agent architecture. This architecture enables agents to work in teams, pooling memory and reasoning skills to manage greater workloads effectively. Its built-in features for agentic search, memory, and session storage further enable it to serve real-time, complex data needs like stock analytics, with tools like YFinance baked into its toolkit.

For those eager to jump in, Agno provides a streamlined path from local development using FastAPI to monitoring live performance on agno.com, promising a seamless transition from concept to real-world application. Whether you are exploring the documentation or crafting your first agent, Agno presents a compelling offer to save time and effort in building next-generation AI systems.

In essence, Agno seems to be paving the way for intuitive, flexible, and powerful development of MAS, promising a bright future for developers and businesses aiming to harness the orchestrated intelligence of multi-agent systems. 

Check out the full repository and dive deeper into what Agno has to offer for your next AI project.

**Summary of Discussion:**

The discussion around Agno highlights a mix of enthusiasm, constructive feedback, and critical concerns:

1. **Praise for Usability & Performance**:  
   - Users like ElleNeal and idan707 commend Agno for simplifying agent development and its effectiveness in production. The framework’s minimal dependencies, scalability (e.g., handling 10k requests/minute), and efficient resource usage are noted as strengths.  
   - Contributors highlight its ability to spawn thousands of agents for tasks like spreadsheet validation, emphasizing real-world applicability.

2. **Concerns About Abstraction & Customization**:  
   - Some users (e.g., mxtrmd) worry that Agno’s structured approach might limit customization, trading flexibility for ease of use. Critics argue that overly abstract frameworks risk becoming "LLM wrappers" without clear value (bosky101).  

3. **Debate Over Framework Necessity**:  
   - lrchm questions the need for dedicated frameworks like Agno when existing models (e.g., Claude) can orchestrate agents via prompts and function calls. Others stress the importance of native JSON schema support and performance optimizations over ad-hoc solutions.  

4. **Performance & Scalability Discussions**:  
   - JimDabell raises concerns about potential bottlenecks when scaling to thousands of agents, citing startup time and memory overhead. Agno’s team (bdshprt) defends its focus on low latency (~3μs startup) and efficient resource management, arguing that performance is critical for production systems.  

5. **Documentation & Examples Feedback**:  
   - While nbtws praises Agno’s cookbook examples for clarifying workflows, others criticize the documentation as messy or incomplete. Suggestions include cleaner examples, helper functions, and session-state management.  

6. **Deployment Considerations**:  
   - fcp notes the trade-offs between local development and cloud deployment, with Agno’s cloud services offering better control and monitoring.  

7. **Mixed Reactions on Use Cases**:  
   - Some users question the practicality of multi-agent systems versus simpler single-agent solutions, urging more compelling examples (bosky101).  

**Overall**: Agno is seen as a promising tool for scalable, production-ready multi-agent systems, but its adoption may hinge on addressing customization limits, documentation clarity, and demonstrating tangible advantages over alternative approaches.

### How can AI researchers save energy? By going backward

#### [Submission URL](https://www.quantamagazine.org/how-can-ai-researchers-save-energy-by-going-backward-20250530/) | 62 points | by [pseudolus](https://news.ycombinator.com/user?id=pseudolus) | [44 comments](https://news.ycombinator.com/item?id=44155391)

Researchers exploring the energy-saving promise of reversible computing are revisiting an old idea that initially seemed like a dead end: running programs backward. Originally championed by IBM physicist Rolf Landauer, who linked information processing with thermodynamics, reversible computing aims to avoid energy waste by not deleting data. However, it was Charles Bennett, a colleague of Landauer's, who revolutionized the concept in 1973 with his idea of "uncomputation." This technique allows calculations to be undone, thereby preserving energy but initially proved impractical due to performance issues.

Fast forward to now, as computing demands, particularly from AI applications, increase and conventional chip improvements stall due to physical limitations, the idea of reversible computing is gaining fresh interest. By carefully orchestrating operations to avoid any loss of information, therefore no energy is lost as heat, this could pave the way for highly efficient computing systems. Research continues into making these computers feasible, with tweaks largely focused on balancing the trade-offs between memory usage, computation time, and energy savings.

With AI's vast energy consumption, such innovation could lead to breakthroughs, sustaining progress diversified approaches like light-based chips are also being considered. However, achieving significant energy savings through reversible computing will necessitate new designs for low-heat transistors from the outset. Efforts by engineers at MIT and elsewhere are underway to configure these machines to fulfill their energy-efficient promise, potentially taking us a step closer to sustainable AI systems.

**Summary of Hacker News Discussion on Reversible Computing:**

The discussion around reversible computing explores its theoretical promise and practical challenges, with contributions from users diving into thermodynamics, hardware limitations, and applications in AI and quantum computing. Key points include:

1. **Thermodynamics & Landauer’s Principle**:  
   - Deleting information is inherently irreversible and generates heat, per Landauer’s principle. Reversible computing avoids this by preserving data, theoretically minimizing energy waste. However, real-world hardware (e.g., NAND gates, resistive components) still dissipate heat, limiting practical gains.  
   - Landauer’s limit (~10⁻²¹ J/operation at room temperature) is far below current transistor energy use, but advancing AI workloads may push hardware closer to this boundary.

2. **Practical Challenges**:  
   - **Memory Overhead**: Storing computation history for reversible operations increases memory usage, complicating efficiency trade-offs.  
   - **Heat from Existing Hardware**: Resistance in modern chips and persistent storage (e.g., SSDs) generates heat during read/write cycles, undermining potential energy savings.  
   - **Hardware Redesign**: Truly reversible systems require new low-heat transistors (e.g., CMOS successors or optical components), which remain underdeveloped.  

3. **Quantum Computing Connection**:  
   - Quantum computers inherently use reversible logic gates, aligning with reversible computing principles. However, classical reversible systems face skepticism about practicality compared to quantum advancements.  

4. **Machine Learning Applications**:  
   - Techniques like invertible neural networks (e.g., Normalizing Flows) and differentiable simulations already leverage reversible computation for tasks like generative modeling. Workshops and research (e.g., 2019-2021 Invertible Neural Networks workshops) highlight interest in energy-efficient ML architectures.  

5. **Skepticism & Counterpoints**:  
   - Some users question if energy savings would follow due to **Jevons paradox** (efficiency leading to increased usage) or unresolved hardware issues (e.g., heat from non-reversible components).  
   - Reversible matrix operations and reversible algorithms were debated, with users noting that even "reversible" steps may still lose information indirectly.  

6. **Theoretical vs. Real-World**:  
   - While reversible systems could theoretically consume no energy when idle, real-world implementations require power to maintain state, especially in classical architectures. Quantum systems, however, may better achieve near-zero energy computation.  

**Conclusion**: The consensus acknowledges reversible computing’s theoretical promise but emphasizes significant hurdles in hardware innovation and system design. Researchers remain optimistic about long-term potential, particularly for sustainable AI, but stress that breakthroughs in materials science and component engineering are prerequisites for meaningful progress.

