## AI Submissions for Wed Sep 06 2023 {{ 'date': '2023-09-06T17:10:24.501Z' }}

### AOL pretends to be the internet

#### [Submission URL](https://thehistoryoftheweb.com/postscript/aol-pretends-to-be-the-internet/) | 148 points | by [janvdberg](https://news.ycombinator.com/user?id=janvdberg) | [243 comments](https://news.ycombinator.com/item?id=37404918)

AOL, formerly known as Quantum Computer Services, started out as a closed-loop network for computer users to connect with one another. However, in the mid-90s, AOL rebranded as America Online and aimed to bring networked computing to the masses. Their marketing strategy included an aggressive direct mailing campaign, sending out floppy disks and later CD-ROMs to households. The disks provided a few hours of free service, and once people started using AOL, they became hooked. AOL positioned itself as a content provider and brokered deals with major publishers and media companies. They wanted to own the experience of digital networks for their consumers and even considered producing original content exclusive to AOL. However, the rise of the Internet posed a threat to AOL's closed system. In 1994, AOL expanded support to certain Internet protocols, but notably, they did not have a web browser until acquiring BookLink in 1995 and adding Microsoft's Internet Explorer in 1996. AOL tried to position themselves as part of the Internet, but their identity eventually became subsumed by the wider, community-driven web and internet.

The discussion revolves around various aspects of URLs, QR codes, and the use of technology in daily life.

- Some users mention the use of AOL keywords and how they were an alternative to typing in full URLs. There is a reminiscence of using AOL and how it was a recognizable domain name that people would type.
- Others share their experiences with using URLs and how some websites reject URLs with a "www" prefix. There is a debate about whether it is necessary to include "www" before a website address.
- The importance of user experience and customer support is discussed, with some users emphasizing the need for intuitive interfaces and responsive support.
- The topic of email addresses is brought up, with users discussing the format and difficulty in understanding email addresses. Some users share their experiences with using different domain names and encountering confusion.
- The use of QR codes in various scenarios is debated, with some users finding them useful while others have had difficulty scanning them. The idea of QR codes in restaurants and the perceived assumptions about people's technological proficiency are discussed.
- The practicality and security of scanning QR codes are also mentioned. Some users express concerns about trust, unknown URLs, and relying on third-party services.
- The discussion also touches on the use of QR codes in different countries and how their popularity varies.

Overall, the discussion provides a range of perspectives on the use of technology in daily life, including URLs, email addresses, and QR codes. There are debates about convenience, security, and usability in different technological contexts.

### Proofs based on diagonalization help reveal the limits of algorithms

#### [Submission URL](https://www.quantamagazine.org/alan-turing-and-the-power-of-negative-thinking-20230905/) | 81 points | by [nsoonhui](https://news.ycombinator.com/user?id=nsoonhui) | [46 comments](https://news.ycombinator.com/item?id=37401228)

In a recent article on Quanta Magazine, Ben Brubaker explores the power of negative thinking in the work of legendary computer scientist Alan Turing. Turing pioneered the concept of "uncomputable" problems, which are problems that cannot be solved algorithmically. He proved the existence of such problems using a counterintuitive strategy based on a mathematical technique called diagonalization. Diagonalization involves building up a missing string bit by bit to ensure that it differs from every string on a given list. This approach plays well with infinity and has been used by mathematicians and computer scientists, including Turing, to prove various results. In Turing's case, he utilized diagonalization to construct an obstinate problem that would thwart every algorithm on an infinite list, demonstrating the existence of unsolvable problems. This contrarian approach to problem-solving has had a profound impact on the field of computer science and continues to inspire researchers today.

The discussion on this submission revolves around the concept of diagonalization and its application to proving the existence of uncomputable problems. Some commenters argue that the diagonalization argument used by Turing is not sufficient to prove the existence of uncomputable real numbers, while others provide counterarguments to support Turing's approach. There is also a discussion about constructive vs non-constructive mathematics and the limitations of diagonalization in a constructive framework. Overall, the discussion explores different perspectives on the power of negative thinking and its impact on computer science and mathematics.

### Can LLMs learn from a single example?

#### [Submission URL](https://www.fast.ai/posts/2023-09-04-learning-jumps/) | 426 points | by [jdkee](https://news.ycombinator.com/user?id=jdkee) | [133 comments](https://news.ycombinator.com/item?id=37399873)

A recent study on large language models (LLMs) has discovered a surprising anomaly in their training process. It was found that these models are capable of rapidly memorizing examples from the dataset after just a single exposure, which goes against the conventional wisdom about neural network sample efficiency. This unexpected phenomenon prompted further investigation and a series of experiments to validate and understand this behavior. The results indicate that the models indeed exhibit rapid memorization capabilities, questioning the current approach to training and utilizing LLMs. This finding opens up new possibilities for improving the efficiency and effectiveness of neural networks and challenges the long-standing assumption of their slow learning process.

The discussion surrounding the submission begins with a user named "jph00" expressing surprise at the rapid memorization capabilities of large language models (LLMs) and how it challenges existing assumptions about neural network sample efficiency. Another user, "og_kalu," suggests trying to freeze the training of LLMs after initial exposure to improve efficiency. "jph00" shares that the experiments conducted didn't show catastrophic forgetting, but rather the models becoming overly confident. They see this as an opportunity rather than a problem.

The conversation then shifts to other topics related to training and understanding neural networks. Users discuss training natural intelligence models, the potential for catastrophic forgetting, and the importance of freezing lower layers in LLMs to prevent forgetting fundamental knowledge. There is also mention of the Internal Family Systems model and its similarities to neural network training.

Some users express their appreciation for the investigation and its potential implications, while others request further explanation and clarification. There is a brief discussion about catastrophic forgetting and its impact on model performance. Another user asks for an explanation of the phenomenon in simpler terms (eli5). The concept of catastrophic forgetting is further explored, with some users suggesting modifications to the sampling model.

Overall, the discussion touches upon various aspects of neural network training, the mechanisms underlying LLMs, and the potential implications of the anomaly discovered in their training process.

### Artificial Consciousness Remains Impossible (Part 2)

#### [Submission URL](https://mindmatters.ai/2023/09/artificial-consciousness-remains-impossible-part-2/) | 20 points | by [momirlan](https://news.ycombinator.com/user?id=momirlan) | [82 comments](https://news.ycombinator.com/item?id=37412555)

In this article, the author presents responses to counterarguments against their thesis from part one. They address objections claiming that consciousness and intentionality are subjective and cannot be proven. The author argues that without intentionality, we would not understand anything, as words would not refer to anything. They also refute the idea that consciousness is something that is "done" and argue that an AGI could perform tasks without being conscious. 

The author then discusses objections from functionalism, which suggest that replicating the functions of a brain would lead to artificial consciousness. They argue that functionalist arguments fail because duplicating a function requires complete visibility and measurability of all functions and dependencies, which is not possible due to underdetermination. 

Finally, the author addresses behaviorist objections, which claim that reproducing conscious behaviors equates to producing consciousness. They disagree with the idea that observable behaviors alone indicate consciousness and reference the Chinese Room argument. 

Overall, the author provides counterarguments against various objections to their thesis, emphasizing the importance of intentionality and highlighting the limitations of functionalism and behaviorism in explaining consciousness.

The discussion on this submission covers various perspectives on consciousness and artificial intelligence. One commenter argues that artificial neural networks today do not have the same complexity and topology of states as real neural networks, while another points out that replicating the topology is not sufficient for generating consciousness. There is also a debate about whether consciousness is dependent on specific states of neurons or if it is a separate non-physical property. 

The conversation then shifts to the limitations of current AI models and the importance of memory in consciousness. Some commenters express their experiences of consciousness and memory, while others argue that consciousness cannot exist in artificial systems. 

The discussion also touches on functionalism, with one commenter arguing that replicating brain functions would not lead to artificial consciousness. Another commenter brings up the example of AlphaZero, a computer program that plays creative games of chess, to demonstrate that computers can exhibit creativity without conscious intention. 

There is also a disagreement about the significance of considering individuals' consciousness and the possibility of artificial consciousness. Some commenters argue that humans place too much importance on themselves, while others posit that empathy and understanding others' experiences are essential. 

The discussion concludes with a debate about the nature of intentional and qualitative experiences and their role in determining the existence of consciousness.

### Run ChatGPT-like LLMs on your laptop in 3 lines of code

#### [Submission URL](https://github.com/amaiya/onprem) | 141 points | by [amaiya](https://news.ycombinator.com/user?id=amaiya) | [33 comments](https://news.ycombinator.com/item?id=37412793)

OnPrem.LLM is a Python package that simplifies running large language models (LLMs) on-premises using non-public or sensitive data, even behind corporate firewalls. It was inspired by the privateGPT GitHub repo and Simon Willison's LLM command-line utility, and is designed to seamlessly integrate local LLMs into practical applications.

To get started, simply install OnPrem.LLM after installing PyTorch. You can then import the LLM class and instantiate it. By default, a 7B-parameter model is used, but you can specify a 13B-parameter model or provide the URL to an LLM of your choice.

Once set up, you can send prompts to the LLM to solve problems using few-shot prompting. You provide an example of what you want the LLM to do, and it generates the desired output. You can also talk to your documents by ingesting them into a vector database and asking questions about them. The LLM will generate answers based on the content of your documents.

Ingesting the documents involves specifying the directory containing the documents and creating embeddings for them. This process may take some time, but once complete, you can query your documents using the LLM's `ask` method to get answers to specific questions.

OnPrem.LLM is a powerful tool for running large language models on-premises and working with non-public data. With its user-friendly interface and seamless integration, it enables practical applications using LLMs behind corporate firewalls or with sensitive data. Give it a try and see how it can enhance your workflow!

The discussion on this submission revolves around various aspects of running large language models (LLMs) locally and the features and limitations of the OnPrem.LLM tool.

- One user shares a link to instructions on how to run the LLM locally on a Macbook with M1/M2 and 32GB RAM using the OnPrem.LLM tool. They mention that following these instructions allows them to run 34B models using both the CPU and GPU.

- Another user mentions that GGUF (Great Generic Unifying Framework) format was introduced in August 2023 as a replacement for GGML (Great Generic Modeling Language), and GGUF is supported by the OnPrem.LLM tool.

- A conversation ensues regarding the compatibility of OnPrem.LLM tool with different formats, and it is clarified that while it currently supports GGML format, there are plans to support GGUF format in future versions.

- A user praises the simplicity and usefulness of the OnPrem.LLM tool, mentioning that they have tried it and found it to be a great way to reduce the complexity of working with LLMs locally.

- Another user mentions that they learned about Simon Willison's LLM from the discussion and finds it interesting.

- A user shares a related project they have written, which handles streaming requests to local LLMs, and they mention that the OnPrem.LLM library offers a similar functionality.

- There is a brief discussion about the size of the LLM models, with one user explaining that a 7B-parameter model takes around 7GB of space.

- The effectiveness and efficiency of local LLM models are discussed, with one user mentioning that it depends on the specific use case and the optimization of the models.

- Some users express interest in trying out the OnPrem.LLM tool and commend the OP (original poster) for their work.

Overall, the discussion highlights the functionality and potential use cases of the OnPrem.LLM tool, as well as some technical details and comparisons with other similar tools and libraries.

### AI-generated child sex imagery has every US Attorney General calling for action

#### [Submission URL](https://arstechnica.com/information-technology/2023/09/ai-generated-child-sex-imagery-has-every-us-attorney-general-calling-for-action/) | 17 points | by [CharlesW](https://news.ycombinator.com/user?id=CharlesW) | [24 comments](https://news.ycombinator.com/item?id=37412525)

Attorneys general from all 50 states in the US, along with officials from four territories, have called on Congress to establish an expert commission to study how generative artificial intelligence (AI) could be used to exploit children through the creation of child sexual abuse material (CSAM). The attorneys general are concerned that AI is creating a new frontier for abuse that makes prosecution more difficult, particularly as open-source image synthesis technologies make it easy to create AI-generated pornography. They also want existing laws against CSAM to be expanded to explicitly cover AI-generated materials.

The discussion on this submission covers a range of perspectives on the issue of AI-generated child sexual abuse material (CSAM). Some users argue that AI-generated CSAM should not be legally considered CSAM, as it is fundamentally different from traditional material and may not involve the exploitation of actual children. Others express concerns about the potential harm caused by AI-generated CSAM and suggest that existing laws against CSAM should be expanded to explicitly cover AI-generated materials. There is also debate about the role of AI in identifying and preventing CSAM, with some users suggesting that AI could be used for better recognition and verification of CSAM, while others argue that AI could also be used to create more convincing and harmful content. Overall, the discussion explores different perspectives on the legal and ethical implications of AI-generated CSAM.

