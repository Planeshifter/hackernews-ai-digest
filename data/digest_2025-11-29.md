## AI Submissions for Sat Nov 29 2025 {{ 'date': '2025-11-29T17:09:06.407Z' }}

### Leak confirms OpenAI is preparing ads on ChatGPT for public roll out

#### [Submission URL](https://www.bleepingcomputer.com/news/artificial-intelligence/leak-confirms-openai-is-preparing-ads-on-chatgpt-for-public-roll-out/) | 776 points | by [fleahunter](https://news.ycombinator.com/user?id=fleahunter) | [682 comments](https://news.ycombinator.com/item?id=46086771)

What’s new:
- Strings found in ChatGPT Android 1.2025.329 beta reference an “ads feature,” “bazaar content,” “search ad,” and a “search ads carousel,” suggesting OpenAI is preparing to show ads inside ChatGPT’s search experience. Spotted by Tibor on X; reported by BleepingComputer.

Why it matters:
- If ChatGPT starts inserting ads into answers or search-style results, it could shift ad spend from traditional web search to AI assistants.
- With an estimated 800M weekly users and roughly 2.5B prompts per day, OpenAI has the scale—and potentially richer conversational context—to deliver highly personalized ads.

Details and context:
- The leak points to ads limited to search initially, but scope could expand.
- Positioning would echo Google’s search ads, but inside an assistant interface.
- Mentions of “bazaar content” hint at a first-party marketplace approach rather than standard web ad networks.

Open questions:
- Will Plus/Team/Enterprise tiers be ad-free?
- How clearly will ads be labeled inside answers?
- What data (chat history, “memory”) will be used for targeting, and can users opt out?
- Will there be revenue sharing with content providers whose info anchors responses?

Status: Internal testing; no official rollout announced.

Here is a summary of the discussion:

**The Economics of "Enshittification"**
Commenters debate the inevitability of this move. Some argue that given the astronomical costs of compute and OpenAI's scale (roughly 1 billion users), an ad-supported model for free tiers was unavoidable. Users describe this as the beginning of "enshittification," fearing OpenAI will repeat Google’s trajectory where the drive for ad revenue eventually degrades the quality of the user experience.

**Threats to the Duopoly**
There is speculation that OpenAI represents an existential threat to Google and Meta. Users note that the "extremely personal data" and context windows in LLMs allow for higher-value, hyper-targeted advertising than traditional search or social feeds ever could. However, skeptics counter that Google’s defensive moat is vast and that OpenAI must tread carefully to avoid alienating users who switched to ChatGPT specifically to escape the "SEO-filled nonsense" of modern search engines.

**Product Placement vs. Answer Integrity**
A significant portion of the thread is dedicated to satirizing how "embedded" ads might look. Users mockingly script scenarios where, for example, a turkey recipe explicitly instructs the user to set their "GE Oven" to 350° or drink a "Coke Zero" while waiting.
*   **Legal Gray Areas:** This satire leads to a serious discussion about the legality of undisclosed ads. Users question whether weaving brand recommendations into "factual" answers constitutes deceptive marketing, drawing comparisons to product placement in film ("The Truman Show," "Seinfeld") versus strictly regulated media types.

**Adoption and Normalization**
One user suggests that while current tech-savvy users hate the idea, the "boiling frog" effect applies: if introduced slowly, the broader public—and specifically younger generations—will likely accept ads in AI interfaces as a normal part of the digital landscape, just as they did with the web.

### Student perceptions of AI coding assistants in learning

#### [Submission URL](https://arxiv.org/abs/2507.22900) | 93 points | by [victorbuilds](https://news.ycombinator.com/user?id=victorbuilds) | [115 comments](https://news.ycombinator.com/item?id=46089546)

New Kid in the Classroom: Exploring Student Perceptions of AI Coding Assistants (arXiv)
A small exploratory study (n=20) in an intro programming course finds AI coding assistants boost novices’ confidence and help them grasp code concepts during initial development—but many stumble when asked to extend solutions without AI. The two-part exam design (first with AI support, then without) surfaced signs of overreliance and weak knowledge transfer. The author argues for pedagogy that integrates AI while explicitly strengthening core programming skills, rather than letting tools “impersonate” them.

Why it matters: As AI helpers become standard in CS classrooms, instructors may need assignments with “AI-on” and “AI-off” phases, reflection prompts, and assessment that tests understanding beyond tool output.

Caveats: Perception-focused, small sample, single course; results are suggestive rather than definitive.

Paper: https://arxiv.org/abs/2507.22900 (shorter version accepted to CCC 2025)

**New Kid in the Classroom: Exploring Student Perceptions of AI Coding Assistants**
[https://arxiv.org/abs/2507.22900](https://arxiv.org/abs/2507.22900)

**Summary of the Discussion:**
The discussion on Hacker News wrestles with whether AI in education represents a "slide rule" evolution—a new layer of abstraction—or a detrimental shortcut that creates dependency.

*   **The Ironies of Automation:** Several commenters cited Lisanne Bainbridge’s "Ironies of Automation," arguing that while AI intends to help, it often de-skills the user. If students "outsource" the struggle of learning syntax and logic to an LLM, they may lack the fundamental mental model required to debug or extend that code when the AI inevitably fails.
*   **Analogy Wars:** There is a fierce debate over the correct analogy. Some view AI as a calculator or spellcheck (a necessary productivity booster for "knowledge work"). Others critique this, suggesting a better analogy is physical exercise: if a machine lifts the weights for you, you don't gain the muscle.
*   **Credentialism and Hiring:** A major concern is the devaluation of computer science degrees. If assignments can be solved effortlessly by AI, academic credentials may lose their signaling power, forcing employers to lengthen interview processes with rigorous in-person testing to verify actual competence.
*   **The Student Perspective:** Students in the thread expressed anxiety about their own learning paths. They questioned whether skipping rote memorization of syntax to focus on "broad strokes" logic via AI is efficient learning or an intellectual trap that leaves them unable to code "blank slate."
*   **Determinism vs. Stochasticism:** While some argued that relying on AI is just the next step after migrating from Assembly to Python, skeptics countered that high-level languages are deterministic and reliable, whereas LLMs are stochastic and prone to hallucination, making them a risky foundation for novices.

**Methodology Note:** While some dismissed the paper for its small sample size ($N=20$), others defended qualitative research as essential for understanding the *nature* of how students navigate new tools, rather than just measuring output statistics.

### Major AI conference flooded with peer reviews written by AI

#### [Submission URL](https://www.nature.com/articles/d41586-025-03506-6) | 207 points | by [_____k](https://news.ycombinator.com/user?id=_____k) | [131 comments](https://news.ycombinator.com/item?id=46088236)

ICLR 2026 rocked by AI-written peer reviews, organizers launch probe

- After Graham Neubig (CMU) suspected chatbot-written reviews for his ICLR 2026 submission, Pangram Labs scanned the entire conference corpus: 19,490 papers and 75,800 reviews.
- Findings: about 21% of peer reviews were flagged as fully AI-generated, and more than half showed signs of AI use. On the submissions side, 1% of manuscripts were deemed fully AI-generated; 9% had more than 50% AI-generated text; 61% were mostly human-written.
- Red flags authors reported included hallucinated citations, long and vague bullet-point feedback, incorrect numerical claims, and non-standard analysis requests.
- Pangram used its in-house detector, described in a preprint submitted to ICLR—whose own reviews included at least one that Pangram’s tool flagged as fully AI-generated.
- ICLR organizers say they will now use automated tools to assess possible policy breaches in submissions and peer reviews. Senior program chair Bharath Hariharan called it the first time the conference has faced the issue at scale.
- Researchers like Desmond Elliott (University of Copenhagen) say AI-written reviews are affecting outcomes; one flagged review gave his paper the lowest rating, leaving it borderline for acceptance.
- The episode underscores mounting pressure on peer review and raises questions about detection reliability, enforcement, and how much AI assistance—if any—should be acceptable.

Here is a summary of the discussion on Hacker News:

**Skepticism regarding AI Detectors:**
A significant portion of the discussion challenged the reliability of AI detection tools, with several users viewing the article as a PR stunt for Pangram Labs. Commenters argued that detectors suffer from high false-positive rates (citing instances where historical texts like the Declaration of Independence were flagged as AI) and that detecting LLM output is theoretically impossible as models improve.

**Pangram Co-founder Response:**
A user identifying as a Pangram co-founder ("mxspr") actively defended their technology in the comments. They distinguished their deep-learning approach from simpler "perplexity-based" methods used by competitors. They cited recent research claiming near-zero false positives on human documents and argued that skepticism is often based on outdated benchmarks, though other users continued to press for clean, public datasets to verify these claims.

**Systemic Issues in Academia:**
The conversation shifted from technical detection to the culture of peer review. Users argued that the prevalence of AI reviews signals a "freefall" in academic standards. Commenters described a "maximum extraction" mindset where researchers and reviewers utilize shortcuts to game the system, exacerbated by "reviewer rings" and a loss of professional duty. Some suggested that AI is merely automating a lack of care that already existed.

**The "Arms Race":**
Users noted the adversarial nature of the problem, predicting that better detection tools will simply encourage model providers (like OpenAI) to train models specifically to evade detection, ensuring that AI writing eventually becomes indistinguishable from human text.

### Show HN: Zero-power photonic language model–code

#### [Submission URL](https://zenodo.org/records/17764289) | 15 points | by [damir00](https://news.ycombinator.com/user?id=damir00) | [5 comments](https://news.ycombinator.com/item?id=46089764)

Entropica: an open photonic language model with “zero-power” optical inference

- What’s new: A 1024‑mode, 32‑layer unitary network whose entire forward pass can be realized as a passive linear‑optical interferometer (Reck MZI mesh). Tokens are sampled via a Born‑rule readout. Trained on TinyStories to produce coherent outputs in under 1.8 hours on a single laptop GPU.

- Why it matters: Inference can, in principle, run with no active electronics in the compute path (passive optics), offering near–speed-of-light latency and dramatically lower energy use compared to electronic hardware. The authors show a practical path to hardware with printed phase masks and even a $30 laser pointer.

- How it works: 
  - The model uses strictly unitary layers implementable by MZI meshes; sampling uses the Born rule over output intensities.
  - Training is conventional (Python/GPU); inference can be done optically by setting phase shifts and letting light propagate through the mesh.
  - All code, weights, and dataset generation scripts are public.

- Results: Learns TinyStories-style generation and demonstrates that a fully passive, linear-optical forward pass is viable for small generative models.

- Caveats: This is a TinyStories-scale demo, not a general LLM. “Zero-power” refers to the passive interferometer; a light source and detection still consume power. Scaling, noise, IO/memory, and broader benchmarks remain open questions.

- Links:
  - Repo: https://github.com/dwallener/EntropicaPublic
  - Tech note (PDF, CC BY 4.0): https://doi.org/10.5281/zenodo.17764289

**Discussion Summary:**

Commenters focused on the practical realities behind the "zero-power" claim, noting that while the inference path is passive, significant energy is still required for the light source and converting information between electronic and optical domains. Technical questions arose regarding how non-linearity—essential for neural networks—is achieved in a linear optical system; the explanation provided is that non-linearity occurs at the detection stage via intensity measurement ($E^2$). Skepticism remained regarding physical viability, with users warning that translating simulations to reliable optical hardware is notoriously difficult and questioning the architecture's ability to scale to useful models like GPT-2.

### Users brutually reject Microsoft's "Copilot for work" in Edge and Windows 11

#### [Submission URL](https://www.windowslatest.com/2025/11/28/you-heard-wrong-users-brutually-reject-microsofts-copilot-for-work-in-edge-and-windows-11/) | 85 points | by [robtherobber](https://news.ycombinator.com/user?id=robtherobber) | [28 comments](https://news.ycombinator.com/item?id=46087333)

Windows Latest highlights growing frustration with Microsoft’s AI-first direction in Edge and Windows 11. Copilot Mode—Microsoft’s agentic browsing experience akin to Perplexity’s Comet or ChatGPT’s Atlas—is now the default UX in Edge (you can turn it off in settings). Microsoft pitches it as “AI browsing that’s safe for work,” promising automated multi-step workflows and “multi-tab reasoning” that pulls from up to 30 tabs.

The reception on X has been harsh. Longtime Windows users and IT admins say no one asked for deeper Copilot integration and want ways to remove it. Microsoft’s social accounts are amplifying praise while largely ignoring critical replies, per the report. The piece also notes Microsoft plans to hide the “AI can make mistakes” disclaimer because some users found it distracting—despite ongoing accuracy concerns.

This follows earlier backlash to “agentic” Windows features, after which a Windows exec locked replies on social posts; Microsoft is now testing agent invocation from the taskbar. Meanwhile, Microsoft AI chief Mustafa Suleyman defended the AI push, arguing critics are underestimating progress, likening it to moving from Nokia Snake to today’s generative systems.

Why it matters: Default-on AI agents, muted disclaimers, and mixed accuracy claims risk eroding user trust—especially among power users and enterprises who didn’t ask for it. Source: Windows Latest

**The Exodus to Linux:** The overwhelming sentiment in the discussion is that Microsoft’s aggressive AI integration and "hostile" user experience changes are finally driving power users—particularly gamers—to switch to Linux. Commenters cite the "Year of the Linux Desktop" becoming a reality not through marketing, but because of Microsoft's alienation of its user base combined with Valve's heavy lifting on Linux gaming compatibility (Proton/SteamOS). Specific distributions mentioned as viable alternatives include CachyOS, Nobara, and Ubuntu.

**Enterprise vs. Consumer Bloat:** A sub-thread debated whether users should simply use Enterprise or LTSC (Long-Term Servicing Channel) versions of Windows to avoid "consumer" annoyances like Copilot and ads. However, others countered that acquiring these licenses as an individual is intentionally difficult, requiring resellers or minimum order quantities, validating the feeling that Microsoft does not offer a clean product to individual consumers.

**Echo Chambers vs. Utility:** While the majority expressed deep frustration with performance degradation and dark patterns (such as forced OneDrive syncing or Edge defaults), a contrarian voice argued that the backlash represents a power-user bubble; they posited that for the average corporate worker, Copilot is actually a useful productivity tool for summarizing and tedious tasks. Rebuttals focused on the lack of trust, arguing that even if the tool is useful, Microsoft’s history of overriding user preferences makes the integration unwelcome.

### I Know We're in an AI Bubble Because Nobody Wants Me

#### [Submission URL](https://petewarden.com/2025/11/29/i-know-were-in-an-ai-bubble-because-nobody-wants-me-%f0%9f%98%ad/) | 84 points | by [iparaskev](https://news.ycombinator.com/user?id=iparaskev) | [62 comments](https://news.ycombinator.com/item?id=46086410)

Pete Warden: The real AI bubble is in hardware, not software efficiency

- Warden (Jetpac cofounder, ex-Google/TensorFlow mobile lead) recounts how chasing efficiency—first to run AlexNet cheaply at scale, then to push inference onto phones—has always been where the biggest, most durable wins come from.
- He argues today’s AI spending is badly misallocated: hundreds of billions are flowing into GPUs, data centers, and power, while ML infrastructure/optimization teams struggle to raise money—even though GPU utilization is often under 50% (and much worse for interactive, small-batch, memory-bound workloads).
- There are well-known headroom and alternatives: hand-tuned kernels can beat vendor libraries; many inference loads can run far cheaper on CPUs; and efficiency brings real environmental benefits.
- So why the hardware arms race? Incentives and signaling. Buying GPUs is an easy-to-measure moat story, simpler to manage than deep software work, and it flatters investors’ narratives—“nobody ever got fired for buying IBM,” now transposed to OpenAI-scale capex.
- His company, Moonshine, has had to swim against that tide but expects to be cashflow-positive in Q1 2026; he believes the rational bet for any firm burning billions on GPUs is to invest hundreds of millions in software efficiency.

Why it matters
- If Warden is right, the next big AI gains won’t come from more H100s but from ruthless, full-stack optimization—model, runtime, kernel, OS, hardware choices—unlocking higher utilization, lower cost, and lower power. The market may be rewarding the wrong moat.

Based on the discussion, here is a summary of the comments:

**The Rise of "Assetocracy" vs. Local AI**
A significant portion of the discussion focuses on the shifting dynamics of power in tech. Commenters coined the term "assetocracy" to describe the current state where those with capital (access to expensive assets like H100s) control the market, replacing the traditional software "meritocracy." However, users noted that if Warden is right and efficiency creates viable "local AI," the business case for centralized cloud monopolies could evaporate, returning control to widespread, decentralized hardware.

**Infrastructure Plays and the "Bubble" Debate**
Users debated whether the current hardware spending is a bubble or a strategic necessity.
*   **The Long Game:** Some argued that buying infrastructure isn't just about today's needs, but about controlling the industry five years from now.
*   **Supply vs. Demand:** Users cited Microsoft CEO Satya Nadella’s statements that hyperscalers are "capacity constrained" (they have more demand than chips), suggesting the spending is justified.
*   **Skepticism:** Others remained skeptical of CEO narratives, debating whether big tech companies are signaling growth to shareholders rather than fulfilling genuine technical requirements, though some noted that lying to shareholders carries significant legal risk (SEC).

**Cultural Aversion to Optimization**
Commenters largely validated Warden’s observation that companies prefer buying hardware over optimizing software.
*   **The "Sun Microsystems" Parallel:** One user compared this to the dot-com boom, where startups burned VC money on expensive Sun servers rather than optimizing code.
*   **Management Ease:** Users noted that paying a cloud bill is easier for management than hiring and directing engineers to perform deep optimization work. It is often seen as "banging your head against a wall," and companies prefer releasing features over saving computing resources.

**Technical Viability and Market Signals**
There was debate regarding the technical feasibility of Warden's claims.
*   **CPU vs. GPU:** Some questioned the reality of running modern LLMs on CPUs, though others clarified that Warden’s focus is on specific inference tasks (speech recognition, translation) rather than massive model training.
*   **Plummeting Costs:** Counter to the idea that efficiency is being ignored, some users pointed out that token prices are dropping rapidly and models are becoming cheaper to run, suggesting that ruthless optimization is arguably already happening at the hyperscaler level to protect margins.

