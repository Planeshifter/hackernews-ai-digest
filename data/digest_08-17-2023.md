## AI Submissions for Thu Aug 17 2023 {{ 'date': '2023-08-17T17:10:04.945Z' }}

### RoboAgent: A universal agent with 12 Skills

#### [Submission URL](https://robopen.github.io/) | 132 points | by [reqo](https://news.ycombinator.com/user?id=reqo) | [17 comments](https://news.ycombinator.com/item?id=37167698)

Introducing RoboAgent: A Universal Robotic Agent with 12 Skills

The quest for a single robot that can manipulate any object in diverse settings has been a distant goal for decades. However, the lack of diverse robotics datasets and generic agents capable of generating such datasets has hindered progress. Enter RoboAgent, a universal robotic agent developed by a team at Carnegie Mellon University and Meta AI.

RoboAgent is a culmination of two years of work and aims to efficiently acquire multiple skills within a practical data budget and generalize them to unseen situations. The team built RoboPen, a distributed robotics infrastructure, and RoboHive, a framework for robot learning across simulation and real-world operations. They also created RoboSet, a high-quality dataset representing multiple skills with everyday objects in diverse scenarios.

To tackle the challenge of low-data regimes and overfitting, the team employed an efficient language conditioned multi-task offline imitation learning framework called MT-ACT. They used semantic augmentations to inject world priors from existing foundation models into the RoboSet dataset, creating a heavily multi-modal dataset with a rich diversity of skills, tasks, and scenarios. They then developed MT-ACT, a novel efficient policy representation that can handle this multi-modal dataset while avoiding overfitting.

RoboAgent was trained on just 7,500 trajectories, a fraction of the data used by previous methods, yet it exhibited 12 non-trivial manipulation skills across 38 tasks and could generalize to hundreds of diverse unseen scenarios. It even showed the ability to evolve its capabilities with new experiences.

The team also released the RoboSet dataset, consisting of 100,050 trajectories, and open-sourced their entire dataset to accelerate research in robot-learning. With RoboAgent's sample efficiency and ability to generalize, the dream of a universal robotic agent may be within reach.

The discussion on this submission begins with users commenting on the 12 skills that RoboAgent has learned. One user points out that there are skills beyond the 12 mentioned in the article. Another user jokingly adds "computer hacking skills" to the list. 

There are later comments discussing the related research paper "RT-2" and its comparison to "RT-1." 

One user criticizes the presentation format of the article, finding it somewhat contradictory and lacking in clarity.

A couple of comments question the purpose of RoboAgent and its skills. One user sarcastically states that it is a "battery god" while another user expresses their wish for 12 skills.

The discussion then shifts to comparing RoboAgent's achievements to those of Boston Dynamics. Some users express enthusiasm for the advancements in robotics, while others bring up the use of classical versus deep learning techniques and the need for AI to intersect with robotics.

There is a discussion about the comments section in general, with some users stating that negative comments are discouraging and hinder progress. A user adds that Boston Dynamics largely uses classical learning techniques and that AI research needs to focus on creating practical and usable results.

Other comments mention finding Boston Dynamics' GitHub resources and note that Boston Dynamics' robots perform tasks in a pre-programmed manner.

The discussion ends with one user praising Boston Dynamics' robots and their impressive abilities.

### I am afraid to inform you that you have built a compiler (2022)

#### [Submission URL](https://rachit.pl/post/you-have-built-a-compiler/) | 251 points | by [mutant_glofish](https://news.ycombinator.com/user?id=mutant_glofish) | [88 comments](https://news.ycombinator.com/item?id=37162898)

In a hilariously tongue-in-cheek post, the author addresses those who set out to avoid building a compiler but ended up inadvertently creating one. The letter highlights the irony of starting with a simple prototype and ending up with a complex mess of string mangling scripts. The author playfully mocks the idea of stubbornly avoiding a proper compiler infrastructure, only to later realize the benefits of using an abstract syntax tree (AST) library. Yet, even with the AST, the challenges continue, as users push the boundaries of the language with nested constructs, leading to constant patches and distractions from essential features. The post amusingly describes the temptation to simplify the AST by assuming certain program shapes but warns of the risks of losing crucial information and the burden it places on future developers. Finally, the author hilariously recounts the demand for backward compatibility with an older version, forcing further code simplification and implementation tweaks. The punchline: despite all endeavors to avoid building a compiler, the end result is unmistakably a compiler, complete with a parser, intermediate representation, transformation passes, and a code generator. So, to all those who never wanted to build a compiler, the author informs them: "Dear Sir, you have built a compiler."

The discussion on this submission covers various perspectives on the topic of building a compiler. Some users share their experiences and opinions on the challenges and benefits of building a compiler. There is also a discussion about billing clients for development work, with differing views on whether hourly billing is fair or if a different approach should be taken. Additionally, there are comments discussing the use of domain-specific languages (DSLs) and the potential advantages and disadvantages they offer. Some users also discuss their experiences with building and using compilers in different programming languages. Overall, the discussion provides a range of insights and perspectives on the topic of building compilers.

### AI bots are now better than humans at decoding CAPTCHAs

#### [Submission URL](https://qz.com/ai-bots-recaptcha-turing-test-websites-authenticity-1850734350) | 279 points | by [geox](https://news.ycombinator.com/user?id=geox) | [234 comments](https://news.ycombinator.com/item?id=37160744)

Artificial Intelligence (AI) bots are now surpassing humans in their ability to solve CAPTCHA puzzles used to verify the authenticity of website users, according to a research paper published last month. The study, conducted by researchers from the University of California, Irvine, ETH Zurich, Lawrence Livermore National Laboratory, and Microsoft, found that AI bots are more accurate and faster at deciphering CAPTCHAs than humans. The bots' accuracy rates ranged from 85% to 100%, while human accuracy rates fell between 50% and 85%. The study suggests that the ongoing effort to solve CAPTCHAs may not be as effective or worthwhile as previously thought.

The discussion surrounding the submission about AI bots surpassing humans in solving CAPTCHA puzzles on Hacker News touched on several points. 

Some users expressed skepticism about the effectiveness of CAPTCHAs in thwarting malicious activities, suggesting that CAPTCHAs may not be as useful as previously thought. They argued that designing internet bots that can navigate websites and content without CAPTCHAs might be a better solution.

Others mentioned that the cost of solving CAPTCHAs using human-staffed services is expensive, and AI bots could potentially reduce the cost. They also pointed out that CAPTCHAs can lead to wasted time for humans and increased costs for businesses, making them less desirable.

A few users brought up the issue of CAPTCHA spam and how big tech companies have tried to scrape data from platforms like LinkedIn and Facebook. There was also discussion about the potential privacy implications of CAPTCHAs.

Some users argued that the current design of CAPTCHAs is flawed and suggested alternative methods, such as using machine learning models and analyzing IP addresses or physical body movements.

There were also comments about the difficulty humans face in solving CAPTCHAs correctly and how AI bots can easily bypass them.

Overall, the discussion highlighted doubts about the effectiveness and practicality of CAPTCHAs and proposed alternative approaches to prevent malicious activities on the internet.

### Future Intel CPUs May Dump Hyper-Threading for Partitioned Thread Scheduling

#### [Submission URL](https://hothardware.com/news/future-intel-cpu-partition-threads) | 50 points | by [rbanffy](https://news.ycombinator.com/user?id=rbanffy) | [20 comments](https://news.ycombinator.com/item?id=37163191)

Intel is considering ditching Hyper-Threading in future CPUs in favor of a new technique called partitioned thread scheduling, according to a recently published patent. Hyper-Threading, which is Intel's version of simultaneous multi-threading (SMT), aims to improve occupancy and utilization of a processor core's functional units. However, it is not always efficient, as a single program thread rarely fills all of a processor's functional units. The new partitioned thread scheduling technique analyzes the work required by a thread and breaks application threads into segments using partitions. These partitioned threads are then scheduled onto processor cores based on their performance requirements. This technique aims to optimize core utilization and improve overall efficiency.

The discussion on Hacker News revolves around the implications of Intel potentially ditching Hyper-Threading in favor of partitioned thread scheduling. Some users express their skepticism about the performance benefits of Hyper-Threading, citing benchmark tests that showed minimal speed improvements. Others discuss the potential drawbacks of partitioned thread scheduling, such as increased cache misses and the need for software compatibility. 

One user mentions that Intel's Thread Director technology in Alder Lake CPUs may address some of the performance and power consumption issues associated with Hyper-Threading. Another user raises the point that the patent application does not necessarily indicate an imminent replacement of Hyper-Threading but rather suggests a possible alternative scheduling technique.

There is also a discussion about the difference between physical cores and logical cores and how Hyper-Threading fits into this context. Some users argue that physical cores are more important for performance and that SMT can lead to limiting factors and behavior that affect overall efficiency.

Overall, the discussion highlights various perspectives and considerations regarding the potential shift from Hyper-Threading to partitioned thread scheduling in future Intel CPUs.

### Microsoftâ€™s AI recommends the Ottawa Food Bank as a tourist destination

#### [Submission URL](https://www.theverge.com/2023/8/17/23836287/microsoft-ai-recommends-ottawa-food-bank-tourist-destination) | 35 points | by [ilamont](https://news.ycombinator.com/user?id=ilamont) | [3 comments](https://news.ycombinator.com/item?id=37168135)

Microsoft recently published an AI-generated travel article promoting Ottawa, Canada. However, the article included a recommendation for tourists to visit the Ottawa Food Bank, which caused controversy and led to the article being pulled. The food bank was listed as the third recommendation on the list, after the National War Memorial. Microsoft has stated that it is investigating how the article made it through their review process. The company previously replaced journalists at Microsoft News and MSN with artificial intelligence. The Ottawa Food Bank has since released a statement, calling the AI-generated article's messaging insensitive and highlighting the importance of human researchers, writers, and editors.

The discussion surrounding the submission mainly consists of users expressing their concerns and opinions about the AI-generated travel article promoting Ottawa. One user comments with a sense of shock, stating "Empty stomach? Yikes!" Another user points out that the AI's output may statistically resemble writing but lacks the ability to grasp appropriate context, saying, "Statistically assembling bytes produces noisy output. Hmm." It seems that users are skeptical about relying solely on AI for content generation due to its limitations in understanding sensitive topics and appropriate messaging.

### Report: Potential NYT lawsuit could force OpenAI to wipe ChatGPT and start over

#### [Submission URL](https://arstechnica.com/tech-policy/2023/08/report-potential-nyt-lawsuit-could-force-openai-to-wipe-chatgpt-and-start-over/) | 24 points | by [pseudolus](https://news.ycombinator.com/user?id=pseudolus) | [11 comments](https://news.ycombinator.com/item?id=37165646)

The New York Times may be considering a lawsuit against OpenAI over alleged infringement of its intellectual property rights. The Times recently updated its terms of service to prohibit AI companies from scraping its articles and images to train AI models. If the Times were to sue OpenAI, it could result in fines of up to $150,000 per infringing piece of content and the destruction of ChatGPT's dataset. This potential lawsuit could become the most high-profile legal battle over copyright protection in the AI industry. OpenAI is already facing legal challenges from popular authors over similar concerns. The Times' main concern is that ChatGPT could use its content to become a competitor by creating text that answers questions based on the original reporting and writing of the paper's staff. The Times had been considering a licensing deal with OpenAI, but meetings between the two parties have reportedly become contentious, making the deal increasingly unlikely. OpenAI would likely have to claim fair use of the web content it scraped to train its models in order to defend itself, but experts believe this would be challenging since ChatGPT could replace the Times' website as a source of its reporting for some users. The Associated Press and other news organizations have also expressed concerns about their material being used by AI companies without permission or payment, and have been developing standards for the use of AI in newsrooms.

The discussion on this submission revolves around various aspects of copyright, intellectual property, and the potential legal battle between The New York Times (NYT) and OpenAI. Here are the main points of the discussion:

1. Some users argue that the competition between AI and human-written content is a more interesting aspect to focus on, as static training data from copyrighted material presents challenges due to accuracy and the copying of information.
2. Another user mentions that humans should have a claim to intellectual property since many smart AIs are built on their intellectual contributions.
3. One user points out that copyright prevents the copying and mimicking of style in reporting but does not protect the ideas or concepts discussed. They argue that artificial intelligence allows for unprecedented scale in replicating information.
4. The importance of compensating creators is mentioned, and the idea that copyright protections should last longer to encourage creativity in various industries.
5. A user argues that copyright does not benefit society as a whole, and that protection should be limited to prevent the excessive restriction of resources.
6. It is mentioned that copyright law has a significant impact on people's lives, including textbooks, medical research, and the distribution of resources. The user advocates for acknowledging the limitations of intellectual property law.
7. The Associated Press and other news organizations are mentioned as taking steps to strike licensing deals with OpenAI, as disclosed in their terms of service update.
8. One user comments on the recent licensing developments, expressing surprise at the lack of significant discussion.

Overall, the discussion highlights the various perspectives on copyright, intellectual property, and the potential implications of a legal battle between The New York Times and OpenAI.

