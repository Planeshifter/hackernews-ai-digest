## AI Submissions for Sun Apr 06 2025 {{ 'date': '2025-04-06T17:12:30.955Z' }}

### SeedLM: Compressing LLM Weights into Seeds of Pseudo-Random Generators

#### [Submission URL](https://machinelearning.apple.com/research/seedlm-compressing) | 165 points | by [pizza](https://news.ycombinator.com/user?id=pizza) | [36 comments](https://news.ycombinator.com/item?id=43599967)

In the ever-evolving field of Natural Language Processing (NLP), the challenge of deploying Large Language Models (LLMs) efficiently has reached a breakthrough with the introduction of SeedLM, presented at the ICLR 2025 conference. This paper by Rasoul Shafipour and co-authors rolls out a game-changing compression method that could redefine how NLP models manage time and resource demands.

SeedLM operates by transforming the cumbersome weight files of LLMs into seeds for a pseudo-random generator, specifically leveraging a Linear Feedback Shift Register (LFSR). This technique circumvents the usual bottleneck of memory access during inference, allowing models like Llama3 70B to maintain their zero-shot accuracy even when compressed down to 3- or 4-bit numbers and remaining on par or surpassing current state-of-the-art methods.

This novel approach shines not only because it delivers robust compression ratios but crucially does so without needing additional calibration data‚Äîstreamlining the deployment across a range of tasks without bespoke adjustments. For practitioners deploying models on-device, the implications are significant: a potential fourfold increase in speed over FP16 baselines, as observed in FPGA-based benchmarks, promises enhanced efficiency as model scales continue to grow.

Moreover, the paper reflects on the balance between accuracy and efficiency, a task made easier by SeedLM's data-free compression, which harnesses unused computational capacity for quicker memory-bound processes. This minimization of resource demand makes SeedLM an attractive antidote to the high costs usually associated with running LLMs, paving the way for wider application in both consumer and enterprise systems.

With such advancements, SeedLM represents a beacon of hope for the NLP community, pushing the boundaries of how far and fast LLMs can go without incurring prohibitive costs or complexity increases, all while maintaining top-tier performance.

The Hacker News discussion on SeedLM, a novel LLM compression method, highlights technical debates, comparisons, and skepticism:

1. **Technical Insights & Comparisons**:  
   - Users note SeedLM‚Äôs use of pseudo-random seeds (via LFSR) to compress weights into 3-4 bits, avoiding calibration data. This contrasts with quantization methods like AWQ or OmniQuant, which require training or calibration.  
   - Accuracy debates arise: SeedLM‚Äôs results for models like LLaMA 70B (4-bit: 78.06% vs. FP16 baseline: 79.51%) are seen as competitive but not universally superior. Some argue larger models degrade more gracefully with quantization.  

2. **Implementation Advantages**:  
   - The method‚Äôs simplicity‚Äîgenerating weights on-the-fly via PRNG seeds‚Äîis praised for reducing memory bandwidth and enabling faster inference, especially on memory-bound hardware (e.g., FPGAs).  
   - Unlike quantization-aware training, SeedLM‚Äôs "data-free" approach streamlines deployment but raises questions about handling outlier weights in sparse LLM activations.  

3. **Skepticism & Humor**:  
   - The paper‚Äôs October 2024 date sparks jokes about it being an April Fools‚Äô prank. Others question the information-theoretic feasibility of compressing models using pseudo-random sequences, comparing it to JPEG‚Äôs DCT or the Library of Babel.  
   - Some liken the approach to ‚Äúlossy compression,‚Äù replacing exact weights with pre-defined patterns, akin to image compression.  

4. **Broader Implications**:  
   - Discussions link SeedLM to Apple/Meta‚Äôs on-device AI efforts, noting hardware constraints (e.g., iPhones‚Äô 8GB RAM) and the gap between research and productization.  
   - Parallels are drawn to knowledge compression in documentation, emphasizing minimal principles to reconstruct complex systems‚Äîa theme resonating with LLM efficiency goals.  

Overall, the community recognizes SeedLM‚Äôs potential but remains cautious about its claims, balancing excitement for faster, cheaper LLMs with technical scrutiny.

### TripoSG ‚Äì Text to 3D Model

#### [Submission URL](https://github.com/VAST-AI-Research/TripoSG) | 30 points | by [taikon](https://news.ycombinator.com/user?id=taikon) | [4 comments](https://news.ycombinator.com/item?id=43598353)

Hey Hacker News community! Today we're diving into an exciting leap forward in the world of 3D shape synthesis with the newly released TripoSG model by VAST-AI-Research. This advanced model promises high-fidelity and high-quality 3D shape generation directly from images, thanks to its use of large-scale rectified flow transformers and a meticulous dataset of Image-SDF pairs.

**Key Highlights:**

- **Sharp Precision**: The model excels in producing mesh outputs with intricate geometric features and fine surface details, maintaining strong semantic consistency.
- **Versatile Input Handling**: Whether it's a photorealistic image, cartoon, or sketch, TripoSG consistently delivers coherent 3D shapes, even with complex topology.
- **Robust Architecture**: The model incorporates an advanced VAE and rectified flow transformer for efficient scaling, supporting stable performance across various scales.

**Exciting News and Community Engagement:**

The team announced the release of a new 1.5B parameter rectified flow model and interactive features that include inference code. Plus, there's an interactive demo available on Hugging Face Spaces for users to test out the capabilities of TripoSG.

**Get Started:**

To jump in, simply clone the repository and set up the required dependencies. A CUDA-enabled GPU with at least 8GB VRAM is recommended for optimal performance.

For those interested in contributing or reporting issues, the project is open to collaboration with a welcoming community on GitHub.

Check out the TripoSG repository for more details and give it a star if you're impressed by its capabilities! üéâ

Here's a concise summary of the discussion:

1. **User "jlks"** mentions working on a 3D model ("mg 3D mdl"), to which **"pkff"** replies affirmatively ("Yes ts mg 3D"), suggesting agreement or acknowledgment of the project.

2. **User "brcdr"** expresses interest in exploring the backend process of creating 3D models using Blender ("Im ntrstd sng bcknd crt 3D mdls Blender").

3. **User "th"** reacts with excitement ("OMG"), likely in response to the technical discussion or the potential of the tools mentioned.  

**Key Takeaways**: The conversation revolves around 3D modeling workflows, with a focus on Blender‚Äôs backend capabilities and enthusiasm for the topic. Abbreviations are decoded contextually (e.g., "mg" = making, "ntrstd" = interested).

