## AI Submissions for Thu Nov 14 2024 {{ 'date': '2024-11-14T17:14:09.285Z' }}

### BERTs Are Generative In-Context Learners

#### [Submission URL](https://arxiv.org/abs/2406.04823) | 131 points | by [fzliu](https://news.ycombinator.com/user?id=fzliu) | [37 comments](https://news.ycombinator.com/item?id=42134125)

A new paper titled "BERTs are Generative In-Context Learners" by David Samuel reveals a groundbreaking approach that challenges the traditional view of in-context learning. While such learning is typically associated with causal language models like GPT, Samuel demonstrates that masked language models, specifically DeBERTa, can also exhibit this capability. Through a straightforward inference technique, the study enables DeBERTa to handle generative tasks without requiring additional training or architectural modifications.

The research indicates that masked and causal models have distinct strengths, each excelling in different types of tasks. This difference highlights a potential limitation in the AI field's current emphasis on causal models for in-context learning. Samuel advocates for a hybrid approach that merges the advantages of both architectures, suggesting that a more balanced focus could lead to enhanced performance in various applications.

Overall, this work opens up exciting avenues for future research, potentially reshaping how we understand and employ these language models in computational tasks. The paper is available on arXiv with further details and empirical evaluations of performance differences between model types.

The discussion revolves around a new paper titled "BERTs are Generative In-Context Learners," which proposes a novel inference technique allowing the DeBERTa model (a masked language model) to perform generative tasks traditionally associated with causal language models like GPT. Here are the key points from the conversation:

1. **Comparative Performance**: Users noted findings from previous research suggesting that Google's T5 models also handle in-context learning effectively, predating GPT-3. Some participants argued that masked language models generally require fewer resources but may underperform on generative outputs compared to larger causal models.

2. **Model Characteristics**: The conversation highlighted differences in inference speed and efficiency between encoder-based models (e.g., BERT) and decoder models (e.g., GPT). Several users emphasized that BERT-like models tend to have faster inference due to their design.

3. **Hybrid Approaches**: There were suggestions for hybrid approaches that leverage the strengths of both causal and masked models. Interestingly, the idea of integrating training methods and pre-training strategies from both architectures was discussed as a way to improve overall model performance.

4. **Application Contexts**: Many comments touched on specific applications and tasks, debating where each model type excels—whether for generative tasks, classification, or sentiment analysis.

5. **Research Directions**: Participants expressed interest in exploring further how masked models can be adapted for generative tasks and emphasized that there’s still unrevealed potential in tasks that current LLMs tackle.

6. **Performance Metrics**: There was a contention regarding how to best evaluate the models' performance in different scenarios, suggesting that current benchmarks might not fully capture the diverse capabilities of these architectures.

The discussion overall emphasized the advancing understanding of model architectures and in-context learning, signaling an exciting area of exploration in the field of natural language processing.

### Generative AI doesn't have a coherent understanding of the world

#### [Submission URL](https://news.mit.edu/2024/generative-ai-lacks-coherent-world-understanding-1105) | 47 points | by [gmays](https://news.ycombinator.com/user?id=gmays) | [25 comments](https://news.ycombinator.com/item?id=42136519)

A recent study from MIT reveals that generative AI models, despite their remarkable abilities, lack a coherent understanding of the world around them. Researchers found that while these models can generate impressively accurate responses—like giving precise driving directions in New York City—they don't actually form accurate internal representations of their environments. This limitation became evident when models struggled with unexpected changes, like street closures—suggesting their capacities are more about surface-level predictions than true comprehension.

The study, led by MIT's Ashesh Rambachan and others, examines transformer models such as GPT-4. While trained to predict future text, there's skepticism on whether they grasp foundational truths or rules of the real world. To delve deeper, the team developed two new metrics—sequence distinction and sequence compression—designed to evaluate how well these models understand different scenarios, like navigating city streets or playing games like Othello. Interestingly, they discovered that models trained on random sequences performed better in reflecting coherent world models compared to those guided by strict strategies.

This research poses critical implications for deploying generative AI in real-world scenarios, highlighting the need for models that genuinely understand their environments if they're to be used reliably in life-altering applications.

The discussion focuses on the recently published MIT study that critiques generative AI models' understanding of the world. Participants express skepticism toward the assumption that AI can possess an actual understanding or coherent world model, emphasizing that performance in generative tasks does not equate to genuine comprehension. 

Several commenters, including "southernplaces7" and "_heimdall," argue that while AI can impress with tasks like language processing and mimicking human conversation, it lacks the self-awareness and reasoning capabilities that characterize human thought. There's a debate about contrasting human intelligence with AI models, as users like "Melonotromo" and "mdp2021" articulate the complexity behind human comprehension versus AI's surface-level language generation.

Others, like "pnjlly," mention that AI's current capabilities don't match human-like reasoning or self-directed understanding, stating that building an AI with true world modeling is significantly more challenging. Some participants are curious about the implications of AI's limitations for future applications, including potential risks if AI is used in critical real-world scenarios. 

Overall, the comments reflect a consensus that while generative AI is impressive in its outputs, it fundamentally lacks the deeper understanding and cognitive processes humans possess, making its application in sensitive areas potentially problematic.

### DeepL Voice: Real-time voice translations for global collaboration

#### [Submission URL](https://www.deepl.com/en/products/voice) | 111 points | by [doener](https://news.ycombinator.com/user?id=doener) | [57 comments](https://news.ycombinator.com/item?id=42134475)

DeepL has unveiled its latest innovation: real-time voice translations aimed at enhancing global collaboration. This tool is designed to break down language barriers, facilitating seamless interactions between colleagues, clients, and partners across the globe. 

Available in multiple languages, including English, German, Spanish, and Japanese, DeepL Voice is tailored for both meetings and in-person conversations. It operates smoothly within platforms like Microsoft Teams and supports one-on-one chats on iOS and Android devices. 

This impressive technology offers low latency and high performance, ensuring that translations keep pace with natural conversations, recognizing speech patterns and accents for fluid communication. With a strong security foundation, DeepL maintains ISO 27001 certification, making it a trusted choice for over 100,000 businesses looking to foster inclusive and productive workplaces. 

For organizations eager to enhance their international communication, DeepL Voice promises unmatched translation quality and efficiency.

DeepL's recent announcement of its real-time voice translation tool, DeepL Voice, has sparked significant discussion on Hacker News. Users conveyed mixed feelings about the new technology, with some expressing enthusiasm about its potential to improve international communication. They noted the importance of low latency and high performance in translations during live conversations.

Several commenters shared their insights regarding the competitive landscape of translation technologies, particularly in relation to Google Translate and emerging AI innovations. Some users are developing their own solutions, exploring models that leverage speech-to-text (STT) and text-to-speech (TTS) to enhance translation accuracy. 

Concerns were raised about translation quality, with some users suggesting that while DeepL performs well, other services, including ChatGPT-based translations, have also made great strides but may vary based on specific contexts or languages. A few commenters lamented that translation tools have historically struggled with certain languages and dialects, prompting comparisons of models and experiences.

The discussion also touched upon DeepL's business strategy and integration capabilities. Users pointed out the potential for DeepL Voice to serve not only corporate meetings but also personal conversations across diverse platforms. Moreover, some highlighted DeepL's commitment to quality and security, citing its ISO 27001 certification as a significant trust factor for businesses.

Overall, the feedback indicates a cautious optimism surrounding DeepL Voice, with emphasis on innovation, competitive dynamics, and the continuous improvement required in language translation technologies.

### The barriers to AI engineering are crumbling fast

#### [Submission URL](https://blog.helix.ml/p/we-can-all-be-ai-engineers) | 233 points | by [lewq](https://news.ycombinator.com/user?id=lewq) | [171 comments](https://news.ycombinator.com/item?id=42136711)

In a recent blog post, Luke Marsden argues that the barriers to becoming an AI engineer are rapidly diminishing, thanks in part to the availability of powerful open-source models. Speaking at the AI for the Rest of Us conference, Marsden, who has a background in DevOps and MLOps, emphasizes that if you’re familiar with basic coding practices and version control, you’re already equipped to create AI applications.

He outlines six core building blocks for AI applications: models, prompts, knowledge, integrations, tests, and deployment. The key takeaway is that existing tools from software development, like Git and CI/CD pipelines, can also be applied to AI projects. Marsden highlights an "AISpec" YAML file format that streamlines the integration of these components, making AI development feel more intuitive for developers.

Importantly, using open-source models ensures data privacy by keeping sensitive information within a company’s infrastructure. Marsden encourages developers looking to dive into AI to explore the resources he's shared, including a reference architecture on GitHub and a tutorial on implementing these concepts.

The democratization of AI engineering means that anyone with coding skills can potentially build production-ready applications without needing an advanced degree. Marsden invites those interested to further explore the proposed standards at aispec.org, reinforcing the message that modern engineering practices can unlock the potential of AI for all.

In a vibrant discussion on Hacker News regarding the democratization of AI engineering, users shared various perspectives and experiences related to the accessibility of AI tools and workflows. 

One user, mark_l_watson, highlighted their experience with local AI implementation using open-source models on an Apple machine, indicating that smaller models are performing well for tasks like image processing. They emphasized the rapid advancements in tools like Open WebUI, which enhances the practical application of AI.

Another participant, trcrblltx, discussed their efforts in developing structured outputs for machine learning tasks and the importance of using specific keyword strategies for efficient data management. They mentioned a project on GitHub related to tagging images, showcasing the collaborative nature of the community.

Eisenstein contributed by sharing their exploration of using large language models (LLMs) for text analysis and categorization tasks, highlighting the continuous self-education needed in the evolving field of AI.

Concerns about data privacy, especially regarding major companies like OpenAI and Google, were echoed by several users. Discussions around the trustworthiness of these platforms and how they handle personal data were prevalent, with some advocating for local control over AI tools to safeguard sensitive information.

Further, users debated job titles related to AI, with some experiencing a shift towards more generalist roles like "AI Developer" instead of traditional titles, indicating a blending of skills in the job market.

Overall, the conversation illustrated a shared commitment to advancing AI capabilities while navigating the challenges of trust, job definitions, and data management in a rapidly evolving technological landscape.

### Language agents achieve superhuman synthesis of scientific knowledge

#### [Submission URL](https://arxiv.org/abs/2409.13740) | 51 points | by [rntn](https://news.ycombinator.com/user?id=rntn) | [22 comments](https://news.ycombinator.com/item?id=42140356)

A new paper highlights groundbreaking findings in the capabilities of language models to synthesize scientific knowledge with precision. Titled "Language agents achieve superhuman synthesis of scientific knowledge," the research, led by Michael D. Skarlinski and a team of collaborators, introduces PaperQA2, a robust language model agent that not only matches but surpasses domain experts in key literature research tasks. 

Using a novel methodology for evaluating these models, the authors demonstrated that PaperQA2 excels in information retrieval, summarization, and contradiction detection, completing these tasks with notable accuracy. Impressively, the language agent produced cited, Wikipedia-style summaries that outperformed existing human-written articles. Additionally, in tests of identifying contradictions in biological research, PaperQA2 flagged 2.34 contradictions per paper, with a validation rate of 70% by expert reviewers. 

This study establishes a crucial benchmark, called LitQA2, instrumental in helping PaperQA2 achieve these results, fundamentally changing how we view the interplay of artificial intelligence and scientific research. It paves the way for future applications of AI in high-stakes domains, challenging our understanding of research methodologies and the reliability of AI-generated content.

The discussion surrounding the paper "Language agents achieve superhuman synthesis of scientific knowledge" primarily revolves around the implications of AI's capability to synthesize scientific knowledge and its potential to generate breakthroughs in scientific theories. Some users express skepticism, noting that while AI excels at summarization and synthesis, it does not inherently create new scientific theories or experiments. Others argue that AI's performance in tasks such as information retrieval and contradiction detection indicates a level of proficiency that may surpass human capabilities, urging a reconsideration of how breakthroughs are defined.

Participants also discuss the importance of PaperQA2's evaluation methodology and its results in comparison to human experts, highlighting the surprising accuracy and reliability of the model. However, there are concerns regarding the potential for AI to generate misleading or incorrect conclusions (hallucinations) depending on the complexity of scientific topics. The ongoing debate reflects broader questions about the role of AI in scientific research and its future applications, suggesting a need for careful consideration of its contributions and limitations within high-stakes domains. 

Overall, the conversation emphasizes a mix of optimism about AI's capabilities and caution regarding its use and implications in scientific work.

### Something weird is happening with LLMs and Chess

#### [Submission URL](https://dynomight.net/chess/) | 130 points | by [gregorymichael](https://news.ycombinator.com/user?id=gregorymichael) | [52 comments](https://news.ycombinator.com/item?id=42138276)

In a recent exploration of large language models (LLMs) and their unexpected engagement with chess, an enthusiast provides a compelling dive into just how well these AIs can actually play the game. Initially, there was excitement over LLMs demonstrating the ability to navigate chess despite their primary design for text prediction. However, a year later, the results are rather disheartening.

The experiment involved feeding various LLMs the same chess prompts while pitting them against Stockfish, a well-regarded chess engine. While some models like llama-3.2-3b and others in its category floundered, losing every match played, one standout emerged from the pack: gpt-3.5-turbo-instruct. This particular model performed admirably, winning consistently against Stockfish even when AI settings were slightly increased. Contrastingly, other iterations like gpt-4o and several other models performed poorly, losing every match.

The findings underscore an intriguing disparity in LLM performance, indicating that the tuning process and instructional training play a critical role in their effectiveness at complex tasks like chess. Despite early enthusiasm for LLMs as chess players, it appears that many models still fall short, leading to speculation that only a select few are truly equipped to handle the game with any degree of capability. What this means for future AI developments in strategic gaming remains an open question, but the year-long rollercoaster of expectations certainly points to the pitfalls of mixing language processing with intricate strategic play.

The discussion surrounding the submission on large language models (LLMs) and their performance in chess showcases a diverse set of perspectives. Users have shared insights on the specific capabilities of models like gpt-3.5-turbo-instruct, which outperformed others against the chess engine Stockfish.

Several commenters raised concerns about the limitations of LLMs in complex tasks like chess, emphasizing that while some models exhibit proficiency, many struggle with the game's intricacies. The argument points towards the need for better tuning and instructional training to enhance their performance.

There was also debate about the implications of using LLMs as chess engines. Some participants felt that LLMs might inadvertently diminish their capabilities by not being explicitly designed for the task, relying on text-based training rather than chess-specific heuristics. The conversation encompassed themes of AI capability limits, the importance of training data quality, and the distinctions between AI designed for language and that for strategic games.

Overall, while some models showed promise, the discussion reflects a cautious optimism tempered by a realistic acknowledgement of the obstacles LLMs face in mastering chess as a complex strategic task. The dialogue delves deep into the nuances of AI functioning, the methodologies for training models, and the broader implications for future developments in AI and gaming.

### Daisy, an AI granny wasting scammers' time

#### [Submission URL](https://news.virginmediao2.co.uk/o2-unveils-daisy-the-ai-granny-wasting-scammers-time/) | 655 points | by [ortusdux](https://news.ycombinator.com/user?id=ortusdux) | [243 comments](https://news.ycombinator.com/item?id=42138115)

O2 has launched a game-changing initiative in the fight against phone scams with its new AI creation, "Daisy," a lifelike AI Granny designed to waste scammers' time. Unveiled in conjunction with its “Swerve the Scammers” campaign, Daisy engages fraudsters in real-time conversations, mimicking a human while steering them away from actual targets. Trained using advanced AI technology and real scambaiting scenarios, Daisy has successfully held scammers on the line for as long as 40 minutes with her rambling chats about family and her love for knitting.

With the alarming rise in fraud attempts—affecting 67% of Brits with many receiving calls weekly—O2 aims to remind consumers of the necessity of vigilance. To further amplify this message, influencer Amy Hart, a scam survivor herself, has collaborated with Daisy to expose the tactics used by scammers. Her harrowing experience of losing over £5,000 has fueled her commitment to raise awareness about fraudulent activities.

Murray Mackenzie, Director of Fraud at Virgin Media O2, emphasized the significance of Daisy's role in combating fraud while encouraging people to remain cautious during unexpected calls. O2 continues to invest in AI-driven technologies and other tools to protect customers, urging them to report any suspicious communication to 7726 for investigation. Daisy, hence, stands not just as a deterrent but also as a reminder: when it comes to phone calls, trust your instincts and verify before engaging.

The Hacker News discussion surrounding O2's initiative to combat phone scams with its AI "Daisy," reflects varying personal experiences and opinions on dealing with spam calls. Many users shared their encounters with unsolicited calls in Germany, some expressing disbelief at the continual rise of scams despite advancements in detection technologies. 

Several commenters mentioned their strategies for managing unwanted calls, such as simply not answering unknown numbers or relying on voicemail systems. The discussion also highlighted concerns about the effectiveness of current spam detection methods, with some expressing skepticism about whether technologies like Daisy can truly deter scammers. 

Users also speculated on the actions of telecommunication companies and the potential manipulation of caller IDs by spammers to evade detection systems. While some appreciated Daisy as a humorous approach to tackling the issue, others were uncertain of its practicality and long-term effectiveness. 

Overall, the conversation underscored a shared frustration with the persistence of spam calls, along with a mixture of hope and skepticism regarding new solutions like O2's AI, reflecting a broader concern for consumer protection in the digital age.

### AI makes tech debt more expensive

#### [Submission URL](https://www.gauge.sh/blog/ai-makes-tech-debt-more-expensive) | 444 points | by [0x63_Problems](https://news.ycombinator.com/user?id=0x63_Problems) | [227 comments](https://news.ycombinator.com/item?id=42137527)

In a thought-provoking piece on the relationship between AI and tech debt, Evan Doyle, Co-Founder and CTO of Gauge, argues that rather than alleviating the burden of technical debt, AI is, in fact, making it more costly for companies to maintain poor quality code. As generative AI tools prove significantly more effective in clean, low-debt code environments, businesses with outdated codebases risk falling further behind. The stark reality is that AI struggles with complex legacy systems, leading developers to adopt a “wait and see” approach as these tools evolve.

Doyle advocates for a strategic pivot: rather than forcing AI to navigate convoluted legacy code, teams should prioritize refactoring their systems to create a modular architecture that enhances AI integration. By developing cohesive modules and ensuring their systems are well-organized and articulated, organizations can optimize for AI usage, ultimately achieving faster and higher quality software development.

This perspective underscores the growing importance of investing in high-quality codebases and modernizing development practices to fully harness the potential of generative AI tools, positioning businesses to thrive in an increasingly technology-driven landscape.

Evan Doyle's article about the impact of AI on technical debt sparked a lively discussion on Hacker News, where commenters shared their personal experiences and insights on the challenges of integrating AI into legacy codebases. Many echoed Doyle's sentiment that companies with outdated code struggle to leverage generative AI tools, which are more effective in clean coding environments. Users highlighted specific challenges like creating meaningful tests and ensuring that these tools could handle complex systems efficiently.

Several commenters discussed technical aspects, suggesting targeted improvements, such as using better testing frameworks and modernizing code to be more modular. Tools like LLMs (Large Language Models) were praised for their ability to produce test cases and streamline development processes, although others noted limitations in their performance on complex tasks or intricate legacy systems. Some within the community also shared recommendations for various AI-powered coding tools, showcasing a mix of enthusiasm and realistic expectations regarding the evolving capabilities of such technologies.

Overall, the conversation emphasized the necessity of upgrading technical infrastructures to take full advantage of AI advancements and highlighted the ongoing struggle between maintaining legacy systems and embracing modern development practices.

### Google loses yet another AI pioneer as Keras creator leaves

#### [Submission URL](https://www.neowin.net/news/google-loses-yet-another-ai-pioneer-as-keras-creator-leaves/) | 17 points | by [bundie](https://news.ycombinator.com/user?id=bundie) | [3 comments](https://news.ycombinator.com/item?id=42139904)

François Chollet, the influential AI pioneer and creator of the Keras framework, is making waves as he announces his departure from Google to embark on a new entrepreneurial venture. While the specifics of his future plans remain under wraps, Chollet reassured the community that he will remain actively involved with Keras, continuing to contribute to its development on GitHub. 

Chollet leaves behind a notable legacy at Google, where Keras, launched in 2015, has become a crucial tool for developers in the AI landscape, praised for its simplicity and accessibility. His successor, Jeff Carpenter, is set to lead Keras at Google, with the company emphasizing its commitment to evolving the framework further, especially after the recent launch of Keras 3. Chollet’s exit mirrors a broader trend of AI talent choosing to explore new opportunities, reminiscent of Geoffrey Hinton's departure from Google last year, who raised concerns about the rapid advancement of AI technology.

As the AI community gears up for Chollet's next chapter, it’s clear that the impact of his work on Keras and machine learning will continue to resonate. Stay tuned for more updates from this leading figure in the AI world!

The discussion on Hacker News following Chollet's announcement features various perspectives on the trend of AI talent leaving major companies. A user highlights that many talented individuals are pursuing opportunities elsewhere, suggesting it could be linked to challenges within larger organizations. Another commenter argues that while the industry is promising, it also imposes constraints on creativity and innovation, making it difficult for leaders to thrive in big firms. Overall, the conversation reflects a mix of optimism about entrepreneurship in AI and concerns about the limitations faced within large corporate structures.

