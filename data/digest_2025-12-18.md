## AI Submissions for Thu Dec 18 2025 {{ 'date': '2025-12-18T17:11:00.529Z' }}

### History LLMs: Models trained exclusively on pre-1913 texts

#### [Submission URL](https://github.com/DGoettlich/history-llms) | 651 points | by [iamwil](https://news.ycombinator.com/user?id=iamwil) | [315 comments](https://news.ycombinator.com/item?id=46319826)

History-locked LLMs: Researchers plan “Ranke-4B,” a family of time-capsule models
- What it is: An academic team (UZH, Cologne) is building Ranke-4B, 4B-parameter language models based on Qwen3, each trained solely on time-stamped historical text up to specific cutoff years. Initial cutoffs: 1913, 1929, 1933, 1939, 1946.
- Data and training: Trained from scratch on 80B tokens drawn from a curated 600B-token historical corpus; positioned as “the largest possible historical LLMs.”
- Why it’s different: The models are “fully time-locked” (no post-cutoff knowledge) and use “uncontaminated bootstrapping” to minimize alignment that would override period norms. The goal is to create “windows into the past” for humanities, social science, and CS research.
- Sample behavior: The 1913 model doesn’t “know” Adolf Hitler and exhibits period-typical moral judgments, including attitudes that would now be considered discriminatory. The authors include a clear disclaimer that they do not endorse the views expressed by the models.
- Openness: They say they’ll release artifacts across the pipeline—pre/posttraining data, checkpoints, and repositories.
- Status: Announced as an upcoming release; project hub is at DGoettlich/history-llms (GitHub).

**History-locked LLMs: Researchers plan “Ranke-4B,” a family of time-capsule models**
**What it is:** Researchers from UZH and Cologne are developing "Ranke-4B," a series of language models trained exclusively on historical data up to specific cutoff years (starting with 1913). By using "uncontaminated bootstrapping," these 4B-parameter models aim to eliminate modern hindsight bias—for example, the 1913 model has no knowledge of WWI or Adolf Hitler and reflects the moral norms of its era. The project, intended for humanities and social science research, plans to release all checkpoints and datasets openly.

**The Discussion:**
The concept of strictly "time-locked" AI sparked a debate blending literary analysis with geopolitical anxiety.

*   **Sci-Fi as Blueprint:** Users immediately drew parallels to Dan Simmons’ *Hyperion Cantos*, specifically the plotline involving an AI reconstruction of the poet John Keats. This segued into a broader discussion on the "Torment Nexus" trope—the tendency of tech companies to build things specifically warned about in science fiction. Palantir was cited as a prime example, with users noting the irony of a surveillance company naming itself after a villain’s tool from *Lord of the Rings*.
*   **Simulating Leadership:** The conversation pivoted to a related report about the CIA using chatbots to simulate world leaders for analysts. While some users dismissed this as "laughably bad" bureaucratic theater or a "fancy badge on a book report," others speculated that with enough sensory data and private intelligence, modeling distinct psychological profiles (like Trump vs. Kim Jong Un) might actually be feasible.
*   **Prediction vs. Hindsight:** Commenters debated the utility of these models. Some viewed them as generating "historical fiction" rather than genuine insights, while others argued that removing "hindsight contamination" is the only way to truly understand how historical events unfolded without the inevitability bias present in modern LLMs.

### How China built its ‘Manhattan Project’ to rival the West in AI chips

#### [Submission URL](https://www.japantimes.co.jp/business/2025/12/18/tech/china-west-ai-chips/) | 416 points | by [artninja1988](https://news.ycombinator.com/user?id=artninja1988) | [505 comments](https://news.ycombinator.com/item?id=46316907)

China’s EUV breakthrough? Reuters reports that a government-run “Manhattan Project”-style effort in Shenzhen has produced a prototype extreme ultraviolet (EUV) lithography machine—technology the West has long monopolized via ASML. The system, completed in early 2025 and now under test, reportedly spans nearly an entire factory floor and was built by a team of former ASML engineers who reverse‑engineered the tool. Huawei is said to be involved at every step of the supply chain.

Why it matters
- EUV is the chokepoint behind cutting-edge chips for AI, smartphones, and advanced weapons. Breaking ASML’s monopoly would undercut years of U.S.-led export controls.
- If validated and scalable, China could accelerate domestic production of sub‑7nm chips, loosening reliance on Western tools.

Reality check
- Reuters cites two sources; independent verification isn’t public.
- Building a prototype is far from high-volume manufacturing. Throughput, uptime, defectivity, overlay, and ecosystem pieces (masks, pellicles, resists, metrology) are massive hurdles.
- Legal and geopolitical fallout (IP investigations, tighter sanctions, pressure on the Netherlands/ASML) is likely.

What to watch next
- Independent specs: numerical aperture, source power, throughput, overlay.
- Test wafer yields and any tape-outs at advanced nodes.
- How quickly domestic suppliers fill critical EUV subcomponents.
- Policy responses from the U.S., EU, and the Netherlands—and any actions targeting ex‑ASML talent.

If confirmed, this would be the most significant challenge yet to the export-control regime built around EUV.

Here is a summary of the discussion:

**Material Conditions vs. Cultural Narratives**
The discussion opened with a debate on whether checking reported breakthroughs against "national character" is useful. User *ynhngyhy* noted that EUV machines "weren't made by God," implying that reverse engineering is simply a matter of time and resources, though they cautioned that corruption and fraudulent projects have historically plagued China's semiconductor sector. Others, like *snpcstr* and *MrSkelter*, argued that cultural explanations for technological dominance are "fairy tales"; they posit that U.S. dominance has been a result of material conditions (being the largest rich country for a century) and that China’s huge population and middle class will inevitably shift those statistics.

**Comparative Inefficiencies**
A significant portion of the thread pivoted to comparing structural weaknesses in both nations. While users acknowledged corruption as a drag on China, *dngs* and others highlighted systemic inefficiencies in the U.S., citing exorbitant healthcare costs, poor urban planning (car dependency), and the inability to build infrastructure (subways) at reasonable prices compared to China’s high-speed rail network. The consensus among these commenters was that while the U.S. benefits from efficiency in some sectors, it wastes immense resources on litigation and protectionism.

**The "Brain Drain" Model vs. Domestic Scale**
The role of talent acquisition fueled a debate on diversity and immigration. Users discussed the U.S. model of relying on global "brain drain" to import top talent, contrasting it with China's strategy of generating massive domestic engineering capacity.
*   *mxglt* noted a generational divide in massive Chinese tech firms: older leaders often view the West as the standard, while a younger wave of "techno-optimists" and nationalists believe they can overtake incumbents.
*   A sub-thread explored U.S. visa policy, with users like *cbm-vc-20* suggesting the U.S. should mandate or incentivize foreign graduates to stay to prevent them from taking their skills back to compete against the U.S.

**Skepticism and Pragmatism**
Overall, the sentiment leaned away from dismissing the report based on ideology. As *heavyset_go* summarized, relying on cultural arguments to predict economic velocity is like "Schrodinger's cat"—often used to explain why a country *can't* succeed until they suddenly do.

### Firefox will have an option to disable all AI features

#### [Submission URL](https://mastodon.social/@firefoxwebdevs/115740500373677782) | 514 points | by [twapi](https://news.ycombinator.com/user?id=twapi) | [484 comments](https://news.ycombinator.com/item?id=46316409)

I’m ready to summarize, but I don’t have the submission. Please share one of the following:
- The Hacker News thread URL
- The article link or pasted text
- The title plus key points or notable comments

If you want a full daily digest, tell me:
- How many top stories to include and for which date/time window
- Any preference on length (e.g., 3–5 sentence summaries vs. deeper dives)

By default, I’ll deliver:
- What happened and why it matters
- Key technical/market takeaways
- Notable community reactions (top comments/themes)
- Links for further reading and a quick TL;DR

Here is a summary of the provided discussion regarding Mozilla, AI, and browser development.

### **The Story: Mozilla’s AI Focus vs. Core Browser Health**

**What happened:**
A discussion erupted regarding Mozilla’s recent push into AI features. The community sentiment is largely critical, arguing that the backlash against AI isn't simply "anti-AI," but rather frustration that Mozilla is chasing "fads" (crypto, VR, AI) while neglecting the core browser and stripping away power-user features.

**Why it matters:**
Firefox remains the only significant alternative to the Chromium browser engine monopoly (Chrome, Edge, Brave, etc.). As Mozilla struggles for financial independence from Google, their strategy to bundle revenue-generating services (like AI or VPNs) is clashing with their core user base, who prioritize privacy, performance, and deep extensibility.

### **Key Technical & Market Takeaways**

*   **The "Fad" Cycle vs. Sustainability:** Commenters argue Mozilla has a history of "jumping fads" (allocating resources to VR or Crypto) instead of maintaining the browser core. However, counter-arguments suggest this is a survival tactic: "Mozilla isn't jumping fads, it's jumping towards money." Because users rarely pay for browsers directly, Mozilla chases where the investment capital flows (currently AI).
*   **Extensibility vs. Security:** A major friction point remains the death of XUL and NPAPI (old, powerful extension systems) in favor of WebExtensions and Manifest v2/v3.
    *   *The Critique:* Users feel the browser has become a "bundled garbage" suite rather than an extensible platform.
    *   *The Technical Reality:* While deep access (XUL) allowed for total customization, it was a security nightmare and hampered performance. The debate continues on whether modern WebAPIs (WebUSB, WebNFC) are sufficient replacements or if they just turn the browser into a bloated operating system.
*   **The "Platform" Debate:** There is disagreement on the intent of a browser. Some view the web as a "de-facto standard application platform" that requires hardware access (USB/Serial), while others see this scope creep as a security risk that turns the browser into a resource-heavy OS layer.

### **Notable Community Reactions**

*   **The "Power User" Lament:** User `tlltctl` initiated the discussion by arguing that the real issue isn't AI itself, but the lack of "genuine extensibility." They argue Mozilla should remove bundled features and instead provide APIs so users can add what they want (including AI) via extensions.
*   **The "Fork" Fantasy:** `gncrlstr` and others voiced a desire for a "serious fork" of Firefox that removes the "nonsense" and focuses purely on the browser engine, though others acknowledged the immense cost and difficulty of maintaining a modern browser engine.
*   **The Irony of "Focus":** User `forephought4` proposed a sarcastic/idealistic "5-step plan" for Mozilla to succeed (building a Gmail competitor, an office suite, etc.). Another user, `jsnltt`, pointed out the irony: the plan calls for "focusing on the core," yet simultaneously suggests building a massive suite of non-browser products.
*   **Implementation Ideas:** `mrwsl` suggested a technical middle ground: rather than bundling a specific AI, Mozilla should architect a "plug-able" system (similar to Linux kernel modules or Dtrace) allowing users to install their own AI subsystems if they choose.

### **TL;DR**
Users are angry that Mozilla is bundling AI features into Firefox, viewing it as another desperate attempt to monetize a "fad" rather than fixing the core browser. The community wants a fast, stripped-down, highly extensible browser, but acknowledges the harsh reality that "core browsers" don't attract the investor funding Mozilla needs to survive against Google.

***

*Note: The input text was heavily abbreviated (vowels removed). This summary reconstructs the likely intent of the conversation based on standard technical context and the visible keywords.*

### T5Gemma 2: The next generation of encoder-decoder models

#### [Submission URL](https://blog.google/technology/developers/t5gemma-2/) | 141 points | by [milomg](https://news.ycombinator.com/user?id=milomg) | [26 comments](https://news.ycombinator.com/item?id=46317657)

Google’s next-gen encoder‑decoder line, T5Gemma 2, brings major architectural changes and Gemma 3-era capabilities into small, deployable packages—now with vision, long context, and broad multilingual support.

What’s new
- Architectural efficiency: 
  - Tied encoder/decoder embeddings to cut parameters.
  - “Merged” decoder attention that fuses self- and cross-attention in one layer, simplifying the stack and improving parallelization.
- Multimodality: Adds a lightweight vision encoder for image+text tasks (VQA, multimodal reasoning).
- Long context: Up to 128K tokens via alternating local/global attention, with a separate encoder improving long-context handling.
- Multilingual: Trained for 140+ languages.

Model sizes (pretrained, excluding vision encoder)
- 270M-270M (~370M total)
- 1B-1B (~1.7B)
- 4B-4B (~7B)
Designed for rapid experimentation and on-device use.

Performance highlights
- Multimodal: Outperforms Gemma 3 on several benchmarks despite starting from text-only Gemma 3 bases (270M, 1B).
- Long context: Substantial gains over both Gemma 3 and the original T5Gemma.
- General capabilities: Better coding, reasoning, and multilingual performance than corresponding Gemma 3 sizes.
- Post-training note: No instruction-tuned checkpoints released; reported post-training results use minimal SFT (no RL) and are illustrative.

Why it matters
- Signals a renewed push for encoder‑decoder architectures—especially compelling for multimodal and very long-context workloads—while keeping parameter counts low enough for edge/on-device scenarios.

Availability
- Pretrained checkpoints now on arXiv (paper), Kaggle, Hugging Face, Colab, and Vertex AI (inference).

**T5Gemma 2: Architecture and Use Cases**
Discussion focused on the practical distinctions between the T5 (Encoder-Decoder) architecture and the dominant Decoder-only models (like GPT).
*   **Architecture & Efficiency:** Users clarified confusion regarding the model sizes (e.g., 1B+1B). Commenters noted that due to tied embeddings between the encoder and decoder, the total parameter count is significantly lower than simply doubling a standard model, maintaining a compact memory footprint.
*   **Fine-Tuning Constraints:** There was significant interest in fine-tuning these models. Experienced users warned that fine-tuning a multimodal model solely on text data usually results in "catastrophic forgetting" of the vision capabilities; preserving multimodal performance requires including image data in the fine-tuning set.
*   **Use Case Suitability:** Participants discussed why one would choose T5 over Gemma. The consensus was that Encoder-Decoder architectures remain superior for specific "input-to-output" tasks like translation and summarization, as they separate the problem of understanding the input (Encoding) from generating the response (Decoding).
*   **Google Context:** A member of the T5/Gemma team chimed in to point users toward the original 2017 Transformer paper to understand the lineage of the architecture.

### FunctionGemma 270M Model

#### [Submission URL](https://blog.google/technology/developers/functiongemma/) | 211 points | by [mariobm](https://news.ycombinator.com/user?id=mariobm) | [54 comments](https://news.ycombinator.com/item?id=46316533)

FunctionGemma: a tiny, on-device function-calling specialist built on Gemma 3 (270M)

What’s new
- Google released FunctionGemma, a 270M-parameter variant of Gemma 3 fine-tuned for function calling, plus a training recipe to specialize it for your own APIs.
- Designed to run locally (phones, NVIDIA Jetson Nano), it can both call tools (structured JSON) and talk to users (natural language), acting as an offline agent or a gateway that routes harder tasks to bigger models (e.g., Gemma 3 27B).

Why it matters
- Moves from “chat” to “action” at the edge: low-latency, private, battery-conscious automation for mobile and embedded devices.
- Emphasizes specialization over prompting: on a “Mobile Actions” eval, fine-tuning boosted accuracy from 58% to 85%, highlighting that reliable tool use on-device benefits from task-specific training.
- Built for structured output: Gemma’s 256k vocab helps tokenize JSON and multilingual inputs efficiently, reducing sequence length and latency.

When to use it
- You have a defined API surface (smart home, media, navigation, OS controls).
- You can fine-tune for deterministic behavior rather than rely on zero-shot prompting.
- You want local-first agents that handle common tasks offline and escalate complex ones to a larger model.

Ecosystem and tooling
- Train: Hugging Face Transformers, Unsloth, Keras, NVIDIA NeMo.
- Deploy: LiteRT-LM, vLLM, MLX, Llama.cpp, Ollama, Vertex AI, LM Studio.
- Available on Hugging Face and Kaggle; demos in the Google AI Edge Gallery app; includes a cookbook, Colab, and a Mobile Actions dataset.

Demos
- Mobile Actions (offline assistant: calendar, contacts, flashlight).
- TinyGarden (voice → game API calls like plantCrop/waterCrop).
- Physics Playground (browser-based puzzles with Transformers.js).

Caveats
- The strongest results come after fine-tuning on your specific tools and schemas.
- At 270M, expect limits on complex reasoning; treat it as a fast, reliable tool-caller and router, not a general-purpose heavy thinker.

Here is a summary of the discussion:

**A Google Research Lead participated in the thread**
canyon289 (OP) engaged extensively with commenters, positioning FunctionGemma not as a general-purpose thinker, but as a specialized component in a larger system. He described the model as a "starter pack" for training your own functions, designed to be the "fast layer" that handles simple tasks locally while escalating complex reasoning to larger models (like Gemma 27B or Gemini).

**The "Local Router" Architecture**
There was significant interest in using FunctionGemma as a low-latency, privacy-preserving "switchboard."
*   **The Workflow:** Users proposed a "dumb/fast" local layer to handle basic system interactions (e.g., OS controls) and route deeper reasoning prompts to the cloud. OP validated this, noting that small, customizable models are meant to fill the gap between raw code and frontier models.
*   **Security:** When asked about scoping permissions, OP advised against relying on the model/tokens for security. Permissions should be enforced by the surrounding system architecture, not the LLM.

**Fine-Tuning Strategy**
Users asked how to tune the model without "obliterating" its general abilities.
*   **Data Volume:** The amount of data required depends on input complexity. A simple boolean toggle (Flashlight On/Off) needs very few examples. However, a tool capable of parsing variable inputs (e.g., natural language dates, multilingual queries) requires significantly more training data to bridge the gap between user intent and structured JSON.
*   **Generality:** To maintain general reasoning while fine-tuning, OP suggested using a low learning rate or LoRA (Low-Rank Adaptation).

**Limitations and Concerns**
*   **Context Window:** Replying to a user wanting to build a search-based Q&A bot, OP warned that the 270M model's 32k context window is likely too small for heavy RAG (Retrieval-Augmented Generation) tasks; larger models (4B+) are better suited for summarizing search results.
*   **Reasoning:** The model is not designed for complex zero-shot reasoning or chaining actions without specific fine-tuning. One user questioned if the cited 85% accuracy on mobile actions is "production grade" for system tools; others suggested techniques like Chain-of-Thought or quorum selection could push reliability near 100%.
*   **No Native Audio:** Several users asked about speech capabilities. OP clarified that FunctionGemma is text-in/text-out; it requires a separate ASR (Automatic Speech Recognition) model (like Whisper) to handle voice inputs.

**Demos & Future**
Users were impressed by browser-based WebML demos (games controlled by voice/actions). OP hinted at future releases, suggesting 2026 would be a significant year for bringing more modalities (like open-weights speech models) to the edge.

### Local WYSIWYG Markdown, mockup, data model editor powered by Claude Code

#### [Submission URL](https://nimbalyst.com) | 27 points | by [wek](https://news.ycombinator.com/user?id=wek) | [5 comments](https://news.ycombinator.com/item?id=46318191)

Nimbalyst is a free, local WYSIWYG markdown editor and session manager built specifically for Claude Code. It lets you iterate with AI across your full context—docs, mockups, diagrams, data models (via MCP), and code—without bouncing between an IDE, terminal, and note-taking tools. Sessions are first-class: tie them to documents, run agents in parallel, resume work later, and even treat past sessions as context for coding and reviews. Everything lives locally with git integration, so you can annotate, edit, embed outputs, and build data models from your code/doc set in one UI. It’s available for macOS, Windows, and Linux; free to use but requires a Claude Pro or Max subscription.

**Nimbalyst: Local WYSIWYG Editor for Claude Code**
The creator, `wk`, introduced Nimbalyst as a beta tool designed to bridge the gap between Claude Code and local work contexts, allowing users to manage docs, diagrams, and mockups in a unified interface. Key features highlighted included iterating on HTML mockups, integrating Mermaid diagrams, and tying sessions directly to documents. Early adopter `iman453` responded positively, noting they had already switched their default terminal to the tool. Additionally, the creator confirmed to `radial_symmetry` that the implementation focuses on a WYSIWYG markdown editing experience rather than a plain text view.

### AI helps ship faster but it produces 1.7× more bugs

#### [Submission URL](https://www.coderabbit.ai/blog/state-of-ai-vs-human-code-generation-report) | 202 points | by [birdculture](https://news.ycombinator.com/user?id=birdculture) | [164 comments](https://news.ycombinator.com/item?id=46312159)

CodeRabbit’s new analysis compares AI-generated pull requests to human-written ones and finds AI contributions trigger significantly more review issues—both in volume and severity. The authors note study limitations but say the patterns are consistent across categories.

Key findings
- Overall: AI PRs had ~1.7× more issues.
- Severity: More critical and major issues vs. human PRs.
- Correctness: Logic/correctness issues up 75% in AI PRs.
- Readability: >3× increase with AI contributions.
- Robustness: Error/exception handling gaps nearly 2× higher.
- Security: Up to 2.74× more security issues.
- Performance: Regressions were rarer overall but skewed toward AI.
- Concurrency/deps: ~2× more correctness issues.
- Hygiene: Formatting problems 2.66× higher; naming inconsistencies nearly 2×.

Why this happens (per the authors)
- LLMs optimize for plausible code, not necessarily correct or project-aligned code.
- Missing repository/domain context and implicit conventions.
- Weak defaults around error paths, security, performance, concurrency.
- Drift from team style/readability norms.

What teams can do
- Provide rich context to the model (repo, architecture, constraints).
- Enforce style and conventions with policy-as-code.
- Add correctness rails: stricter tests, property/fuzz testing, typed APIs.
- Strengthen security defaults: SAST, secrets scanning, dependency policies.
- Steer toward efficient patterns with prompts and linters/perf budgets.
- Use AI-aware PR checklists.
- Get help reviewing and testing AI code (automated and human).

Bottom line: AI can speed up coding, but without strong guardrails it increases defects—especially in correctness, security, and readability. Treat AI code like a junior contributor: give it context, enforce standards, and verify rigorously.

Based on the discussion, commenters largely validated the report’s findings, drawing heavily on an analogy to "VB (Visual Basic) Coding" to describe the specific type of low-quality code AI tends to produce.

**The "VB Coding" and "Zombie State" Problem**
The most prominent theme was the comparison of AI code to bad "Visual Basic" habits, specifically the use of `On Error Resume Next` or blind null-checking.
*   **Swallowing Exceptions:** Users argued that AI optimizes for "not crashing" rather than correctness. It tends to insert frequent, unthoughtful null checks or `try/catch` blocks that suppress errors silently.
*   **The Consequence:** While the application keeps running, it enters a corrupted or "zombie" state where data is invalid, making root-cause debugging nearly impossible compared to a hard crash with a stack trace.
*   **Defensive Clutter:** One user noted AI operates on a "corporate safe style," generating defensive code intended to stop juniors from breaking things, but resulting in massive amounts of cruft.

**Automated Mediocrity**
Commenters discussed the quality gap between senior developers and AI output.
*   **Average Inputs:** Since models are trained on the "aggregate" of available code, they produce "middle-of-the-road" or mediocre code.
*   **The Skill Split:** "Subpar" developers view AI as a godsend because it works better than they do, while experienced developers find it irritating because they have to fight the AI to stop it from using bad patterns (like "stringly typed" logic or missing invariants).
*   **The Long-Term Risk:** Users worried about the normalization of mediocrity, comparing LLMs to "bad compilers written by mediocre developers."

**The Productivity Illusion vs. Tech Debt**
Several users shared anecdotes suggesting that the speed gained in coding is lost in maintenance.
*   **The "StackOverflow" Multiplier:** Users compared AI to the "copy-paste developer" of the past who blindly stole code from StackOverflow, noting that AI just automates and accelerates this bad behavior.
*   **Real-world Costs:** One user described a team where 40% of capacity is now spent on tech debt and rework caused by AI code. They cited an example where an AI-generated caching solution looked correct but silently failed to actually cache anything.
*   **Design Blindness:** Commenters emphasized that AI is good at syntax ("getting things on screen") but fails at "problem solving" and proper system design.

**Valid Use Cases**
Despite the criticism, some users offered nuance on where AI still succeeds:
*   **Explainer Tool:** One user noted that while they don't trust AI to write code, it is excellent at reading and explaining unfamiliar open-source packages or codebases, effectively replacing documentation searches.
*   **Boilerplate:** For simple CRUD/business apps or "tab-complete" suggestions, it remains useful if the developer strictly enforces architectural rules.

