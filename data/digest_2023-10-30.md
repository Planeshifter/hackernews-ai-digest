## AI Submissions for Mon Oct 30 2023 {{ 'date': '2023-10-30T17:11:39.851Z' }}

### RedPajama v2 Open Dataset with 30T Tokens for Training LLMs

#### [Submission URL](https://together.ai/blog/redpajama-data-v2) | 216 points | by [programd](https://news.ycombinator.com/user?id=programd) | [54 comments](https://news.ycombinator.com/item?id=38077521)

Today, Together released a new version of the RedPajama dataset, called RedPajama-Data-v2. It is an open dataset containing 30 trillion filtered and deduplicated tokens from 84 CommonCrawl dumps in five languages: English, French, Spanish, German, and Italian. RedPajama-Data-v2 is the largest public dataset specifically released for training large language models (LLMs). What makes this release even more exciting is that it includes over 40 pre-computed quality annotations, allowing the community to further filter and weigh the data. The RedPajama-Data-v2 dataset aims to make it easier for LLM developers by providing a pool of web data that can serve as a base for high-quality datasets for LLM training. The dataset also includes code snippets that demonstrate how commonly used filtering rules can be implemented with RedPajama-V2. Overall, this release is a significant step towards the development of open datasets for training large language models.

The comments on this submission cover a wide range of topics related to the RedPajama dataset and language models.

One user suggests adding more features to the RedPajama dataset, such as topic modeling and extractive summarization. They also mention the importance of addressing the distribution of training data to better answer questions and provide supporting examples.

Another user points out a potential issue with the dataset, discussing the reversal curse problem where models may generate "B sn A" instead of "A's daughter."

There is a discussion about the hosting and downloading of the dataset. One user is confused about how the dataset is being released and asks for clarification. Another user explains that the dataset is being processed and filtered from CommonCrawl data and suggests contributing features or asking questions on GitHub.

There are discussions about the size and duplication of the dataset. One user points out that the RedPajama dataset is 150TB in size, and there is a debate about the redundancy and quality of the training data.

There are comments about the relevance of the dataset and the languages included. Some users express surprise at the inclusion of certain languages, while others discuss the importance of having a diverse range of languages for training language models.

There is a brief discussion about copyright infringement and the legal implications of using certain datasets. Users debate whether AI models that generate copyrighted content violate copyright laws.

Overall, the comments cover various aspects related to the RedPajama dataset, including its features, hosting, quality, relevance, and legal implications.

### Executive Order on Safe, Secure, and Trustworthy Artificial Intelligence

#### [Submission URL](https://www.whitehouse.gov/briefing-room/statements-releases/2023/10/30/fact-sheet-president-biden-issues-executive-order-on-safe-secure-and-trustworthy-artificial-intelligence/) | 222 points | by [Mandelmus](https://news.ycombinator.com/user?id=Mandelmus) | [329 comments](https://news.ycombinator.com/item?id=38067314)

President Biden Issues Executive Order to Advance AI Safety and Security, Protect Privacy, and Promote Equity
Today, President Joe Biden issued an Executive Order aimed at ensuring that the United States leads the way in harnessing the potential of artificial intelligence (AI) while managing its risks. The Order establishes new standards for AI safety and security, protects Americans' privacy, advances equity and civil rights, promotes innovation and competition, and strengthens American leadership in AI globally. 

One of the key actions mandated by the Order is the requirement for developers of powerful AI systems to share safety test results and critical information with the U.S. government. This will ensure that AI systems are safe, secure, and trustworthy before they are made public. Additionally, the National Institute of Standards and Technology will develop rigorous standards for extensive red-team testing of AI systems to ensure their safety. The Department of Homeland Security will apply these standards to critical infrastructure sectors and establish an AI Safety and Security Board.

The Executive Order also addresses the risks associated with using AI to engineer dangerous biological materials. Strong new standards for biological synthesis screening will be developed, ensuring that appropriate screening is carried out and potential risks exacerbated by AI are managed effectively. Furthermore, the Order aims to protect Americans from AI-enabled fraud and deception by establishing standards and best practices for detecting AI-generated content and authenticating official content.

In terms of privacy protection, the President calls on Congress to pass bipartisan data privacy legislation that safeguards Americans' privacy, especially that of children. The Order emphasizes federal support for the development and use of privacy-preserving techniques, including those that utilize AI, and directs funding towards research and development in this area. Additionally, the Order aims to evaluate and strengthen privacy guidance for federal agencies in light of AI risks and commercial data procurement.

Finally, the Executive Order addresses the potential for discrimination, bias, and other abuses in justice, healthcare, and housing resulting from irresponsible uses of AI. The Biden-Harris Administration has already taken steps to address these issues by publishing a Blueprint for a Coordinated Approach to AI, which prioritizes fairness and civil rights in the development and deployment of AI systems.

Overall, President Biden's Executive Order on AI signifies a comprehensive strategy for responsible innovation that addresses key areas of concern such as safety, security, privacy, and equity. It sets a new precedent for AI governance and aims to position the United States as a global leader in the responsible development and deployment of AI technologies.

The discussion on Hacker News revolves around different aspects of the Executive Order and its potential implications. Some users express concern about the potential for increased regulation stifling innovation, while others argue that regulation is necessary to ensure public safety and accountability. There is also a discussion about the ethics of AI development, with some users pointing out the risks associated with AI being used to create dangerous biological materials or autonomous weapons. The discussion touches on various topics, including government regulation, the role of large tech companies, the balance between innovation and safety, and the importance of privacy protection.

### Can I remove my personal data from GenAI training datasets?

#### [Submission URL](https://knowingmachines.org/knowing-legal-machines/legal-explainer/questions/can-i-remove-my-personal-data-from-genai-training-datasets) | 99 points | by [randomlogin](https://news.ycombinator.com/user?id=randomlogin) | [99 comments](https://news.ycombinator.com/item?id=38075924)

The short answer is that it is practically difficult to remove personal data from GenAI training datasets. Many GenAI products are trained on massive datasets that contain personal information scraped from popular websites, such as social media platforms and online encyclopedias. These datasets often include people's names and contact information. While some tools like "Have I Been Trained" and the earlier Exposing.ai project allow users to investigate whether their personal data is in these datasets, it is challenging to ascertain and remove personal data completely. 

Companies that create GenAI products often do not disclose the sources of their training data, making it even harder for individuals to determine if their data is included. Taking legal action or making data requests under laws like the California Consumer Privacy Act (CCPA) may be potential avenues for seeking the removal of personal data. However, companies may respond in idiosyncratic ways and further information may be required to process such requests. 

Currently, there is no foolproof method to force companies to disclose if and how much personal data has been used in training datasets. Even if companies were able to disclose and remove personal data, fully removing it from training datasets could unpredictably impact the performance of the GenAI models. The issue is being addressed by big tech players like Google, who are working on technical solutions such as the "Machine Unlearning Challenge" to protect privacy rights. However, until significant technical progress is made, companies may struggle to comply with data deletion provisions in privacy laws like the CCPA.

The discussion on Hacker News revolves around different aspects of the article. Here are some key points mentioned:

- Some users discuss the legal challenges of removing personal data from AI training datasets, highlighting the difficulties in compliance with laws like GDPR and CCPA.
- There is a debate about the expectation of privacy on the internet, with some users arguing that public postings should not be expected to remain private.
- The discussion also touches on the potential risk of misusing AI technology, such as the concern that AI models could be trained on sensitive or inappropriate content.
- Users highlight the importance of technological solutions to protect privacy rights, with some pointing out the need for better data handling practices by companies.
- There is a discussion about the potential role of legislation in regulating AI and protecting privacy, with some users expressing skepticism about the effectiveness of regulations in the face of technical challenges.
- Some users raise concerns about the power dynamics between individuals and companies, suggesting that companies should take responsibility for data privacy and make efforts to remove personal data from their AI products.

Overall, the discussion reflects a range of perspectives on the challenges and implications of removing personal data from AI training datasets and the broader issues surrounding privacy in the age of AI.

### Show HN: EnfinBref- {GPT3-5|Mistral-7B} YouTube summaries, segment by segment

#### [Submission URL](https://enfinbref.io/en/) | 21 points | by [bclavie](https://news.ycombinator.com/user?id=bclavie) | [10 comments](https://news.ycombinator.com/item?id=38077725)

Today's top story on Hacker News is a fascinating discussion on the future of artificial intelligence and its societal implications. One user shared an article that explores how AI is evolving at an unprecedented rate and what that means for humans. Are we on the verge of being outsmarted by machines? The comments are filled with thought-provoking insights from experts in the field, sparking a lively debate on the potential dangers and benefits of AI. Whether you're an AI enthusiast or simply curious about its impact on our lives, this discussion is a must-read. Dive in, and join the conversation!

The discussion on this submission revolves around segmenting videos, improving transcription, and the practical applications of AI in language processing.

- User "el_isma" suggests breaking down video segments based on specific time intervals, which could be useful for speech-to-text transcription and mastering complex tasks.
- User "zdmnsn" adds that they have tried summarizing sections of 20-40 minute French videos using section-dividing techniques and found it to be impressive.
- User "scfrnd" finds it useful to extract information from videos but mentions that sometimes the speech recognition software doesn't work efficiently.
- User "bnsystm" expresses their appreciation for local videos and asks for suggestions regarding specific scenarios and models related to video breakdowns and transcription.
- User "bclv" responds with gratitude and elaborates on different scenarios and models for video breakdowns and transcription. They also mention that their project started in 2018 but fizzled out and didn't have much success with pre-LLM implementations.
- User "bnsystm" thanks them and suggests that they should take their time and research more before fully committing to a new project.

Overall, the discussion highlights the challenges and potential applications of AI in language processing, especially in video segmentation and transcription tasks. Users appreciate the progress made so far and provide insights into the different factors to consider when implementing AI models for specific use cases.

### Google Brain founder says big tech is lying about AI danger

#### [Submission URL](https://www.afr.com/technology/google-brain-founder-says-big-tech-is-lying-about-ai-human-extinction-danger-20231027-p5efnz) | 391 points | by [emptysongglass](https://news.ycombinator.com/user?id=emptysongglass) | [390 comments](https://news.ycombinator.com/item?id=38072218)

Big Tech companies like OpenAI are using fear-mongering tactics to push for strict regulations on artificial intelligence (AI) that would ultimately stifle competition in the industry, according to Stanford University professor Andrew Ng. He argues that the idea that AI could lead to the extinction of humanity is a baseless claim being used as a ploy to promote heavy regulation. Ng believes that transparency rather than burdensome licensing requirements is what is truly needed in the AI sector. He also criticizes the regulatory capture campaign orchestrated by large tech firms to impose legislation harmful to the open-source community. While Ng agrees that some form of regulation is necessary, he emphasizes the importance of thoughtful regulation rather than arbitrary measures. Ng highlights the need for transparency from tech companies, as it would help prevent future AI disasters caused by the actions of big tech.

The discussion on the submission revolves around various viewpoints on the role of regulation and innovation in the AI industry, particularly in relation to China. Some users argue that strict regulations ensure safety and protect against intellectual property theft, while others believe that excessive regulations stifle competition and hinder small startups. There are also discussions on China's approach to innovation and its alleged practice of copying and stealing foreign designs. Some users emphasize the importance of cultural understanding in analyzing China's actions, while others highlight the need for fair and reasonable rules that align with global values. Overall, the discussion raises questions about the balance between regulation and innovation in the AI industry and the role China plays in this context.

### WebAuthn.wtf

#### [Submission URL](https://webauthn.wtf/) | 48 points | by [mooreds](https://news.ycombinator.com/user?id=mooreds) | [16 comments](https://news.ycombinator.com/item?id=38065083)

WebAuthn is an API specification that allows applications to use secure authentication methods for user registration and login. Instead of relying solely on passwords, WebAuthn enables users to authenticate themselves using hardware- or software-based authenticators. These authenticators use public-key cryptography to ensure secure registration and authentication of accounts.

With WebAuthn, users generate a public-private key pair, where the private key is stored securely on the user's device and the public key is registered with the web application server. During login, the user verifies their identity using the registered device, and the server validates the signature using the previously registered public key.

As a developer, you can utilize WebAuthn to provide your users with a more secure and user-friendly authentication mechanism. It is supported by most modern web browsers and platforms, and integration with existing authentication flows is made easier with open-source libraries and identity platforms.

In the WebAuthn world, the user's credential is the public-private key pair generated by an authenticator, rather than the traditional username and password. These credentials are commonly referred to as "passkeys." While WebAuthn is designed for single-device usage, syncing passkeys between devices is outside the scope of the specification.

To address the need for passkey syncing, large platform and operating system companies like Apple and Google have implemented their own secure channels. For example, Apple uses iCloud Keychain and Google uses Google Password Manager to sync passkeys across a user's compatible devices. This eliminates the need for a separate backup authenticator and provides a more convenient experience for users.

The discussion on this submission covers various aspects of WebAuthn and passkey syncing. Some users point out that certain password managers like 1Password, Bitwarden, and KeePassXC do not currently support WebAuthn. Others discuss the technical restrictions on authenticator attestation GUIDs and the different levels of support for WebAuthn across platforms and devices.

There are also discussions about the potential benefits and challenges of passkey syncing. Some users argue that passkey syncing can be achieved through existing platforms like iCloud Keychain and Google Password Manager, while others express concerns about the security and backup options for passkeys.

Further discussions highlight the need for better documentation and communication from the FIDO Alliance and criticize the current state of passkey implementation in various password managers. Some users mention the Linux support for passkeys and express frustration with the lack of support in Firefox.

Overall, the discussion revolves around the technical aspects and practical considerations of adopting WebAuthn and passkey syncing in different contexts.

### AI.gov

#### [Submission URL](https://ai.gov/) | 327 points | by [KoftaBob](https://news.ycombinator.com/user?id=KoftaBob) | [238 comments](https://news.ycombinator.com/item?id=38067206)

President Biden is making AI work for the American people, recognizing its potential as one of the most powerful technologies of our time. The Biden-Harris Administration has taken decisive action to protect safety and rights in the age of AI, ensuring that everyone can benefit from its promise. This includes signing an Executive Order to advance agencies' efforts on AI across the federal government and rapidly hiring talent to build and govern AI based on the administration's priorities. American students, workers, and educators are encouraged to build their AI skills to contribute to the nation's leadership in this field. Furthermore, the government is harnessing the opportunities of AI to improve its services for the public, and the National AI Advisory Committee advises the White House on AI-related issues.

The discussion on the article about President Biden's initiatives with AI focused on various aspects of government involvement in the field and its impact on the workforce.

One user pointed out that the US government often struggles with hiring and paying competitive salaries to attract top AI talent. They suggested adopting a similar model to the UK's Civil Service, which has been highly functional and effective in utilizing external consultants and contractors.

Another user mentioned the potential dangers of relying too heavily on innovation and neglecting the roles of civil servants. They emphasized the importance of balancing innovation with maintaining core government services.

The Digital Services team was also mentioned as a potential solution to improving government systems and services. However, there were concerns about the complexity and effectiveness of the Government Digital Services in the UK.

The discussion veered toward the topic of drug testing as a requirement for government jobs, with some arguing that it should not be a determining factor for employment. One user highlighted the federal legalization of marijuana and questioned the enforcement of drug testing in the workplace.

There were also mentions of issues with bureaucracy and the challenges of implementing quick changes and innovation within government structures.

Overall, the discussion touched on issues of hiring, innovation, privacy, and the role of government in shaping the AI landscape.

### Show HN: Web app to generate AI pictures with logos "hidden" in them

#### [Submission URL](https://logopictureai.com/) | 53 points | by [igorkotua](https://news.ycombinator.com/user?id=igorkotua) | [55 comments](https://news.ycombinator.com/item?id=38072074)

Introducing LogoPicture AI, the ultimate solution for creating stunning optical illusion art with your logo. No more struggling to find the perfect images for your brand – now you can easily generate captivating logo art in just a few minutes. With endless picture ideas limited only by your imagination, you'll never run out of inspiration. Even if you're feeling stuck, our AI will suggest creative ideas to get you started. Here's how it works: simply upload your logo in png or jpeg format, choose a style from our predefined prompts or create your own, and within minutes, you'll receive a collection of visually striking pictures right in your email. And the best part? It's incredibly affordable, with pricing plans starting at just $9.90 for 50 pictures. Plus, with a 7-day money-back guarantee for the Starter plan, you can try LogoPicture AI risk-free. Still have questions? Check out our FAQ section for answers to common queries. Don't wait – start generating mesmerizing logo art with LogoPicture AI today!

There is a mixed reaction to the submission of LogoPicture AI on Hacker News. Some users express skepticism about the tool, stating that people with Photoshop skills can easily create similar effects without the need for AI. Others compare it to Thomas Kinkade-style art and express doubts about its originality and quality. One user points out that there are already low-cost niche services available for image manipulation and suggests alternative tools. The pricing plans and refund policy of LogoPicture AI are also discussed, with some users expressing interest in trying the service for a lower price. The user interface and mobile optimization of the website receive criticism, while others appreciate the convenience of the predefined prompts and the ease of generating logo art. There is a brief discussion about the implementation of ControlNet and the demand for high-quality graphics. Some users mention the availability of similar tools and APIs, while others highlight the potential risks and ethical concerns of AI-generated content. Grammar issues on the website and the lack of a download feature for the generated images are also mentioned by users. The discussion concludes with users expressing varying levels of interest in trying the tool and suggesting potential improvements.

### An AI firm harvested billions of photos without consent. UK is powerless to act

#### [Submission URL](https://www.politico.eu/article/ai-ruling-obstruct-british-efforts-protect-citizens-images-us-data-harvesting/) | 42 points | by [ColinWright](https://news.ycombinator.com/user?id=ColinWright) | [17 comments](https://news.ycombinator.com/item?id=38066455)

In a recent ruling, Britain's top privacy regulator, the Information Commissioner's Office (ICO), has lost its appeal against Clearview AI, an American-based AI firm. Clearview AI came under fire in 2020 for harvesting billions of social media images without users' consent. The ICO had taken action against Clearview last year, issuing a £7.5 million fine and ordering the deletion of data of UK residents from its database. However, the recent ruling states that the ICO has no power to sanction the AI firm, leaving it unclear whether Clearview ever deleted the data. The ruling also highlights that the ICO lacks jurisdiction in the case as the data processing was carried out on behalf of foreign government agencies. The ICO is considering its next steps in response to the ruling.

The discussion on this submission covered a variety of topics related to Clearview AI and data privacy:

- One user mentioned that generally, pictures on the internet are restricted in usage and require the consent of the photographer. They also pointed out that using these pictures for training AI models could potentially infringe on copyright laws.
- Another user questioned whether it is legally acceptable for an AI program to send an explicitly reconstructed version of copyrighted images without permission.
- There was a brief tangent about copying and selling copies of Disney movies and recorded cable broadcasts without consent.
- One user highlighted the issue of the UK Information Commissioner's Office (ICO) being powerless to take action against Clearview AI, which brings into question whether the company deleted the data as ordered.
- Brexit and its impact on international trade and corporate strategy related to data privacy rights were also discussed. The user mentioned that the UK's powerless position may make it difficult to comply with international arrest warrants related to data protection laws.
- The availability of AI models like OpenAI's DALL-E, capable of generating content based on consented data, led to discussion on the potential for AI to circumvent content restrictions.
- The user pointed out that the theft of personal data by algorithms is a serious concern, given that AI systems process personal data subject to GDPR regulations.
- There was a brief comment about the idea of copying and stealing as related to the Clearview AI case.

Overall, the discussion touched on issues of consent, copyright, data privacy, international trade, and the role of AI in processing personal data.

