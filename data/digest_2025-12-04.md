## AI Submissions for Thu Dec 04 2025 {{ 'date': '2025-12-04T17:10:37.442Z' }}

### How elites could shape mass preferences as AI reduces persuasion costs

#### [Submission URL](https://arxiv.org/abs/2512.04047) | 649 points | by [50kIters](https://news.ycombinator.com/user?id=50kIters) | [602 comments](https://news.ycombinator.com/item?id=46145180)

TL;DR: A theory paper argues that as AI slashes the cost and boosts the precision of persuasion, political elites have incentives to strategically engineer the distribution of public preferences—often nudging societies toward polarization. With rival elites, the same tech can instead “park” opinions in harder-to-flip zones, so advances could either amplify or dampen polarization depending on the competitive environment.

What’s new
- Treats the mass distribution of policy preferences as a controllable variable when persuasion becomes cheap and precise via AI.
- Frames polarization not as an organic byproduct, but as an instrument of governance under majority rule.

Core model (intuition)
- Elites choose how much to reshape opinion distributions subject to persuasion costs and the need to win majorities.
- Lower costs (AI targeting, automation) expand feasible interventions.

Key findings
- Single-elite setting: Optimal strategies exert a “polarization pull,” pushing opinions toward more extreme profiles; better persuasion tech accelerates this drift.
- Two opposed elites alternating in power: Incentives emerge to create “semi-lock” regions—more cohesive, hard-to-overturn opinion clusters. Depending on parameters, tech improvements can either raise or reduce overall polarization.

Why it matters
- Recasts polarization as a strategic choice in an AI era, suggesting a governance arms race over opinion engineering.
- Highlights risks to democratic stability and the potential value of policy guardrails around AI-driven persuasion.

Caveats
- Theoretical model; outcomes hinge on assumptions about costs, majority rules, and elite behavior. Real-world frictions (backlash, regulation, norms) may blunt or reshape effects.

Paper: arXiv:2512.04047 (econ.GN) — “Polarization by Design: How Elites Could Shape Mass Preferences as AI Reduces Persuasion Costs” by Nadav Kunievsky
Link: https://arxiv.org/abs/2512.04047

Here is a summary of the discussion:

**The Nature of Democracy and Public Opinion**
The conversation opens with a philosophical debate on the role of the electorate. Citing Philip Converse’s 1964 work, *The Nature of Belief Systems in Mass Publics*, users discuss whether the average voter actually holds coherent policy preferences or if they are merely swayed by elite grouping.
*   **Corrective vs. Prescriptive:** Participants debate the purpose of democracy. While some argue it creates a "corrective system" designed only to peacefully remove bad leaders (majority rule as a safety valve), others express cynicism, arguing that modern systems fail to produce quality leadership or effectively remove incompetence.
*   **Educational Decay:** Some attribute the malleability of public opinion to a failure in the education system, suggesting that "intellectually soft" schooling has left a vacuum that social media algorithms now fill.

**Case Study: The Standardization of Opinion on Tariffs**
The abstract concepts of the paper are immediately applied to a granular debate about tariffs, serving as a proxy for how complex economic policies are polarized or misunderstood.
*   **Intent vs. Outcome:** Users distinguish between the *desire* for tariffs (national security, bringing back manufacturing jobs) and the *mechanics* (companies shifting costs downstream to consumers). Critics argue that tariffs on intermediate goods (like steel) actually hurt domestic manufacturers by raising their input costs.
*   **Externalities and Ethics:** A segment of the discussion defends tariffs not as economic boosters, but as tools to address externalities—specifically, penalizing foreign competitors who rely on pollution or weak labor laws (e.g., child labor) to undercut prices.
*   **Corruption and Implementation:** Skeptics view tariffs as vectors for corruption, noting that they encourage companies to lobby for exemptions (e.g., the Apple/Trump dynamic) rather than innovate. Others note that for tariffs to work, they require long-term credibility; otherwise, they are viewed as temporary political signaling.

### We gave 5 LLMs $100K to trade stocks for 8 months

#### [Submission URL](https://www.aitradearena.com/research/we-ran-llms-for-8-months) | 320 points | by [cheeseblubber](https://news.ycombinator.com/user?id=cheeseblubber) | [262 comments](https://news.ycombinator.com/item?id=46154491)

Got it—please share the Hacker News submission you want summarized. You can paste:
- The HN link, or the article title + body/text
- Optional: a few top comments if you want the discussion reflected

Also tell me your preference:
- Format: 3-bullet quick take, 1-paragraph brief, or 5–7 bullet deep-dive
- Extras: “Why it matters,” notable dissent, key numbers, or caveats

Here is a summary of the discussion regarding the comparison of AI models (Grok, DeepSeek, Gemini) for stock portfolio generation.

**Format:** Deep-Dive (5 Bullets)

*   **The Problem of Data Leakage:** The primary critique of the submission is the difficulty of valid backtesting. Commenters argue that because LLMs are trained on vast amounts of internet data (news, papers, stock mechanics), they effectively "know" the future of the test dataset. Even if you hide specific stock prices, the models have ingested the general narrative of which companies succeeded (e.g., Nvidia's rise), making them look artificially prophetic.
*   **Methodological Flaws in Splitting Data:** Users debated how to properly train/test an AI trader. Splitting by time is flawed (as noted above). Splitting by stock (training on 90% of the market, testing on 10%) is also rejected due to **autocorrelation**; stocks in the same sector (like AMD and Nvidia) move together. If the model knows Nvidia went up (from training data), it can infer AMD likely did too, nullifying the "blind" test.
*   **Grok’s Potential "Uncensored" Edge:** A sub-thread debated whether Grok has an advantage over Gemini or ChatGPT. Proponents argued that Grok’s access to real-time X (Twitter) data and fewer "safety/political correctness" guardrails might result in less "distorted" reality processing compared to corporate-safe models. Others countered that this is likely irrelevant to market mechanics or that superior reasoning capabilities (OpenAI/Anthropic) still outweigh "edginess."
*   **Market Reflexivity and Saturation:** Commenters noted that institutional trading firms likely already integrate LLMs for sentiment analysis (reading news/socials). There is skepticism that a retail trader using an off-the-shelf LLM can find alpha that High Frequency Trading (HFT) firms haven't already arbitraged away. Furthermore, if enough retail traders follow AI picks, they create a "reflexive loop" where the AI influences the price rather than predicting it.
*   **The "Hot Hand" Fallacy:** The discussion touched on the nature of market beating. One user noted that hedge funds often beat the market for 2–4 years before reverting to zero or underperforming. This suggests that even if an AI model performs well for a short cycle, it may simply be riding a sector beta (like Tech) rather than possessing true predictive skill, echoing the "lucky 10,000" concept.

**Why it matters:**
This discussion highlights the gap between **Generative AI capabilities** (writing code, summarizing text) and **predictive financial modeling**. It underscores that LLMs are fundamentally "historians" that have read the entire internet, making them poor candidates for forecasting chaotic systems where they cannot separate their training data from the "future" events they are supposed to predict.

### CUDA-l2: Surpassing cuBLAS performance for matrix multiplication through RL

#### [Submission URL](https://github.com/deepreinforce-ai/CUDA-L2) | 122 points | by [dzign](https://news.ycombinator.com/user?id=dzign) | [14 comments](https://news.ycombinator.com/item?id=46153058)

CUDA-L2: RL-tuned CUDA kernels that (claim to) beat cuBLAS on A100 HGEMM

- What it is: An open-source system that combines LLMs with reinforcement learning to auto-generate and tune half-precision GEMM (HGEMM) CUDA kernels. The authors claim consistent speedups over torch.matmul, cuBLAS, and cuBLASLt (both heuristic and autotuning) across 1,000 M×N×K shapes on an A100.

- What’s new: A release of A100-optimized HGEMM kernels covering 1,000 configurations. The repo includes benchmarking scripts and results.

- Why it matters: cuBLAS/cuBLASLt are tough baselines; surpassing them—even for a subset of shapes/precisions—suggests automated kernel search via RL+LLMs can uncover non-trivial performance wins and could generalize to broader GPU ops.

- Scope and caveats:
  - Hardware: Tuned for A100 (SM80). The authors say speedups on other GPUs (e.g., RTX 3090, H100) aren’t guaranteed; more architectures planned (Ada, Hopper, Blackwell).
  - Precision: Current kernels are F16×F16→F16 (16-bit accumulator); F32 accumulation variants are on the roadmap.
  - Coverage: Fixed set of 1,000 shapes; for missing sizes they suggest using the nearest larger config and padding.
  - Generality: Claims are specific to HGEMM and these shapes; real-world gains depend on your model’s matmul patterns and batch sizes.

- How it works (at a high level): Uses CUTLASS as the foundation and RL to search kernel parameters/schedules. The repo contains precompiled kernels (e.g., SM80_16x8x16_F16F16F16F16) and tooling to compile/evaluate.

- Getting started:
  - Requirements: PyTorch ≥ 2.6.0, CUTLASS v4.2.1 (exact), TORCH_CUDA_ARCH_LIST="8.0".
  - Env: export CUTLASS_DIR=/path/to/cutlass and TORCH_CUDA_ARCH_LIST="8.0".
  - Run: ./eval_one_file.sh --mnk M_N_K --warmup_seconds 5 --benchmark_seconds 10 --mode offline|server [--target_qps N]
  - Modes: offline (batch) or server (QPS-targeted microbenchmarking).
  - License: MIT.

- If your shape isn’t included: Open a GitHub issue with your M/N/K, or pad to a supported size.

- Contact: jiwei_li@deep-reinforce.com

Discussion starters:
- How robust are the gains across real LLM inference/training pipelines vs microbenchmarks?
- Will RL-found kernels transfer across GPU generations, or need per-arch training?
- Could this approach extend to BF16/F32 or attention kernels, and rival Triton/TVM autotuners at scale?

The discussion centers on the novelty of the optimization techniques, the practical limits of the fixed-shape approach, and the difficulty of using RL for kernel generation.

**Novelty vs. Implementation**
Commenters debated whether the "novel" techniques discovered by the system (Section 5 of the paper) were genuinely new or simply a reshuffling of well-known methods. Some described the output as "standard *GPU Gems* advice" rather than algorithmic breakthroughs. Others argued that the value lies not in new theory—standard matrix multiplication theory hasn't changed drastically in decades—but in using LLMs to navigate the massive search space for optimal *implementations* on specific hardware.

**Practical limitations and Padding**
Users scrutinized the requirement to pad unsupported shapes to the nearest included configuration. One user noted that padding zeros could easily negate the performance gains, potentially making these specialized kernels slower than general-purpose libraries for specific dimensions. However, others defended "code specialization" as a cheap way to gain performance percentages on critical operations where standard libraries are too generalized.

**RL Challenges and Benchmarking**
The difficulty of applying RL to CUDA was highlighted by a user with similar experience; they noted that while generating valid code is easy, getting a model to "escape its distribution" to invent truly novel instruction sequences (like complex double-buffering patterns) remains very hard. Regarding the benchmarks, there was confusion over the visualization—readers found the "speedup percentage" charts (where 0% implies parity with cuBLAS) less intuitive than raw performance numbers. There was also a brief dispute regarding whether the benchmarks fairly compared FP16 inputs against FP32 baselines.

### State of AI: An Empirical 100T Token Study with OpenRouter

#### [Submission URL](https://openrouter.ai/state-of-ai) | 196 points | by [anjneymidha](https://news.ycombinator.com/user?id=anjneymidha) | [91 comments](https://news.ycombinator.com/item?id=46154022)

State of AI: An Empirical 100 Trillion Token Study with OpenRouter (a16z + OpenRouter)

What’s new
- A large-scale, usage-focused study of LLMs based on more than 100 trillion tokens routed through OpenRouter, spanning many models, tasks, regions, and time.
- Frames December 5, 2024 (OpenAI’s o1) as an inflection point: a shift from single-pass generation to multi-step, reasoning-style inference.

Key findings
- Open-weight surge: Meaningful real-world adoption of open-source models alongside closed APIs.
- Roleplay is big: Creative roleplay emerges as an outsized category, rivaling the usual suspects like coding assistance.
- Agentic inference rises: More multi-step, tool-assisted flows; models are increasingly components in larger automated systems rather than single-turn chatbots.
- Cohort durability: Early “foundational” cohorts retain far longer than later cohorts—the “Cinderella Glass Slipper” effect.
- Sensitivity to market dynamics: Pricing and new model launches materially shift usage patterns.

Why it matters
- Product direction: Don’t assume productivity-only use; roleplay and coding remain major drivers. Build for multi-step/agent workflows, not just single responses.
- Model strategy: Open weights are competitive in real usage; pricing and reliability for tool use and long chains matter as much as raw benchmarks.
- Infra implications: Orchestration across diverse models is now a norm; latency, cost controls, and agent-friendly features are key differentiators.

Caveats
- Single platform lens: Data comes from OpenRouter, which may skew toward developers using a router and experimenting across models.
- Affiliations: Authors include a16z and OpenRouter; interpret comparative claims with that context.
- Privacy/aggregation details aren’t in the excerpt; methodology quality will matter for any task labeling and geographic breakdowns.

Bottom line
- Real-world LLM use is more diverse and agentic than many narratives suggest, with open-source models gaining share, roleplay unexpectedly dominant, and early users sticking around far longer. If you’re building with LLMs, optimize for multi-step workflows, cost-aware routing, and the categories people actually spend time on.

**Discussion Summary**
Hacker News users analyzed the report with a skeptical eye, focusing primarily on selection bias inherent to the OpenRouter platform and privacy concerns regarding the data methodology.

*   **Platform Selection Bias:** Commenters argued that OpenRouter’s data does not represent the broader market. They suggested the platform attracts specific niches—indie hackers, developers, and roleplay enthusiasts—while excluding large enterprise sectors (fintech, healthcare) that cannot use data aggregators due to security compliance.
*   **The "Roleplay" Anomaly:** The dominance of roleplay (nearly 60% of open-source tokens) was attributed to users seeking uncensored or low-cost models for applications like SillyTavern or creative writing. Users noted that "mainstream" commercial APIs (OpenAI/Anthropic) are often too expensive or heavily moderated for these use cases, naturally funneling that specific traffic to OpenRouter and skewing the statistics.
*   **Small Models Are Self-Hosted:** Several users disputed the finding that small model usage is declining. They argued that 7B-parameter models are increasingly self-hosted on consumer hardware (e.g., Mac Studio, gaming GPUs) for privacy and zero marginal cost. Consequently, API aggregators are primarily used for massive "frontier" models (like o1 or Claude 3.5) that cannot run locally, creating a false signal that small model usage is dropping.
*   **Privacy & Methodology:** There was significant criticism regarding OpenRouter’s methodology of inspecting and classifying user prompts (sometimes via Google APIs). While some noted that users opt-in for a discount, others viewed this as a "privacy theater" deal-breaker for serious business use, reinforcing the idea that the data lacks B2B representation.
*   **Geographic Skews:** The high usage ranking for Singapore was widely interpreted as a proxy for Chinese users and companies utilizing VPNs and Singaporean billing entities to bypass blocking by major US AI labs.

### Microsoft drops AI sales targets in half after salespeople miss their quotas

#### [Submission URL](https://arstechnica.com/ai/2025/12/microsoft-slashes-ai-sales-growth-targets-as-customers-resist-unproven-agents/) | 418 points | by [OptionOfT](https://news.ycombinator.com/user?id=OptionOfT) | [326 comments](https://news.ycombinator.com/item?id=46148748)

Microsoft reportedly cut AI agent sales targets after widespread quota misses

- The Information reports Microsoft lowered growth targets for its AI “agent” products after many Azure sales teams missed quotas in the fiscal year ending June. One US unit set a 50% growth target for Azure AI Foundry and saw fewer than 20% of reps hit it; targets were cut to ~25% this year. Another unit’s “double Foundry sales” goal was trimmed to 50%.
- This follows a year of heavy “agentic” marketing—Build and Ignite showcased Word/Excel/PowerPoint agents in Microsoft 365 Copilot plus tools like Copilot Studio and Azure AI Foundry—yet enterprise appetite for premium-priced agent tools appears soft.
- Copilot faces brand and usage headwinds versus ChatGPT. Bloomberg cited Amgen, where staff reportedly gravitated to ChatGPT, using Copilot mainly for Microsoft-specific tasks (Outlook/Teams).
- A deeper issue: today’s agentic systems still confabulate and behave brittly on novel tasks, making fully autonomous, high‑stakes workflows risky without humans in the loop.
- Despite slower enterprise uptake, Microsoft is still spending aggressively: $34.9B capex in the October quarter, with much AI revenue coming from AI companies renting Azure compute rather than traditional enterprises adopting agents.

Why it matters: There’s a gap between the “era of AI agents” pitch and what enterprises will pay for today. Expect more human‑supervised designs, tighter ROI proofs, pricing/bundling tweaks, and continued competition with general chat tools even inside Microsoft shops. The near‑term AI business for hyperscalers still looks more like selling picks-and-shovels (compute) than selling autonomous workers.

**Productivity and Usability Frustrations**
Commenters describe Microsoft’s current AI implementation as clunky and intrusive, with one user likening it to a "bad autocomplete" that requires constant correction (pressing escape/backspace) and wastes time on trivialities rather than optimizing workflows. Several users criticized the "feature checklist" culture at Microsoft, arguing that the push for AI is driven by internal OKRs and promotion incentives rather than user needs, resulting in hundreds of disjointed, low-quality integrations rather than a cohesive, functional product.

**Technical Competence and Hallucinations**
A recurring complaint is that Microsoft's purpose-built tools fail at their specific jobs.
*   **Azure:** Users report that Copilot inside Azure provides useless troubleshooting advice, while pasting the same error logs into generic external models (like Claude or ChatGPT) yields actual solutions.
*   **Coding:** Developers shared "horror stories" of AI autocomplete, including one instance where an AI suggested a `DROP TABLE` command mixed into SQL code. Others noted that LLM-based assistants in IDEs (Visual Studio, JetBrains) often hallucinate non-existent properties or remove valid import statements, forcing users to revert to older, heuristic-based IntelliSense for reliability.

**Degradation of Assistant Utility**
The discussion extends beyond Microsoft to the broader industry trend (including Google's Gemini), where deterministic, functional tools are being replaced by "chatty" but unreliable LLMs. Users expressed frustration that voice assistants have lost the ability to reliably perform simple tasks (like setting timers or navigation) in favor of probabilistic models that increase cognitive load. The consensus views the current wave of enterprise AI as "en-shittification," prioritizing marketing hype over functional stability.

### Show HN: RAG in 3 Lines of Python

#### [Submission URL](https://pypi.org/project/piragi/) | 32 points | by [init0](https://news.ycombinator.com/user?id=init0) | [5 comments](https://news.ycombinator.com/item?id=46142050)

Piragi (v0.3.0): a batteries‑included RAG interface with one‑line setup, local by default

What it is
- A Python library that turns folders, code globs, and URLs into a queryable knowledge base in one line (Ragi([...]).ask("...")). It ships with a vector store, embeddings, citations, and background auto‑updates.

Why it’s interesting
- Zero‑to‑RAG fast: Works out of the box with local models via Ollama; OpenAI‑compatible if you want hosted.
- Always fresh: Background indexing so queries aren’t blocked by updates.
- Built‑in citations and filters for traceable answers.
- Pluggable storage: Local LanceDB (including S3), PostgreSQL/pgvector, or Pinecone.

Notable features
- Formats: PDFs, Office docs, Markdown, code, URLs, images, audio.
- Retrieval: HyDE, hybrid BM25+vector with RRF fusion, and cross‑encoder reranking.
- Chunking: fixed, semantic, contextual, and hierarchical (parent/child) strategies.
- Retrieval‑only mode so you can bring your own LLM or framework.
- Configurable embeddings (default all‑mpnet‑base‑v2; options from ~90MB to ~8GB) and LLM (default llama3.2 via Ollama).

Who it’s for
- Developers who want a simple, local‑first RAG stack with citations and sensible defaults, and teams prototyping doc/code QA without assembling multiple tools.

Caveats and questions
- Development Status: Alpha (PyPI classifier).
- Performance/quality will hinge on chosen models and chunking; large embedding models have hefty RAM/VRAM footprints.
- No claims here about multi‑tenant/enterprise features or security posture.

Meta
- License: MIT; Python 3.9+; author listed as Hemanth HM; PyPI “verified details” flag shown. Released Dec 4, 2025.

Discussion around Piragi was generally positive, highlighting successful testing and specific integration questions.

**Key themes included:**

*   **Documentation & Clarity:** One user critiqued the absence of a definition for "RAG" on the project page, noting that defining acronyms makes the tool friendlier and aids discoverability.
*   **User Experience:** Feedback was complimentary regarding the developer experience, with users praising the "great documentation" and confirming the library worked "brilliantly" during initial testing.
*   **Feature Requests:** Commenters asked about specific capabilities, including future support for Graph/RDF and compatibility with AWS Bedrock.

