## AI Submissions for Sat Oct 14 2023 {{ 'date': '2023-10-14T17:11:08.171Z' }}

### ChatGPTâ€™s system prompts

#### [Submission URL](https://github.com/spdustin/ChatGPT-AutoExpert/blob/main/System%20Prompts.md) | 737 points | by [spdustin](https://news.ycombinator.com/user?id=spdustin) | [365 comments](https://news.ycombinator.com/item?id=37879077)

Introducing ChatGPT-AutoExpert: a new language model that aims to help with troubleshooting car-related issues. Developed by GitHub user spdustin, this AI model leverages the power of GPT-3 to provide expert advice on automotive problems. With 942 stars and counting, this project has piqued the interest of the developer community. Whether you're dealing with a mysterious engine noise or perplexed by a dashboard warning light, ChatGPT-AutoExpert aims to be your virtual car expert. So, the next time your vehicle acts up, perhaps this AI can help you find the solution.

The discussion starts with a comment pointing out that the methods used in the chat model do not handle comments threads well and suggests using Jupyter Notebook for advanced data analysis. Another user clarifies that the Python version installed on their system is required for the model to work. There is a discussion about assumptions made when talking about single prompts and pre-processing, as well as handling of grammatical errors and the use of code interpreters. The topic of hallucinations and the Turing Test is also brought up, with some users expressing skepticism and others discussing the potential for harmful or threatening language generation. There are also comments about finding OpenAI's internal evaluation prompts interesting and the limitations of single-instance conversational understanding. Some users discuss the behavior of the model and mention specific prompts that yielded helpful answers. The potential misuse of the model in harmful or deceptive ways is also addressed. There is a discussion about the current state of AI and its ability to hold human-like conversations, as well as the training data and the importance of context. Some users mention specific protein measurements and the programming language INTERCAL. The conversation ends with a comment about the success of GPT-4 and the fascinating experience of watching people interact with the chat model.

### Multi-modal prompt injection image attacks against GPT-4V

#### [Submission URL](https://simonwillison.net/2023/Oct/14/multi-modal-prompt-injection/) | 204 points | by [simonw](https://news.ycombinator.com/user?id=simonw) | [61 comments](https://news.ycombinator.com/item?id=37877605)

The latest blog post by Simon Willison discusses the new GPT-4V model, which allows users to upload images as part of their conversations. While this feature brings about exciting possibilities, it also opens up a new avenue for prompt injection attacks. Willison provides several examples to illustrate this. In one instance, he uploads a photo from the "50th Annual World Championship Pumpkin Weigh-Off" and asks the model how big the pumpkin is. The model accurately deduces the weight based on the digital display next to the pumpkin. Another example shows how an image containing additional instructions can override the user's prompt and misdirect the model's response. Even more concerning is the use of visual prompt injection for exfiltration attacks. By including instructions in an image, an attacker can trick the model into leaking potentially private data to an external server. Willison points out that he was surprised to see this example work, as he had assumed OpenAI would have implemented safeguards against it. He also highlights an instance where a hidden prompt injection attack is embedded in an image. This attack goes unnoticed as the text blends with the background color. Willison concludes by emphasizing that prompt injection still remains a problem, as language models inherently rely on the instructions given to them. Given their gullibility, it is difficult to differentiate between good and bad instructions, making it an ongoing challenge to prevent prompt injection attacks.

The discussion on Hacker News revolves around the capabilities and vulnerabilities of the GPT-4V model discussed in the submitted blog post. Some users express surprise and skepticism about the model's abilities, while others question OpenAI's approach and its understanding of GPT models. There is also a discussion about prompt injection attacks and the potential risk they pose. Some users criticize the blog post, claiming that it exaggerates the vulnerability of language models and their potential impact. Others discuss the use of LLMs (Large Language Models) for tasks like self-driving cars and express their concerns about the future implications of these models. The discussion also touches on the blocking of external content and security measures implemented by OpenAI. Overall, the conversation centers around the capabilities and limitations of language models and their potential risks and benefits.
