## AI Submissions for Sat Oct 14 2023 {{ 'date': '2023-10-14T17:11:08.171Z' }}

### ChatGPTâ€™s system prompts

#### [Submission URL](https://github.com/spdustin/ChatGPT-AutoExpert/blob/main/System%20Prompts.md) | 737 points | by [spdustin](https://news.ycombinator.com/user?id=spdustin) | [365 comments](https://news.ycombinator.com/item?id=37879077)

Introducing ChatGPT-AutoExpert: a new language model that aims to help with troubleshooting car-related issues. Developed by GitHub user spdustin, this AI model leverages the power of GPT-3 to provide expert advice on automotive problems. With 942 stars and counting, this project has piqued the interest of the developer community. Whether you're dealing with a mysterious engine noise or perplexed by a dashboard warning light, ChatGPT-AutoExpert aims to be your virtual car expert. So, the next time your vehicle acts up, perhaps this AI can help you find the solution.

The discussion starts with a comment pointing out that the methods used in the chat model do not handle comments threads well and suggests using Jupyter Notebook for advanced data analysis. Another user clarifies that the Python version installed on their system is required for the model to work. There is a discussion about assumptions made when talking about single prompts and pre-processing, as well as handling of grammatical errors and the use of code interpreters. The topic of hallucinations and the Turing Test is also brought up, with some users expressing skepticism and others discussing the potential for harmful or threatening language generation. There are also comments about finding OpenAI's internal evaluation prompts interesting and the limitations of single-instance conversational understanding. Some users discuss the behavior of the model and mention specific prompts that yielded helpful answers. The potential misuse of the model in harmful or deceptive ways is also addressed. There is a discussion about the current state of AI and its ability to hold human-like conversations, as well as the training data and the importance of context. Some users mention specific protein measurements and the programming language INTERCAL. The conversation ends with a comment about the success of GPT-4 and the fascinating experience of watching people interact with the chat model.

### Kalman filter from the ground up

#### [Submission URL](https://www.kalmanfilter.net/default.aspx) | 311 points | by [ColinWright](https://news.ycombinator.com/user?id=ColinWright) | [138 comments](https://news.ycombinator.com/item?id=37879715)

icated beam towards it and measuring the reflected signal. Based on these measurements, the radar estimates the position and velocity of the target.

Now, let's say we receive a measurement at time t=0. With this information, we can make an initial estimation of the target's position and velocity. But what about the next time step, t=5? Without any updates, we can't accurately predict where the target will be.

This is where the Kalman Filter comes in. It takes the initial estimation and combines it with the latest measurement to update the estimate of the target's state. It then uses this updated estimate to predict the target's state at the next time step. This process continues, constantly improving the estimate based on new measurements and predictions.

The beauty of the Kalman Filter lies in its ability to handle uncertainty. It takes into account the uncertainty in both the measurements and the predictions, providing a more accurate estimate of the target's state. This makes it ideal for applications such as target tracking, navigation, and control.

The online tutorial and book by author Alex Becker aim to demystify the Kalman Filter algorithm. They provide intuitive explanations and practical examples, making the topic accessible to a wider audience. The tutorial covers the basics of the Kalman Filter, while the book delves into more advanced topics like non-linear Kalman Filters and practical implementation guidelines.

By the end of the book, readers will have a solid understanding of the Kalman Filter and be able to design, simulate, and evaluate its performance. Whether you're a beginner looking to grasp the concept or a seasoned professional seeking a comprehensive resource, this tutorial and book are valuable tools for mastering the Kalman Filter.

The discussion on this submission revolves around the difficulty of learning and understanding the concepts behind the Kalman Filter algorithm. Some users express frustration with the lack of clear explanations and definitions, while others point out helpful resources such as tutorials and videos that make the topic more accessible. There is also a mention of the Elo rating system and its similarity to Kalman filters, as well as a humorous meme related to the complexity of explaining the concept. Some users discuss the importance of understanding the underlying principles and concepts before diving into the implementation details. Others share their own experiences implementing the Kalman Filter in projects and provide suggestions for resources and libraries to use. Overall, the discussion highlights the challenges of learning and teaching the Kalman Filter algorithm, as well as the importance of clear explanations and foundational knowledge.

### Large Language Models Are Zero-Shot Time Series Forecasters

#### [Submission URL](https://arxiv.org/abs/2310.07820) | 138 points | by [Anon84](https://news.ycombinator.com/user?id=Anon84) | [27 comments](https://news.ycombinator.com/item?id=37877125)

Researchers Nate Gruver, Marc Finzi, Shikai Qiu, and Andrew Gordon Wilson have published a paper titled "Large Language Models Are Zero-Shot Time Series Forecasters" on arXiv. In their study, they explore the use of large language models (LLMs), such as GPT-3 and LLaMA-2, for time series forecasting.

The researchers propose encoding time series data as a string of numerical digits and treating time series forecasting as next-token prediction in text. Surprisingly, they find that LLMs can extrapolate time series data without any training on the specific task and achieve comparable or even better performance than purpose-built time series models.

To enable this performance, the authors develop procedures for tokenizing time series data effectively. They also introduce methods for converting discrete distributions over tokens into flexible densities over continuous values, allowing LLMs to handle multimodal distributions.

The success of LLMs in time series forecasting is attributed to their ability to naturally represent multimodal distributions, aligning with the salient features in many time series, such as repeated seasonal trends. LLMs can also handle missing data without imputation through non-numerical text and incorporate textual side information to improve predictions. Additionally, LLMs can answer questions to provide explanations for their forecasts.

The authors note that increasing the size of LLMs generally improves their performance in time series forecasting. However, they find that GPT-4 can perform worse than GPT-3 due to how it tokenizes numbers and its poor uncertainty calibration, likely resulting from alignment interventions.

Overall, this research highlights the potential of large language models for zero-shot time series forecasting, offering an alternative approach to traditional models. The code for their study is publicly available, and the paper will be presented at NeurIPS 2023.

The discussion on this submission covers a range of topics related to the use of large language models (LLMs) for time series forecasting and the potential implications of this research.

One commenter expresses skepticism about using LLMs for time series forecasting, stating that LLMs are typically used for tasks in natural language processing and translation. They highlight that traditional AI systems were designed for specific tasks and environments, unlike LLMs, which can generalize and combine knowledge from various sources.

Another commenter responds by stating that LLMs have powerful inference capabilities, which arise from their ability to process statistics and generate intelligent outputs. They argue that LLMs can surpass traditional AI systems by combining the knowledge and insights of millions of individual minds.

A discussion arises about the intelligence of LLMs and the analogy of LLMs to human brains. One user compares LLMs to "addition machines," emphasizing that their intelligence comes from their ability to calculate and combine knowledge. Others contribute to this discussion, highlighting the differences in how human brains process information and generate meaning compared to LLMs.

There are also comments discussing the practical applications of LLMs for time series forecasting. Examples include applying them to stock trading, where one user suggests buying low and selling high, and using them to predict cryptocurrency prices.

The discussion touches on the use of LLMs in financial forecasting and the potential benefits they can provide compared to traditional statistical and numerical forecasting models. However, there is a reminder that LLMs have limitations in predicting events that are not explicitly defined or related to other discrete events.

Some comments discuss the technical aspects of LLMs, such as their memory capabilities and the compression of internet data. There is also a debate about the significance of LLMs in terms of their potential computational power and their ability to manage large amounts of data.

One commenter expresses surprise at the capabilities of the timeGPT model, highlighting that it is impressive to see how well it performs in time series forecasting, considering its primary focus on text generation.

Overall, the discussion reflects a mix of enthusiasm, skepticism, and technical considerations surrounding the use of LLMs for time series forecasting.

### Multi-modal prompt injection image attacks against GPT-4V

#### [Submission URL](https://simonwillison.net/2023/Oct/14/multi-modal-prompt-injection/) | 204 points | by [simonw](https://news.ycombinator.com/user?id=simonw) | [61 comments](https://news.ycombinator.com/item?id=37877605)

The latest blog post by Simon Willison discusses the new GPT-4V model, which allows users to upload images as part of their conversations. While this feature brings about exciting possibilities, it also opens up a new avenue for prompt injection attacks. Willison provides several examples to illustrate this. In one instance, he uploads a photo from the "50th Annual World Championship Pumpkin Weigh-Off" and asks the model how big the pumpkin is. The model accurately deduces the weight based on the digital display next to the pumpkin. Another example shows how an image containing additional instructions can override the user's prompt and misdirect the model's response. Even more concerning is the use of visual prompt injection for exfiltration attacks. By including instructions in an image, an attacker can trick the model into leaking potentially private data to an external server. Willison points out that he was surprised to see this example work, as he had assumed OpenAI would have implemented safeguards against it. He also highlights an instance where a hidden prompt injection attack is embedded in an image. This attack goes unnoticed as the text blends with the background color. Willison concludes by emphasizing that prompt injection still remains a problem, as language models inherently rely on the instructions given to them. Given their gullibility, it is difficult to differentiate between good and bad instructions, making it an ongoing challenge to prevent prompt injection attacks.

The discussion on Hacker News revolves around the capabilities and vulnerabilities of the GPT-4V model discussed in the submitted blog post. Some users express surprise and skepticism about the model's abilities, while others question OpenAI's approach and its understanding of GPT models. There is also a discussion about prompt injection attacks and the potential risk they pose. Some users criticize the blog post, claiming that it exaggerates the vulnerability of language models and their potential impact. Others discuss the use of LLMs (Large Language Models) for tasks like self-driving cars and express their concerns about the future implications of these models. The discussion also touches on the blocking of external content and security measures implemented by OpenAI. Overall, the conversation centers around the capabilities and limitations of language models and their potential risks and benefits.

### Robots Ranked

#### [Submission URL](https://robotsguide.com/rankings) | 27 points | by [sundarurfriend](https://news.ycombinator.com/user?id=sundarurfriend) | [3 comments](https://news.ycombinator.com/item?id=37878997)

In today's top stories on Hacker News, there is a discussion about the creepiest robots out there. Topping the list is Telenoid, which has been rated as "Really Creepy" by the most number of people. Other robots on the list include Kaspar, Diego-san, CB2, and Flobi. These robots have received high ratings for being genuinely creepy. The thread is filled with users sharing their experiences and opinions about these unsettling creations. If you're into the darker side of technology, this thread is definitely worth checking out.

In the discussion on the submission, one user named "sndrrfrnd" answers a question about how the robots on the list were rated. They suggest that it may be based on ratings from the IEEE (Institute of Electrical and Electronics Engineers) website, specifically on the individual robots page. They provide a link to the page where the question was asked, and mention that Spot and Atlas have the highest numbers of votes compared to other robots.

Another user named "pwrdbnd" comments that robot kits are now surprisingly affordable and shares a link to an Amazon page about robot kits.

