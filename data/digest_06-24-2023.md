## AI Submissions for Sat Jun 24 2023 {{ 'date': '2023-06-24T17:11:16.412Z' }}

### Many in the AI field think the bigger-is-better approach is running out of road

#### [Submission URL](https://www.economist.com/science-and-technology/2023/06/21/the-bigger-is-better-approach-to-ai-is-running-out-of-road) | 207 points | by [pseudolus](https://news.ycombinator.com/user?id=pseudolus) | [287 comments](https://news.ycombinator.com/item?id=36462282)

As the cost and strain of training and running AI models grows, researchers are beginning to focus on making these models more efficient, rather than simply larger. Instead of pursuing a bigger-is-better approach, the goal is to get more performance out of fewer resources. One approach is to optimize trade-offs, such as cutting the number of parameters but training models with more data, so they are smaller, faster, and cheaper to use. Another option is to use a similar rounding technique, which has been shown to cut down memory consumption, while some users are fine-tuning general-purpose LLMs to focus on a specific task. Google has invented a different option, which involves extracting specific knowledge from a larger model into a smaller, specialized one.

The comments section features a debate about the nature of language processing and the limits of deterministic systems in understanding and modeling language. Some participants argue that human language understanding is complex and probabilistic, and that current language models are limited in their ability to recognize nuanced meanings and higher level concepts. Others suggest that human reasoning is a deterministic process, and that deterministic systems can model language effectively given enough training data. There is also discussion about the potential and limitations of machine learning in emulating human cognitive processes and generating language.

### Semantic MediaWiki

#### [Submission URL](https://www.semantic-mediawiki.org/wiki/Semantic_MediaWiki) | 79 points | by [Tomte](https://news.ycombinator.com/user?id=Tomte) | [35 comments](https://news.ycombinator.com/item?id=36462354)

Semantic MediaWiki is a free, open-source extension to MediaWiki that enables data storage and querying within a wiki's pages. It also acts as a powerful knowledge management system with numerous spin-off extensions. Data created with SMW can be published via the Semantic Web, allowing for seamless integration with other systems. Recently, the SMW sponsorship program was initiated and version 4.1.1 was released. The software is used by a variety of organizations, including KM-A Knowledge Management Associates and Professional Wiki Wiki Services.

Discussions on Hacker News revolved around the potential benefits and challenges of adopting Semantic MediaWiki, with some users highlighting its usefulness in terms of structured content and data management while others raised issues related to scalability and integration with other systems. Some users also shared their experiences in using Semantic MediaWiki for various applications, such as for sports data and knowledge organization.

### Creating an autonomous system for fun and profit (2017)

#### [Submission URL](https://blog.thelifeofkenneth.com/2017/11/creating-autonomous-system-for-fun-and.html) | 83 points | by [bsilvereagle](https://news.ycombinator.com/user?id=bsilvereagle) | [29 comments](https://news.ycombinator.com/item?id=36459881)

In a blog post, a networking and systems engineer discussed how he set up his own autonomous system with twofold motivation: wanting to lower his monthly hosting expenses by sharing a rack with a friend in Hurricane Electric's data center, and a challenge to set it up as an autonomous system. The post explains how the internet is an interconnected fabric of separate networks and each network only interconnects with others in clearly defined places. The post will appeal to networking enthusiasts and those interested in setting up their own internet service provider or web hosting service.

The comments section includes a discussion about the trustworthiness of network equipment and how companies should prioritize security. There is also a discussion about the power consumption of servers and how different companies handle it. Other comments discuss the limitations of Cisco's routers and the challenges involved with handling large amounts of data.

### Visual Studio’s IntelliSense list can now steer GitHub Copilot code completions

#### [Submission URL](https://devblogs.microsoft.com/visualstudio/github-copilot-visual-studio-intellisense/) | 115 points | by [samwillis](https://news.ycombinator.com/user?id=samwillis) | [28 comments](https://news.ycombinator.com/item?id=36459827)

GitHub Copilot is now integrated with Visual Studio's IntelliSense list, making it even easier for developers to explore and find the code completions they need. With this integration, changing a selection in IntelliSense provides GitHub Copilot with additional context about the code, allowing for more accurate predictions and multi-line code completions. Users can accept a Copilot completion by pressing TAB, and IntelliSense member ranking can also steer Copilot predictions. This update also allows for multi-line predictions to be previewed by pressing the Left CTRL button, and users are required to manually update to version 1.84 of Copilot.

### The Magic of Embeddings

#### [Submission URL](https://stack.convex.dev/the-magic-of-embeddings) | 113 points | by [gk1](https://news.ycombinator.com/user?id=gk1) | [21 comments](https://news.ycombinator.com/item?id=36454494)

Have you ever wondered how AI models can compare the meaning of two text strings? The answer is through embeddings, which are lists of numbers that describe a piece of text. By comparing these embeddings, we can search, cluster, recommend, classify, and even measure diversity among text strings. In this article, the writer takes a closer look at using OpenAI's text-embedding-ada-002 model to obtain embeddings, and how to efficiently store and search them using Pinecone or the Convex database. Whether you're working with text strings, images, or audio, embeddings can help unlock their hidden similarities and associations.

Commenters discuss the limitations of Google's semantic understanding and suggest alternative search methods, the challenges of cross-domain work with embeddings, and the trade-off between vector length and computational efficiency. Some commenters share positive experiences with embeddings in their work, and recommend databases like Convex for vector search.
