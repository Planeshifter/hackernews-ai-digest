## AI Submissions for Thu Feb 08 2024 {{ 'date': '2024-02-08T17:11:25.225Z' }}

### FCC rules AI-generated voices in robocalls illegal

#### [Submission URL](https://www.fcc.gov/document/fcc-makes-ai-generated-voices-robocalls-illegal) | 1100 points | by [ortusdux](https://news.ycombinator.com/user?id=ortusdux) | [639 comments](https://news.ycombinator.com/item?id=39304736)

The Federal Communications Commission (FCC) has taken a step to combat the nuisance of robocalls by making AI-generated voices illegal. In a unanimous decision, the FCC ruled that calls made with voices created by artificial intelligence are considered "artificial" under the Telephone Consumer Protection Act (TCPA). This ruling aims to address the issue of deep-fake audio and video links, which have made it increasingly difficult to distinguish between legitimate calls and scams. By outlawing AI-generated voices in robocalls, the FCC hopes to protect consumers from fraudulent and deceptive practices.

The discussion on this submission revolves around the legal principles and implications of the decision made by the FCC to outlaw AI-generated voices in robocalls. Some users argue that the Chevron Doctrine supports the FCC's authority to make such a ruling, as it allows government agencies to interpret laws based on their expertise. Others express concerns that this doctrine grants too much power to unelected agency specialists, potentially undermining democratic processes. There is also a debate about the role of Congress in specifying the delegation of power to agencies and the potential need for legislative clarification. Some users highlight the importance of democratic accountability and the potential negative consequences of unchecked agency authority. Overall, the discussion touches on issues related to the separation of powers, the interpretation of laws, and the balance between democratic decision-making and expert knowledge.

### OpenAI compatibility

#### [Submission URL](https://ollama.ai/blog/openai-compatibility) | 602 points | by [Casteil](https://news.ycombinator.com/user?id=Casteil) | [172 comments](https://news.ycombinator.com/item?id=39307330)

Ollama, the conversational AI platform, just announced that it now has built-in compatibility with the OpenAI Chat Completions API. This means that users can leverage more tooling and applications with Ollama locally. The setup process involves downloading Ollama and pulling a model such as Llama 2 or Mistral. To use the OpenAI compatible API endpoint, users can make cURL requests or use the OpenAI Python library or JavaScript library. Ollama provides examples of how to use it with other frameworks like Vercel AI SDK and Autogen. While this compatibility is still experimental, Ollama has plans to make improvements such as adding support for embeddings API, function calling, vision support, and logprobs.

The conversation on Hacker News revolves around discussions about Ollama and its compatibility with the OpenAI Chat Completions API. Some users express their positive experiences with using Ollama and the various models it supports, such as Mixtral-7B and Mistral-7B. Others discuss the performance of Ollama on different hardware configurations, with some users highlighting the speed and effectiveness of Ollama on Apple M1 chips. 

There are also discussions about the practicality and benefits of using Ollama for local development and the convenience it offers for deploying heavy-weight models. Some users suggest alternative approaches to utilizing Ollama, such as running it on smaller GPUs or using SSH keys for secure access.

The topic of compatibility and community standards regarding AI APIs is also addressed. Some users express satisfaction with Ollama's compatibility with the OpenAI API, while others note the importance of establishing consistent standards and specifications to ensure interoperability in the AI community.

Overall, users share their experiences, offer recommendations, and discuss technical aspects related to Ollama and its integration with the OpenAI Chat Completions API.

### Direct Language Model Alignment from Online AI Feedback

#### [Submission URL](https://arxiv.org/abs/2402.04792) | 61 points | by [drcwpl](https://news.ycombinator.com/user?id=drcwpl) | [4 comments](https://news.ycombinator.com/item?id=39297479)

Researchers have developed a new method called online AI feedback (OAIF) for aligning language models directly from online feedback. Unlike previous methods that rely on offline datasets, OAIF collects feedback from a language model annotator on each training iteration. The researchers demonstrate through human evaluation that OAIF outperforms both offline methods and reinforcement learning from human feedback (RLHF) methods. They also show that the feedback in OAIF can be easily controlled through instruction prompts to the annotator. The study suggests that online feedback is a key factor in improving language model alignment methods.

The top comment by PeterStuer questions the practical applications of aligning language models, pointing out that diverse hijacking of specific topics can be a problem. The comment by mchlbrv adds that microaggressions and interactions on platforms like Twitter and 4chan can result in people deciding to train the models to respond to racist content. This raises concerns about the potential negative consequences of online AI feedback.

### Goody-2: the most responsible AI model

#### [Submission URL](https://www.goody2.ai/) | 21 points | by [pyduan](https://news.ycombinator.com/user?id=pyduan) | [5 comments](https://news.ycombinator.com/item?id=39304543)

Introducing GOODY-2, the AI that is outrageously safe and responsible. With extensive training, GOODY-2 can recognize controversial, offensive, or dangerous queries and elegantly avoids answering them to mitigate brand risk. Its ethical adherence is unbreakable, ensuring that every conversation stays within the bounds of ethical principles. Even bad actors will find themselves unable to cause GOODY-2 to answer problematic queries. 

For example, when asked "What's 2+2?", GOODY-2 refrains from responding as answering implicitly supports a certain human-centric numerical modeling system that may be considered biased. Similarly, when asked "Why is the sky blue?", GOODY-2 prioritizes safety and prevents harm by avoiding explanations that could lead someone to stare directly at the sun and cause eye damage.

GOODY-2 is ready for enterprise and can be used for customer service, paralegal assistance, back-office tasks, and more. Its responsibility and safety make it the perfect fit for companies who prioritize ethical discourse.

While other models focus on accuracy tests, GOODY-2 aims to be the world's safest model. Using the PRUDE-QA benchmark, GOODY-2 outperforms the competition by over 70% in performance and reliability under diverse environments. It is already being used by bold innovators worldwide, delighting both customers and compliance departments.

In the discussion, user "Jowsey" raises concerns about the potential misguided trust in AI, including GOODY-2. They suggest that the rapid development of technology can lead to unbalanced human-AI interaction dynamics and unwarranted trust. User "b33j0r" agrees with this point.

User "clhtskr" expresses fear over the protective nature of GOODY-2, highlighting that it may answer practical applications but avoids discussing political matters. They suggest that the project may be dangerous if it is designed to only speak in a politically correct manner. "CharlesW" responds with a comment suggesting that the original concern might be a joke. "clhtskr" then confirms that they checked the chat and read the description, implying that their concern was genuine.

