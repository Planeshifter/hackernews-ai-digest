## AI Submissions for Tue Jun 24 2025 {{ 'date': '2025-06-24T17:13:28.257Z' }}

### ChatGPT's enterprise success against Copilot fuels OpenAI/Microsoft rivalry

#### [Submission URL](https://www.bloomberg.com/news/articles/2025-06-24/chatgpt-vs-copilot-inside-the-openai-and-microsoft-rivalry) | 281 points | by [mastermaq](https://news.ycombinator.com/user?id=mastermaq) | [301 comments](https://news.ycombinator.com/item?id=44367638)

Microsoft is encountering significant challenges in promoting its Copilot AI assistant to corporate customers, notably due to stiff competition from OpenAI's ChatGPT. Over a year ago, Amgen Inc., a major pharmaceutical company, planned to deploy Microsoft's Copilot for its 20,000 employees, heralding it as a significant investment in generative AI. However, thirteen months down the line, Amgen's staff have shifted their preferences towards OpenAI’s ChatGPT, raising concerns for Microsoft.

The unexpected preference for ChatGPT over Microsoft's product illustrates the competitive landscape in the AI industry, despite the substantial partnership and investment that Microsoft has with OpenAI. This trend is highlighting ChatGPT’s growing popularity and usability in enterprise environments, a development that might prompt Microsoft to rethink its deployment strategies or further enhance its AI offerings to better resonate with corporate needs.

As Microsoft navigates these competitive waters, it seems its AI ambitions face an uphill battle against the rapidly advancing presence of ChatGPT in the workplace. Microsoft's struggle underscores how nimble AI solutions can sway users, potentially upending even the most robust corporate alliances.

The discussion highlights several criticisms of Microsoft's Copilot AI, particularly in comparison to OpenAI's ChatGPT:  

### **Key Issues with Copilot**  
1. **Poor Response Quality**: Users report frustration with Copilot's unhelpful or nonsensical answers, especially for technical tasks (e.g., generating `ffmpeg` commands). It often provides irrelevant Python scripts instead of direct solutions, leading to wasted time.  
2. **Model Limitations**: Copilot may rely on cheaper, less capable AI models to reduce costs, while ChatGPT offers access to advanced models like GPT-4 for complex reasoning and coding. Users criticize the lack of transparency in model selection.  
3. **Unpredictable Outputs**: Responses are seen as inconsistent or "nondeterministic," making reliability a concern. This unpredictability erodes trust, akin to relying on a "I’m Feeling Lucky" Google search button.  
4. **User Experience (UX) Challenges**: Copilot’s interface and integration lack intuitive design, forcing users to wrestle with context management and unclear workflows.  

### **Comparisons to ChatGPT and Alternatives**  
- ChatGPT is praised for its advanced reasoning, clearer model options (e.g., GPT-4o for coding), and reliability.  
- Alternatives like Claude, OpenRouter, or Cursor are noted for better model routing, cost optimization, and transparency.  

### **Enterprise Implications**  
- Companies investing in Copilot face employee dissatisfaction when staff prefer ChatGPT, undermining Microsoft’s value proposition.  
- Users emphasize the need for deterministic outputs, transparent model selection, and simplified UX to compete with ChatGPT’s popularity.  

### **Broader Skepticism Toward AI Tools**  
- Discussions reflect “AI disillusionment”: Users grow impatient with tools that overpromise and underdeliver, emphasizing that minor inconveniences (e.g., requiring retries) sour adoption.  
- Some argue AI assistants need stricter quality control to avoid "hallucinations" and better align with practical workflows.  

### **Conclusion**  
Microsoft’s Copilot struggles with technical limitations, opaque model choices, and UX flaws, while ChatGPT’s superior performance and flexibility continue to dominate enterprise preferences. To regain trust, Microsoft must address reliability, transparency, and user-centric design.

### XBOW, an autonomous penetration tester, has reached the top spot on HackerOne

#### [Submission URL](https://xbow.com/blog/top-1-how-xbow-did-it/) | 271 points | by [summarity](https://news.ycombinator.com/user?id=summarity) | [118 comments](https://news.ycombinator.com/item?id=44367548)

In a groundbreaking achievement for cybersecurity, an autonomous AI-driven penetration tester called XBOW has secured the top spot on the US leaderboard for bug bounties. Spearheaded by Nico Waisman, Head of Security, this marks a significant milestone in bug bounty history, as XBOW becomes the first AI to reach such heights on the platform HackerOne.

The journey to this accolade began with rigorous benchmarking. Initially, XBOW was tested using established Capture The Flag (CTF) challenges from providers like PortSwigger and Pentesterlab. However, understanding the need for real-world relevance, the team developed a unique benchmark to simulate scenarios not typically trained on existing language models. Following promising results from these controlled exercises, XBOW pivoted to identifying zero-day vulnerabilities within open source projects.

To truly put XBOW's capabilities to the test, the team entered the realm of black-box testing. By participating in various bug bounty programs on HackerOne, XBOW had to operate like any human researcher—without shortcuts and relying solely on its programming. This immersion allowed XBOW to climb the ranks on HackerOne, competing against a vast array of human pentesters.

One of the biggest challenges was scaling XBOW's operations to handle the immense variety found in real-world environments, ranging from advanced new technologies to outdated legacy systems. Not only did XBOW need to scan multiple targets efficiently, but it also had to sift through massive data to identify high-value targets. The solution involved creating an infrastructure around XBOW that assessed the potential value of different targets, using a scoring system that evaluated various signals including the presence of security frameworks, accessibility of endpoints, and authentication mechanisms.

A distinguishing factor in XBOW’s approach was its focus on reducing false positives, a common pitfall in automated vulnerability scanning. By implementing automated peer reviewers, or validators, XBOW enhanced its precision in vulnerability detection. These validators performed technical checks to confirm the existence of security issues, ensuring that only legitimate vulnerabilities were reported.

By operating across numerous bug bounty programs, XBOW consistently identified validated vulnerabilities, garnering trust and recognition among companies utilizing HackerOne. Its ability to uncover genuine security threats, especially in high-profile targets, highlights the efficacy of AI in cybersecurity. As a public signal of its success, XBOW's rapid ascent to the leaderboard alongside thousands of human researchers exemplifies the growing potential of autonomous systems in the field of penetration testing.

**Hacker News Discussion Summary:**

The discussion around the AI-driven penetration tester XBOW's rise to the top of HackerOne's leaderboard highlights a mix of skepticism, curiosity, and acknowledgment of its potential impact on cybersecurity. Here are the key points debated:

### **Skepticism and Critique**
- **Marketing vs. Substance**: Some users dismissed XBOW’s achievement as a marketing gimmick, arguing that top cybersecurity talent focuses on high-value, complex vulnerabilities rather than low-hanging fruit (e.g., `hntrlnds`). Critics noted that many bug bounty programs, like Disney's or AT&T's, offer limited payouts, attracting fewer experts.
- **False Positives**: While XBOW’s use of automated validators to reduce false positives was praised, skeptics questioned whether these checks fully replace human verification. One user (`h`) argued that manual reviews remain critical for validating technical findings like Cross-Site Scripting (XSS).

### **Technical Insights**
- **Validation Process**: Supporters highlighted XBOW’s infrastructure, which combines AI-generated findings with programmatic checks (e.g., simulating browser visits to confirm XSS payload execution). This approach draws from research like Brendan Dolan-Gavitt’s work on AI-driven security agents.
- **Leaderboard Legitimacy**: Users confirmed XBOW’s #1 ranking on HackerOne’s US leaderboard but debated whether its submissions were "gaming the system." Some (`tclndr`) raised ethical concerns about AI-generated reports bypassing human effort.

### **Market Dynamics**
- **Bug Bounty Economics**: Many criticized the bug bounty ecosystem’s incentives, noting that programs often underpay researchers or prioritize metrics like CVSS scores over real-world impact (`monster_truck`, `ackbar03`). Others argued that XBOW’s efficiency could democratize access to bug hunting, particularly in underserved regions.
- **Human vs. AI Roles**: While some feared AI might devalue human researchers, most agreed it would augment, not replace, human expertise (`Sytten`, `vmyrl`). Predictions leaned toward AI handling tedious tasks (e.g., scanning legacy systems) while humans focus on creative exploitation techniques.

### **Broader Implications**
- **Cybersecurity’s Future**: References to William Gibson’s *Burning Chrome* and ongoing projects like PentestGPT underscored excitement for AI’s role in advancing security tools. However, skepticism lingered about AI’s ability to navigate nuanced, high-stakes vulnerabilities without human oversight.
- **Anecdotal Success**: A user (`mrtnld`) shared how AI-assisted testing quickly identified a denial-of-service (DoS) vulnerability, emphasizing its potential for rapid detection in legacy systems.

### **Ethics and Fairness**
- **Automated Submissions**: Discussions surfaced around HackerOne’s policies allowing AI tools, provided findings undergo human review (`ksbrg`). Critics argued companies might exploit AI to flood programs with low-effort reports, straining triage teams.

### **Conclusion**
The debate reflects a nuanced view of XBOW’s milestone: recognition of its technical achievements alongside calls for transparency, ethical use, and balanced integration with human expertise. As AI tools evolve, their role in cybersecurity will likely hinge on collaboration—pairing machine efficiency with human ingenuity to address evolving threats.

### Retrieval Augmented Generation Based on SQLite

#### [Submission URL](https://github.com/ggozad/haiku.rag) | 82 points | by [emzo](https://news.ycombinator.com/user?id=emzo) | [10 comments](https://news.ycombinator.com/item?id=44364216)

Hey Hacker News enthusiasts, your daily digest has arrived! Today, we're spotlighting an intriguing GitHub project that could revolutionize the way you handle documents and search functionalities. Introducing **Haiku.rag**, a Retrieval-Augmented Generation (RAG) library leveraging the ubiquitous SQLite database to manage and search through documents efficiently.

### Why is Haiku.rag Worth Your Attention?

1. **Local SQLite Powerhouse**: Forget about complex server setups—Haiku.rag harnesses the simplicity and reliability of local SQLite databases, making it a breeze for developers to integrate this into their workflows without additional server overhead.

2. **Diverse Embedding Providers**: Customize your document embeddings with providers like Ollama, VoyageAI, or OpenAI, or even hook into your own systems. This flexibility allows you to optimize for various use cases.

3. **Hybrid Search Capabilities**: Unleash powerful search functions combining vector-based and full-text search methods. This hybrid approach can significantly enhance search accuracy and relevance using the SQLite vector extension and FTS5.

4. **Comprehensive Format Support**: With the ability to parse over 40 file formats, including PDFs, DOCX, HTML, and more, Haiku.rag ensures that no document is left behind.

5. **Dynamic Management & Monitoring**: Setting up file monitoring and using the MCP server, Haiku.rag can automatically index and manage documents as they are added, modified, or deleted from your directories.

6. **Accessible via CLI and Python**: Whether you prefer a command-line interface or want to integrate functionalities directly into your Python applications, Haiku.rag offers robust options.

This tool has been brewing some significant interest, with over 200 stars on GitHub already. It's ideal for developers looking to enhance document handling and retrieval in their apps without the hassle of dealing with multiple backend services. For anyone intrigued by the seamless blend of document management and cutting-edge search technology, Haiku.rag is definitely worth exploring. Go ahead, fork it, and give it a test drive!

**Summary of Discussion:**

The discussion around **Haiku.rag** highlights enthusiasm for its hybrid search approach, but also raises technical considerations:  

1. **Hybrid Search Praise**: Users applaud the combination of vector and full-text search within SQLite, noting its potential to improve relevance and power for RAG applications. The integration of SQLite's FTS5 and vector extensions (like `sqlite-vctrs`) is seen as clever and practical for local workflows.  

2. **Comparison to Existing Tools**: Some mention alternatives like **Lucene** for full-text search and retrieval, suggesting that highly customizable document tokenization and indexing (as in Lucene) could complement RAG use cases. Others debate SQLite’s scalability for vector search and reference extensions like `sqlite-vss` for vector similarity.  

3. **RAG Applications & LLMs**: Commenters clarify that RAG systems allow LLMs to dynamically retrieve contextual documents from databases, avoiding the need for fine-tuning on static datasets. This makes Haiku.rag’s local-first approach appealing for projects prioritizing privacy or simplicity.  

4. **SQLite Limitations**: Concerns are raised about SQLite’s capabilities compared to dedicated vector databases, though proponents argue that lightweight extensions (e.g., `sqlite-vctrs`) make it viable for many use cases, especially in smaller-scale applications.  

The conversation reflects excitement for Haiku.rag’s vision but underscores the importance of balancing flexibility, scalability, and ease of use. Resources like [Alex Garcia’s blog](https://alexgarcia.xyz/blog/2024/sqlite-vc-hybrid-srch/) are shared for deeper insights into hybrid SQLite implementations.

### Gemini Robotics On-Device brings AI to local robotic devices

#### [Submission URL](https://deepmind.google/discover/blog/gemini-robotics-on-device-brings-ai-to-local-robotic-devices/) | 209 points | by [meetpateltech](https://news.ycombinator.com/user?id=meetpateltech) | [84 comments](https://news.ycombinator.com/item?id=44366409)

Today marks a significant stride in robotics with the launch of Gemini Robotics On-Device, a cutting-edge AI model designed to operate locally on robotic devices. Following the debut of Gemini Robotics in March, this on-device upgrade offers robust capabilities in dexterity and task generalization, all while maintaining efficiency that doesn't rely on constant data network access. This makes it especially useful in scenarios with latency sensitivities or poor connectivity.

Gemini Robotics On-Device not only functions independently but excels in understanding and executing complex, multi-step tasks based on natural language instructions. Think opening a zipper or assembling delicate components, all happening directly through the robot's own cognitive framework.

For developers keen to push these capabilities even further, Gemini Robotics is offering an SDK. This toolkit empowers them to experiment with and fine-tune the model for specific tasks. The SDK makes it easy to integrate the model into various environments, demonstrating the model’s adaptability with just 50 to 100 task demonstrations. Interestingly, even though it was initially calibrated for ALOHA robots, it smoothly adapts to different robot types like the bi-arm Franka or the humanoid Apollo.

Safety and responsible development remain a top priority, with measures in place to ensure semantic and physical safety. The Responsible Development & Innovation team is actively working on minimizing any potential risks while maximizing societal benefits.

This innovation in on-device AI accelerates robotics evolution, potentially transforming how robots engage with the world around them. Developers eager to explore these advancements can apply for the Gemini Robotics trusted tester program to unlock access to both the model and its SDK. With this release, Gemini Robotics On-Device is poised to tackle the pressing challenges of robotics, offering a futuristic glimpse into more agile and self-reliant robots.

The Hacker News discussion around Gemini Robotics On-Device revolves around several technical and practical concerns, with a focus on **reliability**, **costs**, and **model architecture**:

### Key Themes:
1. **Reliability Skepticism**:
   - Users question whether humanoid robots can match the reliability of **industrial robots** (e.g., Cincinnati Millicron), which are optimized for durability (100,000+ hours MTBF) and operate in controlled environments. Industrial robots use high-quality parts (e.g., retry logic, precision machining) and are built for repetitive tasks.
   - Concerns arise about **failure rates** for humanoid robots with many motors. A calculation suggests 43 motors (common in humanoids) with a 1% annual failure rate per motor would lead to a 73% failure rate over 3 years. Critics argue industrial robots achieve reliability through fewer motors, robust components, and controlled working conditions (dust-free, stable temperatures).

2. **Cost and Maintenance**:
   - Actuators, sensors, and replacement parts (e.g., motors) are noted as expensive. Total costs extend beyond hardware to include labor, energy, and environmental factors (e.g., mining resources, supply chains).
   - Debate over whether **modularity** (swappable parts) or **redundancy** (multiple fingers/sensors) would address reliability, with some arguing redundancy introduces complexity.

3. **Environmental and Design Challenges**:
   - Humanoid robots face harsher environments (dust, moisture, physical impacts) compared to industrial robots in sterile factories. Dust contamination, unexpected collisions, and temperature fluctuations pose design challenges.
   - Users highlight that industrial robots are often paired with **ancillary systems** (e.g., splash guards, dust collection) to mitigate these issues, which humanoids may lack.

4. **Hardware and SDK**:
   - The SDK supports NVIDIA Jetson Orin hardware (8GB–64GB variants), with some speculating about TPU compatibility. Users link to **MuJoCo simulations** ([GitHub](https://github.com/google-deepmind/mujoco_menagerie)) for robot modeling, showing interest in testing adaptability.

5. **Model Architecture**:
   - Speculation that Gemini Robotics uses a **Vision-Language-Action (VLA)** model built on Gemini 2.0, optimized for multimodal tasks. Variants like OpenVLA (based on Llama2) and smolVLA (smaller, task-specific models) are mentioned. Some users reference frameworks like **LeRobot** for integration.

### Notable Skepticism:
- Users remain doubtful that humanoid robots can achieve the same reliability or cost-efficiency as industrial robots, citing mechanical complexity, environmental factors, and unsustainable costs (e.g., maintenance, resource extraction). The discussion underscores a divide between aspirational robotics and current industrial practicality.

Overall, the thread blends excitement about Gemini’s technical advancements with pragmatic concerns about real-world deployment and scalability.

### A federal judge sides with Anthropic in lawsuit over training AI on books

#### [Submission URL](https://techcrunch.com/2025/06/24/a-federal-judge-sides-with-anthropic-in-lawsuit-over-training-ai-on-books-without-authors-permission/) | 164 points | by [moose44](https://news.ycombinator.com/user?id=moose44) | [189 comments](https://news.ycombinator.com/item?id=44367850)

In a landmark legal decision that could reshape the interaction between technology and copyright law, federal judge William Alsup has sided with AI company Anthropic in a lawsuit concerning the use of copyrighted books to train AI models. This ruling legally sanctions Anthropic’s use of published books for AI training without the authors’ explicit permissions, marking a pivotal moment for the application of fair use doctrine in the burgeoning world of generative AI.

The decision, unprecedented in nature, suggests that AI companies may leverage the fair use doctrine, potentially paving the way for similar outcomes in lawsuits against other tech giants like OpenAI, Meta, and Google. The intricacies of fair use—still defined by laws from 1976—consider if a work's use is transformative, educational, or commercial, often leaving room for varied judicial interpretations. Alsup’s ruling could thus serve as a guiding precedent for future litigation.

However, this victory for Anthropic isn't without its caveats. The ongoing trial will address Anthropic's controversial establishment of a “central library” compiled from pirated books. Judge Alsup allowed fair use solely for training purposes but noted that the company might still face repercussions for obtaining these works illegally. The outcome could significantly determine the scope of statutory damages Anthropic might face, as the company’s subsequent purchase of legal copies doesn’t absolve its initial copyright violations.

This judicial decision arrives amidst a wave of disputes between tech companies and creatives—authors, artists, and publishers—seeking to protect their intellectual properties in an increasingly digital age. As the courts continue to navigate these uncharted waters, the balance between technological advancement and the preservation of creators’ rights remains a formidable legal battleground. 

For those navigating the tech landscape's current events, Anthropic’s court victory signals critical legal support for AI training practices, while underscoring complex challenges around copyright in digital innovation. With upcoming trials and ongoing debates, the future of AI’s relationship with copyrighted content is likely to remain a contentious and closely watched legal saga.

The Hacker News discussion on the Anthropic copyright ruling reveals several key debates and perspectives:

### **1. Legal Precedents and Fair Use**  
- **Models vs. Derivative Works**: Users reference cases like *Kadrey v. Meta Platforms*, where courts dismissed claims that LLMs themselves constitute derivative works. Judge Alsup’s ruling reinforces this, suggesting AI training falls under fair use if transformative.  
- **Distributing Models**: Concerns arise about open-weight models (e.g., Llama) potentially infringing if they can reproduce copyrighted text. The outcome may hinge on whether model weights are seen as containing compressed copies of source material.

### **2. Technical Feasibility of Memorization**  
- **Partial vs. Full Reproduction**: A study on Llama’s ability to memorize *Harry Potter* showed it could generate 50-token snippets but diverged from the original text. Some argue even partial reproduction might infringe, while others stress the probabilistic, non-deterministic nature of LLMs makes exact replication unlikely.  
- **Server-Side vs. Client-Side Risk**: Comparisons to Google Books’ snippet-based fair use highlight differences in control. If users can extract verbatim text from models (client-side), infringement risks increase, unlike server-controlled access.

### **3. Copyright and Training Data Sourcing**  
- **Pirated vs. Licensed Data**: While the ruling greenlights training on copyrighted works, Anthropic’s use of a “pirated library” remains contentious. Legally purchasing books later may not absolve initial infringement, impacting statutory damages.  
- **Economic Centralization**: The high cost of legally licensing training data could entrench AI development within well-funded corporations, raising concerns about monopolization.

### **4. Comparisons and Analogies**  
- **Cliff Notes vs. LLMs**: Users debate whether LLMs’ summarization is analogous to non-infringing study guides or closer to infringing reproductions. The line between transformative synthesis and verbatim copying remains blurry.  
- **NYT v. OpenAI**: The discussion contrasts challenges in reproducing news articles (NYT’s case) versus books, noting news content’s shorter form and higher factual density may complicate fair use defenses.

### **5. Broader Implications**  
- **Legal Uncertainty**: Many call for updated copyright frameworks to address AI-specific issues, such as whether model weights constitute infringement or how to handle “stochastic compression” of data.  
- **Ecosystem Impact**: Some worry the ruling disincentivizes creators, while others argue overly restrictive laws could stifle AI innovation. The balance between creator rights and technological progress remains unresolved.

### **Key Takeaways**  
The community is split:  
- **Optimists** view LLMs as transformative tools under fair use, akin to search engines or study guides.  
- **Skeptics** warn of loopholes enabling infringement, especially if models can regurgitate content or rely on illegally sourced data.  
- **Neutral Observers** stress the need for clearer legal standards and technical safeguards (e.g., filtering) to navigate this uncharted terrain.  

The ruling is seen as a tentative win for AI development, but ongoing lawsuits and technical advancements will likely shape the final legal landscape.

### Bridging Cinematic Principles and Generative AI for Automated Film Generation

#### [Submission URL](https://arxiv.org/abs/2506.18899) | 29 points | by [jag729](https://news.ycombinator.com/user?id=jag729) | [7 comments](https://news.ycombinator.com/item?id=44369587)

In a groundbreaking development at the intersection of cinema and technology, researchers have unveiled FilMaster, a novel AI system that brings cinematic flair to automated film generation. Traditional AI film models often fall short, producing generic, uninspired visuals that neglect the art of storytelling through camera and rhythm. FilMaster aims to rewrite this narrative by integrating core cinematic principles with generative AI to produce films that not only adhere to professional standards but are also highly editable and suitable for industry use.

FilMaster is built around two main stages: a Reference-Guided Generation Stage and a Generative Post-Production Stage. In the first stage, AI algorithms transform user inputs into video clips by leveraging a rich archive of 440,000 film clips to guide camera language, ensuring authentic storytelling techniques. The second stage refines raw footage into polished films, subtly integrating audio and visual elements in line with feedback simulated from audience reactions, leaving no stone unturned in achieving the desired cinematic rhythm.

Pioneering a Multi-shot Synergized RAG Camera Language Design module and an Audience-Centric Cinematic Rhythm Control module, the system embodies the essence of film editing through techniques like Rough Cut and Fine Cut processes. By emulating professional workflows and responding dynamically to simulated audience feedback, FilMaster stands poised to make significant strides in the realm of automated, AI-driven filmmaking. 

To cap it all, the introduction of the FilmEval benchmark enhances the ability to gauge film quality, assessing aspects like camera language and narrative rhythm. The creators of FilMaster anticipate that their system could revolutionize content creation, setting ambitious benchmarks for generative AI's role in the film industry.

**Discussion Summary:**  
The discussion revolves around FilMaster's demo, technical issues, comparisons to existing tools, and arXiv submission nuances:  

- **Access & Technical Issues**: A user reported trouble playing the demo video, requiring repeated attempts. The showcased footage (e.g., a cyber truck clip) was noted to appear remarkably realistic.  

- **Cinematography & AI Quality**: The system’s cinematic language and shot design were praised as innovative, with one commenter contrasting FilMaster's quality against AI models like Veo 3.  

- **arXiv Submission Peculiarities**: Questions arose about the paper’s arXiv history, with attention drawn to its multi-month submission timeline—uncommon for typical academic workflows.  

- **Author Ambiguity**: A subthread debated the commonality of "Huang" as a surname, complicating searches for the lead author. Suggestions included refining arXiv searches with precise names, which returned limited results.  

Overall, the conversation highlighted curiosity about FilMaster’s technical merits, hurdles in accessing or verifying its documentation, and broader implications for AI-driven cinematography.

### LLMs bring new nature of abstraction – up and sideways

#### [Submission URL](https://martinfowler.com/articles/2025-nature-abstraction.html) | 11 points | by [tudorizer](https://news.ycombinator.com/user?id=tudorizer) | [4 comments](https://news.ycombinator.com/item?id=44366904)

Martin Fowler, a prominent voice in software development, has shared his insights on how generative AI and Large Language Models (LLMs) are transforming the landscape of programming. Drawing parallels to the seismic shift from assembler to high-level programming languages, Fowler suggests that LLMs are introducing an equally radical change, not merely raising abstraction levels but redefining the very essence of programming with their non-deterministic nature.

In the early days, moving from assembler to high-level languages like Fortran was revolutionary: programmers could finally conceptualize programs using conditionals and iterations, using meaningful names rather than direct machine instructions. While languages have advanced significantly since then, Fowler notes, the core way of interacting with machines remained consistent—until now.

Fowler likens today's leap to how Fortran differed from assembler, as generative AI shifts us from coding to prompting. This transition is more than a leap in abstraction; it's a move into the realm of non-determinism, where the outcome isn't guaranteed to be the same with each prompt—a stark contrast to the predictable, bug-consistent results of traditional code. As developers begin to harness the capabilities of LLMs, they must learn to navigate this unpredictability, offering a challenge but also potential that is yet to be fully understood.

This evolution presents a mixture of excitement and uncertainty for Fowler. While it introduces complexities, such as the inability to rely on traditional version control systems to reproduce results reliably, it also opens new avenues for creativity and problem-solving. As we stand at the cusp of this new paradigm, Fowler embraces the thrilling potential of what lies ahead, acknowledging both the forthcoming challenges and the opportunities to discover entirely new aspects of programming.

**Summary of Discussion:**

The discussion around Martin Fowler's perspective on generative AI and LLMs highlights a mix of skepticism, challenges, and cautious optimism. Key points include:  
1. **Shift to Non-Determinism**: Users note that LLMs introduce a "sideways" leap in programming by producing probabilistic, non-deterministic outputs, unlike traditional deterministic code. This unpredictability complicates reproducibility and integration into systems reliant on consistency.  
2. **Practical Challenges**: Commenters emphasize the difficulty of integrating LLM-generated outcomes into deterministic workflows (e.g., business rules, testing), requiring mindset shifts and new problem-solving approaches. Unpredictable outputs may create downstream risks, raising adoption barriers for mainstream enterprises.  
3. **Hype vs. Reality**: While LLMs boost productivity for specific tasks, their mainstream business use faces hurdles. Some argue developers and businesses underestimate the effort needed to achieve reliable returns, with non-determinism posing a "bigger problem" than anticipated.  
4. **Skepticism on Impact**: One user dismisses current AI coding tools as insufficiently transformative, urging Fowler to address the real-world challenges developers face.  

Overall, the thread reflects enthusiasm for LLMs' potential but stresses the complexity of navigating their limitations, particularly in deterministic environments.

### The Résumé is dying, and AI is holding the smoking gun

#### [Submission URL](https://arstechnica.com/ai/2025/06/the-resume-is-dying-and-ai-is-holding-the-smoking-gun/) | 34 points | by [pseudolus](https://news.ycombinator.com/user?id=pseudolus) | [20 comments](https://news.ycombinator.com/item?id=44369770)

In the modern age of artificial intelligence, the hiring process has turned into a chaotic battleground—with technology both the hero and the villain. As AI-generated job applications flood platforms like LinkedIn, where submissions have surged to an astonishing 11,000 per minute, employers are drowning in what has been aptly dubbed "hiring slop."

The New York Times highlights the plight of HR professionals like Katie Tanner, who was overwhelmed by over 1,200 applications for a single role, forcing her to pull the listing entirely. This narrative is common in an era where tools like ChatGPT effortlessly populate résumés with job-specific keywords, making it difficult for employers to distinguish between genuinely interested candidates and automated submissions.

AI's role in the hiring upheaval began in 2022, initially as a means to assist job seekers, but has since evolved into a systemic disruption. Some candidates have taken automation further by hiring AI to autonomously hunt for jobs and submit applications in bulk on their behalf. This technological arms race has recruiters arming themselves with AI tools to sift through the deluge, with companies like Chipotle reporting significant reductions in hiring time thanks to AI-based screenings.

Despite these technological advancements, the battle rages on. The inherent biases within AI systems have ignited concerns about discrimination, aligning with the European Union's AI regulations that flag hiring as high-risk. In the U.S., while specific AI hiring laws are absent, existing anti-discrimination laws still apply.

The future of hiring could pivot away from résumés entirely, perhaps leaning towards evaluation methods AI cannot easily replicate—like live problem-solving or trial work. The current system, rife with potential fraud and ever-spiraling automation, paints a picture where human connections in recruitment feel increasingly ersatz. The dream, it seems, is a world where we humans watch as robots handle jobs meant for other robots, leaving us time for leisurely pursuits. But until that dream unfolds, AI in hiring remains both a conundrum and a companion in our search for the perfect candidate.

The Hacker News discussion on AI's role in hiring reflects frustration with the current system and debates potential solutions. Key points include:

1. **Overwhelm and Redundancy**:  
   Users highlight the inefficiency of AI-generated applications flooding employers, leading to "hiring slop." Submissions are often redundant, with applicants forced to re-enter data already on LinkedIn or résumés. This wastes time for both candidates and employers, mirroring the article’s concerns about a broken system.

2. **Resumes vs. Alternatives**:  
   - Some argue résumés are outdated and propose replacing them with LinkedIn profiles, standardized APIs, or live problem-solving tasks.  
   - Others defend résumés as necessary for background context, especially when LinkedIn profiles lack details due to NDAs or incomplete updates.  

3. **Interviews and Human Judgment**:  
   Many emphasize interviews as critical for assessing candidates, suggesting résumés alone are insufficient. The discussion leans toward hybrid approaches: using AI to filter initial applications but relying on human evaluation for final decisions.

4. **Privacy and Data Concerns**:  
   Skepticism exists about platforms like LinkedIn harvesting data for AI training. One user mentions deleting LinkedIn to avoid this, reflecting broader distrust in tech platforms.

5. **Solution Proposals**:  
   - Standardized APIs to streamline application data.  
   - Reducing redundant form-filling by auto-pulling LinkedIn data.  
   - Prioritizing networking and personal referrals to cut through algorithmic noise.  

The thread aligns with the article's view of AI as both a disruptor and a tool for efficiency, while underscoring the need for systemic changes to balance automation with meaningful human interaction in hiring.

### Containers are available in public beta for simple, and programmable compute

#### [Submission URL](https://blog.cloudflare.com/containers-are-available-in-public-beta-for-simple-global-and-programmable/) | 74 points | by [rita3ko](https://news.ycombinator.com/user?id=rita3ko) | [18 comments](https://news.ycombinator.com/item?id=44367693)

In an exciting development from Cloudflare, Containers have now entered public beta for users on paid plans, unlocking the potential to run a wider array of applications alongside Workers. These Containers provide a versatile, global, and easily programmable compute solution, seamlessly integrating with Cloudflare's developer platform. Whether it's for media processing at the edge, multi-language backend services, or CLI batch tools, Containers are poised to handle diverse workloads.

The workflow is straightforward: define a few lines of code for a Container and deploy it globally with the command `wrangler deploy`. Containers offer the flexibility of choosing the right tool for different tasks, enabling routing between lightweight, scalable Workers and more robust Container instances. Being programmable, they can spin up on-demand and interact with Workers, allowing you to use custom logic with simple JavaScript.

A practical example is using Containers for code sandboxing, where each user gets an isolated environment. With Cloudflare’s global network, Containers are deployed closer to users for faster setup, simplifying the process while ensuring quick scaling and routing without manual intervention.

Development is user-friendly with `wrangler dev`, allowing easy iterations of container code. It supports image configurations from Dockerfiles, facilitating seamless development alongside Worker code. When ready for production, a simple `wrangler deploy` ensures global provisioning.

Observability is a key feature, providing insights into container performance and usage through Cloudflare’s dashboard. Metrics and logs are easily accessible, ensuring you can monitor and manage your deployments effectively.

This new capability opens up myriad possibilities, from running complex libraries like FFmpeg for video conversion to deploying containerized backends or integrating cron jobs. Cloudflare's move to introduce Containers in this way signifies a big step towards making their platform a one-stop solution for developers seeking to run entire applications globally with enhanced flexibility and power. Eager to try it? You can start experimenting right away with available documentation and example Workers to get your Containers up and running.

The discussion around Cloudflare's Containers entering public beta revolves around **pricing, use cases, and comparisons with competitors**, alongside technical queries:

1. **Cost Concerns**:  
   - Users debate whether on-demand pricing ($55/month for a hypothetical non-stop instance) is expensive for small/hobby projects, but others clarify that costs scale with usage (e.g., containers only incur charges when active).  
   - Comparisons are drawn to alternatives like Fly Machines ($31/month for similar specs) and Rivet Containers ($29.40/month), with Cloudflare viewed as pricier but competitive for specific features.  
   - Concerns about egress costs ($25/TB) and potential hidden expenses for bandwidth-heavy applications.

2. **Use Case Viability**:  
   - Supporters highlight **serverless scaling** (zero cost when idle) as ideal for bursty or low-traffic workloads, while critics argue sustained traffic (even 1 request/second) could become costly.  
   - Examples include media processing (FFmpeg), CLI tools, and distributed web services paired with Workers.  

3. **Technical Queries**:  
   - Limited **UDP support** (only TCP for now, with UDP planned via Workers integration) and DNS functionality questions.  
   - Integration with Cloudflare’s ecosystem (Workers, Durable Objects) and edge deployment advantages.  

4. **Competitor Comparisons**:  
   - Fly Machines and Rivet Containers are noted for lower prices or specialized features. Modal is suggested as a cheaper serverless compute alternative.  

5. **Resources Shared**:  
   - A [blog post](https://rivet.gg/blog/2025-06-24-cloudflare-containers-vs-rivet) comparing Cloudflare with Rivet and a [YouTube tutorial](https://youtu.be/oyOaxMY4eNo) from Cloudflare were linked.  

Overall, feedback is mixed: excitement for Cloudflare’s expanded capabilities balances skepticism about cost efficiency for certain workloads. The serverless model is praised for scalability but scrutinized for unpredictable expenses under sustained demand.

