## AI Submissions for Thu Dec 07 2023 {{ 'date': '2023-12-07T17:10:34.142Z' }}

### Meta's new AI image generator was trained on 1.1B Instagram and FB photos

#### [Submission URL](https://arstechnica.com/information-technology/2023/12/metas-new-ai-image-generator-was-trained-on-1-1-billion-instagram-and-facebook-photos/) | 325 points | by [my12parsecs](https://news.ycombinator.com/user?id=my12parsecs) | [202 comments](https://news.ycombinator.com/item?id=38557054)

Meta, the parent company of Facebook, has released a free standalone AI image-generator website called "Imagine with Meta AI." The website is based on Meta's Emu image-synthesis model, which can generate unique images from written prompts. The AI model was trained using 1.1 billion publicly available images from Facebook and Instagram. Previously, this technology was only available in messaging and social networking apps like Instagram. Users can now access the image generator on the "Imagine with Meta AI" website. The AI-generated images have been described as aesthetically novel, with the model being able to handle complex prompts and create photorealistic images relatively well. However, it doesn't perform well in text rendering and other media outputs. Overall, the AI image synthesis seems to be average compared to similar models.

The discussion on the submission revolves around the legal implications of using Meta's AI image generator and whether it infringes copyrights. Some users mention that Meta's terms of service grant them a non-exclusive, royalty-free license to host and distribute the generated images. Others argue that the AI model does not explicitly violate copyright as it does not reproduce exact copies of copyrighted material. 

There is also a discussion about the limitations of AI image synthesis and its ability to be considered creative or copyrightable. Some users point out that current AI models lack the ability to produce substantial creative expression and therefore may not be subject to copyright protection. However, others argue that copyright can be infringed if the AI generates images that closely resemble copyrighted material.

Additionally, there are discussions about the complexity of copyright law and the interpretation of certain provisions. Some users mention the Digital Millennium Copyright Act (DMCA) and its safe harbor provisions, while others highlight the criteria for copyrightability and the level of creativity required for a work to be protected.

Overall, the discussion explores the legal aspects and technical limitations of AI image synthesis in relation to copyright law.

### Purple Llama: Towards open trust and safety in generative AI

#### [Submission URL](https://ai.meta.com/blog/purple-llama-open-trust-safety-generative-ai/) | 332 points | by [amrrs](https://news.ycombinator.com/user?id=amrrs) | [302 comments](https://news.ycombinator.com/item?id=38556771)

Purple Llama, an umbrella project aimed at promoting open trust and safety in the realm of generative AI, has been announced. The project will provide tools and evaluations to help developers responsibly deploy and use generative AI models. As a first step, Purple Llama is releasing CyberSec Eval, a set of cybersecurity safety evaluations benchmarks for LLMs (large language models), and Llama Guard, a safety classifier for input/output filtering. Purple Llama is also partnering with organizations like AI Alliance, AMD, AWS, Google Cloud, and Microsoft to improve and make these tools available to the open-source community. By promoting collaboration on safety and responsible AI, Purple Llama aims to build trust in the developers driving innovation in generative AI.

The discussion on Hacker News revolves around the topic of prompt injection and the potential risks associated with it. Some users express concerns about the security vulnerabilities of large language models (LLMs) and the need for safeguards against prompt injection attacks. Others argue that prompt injection is not a significant concern and that LLMs have limitations in terms of their ability to generate malicious content. 

There is a debate about the effectiveness of prompt injection as a security threat and whether it is a valid concern in real-world LLM applications. Some users point out that prompt injection is only a limited risk and can be addressed by implementing relatively simple techniques. Others highlight the potential dangers of prompt injection, such as the leakage of private data or the manipulation of privileged server information.

The discussion also touches on the challenges of implementing LLMs and the importance of trust and reliability in these models. Some users express skepticism about the trustworthiness of LLMs and argue that human-like expertise and stochastic information production are crucial for generating trustworthy content. There are also mentions of potential solutions, such as using validation benchmarks or rejecting requests that contain subversive instructions.

The debate extends to the topic of corporate responsibility and the risks associated with giving LLMs access to databases. Some users argue that granting LLMs write access to databases could lead to the overwriting of critical data or the exploitation of customer information. Others emphasize the importance of considering the risks and ensuring the robustness and reliability of LLM applications.

Overall, the discussion on Hacker News highlights the varying perspectives on prompt injection and the need for responsible deployment and use of LLMs to address potential security and trust issues.

### Brain-Inspired Efficient Pruning: Criticality in Spiking Neural Networks

#### [Submission URL](https://arxiv.org/abs/2311.16141) | 52 points | by [PaulHoule](https://news.ycombinator.com/user?id=PaulHoule) | [13 comments](https://news.ycombinator.com/item?id=38552186)

A new research paper titled "Brain-Inspired Efficient Pruning: Exploiting Criticality in Spiking Neural Networks" has been submitted to arXiv. The authors, Shuo Chen and his colleagues, discuss the challenges of pruning deep Spiking Neural Networks (SNNs) due to the binary and non-differentiable nature of spike signals. Pruning is an important technique for reducing the computational and storage requirements of SNNs, making them more suitable for deployment on devices with limited resources. However, existing pruning methods for SNNs often require high time overhead to make pruning decisions. 

To address this issue, the authors propose a regeneration mechanism based on criticality, inspired by the critical brain hypothesis in neuroscience. They introduce a low-cost metric for the criticality of pruning structures and rerank the pruned structures based on their criticality. By regenerating the structures with higher criticality, they are able to obtain more efficient pruned networks. The authors evaluate their method using popular deep neural network architectures, VGG-16 and ResNet-19, for both unstructured and structured pruning. Their method achieves higher performance compared to the current state-of-the-art methods with the same time overhead. Furthermore, they achieve comparable performances, and even better results on VGG-16, compared to the state-of-the-art methods with 11.3x and 15.5x acceleration.

The authors also investigate the underlying mechanism of their method and find that it efficiently selects potential structures, learns consistent feature representations, and reduces overfitting during the recovery phase. The paper is categorized under Neural and Evolutionary Computing, Artificial Intelligence, Computer Vision and Pattern Recognition, and Machine Learning.

Overall, this research paper presents a novel approach to efficient pruning of Spiking Neural Networks by exploiting criticality, offering promising results compared to existing methods.

The discussion on this submission includes several different points of view. Some users discuss the challenges of training Spiking Neural Networks (SNNs) compared to traditional neural networks, emphasizing that SNNs are not differentiable and cannot use backpropagation. However, there is disagreement on this point, with one user arguing that recent work has shown it is possible to compute gradients in SNNs using backpropagation algorithms that involve spike communication. Another user suggests a technique for recording spike events in an SQL database, allowing for efficient queries and determining the contributors to specific output spikes.

There is also discussion about the feasibility of implementing SNNs on hardware and the potential benefits of event-driven architectures. A user mentions that SNN models can be run on simple CPUs but may require specialized hardware for improved performance. Another user highlights the efficiency of event-driven software and its implications for simulating networks.

In response to a comment about the gradient computation in SNNs, another user suggests using differentiable approximations for spike-based learning in order to optimize the weights of the network. They propose a principled version of injecting noise into the network, which can help in classifying patterns with different probabilities.

There is a brief exchange about the scalability of training large SNNs, with one user mentioning that they stopped using a method that involved billions of neurons for training long short-term memory (LLM) SNNs. Another user points out that current hardware may not efficiently support the constraints of SNNs.

Towards the end of the discussion, a user shares their curiosity about improving the functioning of brains, prompting another user to express interest in improved brain models, particularly in relation to memory and communication abilities.

Overall, the discussion delves into various aspects of SNNs, including training challenges, hardware implementation, noise injection, and scalability. There is also interest in understanding and improving the functioning of biological brains.

### Android phones can be taken over remotely – update when you can

#### [Submission URL](https://www.malwarebytes.com/blog/news/2023/12/android-phones-can-be-taken-over-remotely-update-when-you-can) | 22 points | by [akyuu](https://news.ycombinator.com/user?id=akyuu) | [5 comments](https://news.ycombinator.com/item?id=38556120)

Google has released its Android security bulletin for December, which includes patches for 94 vulnerabilities, with five rated as "Critical". One of the most severe flaws is a remote code execution vulnerability in the System component that could be exploited without any additional execution privileges. Another critical vulnerability is an Elevation of Privilege (EoP) flaw in the Android Framework, which could lead to a race condition and give an attacker unauthorized permissions. The updates have been made available for Android 11, 12, 12L, 13, and 14, but availability may differ among vendors. Android partners are notified of issues at least a month before publication.

The discussion around the submission includes a few comments. 

- User "rdx" mentions that they have updated their device.
- User "DistractionRect" expresses frustration with their Pixel 4a device and some issues they are facing.
- User "dr_kiszonka" responds with empathy and wishes the user good luck in resolving their problems.
- User "Xiol32" comments that the December update is not yet available for their device, the 7 Pro.
- User "mdnl" mentions that the Pixel 8 will report a rebooted phone rather than a checked product, and explains some steps they took before checking their device and finding no issues.

### Apple demonstrates its commitment to AI with new open source code release

#### [Submission URL](https://appleinsider.com/articles/23/12/06/apple-demonstrates-its-commitment-to-ai-with-new-open-source-code-release) | 20 points | by [Brajeshwar](https://news.ycombinator.com/user?id=Brajeshwar) | [16 comments](https://news.ycombinator.com/item?id=38556949)

Apple has released a free and open-source framework called MLX for AI developers to build on with Apple Silicon. The framework, developed by Apple's Machine Learning team, is designed to be efficient and user-friendly for training and deploying models. Apple's move to contribute to open source development showcases its commitment to AI and machine learning. The company aims to make it easy for researchers to extend and improve MLX, demonstrating that it is not behind in the AI field. The full source code for MLX is available on GitHub, allowing developers to explore and collaborate on the framework.

The discussion revolves around Apple's release of the open-source framework MLX for AI developers. Some users express skepticism about Apple's commitment to AI, suggesting that it is just a strategic move and not a true commitment. Others compare Apple's approach to Nvidia's and discuss the limitations and factors influencing AI development. Some users mention the significance of Apple's strategy in relation to its products and the company's dedication to making its own OS and hardware. Others comment on the manipulation of language and the distinction between AI and machine learning. There are also discussions about the current state and future trends of AI and its impact on various industries. Overall, the comments reflect a variety of viewpoints and opinions on Apple's contribution to the AI community.

