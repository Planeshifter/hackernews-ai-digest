## AI Submissions for Wed Sep 13 2023 {{ 'date': '2023-09-13T17:10:54.284Z' }}

### Any sufficiently advanced uninstaller is indistinguishable from malware

#### [Submission URL](https://devblogs.microsoft.com/oldnewthing/20230911-00/?p=108749) | 856 points | by [mycall](https://news.ycombinator.com/user?id=mycall) | [495 comments](https://news.ycombinator.com/item?id=37491862)

Today's Hacker News digest highlights an interesting article discussing the blurry line between advanced uninstallers and malware. The author, Raymond Chen, explores a spike in Explorer crashes and dissects the code to uncover potential malicious behavior. Through reverse-compiling the code, Chen discovers that it contains function pointers designed to wait for a process to exit, close handles, and potentially manipulate files and directories. Specifically, the code attempts to interact with Contoso's auto-updater. The article provides a thorough examination of the code and poses questions about the intentions behind it. It's a fascinating look at the complexities of software behavior and the potential risks users face when dealing with uninstallers.

In the discussion, users debated various aspects of the article and shared their perspectives on the behavior of the code in question. Some users pointed out similarities between Windows packages and macOS applications, stating that most Windows applications store their program files directly in the drive, unlike macOS, which separates them into two folders. Others shared links to code projects and discussed the legality of self-deleting executables. There were also discussions about the behavior of JavaScript scripts and the potential risks they pose. Users debated the legitimacy of injecting code into processes and shared possible solutions to the problem. The discussion also touched on the role of antivirus software and its ability to detect unwanted behaviors. Some users provided alternative solutions to identify and address malicious behavior in software. Overall, the discussion delved into technical details and offered different perspectives on the intricacies of software behavior and potential vulnerabilities.

### Stable Audio: Fast Timing-Conditioned Latent Audio Diffusion

#### [Submission URL](https://stability.ai/research/stable-audio-efficient-timing-latent-diffusion) | 363 points | by [JonathanFly](https://news.ycombinator.com/user?id=JonathanFly) | [192 comments](https://news.ycombinator.com/item?id=37494620)

Stable Audio, a new audio generative model, has been introduced by Stability AI's generative audio research lab, Harmonai. The model allows for control over the content and length of generated audio by conditioning on text metadata, audio file duration, and start time. This additional timing conditioning enables the generation of audio of specified lengths, even for music. The model utilizes diffusion-based generative models and a variational autoencoder to compress and process the audio. It also uses a text encoder for conditioning on text prompts and timing embeddings for specifying the overall length of the output audio. The model has been trained on a dataset of over 800,000 audio files and shows promising results in terms of output quality and controllability. Harmonai plans to release open-source models and training code in the future.

The comments on this submission cover various aspects of the Stable Audio model and its potential applications. Some users express interest in the ability to generate music and audio using text prompts and timing conditioning, while others mention similar existing models and methods. Discussions also touch on the challenges of generating musical compositions and the limitations and possibilities of MIDI as a representation format. Overall, users are intrigued by the capabilities of the Stable Audio model and discuss its potential use cases and improvements.

### AI and the End of Programming

#### [Submission URL](http://bit-player.org/2023/ai-and-the-end-of-programming) | 35 points | by [082349872349872](https://news.ycombinator.com/user?id=082349872349872) | [24 comments](https://news.ycombinator.com/item?id=37501456)

In a recent article on Hacker News, Brian Hayes discusses the idea of the end of programming as we know it. He refers to Matt Welsh's statement that AI systems will replace most software and generate programs themselves. However, Hayes expresses skepticism about this notion and emphasizes his love for programming and the importance of writing code to understand ideas fully. He also discusses the concept of large language models (LLMs), which are AI systems built on artificial neural networks and trained on massive amounts of text. Hayes notes that while LLMs may have their strengths, they also have limitations and can make spectacular failures. He concludes by stating that even if AI becomes better at programming, he will still embrace his code editor and compiler.

The discussion on this article covers various topics related to the idea of AI replacing programmers and the capabilities of large language models (LLMs). 

One commenter agrees that LLMs can be magical and believes that they have the potential to exponentially expand program content. They argue that LLMs can complement human work instead of being a complete substitute.
Another commenter discusses the potential of LLMs, suggesting that a more advanced version like GPT-4 could constantly work towards specific goals and even inhabit robot bodies, similar to Boston Dynamics' robots. They predict that self-driving technology will become widespread and solve many problems in the future.
An ongoing sub-thread raises concerns about the limitations of LLMs, pointing out that they currently cannot solve safety-critical driving problems. Another commenter counters by stating that LLMs can mimic language patterns effectively and note their concerns about AI's inherent black box nature when it comes to processing and reporting data.
Some participants chat about the intelligence of AI and debate how to quantify it. One commenter suggests that intelligence should be measured by the quality of timely decision-making rather than passing IQ tests. However, another commenter points out that AI can be fooled, implying that it may not be as intelligent as some claim.
The capabilities of LLMs are discussed further, with one commenter mentioning that LLMs currently lack a full understanding of context. They argue that this is a challenging problem in the programming world that AI has not yet completely solved.
A commenter expresses skepticism about the notion of machines reaching a threshold where they can solve complex computational problems and believes that we may not be headed in a specific direction as some claim.
Other topics brought up in the discussion include the potential impact of AI on the programming world, the reliability of LLMs, and the role of humans in writing code and kickstarting the learning process for AI systems.

### Show HN: Lantern â€“ a PostgreSQL vector database for building AI applications

#### [Submission URL](https://docs.lantern.dev/blog/2023/09/13/hello-world) | 182 points | by [ngalstyan4](https://news.ycombinator.com/user?id=ngalstyan4) | [41 comments](https://news.ycombinator.com/item?id=37499375)

Lantern, a PostgreSQL vector database extension, is making waves in the AI application development scene. It offers a complete feature set, allowing developers to build AI applications without leaving their database. The extension supports end-to-end AI application creation, embedding generation for popular use cases, and interoperability with pgvector's data type. One standout feature is its parallel index creation capabilities, which enable users to create indexes without interrupting database workflows.

In addition to its current features, Lantern has exciting plans for the future. They are working on a cloud-hosted version of the extension, templates and guides for building applications specific to different industries, tools for generating embeddings from third-party model APIs, and support for version control and A/B testing of embeddings. They are also developing an autotuned index type that will select appropriate parameters for index creation and expanding vector element support.

When it comes to performance, Lantern shines bright. It outperforms competitors like pgvector and pg_embedding (Neon) in key metrics such as CREATE INDEX time, SELECT throughput, and SELECT latency. The extension is built on top of usearch, a highly scalable and performant algorithm for vector search.

Lantern's decision to build on top of PostgreSQL stems from the belief that it is essential to leverage the existing power and familiarity of PostgreSQL within the developer community. By building on PostgreSQL, Lantern benefits from the extensive optimizations and data storage/access capabilities that have been developed over the past 30 years. This approach also enables companies already using PostgreSQL to seamlessly integrate Lantern into their existing infrastructure.

Lantern has a couple of asks and offers for the community. They encourage users to provide feedback and report bugs as they continue to improve the extension. For those currently using pgvector, Lantern offers free AirPods Pro as an incentive to switch over. They are also available to assist developers who want to get started with building AI applications using Lantern. Moreover, Lantern is actively seeking contributors and hiring full-time engineers.

Overall, Lantern aims to be the most performant vector database with a comprehensive set of tools for AI application development. Their goal is to help companies leverage their structured and unstructured data to build useful applications. So, whether you're an AI developer looking for a powerful vector database or someone interested in contributing to Lantern's mission, they are eager to hear from you.

The discussion surrounding the submission is quite mixed. Some users are skeptical of Lantern's claims and question its effectiveness compared to other solutions. One user points out that the offer of free AirPods Pro as an incentive to switch to Lantern seems suspicious. Others express concerns about the cost and scalability of using PostgreSQL for AI applications.

However, there are also users who are impressed with Lantern's performance and are interested in trying it out. They discuss specific use cases and potential optimizations for certain scenarios. Some users appreciate Lantern's focus on leveraging PostgreSQL's existing capabilities and its plans for future improvements.

There is also a discussion about the limitations and potential improvements of Lantern. Users inquire about maintaining indexes for updated data, handling conflicts with other extensions, and supporting sparse vectors. The Lantern team actively engages in the discussion, providing clarifications and explanations.

Overall, the discussion highlights both skepticism and interest in Lantern as a PostgreSQL vector database extension. Users raise valid concerns and questions while also acknowledging the potential benefits of using Lantern for AI application development.

### Show HN: Vips â€“ Emacs Interface for OpenAI's GPT API and DeepL's Translation API

#### [Submission URL](https://github.com/marcklemp/vips) | 5 points | by [mklemp](https://news.ycombinator.com/user?id=mklemp) | [4 comments](https://news.ycombinator.com/item?id=37502387)

Meet vips.el: the Emacs interface for OpenAI's GPT API and DeepL's translation API. Developed by marcklemp, this tool allows Emacs users to seamlessly work with OpenAI's GPT-4 and GPT-3.5-turbo models. Users can customize various parameters such as max tokens, temperature, top-p, frequency penalty, and presence penalty. Additionally, vips.el enables text translation using DeepL's API, with support for multiple target languages. To get started, users need to download vips.el, add it to their Emacs load-path, and activate vips-mode. From there, they can leverage shortcuts to send selected text to the GPT models or translate text using DeepL. Importantly, valid API keys for GPT and DeepL are required. Vips.el is distributed as free software under the GNU General Public License. If you're an Emacs power user looking to enhance your text generation and translation capabilities, vips.el might be worth checking out.

The discussion on this submission primarily consists of a conversation between "pmntr" and "mds" about the convenience of using the vips.el tool for text generation and translation. "pmntr" mentions that it's convenient to use vips.el for selecting a region of text and sending it to GPT models or translating it. "mds" expresses gratitude for maintaining GPTel and shares that they found Vips to be simpler. "mds" also requests links to more information about vips.el. 

Another user named "kng" makes a comment simply saying "hack." "mds" apologizes for their confusing message, stating that their previous comment was unnecessarily redundant and they had revised it in hopes of finding the current version acceptable.

### Exllamav2: Inference library for running LLMs locally on consumer-class GPUs

#### [Submission URL](https://github.com/turboderp/exllamav2) | 315 points | by [Palmik](https://news.ycombinator.com/user?id=Palmik) | [122 comments](https://news.ycombinator.com/item?id=37492986)

Introducing ExLlamaV2: A Fast Inference Library for Local LLMs

Turboderp has released ExLlamaV2, an inference library designed to run local LLMs (large language models) on modern consumer-class GPUs. This new library promises faster and better performance, with cleaner and more versatile code compared to its predecessor, ExLlamaV1. ExLlamaV2 also introduces support for a new quant format called "EXL2," which allows for 2 to 8-bit quantization, giving users more flexibility in achieving their desired average bitrate. The library is still in its early stages and requires further testing and tuning, but initial performance tests show promising results. Users can clone the repository and install dependencies to try it out.

The discussion on Hacker News regarding the submission about ExLlamaV2, a fast inference library for local LLMs, covered various topics. Here are some key highlights:

- One commenter mentioned that they had been running a 70B 24GB model for several months, and the performance improvement with 2-bit quantization was around 2x. They also noted that the quantization trade-off was an interesting question, with larger models performing better with lower bit quantization, but smaller models benefiting from higher bit quantization.
- Another commenter mentioned the OmniQuant method, which offers noteworthy performance improvements in quantization methods compared to other approaches.
- There was a discussion about training and running quantized models. A paper was shared that discussed the attempts to use LoRA (Lossy Recompression Algorithm) for training quantized LLMs. The paper highlighted that LoRA decreased model precision but allowed for faster inference.
- Some commenters discussed the performance of LLMs on different hardware, such as CPUs and GPUs. One commenter shared their experience of running LLMs on half of the layers on CPUs and the other half on GPUs, while others mentioned different command line flags and options for running LLMs on GPUs.
- The topic of the LLM's performance on different tasks was brought up, with one commenter expressing doubts about the 255-bit quantized 70B LLM model's performance on GPT-35-trb tasks.
- The use of LLMs for specific use cases, such as high throughput and low latency, was discussed. Some commenters asked about using multiple lower-memory GPUs to horizontally split models for batch inference, and the handling of common sensors in LLMs.
- The discussion delved into the comparison between GPT-3, GPT-35, and LLaMa models in terms of performance and benchmarking. Performance figures and benchmarks were shared, including a HuggingFace benchmark and the ARC benchmark, which evaluates LLM performance by testing their reasoning abilities on language tasks.

Overall, the discussion covered a range of topics related to the performance, quantization, hardware, and benchmarking of LLM models, providing different perspectives and insights.

### Lessons from YC AI Startups

#### [Submission URL](https://www.ignorance.ai/p/5-lessons-from-139-yc-ai-startups) | 132 points | by [charlierguo](https://news.ycombinator.com/user?id=charlierguo) | [92 comments](https://news.ycombinator.com/item?id=37490924)

This week's YC Demo Day showcased a record-breaking 139 AI startups, up from 112 in the last batch. The top four categories among these startups were AI Ops, developer tools, healthcare and biotech, and finance and payments. AI Ops is emerging as a crucial sector, with startups focusing on various aspects such as training, deploying, and fine-tuning large language models (LLMs). Additionally, there was a notable presence of "Copilot for X" companies, providing B2B AI assistants to assist with tasks ranging from corporate event planning to contract negotiation. Despite the hype around AI, building a defensible company remains crucial, as AI alone is not a guarantee of success.

The discussion on the submission revolves around various aspects of AI and its practical applications. Here are some key points from the comments:

1. The effectiveness of large language models (LLMs): One user discusses the limitations and challenges of using LLMs for tasks like logistics and suggests alternative approaches. They mention the need for statistical and semantic tests to verify the performance of LLMs.
2. AI therapy: There is a conversation about the potential of AI in providing therapy and mental health treatment. The discussion touches upon the advantages and limitations of AI as a substitute for human therapists.
3. Criticism of AI therapy: Some users question the viability and effectiveness of AI therapy compared to licensed therapists. They highlight the importance of human interaction and personal experience in mental health treatment.
4. Building AI applications: Users discuss the process of building AI applications and the value they bring to various industries. They mention examples like AI-powered assistants for logistics and AI-based rating systems.

5. Potential applications of AI in energy, materials science, and security: The conversation expands to explore the potential of AI in fields like energy, materials science, and security. Users discuss the intersection of AI and material science, as well as its applications in exploration and research.

Overall, the discussion covers a range of perspectives on the practicality, limitations, and potential of AI in various industries.

