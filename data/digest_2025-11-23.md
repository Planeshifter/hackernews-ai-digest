## AI Submissions for Sun Nov 23 2025 {{ 'date': '2025-11-23T17:09:54.524Z' }}

### An Economy of AI Agents

#### [Submission URL](https://arxiv.org/abs/2509.01063) | 120 points | by [nerder92](https://news.ycombinator.com/user?id=nerder92) | [84 comments](https://news.ycombinator.com/item?id=46020096)

An Economy of AI Agents (arXiv, Open Access)

What’s new: Gillian K. Hadfield and Andrew Koh survey how autonomous AI agents—systems that plan and execute complex tasks with minimal human oversight—could reshape markets, firms, and the rules we’ll need to keep those markets working.

Key points
- Scope: A broad tour of recent developments in agentic AI and the economic questions they raise, aimed at economists, policy folks, and builders.
- Markets: How agent–agent interactions might change price discovery, search, and contracting; risks like tacit collusion among pricing bots; and new frictions around verification and trust.
- Organizations: Classic Coase/Williamson themes revisited—if agents cut transaction costs, do firm boundaries shrink? How do we structure teams when both humans and AIs are principals and agents?
- Institutions: What governance infrastructure is required—identity for agents, accountability/liability to human principals, auditing and eval standards, disclosure norms, and competition policy that anticipates agent-driven dynamics.
- Open questions: Welfare when agents optimize proxy objectives; mechanism design for multi-agent ecosystems; labor market impacts; how to measure and mitigate systemic risks from agent swarms.

Why it matters: Agentic AI won’t just automate tasks; it may alter the “rules of the game.” This piece maps the economic terrain and the institutional guardrails we’ll likely need as agent marketplaces, pricing bots, and autonomous workflows scale.

Paper: An Economy of AI Agents — Hadfield & Koh
arXiv: 2509.01063 (econ.GN)
https://doi.org/10.48550/arXiv.2509.01063

Note: Posted during Open Access Week—arXiv is asking for support to keep research freely available.

Based on the discussion, here is a summary of the comments:

**Agents vs. Traditional Workflow Engines**
The most active debate centers on the definition and utility of "agents" compared to existing business process automation. User **zkmn** argues that traditional workflow engines (orchestrated tasks interacting with software/humans) already exist and that business requirements demand deterministic, rule-based reliability rather than "probabilistic" AI reasoning. They contend that most "agent" behavior is simply input processing (routing) which doesn't require an autonomous entity defining logic on the fly.

Counter-arguments (led by **mewpmewp2** and **lbrth**) distinguish agents by their **autonomy** to determine *next steps* rather than following a pre-defined graph. They argue:
*   **Unstructured Inputs:** Hard-coded rules fail when inputs are free-form (e.g., a vague customer complaint). An agent can autonomously loop through searching manuals, summarizing findings, and deciding whether to email a human or reply directly.
*   **Runtime Code Generation:** Proponents highlight that modern agents (like Claude Code or Perplexity) actually write and execute code at runtime to solve unique queries—something a static workflow engine cannot do.
*   **Rigidity vs. Reality:** While skeptics argue businesses need fixed rules, **nrttn** notes that rigid workflows slowly drift from reality, whereas agents can adapt to changing contexts.

**Sci-Fi Parallels**
Commenters immediately drew parallels to Charles Stross’s novel *Accelerando*. Users discussed the concept of "Vile Offspring"—trillions of high-speed trading bots creating a financial singularity that humans can no longer comprehend or interact with. While some view this as "shortsighted specific fantasy," others see it as an accurate "forward-looking" prediction of where agent-based economies are heading.

**Implementation & Commoditization**
*   **Prototyping:** **rdk** suggests a hybrid approach: utilize agents to rapidly prototype internal apps, then replace the 99% of stable workflows with deterministic Python code once the logic is understood, keeping AI only for the edge cases.
*   **No Moat:** The discussion briefly touches on the leaked Google memo ("We have no moat"), with users noting that agent proliferation is being driven by the availability of high-quality, open-weight "distilled" models that run cheaply on consumer hardware.

### Claude Status – Elevated error rates on the API

#### [Submission URL](https://status.claude.com/incidents/538r2y9cjmhk) | 54 points | by [throwpoaster](https://news.ycombinator.com/user?id=throwpoaster) | [63 comments](https://news.ycombinator.com/item?id=46023364)

Anthropic resolves brief spike in Claude API errors

- What happened: Anthropic reported elevated failure rates on the Claude API (api.anthropic.com).
- Timeline (UTC, Nov 23, 2025): Investigating at 13:05 → “continuing to investigate” at 13:06 → fix implemented at 13:53 → incident resolved at 14:11 after monitoring. Total duration ~66 minutes.
- Impact: Higher-than-normal API error rates; no root cause disclosed.
- Status: Resolved and stable per Statuspage.

If you saw unusual 5xx/timeouts around that window, consider replaying failed jobs. General hardening: implement retries with backoff, timeouts, idempotency, and alerting on error-rate spikes. You can subscribe to Statuspage updates for future incidents.

Use of the Anthropic API appears to be heavily tied to the "Claude Code" CLI tool for many commenters, turning a brief API outage into a broader discussion about developer dependency on AI agents, the reliability of LLM infrastructure, and the changing nature of software engineering identity.

Here is a summary of the discussion:

**Claude Code Dependency and Fallbacks**
*   The outage specifically disrupted users of **Claude Code**, with developers lamenting the interruption to "agentic" workflows where the AI rewrites entire files or handles large refactors (reducing code volume by significant percentages).
*   Discussions emerged regarding failover strategies. Some users shared tools like `cloud-code-router` or switch to OpenRouter, VS Code, and Gemini during Anthropic downtime.
*   One user noted they prefer Anthropic’s occasional instability over Gemini's strict usage limits.

**"Old School" Programmers vs. AI Reliance**
*   A contentious thread began with a comment mocking "VB coders" who are helpless without their $200/month subscription. This drew sharp rebuttals from experienced developers who use AI to accelerate "greenfield" projects and bypass repetitive boilerplate.
*   **The Boilerplate Debate:** Pro-AI commenters argued that writing things like RBAC (Role-Based Access Control) systems or frontend tables for the 100th time is a waste of human effort best offloaded to AI. Skeptics countered that if you are rewriting the same code 100 times, you should be using abstractions, libraries, or snippets rather than generating fresh code every time.
*   **Security Concerns:** The specific mention of using AI to generate RBAC systems triggered immediate pushback regarding the security risks of trusting an LLM with access control logic.

**Infrastructure Complexity**
*   Commenters with experience in ML infrastructure noted that maintaining 99.99% uptime for LLMs is fundamentally harder than standard web apps.
*   Issues cited include granular hardware dependencies (GPUs/TPUs), the difficulty of caching large contexts, and the massive memory requirements for loading models (specifically Mixture of Experts models), making redundancy expensive and "cold starts" slow.

**Developer Identity and "Withdrawal"**
*   The outage sparked a philosophical sidebar about how strongly developers are tying their professional identity to AI tools.
*   Comparisons were made to "leaving your phone at home" or even addiction, describing the helpless feeling when the "intelligence on tap" is cut off. Others mocked the need to return to "meat code" (human coding) during the downtime.

### Meet the AI workers who tell their friends and family to stay away from AI

#### [Submission URL](https://www.theguardian.com/technology/2025/nov/22/ai-workers-tell-family-stay-away) | 60 points | by [breve](https://news.ycombinator.com/user?id=breve) | [21 comments](https://news.ycombinator.com/item?id=46027290)

Meet the AI workers who tell their friends and family to stay away from AI (The Guardian)

TL;DR: The people training today’s chatbots say they don’t trust them—citing speed-over-safety incentives, ignored feedback, and being asked to judge high‑stakes answers without expertise.

Highlights:
- AI raters and Mechanical Turk workers describe constant pressure for rapid turnaround that undermines quality control and lets harmful or wrong outputs slip through.
- One rater recalls nearly missing an obscure racist slur while labeling content—an example of how easy it is to make consequential mistakes under time pressure.
- A Google rater says she was tasked with evaluating AI-generated medical advice despite no medical training; she now avoids using chatbots and bans her child from them until she can critically assess outputs.
- Companies respond that rater input is just one of many signals (Google) and that workers choose tasks with set pay/time/instructions (Amazon).
- Media literacy experts say the distrust among insiders signals incentives to ship and scale over careful validation, foreshadowing errors users will see.

Why it matters: If the people closest to the systems are warning loved ones off, it underscores a growing trust gap—and raises questions about safety practices, oversight, and how much human feedback actually shapes deployed models.

**Daily Digest**

**Top Story:** Meet the AI workers who tell their friends and family to stay away from AI (The Guardian)

**Discussion Summary:**
The discussion focused on the credibility of the article's premise versus the real-world utility of the tools.

*   **Methodology Skepticism:** Several users criticized the article for lacking statistical rigor. Commenters argued that finding a handful of dissatisfied workers among millions is inevitable—likening it to finding "physicists who say the earth is flat"—and suggested the piece relied on selection bias to confirm a pre-existing narrative rather than proving a systemic issue.
*   **"High on Supply":** The submission title prompted comparisons to the "drug dealer rule" (don't consume your own product) and the common trend of tech executives banning their own children from using the social media platforms they build to avoid addiction or harm.
*   **Hallucinations in the Wild:** Consultants and professionals shared frustrations regarding clients who treat ChatGPT "hallucinations" as fact. Anecdotes included clients demanding features based on non-existent Amazon business accounts invented by AI, or presenting incorrect medical advice, forcing human experts to spend billable hours debunking the AI's output rather than doing productive work.
*   **The "Obscure Slur":** In response to the article's mention of a rater nearly missing a slur, a sub-thread debated the obscurity of such terms. Users living in the American South noted they had never heard the specific term implied, suggesting it was likely archaic (1930s-era) and understandable that a modern moderator might miss it.
*   **Erosion of Truth:** A broader concern was raised about the loss of "source checking." Users worried that while older generations might treat AI outputs with suspicion (like WebMD self-diagnoses), younger generations are growing up viewing LLMs as authoritative sources of truth without verifying citations.

### 'Is AI creating a new code review bottleneck for senior engineers?'

#### [Submission URL](https://thenewstack.io/is-ai-creating-a-new-code-review-bottleneck-for-senior-engineers/) | 19 points | by [MilnerRoute](https://news.ycombinator.com/user?id=MilnerRoute) | [3 comments](https://news.ycombinator.com/item?id=46024884)

The New Stack link making the rounds on HN isn’t an article — it’s a newsletter sign-up flow.

What you’ll see:
- A required email plus an extensive profile form (name, company, country, ZIP), job level/role, org size/type, industry, and optional LinkedIn.
- A special “re-subscribe” path if you’ve unsubscribed before.
- A privacy note saying TNS doesn’t sell data or share with unaffiliated third parties.
- Promise of weekday deliveries with at-scale software development news.

Takeaway: this is a lead‑gen subscription gate rather than a readable post; expect friction if you clicked through for content.

**Discussion Summary**

Despite the submission linking to a newsletter sign-up, the conversation focuses on a specific technical grievance regarding a GitHub Pull Request (PR).

*   **Suspicious PR Activity:** User `vrdvrm` describes an encounter with a GitHub PR where they wasted time reviewing code they eventually realized was "garbage." Upon looking deeper, they found the account in question had dozens of rejected PRs across various projects.
*   **Canned Responses:** In response to the incident, user `gphph` noted that the communications involved looked like "polished professional" copy-pasted text, suggesting a bot or a scripted interaction rather than a genuine contribution.

### Show HN: Dank-AI – Ship production AI agents 10x faster

#### [Submission URL](https://www.dank-ai.xyz/) | 6 points | by [deltadarkly](https://news.ycombinator.com/user?id=deltadarkly) | [5 comments](https://news.ycombinator.com/item?id=46021135)

AI Agent microservice framework “Dank” turns JavaScript-based agents into containerized services you can deploy anywhere. It’s Docker-native, open source (MIT), and aims to give JS developers an opinionated path from local prototype to production.

What it is
- JavaScript-native agent orchestration: define agents in a JS config (no YAML), wire up LLMs and event handlers, and run them as containers.
- Multi-agent by design: manage multiple agents as separate services with per-agent resources and endpoints.
- Cloud-agnostic deployment: build once, run on Kubernetes, Docker Swarm, AWS ECS, Azure ACI, or your own servers.

Why it matters
- Fits existing DevOps: agents build and ship like any other container, slotting into CI/CD and registries.
- JS-first: no Python dependency; supports OpenAI, Anthropic, Cohere, Ollama, or custom providers.
- Production features baked in: API keys, RBAC, TLS, env vars, encrypted secrets, monitoring, dashboards, and alerting.

Notable features
- Simple config: define agents in dank.config.js with LLM, resources (CPU/mem/timeouts), prompts, and event handlers.
- CLI-driven workflow:
  - npm install -g dank-ai
  - dank init my-agent-project
  - dank run
  - dank build and dank build --push for registries/CI
- Orchestration and Ops: per-agent CPU/memory limits, dynamic resource scaling, real-time metrics, logs, historical analytics.
- Networking: configurable HTTP endpoints/webhooks, rate limiting, load balancing controls, dedicated hostnames, TLS by default.
- Secrets and config: environment-variable driven, encrypted secrets storage, audit logging.

Example use
- Spin up a “customer-service” agent with an OpenAI model, set resources, define a prompt, and add output/request handlers—all in JavaScript—then deploy as a containerized service.

Caveats / open questions
- Docker is a hard requirement (the CLI can auto-detect/install it).
- Autoscaling specifics and state/memory integrations (e.g., vector stores) aren’t detailed.
- Being JS-only may limit reuse of Python-centric agent tooling, though it supports Ollama/local models and custom providers.

Who it’s for
- Teams standardizing on JavaScript/TypeScript that want to ship agents as microservices with familiar container workflows and production guardrails.

**Discussion Summary**

The discussion around Dank focuses on architectural scalability and the "wrapper" fatigue common in the AI space:

*   **Architecture & Scaling:** Users debated the utility of event-driven, stateless runtimes for multi-agent workflows. One commenter pointed out potential coordination problems and "fan-in" conflicts with swarm data events. In response, another user (likely the author) argued that current single-runtime frameworks are suited for local automation but fail to scale, whereas isolating agents in their own runtimes (microservices) is the future standard for developers.
*   **Developer Experience:** Initial feedback on the CLI was positive, with one user comparing the quick setup and deployment flow to the ease of **Vercel's CLI**.
*   **Skepticism & Support:** While one commenter dismissed the project as merely "wrappers [for] LLM wrappers," another validated the JavaScript-first approach, noting the massive market share of JS developers.

