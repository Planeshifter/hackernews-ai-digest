## AI Submissions for Sun Nov 23 2025 {{ 'date': '2025-11-23T04:30:02.051Z' }}

### An Economy of AI Agents

#### [Submission URL](https://arxiv.org/abs/2509.01063) | 100 points | by [nerder92](https://news.ycombinator.com/user?id=nerder92) | [64 comments](https://news.ycombinator.com/item?id=46020096)

An Economy of AI Agents (arXiv). Survey/primer by Gillian K. Hadfield and Andrew Koh on what happens when long-horizon, largely autonomous AI agents start transacting across the economy.

Why it matters
- Treats AI agents as economic actors that plan, bargain, buy/sell, and coordinate—with minimal human oversight—forcing a rethink of market design, firm boundaries, and regulation.

What the authors survey and the open questions they raise
- Markets: How agent-agent and human-agent interactions affect price discovery, liquidity, and competition; risks of tacit algorithmic collusion and rapid cascades.
- Organizations: How firms restructure when “swarms” of agents do procurement, logistics, R&D, and negotiations; new principal–agent problems and monitoring.
- Institutions: What’s needed for well-functioning markets—identity and licensing for agents, liability and insurance, auditability and logging, reputation/credit systems, standards and protocols, and dispute resolution.
- Policy and design: Mechanism design for mixed human/agent markets, antitrust in an algorithmic world, data and privacy constraints, security and authentication, and measurement of productivity and labor impacts.

Bottom line
- It’s a map of the economics research and policy agenda for an agentic future: don’t just build smarter agents—build the market rules and infrastructure that make their interactions safe and efficient.

**Discussion Summary**

The discussion focuses heavily on the practical and theoretical frictions of integrating autonomous agents into existing economic and technical structures:

*   **Determinism vs. Probabilistic Autonomy:** A lengthy technical debate dominates the thread regarding whether AI agents can replace traditional workflow engines. User `zkmn` argues that businesses depend on deterministic, rule-based logic for accountability and requirements, asserting that probabilistic, "on-the-fly" decision-making is too risky for core business processes. In contrast, `mewp2` argues that rigid rules fail against unstructured real-world data (like customer service queries), suggesting agents are superior for categorizing inputs, routing tasks, and even generating runtime code to solve specific problems (e.g., scraping travel data) that hardcoded logic cannot handle.
*   **Sci-Fi Parallels:** Commenters drew immediate comparisons to science fiction, specifically Charles Stross’s *Accelerando*—which depicts an economics of AI entities operating at speeds/complexities incomprehensible to humans—and Robin Hanson’s *The Age of Em*.
*   **Organizational Structure:** The conversation touched on the revival of Decentralized Autonomous Organizations (DAOs). While some users dismissed DAOs as merely "open source co-ops" or failed blockchain experiments, others argued that the introduction of truly agentic AI makes the DAO concept viable in ways previous iterations were not.
*   **Labor Implications:** A cynical undercurrent noted that these efficiencies likely signal a future where companies simply stop hiring people, shifting the economy toward capital efficiency over labor participation.

### Claude Status – Elevated error rates on the API

#### [Submission URL](https://status.claude.com/incidents/538r2y9cjmhk) | 53 points | by [throwpoaster](https://news.ycombinator.com/user?id=throwpoaster) | [63 comments](https://news.ycombinator.com/item?id=46023364)

Anthropic resolves brief Claude API hiccup

- What happened: Anthropic reported elevated failure rates on the Claude API (api.anthropic.com) on Nov 23, 2025.
- Timeline (UTC):
  - 13:05 Investigating reports of higher-than-normal failures
  - 13:06 Continued investigation
  - 13:53 Fix implemented; monitoring
  - 14:11 Incident marked resolved
- Duration: Roughly 1 hour from first report to resolution.
- Scope: Claude API only; no root cause disclosed yet.
- Dev takeaway: If you saw spikes in errors around that window, they should be cleared now. Worth checking logs and ensuring robust retry/backoff logic. You can subscribe to Anthropic’s status updates for future incidents.

**Dependency vs. Skill:** The outage sparked a heated "old guard vs. new guard" debate. Some users scoffed at "VB coders" and "wrapper wrappers" who were paralyzed by the downtime, implying that "real" programmers shouldn't be reliant on AI. Others pushed back, arguing that for greenfield projects and boilerplate (like writing RBAC systems or stitching together client-mandated libraries), maintaining high velocity without AI is inefficient.

**Infrastructure & Cause:** Speculation led many to believe the release of "Claude Code" caused the instability. This segued into a technical discussion about the fragility of LLM infrastructure compared to standard web services, noting that the challenges of loading massive models (MoE/expert loading) and managing hardware dependencies (GPUs/TPUs) make "five nines" reliability much harder to achieve.

**Identity Crisis:** A philosophical sub-thread explored why developers feel so threatened by these tools. Commenters discussed how AI challenges the "mystique" and high-status nature of the profession, suggesting that the gatekeeping seen in the comments stems from a fear of coding becoming a commodity.

**Tooling & Fallbacks:** Users discussed redundancy strategies, such as routing traffic to Gemini, OpenAI, or ZAI when Anthropic wobbles. There was also a specific critique of the new "Claude Code" tool, with one user noting that while good for scaffolding, the standard Sonnet web interface was significantly better at deep refactoring and code reduction.

**Humor:** Amidst the frustration, jokes emerged about human-written software becoming a premium artisanal product, dubbed "Meat Code."

### Meet the AI workers who tell their friends and family to stay away from AI

#### [Submission URL](https://www.theguardian.com/technology/2025/nov/22/ai-workers-tell-family-stay-away) | 48 points | by [breve](https://news.ycombinator.com/user?id=breve) | [14 comments](https://news.ycombinator.com/item?id=46027290)

Meet the AI workers who tell their friends and family to stay away from AI (The Guardian): The piece profiles the contract raters and Mechanical Turk annotators who help make chatbots seem reliable—many of whom now avoid using generative AI themselves and warn loved ones off it. One rater nearly misclassified a racist slur, sparking fears about unseen errors at scale; a Google contractor says she was asked to judge AI Overviews’ health answers without medical training and has banned her 10-year-old from chatbots. Workers describe pressure for speed over rigor and say their feedback often feels ignored—an incentive mismatch experts say favors shipping over validation, a bad sign as more people rely on LLMs for information. Amazon says MTurk workers choose tasks and requesters set terms; Google says ratings are just one signal and don’t directly shape models, and that it has safeguards. The broader takeaway: the people paid to make AI safer increasingly don’t trust the outputs—or the companies racing them to market.

**Selection Bias & Methodology**
A significant portion of the discussion focused on the article's methodology. Commenters argued that interviewing a half-dozen dissatisfied workers out of an estimated global workforce of millions lacks statistical rigor. Critics felt this approach was "cherry-picking" to reinforce pre-existing biases against AI, comparing it to finding physicists who believe the earth is flat or doctors opposed to vaccines—possible, but not representative.

**Evaluating "Moon Cricket"**
The article’s mention of a rater nearly misclassifying the slur "moon cricket" sparked a sub-thread on the term’s obscurity. Several users, including those living in the American South near active hate groups, noted they had never heard the term firsthand. Others identified it as extremely dated slang (circa 1930s), suggesting that failing to flag such an archaic term might be an understandable lapse rather than a sign of systemic failure.

**Client Hallucinations & Medical Advice**
The conversation shifted to personal anecdotes regarding AI reliability.
*   **Consulting Headaches:** One user described a client who believed a ChatGPT hallucination about a "special Amazon business account" over their consultant's advice; another noted that while fixing these misconceptions creates billable hours, it feels wasteful.
*   **Medical Risks:** Users expressed concern over people using LLMs for medical diagnosis, noting discrepancies between AI answers and cited sources. However, a counterpoint was raised that Internet users have long scared themselves with medical misinformation via sites like WebMD ("It's always cancer").

**The "Drug Dealer" Analogy**
Echoing the sentiment that tech workers ban their own families from using their products, the top comment invoked the "drug dealer doesn't consume their own product" trope, noting that social media elites often restrict their children’s access to technology in favor of older, disconnected hardware (e.g., ThinkPads running Debian).

### 'Is AI creating a new code review bottleneck for senior engineers?'

#### [Submission URL](https://thenewstack.io/is-ai-creating-a-new-code-review-bottleneck-for-senior-engineers/) | 16 points | by [MilnerRoute](https://news.ycombinator.com/user?id=MilnerRoute) | [3 comments](https://news.ycombinator.com/item?id=46024884)

The New Stack’s “subscribe” wall is doing a lot more than asking for an email. Visitors are met with a full-page sign-up flow that blocks content and requires numerous fields: first/last name, company, country, ZIP, job level and role, org size, organization type, and industry. If you previously unsubscribed, it forces a separate re-subscribe step before you can proceed. While TNS says it doesn’t sell data and requires agreement to its Terms and Privacy Policy, the experience reads more like a B2B lead-gen form than a simple newsletter opt-in. It’s a striking example of the growing shift from paywalls to data walls—trading access for detailed personal and professional info, with clear UX friction for readers.

The discussion thread appears to be unrelated to the article about The New Stack's subscription wall, focusing instead on **GitHub contribution quality**.

User **vrdvrm** describes a frustrating experience encountering "garbage" code in a Pull Request; after reviewing the comments and commits, they realized the account involved had dozens of rejected PRs across various projects. **Gphph** suggests handling such situations with a "polished professional response" copy-pasted directly, while **vrdvrm** adds that the associated project appears suspicious.

### Show HN: Dank-AI – Ship production AI agents 10x faster

#### [Submission URL](https://www.dank-ai.xyz/) | 6 points | by [deltadarkly](https://news.ycombinator.com/user?id=deltadarkly) | [5 comments](https://news.ycombinator.com/item?id=46021135)

HN: Dank — a JavaScript‑native, Docker‑first framework for AI agent microservices

What it is
- An open-source (MIT) toolkit to build, containerize, and run multiple AI agents as microservices using plain JavaScript/Node—no Python stack required.
- CLI-driven workflow: npm install -g dank-ai → dank init → dank run. It auto-builds optimized Docker images, can push to registries, and offers real-time monitoring out of the box.

How it works
- Define agents in JavaScript (not YAML): pick an LLM (OpenAI, Anthropic, Cohere, Ollama, or custom), set CPU/memory/timeouts, write prompts, and attach event handlers (request/response hooks, etc.).
- Multi-agent orchestration: each agent runs as a containerized service. Designed for CI/CD—dank build and dank build --push integrate with GitHub Actions/GitLab CI.
- Infra-agnostic deployment: run on Kubernetes, Docker Swarm, AWS ECS, Azure ACI, or your own servers. Each agent can get a dedicated hostname with TLS.
- Management features: per-agent resource quotas, dynamic scaling, live dashboards, historical metrics, endpoint routing and rate limiting, API key management, RBAC, secrets and env vars.

Why it matters
- Targets the huge JS/Node ecosystem with a production-friendly, Docker-native path to ship “agents as microservices,” reducing glue code between LLM logic and ops.
- Compared to many Python-first agent frameworks, Dank centers on containerization and deployment from day one, which may simplify moving from prototype to prod.

Notable commands
- Install: npm install -g dank-ai
- Init: dank init my-agent-project
- Run: dank run
- Build/push: dank build and dank build --push registry.com/my-agent:v1.0

Caveats and questions
- Docker is a hard dependency; cold starts and container overhead still apply.
- GPU/accelerator scheduling isn’t documented; unclear for heavy or on-prem LLMs.
- Monitoring/alerting depth and scaling policies may need a closer look for larger fleets.

Bottom line: If you want to ship JS-based AI agents like any other microservice—with Docker images, registries, TLS, RBAC, and infra portability—Dank aims to be the batteries-included path.

**Discussion Summary**

The conversation focused on the architectural viability of containerized agent workflows versus current single-runtime frameworks. While one user raised concerns about coordination conflicts in event-driven swarm architectures, the maintainer argued that the current pattern of running agents locally doesn't scale, positioning Dank’s microservice approach as a necessary evolution for connecting agents to "real work" (with a dedicated "Workflows" feature coming soon).

Other comments highlighted the practical appeal of the tool:
*   **Market Fit:** Users noted the utility of targeting the massive JavaScript developer pool (referencing a "98%" statistic).
*   **UX:** One tester praised the quick setup, comparing the CLI experience favorably to Vercel.
*   **Skepticism:** A detractor dismissed the project as simply another "wrapper for LLM wrappers."

