## AI Submissions for Sat Jun 01 2024 {{ 'date': '2024-06-01T17:14:11.197Z' }}

### Lisp: Icing or Cake?

#### [Submission URL](https://dthompson.us/posts/lisp-icing-or-cake.html) | 190 points | by [psj](https://news.ycombinator.com/user?id=psj) | [36 comments](https://news.ycombinator.com/item?id=40549250)

In the latest update from Hacker News, David Thompson discusses the insights he gained from the recent Spring Lisp Game Jam 2024, where a record-breaking 48 games were submitted. The breakdown of submissions by language reveals Guile as the most popular choice, with more games leveraging the Scheme-to-WebAssembly compiler 'Hoot'. Fennel, a Lisp that compiles to Lua, was another favored language for game development.

Thompson identifies two predominant approaches in building games with Lisp: the 'Lisp as icing' and 'Lisp as cake' paradigms. The former involves using Lisp as a scripting language atop a base code in static languages like C or Rust, while the latter integrates Lisp throughout the entire software stack, minimizing external dependencies. The discussion touches on specific implementations like Guile for extending C programs and Common Lisp for full-stack Lisp development.

Case studies from the game jam illustrate these patterns, with Fennel paired with love2d exemplifying 'Lisp as icing' - where Fennel acts as a scripting layer on top of the C++ game engine. On the other hand, the S7 and raylib stack represents a 'Lisp as cake' approach, where Lisp plays a significant role in the project architecture.

Thompson's analysis provides valuable insights into the diverse ways Lisp is being utilized in game development, showcasing the flexibility and versatility of this language in creating engaging and innovative projects.

The discussion revolves around the recent insights shared by David Thompson on the Spring Lisp Game Jam 2024 and the use of Lisp in game development. Here are some highlights from the comments:

- **nctdncn**: Discusses the experience of using Scheme in Max MSP and the comparison between Guile, Clojure, and Common Lisp.
- **rnlszlrn**: Expresses surprise at the number of games using Janet and mentions encountering gaming surprises.
- **hzn**: Encourages readers to check out the special thread on Lisp.
- **rcrm**: Mentions using Guile and discusses WASM development.
- **rtctn**: Shares a 3D bass fight prototype using Clojure.
- **bmch**: Mentions a joint project and critiquing a vintage article.
- **swatson741**: Shares an interest in creating games using Lisp.
- **skssn**: References a game called Dunnet and its history.
- **nthk**: Discusses experiences with the ZMachine interpreter for interactive fiction.
- **ngcc_hk**: Engages in a conversation about the evolution of Lisp and CPUs.

Overall, the discussion delves into various aspects of game development using Lisp and shares insights and experiences related to utilizing different Lisp dialects for programming projects.

### How to Distribute Postgres Geographically

#### [Submission URL](https://xata.io/blog/geo-distributed-postgres) | 94 points | by [mebcitto](https://news.ycombinator.com/user?id=mebcitto) | [11 comments](https://news.ycombinator.com/item?id=40542940)

In the blog post titled "Geographically distributed Postgres for multi-tenant applications" by Tudor Golubenco, a pattern for global distribution of Postgres databases for multi-tenant applications is detailed. The approach involves separating per-tenant data tables from control plane tables, placing tenant data in the region closest to users, and creating a global view using Postgres Foreign Data Wrappers (FDW) and partitioning.

The pattern is useful for applications with a global customer base due to reasons such as reducing latencies, complying with data residency laws, running apps on the edge, and enabling multi-region or multi-cloud setups. It works when data can be segmented by a key (e.g., customer id) into tenants and when each tenant has a natural affinity to a particular region.

The control plane data, including authentication and user tables, is kept in a centralized region, while tenant data is distributed based on affinity regions. This ensures that foreign keys point inside the tenant, allowing access to all tenant data by connecting to a single region. Authentication is done centrally, with subsequent requests directed to the region storing the relevant tenant data.

The blog post uses a Notion clone as an example to illustrate how pages data could be distributed based on tenant and region. By following this pattern, multi-tenant SaaS applications can efficiently manage data across multiple regions while maintaining centralized control and authentication.

The discussion on the blog post about geographically distributed Postgres for multi-tenant applications covered various aspects related to implementing and managing such systems.

1. **jtl and tdrg** discussed experimenting with sharded tenant-clustered tables using Postgres Foreign Data Wrappers (FDW) and the challenges of routing queries efficiently in a multi-region setup. They also mentioned the complexity and risks involved in altering the schema across multiple Postgres instances.

2. **Ozzie_osman** shared familiarity with AWS in the context of preparing sharded Postgres databases and questioned the approach detailed in the blog post.

3. **sgrlnd** raised concerns about the performance limitations of Aurora Serverless and the high costs associated with serverless approaches.

4. **nrpl** discussed the complexity of managing sharded Postgres databases, emphasizing the challenges in handling schema changes, balancing data, and preventing production outages. They highlighted the benefits of using smaller databases for better performance and operational efficiency.

5. **hzskll** provided a critical perspective on the manageability of shared data in Postgres using FDW, emphasizing the importance of consistent backups and mechanisms to handle schema changes across multiple regions. They also questioned the necessity of a distributed system and the transparency of its design for reliability.

6. **tdrg and hzskll** further discussed the challenges and considerations of designing systems for distributed data sharing, emphasizing the security, scalability, and performance implications of such architectures.

Overall, the discussion delved into the technical complexities, performance considerations, and operational challenges associated with implementing geographically distributed Postgres databases for multi-tenant applications.

### The quest to craft the perfect artificial eye through the ages

#### [Submission URL](https://www.popsci.com/health/artificial-eye-history/) | 32 points | by [chat](https://news.ycombinator.com/user?id=chat) | [3 comments](https://news.ycombinator.com/item?id=40548399)

The art of prosthetic eyes has a rich history dating back nearly 5,000 years, involving materials from tar to gold wires to PMMA. Glassblowing techniques from 16th-century Venetian artisans laid the foundation, with a shift to acrylic like PMMA during World War II due to a lack of German glass exports. Ocularists, the skilled artisans who craft these intricate eyes, pass down their expertise through generations, with a typical apprenticeship taking around five years.

The quest for realistic prosthetic eyes mirrors the importance of eye contact in human communication. Recent studies have shown that eye contact triggers important brain activities and plays a role in romantic attraction and social interactions. Motility, or the ability of the prosthetic eye to move in sync with the natural eye, is crucial for a natural appearance.

The evolution of prosthetic eyes took a significant step forward in the late 19th century with the introduction of orbital implants, allowing for more natural movement and function. Today, advancements in technology, such as 3D printing, continue to improve prosthetic eyes, enhancing their appearance, comfort, and functionality.

The discussion revolves around personal experiences with prosthetic eyes and advancements in the field. The original commenter, "Daub," shares their story of experiencing an odd feeling and adjustments after a workshop accident that resulted in their prosthetic eye. They mention how the artificial eye behaved differently from the natural eye, with slightly slower movements and limited vision. Despite the artificial eye being perfectly matched to the natural one, they still faced challenges in various situations, such as navigating traffic.

Another user, "acheong08," mentions hearing about scientific progress in connecting a camera directly to the brain. "Daub" then brings up the topic of artificial vision in terms of success for blind people but notes that it may still be a challenge for some individuals.

Overall, the discussion provides personal anecdotes and touches on the technological advancements in artificial vision systems.

### Re-Evaluating GPT-4's Bar Exam Performance

#### [Submission URL](https://link.springer.com/article/10.1007/s10506-024-09396-9) | 27 points | by [rogerkeays](https://news.ycombinator.com/user?id=rogerkeays) | [7 comments](https://news.ycombinator.com/item?id=40543535)

A recent paper has raised doubts about the reported 90th percentile performance of GPT-4 on the Uniform Bar Exam. The study highlights methodological challenges and presents findings that suggest OpenAI's estimates of GPT-4's performance may be overinflated. Data from different exam administrations indicate varying percentiles for GPT-4, with estimates ranging from below the 69th percentile to as low as the 15th percentile on essays for licensed or license-pending attorneys. The paper also questions the validity of GPT-4's reported essay score and explores the impact of hyperparameter combinations on its performance. These findings raise important considerations for the use of AI in legal tasks and underscore the need for rigorous evaluations to ensure the reliability of AI models like GPT-4.

The discussion on the submission revolves around skepticism regarding the reported performance of GPT-4 on the Uniform Bar Exam. Some users argue that the tests designed to discriminate between humans and AI may not accurately measure the abilities needed for legal practice, with one user emphasizing the importance of subjective interpretation in legal analysis. Another user points out discrepancies in GPT-4's reported 92nd percentile score, with findings indicating scores as low as the 15th percentile in specific sections. There is also a mention of the competitive nature of legal practice and the significance of passing the bar exam. Additionally, users question the credibility of GPT-4's essay scores and its ability to perform legal tasks effectively. One user expresses surprise at the lack of replication of the research and doubts the validity of the reported 90th percentile performance of GPT-4.

### Meta is using your posts to train AI. It's not easy to opt out

#### [Submission URL](https://mashable.com/article/meta-using-posts-train-ai-opt-out) | 30 points | by [belter](https://news.ycombinator.com/user?id=belter) | [3 comments](https://news.ycombinator.com/item?id=40548098)

Today on Hacker News, the spotlight is on Meta's AI models and their insatiable hunger for user data. The company's recent data sharing practices have raised concerns among users, especially those in Europe who received notifications about updates to the privacy policy as Meta introduces new generative AI features in the region. 

Meta's generative AI privacy policy specifies that it utilizes information shared on its platforms, such as posts, photos, and captions, to train its AI models. While private messages are not used for training data, users have expressed discomfort with the idea of their content being utilized in this way. Notably, users in the UK and EU have the option to opt out of sharing their data to train Meta's AI models, although the process is described as cumbersome and intentionally challenging to navigate. 

To limit the amount of data shared with Meta's AI models, users can explore options such as disconnecting specific sites that share information with Meta, clearing previous data activity, and managing future data sharing settings. Additionally, users can request access, deletion, or file complaints about personal information used by third parties to develop and enhance AI at Meta. However, there are limitations to these options, as requests are reviewed based on local laws, and fulfilling them is not guaranteed.

The evolving landscape of data privacy and AI utilization continues to spark discussions and considerations about the balance between convenience and privacy in the digital age.

The first user, "pjk," shared a link to a tool called FacebookClearActivityLog, which presumably helps users clear their activity log on Facebook.

The second user, "mrg2k8," seems to be discussing Prompt ChatGPT, mentioning that providing a full description of the tool requires knowing how much privacy or personal information individuals disclose and follow. They further mention processing impacts, additional information to help review objections, and posting results confirmed through the fast wonder people.

The user "7bit" intervenes in the conversation, mentioning writing various rights under GDPR and defining the position that they don't date AI training at Meta and mention that thousands of forms screening people falling from belief.

Lastly, "hole_in_foot" doesn't provide a clear comment, simply stating "dd."

The discussion covers a mix of tools, privacy concerns, GDPR rights, and opinions on AI training, adding depth to the conversation about Meta's data practices and privacy policies.

### Instagram will use users' content to train it's AI

#### [Submission URL](https://in.mashable.com/tech/76239/meta-is-using-your-posts-to-train-ai-its-not-easy-to-opt-out) | 8 points | by [nehagup](https://news.ycombinator.com/user?id=nehagup) | [5 comments](https://news.ycombinator.com/item?id=40546888)

Meta, the tech giant behind Facebook and Instagram, is using your posts to train its AI models, but opting out is not as easy as it seems. The company's data sharing practices have drawn attention, especially in Europe where users were recently notified about changes to privacy policies related to new generative AI features. This move has sparked concerns about privacy and data usage.

Meta's generative AI privacy policy reveals that it uses information shared on its platforms, such as posts and photos, to train its AI models. While private messages are not used for training, public content is fair game. Users in the UK and EU have the option to object to this data sharing, but the process is cumbersome and intentionally challenging to navigate, as noted by some users.

To limit what you share with Meta's AI models, one option is to delete your accounts, although that's a drastic measure. Users can also submit requests to access, delete, or file complaints about personal information used for AI development. However, the process has limitations and may not guarantee full opt-out.

Another avenue to safeguard your data is through the "activity off Meta" settings, where you can manage sites and apps sharing information with Meta and control future data sharing. Disconnecting and managing future activity can help prevent further data sharing with third parties.

Despite these measures, Meta's privacy settings primarily address sharing with third parties, leaving questions about internal data usage for training AI models. While the options may not offer a foolproof solution, they are steps towards regaining some privacy control. Clarifications from Meta on data usage internally are awaited to provide a clearer understanding of the situation.

The discussion around Meta using posts to train its AI models touches on various points. One user pointed out the challenge of opting out of this data sharing practice with Meta's AI, while another mentioned the difficulty in working around internet safety caveats when attempting to limit data sharing while still allowing AI training. The conversation also delved into the legal implications of scraping data and the relevance of Zuckerberg's comments today in light of the consequences it may bring. Additionally, there was a brief mention of corporate intellectual property, indicating the complexity of the situation regarding data usage and privacy with Meta's AI models.

