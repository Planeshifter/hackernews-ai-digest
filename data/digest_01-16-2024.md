## AI Submissions for Tue Jan 16 2024 {{ 'date': '2024-01-16T17:13:07.611Z' }}

### US developers can offer non-app store purchasing, Apple still collect commission

#### [Submission URL](https://www.macrumors.com/2024/01/16/us-app-store-alternative-purchase-option/) | 733 points | by [virgildotcodes](https://news.ycombinator.com/user?id=virgildotcodes) | [1038 comments](https://news.ycombinator.com/item?id=39020365)

In a recent update to its policies, Apple has allowed U.S. developers to direct customers to a non-App Store purchasing option for digital goods. This means that some apps in the U.S. storefront will soon feature a link to their website where subscriptions and other content can be purchased outside of the App Store's in-app purchase system, potentially at a discounted price. However, Apple will still collect a commission on these purchases, ranging from 12% to 27%, depending on the type of transaction. Developers who want to offer this option will need to apply for a StoreKit External Purchase Link Entitlement. The new policy aims to maintain the privacy and security of the App Store ecosystem while giving developers more flexibility in their pricing and purchasing options.

There are several discussions happening in the comments section of the submission. Here are some key points raised:

- Some users express frustration with Apple's 30% commission and argue that it is excessive and unfair.
- Others argue that Apple's commission is justifiable for the security and protection it provides in the App Store ecosystem.
- The debate touches on issues of market dominance, competition, and the role of large corporations.
- There is a discussion about the possibility of investing in Apple stock and its impact on retirement funds.
- Some users criticize Apple's practices, while others defend the company's success and argue that it is a result of its strategic marketing and product factors.
- Users share personal stories and experiences related to Apple's App Store policies, such as the rejection of certain apps and the impact on developers and publishers.
- The potential of alternative platforms, such as Android and PWAs (Progressive Web Apps), is discussed as an alternative to Apple's ecosystem.

Overall, the discussion covers a wide range of opinions on Apple's policies and their impact on developers, consumers, and the market as a whole.

### Web AI Model Testing: WebGPU, WebGL, and Headless Chrome

#### [Submission URL](https://developer.chrome.com/blog/supercharge-web-ai-testing) | 191 points | by [kaycebasques](https://news.ycombinator.com/user?id=kaycebasques) | [41 comments](https://news.ycombinator.com/item?id=39017607)

The Chrome for Developers blog has published a new post about automating browser testing with GPUs. The post focuses on the challenges faced by developers working with on-device machine learning models that require GPU capabilities. The article explains how to set up a consistent testing environment using tools like Google Colab, Puppeteer, and Chrome's WebGPU feature. It provides step-by-step instructions on how to enable GPU acceleration in Headless Chrome and overcome common issues related to GPU detection. The post aims to help web developers improve their application's performance and is also relevant for those working on web gaming or graphics.

The discussion on the Chrome for Developers blog post about automating browser testing with GPUs covers a range of topics related to machine learning, browser performance, memory limitations, and AI applications. Here are some notable points from the comments:

- One commenter shares a link to an article about the birth and death of JavaScript, suggesting that the use of GPUs in browser testing could be a step towards a different paradigm.
- There is a discussion about the performance difference between CPUs and GPUs in machine learning models and web applications. Some argue that GPUs significantly outperform CPUs for specific types of applications.
- The compatibility of training models with different GPU brands and Chrome is brought up as a potential issue.
- There is a debate about the popularity of TensorFlow and PyTorch among researchers, with some pointing out that PyTorch has gained popularity more recently.
- The discussion touches on alternative web platforms and their advantages over traditional web browsing.
- The memory limitations of Chrome, especially on mobile devices, are mentioned. One commenter questions whether there is a way to allocate more memory or if the limit is a deliberate choice.
- The recent update in Mobile Safari that increases the tab limit to 500 is mentioned. There is a discussion about the reasons for the limit and possibilities for increasing it further.
- Some commenters share their experiences with running large machine learning models and the memory efficiency of TensorFlow.js.
- The issue of browser crashes and memory restrictions when working with memory-intensive AI applications like Figma and video editing tools is discussed.
- The limitations of 4GB of memory in AI applications and the potential impact on using LLM are mentioned.
- SIMD support in WebAssembly for leveraging WebGPU is brought up as an expectation for future browser releases.
- There is a mention of the possibility of other browsers following Chrome's lead in supporting WebGPU for AI training and different types of models.
- The speed difference between CPU inference and GPU inference is discussed, with one commenter stating that CPU inference can be 10 times slower than GPU inference.

Overall, the discussion encompasses various technical aspects and considerations related to browser automation, machine learning, AI applications, and memory management.

### Clay Foundation Model: An open source AI model for Earth

#### [Submission URL](https://clay-foundation.github.io/model/) | 35 points | by [jasondavies](https://news.ycombinator.com/user?id=jasondavies) | [10 comments](https://news.ycombinator.com/item?id=39020175)

The Clay Foundation Model is an open-source AI model for Earth that utilizes a Vision Transformer architecture to understand geospatial and temporal relations in Earth Observation data. It is trained using a Self-supervised learning (SSL) approach with a Masked Autoencoder (MAE) method. The Clay model has three main applications: generating semantic embeddings for any location and time, fine-tuning for various downstream tasks like classification, regression, and generative tasks, and using it as a backbone for other models. The Clay model code can be found on GitHub, the model weights on Hugging Face, and the documentation on their website. They also have a set of embeddings available on Source Cooperative. Clay is a project of the non-profit Radiant Earth Foundation. The website madewithclay.org provides further information.

The discussion starts with a comment highlighting the lack of documentation about the Clay Foundation model's ability to process satellite images and understand spatial data. Another user responds by pointing out that the FAQ on the madewithclay.org website provides answers to questions about Clay's use of AI for satellite imagery and its applications in tasks like land cover mapping and carbon stock prediction. The conversation then shifts to a brief overview of Clay's training process using a self-supervised Masked Autoencoder method and its potential applications in pollution detection and deforestation monitoring. 
Another user mentions that they find the location embeddings particularly interesting and provides a link to more information about the specific embeddings used by Clay. They explain that the embeddings represent geospatial coordinates and time stamps and speculate that they could be used to compare vectors and categorize similar features related to specific locations.
One user expresses confusion about the benefits of using Clay compared to traditional data analysis methods, suggesting that it may be useful for tracking changes in land cover and allocating environmental justice funding. Another user simply comments with "nvsnd" which seems unrelated to the discussion.
Another user praises the project's website, mentioning that they enjoyed the landing page. However, another user responds that they find the website lacking practical application examples and that it reminds them of cryptic patches and startup accelerators. The FAQ claims that Clay provides pre-computed raw data analysis, API access, cutting-edge models, and benchmark tools, but the commenter cannot find any concrete examples on the website. They also mention difficulties in copy-pasting text from the website.

The conversation concludes with a user remarking that they couldn't find convincing material on Radiant Earth's website, a non-profit associated with Clay, suggesting that there may be a need for better information sharing.

### OpenAI drops ban on military tools to partner with The Pentagon

#### [Submission URL](https://www.semafor.com/article/01/16/2024/openai-is-working-with-the-pentagon-on-cybersecurity-projects) | 323 points | by [mfiguiere](https://news.ycombinator.com/user?id=mfiguiere) | [416 comments](https://news.ycombinator.com/item?id=39020778)

OpenAI, the creator of ChatGPT, has announced that it is collaborating with the Pentagon on software projects, including those related to cybersecurity. This marks a significant shift from OpenAI's previous ban on providing its AI technology to militaries. The company is also in talks with the U.S. government about developing tools to reduce veteran suicides. However, OpenAI has reiterated that it will maintain its ban on developing weapons. This change in policy has sparked concerns among AI safety advocates, who believe that integrating AI into warfare could come with profound risks due to the technology's tendency to "hallucinate" and generate fake information. The divide between data analysis and warfare may also become blurred, as exemplified by Ukraine's use of AI-powered software for artillery operations. The announcement has prompted discussions on the implications of collaborating with the military and raised questions about AI safety within OpenAI.

The discussion surrounding OpenAI's collaboration with the Pentagon on software projects has raised several concerns and viewpoints. Some users speculate about the loyalty of OpenAI's CEO, Sam Altman, after he signed a letter expressing support for the event, while others question the motivations behind OpenAI's change in policy. AI safety advocates highlight the risks of integrating AI into warfare, citing its tendency to generate fake information. The blurring of boundaries between data analysis and warfare, as seen in Ukraine's use of AI-powered software for artillery operations, further contributes to the concern. The discussion also touches upon issues of loyalty towards foreign entities, particularly regarding the Chinese government. Some users express concerns about AI capabilities being used by the military and the potential threat of misuse. Others support the idea of defense advancements but call for alignment with the company's principles. The role of AI in international conflicts and the potential for nuclear proliferation are also debated. Additionally, the discussion explores the definition of a thriving liberal democracy and its relevance to the topic at hand. There is a mention of Ukraine and its situation as well as a comparison to the situation in Taiwan. The mention of corruption within the political class and the role of propaganda is discussed in relation to Ukraine's democracy.

### The mechanical Bendix Air Data Computer, part 3: pressure transducers

#### [Submission URL](https://www.righto.com/2024/01/bendix-cadc-pressure-transducers.html) | 59 points | by [picture](https://news.ycombinator.com/user?id=picture) | [16 comments](https://news.ycombinator.com/item?id=39018106)

In this article, the author continues their series on the Bendix Central Air Data Computer (CADC), focusing on the pressure transducers used in the system. The CADC is an analog computer that was used in military planes to compute airspeed, Mach number, and other "air data". The pressure transducers are responsible for converting air pressure into shaft rotations, which are then used in the calculations performed by the CADC. The pressure transducers use a servo loop to amplify small pressure changes into accurate rotations, ensuring accurate readings. The article provides a detailed explanation of how the pressure transducers work and their role in the overall functioning of the CADC.

In the discussion, various points are raised about the Bendix Central Air Data Computer (CADC) and its maintenance. One commenter mentions that the Bendix CADC was not considered field-repairable at the Sacramento Air Logistics Center, and another user points out that Bendix manuals and Air Force Technical Orders cover repair and servicing of the equipment. There is a discussion on the maintenance intervals and the reliability of the computer, with one user mentioning that it is designed for a minimum of 1,000 hours of service before any major maintenance is required. Another user explains that the mean time between failures (MTBF) of the CADC is essentially the average time between malfunctions, which is usually longer if the system is serviced and tested properly. 

The author of the article asks some questions related to computer maintenance, and another commenter suggests looking into general-purpose hardware programming manuals or technical manuals specific to machines, especially those used in the navy. They also provide links to resources on mechanisms and machines. 
There is a discussion about the complexity of the Bendix CADC and its mechanical parts, with one user mentioning that the number of mechanical bits and pieces is impressive compared to solid-state systems. 
Another user shares their experience with older computers and the challenges of programming and maintaining them, as well as the importance of having functional inputs and outputs. 
There is a brief discussion about the design of the pressure transducers in the CADC, with one user suggesting that the housing of the transducers is pressurized and another mentioning that the standard reference pressure is applied to the transducers. 

A user expresses their appreciation for the article and commends the work of the original designers and manufacturers of the CADC. The author thanks them and notes the amazing technology and design available at the time, especially considering the vibration forces involved.

### V*: Guided Visual Search as a Core Mechanism in Multimodal LLMs

#### [Submission URL](https://arxiv.org/abs/2312.14135) | 57 points | by [jonbaer](https://news.ycombinator.com/user?id=jonbaer) | [4 comments](https://news.ycombinator.com/item?id=39016086)

Researchers Penghao Wu and Saining Xie have published a paper titled "V*: Guided Visual Search as a Core Mechanism in Multimodal LLMs" in the Computer Science > Computer Vision and Pattern Recognition section of arXiv. The paper addresses the lack of a visual search mechanism in current multimodal LLMs (Language-Conditioned Latent Models), which hinders their ability to focus on important visual details. The authors propose V*, an LLM-guided visual search mechanism that incorporates world knowledge in LLMs for efficient visual querying. When combined with an MLLM, this mechanism enhances collaborative reasoning, contextual understanding, and precise targeting of specific visual elements. The authors also introduce V*Bench, a benchmark designed to evaluate the ability of MLLMs to process high-resolution images and focus on visual details. The study highlights the necessity of incorporating visual search capabilities into multimodal systems. The code for V* is available on the project's page.

The discussion on this submission involves two comment threads. In the first comment thread, user "krmm" mentions a recent development in rendering for virtual reality and augmented reality. They suggest that adding 3D laptops with tracking data and machine learning systems can improve visual processing by incorporating human attention and perception. User "htrp" responds, stating that outside of Facebook, there are systematic data collection efforts. Then, user "krmm" adds a comment mentioning Apple's AI ambitions and how media campaigns will eventually roll out new hardware and that visual training data is supplementary but still important.

In the second comment thread, user "dbsh" finds the paper interesting and wonders if there are similar approaches for complex tasks in a multi-modal virtual environment such as working across multiple applications, web pages, input methods, and interactions. They suggest that a general approach closer to how humans work, like the mentioned paper, could be beneficial.

Overall, the discussion revolves around the potential applications of the proposed research on multimodal LLMs and raises questions about the feasibility of similar approaches in different contexts, such as virtual reality and working across multiple applications.

### Field experimental evidence of AI on knowledge worker productivity and quality

#### [Submission URL](https://www.oneusefulthing.org/p/centaurs-and-cyborgs-on-the-jagged) | 138 points | by [CharlesW](https://news.ycombinator.com/user?id=CharlesW) | [99 comments](https://news.ycombinator.com/item?id=39014521)

A recent study conducted by social scientists in collaboration with Boston Consulting Group suggests that AI is indeed a game-changer for the future of work. The study focused on the impact of AI on professional work and found that consultants using ChatGPT-4 outperformed those who did not on a range of tasks. The consultants using AI completed tasks more quickly, finished more tasks on average, and produced higher quality results. However, the study also highlighted the complexity of AI, with some tasks being more suited to AI capabilities than others. The researchers referred to this as the "Jagged Frontier" of AI, where the difficulty of tasks varies. Overall, the study provides compelling evidence for the transformative effects of AI on professional work.

The discussion about this submission on Hacker News revolves around various aspects of AI and its impact on different industries and professions. Here are some key points from the discussions:

1. AI's Impact on Professional Work: The study provides evidence that AI, specifically ChatGPT-4, can significantly improve the performance of professionals. Consultants using AI completed tasks more quickly, produced higher quality results, and finished more tasks on average. However, the complexity of AI means that some tasks are better suited for AI capabilities than others.
2. Complexity of AI: Some commenters discuss the "Jagged Frontier" of AI, which refers to the varying difficulty levels of tasks that can be performed by AI. Certain tasks may require higher AI capabilities, while others may not be suitable for AI at all. This highlights the need for further research and refining AI capabilities.
3. Skill Development: Some comments discuss the importance of skills and practice in the field of software engineering. While AI can enhance certain tasks, it is not a substitute for developing skills and gaining experience in complex programming and hardware-related applications.
4. Undefined and Indeterminate Behavior: The discussions touch upon the challenges of AI systems with undefined or indeterminate behavior. It is important to ensure that AI models do not produce incorrect or ill-defined outputs, especially in critical applications like self-driving cars or aviation.
5. Potential of AI and Human Collaboration: A few commenters highlight the potential of AI-human collaboration, where AI systems assist professionals in performing tasks more efficiently. They emphasize that AI should be viewed as a tool that enhances human capabilities rather than replacing them entirely.
6. Limitations of AI: Some commenters express skepticism about the transformative effects of AI in all aspects of professional work. They argue that AI might not be able to fully replace skills and expertise in certain areas, and human intervention and decision-making are still crucial.

Overall, the discussions provide interesting insights into the complex dynamics between AI and professional work, highlighting the potential benefits, challenges, and limitations of incorporating AI into various industries.

### Many AI safety orgs have tried to criminalize currently-existing open-source AI

#### [Submission URL](https://1a3orn.com/sub/machine-learning-bans.html) | 377 points | by [sroussey](https://news.ycombinator.com/user?id=sroussey) | [370 comments](https://news.ycombinator.com/item?id=39009779)

In an essay-style post on Hacker News, the author discusses their concerns regarding certain AI safety organizations and their efforts to criminalize existing open-source AI models. The author claims that many of these organizations have advocated for bans that would restrict or ban the open-sourcing of current AI models.
One organization mentioned is the Center for AI Safety, which proposed regulatory rules defining "powerful AI systems" and subjecting them to certain requirements that would effectively ban open-sourcing them. Another organization, the Center for AI Policy, initially proposed rules that would regulate already-released AI models. In a recent interview, they mentioned that models trained with a certain number of FLOPs or achieving a specific benchmark score would be banned from being open-sourced.
The author also mentions Palisade Research, a non-profit organization that aims to demonstrate dangerous capabilities of AI to inform policymakers about potential risks. They have called for governmental action to prevent the release of certain AI models.

The author expresses concern about the potential harm these bans could cause, as open source has been important for AI safety work. They argue that similar arguments for future bans would be likely to have harmful consequences. However, they acknowledge that not all AI safety organizations or individuals share these views.
Overall, the article highlights concerns about the potential impact of banning open-source AI models and emphasizes the importance of open source for AI safety research.
The discussion on Hacker News revolves around the points raised in the essay-style post about banning the open-sourcing of AI models. Some users argue that AI safety organizations advocating for bans are hypocritical since they have called for AI to be banned while also developing powerful AI systems themselves. Others point out that the risks associated with AI and AGI development warrant regulations to prevent potential harm.
There is a debate about the validity of comparing AI safety concerns to nuclear weapons, with one user arguing that the two are not equivalent as nuclear weapons have been extensively tested and their risks are well-known. Another user argues that the concerns raised by AI safety researchers are not exaggerated and that regulations are necessary to prevent existential risks.
The discussion also touches upon the intentions of companies building AI, with some users highlighting that companies are declaring their attempts to create AGI while acknowledging the potential dangers. One user adds that trying to develop AGI is akin to building nuclear bombs, and concerns about AI x-risks are similar in nature.
In response to the argument that AGI development is unlikely, another user states that there is a plausible path to AGI and highlights various reasons such as the capabilities of smart people, investment in AI development, and recent advancements in AI systems. However, there is skepticism from users who believe that the achievements mentioned are not as impressive as suggested.

Overall, the discussion presents a range of perspectives regarding the potential risks of open-sourcing AI models and the need for regulations in AI development. Some users argue for caution, while others express skepticism about the severity of the risks and the possibility of AGI development.

### Raspberry Pi Compute Module 5 Is in Development

#### [Submission URL](https://www.tomshardware.com/raspberry-pi/raspberry-pi-compute-module-5-confirmed-by-ceo-eben-upton) | 35 points | by [WithinReason](https://news.ycombinator.com/user?id=WithinReason) | [24 comments](https://news.ycombinator.com/item?id=39015200)

Raspberry Pi CEO Eben Upton confirmed in an interview with YouTube channel Raspberry Pi YouTuber Jeff Geerling that production for the Raspberry Pi 5 is ramping up. Additionally, Upton unintentionally confirmed that the Raspberry Pi Compute Module 5 is also in the works. While no release date has been confirmed, Upton mentioned that the company has released guidance for designing products based on the upcoming board. The Compute Module 5 is expected to retain the dual connectors introduced with the Compute Module 4, indicating that boards designed around the CM4 should be compatible. The CM5 is also likely to offer similar configuration options such as RAM, flash storage, and Wi-Fi as the previous model.

The discussion on Hacker News mainly revolves around the technical aspects of designing and assembling PCB boards for the upcoming Raspberry Pi Compute Module 5 (CM5). Users discuss the difficulty of PCB design, including aspects such as differential pairs, transmission line theory, impedance matching, and using tools like KiCAD for design calculations. The discussion also touches on the affordability and accessibility of PCB design tools, the benefits of having an EE degree, and the evolving nature of computing terminology. Some commenters express their frustration with the Raspberry Pi's focus on the hobbyist market rather than industrial-grade products. One commenter mentions using the Compute Module platform for cloud computing and management needs. Another commenter recommends the use of the Compute Module for Home Assistant projects.

### Chromes new Incognito warning admits Google collects your data in "private" mode

#### [Submission URL](https://arstechnica.com/tech-policy/2024/01/chrome-updates-incognito-warning-to-admit-google-tracks-users-in-private-mode/) | 37 points | by [schalkneethling](https://news.ycombinator.com/user?id=schalkneethling) | [5 comments](https://news.ycombinator.com/item?id=39018380)

Google is updating the warning message for Chrome's Incognito mode to clarify that user data can still be collected by Google and other websites. The move comes ahead of a settlement in a class-action lawsuit against Google for privacy violations related to the private browsing mode. The updated warning, currently available in Chrome Canary, specifies that while others using the same device won't see browsing activity, data can still be collected by websites and services, including Google. The change aims to educate users who may not understand the limitations of Incognito mode. It is not yet clear when the updated warning will be rolled out to the stable version of Chrome.

The discussion surrounding the submission on Hacker News seems to have mixed opinions. One user, "jqpabc123," commented on the practical purpose of Google's privacy investigation. On the other hand, "evilDagmar" criticized Google's secretive website visits and expressed concerns about the tracking capabilities of Incognito mode. Another user, "hyn," pointed out that the original article's argument seemed to be missing some relevant points. "extraduder_ire" noted that the screenshot of the warning in the article did not provide much helpful information. Lastly, "dfplygn" simply commented that the news was expected.

### Google lays off "hundreds" more as ad division switches to AI-powered sales

#### [Submission URL](https://arstechnica.com/google/2024/01/google-lays-off-hundreds-more-as-ad-division-switches-to-ai-powered-sales/) | 31 points | by [eysquared](https://news.ycombinator.com/user?id=eysquared) | [3 comments](https://news.ycombinator.com/item?id=39019965)

Google is reportedly laying off "hundreds of employees" from its ad sales division, specifically in the "Large Customer Sales" (LCS) team. These job cuts are said to be a result of AI technologies replacing human workers. Google has been incorporating generative AI features into its Google Ads product, such as a natural-language chatbot to assist with ad navigation and a system that automatically generates ad assets. The AI ad system, called "Performance Max," remixes and tweaks ads based on click-through rates. This move is part of a larger trend of layoffs at Google, with recent cuts in the hardware, Google Assistant, and AR divisions. The layoffs are expected to be the beginning of a wave of job cuts in the industry due to the rise of generative AI.

The discussion on this article seems to be focused on the negative consequences of Google's job cuts and the impact of AI on customer support. One commenter criticizes Google's customer service, stating that it has been getting worse and provides a link to an article supporting this claim. They express frustration with Google's reliance on AI systems and the waste of resources spent on keyword tracking rather than focusing on conversions. Another commenter shares their dissatisfaction with Google's customer support and believes that Google is losing control and wasting time and money on unnecessary tools. They also mention wanting more transparency in how Google handles customer support. Another commenter simply mentions that the managers are to blame, while the final commenter sarcastically adds to the conversation.

