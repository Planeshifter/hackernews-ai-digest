## AI Submissions for Wed Aug 30 2023 {{ 'date': '2023-08-30T17:10:19.632Z' }}

### Show HN: I automated half of my typing

#### [Submission URL](https://github.com/eschluntz/compress) | 738 points | by [eschluntz](https://news.ycombinator.com/user?id=eschluntz) | [298 comments](https://news.ycombinator.com/item?id=37326870)

"Compress" is a tool that automates the process of creating keyboard shortcuts from a corpus of your own writing. It analyzes the text to find common phrases that can be replaced with shorter abbreviations, and then generates configuration files for Autokey, a linux program that implements keyboard shortcuts. The tool also includes an optional feature to parse a Slack Data Export of your messages and create a corpus from it. The suggested abbreviations are ranked by the amount of characters saved multiplied by the frequency of the phrase. The tool aims to generate memorable abbreviations using heuristics and preferences. It provides instructions on how to install and use the tool, and offers customization options for adding or editing shortcuts. Overall, "Compress" simplifies typing by reducing the need to type out frequently used phrases.

The discussion around the submission "Compress" includes various comments and suggestions from the Hacker News community. 

One user mentions that there is already a built-in Chinese language typing system called Shuangpin, which allows users to type English characters that correspond to whole words in Chinese. The user also suggests that there are other typing systems and shorthand methods available for different languages.

Another user raises concerns about the tool's longevity and suggests that a universal version could be more useful. They propose that the tool should be able to process books, emails, text messages, and other sources of diverse backgrounds and contexts to benefit a wider range of people.

A discussion thread also explores the limitations of shorthand systems and abbreviations. One user points out that common shorthand systems, like Evans Basic English Code or Phillips Code, are often insufficient in practice. Another user mentions that certain abbreviations, like dictionary abbreviations or abbreviations for frequently used words, can be considered.

There are also comments discussing the practicality of using the tool on Android phones, with some users mentioning the difficulties they face while typing and suggesting alternative approaches to improve typing speed.

Some users share their experiences with typing speed and discuss various factors that can impact typing efficiency. The discussion touches on factors such as keyboard ergonomics, typing techniques, and the benefits of specialized keyboards or typing systems. The topic of stenography is also brought up, with users recommending the Open Steno Project as a resource for those interested in shorthand and high-speed typing.

Overall, the discussion revolves around the potential benefits and challenges of the "Compress" tool, as well as broader topics related to typing efficiency and alternative typing methods.

### High-Speed AI Drone Overtakes World-Champion Drone Racers

#### [Submission URL](https://www.news.uzh.ch/en/articles/media/2023/Drone-race.html) | 275 points | by [geox](https://news.ycombinator.com/user?id=geox) | [167 comments](https://news.ycombinator.com/item?id=37323834)

Researchers from the University of Zurich and Intel have achieved a major milestone in artificial intelligence by creating an autonomous system capable of beating human champions at a physical sport: drone racing. The AI system, called Swift, won multiple races against three world-class drone racing champions. Swift was trained using reinforcement learning in a simulated environment, where it taught itself to fly by trial and error. The drone flies autonomously in real time, reacting to data from an onboard camera, just like human racers. Swift achieved the fastest lap overall, half a second ahead of the best lap by a human pilot. However, human pilots proved to be more adaptable to changing conditions. The researchers believe that pushing the envelope in autonomous flight is important not only for drone racing, but also for applications such as forest monitoring, space exploration, and rescue missions.

The discussion on this submission covers various aspects of the achievement of the AI system Swift beating human champions in drone racing. Some commenters discuss the technical aspects of training the AI using reinforcement learning and the complexity of modeling the dynamics of drones. Others highlight the challenges of simulating real-world physics accurately and the importance of learning from real-world data. There is also a mention of past achievements in robotics and AI in drone racing, as well as some humor about AI developing winning AI pilots. The limitations of the AI system are also mentioned, such as localization and external system mapping. Overall, the discussion shows excitement about the progress made in autonomous drone racing.

### The Tera Computer System (1990)

#### [Submission URL](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=108b3a24274af0aab079bd94ab7c1ee6543563d4) | 44 points | by [twoodfin](https://news.ycombinator.com/user?id=twoodfin) | [14 comments](https://news.ycombinator.com/item?id=37316280)

I'm sorry, but I cannot interpret this PDF file. Could you please provide me with the text version of the top stories on Hacker News?

Discussion Summary:

1. User "jndrwrgrs" comments on the submission, sharing their experience of working on software generation versions and how the architecture of the Cray XMT affected software design. They mention that software historically had little attention to approximate latency hiding silicon, but recently there have been advancements in designing silicon dedicated to specific customers, especially in the field of graph analytics. They also refer to John von Neumann and his attempt to develop a solution similar to dataflow architecture for improving program performance.

2. User "jcqsm" responds to "jndrwrgrs" and mentions that while modern silicon increasingly adopts different architectures, there are still limitations to its usefulness. They refer to the concept of mixing different programming models (such as procedural, object-oriented, functional, etc.) to increase flexibility and potential benefits in silicon design.

3. User "nvt" agrees with the solution proposed by "jcqsm" but expresses doubts about its generalization.

4. User "jcqsm" provides a link to an article on dataflow architecture and explains that it is typically implemented in specialized-purpose hardware.

5. User "jcl" shares their experience meeting a professor who was implementing dataflow architecture in the late 1980s and early 1990s. They mention that dataflow machines were used to convert x86 code to dataflow graphs and execute it. They also mention a company called Microsoft that demonstrated Windows running on a dataflow processor.

6. User "JonChesterfield" adds to the discussion, stating that GPUs are close to implementing dataflow architecture. They suggest that GPUs have a software support for deterministic memory latency and a runtime scheduler. They also mention that GPUs execute driven by memory access patterns.

7. User "jcqsm" responds to "JonChesterfield" and mentions that while GPUs converge towards dataflow, they are not universally called dataflow computers. They highlight the fundamental difference between SIMD (Single Instruction, Multiple Data) architecture and dataflow, mentioning that SIMD has bottlenecks and limitations, while dataflow computers can perform tasks inherently in parallel.

8. User "cnvlvtrn" shares their experience working on systems that had limited single-thread performance. They mention working on MTA (Massively Threaded Architecture) systems which required synchronizing bits and found it difficult to perform on integrated systems.

9. User "k" admits that they couldn't figure out what a barrel processor is and shares a link to a comment that explains it.

10. User "cnvlvtrn" mentions that they couldn't find a document called "Principles of Operation" but offers to reach out for more information.

Overall, the discussion revolves around dataflow architecture, its implementation in various hardware systems, and the potential benefits and challenges associated with it. Users share their experiences, refer to historical attempts, and discuss the implications of modern technologies like GPUs.

### ChatLZMA – text generation from data compression

#### [Submission URL](http://pepijndevos.nl/2023/07/15/chatlmza.html) | 117 points | by [bschne](https://news.ycombinator.com/user?id=bschne) | [18 comments](https://news.ycombinator.com/item?id=37318810)

A programmer on Hacker News shared an interesting idea on building ChatGPT using data compression techniques. They proposed compressing a large corpus of text to create an encoding table, compressing a prompt along with some random data, and then decompressing the random data to generate a response. While the approach may seem unconventional, the initial results were surprisingly amusing, producing coherent-ish words in mere seconds of "training." The programmer shared some code snippets in Python showcasing their progress so far. Although they are unsure whether the generated words are accidental prompts or just the result of flushing the buffer, the output is intriguing nonetheless. This experiment highlights the potential for leveraging compression algorithms in natural language processing tasks.

The discussion on the Hacker News submission revolves around the idea of using data compression techniques to build ChatGPT, a conversational AI model. Some comments express skepticism about compressing text and the effectiveness of the approach. Others mention similar compression-based approaches in different domains such as gaming FAQs and network data. 

One commenter suggests building a context-token probability table using SQLite and suggests the use of prediction and learning algorithms to improve efficiency. Another commenter introduces the concept of Weighted Finite State Transducers for speech recognition. There is also a mention of GPT-40, a model that uses base64 compression as a potential starting point for implementing compression in ChatGPT.

Some commenters share related resources, including a paper on Parameter-Free Classification Method Compressors and a YouTube video demonstrating simple implementations. Others express interest in exploring the idea further, with one commenter mentioning the possibility of applying the approach to Gulliver's Travels or Finnegans Wake.

Overall, the discussion explores the potential benefits and challenges of incorporating data compression techniques into ChatGPT, with varying levels of enthusiasm and skepticism.

### Designing deep networks to process other deep networks

#### [Submission URL](https://developer.nvidia.com/blog/designing-deep-networks-to-process-other-deep-networks/) | 69 points | by [weird_science](https://news.ycombinator.com/user?id=weird_science) | [9 comments](https://news.ycombinator.com/item?id=37328609)

Researchers have explored the idea of designing deep neural networks (DNNs) that can process the weights of other neural networks, enabling them to perform operations on pretrained models. This concept has practical applications, such as editing 3D objects represented using Implicit Neural Representations or adapting image classifiers to different domains. Previous work has attempted to process deep weight spaces by vectorizing the parameters and applying a fully connected network. However, this approach overlooks the structural properties of neural networks and hurts generalization. To address this, researchers propose using deep architectures that are insensitive to transformations of model weights, similar to how convolutional neural networks are insensitive to image shifts. The study draws upon the field of Geometric Deep Learning, which focuses on learning objects while being invariant to transformations, to develop equivariant architectures. These architectures enable neural networks to effectively process the weights of other neural networks, creating opportunities for more efficient and flexible model operations.

The discussion on this submission begins with a comment by "Culonavirus" stating that the idea of designing deep neural networks that can process the weights of other neural networks is interesting. They mention that Nvidia's recent projects are pushing the boundaries of reconstructing 3D objects using deep learning.

In response, "gbrsr" expresses their surprise at Nvidia's hardware and software collaboration, suggesting that Nvidia is leveraging its strong chips for efficient computation, which they find impressive.

Another user, "rvz," comments that while the post explains the lack of technical questions in the responses, it takes experiments and reimplementation of the findings in papers to fully understand the research. They note that posting non-code explanations generates little interest.

"Pzz" finds the perspective interesting and connects it to a method called "Dynamic Dispatch Type Overloading" in programming. They provide an example to illustrate their point and mention their contribution in implementing a library for optimizing overall distortion in approximation.

"Lbbt" suggests that Nvidia's management is doubling down on AI chips, leading to exponentially increasing demand.

A user with the handle "wht-n-tsts" references the novel "Neuromancer" and its antagonist "Wintermute" in their comment.

"Blammar" responds to "wht-n-tsts" by suggesting that controlling chips that can self-assemble, like those described in the book, has been a long-standing desire for humans. They mention the challenges of self-manufacturing in vacuum tube systems and their resilience in space environments.

Overall, the discussion touches on Nvidia's projects, the implementation challenges, and references to sci-fi novels.

### STM32 Blue Pill as an Hid USB Keyboard

#### [Submission URL](https://www.instructables.com/STM32-As-HID-USB-Keyboard-STM32-Tutorials/) | 42 points | by [thealienthing](https://news.ycombinator.com/user?id=thealienthing) | [31 comments](https://news.ycombinator.com/item?id=37324687)

Ankit Ghevariya has written a detailed tutorial on how to create a HID USB keyboard using STM32, specifically the STM32F103C8T6 board. He breaks down the whole application into smaller parts, modifying the STM32 USB HID code generated with STM32CubeMx. The tutorial covers all the necessary steps, from setting up the project in CubeIDE to making code changes and development. Ankit provides clear instructions and screenshots to guide readers through the process. This tutorial is perfect for anyone interested in learning how to create a DIY USB keyboard using STM32.

The discussion on this submission covers various topics related to the tutorial on creating a DIY USB keyboard using STM32. Here are the key points:

- There is a discussion about the availability and pricing of different STM32 boards. One user mentions that the NUCLEO-F334R8 board is not available despite being recently bought, while another user suggests purchasing boards directly from Mouser.

- There is a conversation about the fluctuating prices of Raspberry Pis and how they have now become more affordable.

- The cost of components like crystal oscillators is also brought up in the discussion.

- One user appreciates the tutorial and mentions that they have started exploring MCU programming recently. They highlight the importance of understanding the steps and tweaks involved in developing custom firmware.

- Different STM32 boards like Blue Pills and Black Pills are discussed, along with their popularity among DIY mechanical keyboard enthusiasts. It is noted that these boards have relatively full-featured USB client support.

- The topic of Windows HID interface flexibility is brought up, with mention of the possibility of using VendorIDProductID for driver assignment.

- The idea of using custom lighting devices and building a system by interfacing controller ICs with USB is discussed.

- Some users comment on the nature of the tutorial, with one user mentioning that it is a tutorial for the STM32CubeMx development environment and provides examples to help beginners navigate complexity.

- The potential difficulties of creating a DIY rubber ducky keyboard and bridging a dumb keyboard to a smart keyboard are discussed.

- The compatibility of DIY keyboards with different platforms, such as Bluetooth support with QMK, is mentioned.

- There is a comment about the potential blogspam nature of the submission, but it is clarified that the linked articles are not considered spam.

- Some users discuss the constraints and issues related to working with USB keyboards on certain systems.

- The availability of resources for developing USB keyboards is also mentioned, along with the development of keyboard-related Android apps.

- One user talks about their experience with a similar Rust project.

### Show HN: Open Interpreter – CodeLlama in your terminal, executing code

#### [Submission URL](https://openinterpreter.com/) | 61 points | by [killianlucas](https://news.ycombinator.com/user?id=killianlucas) | [21 comments](https://news.ycombinator.com/item?id=37315866)

Introducing Open Interpreter, the innovative tool that takes computer usage to a whole new level. Developed by LLMs, this software allows you to run code on your computer and effortlessly complete tasks. With Open Interpreter, possibilities are endless, and no coding challenge is too intricate. Want to delve deeper into this groundbreaking technology? Check out the GitHub page and watch the captivating video demo. Discover how Open Interpreter can revolutionize your computer experience!

The discussion about the submission "Introducing Open Interpreter" on Hacker News starts with a user named "kllnlcs" providing a brief summary of the Open Interpreter CLI tool developed by LLMs. Another user named "jwrty" congratulates the developers and offers feedback, while "kllnlcs" confirms that code written in Open Interpreter can be easily stopped without interrupting the code generation. 

Another user, "kdnnj," tries out Open Interpreter and asks a question. "kllnlcs" replies that it works well and suggests checking out the GitHub repository for more information. 

"user quickthrower2" expresses appreciation for the tool and asks a question about its compatibility with different operating systems. "kllnlcs" responds that Open Interpreter can run on Mac, Windows, Linux, and provides examples of system information commands for each.

A user named "Multiplayer" comments on the fantastic video demonstration of Open Interpreter, and "kllnlcs" suggests using the tool in combination with Rotato for even more possibilities.

"d4rkp4ttern" appreciates the aesthetics of the video and suggests creating a website with a larger font size for a better user experience. "kllnlcs" thanks them and shares a link to a custom-built Tailwind CSS framework for Open Interpreter.

A user named "redindian75" praises the video, and "kllnlcs" thanks them for the positive feedback.

"user pdrvhb" shares their thoughts on the interactive nature of Open Interpreter and its potential to enhance the debugging process. They mention using IPython as a comparison and discuss the challenges of implementing interactive interpreters for models.

"pplonski86" suggests checking out the GitHub repository and blog posts related to Open Interpreter. They also express interest in notebook integration.

"kllnlcs" acknowledges the idea of notebook integration and how it can be a significant future feature for Open Interpreter.

This concludes the summarized discussion on Hacker News about the submission "Introducing Open Interpreter."

### ThirdAI Uses Ray for Parallel Training of Billion-Parameter NN on Commodity CPUs

#### [Submission URL](https://www.anyscale.com/blog/how-thirdai-uses-ray-for-parallel-training-of-billion-parameter-neural-networks-on-commodity-cpus) | 77 points | by [thirdailab](https://news.ycombinator.com/user?id=thirdailab) | [15 comments](https://news.ycombinator.com/item?id=37325173)

In a blog post by ThirdAI, they share how they utilize the Ray framework for distributed training of deep learning and foundational models using only CPUs. ThirdAI is an early-stage startup dedicated to democratizing AI models by training and deploying large neural networks on commodity CPU hardware. Their proprietary BOLT engine, a new deep learning framework, allows their sparse deep learning models to outperform dense architectures on GPUs in both training time and inference latency. To scale their models to terabyte-scale datasets and billion-parameter models, they have built a distributed data parallel engine powered by Ray. They discuss how Ray enables them to build an industry-grade distributed training solution with features such as fault-tolerance, multiple modes of communication, and seamless scalability. They also highlight their recent migration from Ray Core to Ray Trainer, which provides a simplified developer experience and enhanced fault tolerance and automatic scaling. With Ray, ThirdAI is able to achieve near-linear scaling for distributed training on a popular terabyte-scale benchmark dataset.

