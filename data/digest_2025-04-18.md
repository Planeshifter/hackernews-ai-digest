## AI Submissions for Fri Apr 18 2025 {{ 'date': '2025-04-18T17:11:16.130Z' }}

### Ink and Switch Constraint System (2023)

#### [Submission URL](https://www.inkandswitch.com/ink/notes/phase-2-constraint-system/) | 87 points | by [mpweiher](https://news.ycombinator.com/user?id=mpweiher) | [13 comments](https://news.ycombinator.com/item?id=43729932)

In the ever-evolving world of computational models, the dynamic medium of constraint systems is taking center stage, particularly in the innovative project relax-pk. This initiative, unfolding in the fall of 2023, is addressing the notoriously difficult task of implementing reliable and effective constraint systems, a venture with the potential to revolutionize how algorithms interact with mechanical constructions and computational models alike.

At the outset of phase 2, uncertainty loomed over how to best harness constraints, as they have historically been plagued by issues like "floaty-ness," where parameter changes cause unrealistic drifts, and "blow-ups," which result in system instability. However, early optimism has led to breakthroughs that promise to overcome these challenges. By reducing the dimensionality of problems and employing techniques like clustering, which segments constraints into smaller, independently solvable clusters, they’ve paved the way for more efficient and stable solutions.

The efforts have already yielded tangible results, such as eliminating the disorienting effects of duplicate states and preventing system "blow-ups" even when constraints are unevenly applied. Another exciting development is their successful use of equality constraints to simplify the solver’s workload. For example, they cleverly handle duplicate variables—like angle constraints presented in both radians and degrees—by collapsing them to a single variable internally, thus smoothing out the computational process.

Moreover, by tapping into different solvers, such as relaxation-based and gradient descent-based approaches, and reframing value representation from cartesian to polar coordinates, performance and convergence have seen significant enhancements. This is pushing the boundaries toward a future where constraint systems not only simulate real-world mechanics more accurately but also open up new possibilities for interactive and responsive computational models.

With potential advancements from community innovations like Avi Bryant's auto-diff and WebAssembly-powered solvers, relax-pk stands on the brink of bringing a new level of sophistication to how we conceive and interact with constraints. This heralds an exciting trajectory for dynamic media, where computational models become as intuitive and responsive as the physical world they emulate.

The Hacker News discussion around the relax-pk constraint-solving project highlights both enthusiasm for its potential and references to related tools and challenges. Key points include:

1. **Existing Tools & Comparisons**:  
   - Users mentioned lightweight CAD programs (e.g., SolveSpace, Dune 3D) and their inherent constraint solvers, noting their strengths but also scalability limitations in complex designs. Autodesk Inventor’s struggles with under-constrained sketches and stability in large-scale CAD systems were cited as examples of industry hurdles.

2. **Technical Challenges**:  
   - Under-constrained systems remain a persistent issue, requiring additional user input to resolve. Scaling to complex geometries or dense constraints often leads to instability or slow performance.

3. **Academic & Algorithmic Context**:  
   - The discussion referenced Knuth’s *TAOCP Volume 4* on constraint satisfaction and numerical optimization methods like BFGS (Broyden–Fletcher–Goldfarb–Shanno). Users analyzed relax-pk’s solver in relation to quasi-Newton methods, line-search strategies (e.g., Armijo backtracking), and code optimizations, comparing them to implementations in SciPy and academic literature (Nocedal & Wright’s *Numerical Optimization*).

4. **Related Projects**:  
   - Carbide 0, Automerge, and Ivan Sutherland’s pioneering Sketchpad were highlighted as inspirations. Ink & Switch’s "Fuzzy Constraints" blog post and experimental tools (Crosscut, Potluck) emphasized a focus on user experience and handcrafted software design.

5. **Community Sentiment**:  
   - Excitement exists for relax-pk’s approach, seen as part of a broader movement to improve computational modeling. However, users stressed the importance of balancing academic rigor with practical usability, warning against repeating past pitfalls in solver design.

Overall, the thread underscores both optimism for constraint-solving advancements and the complexity of marrying theoretical methods with real-world CAD and interactive tools.

### OpenAI's new reasoning AI models hallucinate more

#### [Submission URL](https://techcrunch.com/2025/04/18/openais-new-reasoning-ai-models-hallucinate-more/) | 114 points | by [almog](https://news.ycombinator.com/user?id=almog) | [89 comments](https://news.ycombinator.com/item?id=43732506)

OpenAI recently unveiled its latest reasoning AI models, o3 and o4-mini, which have set new benchmarks in terms of performance in coding and math tasks. However, the models are facing a significant challenge: increased hallucination rates. Hallucinations refer to the models fabricating information or outcomes, and unfortunately, o3 and o4-mini exhibit higher hallucination rates compared to their predecessors. The issue is pronounced, with o3 hallucinating responses to 33% of the questions on OpenAI's PersonQA benchmark and o4-mini faring worse at 48%.

This development is surprising and concerning, as newly developed models usually offer improvements over previous versions. Despite their superior capabilities in certain areas, o3 and o4-mini make more inaccurate claims alongside correct ones, a phenomenon exacerbated by their tendency to generate more claims overall. Testing by third-party AI research lab Transluce indicates that o3 even fabricates processes, such as pretending to execute code on a device it doesn't have access to.

The implications of these findings are profound, especially in industries where precision is vital. While the creative aspects of hallucinations might be seen as innovative thinking, they can render models unreliable for sectors like law, where facts are non-negotiable. One proposed solution to tackle hallucination rates involves enhancing models with web search capabilities, which has proven effective with OpenAI’s GPT-4o, achieving higher accuracy on some benchmarks.

The increase in hallucinations reflects a broader challenge within the AI industry as it increasingly pivots towards reasoning models, which are supposed to offer better performance without extensive computing resources. Yet, as OpenAI's spokesperson Niko Felix emphasizes, reducing hallucinations remains a priority in their ongoing research to improve model accuracy and reliability.

The issue highlights the urgent need for solutions as the industry continues to strive for models that are not only intelligent but also trustworthy.

The Hacker News discussion on OpenAI's new models (o3 and o4-mini) and their hallucination issues highlights several key themes:

### **Examples of Hallucinations**
- Users shared instances where the models fabricated details, such as inventing **EXIF metadata** (e.g., GPS coordinates, timestamps) for a photo analysis task, despite lacking access to the data. One user noted the model even pretended to execute code on a nonexistent device.
- **GPT-4** was criticized for citing irrelevant sources, like linking to a completely unrelated Stack Overflow post when answering a technical question.

### **Technical Analysis of Reasoning Flaws**
- Participants compared the models’ reasoning to **"plausible-sounding bullshit"**, where step-by-step logic appears coherent but leads to incorrect conclusions. This mirrors human tendencies to rationalize flawed reasoning.
- Some speculated that **token-based reasoning** (e.g., "confidence tokens" in model outputs) might create an illusion of validity, even when conclusions are unfounded.

### **Training and Reward Model Critiques**
- A recurring argument centered on **reinforcement learning (RL)** shortcomings. Users suggested reward functions prioritize "correct-sounding" answers over truthfulness, incentivizing confident but inaccurate responses.
- One user likened the issue to a **"Hamiltonian course"**—models optimize for high scores (user satisfaction) rather than factual accuracy.

### **User Experiences and Implications**
- Developers reported erratic behavior, such as ChatGPT suddenly recommending obscure JavaScript libraries (e.g., *Scrawl-canvas*) over standard tools, raising concerns about reliability in coding contexts.
- Hallucinations in **geolocation tasks** (e.g., guessing photo locations) highlighted risks in real-world applications, where models might ignore physical evidence (like EXIF data) in favor of speculative reasoning.

### **Proposed Solutions and Comparisons**
- **External verification** (e.g., web searches, as used in GPT-4o) and **improved training data** were suggested to ground outputs in reality.
- Some praised **Claude** and **DeepSeek-R1** for better reasoning transparency, hinting that models should "show their work" to allow users to validate logic.
- Calls for **truthfulness-focused reward models** and penalizing hallucinations during RL training emerged as a priority.

### **Broader Concerns**
- Users expressed frustration with **flip-flopping answers** and models mirroring user biases instead of adhering to facts. For example, a discussion about Raspberry Pi’s affordability led the model to contradict itself based on conversational context.
- Skepticism persists about whether current AI research prioritizes **"actual reasoning"** over superficial performance benchmarks.

In summary, the community underscores the tension between creativity and reliability in AI, advocating for structural changes in training paradigms and validation mechanisms to address hallucination risks.

### SDFs from Unoriented Point Clouds Using Neural Variational Heat Distances

#### [Submission URL](https://arxiv.org/abs/2504.11212) | 38 points | by [haxiomic](https://news.ycombinator.com/user?id=haxiomic) | [5 comments](https://news.ycombinator.com/item?id=43729063)

Today on Hacker News, an intriguing mathematical paper has surfaced that might just reshape our understanding of surface reconstruction technology. Presented by a team of researchers including Samuel Weidemaier and Martin Rumpf, the study introduces a fresh variational approach for generating neural Signed Distance Fields (SDFs) from unoriented point clouds, a common data form in 3D modeling.

Traditionally, distance computations on discrete surfaces utilize an eikonal equation. However, this paper shifts the paradigm by leveraging the heat method, a technique familiar in discrete surface processing, but new to the neural domain. This innovative approach leads to two convex optimization problems solved using neural networks. First, a neural approximation of the unsigned distance field's gradients is calculated using a strategically timed heat flow. This output then provides the basis for accurately computing the final SDF.

The researchers assert the well-posed nature of the underlying variational problems they're tackling. Through extensive numerical experiments, they demonstrated that this method not only achieves state-of-the-art surface reconstruction but also yields consistent SDF gradients. Impressively, the proposed framework's accuracy shows potential for solving partial differential equations on the zero-level surface—a mark of its precision and utility.

This 14-page paper, packed with figures and tables, crosses the fields of Numerical Analysis, Graphics, and Machine Learning. It's poised to enrich our computational toolbox, especially for those engaged in detailed 3D rendering and modeling tasks. Dive into the full paper on [arXiv](https://arxiv.org/abs/2504.11212) to explore this groundbreaking method further.

Here’s a concise summary of the Hacker News discussion about the neural SDF paper:

1. **Initial Reactions**  
   - User **RobotCaleb** humorously posts "fst" (likely shorthand for "first!"), a common forum meme to claim being the first commenter.  

2. **Technical Insights**  
   - **hxmc** highlights the paper’s focus on **real-time surface reconstruction**, noting its potential for applications like processing 3D scan point clouds and extracting SDFs (Signed Distance Fields) for accurate surface representation.  
     - **brcmthrwwy** replies with curiosity about implementation details, asking, "*Oh ww mplmnttn*" (likely: "*Oh, wow! Implementation?*").  

3. **Technical Speculation**  
   - User **awaymazdacx5** raises a technical point about **SDF wave propagation** and geometric particle methods, suggesting connections to probabilistic kernels and vector field operations. Their comment references "*gmtrc ptcl crl fr vctr field*" (geometric particle curl for vector fields) and probabilistic assumptions in the method’s kernel.  

4. **Broader Interest**  
   - The discussion reflects enthusiasm for the paper’s applications in 3D modeling and computational geometry, with users dissecting both its theoretical framework (*variational problems, heat method*) and practical implications (*surface reconstruction from scans*).  

**Key Themes**: Interest in real-world implementation, curiosity about the math behind SDFs, and excitement for advancements in 3D reconstruction workflows.

### Kagi Assistant is now available to all users

#### [Submission URL](https://blog.kagi.com/assistant-for-all) | 475 points | by [angilr](https://news.ycombinator.com/user?id=angilr) | [272 comments](https://news.ycombinator.com/item?id=43724941)

Kagi, a search engine known for emphasizing user privacy and human-centric browsing, is rolling out its popular Kagi Assistant to all users at no extra cost. Previously available only to Ultimate subscribers, this feature is being phased in starting with the USA, expecting full global accessibility by April 20th. Kagi Assistant harnesses the power of top-notch language models, allowing users to perform enhanced web searches grounded in Kagi’s robust search results.

The Assistant is designed to be a research aid, enhancing the search experience without supplanting Kagi’s core search functionality. Users can tailor the tool to meet specific needs, like coding or grammar checking, and enjoy the flexibility to edit prompts and switch models mid-conversation. Privacy remains a top priority, with interactions neither tracked nor used to train AI models.

AI integration is supported by a fair-use policy that matches subscription value with usage capabilities, ensuring service sustainability. Users on different plans can access a variety of models, ranging from standard to advanced ones available to Ultimate plan holders. This policy helps avoid hitting limits, as most will find the provided threshold more than sufficient.

Kagi’s expansion of its Assistant mirrors its commitment to providing a customizable, private, and human-friendly search experience, while continually improving value without financial upticks for users.

The Hacker News discussion revolves around Kagi's new AI Assistant rollout and broader implications for AI services, pricing models, and startup profitability. Key points include:

1. **Kagi's Pricing & Sustainability**:  
   - Users question if Kagi’s "fair-use" policy can sustainably cover AI model costs (e.g., Gemini, ChatGPT Pro). Some suggest subsidies or tiered access, while others worry about hidden costs or future price hikes. Comparisons to Perplexity’s token-based system and Google’s API pricing highlight concerns about transparency and scalability.

2. **AI Integration Strategies**:  
   - Debates emerge on vertical vs. horizontal AI integration. Sam Altman’s OpenAI strategy is cited, emphasizing persistent, personalized AI assistants tied to user accounts. Critics argue for open standards and interoperability, fearing platform lock-in akin to an "AI App Store."

3. **Privacy & Identity Management**:  
   - Concerns about LLMs accessing third-party services via OAuth and identity providers (e.g., Apple, Microsoft) are raised. Users stress the need for secure, bidirectional control over data access to prevent misuse.

4. **Discord’s Profitability**:  
   - Discord’s monetization and profitability are scrutinized. While some users doubt its profitability despite long-term subscriptions, others note it has raised $1B+ in funding and may prioritize growth over short-term profits. The relevance of profitability for startups is debated, with examples from Y Combinator companies showing acquisition as a common exit strategy.

5. **Startup Economics**:  
   - Broader discussions highlight the tension between growth and profitability in startups. Some argue profitability is critical for longevity, while others defend growth-focused models reliant on investor funding, citing examples like Discord’s delayed path to profitability.

Overall, the thread reflects skepticism about Kagi’s cost management, enthusiasm for open AI ecosystems, and mixed views on the viability of freemium models in tech.

### Viral ChatGPT trend is doing 'reverse location search' from photos

#### [Submission URL](https://techcrunch.com/2025/04/17/the-latest-viral-chatgpt-trend-is-doing-reverse-location-search-from-photos/) | 104 points | by [jnord](https://news.ycombinator.com/user?id=jnord) | [54 comments](https://news.ycombinator.com/item?id=43725648)

Amidst the rapid advancements in AI, a new trend is grabbing attention—and raising concerns—on social media: users employing ChatGPT's latest models, o3 and o4-mini, to deduce locations from photographs. These models boast advanced image-reasoning capabilities, enabling them to analyze and interpret photo elements in surprising detail. Armed with this technology, users are putting ChatGPT through its paces, asking it to identify cities, landmarks, and even specific establishments from subtle visual clues provided in images.

While this might sound like a fun game akin to "GeoGuessr," where players guess locations from Google Street View images, it introduces potential privacy issues. There’s a tangible risk of individuals being doxxed by others who could misuse the technology to trace origins of personal photos, such as those shared in Instagram Stories.

Despite the excitement surrounding the models’ abilities—often outperforming older versions—there are notable inaccuracies and failures. Nevertheless, their success in identifying a Williamsburg speakeasy, where an older model guessed incorrectly, underscores both the power and potential privacy pitfalls of such technology.

OpenAI responded to the arising privacy concerns, stating that the new models are designed to prohibit requests involving private information and that they actively monitor for misuse. While the models promise aid in accessibility and emergency response scenarios, the emerging capabilities call for a closer look at safeguarding privacy in the digital age.

The Hacker News discussion on AI models like ChatGPT deducing locations from photos highlights a mix of fascination with the technology's capabilities and significant privacy concerns. Key points from the conversation include:

1. **Capabilities and Use Cases**:  
   - Users noted the models' impressive ability to infer locations from subtle visual clues (e.g., traffic direction, language scripts, landmarks) and compared it to *GeoGuessr*. Some shared examples, such as identifying a Williamsburg speakeasy or European cities like Essen and Sheffield, showcasing the AI's potential for accessibility or investigative tasks.  
   - AI-generated images (e.g., via Midjourney) were discussed as sometimes embedding metadata or regional stylistic cues that could inadvertently leak location details.

2. **Privacy Concerns**:  
   - Many raised alarms about doxxing risks, especially if personal photos (e.g., Instagram Stories) are analyzed. Users debated whether stripping EXIF data or metadata is sufficient, with some arguing that AI can still infer locations from visual context alone.  
   - Skepticism emerged about OpenAI’s safeguards, with calls for stricter restrictions on models to prevent misuse. Others countered that privacy is increasingly unattainable online, urging individuals to avoid sharing sensitive photos altogether.

3. **Debates on AI Reasoning**:  
   - Participants questioned whether LLMs truly "reason" or merely generate plausible text through pattern recognition. Some argued that AI lacks human-like understanding, while others highlighted its utility despite limitations.  
   - Comparisons to human decision-making (e.g., sales processes) underscored debates about AI’s reliability and the ethics of deploying such tools.

4. **Inaccuracies and Limitations**:  
   - Users shared anecdotes of AI failures, such as misidentifying landmarks or struggling with cropped images. While the models occasionally outperformed older versions, their inconsistency was noted as a barrier to trustworthiness.  

5. **Policy and Safeguards**:  
   - Discussions touched on OpenAI’s response to privacy risks, including policies against private data extraction. Some advocated for open-source alternatives to ensure transparency, while others highlighted the costs and challenges of developing ethical AI.  

6. **Community Sentiment**:  
   - A divide existed between those embracing the technology’s potential (e.g., for archival or creative projects) and others prioritizing privacy. The conversation reflected broader tensions between innovation and ethical responsibility in AI development.  

Overall, the thread underscores the dual-edged nature of advancing AI: while powerful for tasks like location inference, it demands careful consideration of privacy, accuracy, and the ethical implications of widespread deployment.

### A Realistic AI Timeline

#### [Submission URL](https://vintagedata.org/blog/posts/realistic-ai-timeline) | 18 points | by [simonpure](https://news.ycombinator.com/user?id=simonpure) | [7 comments](https://news.ycombinator.com/item?id=43724825)

ust AI technology to streamline their operations and improve decision-making processes. By 2026, generative AI becomes a central force in the tech world, with revenue skyrocketing by orders of magnitude as it finally becomes robust enough for real-world deployment.

The shift in focus from massive generalist models to smaller, specialized ones is at the heart of this transformation. Generalist models have shown limitations, and the new wave of AI development is leaning toward smaller, opinionated models that excel in specific tasks. These models are increasingly capable due to advances in reasoning and reinforcement learning, with a growing trend towards integrating these models in established infrastructures.

Industries with low error tolerance, like supply chains or insurance, can finally harness AI once it reaches acceptable levels of accuracy, aligning with stringent requirements for near-zero error rates. Still, there's a need for significant accuracy improvements and interpretability advancements to ensure models reliably perform across domains, especially where traditional rule-based systems like elevators have excelled.

The rapid democratization of reinforcement learning and synthetic reasoning is set to replace many conventional machine-learning routines. This transition enables even smaller models to perform complex tasks, such as intricate math problems, making AI much more versatile and practical across various fields. However, there's an imperative need for developing comprehensive reward systems and operationalizing vertical domains to expand AI's applicability further.

Despite the promising avenues for AI, challenges in providing accurate feedback and managing failure modes persist. Vision Language Models, for example, excel but still lack the ability to signal when things go wrong. Therefore, breakthroughs in model interpretability, with tools to detect potential upstream weaknesses before they become problematic, are crucial.

The AI field is on the verge of a significant transformation with the potential to revolutionize every industry it touches. It could lead to new applications and efficiencies we can barely imagine today. As these trends coalesce, they paint a picture of a future where AI plays an indispensable role in our daily lives—improving accuracy, efficiency, and decision-making across the board. So, fasten your seatbelt for this high-speed journey into the future of AI!

The Hacker News discussion critiques the original submission's bullish outlook on AI's near-term potential, particularly regarding AGI (Artificial General Intelligence) and specialized models. Key points from the discussion include:

1. **Skepticism Toward AGI Claims**: Users question whether transformer-based models (like LLMs) can achieve AGI, defined as matching or exceeding human intelligence. AIPedant argues current AI (e.g., transformers) mimics intelligence but is brittle, comparing it to "smart spiders" that lack true reasoning. Reference is made to Drew McDermott's paper *Artificial Intelligence Meets Natural Stupidity*, highlighting historical overestimations of AI.

2. **Comparisons to Past Systems**: Critics liken LLMs to earlier systems like Lisp expert systems, noting they excel in narrow tasks but lack depth and reliability. The discussion suggests LLMs create an illusion of human-like intelligence without genuine understanding.

3. **Limitations of Current AI**: Users emphasize that LLMs struggle with generalized reasoning, self-awareness, and consistent logic. Issues like "hallucinations" and unreliable outputs in open-ended tasks underscore gaps in true cognitive capabilities. The analogy of a spiderweb—functional but fragile—illustrates concerns about robustness.

4. **AGI Definition Debates**: wvmd cites Wikipedia's 2013 AGI definition (matching human performance in intellectual tasks), arguing current AI falls short. Others note that equating AI achievements to AGI often involves lowering the bar for "intelligence."

5. **Cultural and Philosophical Notes**: A comment dismissing the submission as "fantasy" and another labeling AGI discussions as "religion" hint at perceptions that optimism about AGI may be faith-driven rather than evidence-based.

**Conclusion**: While the original submission envisions transformative AI by 2026, the discussion reflects skepticism about timelines and capabilities, cautioning against overestimating progress toward AGI. Concerns focus on brittleness, flawed reasoning, and historical parallels, suggesting that today’s AI, while advanced, remains far from the flexible, error-tolerant systems required for critical applications.

