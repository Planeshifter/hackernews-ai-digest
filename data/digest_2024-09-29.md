## AI Submissions for Sun Sep 29 2024 {{ 'date': '2024-09-29T17:10:48.569Z' }}

### AGI is far from inevitable

#### [Submission URL](https://www.ru.nl/en/research/research-news/dont-believe-the-hype-agi-is-far-from-inevitable) | 77 points | by [mpweiher](https://news.ycombinator.com/user?id=mpweiher) | [114 comments](https://news.ycombinator.com/item?id=41689558)

In a bold challenge to the prevailing narrative around artificial general intelligence (AGI), researchers from Radboud University and other institutions argue that the goal of creating machines with cognition comparable to humans is fundamentally flawed. Published in the journal *Computational Brain & Behavior*, the study, led by Iris van Rooij, highlights that even under ideal conditions—where engineers have perfect datasets and optimal machine learning methods—achieving AGI is virtually impossible. 

The researchers emphasize the vast complexities of human cognition, which cannot be replicated merely through computational power. They warn against the inflated expectations fueled by tech giants like OpenAI and Google DeepMind, arguing that reliance on these claims may lead to misperceptions about AI capabilities. Van Rooij calls for increased “critical AI literacy” to help people discern the realistic potential of AI systems and to question the often-profit-driven promises from the tech industry. 

This analysis serves as a reminder that while AI technology is rapidly advancing, the pursuit of true human-like intelligence remains a distant, and perhaps unrealistic, dream.

The discussion surrounding the submission highlights skepticism about the feasibility of achieving artificial general intelligence (AGI) akin to human cognition. Participants are deliberating on the complexities of human reasoning and the limitations of current technology, specifically large language models (LLMs). 

Key points from the comments include:

1. **Skepticism about AGI**: Some contributors express doubt regarding the capabilities of LLMs, arguing that while they can perform tasks once thought to be difficult, they fundamentally lack the cognitive abilities that define human intelligence.

2. **Human Cognition vs. Computation**: Multiple commenters emphasize that human cognitive abilities are not easily replicable through machines and that reliance on computational power alone is insufficient for achieving AGI. There’s recognition of the challenges in understanding nuanced reasoning and language.

3. **Perception of AI Progress**: Participants reflect on how AI has progressed, citing examples of past beliefs about AI capabilities being proven incorrect. They point out that machines like LLMs, despite their advancements, do not truly understand content but rather generate responses based on patterns in data.

4. **Concerns for the Future**: Some contributors warn about the societal implications of overstating AI capabilities, including potential misunderstandings by the public regarding what AI can achieve. There’s a call for critical AI literacy to manage expectations and foster informed discussions about AI’s limitations.

Overall, the conversation underscores a collective caution regarding claims of AGI, emphasizing the need for a nuanced understanding of both the capabilities of AI and the intricacies of human cognition.

### Text2CAD: Generating sequential cad designs from text prompts

#### [Submission URL](https://sadilkhan.github.io/text2cad-project/) | 140 points | by [RafelMri](https://news.ycombinator.com/user?id=RafelMri) | [69 comments](https://news.ycombinator.com/item?id=41685642)

A groundbreaking development in CAD design has emerged with the introduction of **Text2CAD**, a pioneering AI framework that allows designers to create parametric CAD models directly from a variety of text prompts. This innovative system can interpret both simple shape descriptions and complex parametric instructions, streamlining the design process.

### Key Contributions:
1. **Data Annotation Pipeline**: Text2CAD incorporates an advanced annotation process that harnesses open-source Language and Vision Models (LLMs and VLMs) to prepare the DeepCAD dataset with multi-layered text prompts. This two-stage method first describes shapes with VLM (utilizing LlaVA-NeXT) and then enriches these descriptions with varied complexities using LLM (Mixtral-50B).

2. **Text2CAD Transformer**: At the heart of the framework is a Transformer architecture that translates natural language prompts into 3D CAD designs by outlining each design step in an autoregressive manner. By leveraging a pretrained BeRT encoder for text embedding and a sequence of decoder blocks, the system efficiently generates full CAD sequences from textual input.

### Results:
Visual and qualitative assessments reveal the system’s ability to produce accurate 3D models, with various prompts generating similar designs despite differing specifications. Performance evaluations utilized metrics such as F1 scores for line and arc generation, Chamfer Distance for geometric alignment, and invalidity ratios to measure model integrity.

### Conclusion:
With promising results in both qualitative and quantitative evaluations, Text2CAD stands at the forefront of integrating natural language processing into CAD design, making it a potent tool for both novice and experienced designers. The authors invite further exploration and citation of their research to advance this exciting field. For those interested, the complete study can be accessed [here](https://arxiv.org/abs/2409.17106).

In the discussion surrounding the **Text2CAD** submission on Hacker News, participants shared diverse perspectives on the implications of the AI framework for CAD design. 

1. **Ease of Use vs. Complexity**: Some commenters expressed skepticism about the feasibility of describing complex 3D objects with simple text prompts. They pointed out that while LLMs (Large Language Models) excel in transforming text, the conversion from 1D (text) to 3D (CAD models) presents unique challenges, particularly in maintaining accuracy and precision. Others noted that designing and modifying 3D models often requires advanced understanding and skills that can’t be fully alleviated by AI tools.

2. **Skill Development**: Several discussions touched on the learning curve associated with existing CAD programs. Users highlighted the significant time investment required to master these tools and expressed concerns that even as AI capabilities improve, the foundational knowledge of CAD principles remains essential. Many felt that LLMs might help novices start designing, but proficient use would still require traditional skills and practice.

3. **Practical Applications**: The conversation also featured debates over the practicality of using AI in CAD workflows. Commenters questioned how these AI tools would interact with traditional modeling practices, and whether they might effectively streamline the design process or introduce new layers of complexity.

4. **Future of Design**: The overall sentiment reflected curiosity about how Text2CAD might evolve the role of designers. While some viewed the AI framework as a promising tool for enhancing creativity, others cautioned against over-reliance on any single solution to capture the nuances of design work.

In summary, while there's a strong interest in Text2CAD's potential to democratize CAD design and make it accessible to a broader audience, practical issues regarding design complexity and skill requirements remain central concerns in the discussion.

### Quandoom: A port of DOOM for a quantum computer

#### [Submission URL](https://github.com/Lumorti/Quandoom) | 113 points | by [lgtx](https://news.ycombinator.com/user?id=lgtx) | [36 comments](https://news.ycombinator.com/item?id=41687874)

In an intriguing blend of retro gaming and cutting-edge technology, a developer has achieved the seemingly impossible: they've successfully ported the classic first level of DOOM to a quantum computer with their project, Quandoom. This unusual endeavor utilizes a single QASM file that runs on a compact circuit requiring 70,000 qubits and 80 million gates. Although such advanced quantum computers aren’t available yet, the game can be simulated on a standard laptop at 10-20 fps with a lightweight C++ simulator.

The release highlights the whimsical notion that "everything can run DOOM," as a nod to the gaming community's long-standing joke. While it only features the first level, lacks color, sounds, and many original game mechanics—such as moving enemies and secrets—Quandoom offers a unique take on quantum computing's capabilities. Users can experience this quantum version of DOOM by downloading it and running it through the simulator, though it requires a hefty 5-6 GB of RAM and a bit of patience due to its large file size.

The project stands as a quirky parody in the realm of software development, showcasing the creativity that pervades both the gaming and tech worlds. If there's enough interest, the developer may share further insights and enhancements through an academic paper or additional updates.

The discussion surrounding the port of DOOM to a quantum computer (Quandoom) delves into a variety of topics related to quantum computing, memory requirements, and the implications of running classic games on advanced technology. 

Some participants highlighted the significant memory demand, with remarks on the 5-6 GB RAM requirement for the simulator, reflecting on the evolution of gaming technology over the past decades. Notably, comparisons to older, simpler systems like the Z-Machine and Game Boy illustrate the changes in memory usage and capabilities.

Technical aspects, such as the challenges around reversible computation in the project's architecture, sparked in-depth conversations about quantum registers and binary arithmetic. Users explored the programming hurdles involved in replicating the game’s behavior, including storage details of qubits and the limitations on visual and gameplay elements.

Others took a more whimsical approach, discussing the novelty of achieving classical gaming experiences via quantum methods while referencing the humorous adage that "everything can run DOOM." There were also mentions of other quirky projects and AI-generated interpretations of DOOM, indicating a lively interest in the intersection of AI and traditional gaming.

Lastly, a few comments touched on the future of such projects and potential developments, with some expressing disappointment over additional required features or complexities, while others noted the impressive nature of the accomplishment given the current state of quantum technology.

### Feldera Incremental Compute Engine

#### [Submission URL](https://github.com/feldera/feldera) | 137 points | by [gzel](https://news.ycombinator.com/user?id=gzel) | [53 comments](https://news.ycombinator.com/item?id=41685689)

**Hacker News Daily Digest: Feldera's Incremental Computation Engine**

Today’s highlight features Feldera, a groundbreaking incremental computation engine now available as an open-source project on GitHub. Feldera distinguishes itself with the unique capability to perform evaluations of SQL programs incrementally, thus offering a significant improvement over traditional batch processing and streaming databases.

The engine supports complex SQL queries including joins, aggregates, and window functions, enabling users to construct dynamic, real-time pipelines that efficiently process vast datasets—potentially exceeding local memory capacity. Users have reported achieving remarkable performance, with millions of events processed per second even on standard laptops.

Feldera's architecture fosters seamless integration with popular data sources such as Kafka, S3, and Data Lakes, making it a flexible choice for both batch and real-time data analytics. For those eager to explore its features, a quick start via Docker is available, along with comprehensive documentation and tutorials.

As the platform continues to evolve, the community is invited to contribute, fostering a collaborative development environment. Check out Feldera on GitHub for a deeper dive into its architecture, benchmarks, and more!

### Summary of Discussion on Feldera's Incremental Computation Engine

The discussion centers around Feldera's incremental computation capabilities compared to existing solutions like ClickHouse and Materialize. 

1. **Comparative Capabilities**: Users highlighted Feldera's prowess in handling complex SQL programs incrementally, noting differences in performance and design compared to ClickHouse's materialized views and traditional approaches. Some commenters appreciated Feldera's handling of large datasets and its ability to maintain state during processing, contrasting it with ClickHouse’s batch processes.

2. **User Experience and Community Engagement**: Several participants shared resources for understanding incremental computation, and the technical aspects of Feldera were discussed in a community chat. Users expressed enthusiasm for the collaborative environment surrounding Feldera's development, with suggestions to contribute to discussions and improvements.

3. **Technical Challenges and Solutions**: Discussion touched on technical elements like the handling of complex queries and the maintenance of data consistency in transitional states. Users debated optimal practices for performance, such as the use of Z-sets for maintaining state during aggregations.

4. **Future Development and Research**: There were mentions of ongoing research connections and future contributions to the theory behind incremental computation. Participants also explored various applications and tools related to Feldera, including links to GitHub repositories and academic papers for further exploration.

5. **User Adoption**: Some users shared their experiences with early implementations of Feldera, noting its capabilities and expressing eagerness for its development. Suggestions for further enhancements and features were welcomed.

Overall, the dialogue showcases a vibrant community exploring the implications of Feldera's innovative approach to data processing and SQL handling, highlighting both technical depth and collaborative spirit.

### ReKep: Spatio-Temporal Reasoning of Relational Keypoint Constraints for Robots

#### [Submission URL](https://rekep-robot.github.io/) | 45 points | by [jasondavies](https://news.ycombinator.com/user?id=jasondavies) | [3 comments](https://news.ycombinator.com/item?id=41687071)

A groundbreaking study by researchers from Stanford and Columbia University introduces ReKep, a novel approach to robotic manipulation that leverages Relational Keypoint Constraints for real-time task handling. Instead of relying on task-specific training, ReKep enables robots to generate optimized actions by simply interpreting free-form language instructions and depth data from RGB-D sensors. 

This innovative system represents manipulation tasks as a series of constraints linking keypoints in the environment, allowing robots to adopt versatile behaviors, from multi-stage tasks to bimanual operations. For instance, if an object is unexpectedly moved mid-task, ReKep dynamically adapts, ensuring continuous operation. 

The research demonstrates the system's ability to efficiently perform diverse tasks—from folding clothes to pouring tea—by analyzing the scenarios and responding with human-like manipulation strategies. With no need for manual labeling or training on specific environments, ReKep represents a significant leap forward in autonomous robotic capabilities. 

The full video demonstration allows viewers to see the impressive functionality in action, showcasing how robots can manipulate objects in real-time, even in unpredictable settings. This work promises to reshape how robotic systems learn and adapt to new tasks with their evolving understanding of physical interactions.

The discussion surrounding the ReKep study spans several themes. Participants express a mix of fascination and skepticism about the practical applications of robotic manipulation using Relational Keypoint Constraints. One commenter highlights that while the technology seems promising, it may still face limitations inherent in current machine learning models that tend to rely on pattern recognition rather than adaptive reasoning.

Some users mention existing methods in robotics, such as using frameworks like OpenCV and ROS, emphasizing the importance of reliable object detection and custom training in real-world scenarios. Others note concerns about the complexity and unpredictability of tasks, suggesting that robotic systems may struggle with more intricate challenges despite advancements in AI capabilities. 

A recurring theme is the balance between automation's potential and its current practical limitations, with several commenters suggesting that while ReKep demonstrates advanced manipulation strategies, it may still not fulfill all needs in dynamic environments. Overall, the dialogue reflects an ongoing interest in the topics of AI, robotics, and their applications, tempered by a critical view of their feasibility and potential operational challenges.

### Show HN: An experimental AntiBot, AntiCrawl reverse proxy for the web

#### [Submission URL](https://github.com/pulkitsharma07/OnlyHumans-Proxy) | 25 points | by [pulkitsh1234](https://news.ycombinator.com/user?id=pulkitsh1234) | [34 comments](https://news.ycombinator.com/item?id=41689263)

**OnlyHumans-Proxy: A Novel Approach to Anti-Bot Defense**

An innovative GitHub project, **OnlyHumans-Proxy**, has emerged as an experimental reverse proxy aiming to thwart unwanted web scraping by serving static content as images, rather than traditional HTML. The creator, Pulkit Sharma, outlines a vision where bots face increased work to access human-generated content, shifting the landscape from typical CAPTCHA verification to a solution that minimizes cognitive load on genuine users.

This approach arises from frustration over the ineffectiveness of standard measures against determined scrapers and the growing prevalence of automated crawling services. By transforming web pages into images with watermarks, OnlyHumans-Proxy allows real users to navigate sites seamlessly while complicating access for bots, making text extraction much more difficult and resource-intensive.

Currently, the project supports static websites with basic navigation via HTML links and includes impressive features like in-memory caching for fast load times. Users interested in experimenting can easily run a local instance by setting their website URL and accessing it through the proxy.

As web scraping technologies evolve, OnlyHumans-Proxy represents a proactive step toward enhancing content protection on the internet, sparking discussions on the future of web interactions and bot deterrence strategies.

The discussion surrounding the **OnlyHumans-Proxy** highlights various perspectives on the project's implications and its potential impact on web standards and accessibility. Key points include:

1. **Technical Challenges**: Some commenters express skepticism about rendering websites as images instead of HTML, arguing it could lead to significant resource consumption and may not effectively prevent scraping. Concerns are raised about the performance implications, especially in scenarios where websites would need to handle high traffic or DDOS attacks.

2. **SEO and Usability**: Others question the approach's SEO implications, noting that serving static page content as images could hinder search engine visibility and performance. There are discussions about how such implementations would impact usability for users relying on accessibility technologies.

3. **Web Security Concerns**: Several users mention the importance of balancing security and usability. While the intent is to make it harder for bots to scrape content, there is concern that this could inadvertently complicate interactions for legitimate users or hinder site functionality.

4. **Alternative Solutions**: Suggestions emerge regarding alternative strategies, such as using traditional CAPTCHA or integrating biometric authentication via WebAuthn, prompting some debate about the future of user verification on websites.

5. **Accessibility Compliance**: Some commentators emphasize the importance of adhering to web accessibility standards, mentioning that blocking or rendering pages completely as images could violate guidelines and harm end-user experience, especially for those relying on assistive technologies.

Overall, the conversation reveals a mix of intrigue and caution regarding the OnlyHumans-Proxy approach, with discussions delving into its technical feasibility, implications for web content accessibility, and effectiveness against scraping efforts.

### NotebookLM is quite powerful and worth playing with

#### [Submission URL](https://twitter.com/karpathy/status/1840112692910272898) | 69 points | by [bilsbie](https://news.ycombinator.com/user?id=bilsbie) | [24 comments](https://news.ycombinator.com/item?id=41688804)

It seems there is no specific submission to summarize. If you provide a link or a topic from Hacker News, I can create a daily digest highlighting the key points and significance of the story!

The discussion on Hacker News revolves around the challenges and limitations of podcasts and AI-generated content. Key points include:

1. **Quality Concerns with Podcasts**: Participants expressed frustration over the repetitiveness and lack of engaging content in podcasts, suggesting that many are poorly structured and fail to capitalize on textual analysis or critical thinking.

2. **AI and Content Generation**: There's a significant focus on the role of large language models (LLMs) in creating content, particularly how advancements in AI could enhance the user experience in generating and consuming audio content. Many users are hopeful about future AI capabilities improving the quality of generated outputs.

3. **Potential of NotebookLM**: A tool called NotebookLM was mentioned as a significant development in integrating AI with learning materials, allowing for effective summarization and conversation generation. Users discussed its capability to produce high-quality dialogues and the potential for such technologies to reshape content consumption.

4. **Context and Interaction with AI**: Several comments raised the need for the ability of LLMs to retain context and improve user interaction. A sentiment was expressed that AI-generated media should move beyond basic dialogue to more engaging and informative discussions.

5. **Different Formats for Content Consumption**: The discussion highlighted the importance of exploring various content formats, including presentations and dialogues, to maintain audience engagement and improve the effectiveness of content delivery.

Overall, the conversation reflects a desire for quality content that leverages AI advancements while simultaneously critiquing existing podcast formats and suggesting improvements for future developments in AI-driven media.

