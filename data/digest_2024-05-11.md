## AI Submissions for Sat May 11 2024 {{ 'date': '2024-05-11T17:09:53.442Z' }}

### Citation Needed â€“ Wikimedia Foundation's Experimental LLM/RAG Chrome Extension

#### [Submission URL](https://chromewebstore.google.com/detail/wikipedia-citation-needed/kecnjhdipdihkibljeicopdcoinghmhj) | 116 points | by [brokensegue](https://news.ycombinator.com/user?id=brokensegue) | [35 comments](https://news.ycombinator.com/item?id=40330667)

The Wikimedia Foundation has launched a new Chrome extension called "Wikipedia Citation Needed," aimed at helping users verify the accuracy of information they encounter online. The extension, utilizing the ChatGPT API, scans Wikipedia for relevant articles and quotes to provide context on the information being read. Users can select a snippet of text while browsing to trigger the extension, which will then indicate if the claim is supported by Wikipedia along with article quality details. The tool is in the experimental phase, leveraging generative AI, and feedback on its performance is encouraged for further enhancements. Recently, version 0.1.11 has been released, offering a side panel interface for uninterrupted browsing and the option to donate to Wikipedia after a certain number of verifications. This initiative by the Future Audiences team at Wikimedia Foundation aims to enhance online fact-checking and information validation.

The discussion surrounding the launch of the Wikimedia Foundation's new Chrome extension, "Wikipedia Citation Needed," includes various perspectives. Some users like "prpl-lfy" express expertise in browser extension development and see the potential value of the generative AI behind the tool. On the other hand, concerns are raised by "card_zero" about the extension not checking the source of the claims. Users like "Waterluvian" emphasize the importance of primary sources, while "_notreallyme_" suggests classifying Wikipedia as a tertiary source. Additionally, technical details and suggestions for Safari extension and Firefox compatibility are discussed.

"Daub" brings up the importance of citations on Wikipedia, with "bxd" highlighting concerns about fraudulent citations and the need for proper validation. The debate extends to the reliability of sources, with discussions about utilizing primary and secondary sources and the challenges of fact-checking within the limits of LLM (large language models).

Furthermore, users like "rnd" provide feedback on the extension's functionality and documentation, while "vsrg" discusses the scale at which LLMs generate content. The conversation also touches on the potential political implications of AI in community applications and AI's role in finding and verifying information.

Overall, the discussion on Hacker News reflects a range of viewpoints on the functionality, design, implications, and challenges of using generative AI within the context of the "Wikipedia Citation Needed" extension.

### Why the CORDIC algorithm lives rent-free in my head

#### [Submission URL](https://github.com/francisrstokes/githublog/blob/main/2024/5/10/cordic.md) | 405 points | by [todsacerdoti](https://news.ycombinator.com/user?id=todsacerdoti) | [73 comments](https://news.ycombinator.com/item?id=40326563)

The CORDIC algorithm is the star of the show in the tech community right now! It's a nifty way to compute trigonometric functions like sine and cosine on small devices without the need for floating-point arithmetic or hefty lookup tables. 

By combining vector math, trigonometry, convergence proofs, and a dash of computer science, CORDIC simplifies these complex functions into elegant shifts and additions. It's a top pick for embedded systems, where resources are limited, making it a go-to for microcontrollers and FPGAs.

Dan Mangum's hot take on floating points as a crutch sparked interest in CORDIC and fixed-point arithmetic. By representing numbers with integers divided into whole and fractional parts, calculations can be performed smoothly using shifts and additions.

Basic operations like addition, subtraction, multiplication, and division work seamlessly in fixed-point arithmetic. When trig functions come knocking, CORDIC steps in - rotating vectors around a unit circle to compute sine and cosine values with finesse.

And that's the beauty of CORDIC - simplifying the complex and proving that elegance lies in simplicity!

The discussion on the submission about the CORDIC algorithm on Hacker News delved into various aspects of floating-point calculations, fixed-point arithmetic, IEEE standards, hardware implementations, and historical context. 

One user highlighted the intricacies of floating-point math, emphasizing the challenges faced in deterministic platforms and the advantages of fixed-point physics engines. Another user mentioned the importance of constant folding in compilers and how different processors handle calculations, sparking a debate on compiler optimization and constant handling. 

The conversation expanded to include discussions on the popularity and implementation of fixed-point and floating-point calculations in gaming development from 1980 to 2000 and the technical aspects of hardware implementations and lookup tables. Users also shared insights on hardware implementations of trigonometric functions and CORDIC's efficiency in computing various mathematical operations.

The discussion further explored CORDIC's applications in gaming and hardware, the efficiency of CORDIC in computations, and the comparison of CORDIC to traditional methods. Additionally, references to related articles on hardware implementations of trigonometric functions were shared, and users exchanged information on cost-effective MCUs with CORDIC peripherals and the benefits of dedicated hardware for precision in calculations. 

Furthermore, the discussion touched upon personal experiences with CORDIC, sharing resources like articles on drawing circles and the evolution of gaming technology.

### Vision Transformers Need Registers

#### [Submission URL](https://openreview.net/forum?id=2dnO3LLiJ1) | 155 points | by [cscurmudgeon](https://news.ycombinator.com/user?id=cscurmudgeon) | [19 comments](https://news.ycombinator.com/item?id=40329675)

The paper "Vision Transformers Need Registers" presents a crucial insight into artifacts in feature maps of ViT networks and proposes a novel solution involving additional tokens called "registers" to address this issue effectively. This innovation not only sets a new state of the art for self-supervised visual models but also enhances downstream visual processing. The authors' work demonstrates the power of continuous improvement and innovation within the field of representation learning.

The discussion on the submission "Vision Transformers Need Registers" on Hacker News covers various perspectives and insights related to the paper. 

- User "ttl" provides a detailed overview of how additional tokens called "registers" have been added to ViT models to improve global information retrieval, resulting in better performance in visual processing tasks. This has led to a 2% increase in inference cost while significantly improving ViT model performance.
- User "mclgnn" mentions attempting to add CLS tokens to BERT with spectacular results, providing a link for reference.
- User "swyx" points out the importance of understanding the differences between regular vision transformers and transformers that involve tokens.
- User "johntb86" brings up a discussion on the naming and handling of tokens in the final layer, resulting in investigating the passing of raw data and intermediate steps.
- User "rchdghrty" shares a related link about hidden computation in Transformer Language Models, elaborating on improvements in performance and benchmark results with the addition of extra tokens.

Overall, the discussion touches upon the technical aspects, potential benefits, and implications of introducing additional tokens like "registers" in Transformer models, highlighting the ongoing innovations and explorations in representation learning.

### Cosine Similarity

#### [Submission URL](https://algebrica.org/cosine-similarity/) | 27 points | by [kyroz](https://news.ycombinator.com/user?id=kyroz) | [9 comments](https://news.ycombinator.com/item?id=40327293)

The Loop Math Theory Function Guide received an intriguing update on Hacker News, diving into the world of cosine similarity. This method allows computers to assess document similarity effectively by transforming words into vectors within a vector space. The article explains the concept behind cosine similarity and provides a detailed formula for calculating it. By breaking down a simple example, showcasing the similarity between different sentences through vector transformations, it illustrates how cosine similarity can be applied practically.

Through the example, involving sentences about reading thriller novels and arriving late, the process of converting text into vectors and computing their similarities is elucidated. Using the cosine similarity formula, the article demonstrates how to quantify the resemblance between vectors representing sentences. In the example provided, sentences expressing a preference for thriller novels exhibit a high degree of similarity, reflected in a cosine similarity value of 0.75. The explanation goes further to outline how the angle between vectors can be derived from cosine similarity, emphasizing the significance of angle magnitude in indicating similarity.

Furthermore, the article offers Python code for computing cosine similarity between vectors, enabling readers to experiment with the concept. Overall, this post on Hacker News delves into the practical application of cosine similarity in text analysis, shedding light on its importance in areas such as recommendation systems and semantic search.

The discussion revolves around the topic of cosine similarity and its application in vector spaces, particularly in relation to text analysis. There is a debate on the range of cosine similarity values, with mention of the range being 0 to 1 for vectors with no negative components. Additionally, PostgreSQL's pg_trgm module is highlighted for calculating similarity distances. Various users elaborate on the significance of cosine similarity in measuring similarity between points in vector spaces and the effectiveness of this method in high-dimensional spaces. There are discussions regarding the impact of document length on complexity and ways to enhance textual semantic relationships. Users share insights on techniques like TF-IDF for calculating vectors, the relevance of cosine similarity in retrieving information, and the removal of stop words to improve text analysis. The conversation also touches upon advancements in NLP algorithms like Transformers and their handling of stop words in context.
