## AI Submissions for Sun Oct 01 2023 {{ 'date': '2023-10-01T17:10:38.277Z' }}

### FlashAttention: Fast Transformer training with long sequences

#### [Submission URL](https://www.adept.ai/blog/flashier-attention) | 146 points | by [kristianp](https://news.ycombinator.com/user?id=kristianp) | [8 comments](https://news.ycombinator.com/item?id=37724861)

Transformers have become increasingly powerful, but training them on long sequences has remained a challenge. The attention layer, which is at the core of Transformers, poses a bottleneck in terms of compute and memory. Doubling the sequence length would quadruple the runtime and memory requirements.

However, there is now a solution: FlashAttention, a new algorithm that speeds up attention and reduces its memory footprint without any approximation. Since its release six months ago, FlashAttention has been widely adopted by organizations and research labs to accelerate their training and inference processes.

Tri Dao, a researcher and part-time research fellow at Adept, has been collaborating with the company to improve FlashAttention. They have developed a key improvement that enables FlashAttention to be fast for long sequences, which opens up the possibility of training large language models with longer context.

For example, FlashAttention is now up to 2.7 times faster than a standard PyTorch implementation and up to 2.2 times faster than the optimized implementation from Megatron-LM, even at small batch sizes, when used on sequences with a length of 8k. This increased speed allows for training with longer context, resulting in higher-quality models.

The motivation for tackling long sequences is to scale up the context length of Transformers. Currently, the multihead attention layer in Transformers has a runtime and memory requirement that grows quadratically with the input sequence length. By training models that can understand books, high-resolution images, webpages, multi-turn user interactions, and long-form videos, the hope is to advance AI capabilities.

FlashAttention achieves its speed and efficiency improvements by reordering the attention computation and leveraging classical techniques like tiling and recomputation. These techniques significantly speed up attention and reduce memory usage from quadratic to linear in sequence length. However, FlashAttention was not initially optimized for the case of super long sequences, where batch sizes and numbers of heads are small, due to insufficient parallelism.

To optimize for long sequences with small batch sizes and small numbers of heads, FlashAttention now introduces attention parallelism. Each attention head uses classical tiling techniques to load blocks of query, key, and value from GPU memory to a faster cache, compute attention with respect to that block, and write back the output. This reduction in memory reads and writes brings significant speedup in most cases.

In the case of long sequences with small batch sizes or small numbers of heads, FlashAttention parallelizes over the sequence length dimension in order to make better use of the multiprocessors on the GPU. This results in a significant speedup for this regime.

Overall, FlashAttention offers a solution for training Transformers on long sequences, enabling the training of models with longer context. With its improved speed and memory efficiency, FlashAttention is making strides in advancing AI capabilities and pushing the boundaries of what can be achieved with Transformers.

The discussion about FlashAttention on Hacker News revolves around the release of FlashAttention, the efficiency and speed improvements it offers, and its impact on training Transformers on long sequences. Some key points highlighted in the comments are:

- Tri Dao, a researcher and part-time research fellow, collaborated with Adept to improve FlashAttention and enable it to be fast for long sequences.
- FlashAttention is up to 2.7 times faster than a standard PyTorch implementation and up to 2.2 times faster than the optimized implementation from Megatron-LM, even at small batch sizes.
- FlashAttention achieves its speed and efficiency improvements by reordering attention computation and leveraging techniques like tiling and recomputation. It reduces memory usage from quadratic to linear in sequence length.
- FlashAttention has been widely adopted by organizations and research labs to accelerate their training and inference processes.
- The motivation behind tackling long sequences is to scale up the context length of Transformers and advance AI capabilities.
- FlashAttention is making strides in pushing the boundaries of what can be achieved with Transformers.
- Some users share alternative resources related to FlashAttention, including recent interviews with Tri Dao and benchmark numbers for FlashAttention.

### Decentralized Artificial Intelligence

#### [Submission URL](https://www.chaos-engineering.dev/p/decentralized-artificial-intelligence) | 85 points | by [liqudity](https://news.ycombinator.com/user?id=liqudity) | [41 comments](https://news.ycombinator.com/item?id=37723372)

The author discusses the concept of decentralized artificial intelligence (AI) and argues that a cryptographically secure, decentralized ledger is the only solution to making AI safer. They believe that true artificial general intelligence (AGI) should not be controlled by a single entity or research lab, as it creates too much power in the hands of a few. The author highlights some of the problems in the AI field, such as reproducibility issues, data privacy concerns, stale information, and massive compute requirements. They propose that a decentralized database and the use of federated learning could address these challenges.

The discussion on the submission revolves around the feasibility and drawbacks of using a cryptographically secure, decentralized ledger for AI. One commenter points out that the work required to verify the cryptographic proof in a decentralized AI system would be computationally expensive. Another commenter mentions that the assumptions made in the article about checking work and proof of work are flawed. Some commenters argue that blockchains are not the solution to the problems highlighted in the article, such as reproducibility issues and data privacy concerns. They explain that decentralized AI does not necessarily mean improved safety or control. Other points raised in the discussion include the challenges of resource requirements for training and inference, data privacy, stale data, and the need for interoperability. Some commenters suggest alternative solutions like distributed inference and model distillation to address these challenges. There are also discussions on the limitations of current AI development, the reproducibility of SOTA (state-of-the-art) models, and the potential dangers of decentralized AI. Overall, the discussion highlights the complexity and different perspectives on the concept of decentralized AI and the challenges it presents.

### Nvidia's RTX 5000 Ada Now Available: AD102 with 32GB of GDDR6

#### [Submission URL](https://www.tomshardware.com/news/nvidias-rtx-5000-ada-now-available-ad102-with-32gb-of-gddr6) | 48 points | by [pizza](https://news.ycombinator.com/user?id=pizza) | [34 comments](https://news.ycombinator.com/item?id=37721720)

Nvidia's partners have started quietly selling the Nvidia RTX 5000 Ada Generation graphics card designed for professional visualization applications. The card features Nvidia's flagship AD102 GPU in a cut-down configuration to reduce power consumption. However, retailers are selling the graphics card at inflated prices, with some listing it for as much as $6,999, even though the MSRP is $4,000. The RTX 5000 Ada card offers a peak compute performance of 65.3 FP32 TFLOPS and is equipped with 32 GB of GDDR6 memory. Nvidia's full AD103 chip has a maximum of 10,240 CUDA cores spread over 80 SMs. It remains to be seen how Nvidia will leverage the AD103 chip for its professional offerings.

The discussion on Hacker News about the submission revolves around several key points. 

- Some users compare the RTX 5000 Ada with the RTX 4090, discussing the differences in features and performance. They also mention the reduced power consumption of the RTX 5000 Ada compared to other enterprise-grade cards.
- The limitations of the card's VRAM are debated, with some users wishing for higher VRAM capacity, while others point out that it may not be necessary for certain use cases.
- The potential impact of the RTX 5000 Ada's pricing is discussed, with users questioning the substantial price difference compared to the MSRP. Some users express dissatisfaction with the increased prices, while others speculate on the factors contributing to the inflated costs.
- The discussion touches on the competition between Nvidia and AMD, suggesting that if AMD's GPUs become fully compatible with CUDA, there could be increased competition in the market.
- The potential use of multiple RTX 3090 cards for cost-effectiveness is mentioned, as well as the impact of running large AI models.
- The topic of Apple Silicon devices is brought up, with some users expressing interest in their inference performance and discussing their suitability for AI tasks.
- The discussion also addresses the limitations of the M1 chip for training large models and the differences in inference speed between different hardware options.

Overall, the discussion covers a range of topics related to the Nvidia RTX 5000 Ada graphics card, its features, pricing, and its competition in the market.

