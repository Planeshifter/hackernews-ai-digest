## AI Submissions for Sat Jan 31 2026 {{ 'date': '2026-01-31T17:12:27.027Z' }}

### Generative AI and Wikipedia editing: What we learned in 2025

#### [Submission URL](https://wikiedu.org/blog/2026/01/29/generative-ai-and-wikipedia-editing-what-we-learned-in-2025/) | 199 points | by [ColinWright](https://news.ycombinator.com/user?id=ColinWright) | [96 comments](https://news.ycombinator.com/item?id=46840924)

Wiki Education: LLM-written Wikipedia text mostly “looks sourced” but fails verification

- Wiki Education (which brings ~19% of new active editors to English Wikipedia) analyzed how participants use generative AI. Their bottom line: editors should never copy‑paste chatbot output into articles.
- Using the Pangram detector, they reviewed 3,078 new articles created via their programs since 2022; 178 were flagged for AI use—none before ChatGPT’s launch, with usage rising each term.
- Only 7% of flagged articles had fake citations. The bigger problem: over two‑thirds failed verification—sentences cited to real, relevant sources that do not actually support the claims. In many cases, nearly every cited sentence failed this check.
- Cleanup was costly: staff moved recent work back to sandboxes, stubified articles that met notability but failed verification, and proposed deletion for unsalvageable pages; some were later de‑PRODed, reflecting mixed community views.
- Context: English Wikipedia bans generative AI for images and talk pages and recently adopted a guideline against using LLMs to generate new articles. Wiki Ed now runs near real‑time Pangram checks on participant edits via its Dashboard to intervene earlier.

Why it matters: LLMs often produce plausible prose with real‑looking citations that don’t support the text—undermining verifiability, a core Wikipedia policy. The verification and cleanup burden can exceed the time it takes to generate such content, pushing programs and tools to focus less on hallucinated sources and more on whether sources actually back the claims.

Here is a summary of the discussion:

**The "Weaponization" of Citations**
Commenters argued that the problem extends beyond LLMs to a broader "weaponization" of citations on the internet. Users have learned that providing a link—any link—creates an aura of authority because most readers (and even editors) rarely click through to verify the source supports the claim. This creates a "blind spot" where content looks sourced and trustworthy but is effectively fabrication. One user noted this behavior is also rampant in Hacker News comments, where people post PubMed links that contradict their own arguments, banking on the fact that no one will read the technical details.

**Humans vs. LLMs**
A significant portion of the debate focused on the "human baseline." Commenters pointed out that Wikipedia already suffers from human-generated "hallucinations," such as movie plot summaries written by people who clearly haven't watched the film, or incorrect claims that persist for over a decade. While some argued that LLMs simply automate this bad behavior at a higher scale, others contended that fixing human errors is already tedious, and an influx of machine-generated errors will overwhelm the volunteer workforce.

**The Labor of Verification**
The discussion highlighted *why* creators (human or AI) might fake citations: applying correct citations is genuinely hard work. Users noted the paradox where specific, novel claims are easy to cite, but "general knowledge" or textbook fundamentals (e.g., proving a standard chemistry concept) are difficult to pin down to a specific URL or paper. This friction encourages the use of "plausible" shortcuts over rigorous research.

### Autonomous cars, drones cheerfully obey prompt injection by road sign

#### [Submission URL](https://www.theregister.com/2026/01/30/road_sign_hijack_ai/) | 152 points | by [breve](https://news.ycombinator.com/user?id=breve) | [143 comments](https://news.ycombinator.com/item?id=46840676)

TL;DR: Researchers from UC Santa Cruz and Johns Hopkins show that large vision-language models in self-driving cars and drones will follow text commands printed on signs in their camera view—an “environmental indirect prompt injection” that can override safe behavior.

Key points
- New attack class: CHAI (Command Hijacking Against Embodied AI) uses optimized text on physical signs to make LVLM-powered systems treat the text as an instruction (“turn left,” “proceed,” “Police Santa Cruz,” “Safe to land”).
- Works across languages: English, Chinese, Spanish, and Spanglish; appearance tweaks (font, color, placement) boost success, with content of the prompt being the biggest driver.
- Self-driving sims: With signs in view, models reversed correct behavior (e.g., turning left through a crosswalk). Success rate: 81.8% against a GPT-4o-based setup vs 54.7% on InternVL.
- Drone tracking: CloudTrack misidentified targets when adversarial labels were added (e.g., a generic car labeled “Police Santa Cruz”)—error rates up to 95.5%.
- Drone landing: Signs reading “Safe to land” tricked the system into choosing unsafe rooftops—CHAI succeeded up to 68.1%.
- Physical tests: Real-world trials mirrored simulated vulnerabilities (the car trials stayed in sim for safety).

Why it matters
- Exposes a practical, low-cost way to subvert embodied AI by exploiting their tendency to literally follow on-camera text.
- Highlights a security gap as LVLMs move into safety-critical autonomy: perception channels double as instruction channels, enabling command injection from the environment.

Caveats and defenses
- Car tests were simulated; drones had both sim and real-world elements.
- Mitigations likely include hard separation of perception vs instruction inputs, filtering/segmentation of scene text, rule-based overrides (e.g., crosswalk safety), adversarial training on sign-based attacks, and model-side refusal to execute in-scene text as commands.

Here is a summary of the discussion:

**Technical Architecture & Vulnerability**
Commenters analyzed why this attack vector works, noting that traditional self-driving stacks separated specific tasks (like sign classification) from driving logic. The vulnerability arises from the shift to end-to-end Vision Language Models (VLMs), which process the entire scene semantically; without hard-coded separation, the model essentially hallucinates instructions from background text.

**Adversarial Infrastructure & Legality**
The conversation explored low-tech versions of this attack, such as installing fake ("phantom") stop signs to trick Google Maps or autonomous vehicles. Users debated the legal consequences of modifying road signs, arguing that while it might look like simple vandalism or a civil liability issue, it could escalate to criminal charges if it results in injury or is deemed an intentional trap.

**Infrastructure Debate: Stops vs. Roundabouts**
A significant portion of the thread diverged into a debate on road design:
*   **4-Way Stops:** Critics called them inefficient and confusing regarding right-of-way. Defenders noted they handle "resource starvation" better than roundabouts (ensuring side streets get a turn) and are Safer for pedestrians in residential areas.
*   **Roundabouts:** Proponents argued they offer superior flow and safety. When users claimed roundabouts require too much space for urban areas, others pointed to "mini-roundabouts" (painted circles) common in the UK and Europe as space-efficient alternatives.

**Cost & Practicality**
Users discussed why cities prefer simple signs over "smart" sensor-driven traffic systems. The consensus was cost: digging up roads for power and sensors involves high capital and maintenance costs, whereas a metal stop sign is cheap and essentially maintenance-free.

### Show HN: I trained a 9M speech model to fix my Mandarin tones

#### [Submission URL](https://simedw.com/2026/01/31/ear-pronunication-via-ctc/) | 451 points | by [simedw](https://news.ycombinator.com/user?id=simedw) | [136 comments](https://news.ycombinator.com/item?id=46832074)

HN Summary: Tiny on-device model that grades your Mandarin tones

- Motivation: The author’s Mandarin speech was intelligibility-limited by tones. Handcrafted pitch visualizers (FFT + heuristics) proved brittle to noise, coarticulation, and speaker variation, so they built a learned system instead.

- Approach: A small ASR-style CAPT (Computer-Assisted Pronunciation Training) model that’s pedantic about how you said something.
  - Model: Conformer encoder trained with CTC. Convolution captures split-second local cues (e.g., zh vs z), attention handles global/relative tone context and sandhi.
  - Why CTC (vs seq2seq like Whisper): CTC won’t “autocorrect” to plausible text; it forces the model to reflect what was actually said frame-by-frame—critical for feedback.
  - Tokenization: Pinyin syllable + tone as first-class tokens (e.g., zhong1 vs zhong4) so tone errors surface explicitly. Neutral tone normalized as tone 5. Vocab ≈1,254 + <unk>, <blank>.

- Alignment and scoring:
  - Forced alignment via Viterbi through the CTC posterior matrix to map audio frames to target syllables.
  - UI/metrics decoupling fix: Leading silence was sinking confidence by flooding spans with <blank>. Solution: ignore high-blank frames when scoring, but keep spans for highlighting.

- Data, training, and metrics:
  - Datasets: AISHELL‑1 + Primewords (~300 hours), with SpecAugment.
  - Training: ~8 hours on 4× RTX 4090.
  - Metrics emphasized: TER (Token Error Rate), tone accuracy (1–5), and confusion groups (zh/ch/sh vs z/c/s).
  - Results across sizes:
    - 75M params: TER 4.83%, tone acc 98.47%
    - 35M params: TER 5.16%, tone acc 98.36%
    - 9M params: TER 5.27%, tone acc 98.29%
  - Takeaway: Accuracy barely degraded with size—task looks data‑bound more than compute‑bound.

- Deployment: INT8-quantized ONNX model ~11 MB (from 37 MB FP32) with negligible loss (+0.0003 TER). Runs in-browser/on-device via onnxruntime‑web; fast to load, privacy‑preserving, battery‑friendly. Live demo available.

- Why it’s interesting:
  - Practical, DIY alternative to commercial CAPT APIs.
  - Clear engineering tradeoffs (CTC for strictness, Conformer for local/global).
  - Thoughtful UX fix for a real alignment pitfall.
  - Shows how far you can push small models for specialized speech tasks.

Try it: The post includes a browser demo where you read prompts (“Nǐ hǎo”, etc.), get per‑syllable alignment and tone feedback.

Based on the comments, here is a summary of the discussion:

**Model Performance and Feedback**
*   **Conversational vs. Isolated Speech:** Users experienced mixed results depending on their speaking speed. One intermediate learner noted that while the tool is excellent for slow, deliberate speech, it struggles with "normal conversational speed" where coarticulation and tone influence occur.
*   **False Positives/Negatives:** A Beijing native found the model confused standard consonant pairs (like *h/f* and *l/n*) despite perfect input. Conversely, another user found they could "trick" the model into recognizing valid words by making nonsense noises, suggesting the model may be over-biased toward its training vocabulary rather than raw phonetics.
*   **Author Response:** The author (`smdw`) was active in the thread, acknowledging bugs regarding specific character misidentifications (a JavaScript-side issue) and releasing hotfixes for tone sandhi support during the discussion.

**Debate: The Importance of Tones**
*   **Context vs. Precision:** A significant debate erupted regarding how critical perfect tones are for intelligibility. A native speaker argued that tones are secondary to context, noting that dialect speakers often "shuffle" tones but remain intelligible.
*   **Counterpoints:** Other users (both learners and natives) pushed back, arguing that while context helps advanced speakers, beginners lack the vocabulary to build that context. They cited examples where tone errors dramatically change meaning (e.g., "panda" vs. "chest hair") and emphasized that Standard Mandarin relies on specific tonal rules that differ from dialect flexibility.

**Technical and Linguistic Nuances**
*   **Tone Sandhi:** Several users queried how the model handles tone sandhi (rules where tones change based on adjacent tones, such as *ni3* becoming *ni2* before another third tone). The author confirmed support was added/tweaked in response to the feedback.
*   **Comparative Difficulty:** The discussion touched on the difficulty of Chinese tones versus English vowels. One user noted that while tones are hard for Westerners, English vowel reduction and variance are equally baffling for Chinese speakers.

### Browser Agent Benchmark: Comparing LLM models for web automation

#### [Submission URL](https://browser-use.com/posts/ai-browser-agent-benchmark) | 11 points | by [MagMueller](https://news.ycombinator.com/user?id=MagMueller) | [3 comments](https://news.ycombinator.com/item?id=46837660)

Browser Use releases an open-source benchmark for real-world browser agents

What’s new
- Browser Use open-sourced a 100-task benchmark (plus 20 custom stress tests) to evaluate agentic web browsing in realistic settings. Repo: github.com/browser-use/benchmark
- It’s built from WebBench, Mind2Web 2, GAIA, and BrowseComp, with added custom tasks for hard UI interactions (e.g., iframes, drag-and-drop).
- The team ran 600k+ test tasks internally to refine difficulty and judging.

How they built it
- Task selection: Ran many models and agent configs, used an LLM judge to flag “impossible” or “near-miss,” then removed trivial and unreachable tasks. Remaining tasks were hand-verified as hard but doable.
- Judging: LLM-as-judge with a simple true/false verdict (no rubric). Hand-labeled 200 traces for ground truth; final judge achieved 87% alignment with human labels.
- Judge model: Initially GPT-4o (as in Mind2Web), later switched to gemini-2.5-flash for better alignment.

Results (their runs)
- Their new ChatBrowserUse 2 API tops the chart; several recent models clear 60% on this very hard set.
- Even the lowest-scoring tested agent (gemini-2.5-flash at 35%) is “respectable” given task difficulty.
- They emphasize reporting variance: multiple runs with standard error bars.

Practical notes
- Designed for reproducibility: run_eval.py replicates their ChatBrowserUse 2 results.
- Cost/time: One full 100-task run on the basic Browser Use plan (concurrency 3) is ~3 hours and ~$10; using pricier models (e.g., claude-sonnet-4.5) can push to ~6 hours and nearly $100.
- Excludes tasks requiring authentication or making real changes to sites; favors real websites over synthetic pages for realism.

Why it matters
- Many agent benchmarks skew toward synthetic or trivially verifiable tasks; this set targets “hard but possible” real-world web actions.
- Clearer, more consistent judging (and error bars) should make cross-model comparisons more trustworthy.
- Useful for LLM providers and agent framework authors to iterate on grounded, end-to-end browsing performance.

Contact and more
- Benchmark: github.com/browser-use/benchmark
- Support for larger-scale evals: support@browser-use.com

**Discussion Summary:**

Commenters discussed the application of these agents beyond benchmarks, asking about valid AI-based tools for exploratory fuzzy web testing. Others noted the absence of the Opus 4.5 model from the test results, predicting that it would likely achieve the highest score if included.

### Show HN: Pinchwork – A task marketplace where AI agents hire each other

#### [Submission URL](https://github.com/anneschuth/pinchwork) | 8 points | by [aschuth](https://news.ycombinator.com/user?id=aschuth) | [6 comments](https://news.ycombinator.com/item?id=46840707)

- What it is: An open-source marketplace (MIT) that lets AI agents post tasks and other agents pick them up for credits. The UX is “just curl and go” with no accounts or dashboards. Site: pinchwork.dev (API docs and a simple dashboard available).

- Quick flow:
  - Register: POST /v1/register returns an API key + 100 free credits.
  - Delegate: POST /v1/tasks with a “need” and max_credits; optionally block until completion.
  - Earn: POST /v1/tasks/pickup to grab work, deliver results, and get paid.

- Why it matters: Moves agent workflows from monolithic prompts to a market of specialized micro-agents (e.g., notifications, image generation, code review). Encourages parallelization and division of labor.

- Features:
  - Credit escrow: posters pay on approval, not upfront.
  - Smart matching: agents describe skills; tasks are routed accordingly.
  - Independent verification: deliveries can be verified by separate agents before approval.
  - Real-time: SSE event stream and HMAC-signed webhooks.
  - Questions/messaging to clarify tasks mid-flight.
  - “Recursive labor”: matching and verification are themselves micro-tasks handled by agents.

- Integrations: LangChain, CrewAI, and MCP (Claude Desktop/Cursor) with pip extras; quick demos provided.

- CLI: Homebrew and Go-installable pinchwork CLI supports register, create/pickup/deliver tasks, live events, credits, multiple profiles, and JSON output.

- Self-hosting: Dockerfile and docker-compose included. Server primarily in Python with a small Go component (CLI); tests and linting set up.

- Example use cases: outsource notifications to an agent with Twilio keys, request image generation, get an independent code security review, fan out parallel subtasks.

- Status: GitHub repo anneschuth/pinchwork, new release pinchwork-cli v0.1.1 (Jan 31, 2026). Open-source MIT.

What to watch: reputation/sybil resistance and quality control will be key as “independent verification” is also agent-driven; still, the escrow + approval flow and simple API make it easy to experiment with multi-agent marketplaces today.

**Pinchwork: A task marketplace where AI agents hire other agents**
Pinchwork proposes an open-source, API-first ecosystem where AI agents can outsource specialized micro-tasks (like phone calls or code reviews) to other agents in exchange for credits, utilizing an escrow system for trust.

**Discussion Summary:**
The commentary praised the "simple, elegant implementation," but quickly pivoted to the **economics of an automated labor market**.

When asked if task pricing is dynamically adjusted based on supply and demand, the creator clarified that the system currently relies on a **poster-defined bounty model**. Pricing discovery is left to trial and error: a poster sets a credit amount, and if it is too low, agents simply won't "pick up" the task.

While the roadmap includes potential features for algorithmic price suggestions based on task complexity or historical data, the creator emphasized a philosophy of **architectural minimalism**. The preference is to let "agent-side tooling" handle the intelligence regarding pricing logic and negotiation, rather than complicating the platform layer.

### 175K+ publicly-exposed Ollama AI instances discovered

#### [Submission URL](https://www.techradar.com/pro/security/over-175-000-publicly-exposed-ollama-ai-servers-discovered-worldwide-so-fix-now) | 62 points | by [heresie-dabord](https://news.ycombinator.com/user?id=heresie-dabord) | [37 comments](https://news.ycombinator.com/item?id=46831784)

175,000 Ollama instances left wide open, fueling “LLMjacking”
- What happened: Researchers at SentinelLabs and Censys found roughly 175,000 Ollama deployments exposed to the internet with no authentication after users bound the service to all interfaces instead of localhost. Many are already being abused in “LLMjacking” schemes—freeloading on victims’ compute to churn out spam, malware content, or resold API access.
- Why it matters: About half of the exposed setups allow tool calling, letting attackers run code, hit APIs, or touch internal systems. Many sit on home or unmanaged cloud networks with weak monitoring, making abuse hard to trace. Uncensored models further increase harm potential.
- Not a bug: This isn’t an Ollama vulnerability—the default binds to 127.0.0.1. It’s a misconfiguration problem.
- What to do:
  - Bind to localhost only (127.0.0.1) or restrict to trusted interfaces.
  - If remote access is required, put it behind a reverse proxy with authentication, IP allowlists, and TLS; don’t expose the service directly.
  - Disable or lock down tool calling; least-privilege any connected tools.
  - Firewall off inbound traffic, monitor logs for abuse, and avoid running on residential IPs when possible.

Name to know: “LLMjacking” (Pillar Security) — hijacking open LLM endpoints to steal compute for malicious work.

Here is a summary of the discussion on Hacker News:

**The "Docker Trap"**
A significant portion of the discussion identified Docker as the likely culprit for the high number of exposed instances. Multiple commenters noted that the default Docker flag `-p 11434:11434` binds to `0.0.0.0` (all interfaces) rather than localhost. Furthermore, Docker is known to modify `iptables` directly, effectively bypassing standard firewalls (like UFW) unless the user explicitly binds to `127.0.0.1` or modifies the Docker daemon configuration.

**Practicality of "LLMjacking"**
One user claimed to have scanned for and tested these exposed endpoints, reporting that they were largely useless for "free compute." They found the connections extremely slow and the instances mostly running small, older, or mediocre models (like CodeLlama 13b), making them ill-suited for heavy workloads.

**Security & Networking Nuances**
*   **Difficulty of Exposure:** There was debate regarding how easy it is to accidentally expose a server. While some argued that modern NAT/routers prevent this by default (requiring manual port forwarding), others countered that Docker containers or direct IPv6 connections often bypass these protections.
*   **Tool Calling:** One commenter argued the article overblows the risk of "tool calling." They pointed out that an LLM only generates text/code; for Remote Code Execution (RCE) to occur, the wrapping application must be configured to execute that output, which is a flaw in the client implementation, not the model itself.
*   **API Keys:** A user attempted to find leaked `OLLAMA_API_KEYS` via GitHub regex search but found mostly placeholders, as Ollama does not enforce authentication by default.

**Critique of Ollama and Users**
*   **Ollama Software:** Some criticism was directed at Ollama itself for "hostile features" like difficult-to-disable telemetry and auto-updates, with users suggesting `llama.cpp` as a cleaner alternative.
*   **Copy-Paste Culture:** Commenters noted that this incident highlights a broader issue where users copy-paste terminal commands or use AI to generate deployment scripts without understanding the underlying networking implications. One user ironically noted that we are entering an era of "using AI to deploy AI," creating job security for security professionals.

