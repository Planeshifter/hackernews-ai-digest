## AI Submissions for Sun Dec 03 2023 {{ 'date': '2023-12-03T17:10:56.866Z' }}

### Stuxnet Source Code

#### [Submission URL](https://github.com/research-virus/stuxnet) | 158 points | by [Jimmc414](https://news.ycombinator.com/user?id=Jimmc414) | [106 comments](https://news.ycombinator.com/item?id=38511563)

The top story on Hacker News today is the public release of the Stuxnet malware's open-source code. Stuxnet, also known as MyRTUs, is a notorious piece of malware that was discovered in 2010 and is believed to have been developed as a cyberweapon to target Iran's nuclear program. The code provided in this repository was extracted from Stuxnet binaries and is now being made available for analysis and research purposes. The repository contains not only the Stuxnet code but also a rootkit source code. The authors of the code, Christian Roggia and Amr Thabet, have copyrighted their work, but they have made it available for free and have only asked for recognition and credit for their hard work. This release of Stuxnet's code is expected to provide valuable insights into the workings of the malware and help researchers better understand its capabilities and implications.

The discussion on the release of Stuxnet's open-source code touches on various topics. One user points out that the code contains interesting things related to the development of the malware. Another user discusses the involvement of Israel in the Stuxnet attack on Iran's nuclear program, while another user mentions a joint effort between the US and Israel. The conversation then veers towards the potential risks of mobile devices and the combination of GPS, audio, and video surveillance. There is a mention of a strategy involving tailored cyber warfare and the possibility of self-delivering non-nuclear weapons. The conversation also touches on genetic modifications and testing protocols, French nuclear weapons programs, and the relevance of a Darknet Diaries episode on the topic. The discussion dives into technical aspects of Stuxnet, such as how it did not break the working regime of the control systems and the difficulty of recovering source code. Users discuss obfuscating code, the possibility of decrypting binaries, and the availability of similar techniques since the 1980s. Overall, the discussion covers a wide range of topics related to Stuxnet and cybersecurity.

### Watsonx: IBM's code assistant for turning COBOL into Java

#### [Submission URL](https://www.pcmag.com/articles/ibms-plan-to-update-cobol-with-watson) | 116 points | by [Anon84](https://news.ycombinator.com/user?id=Anon84) | [174 comments](https://news.ycombinator.com/item?id=38508250)

IBM is developing an AI-powered code assistant called WatsonX to modernize COBOL, a programming language widely used in industries such as banking, insurance, and healthcare. COBOL, which handles $3 trillion worth of transactions each day, is becoming increasingly difficult to maintain as programmers with expertise in the language retire. WatsonX aims to save coders time by converting COBOL code into a more modern language. The process involves breaking down the application into smaller pieces and selectively choosing which parts to modernize. However, skeptics have raised concerns about IBM's previous AI project, Watson Health, which failed to deliver on its promises. While WatsonX is still in its early stages, IBM remains optimistic about its potential.

The discussion on the submission covers various perspectives on IBM's AI-powered code assistant, WatsonX, and the challenges of modernizing COBOL.

- Some users express skepticism about IBM's track record with previous AI projects like Watson Health, raising concerns about the success of WatsonX.
- Others argue that the management problem in AI projects can hinder their effectiveness, as AI may not be able to fix management issues.
- One user mentions that the demand for COBOL programmers is high, leading to higher salaries for contractors in the insurance industry.
- There is a discussion about the difficulty of migrating COBOL systems to modern languages, with some users suggesting that rewriting the code from scratch is not feasible due to the complexity and compatibility issues.
- The performance implications and technical challenges of converting COBOL to Java are also discussed.
- Some users point out that COBOL has specific features, such as global variables, that make it challenging to convert to Java.
- Additionally, there is a debate about the motivations and limitations of rewriting COBOL code, with some users suggesting that it is more practical to maintain and modernize legacy systems rather than rewriting them entirely.
- The discussion also touches on the topic of technical debt and the risks and benefits of adopting new technologies versus maintaining existing systems.

Overall, the conversation covers various perspectives on the challenges of modernizing COBOL and the potential efficacy of IBM's WatsonX in addressing these challenges.

### 1960s chatbot ELIZA beat OpenAI's GPT-3.5 in a recent Turing test study

#### [Submission URL](https://arstechnica.com/information-technology/2023/12/real-humans-appeared-human-63-of-the-time-in-recent-turing-test-ai-study/) | 132 points | by [layer8](https://news.ycombinator.com/user?id=layer8) | [44 comments](https://news.ycombinator.com/item?id=38506175)

A preprint research paper titled "Does GPT-4 Pass the Turing Test?" conducted by researchers from UC San Diego compares OpenAI's GPT-4 AI language model to human participants, GPT-3.5, and the 1960s computer program called ELIZA. The study found that human participants were only able to correctly identify other humans in 63% of interactions, and that ELIZA outperformed the GPT-3.5 AI model. However, GPT-4 achieved a success rate of 41%, second only to actual humans. The study raises questions about using the Turing test to evaluate AI model performance and the limitations of current AI models.

In the discussion, there are different viewpoints regarding the research paper and the Turing test. Some participants argue that the Turing test is flawed and that there are better ways to evaluate AI models. They point out that the study shows the limitations of current AI models and raises questions about their performance compared to humans. Others discuss the historical significance of ELIZA and its comparison to modern AI models. Some participants also discuss the practical applications of AI and the importance of human oversight in customer support. The discussion touches on topics such as the nature of consciousness and the definition of intelligence. Overall, there is a mix of opinions and perspectives in the conversation.

### GPT-4 Can Almost Perfectly Handle Unnatural Scrambled Text

#### [Submission URL](https://arxiv.org/abs/2311.18805) | 199 points | by [saliagato](https://news.ycombinator.com/user?id=saliagato) | [137 comments](https://news.ycombinator.com/item?id=38506140)

In a recent study, researchers have found that GPT-4, a large language model, can almost perfectly handle and correct unnatural scrambled text. The researchers designed a suite called the Scrambled Bench to measure the capacity of language models to handle scrambled input. The experimental results showed that GPT-4 was able to reconstruct the original sentences from scrambled ones with an impressive 95% reduction in edit distance, even when all letters within each word were scrambled. This resilience displayed by GPT-4 is counter-intuitive, considering the severe disruption to input tokenization caused by scrambled text. The findings provide novel insights into the inner workings of large language models and their ability to handle unconventional textual input.

The discussion on this submission revolves around the ability of GPT-4, a large language model, to handle scrambled text and its implications. Some users express their surprise at GPT-4's capability to reconstruct original sentences from scrambled ones with a 95% reduction in edit distance. Others discuss the challenges in word segmentation and the use of backtracking. The conversation also touches on the limitations and imperfections of GPT-4 and the role of tokenization in language models. Some users experiment with feeding scrambled text into search engines and observe different results. The discussion concludes with users discussing alternative models and their success in similar tasks.

### Mozilla Lets Folks Turn AI LLMs into Single-File Executables

#### [Submission URL](https://hackaday.com/2023/12/02/mozilla-lets-folks-turn-ai-llms-into-single-file-executables/) | 69 points | by [anonymousiam](https://news.ycombinator.com/user?id=anonymousiam) | [3 comments](https://news.ycombinator.com/item?id=38503588)

Mozilla's innovation group has released an open-source method called "llamafile" to turn a set of weights into a single binary file, making it easier to distribute and run Large Language Models (LLMs). Llamafile supports six different operating systems and ensures that a particular version of an LLM remains consistent and reproducible. It uses the "Cosmopolitan" build-once-run-anywhere framework created by Justine Tunney, along with the llama.cpp tool. Sample binaries using different LLMs are available, with the LLaVA 1.5 LLM being the only one that can run on Windows due to the 4 GB limit on executable files.

The discussion about the submission revolves around the technical aspects of llamafile and how it can be beneficial in distributing and running Large Language Models (LLMs). Some users mention that distributing LLMs as multiple files can be challenging to distribute, as changes and tweaks to the software can lead to different results with different versions. One user mentions that llamafile supports multiple operating systems including macOS, Windows, Linux, FreeBSD, OpenBSD, and NetBSD. They also point out that using llamafile dramatically simplifies the distribution process of LLMs, ensuring that a specific version of an LLM remains consistent and reproducible indefinitely. Another user notes that llamafile relies on the Cosmopolitan build-once-run-anywhere framework developed by Justine Tunney, specifically llama.cpp. This framework is commended for its efficiency in running self-hosted LLMs. In the comments, a link to a related discussion is shared. It is not clear what the specific topic of that discussion is.

Finally, one user simply mentions "ppl" (presumably referring to people) and "llmcpp" without further clarification, leaving their comment's meaning open to interpretation.

### Run 70B LLM Inference on a Single 4GB GPU with This New Technique

#### [Submission URL](https://ai.gopubby.com/unbelievable-run-70b-llm-inference-on-a-single-4gb-gpu-with-this-new-technique-93e2057c7eeb?gi=cbe7920f4cd2) | 108 points | by [gardenfelder](https://news.ycombinator.com/user?id=gardenfelder) | [56 comments](https://news.ycombinator.com/item?id=38508571)

Have you ever wondered if it's possible to run inference on a single GPU with a large language model? Well, Gavin Li has come up with a new technique that allows you to do just that. In a recent article, he explains the key techniques for extreme memory optimization of large models. The most critical technique is layer-wise inference. By leveraging the divide and conquer approach, Li shows that you don't actually need to keep all layers in GPU memory. Instead, you can load whichever layer is needed from disk when executing that layer, do all the calculations, and then free up the memory. This significantly reduces the GPU memory required per layer.

Li also introduces flash attention, which is a critical optimization for large language models. Flash attention deeply optimizes CUDA memory access to achieve multi-fold speedups for inference and training. This optimization reduces the memory complexity from O(n²) to O(n), making it much more efficient. To further optimize the memory usage, Li discusses model file sharding. The original model file is typically sharded into multiple chunks, but loading the entire file for each layer execution wastes a lot of memory and disk reading time. By pre-processing the model file and sharding it by layers, the memory usage is minimized. To implement these techniques, Li utilizes the meta device feature provided by HuggingFace Accelerate. The meta device is a virtual device designed for running ultra large models. It allows you to dynamically transfer parts of the model from the meta device to a real device like the CPU or GPU during execution, reducing memory usage. If you're interested in trying out these optimizations for yourself, Li has open-sourced the code in a library called AirLLM. The library, which can be found on the Anima GitHub, allows you to achieve extreme memory optimization with just a few lines of code.

So there you have it, with these techniques and the AirLLM library, you can run inference on a single 4GB GPU with a 70B large language model. It's truly unbelievable!

The discussion on this submission revolves around various aspects of running inference on a single GPU with a large language model. Here are some key points from the comments:

- One commenter mentions that they tried running the techniques on a GTX 1060 6GB GPU but found it to be slow, taking 13 hours to generate a sentence. They speculate that the GPU memory usage might be the bottleneck.
- Another commenter discusses the technique of model file sharding and suggests looking into PyTorch's gradient checkpointing for efficient memory usage.
- A discussion arises regarding the difference in performance between CPU and GPU inference. Some commenters mention that CPU inference is typically limited by memory bandwidth, whereas GPU inference benefits from loading weights onto the GPU. SSD loading of weights is also mentioned as a way to reduce GPU memory usage.
- There is some skepticism raised regarding the capabilities of non-batched inference for large language models. Commenters mention that certain context and historical understanding provided by complete sentences might be necessary for relevant embeddings and projections in custom matrix operations.
- The potential drawbacks of extreme memory optimization are also discussed, with one commenter pointing out that the process seems to swap VRAM to RAM and disk, which can significantly impact performance.
- Various optimization techniques and resources are shared, including libraries such as AirLLM and llmcpp, as well as discussions on quantizing models and using DirectStorage for improved IO.
- Further discussions revolve around distributed computing and the benefits of using multiple GPUs to process different layers simultaneously, though latency is a concern in such setups.

Overall, the discussion explores different techniques, optimizations, and limitations related to running inference on a single GPU with a large language model.

