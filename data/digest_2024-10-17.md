## AI Submissions for Thu Oct 17 2024 {{ 'date': '2024-10-17T17:12:08.627Z' }}

### setBigTimeout

#### [Submission URL](https://evanhahn.com/set-big-timeout/) | 26 points | by [cfj](https://news.ycombinator.com/user?id=cfj) | [23 comments](https://news.ycombinator.com/item?id=41872010)

In a quirky twist for JavaScript developers, Evan Hahn has introduced *setBigTimeout*, a module designed to overcome the limitations of the native `setTimeout` function, which breaks down after approximately 25 days. While the standard function allows you to delay code execution using a 32-bit signed integer, this means any timeout over about 2.1 billion milliseconds results in unexpected behavior—typically executing the function immediately instead of waiting.

To address this, *setBigTimeout* chains together smaller timeouts, allowing for astronomical wait times, such as 84 years or even a million years. It preserves the native `setTimeout` functionality while enabling developers to explore thrillingly excessive delays. While it may seem absurd, for those who need extreme waiting periods (or just want some fun), this module is available on npm. 

Hahn's light-hearted solution not only tackles a known hiccup in the JavaScript ecosystem but also highlights the creative ways developers can innovate around existing limitations. Check out the module for an entertaining venture into the long-term execution of JavaScript functions!

The discussion surrounding the *setBigTimeout* module primarily focuses on the limitations of JavaScript's native `setTimeout` function, highlighted by various users who delve into technical specifications and potential edge cases:

1. **Integer Limitations**: Users noted that the JavaScript `setTimeout` function operates using a 32-bit signed integer, which restricts wait times to about 25 days. There was some confusion regarding how JavaScript handles numbers, with some participants discussing how the language represents numerical values and their implications for delays.

2. **Security Concerns**: Several comments pointed to potential security issues with `setTimeout`, specifically in scenarios where an attacker could exploit the timing mechanism. Discussions revolved around ensuring inputs are validated to prevent unexpected or malicious behaviors.

3. **Functionality of *setBigTimeout***: Participants expressed curiosity about how *setBigTimeout* chains smaller timeouts effectively and whether it adequately handles precision issues related to waiting times extending beyond the typical limits of `setTimeout`.

4. **Real World Applications**: Some users questioned the practical utility of such extended wait times, debating whether there were genuinely useful scenarios for needing delays in the range of years.

5. **Technical Exploration**: The conversation also touched on broader implications of the new module, with comments reflecting on performance, garbage collection, and concurrency patterns in asynchronous programming.

Overall, while the module brings a playful solution to a real limitation in JavaScript, the discussion underscores the balance between creativity in programming and the necessity for security and practical use cases.

### Grandmaster-level chess without search

#### [Submission URL](https://github.com/google-deepmind/searchless_chess) | 311 points | by [lawrenceyan](https://news.ycombinator.com/user?id=lawrenceyan) | [142 comments](https://news.ycombinator.com/item?id=41872813)

In a groundbreaking study, researchers from Google DeepMind have unveiled their new chess-playing model that operates at a grandmaster level without traditional search algorithms. Titled "Grandmaster-Level Chess Without Search," the model utilizes a 270 million parameter transformer, trained on an impressive dataset of 10 million chess games, leading to an astounding 15 billion annotated data points—thanks to insights from the powerful Stockfish 16 engine. 

This innovative approach challenges the conventional methods of chess engines by demonstrating that high-level performance can be achieved purely through scale and supervised learning. Achieving a staggering Lichess blitz Elo rating of 2895, the model not only eclipses the capabilities of AlphaZero's policy and value networks but also excels at complex chess puzzles—all without relying on domain-specific tweaks or exhaustive search methodologies.

Moreover, the research involved extensive ablation studies on model design and hyperparameters, confirming that only a sufficiently large model and dataset can yield superior chess performance. This breakthrough paves the way for new possibilities in AI chess and demonstrates the profound impact of scaling in AI training methodologies. The project is open-source, inviting enthusiasts and developers alike to explore its codebase on GitHub.

The discussion around DeepMind's new chess model, which plays at a grandmaster level without search algorithms, presents a range of perspectives and technical insights. Users debated the implications of this model on traditional chess engines, suggesting the necessity of high-level training and the potential to minimize blunders, which are typically made by human players.

Several comments focused on the model's ability to perform well against human opponents, with mentions of its impressive Lichess blitz rating of 2895. Some users speculated about the limitations and benchmarks set by other chess engines, like Stockfish, and how this new approach challenges the need for extensive search algorithms traditionally relied upon in AI chess.

The conversation also touched on the intricacies of chess ratings and the psychological aspects of playing against both AI and human opponents. Key points included concerns about the AI's ability to replicate human-like gameplay and the necessity for the model to be trained against a diversity of styles to understand common mistakes and develop robust strategies.

In addition, users shared their own experiences with developing or engaging with chess engines, making comparisons to other models like GPT-4 and discussing their perceived capabilities in playing chess. Overall, the discussion highlighted excitement over the innovative approach taken by DeepMind while also scrutinizing its practical applications and performance in real-world chess scenarios.

### The Fifth Generation Project in Japan (1992)

#### [Submission URL](https://www.sjsu.edu/faculty/watkins/5thgen.htm) | 109 points | by [tosh](https://news.ycombinator.com/user?id=tosh) | [97 comments](https://news.ycombinator.com/item?id=41874275)

In a reflective piece on the ambitious but ultimately unsuccessful Japanese Fifth Generation Project, the author Thayer Watkins delves into Japan's early attempts to outpace Western computer technology through advanced AI and innovative programming languages like PROLOG. Despite an initial investment of $400 million, the program, launched in 1982, failed to meet its lofty goals, leading to a dramatic shift in perspective just a decade later. Once seen as a potential threat to the U.S. tech industry, the project garnered criticism for its inability to adapt to the rapid evolution of computer technology, signaling a monumental disconnect with industry trends by 1992. 

Notably, while Japan’s Ministry of International Trade and Industry had ambitious visions, the project's path proved misaligned with the future of computing, ultimately leading to its dissolution and even the offering of its software for free. Despite its shortcomings, the Fifth Generation Project did contribute to upskilling engineers in advanced computer science, hinting at a glimmer of value in long-term research initiatives. As the piece closes, it suggests that lessons learned from this venture may still inform future tech collaborations, exemplified by Japan’s continued interest in new projects like the Real World Computing initiative.

The discussion surrounding Thayer Watkins' reflective piece on Japan's Fifth Generation Project includes a diverse range of perspectives on its implications and the broader context of Japan's technological endeavors. 

One participant notes the significant proportion of computer systems in Japan that have underperformed historically, attributing the failures to government-led initiatives that often lack adaptability to market needs and technological advancements. Another commenter shares insights from their long-term experience in Japan, reflecting on the complexities of Japanese bureaucracy, which can hinder progress despite notable infrastructure developments.

There's a consensus about the cultural factors influencing Japan's tech landscape, particularly a conservatism that affects innovation and risk-taking. This conservatism is contrasted with the greater freedom seen in other regions, impacting Japan's competitiveness in the global tech arena.

Comments also delve into the missed opportunities for collaboration between Japanese companies, citing the lack of shared standards and cooperative efforts as a barrier to advancement. The discussion highlights a nostalgic take on Japan's past successes in gaming and electronics, while also recognizing the struggle of current industries to establish themselves competitively against Western companies and emerging rivals.

Overall, the conversation reflects a melange of admiration for Japan's historical contributions to technology, tempered with critique of systemic issues that have contributed to its recent stagnation in innovation and global competitiveness.

### Adobe's new image rotation tool is one of the most impressive AI tools seen

#### [Submission URL](https://www.creativebloq.com/design/adobes-new-image-rotation-tool-is-one-of-the-most-impressive-ai-concepts-weve-seen) | 800 points | by [ralusek](https://news.ycombinator.com/user?id=ralusek) | [260 comments](https://news.ycombinator.com/item?id=41870040)

At Adobe's annual MAX conference, excitement buzzed as the company revealed its latest innovations, particularly through a captivating segment known as 'Sneaks.' Among the standout concepts showcased was Project Turntable, an ambitious tool designed to revolutionize how users interact with 2D vector art. This innovative project allows creators to rotate their flat designs into a 3D view while ensuring the final image remains distinctly flat, preserving the original artistic essence.

Developed by research scientist Zhiqin Chen, Project Turntable demonstrates remarkable AI capabilities by seamlessly filling in visual gaps when the art is rotated. For instance, observers were awed as a simple 2D illustration of a horse appeared to sprout an additional pair of legs during the rotation. Although there's no certainty that this feature will reach the market, it is poised to capture attention, reflecting the groundbreaking advancements being made in the realm of design.

In addition to Project Turntable, Adobe rolled out over 100 new creator-centric features, setting the stage for an exciting week for AI developments, alongside major announcements from industry players like Tesla and Meta. With its innovative ideas and tools, Adobe continues to push the boundaries of creativity.

At Adobe's MAX conference, the unveiling of new projects, especially Project Turntable, sparked diverse discussions among Hacker News users. Participants expressed skepticism regarding Adobe's approach to AI and the effectiveness of their product development strategies. Some commenters criticized Adobe's trend of using AI buzzwords without substantial innovations, although others acknowledged the potential for transformative tools like Turntable. 

The conversation revealed a split between those who are excited about the advancements in AI-driven design tools and those who worry that these innovations might only serve as flashy marketing tactics rather than substantive improvements. Concerns about the practical applications of features introduced, and their alignment with user needs were common, with some users recalling previous hype cycles that didn't deliver.

Additionally, users weighed in on Adobe's competitive position against other companies, highlighting the importance of user feedback and real-world utility over speculative features. The tone of the discussion fluctuated between hopefulness about creativity's evolution with AI and skepticism about the commercial motivations behind these releases. Overall, the excitement surrounding Project Turntable was tempered by critical perspectives on Adobe's broader direction in AI product development.

### NotebookLM launches feature to customize and guide audio overviews

#### [Submission URL](https://blog.google/technology/ai/notebooklm-update-october-2024/) | 320 points | by [alphabetting](https://news.ycombinator.com/user?id=alphabetting) | [117 comments](https://news.ycombinator.com/item?id=41871262)

NotebookLM, a sophisticated tool powered by Gemini 1.5 designed to help users decode complex information, has just announced exciting updates that elevate its functionality. With the addition of customizable Audio Overviews, users can now tailor AI-hosted conversations by providing specific instructions on topics and expertise levels prior to generating the audio content. This feature aims to create a more interactive and relevant learning experience, allowing users to maintain productivity while listening to these overviews.

In tandem with these enhancements, NotebookLM is set to launch a business-oriented version, NotebookLM Business, tailored for organizations and educational institutions. This version promises improved features while prioritizing data privacy and security, aligning with the needs of over 80,000 organizations already utilizing the platform. Interested users can apply for the pilot program to gain early access to these new features and support, marking a significant step toward broader application in professional settings.

As NotebookLM sheds its "experimental" label, it continues to expand its offerings, making it a compelling choice for anyone looking to navigate and comprehend complex material more effectively.

The discussion on Hacker News surrounding the submission about NotebookLM's enhancements highlights a mix of excitement and skepticism regarding AI-generated podcasts. Users are intrigued by the new features, particularly the customizable audio overviews and the launch of NotebookLM Business, suggesting that these updates could enhance productivity and the learning experience.

Several commenters noted a shift towards AI-generated content in podcasts, expressing concern about the quality of such outputs compared to human-generated content. While some users reported a positive transition to NotebookLM for podcast listening, citing higher quality and relevance, others criticized the inconsistency in AI-generated audio, leading to discussions about information accuracy and engagement.

Concerns regarding privacy and data security in an increasing reliance on AI tools were also prevalent. Some highlighted the potential risks of AI-generated media diminishing the quality of information and misinforming listeners, while others defended AI's role in enhancing content delivery and accessibility.

Overall, the conversation reflects a cautious optimism about the future of AI in educational and productivity tools, balanced by reservations over quality and the implications of replacing human-generated content.

### The Prompt() Function: Use the Power of LLMs with SQL

#### [Submission URL](https://motherduck.com/blog/sql-llm-prompt-function-gpt-models/) | 50 points | by [sebg](https://news.ycombinator.com/user?id=sebg) | [14 comments](https://news.ycombinator.com/item?id=41873801)

In a significant stride towards making advanced AI accessible, MotherDuck has introduced the new `prompt()` function that integrates small language models (SLMs) like OpenAI's gpt-4o-mini into SQL workflows. This feature is currently available in preview and aims to streamline the process of generating, summarizing, and extracting structured data straight from SQL queries—eliminating the need for separate infrastructure.

The `prompt()` function enables users to easily apply LLM capabilities to their datasets, allowing for features such as bulk text summarization. For example, users can summarize Hacker News comments into a concise Haiku with a single SQL command, drastically reducing the processing time compared to traditional Python loops.

Furthermore, it supports structured outputs by leveraging predefined schemas, making it easier to convert unstructured data into actionable insights. Users can define the format of the output, ensuring the responses conform to specific data types and descriptions, such as extracting sentiment and technologies mentioned within comments.

While the `prompt()` function opens doors to innovative use cases, it is essential for users to experiment with smaller datasets first and choose LLMs judiciously based on context. With this tool, MotherDuck is set to enhance how developers and data analysts interact with language models in their SQL environments, paving the way for faster and more efficient data analysis.

The discussion surrounding MotherDuck's launch of the `prompt()` function on Hacker News reflects a mix of excitement and skepticism among users regarding its implications in SQL and LLM integration. 

Key points include:

1. **Function Limitations**: Users pointed out concerns about the deterministic nature of LLMs in SQL, citing that while SQL functions can be deterministic, LLM outputs often aren't, which could lead to inconsistencies in results.

2. **Use Case Simplicity**: Some participants celebrated the function’s ability to handle simple tasks effectively, emphasizing how small language models can streamline operations like text summarization.

3. **Performance Insights**: Comments also touched on the performance aspects of LLM implementations, mentioning aspects like floating-point arithmetic and the potential for random outcomes in model responses, indicating the complexity of ensuring predictable outputs.

4. **Documentation Issues**: Users noted recent struggles with OpenAI's API documentation, particularly with understanding prompt constraints and changes that might impact workflows, highlighting the need for clearer guidance.

5. **Commercial Concerns**: Some expressed worry about changing model functions from commercial providers, emphasizing the need for transparency in how LLMs handle user data.

Overall, users remain curious about the potential of the `prompt()` function while urging caution regarding its integration and performance within SQL environments.

### Kagi Update: AI Image Filter for Search Results

#### [Submission URL](https://help.kagi.com/kagi/features/exclude-ai-images.html) | 265 points | by [lkellar](https://news.ycombinator.com/user?id=lkellar) | [100 comments](https://news.ycombinator.com/item?id=41873204)

Kagi has launched an innovative AI Image Filter aimed at enhancing image search results by prioritizing authentic, human-created images over AI-generated content. As online searches increasingly return images produced by AI, users can find their results cluttered with these artificial visuals. 

To combat this, Kagi's new feature automatically downranks images sourced from websites heavily populated with AI-generated content. Additionally, thumbnails of potential AI images now carry identifiable badges, allowing users to easily spot them. For those seeking a more tailored experience, Kagi allows users to completely exclude websites with AI images from their results, although it notes that the filter's effectiveness relies more on website reputation than precise image analysis.

While Kagi acknowledges the complexities in accurately detecting AI-generated images, the feature is designed to improve visibility for genuine content based on user feedback and is enabled by default. As they continue to refine this capability, Kagi encourages user feedback to enhance their search tools further.

The discussion surrounding Kagi's new AI Image Filter highlights a variety of user experiences and opinions about the effectiveness and practicality of the service. Here's a summary of the key points:

1. **User Feedback**: Many users are appreciative of Kagi's efforts to downrank AI-generated images and favor authentic content. Some have shared positive experiences, noting that Kagi offers better search results compared to traditional engines like Google and DuckDuckGo.

2. **Mixed Experiences**: Some participants expressed frustration with Kagi's current performance, indicating that while it has improved over time, there are still issues with search quality, particularly in specific queries or local searches. Users reported that they found themselves reverting to Google for more reliable results.

3. **Suggestions for Improvement**: Users suggested that Kagi could further enhance its service by refining its filtering system and improving local search capabilities. There were requests for better handling of specific queries and for more transparent feedback mechanisms.

4. **Concerns About AI Detection**: A few users raised questions about Kagi's ability to accurately detect AI-generated images, with discussions hinting that AI detection in itself presents challenges. Some noted skepticism about the reliability of the filtering process.

5. **General Sentiment**: Overall, the sentiment seems to be a mix of hope and caution. While users appreciate Kagi's unique approach to image searching and its dedication to prioritizing human-generated content, there remains a critical perspective on the practical functionalities and results.

This commentary reflects ongoing user engagement with Kagi's services and the community's interest in evolving solutions in the realm of search engines amidst the rising prevalence of AI-generated content.

### Salesforce CEO Marc Benioff blasts Microsoft's Copilot: 'It just doesn't work'

#### [Submission URL](https://fortune.com/2024/10/17/salesforce-ceo-marc-benioff-blasts-microsoft-ai-copilot/) | 29 points | by [breadwinner](https://news.ycombinator.com/user?id=breadwinner) | [26 comments](https://news.ycombinator.com/item?id=41874006)

In a fiery critique, Salesforce CEO Marc Benioff has openly slammed Microsoft's AI tool, Copilot, describing it as "disappointing" and ineffective. This comes as he compares it to the notorious Clippy, the long-deprecated assistant from past Microsoft Office iterations, implying it may share a similar fate. Benioff's comments suggest that users have struggled to find real value in Copilot, with a survey from Gartner revealing that only 6% of surveyed IT leaders have moved towards adopting the tool widely.

During a podcast interview, he emphasized the poor customer experience with Copilot and highlighted his confidence in Salesforce’s own AI offering, Agentforce, which he believes has the potential to revolutionize enterprise productivity. His critical remarks mark a continuation of a growing rivalry with Microsoft, as he questions whether Copilot will survive against the advances of Salesforce's AI products. Meanwhile, Microsoft has not responded to these recent critiques. As the tech industry evolves rapidly, the effectiveness of AI tools like Copilot will be closely monitored by both customers and competitors alike.

In a recent discussion on Hacker News about Salesforce CEO Marc Benioff's critique of Microsoft's AI tool Copilot, users expressed varying opinions on the effectiveness of both companies' AI offerings. Some comments focused on the user experience and accuracy of Microsoft’s Copilot, with several users noting challenges in features like calendar integration and transcription reliability. There were mentions of frustrations over incorrect meeting notes being generated and concerns about AI's tendency to produce misleading results.

Conversely, there was acknowledgment of Salesforce's competitive advantage with its own AI product, Agentforce, which Benioff praised during his interview. Some users believed that Salesforce’s deep integration with tools like Slack could give it an edge over Microsoft Teams. Others discussed the broader implications of Microsoft's AI strategy and its impact on productivity tools like GitHub and IntelliJ.

The debate highlighted contrasting perceptions of each company's approach to AI—Microsoft's Copilot was seen as struggling, while Salesforce's products were viewed more favorably. Overall, the comments reflected the ongoing rivalry between the two tech giants as they navigate the rapidly evolving AI landscape.
