## AI Submissions for Tue Sep 12 2023 {{ 'date': '2023-09-12T17:09:59.533Z' }}

### Gmail and Instagram are training AI, and thereâ€™s little you can do about it

#### [Submission URL](https://www.washingtonpost.com/technology/2023/09/08/gmail-instagram-facebook-trains-ai/) | 74 points | by [bookofjoe](https://news.ycombinator.com/user?id=bookofjoe) | [75 comments](https://news.ycombinator.com/item?id=37487926)

In a recent analysis by Geoffrey A. Fowler for The Washington Post, it was revealed that tech companies such as Google, Meta (formerly Facebook), and Microsoft are using users' data from platforms like Gmail and Instagram to train their artificial intelligence (AI) systems. For example, Google uses users' Gmail responses to train its AI to finish other people's sentences, and Meta took a billion Instagram posts without permission to train its AI. Microsoft uses users' chats with Bing to improve its AI chatbot, and there is no way for users to opt out of this. This trend of using personal data to train AI raises concerns about privacy and the potential misuse of users' information. While these companies do use data for targeted ads, this new development involves using data to create new technologies that could further expand these tech giants' power and influence. Users have little control over how their data is being used for AI training, and the implications for privacy and personal information are not fully understood at this point.

The discussions on this submission touch on several points. One user points out that objecting to the practices of big tech companies is important, and suggests moving away from platforms that engage in data collection. Another user raises the issue of privacy and the need for clearer definitions and regulations. There is also a discussion about the technical aspects of data encryption and the potential risks of intercepted emails. Some users emphasize the importance of using encryption methods to protect private communications. Another user mentions that companies like Google and Meta have terms of service agreements that allow them to use user data, but it is not clear whether users fully understand the implications of these agreements. There is also a discussion about alternative email services and the advantages and disadvantages of using platforms like Gmail. One user mentions the convenience of Gmail's features, while another user expresses concerns about privacy and the potential misuse of personal data. Lastly, a user suggests stopping the use of Gmail altogether. Overall, the discussions revolve around privacy concerns, alternatives to mainstream platforms, and the need for clearer regulations and user control over data usage.

### Simulating History with ChatGPT

#### [Submission URL](https://resobscura.substack.com/p/simulating-history-with-chatgpt) | 163 points | by [arbesman](https://news.ycombinator.com/user?id=arbesman) | [77 comments](https://news.ycombinator.com/item?id=37480155)

In this article, Benjamin Breen shares his experience of using large language models (LLMs) like ChatGPT as a teaching tool in his history classes. He believes that LLMs can be used to simulate interactive historical settings, allowing students to engage with different historical scenarios. While he acknowledges that these simulations are not always accurate and may contain falsehoods and hallucinations, he sees the potential of using LLMs as a way to enhance the teaching of history. He argues that LLMs can help elevate the importance of the humanities in higher education, as they rely on textual skills and methods that are emphasized in humanities classes. However, he also recognizes that there will be challenges in incorporating LLMs into assignments, as professors will need to rethink their teaching methods. Overall, Breen believes that LLMs have the potential to positively impact higher education and the study of history.

The discussion revolves around the potential use of large language models (LLMs) like ChatGPT as teaching tools in history classes. Some users express skepticism about the accuracy and reliability of LLMs, noting that they may generate falsehoods and hallucinations. Others discuss the possibilities of using LLMs to create historical simulations and interactive scenarios. Some users suggest incorporating regional instructions and prompts to enhance the educational experience. There are also discussions about using LLMs for language learning, critiquing the lack of personal interaction, and drawing parallels to previous software like Timothy Leary's Mind Mirror. One user suggests the idea of using LLMs to create historical simulations on a dedicated platform, while another user expresses interest in developing a web application for historical simulations using APIs and AI.

### Fandom can't decide if leaked songs are real or AI-generated

#### [Submission URL](https://www.404media.co/harry-styles-one-direction-ai-leaked-songs/) | 73 points | by [wpietri](https://news.ycombinator.com/user?id=wpietri) | [109 comments](https://news.ycombinator.com/item?id=37482455)

A controversy is brewing in the Harry Styles fandom over supposed leaked songs that may or may not be AI-generated. Discord communities within the fandom have been selling snippets of unreleased songs, prompting speculation about their authenticity. The situation has become a community-wide obsession, with fans conducting investigations to determine the legitimacy of the tracks. Some users are claiming that the leaked songs are AI-generated, while others argue that certain tracks sound authentic compared to low-quality AI covers. The underground leaked song industry has become increasingly complex, with AI-generated music now reaching a level of sophistication that can deceive the human ear. While some fans are readily paying for these songs, others remain skeptical and believe that they could be total fabrications. Two users, Liz and Haley, have been warning others about the possibility of fake songs by documenting their suspicions in Twitter threads, which have sparked anger among those selling the tracks.

The discussion surrounding the submission revolves around different aspects of AI-generated music and its impact on various industries. Some users argue that AI-generated music is already disrupting the entertainment industry and infringing on copyrights, similar to what happened with Napster and Kazaa. They believe that AI will continue to replace human labor and that companies should invest in AI to stay competitive. However, others argue that the impact of AI on the music industry is overstated and that human creativity is still valuable. They also discuss the legal and ethical implications of AI-generated music, including copyright issues and the fear of artists losing control over their own work. Some users also mention the potential for AI to generate vocals and how it could revolutionize the creative sphere.

### How to run a competitive AI startup fundraise in 2023

#### [Submission URL](https://context.ai/post/how-to-run-a-competitive-ai-startup-fundraise-in-2023) | 38 points | by [henrysg](https://news.ycombinator.com/user?id=henrysg) | [13 comments](https://news.ycombinator.com/item?id=37481424)

In a recent blog post, the team at Context.ai shared their experiences and insights on how to approach an early-stage fundraising process. They likened the process to dating, emphasizing the importance of being friendly, informal, and not desperate. They also provided practical advice on preparing for the raise, crafting the pitch, and running the fundraising process. Some key takeaways include working backwards from business goals to determine the amount to raise, building a strong network of warm introductions, and using social proof to your advantage. They also stressed the importance of selecting the right partners and optimizing for quality over valuation. Overall, their advice offers valuable guidance for founders looking to raise funding in the near future.

The discussion surrounding the submission includes various comments from users. There is a tangent about the specific advantages and disadvantages that AI startups may face compared to non-AI startups. Some users find it interesting how prompting ChatGPT can generate article titles and sub-titles. Another user mentions the problem of investors valuing startups differently, and suggests that Whisper and Llama, two startups, have encountered this issue. They provide some insights on customer advantages and custom models. One user remarks that many VC-funded companies don't focus on producing a viable product and that VCs are the ones who really hold power. Another user questions the usability of LLM-specific analytics and asks if people typically switch analytics providers. There is a comment about the difficulties with tracking REST calls and the specific challenges faced by Langchain. Another user joins the discussion to mention a startup, GenesisAI, and their goal of addressing the limitations of current AI systems. A few users express their skepticism about AI companies and their generated messages. One user suggests an alternative article title. Finally, a user flags the discussion.

### Therac-25

#### [Submission URL](https://en.wikipedia.org/wiki/Therac-25) | 31 points | by [wazbug](https://news.ycombinator.com/user?id=wazbug) | [14 comments](https://news.ycombinator.com/item?id=37480795)

The Therac-25, a computer-controlled radiation therapy machine produced by Atomic Energy of Canada Limited (AECL), was involved in several accidents between 1985 and 1987, leading to massive overdoses of radiation and resulting in death or serious injury for some patients. These accidents shed light on the dangers of software control in safety-critical systems and have become a prominent case study in health informatics, software engineering, and computer ethics. The incidents were attributed to concurrent programming errors and the engineers' overconfidence in their initial work, as well as a lack of proper due diligence in resolving reported software bugs.

The discussion on the submission about the Therac-25 accidents covers several topics.  One user points out that it is important to note that people were killed in these accidents, emphasizing the severity of the consequences. Another user adds that in 1986, a programmer left AECL, and it was difficult to believe that records of the incident were lost. They also mention that it seems a settlement was reached without requiring the programmer to disclose qualifications or experience. A user shares a link to an article discussing causality in the Therac-25 incidents. The discussion then veers off to a different topic, with one user mentioning that software flaws can result in human deaths. They provide an example of a mass failure caused by a floating-point arithmetic error that resulted in the deaths of 28 people.

Another user brings up the recent high-profile case of the Boeing 737 MAX crashes caused by software failures, suggesting that software-induced plane crashes occur at an extremely high rate. A response to this mentions that the software failures were not the only cause of the Boeing 737 MAX crashes, but rather a combination of factors including regulatory oversight and modifications made without proper review. Another user suggests that studying software ethics is necessary. One user points out that the components prior to the Therac-25 were not properly tested, and they argue that regulation for medical equipment should ensure proper testing to prevent such incidents. A response to this brings up the issue of user interface changes in medical equipment and the potential dangers of confusing operators. They mention that regulating user interface changes in medical equipment is justified because unexpected changes can lead to confusion and potentially harm patients.

The discussion ends with a user mentioning the Ariane 5 crash as an example of a failure caused by inadequate testing of components prior to the system's launch.

