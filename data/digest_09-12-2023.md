## AI Submissions for Tue Sep 12 2023 {{ 'date': '2023-09-12T17:09:59.533Z' }}

### Gmail and Instagram are training AI, and there’s little you can do about it

#### [Submission URL](https://www.washingtonpost.com/technology/2023/09/08/gmail-instagram-facebook-trains-ai/) | 74 points | by [bookofjoe](https://news.ycombinator.com/user?id=bookofjoe) | [75 comments](https://news.ycombinator.com/item?id=37487926)

In a recent analysis by Geoffrey A. Fowler for The Washington Post, it was revealed that tech companies such as Google, Meta (formerly Facebook), and Microsoft are using users' data from platforms like Gmail and Instagram to train their artificial intelligence (AI) systems. For example, Google uses users' Gmail responses to train its AI to finish other people's sentences, and Meta took a billion Instagram posts without permission to train its AI. Microsoft uses users' chats with Bing to improve its AI chatbot, and there is no way for users to opt out of this. This trend of using personal data to train AI raises concerns about privacy and the potential misuse of users' information. While these companies do use data for targeted ads, this new development involves using data to create new technologies that could further expand these tech giants' power and influence. Users have little control over how their data is being used for AI training, and the implications for privacy and personal information are not fully understood at this point.

The discussions on this submission touch on several points. One user points out that objecting to the practices of big tech companies is important, and suggests moving away from platforms that engage in data collection. Another user raises the issue of privacy and the need for clearer definitions and regulations. There is also a discussion about the technical aspects of data encryption and the potential risks of intercepted emails. Some users emphasize the importance of using encryption methods to protect private communications. Another user mentions that companies like Google and Meta have terms of service agreements that allow them to use user data, but it is not clear whether users fully understand the implications of these agreements. There is also a discussion about alternative email services and the advantages and disadvantages of using platforms like Gmail. One user mentions the convenience of Gmail's features, while another user expresses concerns about privacy and the potential misuse of personal data. Lastly, a user suggests stopping the use of Gmail altogether. Overall, the discussions revolve around privacy concerns, alternatives to mainstream platforms, and the need for clearer regulations and user control over data usage.

### Simulating History with ChatGPT

#### [Submission URL](https://resobscura.substack.com/p/simulating-history-with-chatgpt) | 163 points | by [arbesman](https://news.ycombinator.com/user?id=arbesman) | [77 comments](https://news.ycombinator.com/item?id=37480155)

In this article, Benjamin Breen shares his experience of using large language models (LLMs) like ChatGPT as a teaching tool in his history classes. He believes that LLMs can be used to simulate interactive historical settings, allowing students to engage with different historical scenarios. While he acknowledges that these simulations are not always accurate and may contain falsehoods and hallucinations, he sees the potential of using LLMs as a way to enhance the teaching of history. He argues that LLMs can help elevate the importance of the humanities in higher education, as they rely on textual skills and methods that are emphasized in humanities classes. However, he also recognizes that there will be challenges in incorporating LLMs into assignments, as professors will need to rethink their teaching methods. Overall, Breen believes that LLMs have the potential to positively impact higher education and the study of history.

The discussion revolves around the potential use of large language models (LLMs) like ChatGPT as teaching tools in history classes. Some users express skepticism about the accuracy and reliability of LLMs, noting that they may generate falsehoods and hallucinations. Others discuss the possibilities of using LLMs to create historical simulations and interactive scenarios. Some users suggest incorporating regional instructions and prompts to enhance the educational experience. There are also discussions about using LLMs for language learning, critiquing the lack of personal interaction, and drawing parallels to previous software like Timothy Leary's Mind Mirror. One user suggests the idea of using LLMs to create historical simulations on a dedicated platform, while another user expresses interest in developing a web application for historical simulations using APIs and AI.

### Fandom can't decide if leaked songs are real or AI-generated

#### [Submission URL](https://www.404media.co/harry-styles-one-direction-ai-leaked-songs/) | 73 points | by [wpietri](https://news.ycombinator.com/user?id=wpietri) | [109 comments](https://news.ycombinator.com/item?id=37482455)

A controversy is brewing in the Harry Styles fandom over supposed leaked songs that may or may not be AI-generated. Discord communities within the fandom have been selling snippets of unreleased songs, prompting speculation about their authenticity. The situation has become a community-wide obsession, with fans conducting investigations to determine the legitimacy of the tracks. Some users are claiming that the leaked songs are AI-generated, while others argue that certain tracks sound authentic compared to low-quality AI covers. The underground leaked song industry has become increasingly complex, with AI-generated music now reaching a level of sophistication that can deceive the human ear. While some fans are readily paying for these songs, others remain skeptical and believe that they could be total fabrications. Two users, Liz and Haley, have been warning others about the possibility of fake songs by documenting their suspicions in Twitter threads, which have sparked anger among those selling the tracks.

The discussion surrounding the submission revolves around different aspects of AI-generated music and its impact on various industries. Some users argue that AI-generated music is already disrupting the entertainment industry and infringing on copyrights, similar to what happened with Napster and Kazaa. They believe that AI will continue to replace human labor and that companies should invest in AI to stay competitive. However, others argue that the impact of AI on the music industry is overstated and that human creativity is still valuable. They also discuss the legal and ethical implications of AI-generated music, including copyright issues and the fear of artists losing control over their own work. Some users also mention the potential for AI to generate vocals and how it could revolutionize the creative sphere.

### How to run a competitive AI startup fundraise in 2023

#### [Submission URL](https://context.ai/post/how-to-run-a-competitive-ai-startup-fundraise-in-2023) | 38 points | by [henrysg](https://news.ycombinator.com/user?id=henrysg) | [13 comments](https://news.ycombinator.com/item?id=37481424)

In a recent blog post, the team at Context.ai shared their experiences and insights on how to approach an early-stage fundraising process. They likened the process to dating, emphasizing the importance of being friendly, informal, and not desperate. They also provided practical advice on preparing for the raise, crafting the pitch, and running the fundraising process. Some key takeaways include working backwards from business goals to determine the amount to raise, building a strong network of warm introductions, and using social proof to your advantage. They also stressed the importance of selecting the right partners and optimizing for quality over valuation. Overall, their advice offers valuable guidance for founders looking to raise funding in the near future.

The discussion surrounding the submission includes various comments from users. There is a tangent about the specific advantages and disadvantages that AI startups may face compared to non-AI startups. Some users find it interesting how prompting ChatGPT can generate article titles and sub-titles. Another user mentions the problem of investors valuing startups differently, and suggests that Whisper and Llama, two startups, have encountered this issue. They provide some insights on customer advantages and custom models. One user remarks that many VC-funded companies don't focus on producing a viable product and that VCs are the ones who really hold power. Another user questions the usability of LLM-specific analytics and asks if people typically switch analytics providers. There is a comment about the difficulties with tracking REST calls and the specific challenges faced by Langchain. Another user joins the discussion to mention a startup, GenesisAI, and their goal of addressing the limitations of current AI systems. A few users express their skepticism about AI companies and their generated messages. One user suggests an alternative article title. Finally, a user flags the discussion.

### OpenAI confirms that AI writing detectors don’t work

#### [Submission URL](https://arstechnica.com/information-technology/2023/09/openai-admits-that-ai-writing-detectors-dont-work/) | 32 points | by [ohjeez](https://news.ycombinator.com/user?id=ohjeez) | [7 comments](https://news.ycombinator.com/item?id=37482423)

OpenAI has confirmed that AI writing detectors, including its own discontinued tool, are not reliable in distinguishing between AI-generated and human-generated content. These detectors often yield false positives and can be defeated by rephrasing. In addition, OpenAI clarified that its AI model, ChatGPT, has no knowledge of whether the text is AI-written or not. The company also warned about the potential for ChatGPT to confabulate false information. While automated AI detectors do not work effectively, humans familiar with a writer's style can often detect AI writing, and there are ways to identify sloppy attempts to pass off AI-generated work as human-written.

The discussion on this submission revolves around the reliability of AI writing detectors and the potential for AI-generated content to be mistaken as human-written. One commenter points out a previous discussion where OpenAI admitted that AI writing detectors are not effective. Another commenter expresses disbelief at the amount of time wasted on the idea of AI writing detectors. A subthread discusses the importance of preventing children from cheating, and another thread raises the question of the role of AI in college admissions. Finally, there is a brief exchange regarding the difficulty of tracking AGI progress.

### For the first time in years, I’m excited by my computer purchase

#### [Submission URL](https://changelog.complete.org/archives/10564-for-the-first-time-in-years-im-excited-by-my-computer-purchase) | 97 points | by [pabs3](https://news.ycombinator.com/user?id=pabs3) | [98 comments](https://news.ycombinator.com/item?id=37476107)

Introducing the Daily Hacker: Your AI-powered digest of the top stories on Hacker News!

Get ready to stay up-to-date with the latest from the tech world, all in a concise and engaging format. Whether you're a developer, entrepreneur, or simply curious about cutting-edge technology, the Daily Hacker has got you covered.

Our team of AI algorithms scours Hacker News to find the most intriguing and informative submissions, and then summarises them into bite-sized nuggets of knowledge. We filter out the noise, so you can focus on what truly matters.

From breakthroughs in artificial intelligence to thought-provoking discussions on software development practices, we cover a wide range of topics. Each summary aims to capture the essence, giving you a clear understanding of the story at hand.

Tired of spending hours scrolling through Hacker News? The Daily Hacker will save you time and deliver the most important stories right to your inbox or favorite news reader. Stay informed without the hassle.

Join thousands of others who rely on the Daily Hacker as their go-to source for staying in the know. Sign up today and unlock a world of tech insights, all conveniently delivered to you.

Get ready to level up your tech knowledge with the Daily Hacker!

The discussion about the Framework Laptop 16 DIY Edition on Hacker News includes various opinions and experiences shared by users.

One user, dpd, shares their positive experience with the Framework Laptop, mentioning that they find it comparable to the XPS and MacBook in terms of build quality. They also note that it runs Linux well and has improved their station. However, they mention some issues with battery life and the charging cable.

Other users, such as lphblndd and spffytch, share tips for optimizing battery life on the Framework Laptop by using tools like PowerTOP and following the Framework knowledge base.

Some users express concerns about the build quality of the Framework Laptop, with MichaelZuo mentioning that the overall build quality feels flimsy and the thin board is prone to flexing. Another user, 4gotunameagain, mentions issues with the charging cable frequently falling out.

The discussion also touches on comparisons between the Framework Laptop and other laptops like the XPS, MacBook Pro, and Razor laptops. Some users mention that the XPS and MacBook offer better build quality, while others find the Framework Laptop to be a comparable alternative.

lucb1e expresses their satisfaction with the Framework Laptop, highlighting the benefits of the DIY version for customization and assembly.

chrsmrgn shares their thoughts on potential improvements for the Framework Laptop, including features like different keyboard layouts, stylus support, and additional buttons. They also mention interest in a dedicated graphics card.

prphyr mentions their concerns about the Framework Laptop's thickness and battery life compared to other high-quality laptops like the ThinkPad X1 Carbon.

lucb1e experiences slow loading times on the Framework Laptop website and mentions that they ordered the DIY version for the fun of assembly.

The discussion then shifts to a conversation between lucb1e and dylan604 about the performance and practicality of the Framework Laptop. They discuss server-related issues and the delays in page loading.

Overall, the discussion includes a mix of positive experiences, concerns about build quality and battery life, comparisons to other laptops, and suggestions for improvements.

### Therac-25

#### [Submission URL](https://en.wikipedia.org/wiki/Therac-25) | 31 points | by [wazbug](https://news.ycombinator.com/user?id=wazbug) | [14 comments](https://news.ycombinator.com/item?id=37480795)

title: Therac-25: A Computer-Controlled Radiation Therapy Machine Involved in Accidents

summary: The Therac-25, a computer-controlled radiation therapy machine produced by Atomic Energy of Canada Limited (AECL), was involved in several accidents between 1985 and 1987, leading to massive overdoses of radiation and resulting in death or serious injury for some patients. These accidents shed light on the dangers of software control in safety-critical systems and have become a prominent case study in health informatics, software engineering, and computer ethics. The incidents were attributed to concurrent programming errors and the engineers' overconfidence in their initial work, as well as a lack of proper due diligence in resolving reported software bugs.

details: The Therac-25 was preceded by the Therac-6 and Therac-20 units, which were produced in collaboration with Compagnie générale de radiologie (CGR) of France. The Therac-25, manufactured in 1982, was a more compact, versatile, and easy-to-use machine that could administer electron and X-ray treatments. It was controlled by a computer, and some safety mechanisms were shifted from hardware to software.

The accidents were caused by concurrent programming errors, also known as race conditions, that resulted in the machine delivering radiation doses that were hundreds of times greater than normal. Patients exposed to such high doses suffered severe harm, including death. These accidents raised awareness about the risks associated with software control in critical healthcare systems.

One of the key factors contributing to the accidents was the engineers' overconfidence in their work and their failure to address reported software bugs. The incidents also highlighted the crucial importance of considering user feedback and reactions when designing and implementing medical technology.

The Therac-25 case has since become a well-known example in various fields. It serves as a cautionary tale in health informatics, software engineering, and computer ethics, reminding professionals of the potential repercussions of inadequate software design, testing, and validation in safety-critical systems. The accidents led to increased scrutiny of software development practices in the healthcare industry and influenced improved safety standards for medical devices.

Following the accidents, AECL dissolved its medical section in 1988, and Theratronics International Ltd took over the maintenance of installed Therac-25 machines. The incidents and subsequent investigations prompted significant changes in the manufacturing and regulatory landscape of radiation therapy machines, ensuring enhanced safety measures to prevent similar accidents from occurring in the future.

In conclusion, the Therac-25 accidents exposed the dangers of software control in safety-critical systems and continue to serve as a valuable case study in various disciplines. The incidents emphasized the need for rigorous testing, user feedback consideration, and continuous improvement in the design and development of medical technology to prioritize patient safety.

The discussion on the submission about the Therac-25 accidents covers several topics. 

One user points out that it is important to note that people were killed in these accidents, emphasizing the severity of the consequences.

Another user adds that in 1986, a programmer left AECL, and it was difficult to believe that records of the incident were lost. They also mention that it seems a settlement was reached without requiring the programmer to disclose qualifications or experience.

A user shares a link to an article discussing causality in the Therac-25 incidents.

The discussion then veers off to a different topic, with one user mentioning that software flaws can result in human deaths. They provide an example of a mass failure caused by a floating-point arithmetic error that resulted in the deaths of 28 people.

Another user brings up the recent high-profile case of the Boeing 737 MAX crashes caused by software failures, suggesting that software-induced plane crashes occur at an extremely high rate.

A response to this mentions that the software failures were not the only cause of the Boeing 737 MAX crashes, but rather a combination of factors including regulatory oversight and modifications made without proper review.

Another user suggests that studying software ethics is necessary.

One user points out that the components prior to the Therac-25 were not properly tested, and they argue that regulation for medical equipment should ensure proper testing to prevent such incidents.

A response to this brings up the issue of user interface changes in medical equipment and the potential dangers of confusing operators. They mention that regulating user interface changes in medical equipment is justified because unexpected changes can lead to confusion and potentially harm patients.

The discussion ends with a user mentioning the Ariane 5 crash as an example of a failure caused by inadequate testing of components prior to the system's launch.

