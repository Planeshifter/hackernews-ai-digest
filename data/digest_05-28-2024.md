## AI Submissions for Tue May 28 2024 {{ 'date': '2024-05-28T17:15:09.093Z' }}

### Reproducing GPT-2 in llm.c

#### [Submission URL](https://github.com/karpathy/llm.c/discussions/481) | 565 points | by [tosh](https://news.ycombinator.com/user?id=tosh) | [106 comments](https://news.ycombinator.com/item?id=40502090)

Andrej Karpathy started a conversation regarding reproducing the GPT-2 (124M) model in llm.c, a compact 4,000 lines of C/CUDA code. The project aims to make training this model accessible, even for those with limited GPU resources. By utilizing llm.c's efficiency, reproducing the 124M Transformer model on a single 8X A100 80GB SXM node takes approximately 90 minutes, costing around $20.

The project demonstrates superior performance compared to the original GPT-2 checkpoint on the FineWeb validation dataset. While acknowledging the challenges of comparing datasets, the HellaSwag accuracy metric provides a fair assessment of the model's performance. Notably, the model achieves a HellaSwag accuracy of 29.9, surpassing the GPT-2 (124M) and nearing the GPT-3 Small model's performance.

For those interested in replicating these results, a detailed guide is provided, requiring a GPU and specific software setup. Developers can embark on reproducing this work by following the outlined process using tools like miniconda, PyTorch, and llm.c. By offering a straightforward path to achieve similar results, Karpathy's project aims to democratize access to advanced language model training.

The discussion on Hacker News regarding the submission about reproducing GPT-2 (124M) in llm.c covers various topics. 

- Users express appreciation for the effort put into the project and inquire about potential future developments such as creating a video series and exploring other versions of llm.c. There is particular interest in the methodologies employed and the potential for expanding the project.

- Questions are raised about the performance comparison between GPT-2 and llm.c in terms of baseline performance, computational resources, and training time. Discussions delve into the technical details of the implementation, including comparisons with PyTorch and JAX, as well as considerations for model scalability and resource optimization.

- Users point out changes in URLs for further reference and share insights on training models like GPT-3 and NanoGPT, emphasizing the differences in resources required and performance enhancements achieved. Discussions also touch on the complexities of model training, hardware support, and the democratization of advanced language model training.

- Participants engage in conversations about the challenges of reproducibility, advancements in training methodologies, and potential strategies for enhancing model performance. The importance of minimal dependencies, efficient resource utilization, and model interpretability are highlighted as key factors in advancing the field of language model training.

### Tinygrad 0.9.0

#### [Submission URL](https://github.com/tinygrad/tinygrad/releases/tag/v0.9.0) | 221 points | by [wozeparrot](https://news.ycombinator.com/user?id=wozeparrot) | [44 comments](https://news.ycombinator.com/item?id=40504212)

The latest release of TinyGrad (v0.9.0) has brought in exciting new features and improvements. The release highlights include new documentation, experimental backends for AMD and NV, Nvidia tensor core support, improved random number generation, and more stabilized multi-tensor API. Additionally, core TinyGrad has been refactored into 4 pieces, with progress towards greater kernel fusion in the scheduler. New load operations allow fusing optimizer updates with grad, and scheduling kernels in BFS order has improved speed. The release also includes MLPerf ResNet and BERT support, a W.I.P. UNet3D, Llama 3 support, and NF4 quantization in Llama examples. The update also addresses known issues and invites users to join the Discord community for further discussion. Overall, TinyGrad v0.9.0 promises a more usable and efficient experience for users.

The discussion on the latest release of TinyGrad (v0.9.0) on Hacker News covered a range of topics. Users discussed experimental backends for AMD, including compatibility with ROCm and the utilization of AMD Instinct hardware. There were contrasting opinions regarding the performance comparison between TinyGrad and PyTorch on AMD hardware, particularly in training GPT-2. Some users also highlighted issues with generic kernel drivers on AMD software and the complexity of writing job scheduling and memory management code. The thread also included comparisons between PyTorch and TinyGrad in terms of codebase size, with users emphasizing the different philosophies behind the two frameworks. Additionally, there were discussions on the limitations of line count as a metric for code complexity and the challenges in maintaining software within set size constraints.

### Llama 3-V: Matching GPT4-V with a 100x smaller model and 500 dollars

#### [Submission URL](https://aksh-garg.medium.com/llama-3v-building-an-open-source-gpt-4v-competitor-in-under-500-7dd8f1f6c9ee) | 443 points | by [minimaxir](https://news.ycombinator.com/user?id=minimaxir) | [73 comments](https://news.ycombinator.com/item?id=40505099)

In a recent article on Hacker News, Aksh Garg introduced Llama3-V as a revolutionary multimodal model that aims to surpass the performance of GPT4 while being 100 times smaller in size and trained under $500. Llama3-V builds on the success of Llama3 by incorporating visual information into the model architecture, showcasing a 10-20% boost over the current state-of-the-art multimodal model, Llava.

The core of Llama3-V's innovation lies in its utilization of the SigLIP model to embed input images as patches and align them with textual tokens through a projection block with self-attention mechanisms. By combining visual and textual information effectively, Llama3-V demonstrates promising results in various benchmarks, competing closely with models much larger in scale.

To optimize training efficiency, the team implemented caching mechanisms and utilized MPS/MLX optimizations to maximize GPU utilization and accelerate inference. By precomputing image embeddings with SigLIP and employing image-splitting techniques for higher resolutions, Llama3-V streamlines the training process while maintaining performance standards.

This breakthrough in multimodal model architecture not only showcases the power of efficient design and training but also hints at the future potential for cost-effective and high-performance AI models. With Llama3-V's release, the landscape of multimodal understanding in AI is set to evolve, offering exciting possibilities for diverse applications and advancements in the field.

The discussion on the submission mainly focused on comparisons between existing models and the potential implications of the new Llama3-V multimodal model introduced by Aksh Garg. Some users highlighted the performance of different models like CogVLM and GPT4, while others discussed the practical applications and challenges of implementing such models. There were also comparisons between OCR technologies like Tesseract and PaddleOCR, as well as discussions about visual captchas, AI development costs, and the accessibility of AI APIs like Phi Vision from Nvidia. Some users expressed skepticism about cost estimates for AI development and the marketing hype surrounding advanced AI models. Overall, the comments reflected a mix of technical insights, practical considerations, and critical perspectives on the current AI landscape.

### Transformers Can Do Arithmetic with the Right Embeddings

#### [Submission URL](https://arxiv.org/abs/2405.17399) | 203 points | by [byt3h3ad](https://news.ycombinator.com/user?id=byt3h3ad) | [203 comments](https://news.ycombinator.com/item?id=40497379)

A team of researchers led by Sean McLeish has made a breakthrough in enhancing transformers' ability to perform arithmetic tasks. By introducing embeddings that encode the position of each digit relative to the number's start, the researchers were able to significantly improve the model's performance. This innovation not only boosted accuracy in arithmetic but also extended the transformer's capabilities to excel in tasks like sorting and multiplication. The study demonstrates that with the right enhancements, transformers can tackle complex arithmetic problems with up to 99% accuracy on 100-digit addition challenges. The findings open up possibilities for further exploration into the logical extrapolation abilities of transformers, paving the way for advancements in machine learning and artificial intelligence research.

The discussion on the submission "Transformers Can Do Arithmetic with the Right Embeddings" explored various aspects of the research. Some users found the approach of introducing embeddings for position encoding to enhance arithmetic capabilities in transformers to be significant, allowing for improved performance in arithmetic tasks and extending to tasks like sorting and multiplication. Others raised points regarding the practicality and implementation of such enhancements, discussing topics like numeral sense association with brain regions, differences between engineers and scientists, and the limitations and deterministic nature of AI, particularly in language models.

There were also debates on the necessity of specialized tokenization versus standard tokenization, concerns about the complexity of implementing arithmetic abilities in AI, and insights into the challenges in understanding and replicating human intelligence through computational models. The discussion delved into technical details such as the impact of tokenization granularity, development of custom tokenizers, and the allocation of computational resources in training AI models.

Overall, the commenters presented a mix of perspectives on the potential implications and challenges of enhancing transformers for arithmetic tasks, highlighting the complexities and opportunities in AI research and development.

### Ex-OpenAI board member reveals what led to Sam Altman's brief ousting

#### [Submission URL](https://www.businessinsider.com/openai-board-member-details-sam-altman-lied-allegation-ousted-2024-5) | 687 points | by [blackmanta](https://news.ycombinator.com/user?id=blackmanta) | [570 comments](https://news.ycombinator.com/item?id=40506582)

An ex-OpenAI board member, Helen Toner, has revealed shocking details about the events that led to Sam Altman's brief ousting as CEO. Toner alleged that Altman had lied to the board multiple times, withheld information, and misrepresented key details about OpenAI. She cited examples where Altman failed to disclose the release of ChatGPT and his ownership of the startup fund, leading to a breakdown in trust among board members. This revelation sheds light on the internal turmoil at OpenAI and the challenges faced in maintaining transparency and accountability within the organization.

The discussion on Hacker News about the submission regarding the revealing details about Sam Altman and OpenAI's internal issues touched upon various aspects. Here are the key points:

- Some users discussed the balance of power and relationships within the board and the CEO's role at OpenAI.
- There was a conversation about the dynamics of power and reliance within organizations, using references such as Game of Thrones and the concept of small-dollar coins versus bills.
- Some users pointed out the implications of OpenAI's reliance on capital and cloud credits from companies like Microsoft Azure and Amazon.
- Others debated the responsibilities of the board in overseeing the CEO's actions and the consequences of legal actions or lack of formal complaints handled by the board.
- Users shared differing opinions on the role and influence of board directors, the extent of control by founders like Sam, and possible power struggles within the organization.
- There were discussions on the necessity of formal complaints and scrutiny in handling governance issues, the boundaries of transparency, and the implications of certain power structures in corporate governance.
- Users also touched upon the idea of maintaining mission alignment and ethical conduct in non-profit organizations, as well as the challenges posed by conflicts of interest in decision-making processes.

Overall, the discussion highlighted the complexities of power dynamics, governance issues, and transparency within organizations like OpenAI, sparking debates on accountability, corporate structure, and ethical considerations.

### Doing is normally distributed, learning is log-normal

#### [Submission URL](https://hiandrewquinn.github.io/til-site/posts/doing-is-normally-distributed-learning-is-log-normal/) | 240 points | by [hiAndrewQuinn](https://news.ycombinator.com/user?id=hiAndrewQuinn) | [71 comments](https://news.ycombinator.com/item?id=40497623)

The latest essay on gwern.net delves into the topic of "leaky pipelines" and log-normal distributions, shedding light on the challenges of software estimation and project management. The concept explores the idea that not all steps in a process follow a normal distribution, making it tricky to predict outcomes accurately.

In the realm of software development, the essay highlights the importance of just-in-time learning and the unpredictability of technical hurdles. It argues that the emphasis on relevant experience and specific tooling in job applications is justified, as the process of transitioning to new technologies often involves a significant learning curve.

Furthermore, the theory suggests that processes dominated by learning phases are more common than those that follow a normally distributed pattern. Mastering a new skill involves navigating through the leaky pipeline of uncertainty until it becomes routine and predictable.

The essay proposes that academic learning, with its structured curriculum and clear learning objectives, may offer a more controlled environment compared to the unpredictable nature of real-world projects. However, unexpected variations in learning times can still occur, highlighting the dynamic and non-linear nature of acquiring new skills.

Overall, the essay presents a thought-provoking perspective on the challenges of estimating project timelines in software development and the inherent uncertainties of the learning process in different domains.

The discussion on the Hacker News submission revolves around various interesting points:

1. **Mathematical Analysis**: Users engaged in a mathematical analysis of the probabilities mentioned in the essay, highlighting potential errors in calculations and the importance of accurate mathematical reasoning in software estimation.

2. **Software Development Practices**: The conversation shifted towards discussing software estimation methodologies, contrasting traditional project management approaches like Waterfall and Gantt charts with the concept of just-in-time learning and non-normally distributed processes in software projects. The debate included perspectives on the challenges of estimating project timelines accurately in the dynamic environment of software development.

3. **Project Types Comparison**: Users compared construction projects to software development projects, emphasizing the differences in predictability, constraints, and solutions between the two domains. The discussion touched upon the complexities of managing uncertainty and adapting to changing requirements in software projects.

4. **Critique of Waterfall Approach**: A critical view of the Waterfall approach in project management was presented, noting its limitations in handling uncertainties and evolving project requirements effectively. The conversation highlighted the importance of flexible methodologies in software development to address changing needs and technical challenges.

5. **Industrial Engineering Perspective**: A user with an Industrial Engineering background shared insights on variance analysis and process design, drawing parallels between software estimation challenges and the fundamentals of engineering processes.

Overall, the discussion provided a multidimensional exploration of software estimation, project management methodologies, and the complexities of handling uncertainties in software development projects. Users offered diverse perspectives on the topic, reflecting on practical experiences and theoretical insights from different fields.
