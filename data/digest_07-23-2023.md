## AI Submissions for Sun Jul 23 2023 {{ 'date': '2023-07-23T17:09:46.007Z' }}

### Interfaces all the way down

#### [Submission URL](https://jjain.substack.com/p/interfaces-all-the-way-down) | 91 points | by [jinay](https://news.ycombinator.com/user?id=jinay) | [63 comments](https://news.ycombinator.com/item?id=36836433)

In his latest article on his Substack, Jinay Jain discusses the importance of designing interfaces and how it leads to happier developers. According to Jain, mastering interface design is the key to success and advancement in the ranks of engineering. As engineers move up, they become responsible for larger interfaces, such as entire classes, APIs between services, and even widely distributed SDKs. High-quality interfaces not only make people appreciate your work but also trust you with bigger tasks. Jain explores the concept of problem decomposition, where breaking down complex tasks into smaller, modular components is crucial. Poorly constructed interfaces can lead to dependencies and technical debt, slowing down developer velocity. To ensure robustness, Jain suggests asking questions about the impact on other parts of the system, unit testing, and maintaining codebase independence. Drawing inspiration from traditional design fields, such as human-centered design, Jain emphasizes the importance of considering developers as end users when designing interfaces. By following design principles like discoverability, affordances, and feedback, interfaces can be intuitive and require no documentation. Jain concludes by highlighting how extensive care in crafting interfaces can make the software "write itself" once the basic structure is outlined.

The discussion on this submission revolves around various aspects of interface design and its importance in software development. Some comments highlight the similarities between function arguments and design parameters, while others discuss the relevance of learning complex tools and techniques in software engineering.

Other comments mention the need for effective communication and management in interface design to avoid mistakes and improve productivity. The discussion also touches on the importance of well-designed interfaces in extracting functionality, ensuring code quality, and balancing front-end design with velocity and testing principles.

One comment raises the point that mathematical interfaces can be more intuitive and friendly, especially for those familiar with mathematical concepts and techniques. However, another comment argues that not all mathematical interfaces are user-friendly or suitable for practical applications.

Some comments also discuss the relevance of other programming languages, such as SQL, in the context of interface design. They mention how SQL's built-in primitives and theoretical properties contribute to its effectiveness in handling relational databases.

Overall, the discussion explores various perspectives on interface design, ranging from the mathematical aspects to practical considerations and the use of different programming languages.

### Retentive Network: A Successor to Transformer for Large Language Models

#### [Submission URL](https://arxiv.org/abs/2307.08621) | 101 points | by [sangel](https://news.ycombinator.com/user?id=sangel) | [16 comments](https://news.ycombinator.com/item?id=36831956)

Researchers from various institutions have proposed a new architecture called the Retentive Network (RetNet) as a successor to the popular Transformer model for large language models. This architecture aims to achieve training parallelism, low-cost inference, and good performance simultaneously. The key innovation in RetNet is the retention mechanism for sequence modeling, which supports three computation paradigms: parallel, recurrent, and chunkwise recurrent. These paradigms allow for efficient training, low-cost inference, and linear complexity in modeling long sequences. Experimental results on language modeling demonstrate that RetNet achieves favorable scaling results, parallel training, low-cost deployment, and efficient inference. The proposed architecture shows promise as a strong successor to Transformer for large language models. Code for RetNet will be made available for further research and development.

The discussion around the submission revolves around the proposed Retentive Network (RetNet) architecture as a successor to the Transformer model. 

One commenter points out that the key difference between RetNet and existing Transformer-based models is the retention mechanism for sequence modeling. This mechanism allows for parallel, recurrent, and chunkwise recurrent computation paradigms, enabling efficient training, low-cost inference, and linear complexity in modeling long sequences.

Another commenter raises skepticism about RetNet as a successor to the Transformer model and mentions two related papers: MetaFormer and Hopfield Networks. They suggest that RetNet may not offer significant improvements compared to these alternatives.

There is a discussion about the scalability of RetNet, with one commenter noting that it seems to perform favorably compared to large language models like H3 and RWKV.

One commenter expresses interest in the paper and the mathematical structure it offers for attention recurrence, anticipating further research and demonstrations of its effectiveness.

A mention is made of a tweet discussing the phase shift phenomenon in scale transformers, but it is not clear if there is any connection to RetNet.

A commenter questions the affiliation of the researchers, questioning the connection between MIT and Tsinghua University.

Overall, the discussion covers various opinions on the potential of RetNet as a successor to the Transformer model and raises questions about its differences from existing alternatives.

### Stable Diffusion and ControlNet: “Hidden” Text (see thumbnail vs. full image)

#### [Submission URL](https://old.reddit.com/r/StableDiffusion/comments/1561k15/free_tool_to_generate_hidden_text_using_stable/) | 98 points | by [b0ner_t0ner](https://news.ycombinator.com/user?id=b0ner_t0ner) | [36 comments](https://news.ycombinator.com/item?id=36832271)

Title: Free Tool to Generate "Hidden" Text Using Stable Diffusion and ControlNet

BoostPixels, a Reddit user, has created a fascinating tool that uses Stable Diffusion and ControlNet to generate hidden text in images. The tool works in a unique way – when the image is small, the text stands out clearly. However, as the image enlarges, decoding the text becomes a challenging task. This curious phenomena has captured the attention of many users who have tried to decipher the hidden messages. Some strategies, such as squinting, blinking rapidly, or scrolling the image, seem to help make the text more visible. While it's an interesting experiment, /u/BoostPixels hopes that this tool won't be used for evil purposes.

The discussion on the submission starts with GaggiX acknowledging the use of the ControlNet model by the OP, BoostPixels. They share a link to an installation guide for the ControlNet model and mention that previous methods of generating QR codes didn't produce high-quality messages. There is then a conversation about AI generating QR codes and patterns similar to natural ones, with LanternLight83 wondering if complex text content can be sorted by the complexity of the ControlNet Local Linear Models (LLMs).
 
ben_w points out that generating text visually is difficult for short words with no distinctive landmarks, but GaggiX argues that ASCII art limits the expressiveness of sentences. cpblwb offers a poetic interpretation of the hidden text. gdlsk joins the discussion, expressing praise for the comments on Reddit and acknowledging the community's collective understanding of the tool's implementation.

The conversation then delves into the different user experiences when viewing the hidden text. Some discuss how the text becomes more visible when zoomed or viewed on a mobile phone, while others mention difficulties in spotting the text unless they hold their phone at a specific angle. There is also discussion about the visibility of the text on different screens, with some users mentioning that enlarging the image on a high-resolution monitor makes the text disappear while others find it readable. 

The link in the submission is mentioned by ch, who confirms that the stable diffusion method works on a MacBook Pro M1 Max. tmm shares a similar illusion they made in 2014 that involves thumbnail squinting. strng comments on the exploitability of resizing algorithms, mentioning that Photoshop filters can produce misleading results. tlstptml and hpfnsprgrj offer explanations related to human contrast sensitivity and letter squinting, respectively. 

The discussion then transitions to the nature of incorrect algorithms in resizing and how they affect the visibility of objects. Some users mention experiences with resizing algorithms and high-resolution monitors, while others question the impact of content contrast and spatial patterns. pngr argues that different spatial patterns will have a well-defined and substantially compressible space. 

lcbrtry finds the effect fascinating and shares personal experience with squinting to see hidden text. mnstmnsmn comments on the inability to see the hidden text on their iPhone 14 Pro Max. smblt issues a PSA about blurred vision affecting the effectiveness of the effect, and ChatGTP mentions an off-topic issue with a broken back button on the page. wmtt tries to provide a solution for viewing the hidden text on an iPhone, while munch117 suggests that the link to the desktop version may work better for iPhone users.

### Apple is already using its chatbot for internal work

#### [Submission URL](https://www.theverge.com/2023/7/23/23804825/apple-gpt-chatbot-apple-care-siri-chatgpt) | 28 points | by [rntn](https://news.ycombinator.com/user?id=rntn) | [10 comments](https://news.ycombinator.com/item?id=36838178)

Apple is using its chatbot internally to prototype features, summarize text, and answer employee questions based on trained data, according to Bloomberg's Mark Gurman. While Apple has yet to determine how it will use the chatbot for customer-facing purposes, it may consider using it to support AppleCare. However, Apple is being cautious about implementing AI due to the potential for misinformation and leaked information. The company is expected to make a significant AI-related announcement next year. While other companies like Meta and Samsung make moves in the AI space, Apple has been more reserved, but its hiring of former Google AI head John Giannandrea in 2018 suggests its serious about exploring generative AI.

The discussion on the article revolves around Apple's use of chatbots internally and their potential applications for customer-facing purposes. One commenter points out that workplace technology often includes extensive chatbots for group support, product suggestions, and notifications. Another user expresses frustration with Siri's lack of helpfulness on iOS. Someone agrees, stating that Siri's performance is embarrassingly bad.

The conversation then shifts to the challenges Apple may face in implementing AI due to its high-profile nature and the potential for misinformation and leaks. One user praises the integration of shortcuts in applications, claiming it makes finding and utilizing applications more intuitive. However, another user criticizes Apple's attempt to mimic programming visual environments, suggesting it is not good practice and promotes the use of variables that are not named descriptively.

Regarding Apple's upcoming AI-related announcement, someone questions what it might entail. They doubt Apple will call it "AI" and speculate that it may be related to an upgrade for Siri or machine learning models with improved language processing. Another user finds this comment interesting, as they have noticed various mentions of AI from the company, including work expressions in video and song terms for training people.

The discussion wraps up with a couple of comments highlighting Apple's focus on device management, preferences, privacy, and low latency reasons for utilizing Siri. Overall, the conversation touches upon the limitations and potential of Apple's AI endeavors.

