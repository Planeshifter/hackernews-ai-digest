## AI Submissions for Sat Nov 01 2025 {{ 'date': '2025-11-01T17:13:37.961Z' }}

### Claude Code can debug low-level cryptography

#### [Submission URL](https://words.filippo.io/claude-debugging/) | 394 points | by [Bogdanp](https://news.ycombinator.com/user?id=Bogdanp) | [184 comments](https://news.ycombinator.com/item?id=45784179)

A Go engineer livecoded a fresh implementation of ML-DSA (a NIST-selected post-quantum signature scheme) for the Go standard library, only to find verify always rejected valid signatures. Exhausted, they pointed Claude Code (v2.0.28, Opus 4.1, “ultrathink”) at the repo and tests. It immediately pinpointed a subtle bug: the verify path was effectively taking the high bits of w1 twice. The author had merged HighBits and w1Encode for signing, then reused it in verify where UseHint already supplies high bits. Claude wrote a confirming test and a working (but mediocre) patch; the author refactored properly to pass high bits explicitly and even improved performance by avoiding an extra Montgomery round-trip.

They ran a second experiment on earlier failing signing code:
- Bug 1: Miscomputed Montgomery constants for 1 and −1 caused the Fiat–Shamir-with-aborts loop to never terminate. Claude replicated the printf sleuthing and fixed it faster than the author.
- Bug 2: A signature field was encoded as 32 bits instead of 32 bytes. Claude took longer, wandered a bit, then found it in a fresh session. Its suggested fix tweaked length but not capacity; the author refined it.

Takeaways:
- AI excels at well-scoped, testable failures in low-level code and can rapidly localize non-obvious defects.
- It may stop after the first fix; fresh sessions help isolate independent bugs.
- Great for hypotheses and scaffolding; humans still polish APIs and correctness.
- Disclosure: author had complimentary Claude Max access from Anthropic; no request to publish.

**Summary of Discussion:**

The discussion explores mixed reactions to using AI (specifically Claude Code) for debugging complex, low-level crypto code, highlighting both potential and limitations:

1. **AI Debugging Effectiveness**  
   - Users note AI excels at *localizing narrow, testable bugs* (e.g., bitwise errors, encoding oversights) but struggles with broader system issues (concurrency, architectural flaws). Restarting AI sessions helps isolate independent bugs.
   - Skepticism exists about relying on AI for *deeper reasoning*: comments suggest AI may generate "plausible but wrong" fixes, requiring human refinement for correctness and maintainability.

2. **Comparison to Junior Engineers**  
   - Some analogize AI tools to *junior developers*: they accelerate grunt work (tedious bug-hunting, scaffolding) but lack critical judgment. A user warns this risks creating "busywork" reviews for seniors if unchecked.

3. **Limitations & Over-Reliance**  
   - AI’s utility depends on *precise prompting* and domain context (e.g., referencing cryptographic primitives like Montgomery constants). Vague prompts yield unhelpful outputs.  
   - Concerns arise about *eroding foundational skills*: one thread debates whether AI reliance parallels overusing calculators in math education, potentially stunting low-level understanding.

4. **Practical Workflow Integration**  
   - Users report success integrating AI for *hypothesis generation* (e.g., "printf-style sleuthing") but stress the need for rigorous testing. AI-authored code often requires refactoring for performance/readability.  
   - Mixed experiences: some find AI reduces tedium; others call it "time-wasting" when debugging ill-defined issues.

5. **Skepticism vs. Optimism**  
   - Critics dismiss hype, calling overly positive claims "forbidden phrases [from] experts." Others cautiously endorse AI as a *complementary tool*, not a replacement for expertise.  
   - A recurring theme: AI’s value hinges on the user’s ability to *guide and validate* its output, balancing speed with meticulous review.  

**Key Takeaway**  
While AI like Claude Code accelerates debugging in niche scenarios (cryptographic bit-twiddling, testable edge cases), its effectiveness is bounded by the user’s expertise to frame problems and vet solutions. The consensus leans toward cautious adoption—valuable for specific, narrow tasks but insufficient for holistic engineering.

### Show HN: Why write code if the LLM can just do the thing? (web app experiment)

#### [Submission URL](https://github.com/samrolken/nokode) | 386 points | by [samrolken](https://news.ycombinator.com/user?id=samrolken) | [277 comments](https://news.ycombinator.com/item?id=45783640)

What it is
- An experiment to skip code generation entirely: every HTTP request is handled by an LLM that decides what to do.
- Tools exposed to the model: database (SQLite queries; model designs schema), webResponse (returns HTML/JSON), updateMemory (persists user feedback to markdown for the next request).

How it works
- The server sends “Handle this HTTP request: {METHOD} {PATH}” and the model chooses SQL, HTML/JS, or JSON to produce.
- It infers UI vs API from the path (/contacts → HTML page; /api/contacts → JSON).
- Each page includes a feedback box; user notes like “make buttons bigger” or “use dark theme” get implemented in subsequent responses.

Results
- It works: forms submit, data persists, APIs return valid JSON, reasonable schemas and parameterized SQL, REST-ish routes, responsive layouts, validation, error handling—without hardcoded app logic.
- Pain points are all performance/ops:
  - Latency: 30–60s per request (≈300–6000x slower than a normal app).
  - Cost: ~$0.01–$0.05 per request (≈100–1000x more expensive).
  - Consistency: design drifts between requests; short memory despite feedback file.
  - Reliability: occasional hallucinated SQL → 500s; 75–85% of time spent “reasoning.”

Why it matters
- Demonstrates that LLMs can execute application logic today; the blockers are speed, cost, consistency, and reliability—not raw capability.
- Points toward a future where “intent → execution” could displace much of traditional app code if inference becomes fast, cheap, and stateful.

Try it
- Node app using Claude 3 Haiku; configure ANTHROPIC_API_KEY and run npm start. Defaults to a contact manager; try routes like /game, /dashboard, /api/stats.
- Budget for per-request inference costs.

The discussion explores the debate around deterministic versus non-deterministic behavior in AI systems, using self-driving cars and LLM-driven applications as key examples:

1. **Determinism Tradeoffs**  
   - Proponents argue deterministic systems (like traditional code) offer reliability and predictability, while critics note LLMs' non-deterministic nature better handles complex/unpredictable real-world scenarios.  
   - Comparisons drawn to human behavior: "Average humans solve complex problems through non-deterministic trial-and-error" vs machines requiring explicit instructions.

2. **Real-World Challenges**  
   - Self-driving car analogy: Even deterministic physics simulations can't prevent accidents if intent interpretation fails. LLMs struggle with reliably translating natural language instructions into consistent actions.  
   - UX concerns: Non-determinism causes frustration when systems like Google Assistant unpredictably fail basic queries, despite potential benefits for complex tasks.

3. **Technical Limitations**  
   - Current LLMs spend 75-85% of time "reasoning," leading to latency and hallucinations (e.g., malformed SQL queries crashing servers).  
   - Core tension: Language inherently ambiguous vs machines needing precise instructions. One user notes "The primary difficulty is articulating intent, not implementing it."

4. **Philosophical Divergence**  
   - Transportation metaphors highlight societal preferences: Some value schedule reliability (deterministic), others prioritize flexibility.  
   - Deeper existential questions emerge about whether AI systems replicating human-like chemical reward mechanisms (dopamine triggers) constitute meaningful progress.

The discussion ultimately frames non-determinism as both a technical hurdle and philosophical frontier, with participants divided on whether unpredictability represents a bug (to be minimized) or feature (enabling adaptability).

### Word2vec-style vector arithmetic on docs embeddings

#### [Submission URL](https://technicalwriting.dev/embeddings/arithmetic/index.html) | 70 points | by [kaycebasques](https://news.ycombinator.com/user?id=kaycebasques) | [13 comments](https://news.ycombinator.com/item?id=45784455)

- The author tests whether the classic “king − man + woman ≈ queen” trick works on modern document-level embeddings. They embed full docs and single words with EmbeddingGemma, then do vector math like: vector(“Testing Your Database” from Supabase) − vector(“supabase”) + vector(“angular”).
- Verification is indirect: they compare the resulting vectors against embeddings of a small set of short docs (Angular, Supabase, Playwright, Skylib, CockroachDB) using cosine similarity.
- Key finding: task type settings matter a lot. With custom task types, the “same topic, different domain” experiment lands on Angular testing docs as hoped:
  - “Testing” (Angular): 0.751
  - “Testing Services” (Angular): 0.629
  With default task types, the result stays closest to the original Supabase doc (0.659) instead of transferring to Angular.
- The author also runs a “different topic, same domain” variant (subtract “testing”, add “vectors”) aiming to hit Supabase’s “Vector Columns,” and notes prior research that task types noticeably affect outcomes.
- Caveats: 2048-token limit (no chunking), small candidate set, mostly testing-related docs, and cosine similarity as a proxy for “semantic correctness.”
- Takeaway: word2vec-style vector offsets can work at document scale with modern models, but only reliably when the embedding task type is tuned; defaults may cling to the source document’s domain. Source code is provided.

The discussion explores the implications and challenges of applying word2vec-style vector arithmetic to modern document embeddings, with several key themes:

1. **Model Comparisons & Technical Insights**  
   - Users note that doc2vec (Paragraph Vectors) produces coarser document similarities compared to transformer-based embeddings, heavily dependent on training data and application. The 2015 Paragraph Vector paper is cited as foundational but limited.  
   - Tests of the "king − man + woman ≈ queen" analogy across models reveal stark differences: OpenAI’s `text-embedding-3-large` scored poorly (0.54), while EmbeddingGemma (0.84) and Qwen3-Embedding4B (0.71) performed better. Skepticism arises around OpenAI’s SOTA claims.  

2. **Evaluation Critiques**  
   - Debates emerge about metrics: Absolute L2 distances and cosine similarity may be misleading in high-dimensional spaces. Relative differences and normalization (e.g., spherical embeddings) are suggested as more meaningful for analogy tasks.  

3. **Practical Applications**  
   - Ideas for technical writing workflows include using embeddings to surface relevant documentation, track completeness via spreadsheets, or integrate CLI tools with LLMs (e.g., Claude) for cross-referencing. Structured, deterministic workflows are emphasized over "messy" docs.  

4. **Stylistic Control & Alignment**  
   - Users discuss leveraging vector arithmetic for controllable writing tools (e.g., style transfer via "person vectors" from Anthropic’s research) or aligning model outputs by manipulating embeddings.  

5. **Skepticism & Limitations**  
   - Concerns persist about modern models’ ability to reliably handle analogies compared to classic word2vec, with domain specificity and task-type tuning (as in the submission) deemed critical. A linked 2019 study underscores lingering challenges in vector arithmetic generalization.  

Overall, the discussion highlights cautious optimism about document-level vector operations but stresses the need for nuanced evaluation and domain-specific tuning.

### You can't refuse to be scanned by ICE's facial recognition app, DHS document say

#### [Submission URL](https://www.404media.co/you-cant-refuse-to-be-scanned-by-ices-facial-recognition-app-dhs-document-says/) | 546 points | by [nh43215rgb](https://news.ycombinator.com/user?id=nh43215rgb) | [428 comments](https://news.ycombinator.com/item?id=45780228)

You Can't Refuse ICE’s Face Scan, DHS Doc Says

- What happened: An internal DHS document obtained via FOIA shows ICE’s field agents are using a facial recognition app, “Mobile Fortify,” to verify identity and immigration status—and people aren’t allowed to decline a scan.
- Data retention: Photos captured by the app are stored for 15 years, regardless of immigration or citizenship status. That includes U.S. citizens.
- Scope: The doc sheds light on the tech, data flow, storage, and DHS’s rationale. It follows 404 Media’s earlier reporting that ICE and CBP are scanning faces on the street to check citizenship.
- Why it matters: This pushes facial recognition from airports and ports into everyday encounters, eliminating consent and expanding long-term biometric databases. It raises questions about accuracy, bias, due process, and how such scans fit under Fourth Amendment protections during stops.

What to watch
- Legal challenges over warrantless, nonconsensual biometric collection in public stops
- Whether DHS creates a deletion or redress process for citizens and lawful residents
- Oversight on data sharing across DHS components and other agencies
- Transparency on error rates, audit trails, and how scans trigger further action

Source: 404 Media (FOIA-driven report; free to read)

### Summary of the Discussion:

1. **Criticism of ICE's Facial Recognition Use**:  
   Users express alarm over ICE’s "Mobile Fortify" app, which mandates facial scans without consent, stores data for 15 years, and lacks transparency. Comparisons are drawn to authoritarian surveillance regimes, with concerns about justifying systemic rights violations.

2. **Debate Over Human vs. AI Verification**:  
   - Skeptics argue facial recognition is error-prone and biased, citing real-world failures (e.g., false arrests) and challenges with diverse lighting/shadow conditions. Links to studies highlight racial bias and inaccuracy.  
   - Proponents counter that human processes (e.g., ID checks) are also flawed, pointing to NIST studies showing AI can outperform humans in controlled settings. However, critics stress the lack of transparency in error rates and auditing.

3. **Technical and Privacy Concerns**:  
   - Discussions mention risks of long-term biometric databases, misuse of data across agencies, and the lack of redress for false matches.  
   - Alternative identity methods (DNA, social recovery) are suggested but deemed impractical due to false positives (e.g., identical twins) and privacy trade-offs.

4. **Distrust in Institutions**:  
   Users critique agencies like TSA for relying on ineffective tech (e.g., "bomb detectors"), fueling skepticism about ICE’s deployment of facial recognition. Others warn against conflating AI accuracy with ethical implementation.

5. **Political and Legal Implications**:  
   Concerns about suppression of criticism (e.g., labeling dissenters as “enemies”) and parallels to dystopian policies. Questions arise about Fourth Amendment violations and the need for legal challenges to nonconsensual scans.

6. **Meta-Discussion on HN Culture**:  
   Side debates erupt about HN’s shift toward political content, moderation practices (shadowbanning, flagging), and the reliability of AI-generated summaries vs. primary sources.

**Key Themes**:  
- **Accuracy & Bias**: Uncertainty about AI’s reliability vs. human oversight.  
- **Privacy & Due Process**: Fear of irreversible surveillance infrastructure.  
- **Institutional Trust**: Skepticism toward government tech adoption without accountability.  
- **Ethical Tech Debate**: Tension between innovation and civil liberties.

### New analog chip capable of outperforming top-end GPUs by as much as 1000x

#### [Submission URL](https://www.livescience.com/technology/computing/china-solves-century-old-problem-with-new-analog-chip-that-is-1-000-times-faster-than-high-end-nvidia-gpus) | 62 points | by [mrbluecoat](https://news.ycombinator.com/user?id=mrbluecoat) | [20 comments](https://news.ycombinator.com/item?id=45779181)

HN Summary: China’s analog AI/6G chip claims 1,000× GPU throughput, 100× energy efficiency

TL;DR
- Peking University researchers built an analog “compute‑in‑memory” chip using RRAM that processes data as electrical currents instead of digital 1s/0s.
- In Nature Electronics (Oct 13), they report digital‑like accuracy on hard communications tasks (e.g., MIMO matrix inversion), while using ~100× less energy.
- With additional tuning, they claim up to 1,000× higher throughput than Nvidia’s H100 and AMD’s Vega 20 on those workloads.

Why it matters
- Analog, in‑memory computing slashes the cost of moving data between memory and compute—today’s biggest bottleneck for AI and advanced wireless.
- If the results hold, this could be a big deal for 6G base stations, edge inference, and low‑power linear‑algebra accelerators.

How it works
- Arrays of RRAM cells both store weights and perform operations; computations happen as continuous currents across the array.
- This architecture is well‑suited to matrix‑vector ops central to signal processing (e.g., massive MIMO) and certain AI inference kernels.

Reality check
- “Up to 1,000×” appears limited to specific linear‑algebra problems under controlled precision; it’s not a general GPU replacement.
- Analog chips face hard problems: device variability, noise/drift, temperature sensitivity, RRAM endurance, and ADC/DAC overhead that can erase gains.
- Programmability, software stacks, and scaling to large, dynamic AI workloads remain open questions.
- Vega 20 is an older GPU; H100 is modern—benchmark details matter. Paper‑level prototypes often trail real‑world deployability.

Big picture
- It’s another strong signal that specialized, domain‑specific hardware (especially compute‑in‑memory) may outpace general GPUs on narrow but important tasks, particularly in comms and edge AI. The leap from lab to production—and to broader AI use cases—will be the real test.

**Summary of Discussion:**

The discussion revolves around skepticism and historical context regarding analog AI chips, emphasizing challenges in deployment and scalability despite their potential efficiency gains. Key points include:

1. **Historical Precedents & Challenges:**
   - Analog computing concepts trace back to Frank Rosenblatt's Perceptron (1950s) and Carver Mead's work in the 1980s. Companies like Synaptics and Mythic AI attempted analog chips but faced commercialization hurdles (e.g., Mythic’s bankruptcy in 2022).
   - Early analog neural networks (e.g., Mead’s *Analog VLSI Neural Systems*) showed promise for low-power applications but struggled with scalability and integration into hybrid systems due to DAC/ADC overhead.

2. **Real-World Applications & Limitations:**
   - Niche successes exist, like Microsoft’s 1999 optical mouse (using Avago’s analog sensor for motion correlation), but these are specialized and not scalable to broader AI/compute tasks.
   - Analog’s advantages (e.g., power efficiency) are countered by device variability, noise, temperature sensitivity, and manufacturing challenges, making digital dominance persist.

3. **Technical Skepticism:**
   - The chip’s claimed 1,000× throughput gains are seen as context-specific (e.g., matrix inversion for MIMO systems), not general-purpose. Scalability beyond small matrices (e.g., 16x16) and software ecosystem gaps remain concerns.
   - Comments highlight that analog’s theoretical efficiency often erodes in practice due to ADC/DAC costs, precision trade-offs, and reliability issues.

4. **Niche Potential vs. Hype:**
   - While analog could excel in edge AI or signal processing, users doubt it will replace GPUs for mainstream AI. Some humorously note that benchmarks against older GPUs (Vega 20) and software bottlenecks (e.g., Windows compatibility) undercut claims.

5. **Cultural References & Humor:**
   - Lighthearted remarks compare the chip’s hype to running *Crysis* or retro games on modern systems, mocking overpromises. Others reference music (*Analog Man* by Joe Walsh) and memes (“Fear” as a motivator).

**Conclusion:** The discussion acknowledges analog’s potential in specialized domains but stresses that overcoming technical barriers, ecosystem development, and real-world validation are critical hurdles. Historical lessons and skepticism temper excitement, emphasizing that lab prototypes rarely translate seamlessly to production.

### Czech police forced to turn off facial recognition cameras at the Prague airport

#### [Submission URL](https://edri.org/our-work/czech-police-forced-to-turn-off-facial-recognition-cameras-at-the-prague-airport-thanks-to-the-ai-act/) | 148 points | by [campuscodi](https://news.ycombinator.com/user?id=campuscodi) | [49 comments](https://news.ycombinator.com/item?id=45784185)

Czech police shut down Prague airport facial recognition after AI Act; watchdog finds it illegal

- Prague’s Václav Havel Airport ran a real-time facial recognition system from 2018 until August 2025, matching travelers’ facial “bio-indexes” against watchlists of wanted or missing people.
- After years of pressure from civil society group IuRe, the Czech Data Protection Authority (DPA) confirmed the setup violated personal data laws. The EU AI Act’s biometric rules, in force since February 2025, require judicial approval per use—none was obtained—rendering operations between Feb–Aug 2025 unlawful.
- Despite repeated warnings and media scrutiny, police kept the system running until the August shutdown. IuRe obtained the DPA’s findings via a Freedom of Information request.
- The DPA’s inspection took nearly four years, highlighting enforcement lag and the need for clear, democratically vetted rules for police use of biometric tech.
- Separately, police continue using the Digital Personal Image Information System, a database of about 20 million ID/passport photos for retrospective face matching. Authorities say it helps identify the deceased, but IuRe and the DPA warn it could also be used to identify protesters.
- IuRe urges the new Interior Minister to overhaul laws to align with EU safeguards; the group continues monitoring via its “Czechia is not China” campaign.

Why it matters: The case shows the AI Act starting to bite—but also how slow oversight can be. It spotlights the risk of repurposing massive ID photo databases for surveillance and the need for tight, court-supervised limits on biometric policing in the EU.

**Summary of Discussion:**

The discussion revolves around the ethical and practical implications of facial recognition systems, particularly in light of the Czech airport case and the EU AI Act. Key themes include:

1. **Trust in Institutions:**  
   Users debate distrust in both governments and corporations handling biometric data. Concerns are raised about mass surveillance enabling targeted enforcement, propaganda, and corporate exploitation. Microsoft and other tech firms are critiqued for lobbying weak privacy standards. Some argue that systems built by private entities (not governments) lack accountability, with enforcement of privacy laws often symbolic.

2. **Privacy vs. Security Trade-offs:**  
   Supporters of facial recognition argue it can enhance security if used responsibly (e.g., identifying criminals), while critics highlight risks like mission creep (e.g., tracking protesters). Automation in airport checkpoints is seen as both efficient and privacy-invasive, with debates over whether security justifies pervasive monitoring.

3. **Historical & Regional Context:**  
   Commentators note former Warsaw Pact countries’ histories of state surveillance, while others compare modern systems to the NSA’s XKeyscore. Anecdotes about lax EU airport security (e.g., unchecked IDs in the 80s/90s) contrast with current biometric checks, though skepticism remains about their effectiveness (e.g., a pilot allowing unauthorized boarding).

4. **Regulatory Challenges:**  
   The EU AI Act is praised for requiring judicial oversight but criticized for slow enforcement. Users stress the need for clear, democratically-vetted rules to prevent misuse of databases like the Czech ID photo repository. Meta-discussions question whether HN overemphasizes privacy battles while ignoring corporate data harvesting (e.g., Google, credit cards).

5. **Community Split:**  
   The thread reflects a roughly 50/50 divide: half view facial recognition as a necessary tool against crime, while half see it as a slippery slope toward authoritarianism. Some urge focusing on corporate data control rather than government overreach.

**Key Takeaway:**  
The discussion underscores deep skepticism about biometric surveillance, emphasizing the need for transparency, judicial safeguards, and public accountability. While pragmatic security benefits are acknowledged, fears of abuse by both states and corporations dominate, with calls for stricter EU-wide regulations and ethical tech deployment.

### AI Broke Interviews

#### [Submission URL](https://yusufaytas.com/ai-broke-interviews/) | 77 points | by [yusufaytas](https://news.ycombinator.com/user?id=yusufaytas) | [104 comments](https://news.ycombinator.com/item?id=45785794)

AI Broke Interviews: the old “LeetCode + system design + behavioral” pipeline was flawed but stable because it compressed signal into 45 minutes. Generative AI blew up the core assumption behind it: that the candidate is the one doing the thinking.

What’s new
- Effortless perfection: candidates deliver polished code and scripted behavioral answers with no visible reasoning. Nudge the problem off-script and some collapse.
- Cheating scales: no need for friends or prep; a phone or overlay can stream “perfect” code and talking points. The author cites multiple CVs tied to the same email, verbatim reading of AI answers, and candidates tripping over AI-inserted syntax.
- Panic responses: companies are tightening identity checks, adding proctoring, and shifting back to in‑person. The post claims Google announced a return to in‑person due to AI misuse.

Why it matters
- The legacy interview was bad, but it worked “well enough” because it filtered on preparation under time pressure. AI erased that filtration layer, making it hard to separate genuine reasoning from AI proxying.
- Measuring “can you solve this now?” no longer maps to “can you think?” when perfect solutions are commodities.

What companies are trying
- Hardening the format: in‑person rounds, no‑internet segments, paper/whiteboard thinking, live code walkthroughs, and off‑script variations to force real-time reasoning.
- Shifting the tasks: debug unfamiliar, messy code; read and critique diffs; write tests; reason about logs and flaky failures; small iterative builds where you must explain trade‑offs as you go.
- Artifact‑based behavioral: ask for concrete stories with PRs, design docs, or incident write‑ups; “teach back” a concept; explain why a change is safe.
- Process changes: paid work trials, stronger references, structured rubrics, portfolio review. Some teams experiment with “AI‑allowed” sections that grade how you prompt, verify, test, and critique model output.

Tensions and risks
- Arms race fatigue: proctoring and AI‑detection are brittle and create false positives.
- Fairness: in‑person favors locals; take‑homes and trials can be exclusionary or legally fraught if not paid and scoped.
- Philosophy split: ban AI to isolate raw reasoning vs embrace AI and evaluate judgment with the tool. The post argues interviews should measure problem‑solving, not prompting—but acknowledges the industry hasn’t cleanly separated the two yet.

The takeaway
AI didn’t just make interviews harder; it invalidated their core signal. The next wave will emphasize observable thinking: debugging, code reading, trade‑off narration, tests and verification, and learning speed—sometimes with AI explicitly in-bounds, sometimes with it off. Until then, expect more in‑person loops, stricter identity checks, and interview designs that reward how you reason, not how perfectly you recite.

The Hacker News discussion on AI's disruption of technical interviews revolves around several key themes:

1. **AI's Impact on Traditional Interviews**:  
   - AI tools allow candidates to generate polished code and scripted answers, undermining the traditional "LeetCode + system design" pipeline. This erodes trust in coding assessments, as candidates may rely on AI instead of demonstrating genuine problem-solving skills.  
   - Companies like Google and Microsoft are pivoting to **in-person interviews**, stricter identity checks, and practical tasks (e.g., debugging, live coding) to combat AI misuse.  

2. **Flaws in Legacy Interview Practices**:  
   - Pre-LeetCode interviews (e.g., brainteasers, haphazard technical rounds) were flawed but valued spontaneity. Modern prep culture (grinding LeetCode for months) filters for rote memorization, not real-world competence.  
   - Overemphasis on **GPAs and elite universities** (e.g., Google’s past reliance on transcripts) is criticized as a poor proxy for job performance, favoring homogenized candidates over diverse skill sets.  

3. **Debates on What to Assess**:  
   - Some argue for **practical evaluations**: code reviews, trade-off discussions, debugging, and artifact-based assessments (e.g., past PRs, incident reports). Others stress behavioral interviews that probe decision-making (e.g., "Why is this change safe?").  
   - Tension exists between **banning AI** (to isolate raw skill) and **integrating it** (to evaluate AI-augmented judgment).  

4. **Process Challenges and Fairness**:  
   - Proctoring and AI-detection tools are seen as brittle, risking false positives. In-person interviews disadvantage remote candidates, while unpaid take-homes raise equity concerns.  
   - Critiques of **stressful, arbitrary interview rituals** (e.g., whiteboard hazing) highlight the need for structured rubrics and work trials to reduce bias.  

5. **Cultural Shifts**:  
   - Comments lament the commodification of interviews, where candidates "perform enthusiasm" rather than demonstrate authentic interest. Some reminisce about informal ’90s-era interviews focused on conversation, not performative testing.  
   - A recurring theme: Interviews should prioritize **observable reasoning** (e.g., narrating trade-offs, explaining code) over perfection or rote answers.  

**Takeaway**: The industry is grappling with AI’s destabilization of interview norms. While solutions like practical assessments and in-person rounds gain traction, systemic issues (fairness, relevance, bias) persist. The future may hinge on separating *problem-solving aptitude* from rehearsed performance, whether through AI-integrated tasks or revamped, transparent evaluation frameworks.

