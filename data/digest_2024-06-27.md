## AI Submissions for Thu Jun 27 2024 {{ 'date': '2024-06-27T17:12:25.184Z' }}

### Infrastructure setup and open-source scripts to train 70B model from bare metal

#### [Submission URL](https://imbue.com/research/70b-infrastructure/) | 228 points | by [thejash](https://news.ycombinator.com/user?id=thejash) | [28 comments](https://news.ycombinator.com/item?id=40816158)

The Imbue Team achieved an impressive feat by training a 70B parameter model from scratch, outperforming GPT-4o on reasoning tasks. They share a detailed guide on setting up the infrastructure from scratch, including host-level health checks, an NCCL patch, stress tests, and more. The process involved provisioning individual machines, setting up InfiniBand, ensuring healthy machines, diagnosing training issues, and improving infrastructure tooling. The cluster comprised 4,092 H100 GPUs across 511 computers, with direct GPU connections through ConnectX-7 cards on a fully non-blocking InfiniBand network. This article provides insights into their extensive infrastructure setup and learnings encountered along the way.

The discussion on this topic covers various aspects such as the technical details of training a 70B parameter model from scratch, comparisons with GPT-40, the challenges and successes encountered during the process, as well as criticisms on the writing style of the article. Some users appreciate the detailed information shared by the Imbue Team, while others raise questions about the hardware setup, budget estimates, and power consumption of the infrastructure. Additionally, there are discussions about the potential use of GPUs for mining cryptocurrencies and the efficiency of training models using different hardware configurations. Overall, the discussion delves into the technical intricacies of training large models and the implications of such endeavors.

### ID verification service for TikTok, Uber, X exposed driver licenses

#### [Submission URL](https://www.404media.co/id-verification-service-for-tiktok-uber-x-exposed-driver-licenses-au10tix/) | 371 points | by [brw](https://news.ycombinator.com/user?id=brw) | [228 comments](https://news.ycombinator.com/item?id=40805949)

A cybersecurity researcher has uncovered a concerning breach involving an identity verification service used by TikTok, Uber, and others. The Israeli company, AU10TIX, inadvertently exposed administrative credentials online for over a year, potentially granting hackers access to sensitive user data. This incident sheds light on the risks associated with identity verification services as more platforms require users to submit real identity documents. As social networks and adult websites adopt identity verification models, the vulnerability of these verification services to cyber attacks becomes increasingly apparent.

The discussion on Hacker News surrounding the cybersecurity breach involving an identity verification service exposed various viewpoints on the incident.

- One user pointed out that companies often claim to have strict security measures in place but may not actually prioritize security until a breach occurs. They cited examples like the Ashley Madison data breach where security practices were lacking.
- Another user highlighted the importance of GDPR regulations in Europe, pointing out that companies must comply with legal requirements for data deletion.
- The conversation also delved into the issue of trust in vendors who handle sensitive data, with some discussing the challenges of managing access credentials securely.
- One user mentioned concerns about LinkedIn's identity verification process, expressing doubts about the security measures in place.
- Furthermore, the discussion touched on the responsibility of companies to handle data securely and the potential consequences for such negligence, including customer compensation and potential legal action.

Overall, the comments reflected a mix of skepticism about companies' security practices, the importance of regulatory compliance, and the need for greater accountability in handling sensitive data.

### Maker of RStudio launches new R and Python IDE

#### [Submission URL](https://www.infoworld.com/article/3715702/maker-of-rstudio-launches-new-r-and-python-ide.html) | 167 points | by [javierluraschi](https://news.ycombinator.com/user?id=javierluraschi) | [99 comments](https://news.ycombinator.com/item?id=40815097)

The company behind the popular RStudio IDE has introduced a new "next-generation" IDE called Positron designed specifically for R and Python programmers. Based on Microsoft's Visual Studio Code, Positron is geared towards making setup easier for users, eliminating the need to install additional extensions for R and Python functionalities. The IDE includes a built-in Data Explorer for exploring data frames and offers various features to facilitate code writing and data exploration. Although still in the early stages of development, Positron aims to be a versatile tool for data scientists and developers working with R and Python. It's definitely worth keeping an eye on this promising new IDE for your coding projects.

The discussion on the Hacker News post about the new "Positron" IDE by RStudio revolves around various aspects of the new IDE, comparisons to existing tools like RStudio and Jupyter, the technology stack used in Positron, and the potential impact on programming workflows for data scientists and developers. Some users express excitement about trying out Positron, while others share their preferences for different IDEs and tools. There is a discussion about the features and functionality of Positron, including the integration of R and Python functionalities, the user interface, and the potential benefits for the programming community. Additionally, there are comparisons made between RStudio, Jupyter, and VSCode, highlighting the strengths and weaknesses of each tool for different use cases. Overall, the conversation provides valuable insights into the interests and preferences of the programming community regarding IDEs, programming languages, and data analysis tools.

### How to think about creating a dataset for LLM fine-tuning evaluation

#### [Submission URL](https://mlops.systems/posts/2024-06-25-evaluation-finetuning-manual-dataset.html) | 130 points | by [sebg](https://news.ycombinator.com/user?id=sebg) | [7 comments](https://news.ycombinator.com/item?id=40809033)

In the latest blog post, the author delves into evaluating the fine-tuned language models they have been working on. The goal is to move beyond gut feelings and quantify the performance of these models objectively. The author showcases various evaluations they are adding to their test suite, including core evaluations for accuracy, handling out-of-domain data, and interpreting gradations in data like 'a couple', 'a few', and 'many'. By comparing model predictions against human annotations, the author aims to identify strengths and weaknesses in the models, such as how they handle edge cases and new information. These evaluations promise to shed light on the effectiveness of the fine-tuned models and provide insights for further model improvements.

- The user "vgbsnsssr" appreciates the similarities between the current blog post and a previous one they read recently but points out a slight omission regarding the understanding of details and potential abstractions in the current work. They highlight the importance of having comprehensive documentation to address similar problems as previously documented processes.
- "strckvl" mentions that there aren't many constraints like the ones mentioned in the post in their computer space, and suggests that unslothing works on a single GPU machine and didn't fit their purpose. They express interest in a blog post on the topic and thank for the sharing.
- "nslth" clarifies that they are testing on a single GPU and are looking forward to following future posts.
- "msp26" talks about the task of data extraction in people's full names for training LoRA classification.
- "swlsh" comments that LoRA is a perfect fit for tasks that deal with domain-specific false touch.
- "clrnbll" mentions that unless GPUs are available, LoRa might not be accessible, and suggests that some tasks can skip the problem entirely by using a small dataset with a simple model.
- "hnkly" discusses the importance of good filtering in the dataset for training AI models to understand the domain problem in a deterministic system.

### Google Sheets ported its calculation worker from JavaScript to WasmGC

#### [Submission URL](https://web.dev/case-studies/google-sheets-wasmgc) | 420 points | by [microflash](https://news.ycombinator.com/user?id=microflash) | [195 comments](https://news.ycombinator.com/item?id=40808820)

Google Sheets recently made a significant update by migrating its calculation worker from JavaScript to WasmGC, a move aimed at boosting performance. The collaboration between the Sheets and Chrome teams led to the development of this new engine that runs on Chrome, setting the stage for more Google apps to adopt WasmGC.

Initially written in Java back in 2006, the Google Sheets calculation engine made a transition to JavaScript for in-browser processing starting in 2013. This shift demanded meticulous validation to ensure accuracy, leading the team to develop an internal validation mechanism. Through this, they discovered that the JavaScript engine was over three times slower than its Java counterpart, highlighting the need for improvement.

JavaScript's dynamic nature albeit fast for certain tasks, couldn't match up to languages like Java and C++ for heavy computations. This performance gap spurred the adoption of WasmGC, an extension to WebAssembly designed to compile garbage-collected languages like Java. With the potential to offer near-native speed performance on the web, WasmGC promises to revolutionize how applications run in the browser.

The partnership between Google Workspace and Chrome proved vital in evaluating WasmGC through the Sheets calculation engine. The collaborative effort led to the successful implementation of WasmGC in Sheets by the end of 2021, albeit facing challenges and requiring extensive optimization. Despite initial performance disparities compared to JavaScript, ongoing refinements and optimizations are gradually bridging the gap.

The transition to WasmGC represents a significant milestone for Google Sheets and exemplifies the tech giant's commitment to pushing boundaries in web technology. This advancement not only enhances the performance of Google Sheets but also paves the way for future applications to leverage the power of WasmGC for improved efficiency and speed.

The discussion on Hacker News regarding the recent update in Google Sheets transitioning its calculation worker from JavaScript to WasmGC involved various opinions and insights. Here are some key points highlighted in the discussion:

1. **Performance Comparison**: Users compared the performance of WasmGC to JavaScript, with some pointing out significant speed improvements in the WasmGC version compared to the initial JavaScript version in Google Sheets.
2. **Optimizations**: The discussion touched upon various optimizations made in the transition to WasmGC, including utilizing Java Virtual Machine (JVM) optimization techniques in the new engine.
3. **Language Comparison**: The conversation delved into the differences in performance and optimization strategies between Java, JavaScript, and WebAssembly, particularly emphasizing the unique advantages of each language in certain scenarios.
4. **Technical Details**: Some users discussed technical aspects such as memory models, method dispatch performance, and shared memory concepts related to the transition to WasmGC.
5. **Performance Metrics**: There were mentions of improved performance metrics in the new engine, highlighting the collaborative effort between Google Workspace and Chrome teams to enhance Google Sheets' efficiency.
6. **Browser Support and Compatibility**: Users raised questions regarding browser support for WasmGC and how it integrates with Google Sheets compared to JavaScript, as well as the potential impact on the user experience across different platforms.
7. **Development Tools**: The conversation highlighted various development tools and resources related to transitioning to WasmGC, including discussions around the Kotlin Multiplatform project and its compatibility with WebAssembly.

Overall, the discussion showcased a mix of technical details, performance comparisons, and considerations about the implications of adopting WasmGC in Google Sheets, providing insight into the advancements in web technology spearheaded by Google.

### AI Revolutionized Protein Science, but Didn't End It

#### [Submission URL](https://www.quantamagazine.org/how-ai-revolutionized-protein-science-but-didnt-end-it-20240626/) | 103 points | by [sblank](https://news.ycombinator.com/user?id=sblank) | [31 comments](https://news.ycombinator.com/item?id=40806151)

In December 2020, during a virtual conference due to the pandemic lockdowns, the protein folding problem saw a groundbreaking moment with the introduction of AlphaFold2 by Google's DeepMind. This AI tool revolutionized protein science by accurately predicting 3D protein structures with over 90% accuracy, leaving the scientific community in awe. The success of artificial intelligence where human efforts had struggled marked a significant shift in how biologists approach protein research.

The impact of AlphaFold2 has sparked debates and discussions within the scientific community, with some fearing their jobs might be at risk while others see the potential for revolutionizing drug development. Despite its remarkable achievements, AlphaFold2 is not a replacement for traditional biological experiments but rather a complementary tool that highlights the importance of combining AI with experimental research.

The successor to AlphaFold2, AlphaFold3, announced in May 2024, has continued to push the boundaries by modeling protein structures in conjunction with other molecules like DNA or RNA. This ongoing advancement in AI-powered protein science has inspired new algorithms, biotech companies, and innovative ways of conducting scientific research, demonstrating the profound impact of artificial intelligence in shaping the future of molecular biology.

The discussion revolves around the article discussing the advances made in protein folding using artificial intelligence, particularly with AlphaFold2 and its successor AlphaFold3. Some users express skepticism about the claims made in the article, questioning the complexity of the protein folding problem and the effectiveness of machine learning tools like AlphaFold. Others delve into the technical aspects of protein folding, addressing concepts like global minimum energy configurations, stability of proteins, and the role of optimization in solving complex problems. References to mathematical progressions and analogies are made to elucidate certain points. Overall, there is a mix of opinions on the implications and future potential of AI in protein science, ranging from excitement about its transformative capabilities to cautious skepticism about its limitations.

### Schild's Ladder by Greg Egan

#### [Submission URL](https://www.gregegan.net/SCHILD/SCHILD.html) | 34 points | by [Bluestein](https://news.ycombinator.com/user?id=Bluestein) | [15 comments](https://news.ycombinator.com/item?id=40813862)

In the latest masterpiece from Greg Egan, "Schild’s Ladder," we are transported into a universe where Cass is on the brink of a groundbreaking discovery. At Mimosa Station, she embarks on an experiment involving quantum graphs that could revolutionize physics as we know it. However, the consequences of her success lead to the emergence of a destabilizing novo-vacuum that threatens entire systems across space.

Caught between Preservationists seeking to undo the damage and the adventurous Yielders embracing the challenge of survival beyond the border, tensions rise as allegiances are tested. As Cass and her allies navigate through these two vastly different universes, the fate of civilizations hangs in the balance.

With a blend of scientific intricacies and gripping narrative, "Schild’s Ladder" takes readers on a journey through parallel worlds where the quest for knowledge collides with the struggle for survival. Dive into this riveting tale that challenges the boundaries of possibility and the essence of existence.

1. LeifCarrotson noted that Greg Egan's works are mildly lacking in character development and that despite Egan's efforts in hard science fiction worldbuilding, the story can feel a bit thin.
2. npnts mentioned that in the book "Diaspora," the characters literally develop, indicating perhaps a different approach to character development by Egan.
3. flngr praised the book "Diaspora," describing it as amazing with a great mind-expanding hard science fiction narrative, and highly recommended it.
4. kmmshtr labeled Greg Egan's work as an all-time favorite in science fiction novels.
5. Bluestein expressed intrigue in "Permutation City" and "Diaspora," and noted the gateway drug-like quality of Egan's writing.
6. qbx shared their love for Greg Egan's work and mentioned that it's a great source for hard science fiction.
7. Khelavaster referred to Greg Egan's book as excellent and truly modern.
8. Bluestein added that Egan's writing introduces hefty philosophical concepts, transcending reality and touching on mathematics and physics, making it a rewarding read.

### 110 new languages are coming to Google Translate

#### [Submission URL](https://blog.google/products/translate/google-translate-new-languages-2024/) | 54 points | by [mfiguiere](https://news.ycombinator.com/user?id=mfiguiere) | [21 comments](https://news.ycombinator.com/item?id=40808584)

Google Translate is breaking language barriers yet again by adding 110 new languages, including Cantonese, NKo, and Tamazight. This expansion, made possible using AI, aims to help over half a billion people worldwide. From Afar in Africa to Manx in the Isle of Man, these new languages represent a diverse array of cultures and communities. Through initiatives like the 1,000 Languages Initiative and the use of PaLM 2 large language model, Google Translate continues to grow and connect people across the globe. Whether translating Afar from Djibouti or Tok Pisin from Papua New Guinea, the world is now more accessible through Google Translate's latest update.

1. **tkglly** pointed out that the accuracy of Google Translate's translations is not as great when compared to translations done by Large Language Models like ChatGPT or Claude. They explained that sensitive contextual prompts are required for accurate translations, and even when explicitly prompted, Google Translate tends to take a non-size-fits-all approach to text translation. For example, they provided an illustration using Japanese words to describe older and younger siblings, highlighting differences in translation by different models.

2. **southernplaces7** mentioned the importance of Google working to improve the quality of translations in existing languages, particularly major languages like English and Spanish. They noted that improving the quality of Google Translate could uncover myriad errors. **pfnnkchn** questioned whether complaining about NASA's funding would solve poverty on Earth.

3. **Yawrehto** provided a link to a full list of the newly added languages by Google Translate, mentioning the inclusion of Ladino and discussing the challenges in learning languages like this hybrid of Spanish and Hebrew. **gmby** and **ratg13** further elaborated on the difficulties and the historical context of the Ladino language.

4. **PoignardAzur** highlighted the Celtic language of Manx spoken in the Isle of Man, discussing its extinction and recent revival movement. **KptMarchewa** expressed hope that the quality of translations would improve with models like ChatGPT and possibly Gemini.

5. **anon1094** appreciated the broad range of languages supported by Google Translate, emphasizing the importance of preserving and promoting minority languages. **mncdr** added that language diversity enriches interactions and relationships, and the extinction of languages can accelerate due to various factors including climate change.

6. **Flimm** expressed delight at the addition of expected languages on Google Translate. **dbbk** discussed the extensive language support by DeepL and comparisons to Google Translate in supporting different languages like Spanish. **snhntr** highlighted the importance of preserving minority languages like Manx and the significance of their implementation in educational systems.

7. **trcrtps** and **dbgnk** elaborated on the challenges of translating Spanish into English and the nuances in regional language variants. **nxrbl** cautioned against solely relying on machine translations and recommended using native speakers for better translation quality.

8. **jinpa_zangpo** expressed disappointment at the unavailability of Tibetan on Google Translate and questioned the decision behind not including certain languages. **jnr** shared a link to the languages available on Google Translate and clarified that Tibetan was among the released languages.

In conclusion, the discussion highlighted various perspectives on the language expansion of Google Translate, the challenges in translation accuracy, the importance of preserving minority languages, and the impact of language diversity on global interactions.

