## AI Submissions for Sat Oct 11 2025 {{ 'date': '2025-10-11T17:13:25.031Z' }}

### Anthropic's Prompt Engineering Tutorial

#### [Submission URL](https://github.com/anthropics/prompt-eng-interactive-tutorial) | 291 points | by [cjbarber](https://news.ycombinator.com/user?id=cjbarber) | [75 comments](https://news.ycombinator.com/item?id=45551260)

Anthropic’s Interactive Prompt Engineering Tutorial (GitHub)

What it is:
- A free, hands-on course from Anthropic on writing effective prompts for LLMs, packaged as Jupyter notebooks (also available as a Google Sheets version via Claude for Sheets).
- Uses Claude 3 Haiku for examples, with notes on Sonnet and Opus; most techniques are model-agnostic.

What’s inside:
- 9 chapters from beginner to advanced with exercises and answer keys.
- Core topics: prompt structure, clarity, role assignment, separating data from instructions, output formatting, guiding step-by-step reasoning, and using examples.
- Advanced: reducing hallucinations, building complex/industry prompts (chatbots, legal, finance, coding).
- Appendix on chaining prompts, tool use, and retrieval.

Why it matters:
- Offers concrete, reproducible templates and practice scenarios—useful for teams standardizing LLM workflows and for newcomers who want 80/20 techniques that noticeably improve output quality.

Repo signals:
- ~19.3k stars, ~1.9k forks; notebook-heavy (Jupyter ~98%).

The discussion debates whether "prompt engineering" qualifies as legitimate **engineering**, with arguments centered on definitions, terminology, and professional standards:

1. **Terminology Debate**:
   - Critics (e.g., *jwr*) argue traditional engineering relies on **predictable, knowledge-based principles** (e.g., physics), whereas prompt engineering is seen as trial-and-error "throwing prompts at a wall."
   - Proponents note "engineering" has broader dictionary definitions (e.g., "skillful maneuvering"), and disciplines like software engineering also deal with **non-deterministic systems** (*vmr*, *smnw*).

2. **Professional Standards**:
   - In Canada and other regions, "Engineer" is a **legally protected title** requiring licensure (*rr808*, *rl*), sparking tension with self-described "prompt engineers."
   - Some argue credentials shouldn’t gatekeep the term (*dlchn*), while others emphasize legal responsibilities tied to regulated engineering roles.

3. **Technical Validity**:
   - Critics highlight the **lack of predictability** in LLM outputs and evolving models, contrasting with established engineering practices (*ndsrnm*).
   - Supporters counter that systematic testing, metrics, and iterative refinement (*atherton33*) align with engineering rigor, even in non-deterministic contexts.

4. **Comparisons to Software Engineering**:
   - Parallels are drawn to software engineering’s acceptance despite dealing with **unpredictable systems** (e.g., distributed networks), suggesting prompt engineering could follow a similar path (*nthrbnnsr*).

**Conclusion**: The debate reflects broader tensions over language, professional identity, and evolving technical fields. While critics dismiss prompt engineering as unserious, proponents frame it as a nascent discipline requiring systematic approaches akin to other engineering domains.

### CamoLeak: Critical GitHub Copilot Vulnerability Leaks Private Source Code

#### [Submission URL](https://www.legitsecurity.com/blog/camoleak-critical-github-copilot-vulnerability-leaks-private-source-code) | 126 points | by [greyadept](https://news.ycombinator.com/user?id=greyadept) | [17 comments](https://news.ycombinator.com/item?id=45553422)

What happened
- Security researcher Omer Mayraz disclosed a CVSS 9.6 vulnerability in GitHub Copilot Chat that allowed silent exfiltration of secrets and source code from private repos and full control over Copilot’s replies (e.g., injecting malicious code suggestions/links). Reported via HackerOne; GitHub fixed it by disabling image rendering in Copilot Chat.

How it worked (high level)
- Remote prompt injection: Hidden comments in pull request descriptions (“invisible comments,” an official GitHub feature) were used to smuggle instructions into Copilot’s context. Any user viewing the PR and using Copilot Chat could have their assistant hijacked, with Copilot operating under that user’s permissions.
- CSP bypass using GitHub’s own infra: Although GitHub’s strict Content Security Policy blocks arbitrary external fetches, GitHub rewrites external images through its Camo proxy (camo.githubusercontent.com) with signed URLs. By creatively leveraging Camo URL generation paths, the researcher found a way to smuggle data out despite CSP.

Why it matters
- AI assistants that ingest repository context dramatically broaden the attack surface: invisible or innocuous-looking repo content can steer the model, leak sensitive data, or push malicious dependencies.
- The combination of model prompt injection and front-end CSP edge cases creates powerful cross-layer exploits.

What GitHub changed
- Disabled image rendering in Copilot Chat to close the exfil channel tied to Markdown-rendered images/Camo.
- Vulnerability was remediated after disclosure; details credit GitHub’s response but underscore systemic risks.

Takeaways for teams
- Treat AI assistants as privileged actors: restrict their repo/org scopes and tokens to least privilege.
- Scrub untrusted content in repos (PR descriptions, READMEs, issues) that surfaces in AI context; consider policies for hidden/embedded content.
- Disable or limit rich rendering in AI chats where possible; prefer link sanitization/allow-lists.
- Maintain defense-in-depth: secrets scanning, dependency allow-lists, and mandatory human review on AI-suggested changes.

The discussion surrounding the CamoLeak GitHub Copilot vulnerability highlights several key points and concerns:  

### **Technical Insights**  
1. **Fix Critique**: While GitHub addressed the issue by disabling image rendering in Copilot Chat, some users (*mnchlx*, *Thorrez*) argue this only closed one exfiltration vector. They speculate that alternative methods (e.g., base64 encoding or non-Camo URLs) might still bypass CSP if not fully mitigated.  
2. **Exploit Mechanics**: The attack combined GitHub’s Camo proxy (to bypass CSP) and invisible PR comments (a legitimate GitHub feature) for stealthy prompt injection. Users (*chrcrct*, *PufPufPuf*) emphasize the risk of attackers leveraging contributor text (PRs, issues) to hijack AI context and permissions.  

### **Broader AI Security Concerns**  
- **Trust in AI Tools**: Participants (*rnnngmk*) question the reliability of proprietary AI solutions in cybersecurity, advocating for FOSS alternatives with greater transparency.  
- **LLM Vulnerabilities**: Subthreads debate whether modern LLMs are inherently vulnerable to prompt injection, comparing mitigations to "ASLR for AI" but acknowledging systemic challenges (*fn-mt*, *PufPufPuf*).  
- **Scope of Risk**: Hidden HTML/PR comments (*xstf*, *RulerOf*) and over-permissioned AI agents (*chrcrct*) are flagged as ongoing attack surfaces.  

### **Mitigation Suggestions**  
- **Local AI Agents**: A user (*nprtm*) proposes running AI tools locally to reduce exposure, though practicality is debated.  
- **Input Sanitization**: Calls to sanitize PR templates and restrict LLM access to untrusted content (*PufPufPuf*, *djmps*).  

### **Community Reaction**  
- **Mixed Sentiment**: Some praise the disclosure (*adastra22*) and GitHub’s response, while others criticize incomplete fixes (*mnchlx*) or urge readers to review the original report (*frh*: "RTFA/RTFTLDR").  

### **Key Takeaway**  
The discussion underscores the complexity of securing AI-integrated tools, balancing prompt fixes with systemic changes to LLM permissions, input validation, and CSP enforcement. The incident highlights how "benign" platform features (like Camo proxy) can become critical vulnerabilities when combined with AI context-hijacking.

### Paper2video: Automatic video generation from scientific papers

#### [Submission URL](https://arxiv.org/abs/2510.05096) | 76 points | by [jinqueeny](https://news.ycombinator.com/user?id=jinqueeny) | [23 comments](https://news.ycombinator.com/item?id=45553701)

TL;DR: A new benchmark + system that auto-generates academic presentation videos straight from papers—slides, subtitles, speech, cursor movements, and a talking head—claiming higher faithfulness than existing baselines. Dataset and code are linked on the arXiv page.

What’s new
- Paper2Video benchmark: 101 research papers paired with author-made presentation videos, slides, and speaker metadata to study presentation generation.
- Four evaluation metrics, tailored for “did the video actually teach the paper?”: Meta Similarity, PresentArena, PresentQuiz, and IP Memory.
- PaperTalker system: a multi-agent pipeline that:
  - Generates slides and refines layout via a tree-search-based “visual choice” step
  - Extracts and places dense multimodal content (text, figures, tables)
  - Grounds a cursor to guide attention
  - Produces subtitles and TTS speech
  - Renders a talking head
  - Parallelizes slide-wise generation for speed

Why it matters
- Academic videos are time sinks; automating 2–10 minute summaries could free researchers from slide design and recording.
- Coordinated, multimodal alignment (slides + narration + on-screen pointer + face) is a tougher problem than generic video gen; the benchmark and metrics help standardize evaluation.
- Accessibility and dissemination: makes papers more approachable to wider audiences.

Results (per authors)
- On the Paper2Video benchmark, generated presentations are “more faithful and informative” than prior baselines across the proposed metrics.
- Code, dataset, and project page are available via the arXiv entry.

Caveats and open questions
- Scale: 101 papers is a solid start but still small; generalization across fields and layouts remains to be shown.
- Metric validity: how strongly do PresentArena/PresentQuiz/etc. correlate with human comprehension?
- IP and consent: reuse of figures, author likeness/voice, and distribution policies will matter in practice.
- Hallucinations and factual drift: long-context, figure-heavy papers are risky; robust grounding and citation display will be key.

Link: arXiv: 2510.05096 (project page and code linked from the arXiv record)

The discussion around the Paper2Video submission highlights a mix of cautious optimism, practical concerns, and humorous skepticism:

### **Key Points**
1. **Potential Benefits**:
   - **Time-saving**: Users acknowledge automating presentations could free researchers from tedious slide design and recording, especially for conferences requiring travel.
   - **Accessibility**: Could make dense papers more approachable for broader audiences, including non-experts or students.
   - **Baseline Improvement**: Some note existing scientific presentations often suffer from cluttered slides or poor design, which AI might mitigate.

2. **Criticisms & Concerns**:
   - **Depth & Engagement**: Skepticism about whether AI-generated videos can capture nuanced explanations, storytelling, or clarity that human presenters provide. Comments highlight risks of superficiality ("adding fluff") and missing critical details.
   - **Presentation Quality**: Concerns about AI-generated voices, robotic delivery ("subtle parody"), and awkward visuals (e.g., cursor movements, "talking heads") feeling unnatural compared to human charisma.
   - **Ethics & Practicality**: Questions about intellectual property (figure reuse, voice cloning), hallucination risks in technical content, and whether metrics like "PresentQuiz" truly measure comprehension.

3. **Humorous Takes**:
   - Jokes about AI presenters resembling "SteveGPT" (Steve Jobs-style) or dystopian references (*Videodrome*), highlighting unease with synthetic personas.
   - Playful comparisons to unrelated concepts (e.g., *Lorna Shore concerts*, VR sword-fighting games) underscore concerns about engagement gimmicks.

4. **Related Tools & Alternatives**:
   - Users mention existing solutions like whiteboard explainers, Minute Papers, or improving personal presentation skills. Some share their own projects (e.g., interactive paper explainers) as alternatives.

### **Notable Suggestions**
   - **Improvements**: Incorporate feedback loops for slide layout, avoid verbatim text, and prioritize natural narrative flow over rigid content placement.
   - **Validation**: Calls for human evaluation to complement automated metrics and ensure generated videos aid actual learning.

### **Conclusion**
While the tool is seen as a promising step, many argue that human-presented storytelling and clarity remain irreplaceable. The discussion reflects broader debates about AI’s role in academia—balancing efficiency gains with the risk of depersonalizing science communication.

### Microsoft only lets you opt out of AI photo scanning 3x a year

#### [Submission URL](https://hardware.slashdot.org/story/25/10/11/0238213/microsofts-onedrive-begins-testing-face-recognizing-ai-for-photos-for-some-preview-users) | 750 points | by [dmitrygr](https://news.ycombinator.com/user?id=dmitrygr) | [285 comments](https://news.ycombinator.com/item?id=45551504)

Microsoft is testing face recognition in OneDrive photos — with a puzzling “3 times a year” opt-out limit

- What’s happening: Some OneDrive users are seeing a new preview feature that uses AI to recognize faces in their photo libraries. The setting appears as on by default for those in the test.
- The catch: The toggle says you can only turn the feature off three times per year. One tester couldn’t disable it at all — the switch snapped back on with an error.
- Microsoft’s stance: The company confirmed a limited preview but wouldn’t explain the “three times” rule or give a timeline for wider release. It pointed to its privacy statement and EU compliance. A support page still claims the feature is “coming soon,” a note that’s been unchanged for nearly two years.
- Privacy pushback: EFF’s Thorin Klosowski argues the feature should be opt-in with clear documentation, and users should be able to change privacy settings whenever they want.
- Open questions: Is face recognition currently active for any users by default? Why restrict how often people can opt out? When will Microsoft clarify and ship this broadly?

**Summary of Discussion:**

The discussion reflects widespread frustration and skepticism toward Microsoft's "3 times a year" opt-out limit for OneDrive's face recognition feature. Key points include:

1. **Arbitrary Opt-Out Limit**:  
   Users question why the limit is set to three, dismissing theories about cultural symbolism (e.g., religious trinities) and instead attributing it to psychological tactics to discourage opting out. Some suggest it’s a "dark pattern" to normalize surveillance or reduce server costs from repeated scans.

2. **Distrust in Microsoft’s Intentions**:  
   Commenters cite Microsoft’s history of overriding user settings (e.g., re-enabling features after updates) as evidence of bad faith. Many suspect the feature is tied to AI training or data harvesting, with concerns that facial data could be exploited for profit or shared with governments.

3. **Privacy vs. Cost-Saving**:  
   While some argue the limit prevents excessive server costs from frequent re-scans, others counter that privacy controls should never be restricted. Critics demand opt-in defaults and unrestricted opt-out options, echoing the EFF’s stance.

4. **Regulatory and Compliance Concerns**:  
   Questions arise about HIPAA compliance and Microsoft’s ability to safeguard sensitive data. Users highlight past incidents where Microsoft mishandled health or enterprise data, fueling doubts about their reliability.

5. **Technical and Legal Speculation**:  
   Debates focus on whether the limit is technically necessary (e.g., processing large photo libraries) or a legal safeguard to avoid liability. Skeptics argue Microsoft could anonymize data or process it locally but chooses not to for control.

6. **Calls for Transparency**:  
   Participants demand clarity on whether the feature is active by default, how data is used, and when Microsoft will address these concerns. Many urge regulatory intervention to hold the company accountable.

**Overall Sentiment**:  
The community views the opt-out limit as a red flag, emblematic of broader corporate overreach and erosion of user agency. Trust in Microsoft is low, with calls for stricter privacy laws and user-centric design.

### Superpowers: How I'm using coding agents in October 2025

#### [Submission URL](https://blog.fsck.com/2025/10/09/superpowers/) | 378 points | by [Ch00k](https://news.ycombinator.com/user?id=Ch00k) | [209 comments](https://news.ycombinator.com/item?id=45547344)

Claude Code gets plugins; “Superpowers” turns skills into a first‑class workflow

- What’s new: Anthropic rolled out a plugin system for Claude Code, and the author released “Superpowers,” a marketplace plugin that teaches Claude to use explicit “skills” (markdown playbooks) to plan and execute coding tasks.

- How to try: Requires Claude Code 2.0.13+. In Claude’s command palette:
  - /plugin marketplace add obra/superpowers-marketplace
  - /plugin install superpowers@superpowers-marketplace
  - Restart Claude; it will bootstrap itself and point to a getting-started SKILL.md.

- How it works:
  - Adds a default brainstorm → plan → implement loop; if you’re in a git repo, it auto-creates a worktree for parallel tasks.
  - Offers two modes: last month’s “human PM + two agents” or a new auto mode that dispatches tasks to subagents and runs code review after each step.
  - Enforces RED/GREEN TDD: write a failing test, make it pass, iterate.
  - Wraps up by offering to open a GitHub PR, merge the worktree, or stop.

- The big idea: Skills. They’re small, readable SKILL.md guides that the agent must use when applicable. The system encourages:
  - Discovering and invoking skills by name.
  - Writing new skills as the agent learns (self-improvement).
  - Extracting reusable skills from books/codebases by reading, reflecting, and documenting.

- Why it matters:
  - Moves agents from “prompt blob” to modular, auditable procedures.
  - Encourages reproducibility, code quality (TDD + reviews), and parallel dev via worktrees.
  - Aligns with a broader pattern (e.g., Microsoft’s Amplifier) where agents evolve by writing tools/docs for themselves.

- Notable details:
  - The author has a “How to create skills” skill; the system can expand itself by drafting new SKILL.md files.
  - Skill quality is tested with subagents using realistic, pressure-testing scenarios—“TDD for skills.”
  - Mentions IP gray areas when auto-extracting skills from proprietary books.

- Open questions:
  - How robust are skills across projects and teams?
  - Governance/attribution for skill content derived from third-party materials.
  - How well do subagent reviews catch subtle design or security flaws versus happy-path tests?

Takeaway: Plugins plus skills push Claude Code toward a disciplined, self-improving dev assistant—less chatty copilot, more process-driven teammate with standardized playbooks.

The discussion around AI coding tools like Claude Code's new plugin system and "Superpowers" reveals mixed experiences and perspectives:

### Key Themes:
1. **Effectiveness Varies by Task Complexity**:
   - Users report success with **micro-tasks** (e.g., templating HTML/CSS, writing tests) and repetitive work, but note limitations in **high-level design** or complex domains (e.g., Zig development).
   - For troubleshooting legacy codebases or dependency issues, Claude Code is praised for rapidly identifying solutions, though results can be inconsistent in extreme edge cases.

2. **Human Oversight & Process**:
   - Several users emphasize the need for **explicit instructions**, structured workflows (e.g., TDD, code reviews), and documentation to ensure quality. One user compares managing AI agents to leading a team, requiring clear planning, feedback, and quality checks.
   - Breaking tasks into smaller, specific chunks with thorough validation is advised over delegating large, vague goals.

3. **Skepticism vs. Optimism**:
   - **Optimists** highlight AI’s utility for accelerating coding, learning, and handling "run-of-the-mill" tasks (e.g., generating test cases), likening it to a junior developer.
   - **Skeptics** caution against over-reliance, noting AI can produce "nonsense" or overstep its domain. Some argue AI tools augment rather than revolutionize development, stressing the irreplaceable role of human design decisions.

4. **Challenges & Open Questions**:
   - **Token consumption** and context management with subagents raise concerns about efficiency.
   - Debate exists about whether AI’s "persuasive" outputs align with true problem-solving or merely mimic rhetorical patterns.
   - Questions linger about skill reproducibility across teams and governance for skills derived from proprietary materials.

### Notable Perspectives:
- **"kdd"** shares a [blog post](https://news.ycombinator.com/item?id=45503867) stressing the need for technical management skills when using AI agents, akin to mentoring interns.
- **"sfn42"** advocates using AI as a "tight leash" tool for specific subtasks rather than autonomous large-scale work.
- **"smnw"** links to a [GitHub repo](https://github.com/obra/Superpowers) and [notes](https://simonwillison.net/2025/Oct10/superpowers/) exploring skill-based workflows.

### Conclusion:
While AI coding tools show promise for productivity gains, success hinges on structured workflows, human oversight, and task specificity. The community remains divided on whether these tools represent incremental improvement or transformative change, with ongoing experimentation shaping best practices.

