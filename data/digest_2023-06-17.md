## AI Submissions for Sat Jun 17 2023 {{ 'date': '2023-06-17T17:10:07.334Z' }}

### Are chiplets enough to save Moore’s Law?

#### [Submission URL](https://www.eetimes.com/are-chiplets-enough-to-save-moores-law/) | 66 points | by [transpute](https://news.ycombinator.com/user?id=transpute) | [68 comments](https://news.ycombinator.com/item?id=36371651)

Nvidia and MediaTek announced at a press conference that Nvidia will supply GPU chiplets to MediaTek for incorporation into a yet-to-be-designed system-on-chip for in-cabin automotive applications. This latest announcement adds more validation for chiplets as a concept that many semiconductor makers are using to help keep Moore’s Law alive. Chiplets are not a new concept. The industry has been making multi-chip modules for decades, however, the ecosystem for commercial chiplets remains absent. In both AMD and Intel's cases, chiplets have proved so successful that they now employ chiplet technology throughout their respective product lines, including their flagship processor products. However, the lack of physical and electrical interface standards continues to hold back the broad commercialization of chiplets.

Nvidia will supply GPU chiplets to MediaTek for system-on-chip for in-cabin automotive applications. While chiplets are not a new concept for the industry, the ecosystem for commercial chiplets still remains absent. However, the lack of physical and electrical interface standards continues to hold back the broad commercialization of chiplets. The discussion revolved around the technical aspects of chiplets, their ability to keep up with Moore’s Law, the challenges faced, and their usage in different areas including the RISC-V community and the progress of Linux desktops. There were also suggestions for solutions to the issues faced, including the development of vertical chip stacking and the creation of a library.

### Show HN: Explore large language models with 512MB of RAM

#### [Submission URL](https://github.com/jncraton/languagemodels) | 133 points | by [jncraton](https://news.ycombinator.com/user?id=jncraton) | [32 comments](https://news.ycombinator.com/item?id=36369934)

LanguageModels is a Python package that allows learners and educators to explore large language models on any computer with 512 MB of RAM. The package provides simple functions using standard types, making it easy to interact with. The models are open and light-weight, and do not require any API keys as they perform all inference locally by default. Examples of basic usage include text completions, instruction following, chat, and external retrieval. The package also includes a semantic search function. All models included are free for educational use.

A Python package called LanguageModels has been introduced in a submission on Hacker News that allows learners and educators to explore large language models on any computer with 512 MB of RAM. Users can interact with the models, which are open and light-weight, without the need for API keys as they perform all inference locally by default. The models' basic usage includes chat, text completion, instruction following, and external retrieval. The thread discussion touches on several aspects of LanguageModels, such as how it works, how large models compare to smaller ones, and whether it is possible to teach an AI system common sense and practical knowledge. The conversation also broadens to cover other AI-related topics such as how LLMs operate, how they rely on statistical patterns, and the technical challenges in developing them.

### Congress is racing to regulate AI. Silicon Valley is eager to teach them how

#### [Submission URL](https://www.washingtonpost.com/technology/2023/06/17/congress-regulating-ai-schumer/) | 35 points | by [robg](https://news.ycombinator.com/user?id=robg) | [39 comments](https://news.ycombinator.com/item?id=36375992)

Washington lawmakers are finding themselves under pressure to draft laws addressing the promises and risks of AI as it rapidly develops. With the European Union already leapfrogging ahead of Washington with regards to advancing robust AI legislation, Members of Congress and their staffs are seeking a crash course on AI. However, the technical complexity behind AI has made it challenging for even experts to address the issue properly. Nevertheless, Corporate interests have started to provide congressional staff with information. These corporations are keenly interested in the process of developing AI without hindrance, and in some cases have outright recommended that they be allowed to set their own rules. Nevertheless, some consumer advocates are concerned about the level of influence corporate interests may have on lawmakers.

Discussion on the submission covers a range of topics, such as why laws should be based on understanding AI, whether Google and other corporations would comply with any regulations, and the significance of AI as a fundamental advancement for the internet and society. Some commenters express concern that potential regulations for AI may be difficult due to the selfish reasons of companies and the existential threat to their businesses, and it is suggested that community efforts will be key in making progress quickly and regulating the technology.

### Researchers warn of ‘model collapse’ as AI trains on AI-generated content

#### [Submission URL](https://venturebeat.com/ai/the-ai-feedback-loop-researchers-warn-of-model-collapse-as-ai-trains-on-ai-generated-content/) | 78 points | by [belter](https://news.ycombinator.com/user?id=belter) | [51 comments](https://news.ycombinator.com/item?id=36368848)

Researchers from the UK and Canada have discovered that as AI-generated content proliferates, it causes "irreversible defects" in generative AI, leading to poorer performance over time and a distortion of reality. The use of model-generated data in AI training ultimately leads to what is called model collapse, which means that the AI learns based on other models, dropping the true underlying data distribution for mistakes. These mistakes in generated data compound over time, ultimately leading the AI model to misperceive reality.

There was also a discussion about the limitations of generative AI models and the potential implications for human-AI interaction, along with some suggestions for potential solutions such as a more systematic training approach and the use of multiple AI models. Additionally, there were some concerns about the exploitation of AI-generated content for profit and the need for privacy laws to limit access to human data.

