## AI Submissions for Fri Oct 20 2023 {{ 'date': '2023-10-20T17:09:32.757Z' }}

### AI tidies up Wikipedia's references and boosts reliability

#### [Submission URL](https://www.nature.com/articles/d41586-023-02894-x) | 141 points | by [clockworksoul](https://news.ycombinator.com/user?id=clockworksoul) | [81 comments](https://news.ycombinator.com/item?id=37955658)

Artificial intelligence (AI) could improve the accuracy and reliability of Wikipedia by cleaning up inaccurate or incomplete reference lists, according to a study published in Nature Machine Intelligence. The study developed a neural-network-powered system called SIDE, which analyzes whether Wikipedia references support the claims they're associated with and suggests better alternatives for those that don't. SIDE was trained using existing featured Wikipedia articles and was able to identify claims within pages that have poor-quality references. When tested, 21% of Wikipedia users preferred the citations suggested by the AI, while 10% preferred the existing citations and 39% had no preference. The tool could potentially save time for editors and moderators that check the accuracy of Wikipedia entries, as long as it is deployed correctly.

The discussion on this submission includes various points of view regarding the use of AI to improve Wikipedia and the accuracy of citations. Some users express skepticism, suggesting that the accuracy of references can still be subjective and that the interpretation of research papers is best left to experts. Others argue that abstracts are sufficient for determining the usefulness of a paper and that the problem lies in people misinterpreting them. One user suggests banning abstracts altogether and focusing on the full papers. Another user points out the importance of context in interpreting papers and the challenges of correcting misinformation on Wikipedia. Some users mention that self-publishing authors and publishers with vested interests can contribute to inaccuracies. The discussion also touches on the difficulty of accessing scholarly papers and the impact factor of journals. Overall, there are differing opinions on the effectiveness of AI in improving the accuracy of Wikipedia and the reliability of its citations.

### Confidential computing and AI fit together

#### [Submission URL](https://www.edgeless.systems/blog/how-confidential-computing-and-ai-fit-together/) | 46 points | by [transpute](https://news.ycombinator.com/user?id=transpute) | [4 comments](https://news.ycombinator.com/item?id=37955620)

In this blog post, Felix Schuster explores the intersection of AI and confidential computing. He explains the concept of "Confidential AI" and highlights three major use cases: secure outsourcing of AI workloads, intellectual property (IP) protection for AI models, and privacy-preserving AI training and inference. 

Schuster notes that until recently, confidential computing couldn't practically be applied to AI due to the lack of trust between secure enclaves and AI accelerators. However, Nvidia's latest GPUs, like the Hopper H100, now come with comprehensive confidential computing features, allowing for secure communication between CPUs and GPUs.

He provides a technical summary of how Nvidia implements confidential computing, including remote attestation, encryption of communication between CPUs and GPUs, and memory isolation. With these foundations in place, confidential AI becomes possible.

The first use case Schuster discusses is the secure outsourcing of AI workloads. This enables companies to outsource AI tasks to an infrastructure they may not fully trust, such as a cloud provider. It can be particularly useful for banks or government institutions.

The second use case is IP protection for AI models. Confidential AI allows for the deployment of AI models in a way that prevents copying or altering. This is valuable when deploying proprietary AI models to customer sites or integrating them into 3rd party offerings.

Finally, Schuster explores privacy-preserving AI training and inference. Confidential computing enables the creation of "black box" systems that verifiably preserve privacy for data sources. By designing software to keep input data private and running it in a confidential-computing environment, data sources can be assured that their data will remain private. With confidential computing-enabled GPUs, AI training and inference can now be performed while preserving data privacy.

Overall, Schuster highlights the exciting possibilities that arise from combining AI and confidential computing, opening up new avenues for secure and privacy-preserving applications.

The discussion on this submission revolves around the importance of protecting people's data and the vulnerabilities that arise when data is not encrypted. The significance of cryptography in the digital world is emphasized, with contributors highlighting its role in ensuring privacy and security.

One user points out that the landscape of confidential computing has shifted from relying on cloud service providers and hardware vendors to technologies like AMD-SEV and Intel's SGX. Trust is central to confidential computing, and trust must exist somewhere in the system.

Another user mentions that the responsibility of creating secure and confidential virtual machines falls on the trusted entities, such as hardware vendors and root of trust (RoT) manufacturers. However, they also highlight the challenge of hardware vendors having direct access to customers' machines, which could pose a threat to confidentiality.

In addition, there is a discussion about the challenges of securely running machine learning models and the importance of ensuring the security of training data. The verifiability and improvements in hardware for confidential computing are recognized as positives.

Overall, the discussion centers around the need for strong security measures, the role of cryptography, and the challenges and advancements in confidential computing.

### 'Mind-blowing' IBM chip speeds up AI

#### [Submission URL](https://www.nature.com/articles/d41586-023-03267-0) | 38 points | by [bookofjoe](https://news.ycombinator.com/user?id=bookofjoe) | [36 comments](https://news.ycombinator.com/item?id=37955421)

Researchers at IBM have developed a brain-inspired computer chip called NorthPole that could revolutionize artificial intelligence (AI) by performing tasks faster and consuming significantly less power than existing technologies. The chip integrates computing and memory on a large scale, eliminating the need for frequent access to external memory. NorthPole runs neural networks and contains 256 computing units, each with its own memory, interconnected in a network inspired by the human cerebral cortex. It outperforms existing AI machines in image recognition tests and uses one-fifth of the energy of state-of-the-art AI chips. However, the chip's limited RAM makes it unsuitable for large language models, and it can only run pre-programmed neural networks. Nevertheless, it holds promise for speed-critical applications like self-driving cars.

The discussion on Hacker News revolves around the features and limitations of IBM's NorthPole brain-inspired computer chip. One user clarifies that the chip is specifically designed for inference-focused neural networks and is not suitable for large language models. Another user points out that Apple's chips also have limited on-package memory. Users also discuss the efficiency and design considerations of different processors, including AMD Threadripper and Intel's upcoming Sapphire Rapids. The limited RAM of NorthPole is highlighted as a limitation for large language models like ChatGPT. However, another user refers to supplementary materials from IBM's paper, stating that NorthPole's memory includes 192 MB of flexible memory and 768KB of onboard memory. The discussion also touches on the size of training data for large language models and the potential future developments in chip technology for AI applications. Different processors and companies, such as TPU, Apple, and Graphcore, are mentioned as well.

### Meta and AI companies recruited striking actors to train AI

#### [Submission URL](https://www.technologyreview.com/2023/10/19/1081974/meta-realeyes-artificial-intelligence-hollywood-actors-strike/) | 14 points | by [ilamont](https://news.ycombinator.com/user?id=ilamont) | [5 comments](https://news.ycombinator.com/item?id=37956254)

A recent project involving AI and emotion recognition has raised concerns among actors about how their likeness and performances may be used without their consent. The project, run by emotion AI company Realeyes and supported by Meta, involved actors participating in a two-hour shoot where their facial expressions and movements were captured to train AI algorithms. While the job posting stated that participants' likenesses would not be used for commercial purposes, the broad rights they signed away suggest otherwise. Actors fear that AI technology could potentially replace them in the industry. Experts warn that companies can only promise so much when it comes to data privacy, especially as AI advancements continue to evolve rapidly. Some actors feel that the project is exploitative, taking advantage of out-of-work actors and pitting them against the use of AI. The lack of clear rules regarding AI and actors' rights adds to the uncertainty. Overall, the project highlights the complex issues surrounding the use of AI in entertainment and the need for informed consent and protections for actors.

The discussion mainly revolves around the concerns raised by actors regarding the use of AI in the entertainment industry. One user, scarface_74, suggests that Facebook might be using people's work without proper approval, highlighting the need for restrictions on the exchange of labor for money. User3456335 agrees and argues that actors should demand restrictions on the use of AI to improve its ethics. In response, scarface_74 remarks that many people are forced to join agencies and work in the industry, which is seen as shocking. However, user two_in_one challenges this perspective by saying that some actors willingly join the industry. Scarface_74 counters by stating that only a minority of actors have major movie and TV show roles. Overall, the discussion touches upon forced labor, actors' agency, and the concentration of opportunities in the entertainment sector.

