## AI Submissions for Thu Sep 25 2025 {{ 'date': '2025-09-25T17:16:23.099Z' }}

### Improved Gemini 2.5 Flash and Flash-Lite

#### [Submission URL](https://developers.googleblog.com/en/continuing-to-bring-you-our-latest-models-with-an-improved-gemini-2-5-flash-and-flash-lite-release/) | 519 points | by [meetpateltech](https://news.ycombinator.com/user?id=meetpateltech) | [261 comments](https://news.ycombinator.com/item?id=45375845)

Google ships faster, cheaper Gemini 2.5 Flash/Flash-Lite previews, plus a “-latest” alias

What’s new
- Updated previews of Gemini 2.5 Flash and 2.5 Flash-Lite on Google AI Studio and Vertex AI.
- Big focus on efficiency: 50% fewer output tokens for Flash-Lite and 24% fewer for Flash, driven by reduced verbosity.
- Quality upgrades:
  - Flash-Lite: better instruction-following, tighter answers, stronger multimodal (audio transcription, image understanding) and translation.
  - Flash: improved agentic/tool use with a 5% gain on SWE-Bench Verified (48.9% → 54%), and better cost-efficiency “with thinking on.”

Why it matters
- Lower latency and cost for high-throughput apps; fewer tokens out means real savings.
- Stronger tool use and multi-step reasoning push agents closer to production viability.
- Multimodal and translation boosts broaden use cases (voice, vision, global apps).

Try it
- Preview model IDs: gemini-2.5-flash-preview-09-2025, gemini-2.5-flash-lite-preview-09-2025.
- New rolling aliases: gemini-flash-latest, gemini-flash-lite-latest (always point to newest).
  - Caveat: rate limits, cost, and features can change behind -latest; Google promises 2 weeks’ email notice before updates/deprecations.
- For stability, stick with gemini-2.5-flash and gemini-2.5-flash-lite.

Early signal
- Manus (autonomous agent startup) reports ~15% improvement on long-horizon agent tasks and strong cost-efficiency with the new Flash.

Bottom line
- This is a preview, not a new stable line, but it meaningfully cuts costs and improves agent/tool performance—good news for teams scaling LLM-powered apps without blowing up budgets.

The Hacker News discussion highlights mixed reactions to Google's Gemini 2.5 Flash/Flash-Lite updates, focusing on technical frustrations, comparisons to competitors, and usability concerns:

### Key Criticisms
1. **Reliability Issues**:  
   - Users report persistent problems like mid-sentence truncation, incomplete responses, and unreliable JSON formatting. Some compare Gemini unfavorably to Claude, GPT-4, and alternatives like GLM-4/Kimi, citing inconsistent performance despite benchmark claims.  
   - Structured output struggles (e.g., JSON, tool-calling) force workarounds like multiple API requests, seen as a "hack" compared to competitors' more reliable implementations.

2. **UI/UX Shortcomings**:  
   - Google’s AI Studio is criticized for poor usability: broken scrolling, syntax highlighting glitches, and missing basic features. Users contrast this with ChatGPT’s smoother experience despite its own flaws.  
   - Complaints about Gemini’s web interface include instability and erratic behavior during tasks like PDF generation.

3. **Versioning Confusion**:  
   - Model naming conventions (e.g., `gemini-2.5-flash-preview-09-2025`) and unclear versioning draw criticism, likened to Apple’s opaque product updates. Users find dates in IDs confusing and request clearer differentiation between versions.

### Workarounds & Alternatives  
- Some developers share solutions, like using plugins (`llm-gemini`) or adjusting prompts to mitigate truncation.  
- Alternatives like OpenAI, Anthropic’s Claude, and open-source models (e.g., GLM-4) are praised for better reliability and tooling.

### Mixed Praise  
- A few users acknowledge Gemini’s cost efficiency and potential in agentic tasks, with startups like Manus reporting ~15% performance gains.  
- The multimodal improvements (audio, vision) and translation upgrades are seen as promising for niche applications.

### Conclusion  
While Gemini’s cost and latency improvements are welcomed, persistent technical issues and poor UX dampen enthusiasm. The consensus suggests Google needs to prioritize stability, documentation, and user experience to compete with rivals effectively.

### Ollama Web Search

#### [Submission URL](https://ollama.com/blog/web-search) | 328 points | by [jmorgan](https://news.ycombinator.com/user?id=jmorgan) | [162 comments](https://news.ycombinator.com/item?id=45377641)

Ollama adds built-in Web Search API to power RAG and agents

- What’s new: Ollama now offers a native web search API with a generous free tier for individuals and higher rate limits via Ollama Cloud. The goal: help models pull fresh information, reduce hallucinations, and improve factual accuracy.
- How it works: A simple REST endpoint (/api/web_search) returns structured results (title, url, content). Deeper tool integrations are available in the official Python (ollama>=0.6.0) and JavaScript (ollama@>=0.6.0) SDKs.
- Developer ergonomics: One-liners in curl, Python, and JS make it easy to drop search into existing apps. Results come back as lightweight summaries you can pass directly into prompts or pipelines.
- Agent tooling: The SDKs expose web_search and web_fetch tools so models can autonomously search and then fetch pages for deeper reads. Example code shows a compact loop using Qwen 3 (4B) to plan, call tools, and synthesize answers.
- Long-running research: Designed to let models (including OpenAI’s gpt-oss models) conduct multi-step research workflows with programmatic tool use and result streaming.
- Why it matters: Puts a turnkey web layer into the Ollama stack, letting developers build retrieval-augmented generation, research bots, and monitoring agents without wiring up third-party search services.
- Getting started: Create an Ollama API key, then hit the REST endpoint or call client.webSearch()/ollama.web_search() from the SDKs. Cloud tier offers higher throughput if you need scale.

The Hacker News discussion on Ollama's new Web Search API highlights several key points and concerns:

1. **Legal and Licensing Concerns**:  
   - Users raised questions about the terms of service of search providers like Brave and Exa, which restrict storing, republishing, or creating derivative works from their API results. This could expose Ollama to legal risks if redistributing or caching results violates these terms.  
   - Examples include Brave’s API terms blocking competitors like Google/Bing and Exa’s strict usage policies. Some argue Ollama’s approach might breach provider agreements, risking takedowns or lawsuits.

2. **Privacy and Compliance**:  
   - Ollama’s California base subjects it to CCPA regulations, requiring compliance for handling Californian users’ data. Concerns were raised about data retention policies and whether Ollama partners with privacy-focused providers (e.g., Brave, DuckDuckGo) or relies on less transparent sources.

3. **Business Model Sustainability**:  
   - Skepticism emerged about Ollama’s long-term viability as a VC-backed, open-source-adjacent tool. Comparisons were drawn to Docker’s trajectory, with users questioning how it will monetize while competing with alternatives like **llama.cpp** and **llm-swap**, which are fully open-source and locally run.

4. **Alternative Approaches**:  
   - Some users advocated for self-hosted or local search solutions to avoid reliance on third-party APIs. Projects like YaCy, Marginalia, and custom crawlers/indexers (e.g., using Common Crawl or Postgres) were mentioned as alternatives, though challenges in quality and scalability were noted.

5. **Broader Implications**:  
   - Discussions touched on AI training ethics, with jokes about Meta’s data-scraping practices and debates over copyright compliance when using web content for LLM training.  
   - The tension between convenience (Ollama’s turnkey API) and control (self-hosted tools) underscored the trade-offs in building AI-powered applications.

In summary, while Ollama’s API simplifies RAG/agent development, the community emphasized legal risks, privacy compliance, and the sustainability of its business model, alongside enthusiasm for decentralized alternatives.

### Can a model trained on satellite data really find brambles on the ground?

#### [Submission URL](https://toao.com/blog/can-we-really-see-brambles-from-space) | 162 points | by [sadiq](https://news.ycombinator.com/user?id=sadiq) | [53 comments](https://news.ycombinator.com/item?id=45377748)

A team validating a simple bramble detector built for hedgehog habitat mapping found that high-confidence hotspots predicted from space consistently matched real, sizeable bramble patches on the ground. Using TESSERA earth representation embeddings (from Sentinel-1/2 via the geotessera library) plus iNaturalist observations, the model ensembles logistic regression with k-NN.

Highlights
- Field check: From Milton Community Centre to Milton Country Park, every high-confidence area had substantial bramble; additional hotspots on a residential plot, Fen Road (“absolute unit”), and North Cambridge’s aptly named Bramblefields.
- Strengths: Surprisingly accurate at locating large, unobscured bramble stands given the model’s simplicity.
- Limitations: Weaker on small patches under partial canopy—consistent with what satellite-derived embeddings can “see.”
- Practical notes: Map overlays were hard to use in bright sunlight; re-running the model in the park on a laptop was impractical.
- Next steps: Gather more GPS/photo validation and explore a phone-based, human-in-the-loop active learning workflow.

Takeaway: Rich remote-sensing embeddings plus lightweight classifiers can deliver useful on-the-ground ecological maps—good enough to guide boots-on-the-ground surveys, with room to improve under cover.

**Summary of Discussion:**

1. **Technical Insights & Model Methodology:**  
   - Participants highlighted the use of **TESSERA embeddings** (combining Sentinel-1/2 satellite data) and lightweight classifiers (logistic regression, k-NN) for bramble detection. Some likened the process to a “Where’s Wally” puzzle, where the model identifies subtle patterns in complex satellite data.  
   - **Hyperspectral vs. Multispectral Data:** Debate emerged about hyperspectral data’s superiority (e.g., SWIR bands for distinguishing rock types), but acknowledged its rarity compared to satellite-based multispectral data. Plane/UAV-based hyperspectral surveys are niche but valuable.  

2. **Applications & Broader Use Cases:**  
   - **Agricultural & Ecological Monitoring:** Suggestions included crop health analysis, detecting illegal farming, or invasive species like Giant Hogweed. One user proposed using temporal-spectral data to track crop growth cycles.  
   - **Practical Challenges:** Mapping in bright sunlight and computational hurdles (e.g., reprocessing data on a laptop in the field) were noted as pain points.  

3. **Limitations & Accuracy Concerns:**  
   - **Resolution Issues:** The model struggles with small bramble patches under tree canopies due to satellite data’s 10m resolution. Some questioned if predictions merely correlated with roads or human infrastructure.  
   - **Validation Needs:** Participants emphasized ground-truthing and active learning (e.g., phone-based photo validation) to improve reliability.  

4. **Tools & Next Steps:**  
   - The **TESSERA interactive notebook** ([link](https://github.com/cm-tssr-interactive-map)) was recommended for experimenting with embeddings and labeling data.  
   - Computational scalability challenges (e.g., processing global data requiring ~200 TB storage) were discussed, with calls for collaborative efforts or prioritization of regions.  

5. **Skepticism & Defense of Approach:**  
   - Some doubted the model’s novelty, arguing it might replicate simple presence/absence mapping. Authors clarified it uses **spatio-temporal embeddings** to capture ecological dynamics, not just static features.  

**Key Takeaways:**  
The discussion reflects enthusiasm for accessible remote-sensing tools but underscores the need for higher-resolution data (hyperspectral, UAVs) and rigorous validation. While the model’s simplicity and cost-effectiveness are strengths, participants urged expanding use cases (e.g., invasive species, crop fraud) and addressing technical limitations through collaboration and iterative learning.

### Gemini Robotics 1.5 brings AI agents into the physical world

#### [Submission URL](https://deepmind.google/discover/blog/gemini-robotics-15-brings-ai-agents-into-the-physical-world/) | 61 points | by [meetpateltech](https://news.ycombinator.com/user?id=meetpateltech) | [11 comments](https://news.ycombinator.com/item?id=45374474)

Google is pushing “agentic” robots a step further with two new Gemini models aimed at turning high‑level goals into real‑world actions.

What’s new
- Gemini Robotics 1.5: a vision‑language‑action (VLA) model that converts visual context and natural‑language instructions into motor commands. It “thinks before acting,” producing an internal reasoning sequence and optional natural‑language explanations, and can learn skills across different robot bodies.
- Gemini Robotics‑ER 1.5: a vision‑language model (VLM) for embodied reasoning that plans multi‑step missions, estimates progress/success, and natively calls tools (e.g., Google Search or user functions) to fetch rules or data.

How it works
- ER 1.5 acts as the high‑level brain: it builds multi‑step plans, queries tools, and issues stepwise instructions.
- Robotics 1.5 executes: it uses vision + language to perform the concrete actions, breaking long tasks into simpler chunks when needed.
- Example: to sort objects into compost/recycling/trash “based on my location,” ER 1.5 can look up local rules, then Robotics 1.5 perceives items and carries out the placements.

Performance
- Google reports state‑of‑the‑art results across 15 embodied/spatial benchmarks (e.g., ERQA, Point‑Bench, RefSpatial, RoboSpatial‑Pointing/VQA), plus capabilities like object state estimation, pointing, trajectory prediction, and task progress detection.

Availability
- Robotics‑ER 1.5: available now via the Gemini API in Google AI Studio.
- Robotics 1.5: limited release to select partners.

Why it matters (and open questions)
- If reliable, this bridges planning, perception, and action for general‑purpose robots. Real‑world robustness, hardware breadth, latency, and safety constraints will be key to watch beyond benchmark claims and curated demos.

**Summary of Discussion:**

The discussion around Google's new Gemini Robotics models highlights several key themes and critiques:

1. **Practical Applications & Technical Focus**:  
   - Users speculate on real-world robotics applications, such as drone navigation, delivery systems, and object recognition. Some emphasize the importance of integrating low-level perception algorithms with high-level planning for tasks like sorting or tracking.  

2. **Skepticism Toward Benchmarks & Hype**:  
   - Skepticism arises about Google’s benchmark claims, with one user noting that the non-ER Gemini model reportedly underperforms GPT-5 in certain puzzle tasks. Others question the marketing of polished demos versus real-world robustness, particularly in motor control and hardware adaptability.  

3. **Research Dynamics & Corporate Influence**:  
   - Debates emerge about Google’s research strategy. Some praise its historical contributions (e.g., 2017 Transformers paper) and willingness to share work despite corporate pressures, while others criticize shareholder-driven growth goals for potentially stifling pure research. A user points to a rumored link between Google’s 2025 roadmap and real-world robotics products.  

4. **Implementation Challenges**:  
   - Concerns are raised about latency, safety, and scalability in deploying these models beyond controlled benchmarks. The discussion underscores the gap between academic advancements and reliable, generalized robotic systems in unpredictable environments.  

**Overall Sentiment**:  
Mixed optimism about the technical potential of Gemini Robotics models, tempered by skepticism of corporate motives, benchmarking transparency, and real-world practicality. The conversation reflects broader tensions in AI research between innovation, profitability, and ethical implementation.

### Video models are zero-shot learners and reasoners

#### [Submission URL](https://video-zero-shot.github.io/) | 89 points | by [meetpateltech](https://news.ycombinator.com/user?id=meetpateltech) | [19 comments](https://news.ycombinator.com/item?id=45372289)

HN TL;DR: Google’s Veo 3 video model is showing LLM-like “emergent” zero-shot skills across a surprisingly wide range of visual tasks, hinting that generative video models may become general vision foundation models.

- What’s new: A paper and podcast claim Veo 3 can perform dozens of tasks it wasn’t explicitly trained for—purely via prompting—spanning perception, manipulation, physical modeling, and early visual reasoning.
- Demos include: edge detection, segmentation, super-resolution, deblurring/denoising, colorization, in/outpainting, background removal, style transfer; material and physics intuition (rigid/soft transforms, gravity, buoyancy, optics, color mixing); affordance recognition and tool-use simulations; Omniglot recognition/generation; and reasoning tasks like maze solving, BFS on graphs, analogies (rotate/reflect/resize), simple sudoku, and “water puzzle” solving.
- Big idea: The same simple recipe that drove LLMs—large generative models trained on web-scale data—may transfer to video, unifying many vision tasks without task-specific training.
- Why it matters: If validated, this could collapse today’s fragmented CV stack into a single generalist model and accelerate robotics/embodied agents that need perception + manipulation + reasoning.
- Caveats: Evidence appears demo-heavy; metrics, robustness, and contamination controls aren’t clear; Veo is closed, so independent replication and standardized benchmarks will be crucial.
- Paper: “Video models are zero-shot learners and reasoners” (arXiv TBD) + an accompanying podcast for a quick overview.

**Summary of Discussion:**

The discussion around Google's Veo 3 video model reflects mixed reactions, blending intrigue with skepticism:  

1. **Emergent Intelligence Debate**:  
   - Some users question whether the model’s capabilities (e.g., segmentation, physics intuition) represent true intelligence or are merely sophisticated pattern-matching. Critics argue that current ML approaches lack the holistic, integrative perception seen in biological systems, likening them to "post-hoc assertions" rather than inherent understanding.  
   - A user (**mllwdrm**) contends that segmentation and perception tasks in AI are fragmented and mechanistic, contrasting them with biological vision systems that evolved for survival. They reference classical theories (e.g., Marr’s vision framework) and criticize modern ML for ignoring integrative, conscious-like processing.  

2. **Technical Skepticism**:  
   - Concerns arise about whether video models truly "understand" motion or merely process individual frames (**rcrdbt**, **fskp**). Some suggest local video models (e.g., WAIN22) already generate plausible stills but lack deeper spatial reasoning.  
   - **pvlln** posits that Veo’s problem-solving might simply reframe tasks as "training + intent = action," akin to LLMs, rather than genuine reasoning.  

3. **Pseudoscience Accusations**:  
   - A heated subthread (**ACCount37**, **mllwdrm**) devolves into accusations of pseudoscience and trolling, with users dismissing claims as "schizophrenic" or "cryptic nonsense." Others mock the debate as unproductive.  

4. **Cautious Optimism**:  
   - A few users (**miguel_martin**) express awe at the demos, while others speculate that multimodal models or hidden architectures might explain Veo’s versatility.  

**Key Takeaway**: While intrigued by Veo’s potential, the community emphasizes the need for rigorous validation, standardized benchmarks, and clarity on whether its skills reflect true generalization or clever prompting. Skepticism centers on overstatements of "intelligence" and the risk of conflating generative prowess with reasoning.

### Windows ML is generally available

#### [Submission URL](https://blogs.windows.com/windowsdeveloper/2025/09/23/windows-ml-is-generally-available-empowering-developers-to-scale-local-ai-across-windows-devices/) | 23 points | by [sorenjan](https://news.ycombinator.com/user?id=sorenjan) | [3 comments](https://news.ycombinator.com/item?id=45378323)

Microsoft is making its on-device AI stack official: Windows ML, first previewed at Build 2025, is now generally available as the built‑in inferencing runtime for Windows 11. It wraps ONNX Runtime (you keep using ORT APIs) and lets the OS distribute and update the runtime plus vendor Execution Providers (EPs) for AMD, Intel, NVIDIA and Qualcomm, so apps don’t have to bundle hundreds of MB of AI deps. The pitch: faster, more private, and cheaper local inference that targets CPUs, GPUs, or NPUs with a single app build.

Highlights
- Hardware abstraction: EPs from silicon vendors plug into Windows ML; the OS detects hardware and fetches the right EPs automatically.
- Smaller apps, simpler ops: No bundling ORT/EPs; Windows handles distribution, updates, and conformance/certification to keep accuracy consistent across builds.
- Performance/power controls: Developers can set device policies to prefer NPU (low power), GPU (high performance), or explicitly pick silicon per model.
- AOT option: Precompile models ahead of time to speed up cold starts and smooth installs.
- Model workflow: Use ONNX models directly or convert from PyTorch via the AI Toolkit for VS Code; deploy across Windows 11 PCs.
- Ecosystem: AMD integrates via its Vitis AI EP; Intel’s EP leverages OpenVINO for CPU/GPU/NPU on Core Ultra. Windows ML underpins Windows AI Foundry and Foundry Local for broader silicon support.

Why it matters
- Brings Windows closer to Apple’s Core ML and Android’s NNAPI model: a unified, OS-level inference layer with vendor-optimized backends.
- Reduces fragmentation and versioning pain for app developers while widening reach across heterogeneous PC hardware.
- Positions NPUs on “Copilot+” class PCs as first-class targets for third‑party apps, not just Microsoft features.

Open questions to watch
- Version pinning and reproducibility when the OS manages ORT/EP updates, especially for offline or enterprise-locked machines.
- Depth of NVIDIA EP support and feature parity across vendors.
- How far ONNX-only workflows go for teams invested in TensorFlow/PyTorch artifacts without conversion.
- Backport/availability on older Windows 11 builds and non-NPU devices.

Getting started
- Keep your ORT code; target ONNX models.
- Use AI Toolkit for VS Code to convert/optimize PyTorch models.
- Set device policies (NPU/GPU/CPU) and consider AOT compilation for faster startup.
- Let Windows handle EP distribution to cut app size and maintenance.

The discussion highlights comparisons between Windows ML and other platforms, along with technical considerations:  
1. **Apple Parallel**: A user likens Windows ML to Apple's approach with Core ML and Apple Intelligence, emphasizing privacy-focused, on-device AI apps regardless of hardware.  
2. **Ollama Comparison**: Another user questions how Windows ML differs from using tools like Ollama for local LLMs, noting potential privacy benefits of keeping data on-device. A reply clarifies that Ollama currently lacks NPU support, a key advantage of Windows ML for Copilot+ PCs.  

The exchange underscores interest in cross-platform privacy standards and Windows ML’s hardware integration (e.g., NPUs) as differentiators.

