## AI Submissions for Mon Sep 18 2023 {{ 'date': '2023-09-18T17:10:15.039Z' }}

### Self-supervised learning: The dark matter of intelligence (2021)

#### [Submission URL](https://ai.meta.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/) | 160 points | by [reqo](https://news.ycombinator.com/user?id=reqo) | [18 comments](https://news.ycombinator.com/item?id=37558813)

The AI field has made great strides in developing AI systems that can learn from labeled data, but there is a limit to how far supervised learning can take us. Supervised learning is not sufficient for building more intelligent generalist models that can perform multiple tasks and acquire new skills without massive amounts of labeled data. To address this limitation, researchers believe that self-supervised learning (SSL) may hold the key to unlocking the "dark matter" of intelligence in AI systems.

SSL enables AI systems to learn from vast amounts of unlabeled data, allowing them to recognize and understand more subtle and less common representations of the world. SSL has already shown great success in natural language processing (NLP), with models like BERT and RoBERTa achieving higher performance than those solely trained in a supervised manner. Recent research projects, such as SEER, have demonstrated that SSL can also excel in computer vision tasks.

Self-supervised learning works by obtaining supervisory signals from the data itself, leveraging the underlying structure in the data. For example, in NLP, a system can hide part of a sentence and predict the hidden words from the remaining words. In computer vision, it can predict future frames in a video from the current ones. By using the structure of the data, self-supervised learning can make use of a variety of supervisory signals without relying on labeled data.

Self-supervised learning has had a significant impact on NLP, enabling models to be pretrained on large unlabeled text datasets and then fine-tuned for specific tasks. However, applying SSL to computer vision tasks is a relatively new frontier. Researchers are exploring energy-based models, joint embedding methods, and latent-variable architectures to further advance self-supervised learning and reasoning in AI systems.

By combining supervised learning with SSL, AI systems can develop a deeper, more nuanced understanding of the world. This can bring us closer to achieving human-level intelligence and enable AI systems to learn new skills without requiring massive amounts of labeled data for each task. Self-supervised learning holds great promise in the quest to unlock the dark matter of intelligence in AI.

The discussion on this submission revolves around various aspects of self-supervised learning (SSL) and its potential in advancing artificial intelligence (AI) systems. Some key points from the comments include:

- The success of SSL in natural language processing (NLP) is noted, with models like BERT and RoBERTa achieving high performance by leveraging large unlabeled text datasets.
- There is a mention of different techniques in SSL, such as SimCLR, BYOL, and masking-based models, and their application in NLP and computer vision tasks.
- The use of SSL in computer vision is considered a relatively new area of exploration.
- The importance of SSL in addressing the limitations of supervised learning and achieving a deeper understanding of the world is emphasized.
- LeCun's contrastive learning course materials are recommended as a resource for understanding SSL.
- There is a discussion on the concept of "dark knowledge" and how AI systems can benefit from accessing subtle and implicit information present in unlabeled data.
- The role of humans in solving arbitrary problems and the capabilities of AI systems in comparison are debated.
- A study exploring the philosophical aspects of dark matter intelligence is suggested as reading material.
- One commenter mentions feeling the presence of dark matter intelligence in the industry and its potential in resolving complex issues.

Additionally, one comment redirects readers to a related Twitter thread for more information.

### Data accidentally exposed by Microsoft AI researchers

#### [Submission URL](https://www.wiz.io/blog/38-terabytes-of-private-data-accidentally-exposed-by-microsoft-ai-researchers) | 699 points | by [deepersprout](https://news.ycombinator.com/user?id=deepersprout) | [218 comments](https://news.ycombinator.com/item?id=37556605)

In a recent mishap, Microsoft's AI research team accidentally exposed 38 terabytes of private data on GitHub. The exposed data includes a backup of two employees' workstations, containing secrets, private keys, passwords, and over 30,000 internal Microsoft Teams messages. The researchers shared their files using an Azure feature called SAS tokens, which allows for data sharing from Azure Storage accounts. However, in this case, the access level was not properly limited, resulting in the unintended exposure. This incident highlights the importance of proper management and monitoring of SAS tokens to avoid potential security risks.

The discussion on this submission covers a variety of topics related to the accidental exposure of Microsoft's private data. One user suggests that AI models should be serialized in a secure format to prevent malicious injection, while another user raises concerns about targeted attacks and the potential manipulation of training data. There is also discussion about the risks of dynamically typed languages and the importance of proper security measures. The conversation touches on topics such as log4j vulnerability, password security, encryption, and the use of programming languages. Some users advocate for stricter language typing, while others argue that programming language choice is not the main issue. There is also a brief discussion about non-encrypted PDFs and the comparison between Microsoft Office and LibreOffice. Overall, the discussion highlights the complexities and challenges of securely managing and protecting data in the context of AI research.

### GPT 3.5 vs. Llama 2 fine-tuning: A Comprehensive Comparison

#### [Submission URL](https://ragntune.com/blog/gpt3.5-vs-llama2-finetuning) | 46 points | by [samlhuillier](https://news.ycombinator.com/user?id=samlhuillier) | [12 comments](https://news.ycombinator.com/item?id=37560125)

In a recent post, the author shares their experiments comparing the fine-tuning performance of GPT 3.5 and Llama 2 in an SQL task and a functional representation task. They found that while GPT 3.5 performed slightly better on both datasets, the cost of training and deploying it was significantly higher. The author provides code and data for both tasks and explains that they wanted to explore the possibility of achieving comparable performance with manual fine-tuning at a lower cost. They used subsets of the Spider dataset and the Viggo functional representation dataset, which are known for teaching structured outputs rather than facts. The author also details the setup of their experiments, including the decision to use Code Llama 34B and Lora fine-tuning. They conclude that while fine-tuning GPT 3.5 may be suitable for initial validation or MVP work, models like Llama 2 might be more cost-effective for advanced tasks.

The discussion among Hacker News users on this post covers several topics related to the comparison between GPT 3.5 and Llama 2, as well as the considerations for cost and lifetime memberships with OpenAI. Here are the key points:

1. Some users express concerns about the cost of using Llama, particularly in comparison to GPT 3.5, suggesting that the lifetime memberships offered by OpenAI do not make sense considering the high ongoing costs of using the models.

2. Others comment on the practicality of relying on cloud computing and the theory behind it. They argue that it may not be the best approach for long-term projects, highlighting the importance of considering cost and scalability.

3. One user mentions that the terminology "functional representation dataset" is not well-defined, but they acknowledge the potential benefits of using structured propositional knowledge, citing examples like Viggo.

4. Another user expresses their struggles in finding good datasets for fine-tuning and asks for tips on creating sufficient datasets for specific use cases.

5. One user expresses interest in a similar comparison involving the RAG model and tasks related to it.

6. A user mentions that the notebook shared in the post demonstrates a reproducible evaluation process that correlates with general value and control evaluation.

Overall, the discussion revolves around the trade-offs between cost, performance, and the practicality of using different models for fine-tuning tasks. Some users express interest in alternative approaches and datasets for structured outputs and long-term projects.

### Those trying to pick AI winners should remember the dotcom days

#### [Submission URL](https://www.ft.com/content/82168156-006f-4d75-a4e9-0b6bdccef3b2) | 30 points | by [ent101](https://news.ycombinator.com/user?id=ent101) | [7 comments](https://news.ycombinator.com/item?id=37559105)

As AI continues to dominate conversations in the tech industry, it's crucial to remember the lessons learned from the dotcom era. The dotcom bubble burst in the early 2000s, leaving many startups and investors in shambles. This article highlights the importance of being cautious and realistic when evaluating AI winners, as history has shown that not every promising technology lives up to the hype.

he author emphasizes the need to exercise caution when assessing the potential winners in the AI space. Just like during the dotcom era, where everyone believed that the internet would revolutionize the world, there is now an overwhelming optimism around AI. However, it is crucial to separate the hype from reality and carefully evaluate each AI technology's actual capabilities and applications.

The article raises important questions for investors and entrepreneurs in the AI space. It reminds them to consider the scalability, practicality, and long-term sustainability of AI solutions before making any commitments. While AI holds immense potential, it's important not to get carried away by lofty promises and to remain realistic about the challenges and limitations that AI technologies face.

Overall, this article serves as a valuable reminder to analyze AI winners through a critical lens and to approach the AI landscape with the lessons learned from the dotcom bubble in mind. It encourages readers to seek a balance between optimism and caution and to make informed decisions when navigating the AI ecosystem.

### Stephen Fry says his voice stolen from Harry Potter audio books,replicated by AI

#### [Submission URL](https://fortune.com/2023/09/15/hollywood-strikes-stephen-fry-voice-copied-harry-potter-audiobooks-ai-deepfakes-sag-aftra-simon-pegg-brian-cox-matthew-mcconaughey/) | 26 points | by [thunderbong](https://news.ycombinator.com/user?id=thunderbong) | [23 comments](https://news.ycombinator.com/item?id=37554055)

Actor Stephen Fry has spoken out about the potential harm of AI in Hollywood, specifically regarding the use of AI to replicate actors' voices without their permission. Fry, who is a member of the actors' union SAG-AFTRA, mentioned his personal experience of having his identity digitally cloned and played a clip of an AI system mimicking his voice at the CogX Festival in London. He warned that AI technology is advancing rapidly and could soon produce deepfake videos that are just as convincing. Other actors, including Brian Cox and Simon Pegg, have also expressed concerns about AI in the film industry.

The discussion on Hacker News revolves around various aspects of AI replication of actors' voices and the potential implications. Some users express skepticism, comparing AI voice replication to long-standing celebrity impersonators and suggesting that legal theories might be able to cover this issue. Others discuss the technical aspects of AI voice cloning and mention Brian Blessed's distinctive voice. 

One user points out that AI recordings of coworkers in web meetings have been created, implying that the theft of voices is not a new issue. Another mentions the history of AI and its impact on various industries. 

The conversation also touches on the debate of whether AI can replace human creativity and whether AI-generated content can be considered art. One user references Walter Benjamin's 1935 philosophy and the implications of AI replication on artistic expression. 

There is a discussion about the commercial applications of AI voice cloning and how it could be used in large-scale projects. The post raises the question of whether actors' consent should be required to replace their voices in certain circumstances.

Some users argue that the focus should be on other more pressing global issues, such as climate change and humanitarian crises, rather than worrying about AI voice cloning. One user suggests that the discussion is radical and the focus should be shifted.

