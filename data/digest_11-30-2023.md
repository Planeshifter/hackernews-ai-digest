## AI Submissions for Thu Nov 30 2023 {{ 'date': '2023-11-30T17:12:31.697Z' }}

### Stanisław Lem's vision of artificial life

#### [Submission URL](https://thereader.mitpress.mit.edu/stanislaw-lems-prescient-vision-of-artificial-life/) | 441 points | by [axiomdata316](https://news.ycombinator.com/user?id=axiomdata316) | [152 comments](https://news.ycombinator.com/item?id=38475545)

Stanisław Lem's novel "The Invincible" is a prescient vision of artificial life that still resonates today. In the story, a space cruiser is sent to investigate the disappearance of a sister spaceship on the planet Regis III. What they discover is a form of life that has evolved from self-replicating machines, possibly the survivors of a robot war. The crew is faced with the quandary of what to do when faced with the unknown. Published in 1964, the book predicted the concept of artificial life before it became a scientific field. Lem explores the idea of whether evolutionary programs and devices can be considered alive or if they simply simulate life. The novel presents a hybrid view of artificial life, where automata on Regis III evolve through a struggle with indigenous life forms and among different types of automata. Lem imagines a world where solar-powered artificial organisms, driven by swarm intelligence, become the dominant force. This vision aligns with contemporary research that shows swarms of artificial beings can exhibit complex behaviors with simple rules. Lem's novel challenges our understanding of life and our place in the universe.

The discussion on Hacker News revolves around various aspects of Stanisław Lem's novel "The Invincible" and its relevance to artificial life. Some users mention other works by Lem, such as "Imaginary Magnitude" and "A Perfect Vacuum," which explore similar themes. There is a debate about the definition of artificial intelligence (AI) and whether it is currently achievable. Some users recommend reading other books by Lem, including "Solaris" and "The Cyberiad." The conversation also touches on AI-generated poems and the history of artificial life concepts in mythology and literature.

### Animate Anyone: Image-to-video synthesis for character animation

#### [Submission URL](https://humanaigc.github.io/animate-anyone/) | 311 points | by [jasondavies](https://news.ycombinator.com/user?id=jasondavies) | [141 comments](https://news.ycombinator.com/item?id=38476482)

A team of researchers from the Institute for Intelligent Computing at Alibaba Group has developed a new framework for character animation called "Animate Anyone." The framework uses diffusion models to generate character videos from still images, ensuring consistency and control over the animation. The researchers introduced a pose guider to direct the character's movements and employed a temporal modeling approach to ensure smooth transitions between video frames. By expanding the training data, the framework can animate arbitrary characters, achieving superior results compared to other image-to-video methods. The researchers evaluated the framework on benchmarks for fashion video and human dance synthesis, achieving state-of-the-art results. The paper provides detailed information on the methodology and results of the research.

The discussion on this submission covers a range of topics. 

One commenter points out that the current state of generating movement skeletons is limited and does not fully capture the nuances of realistic movement. They suggest using OpenPose, a software that is capable of generating accurate skeletons, instead.
Another commenter mentions that the framework is highly relevant to 2D animation and compares it to rotoscoping, a technique used in the past to trace movement from filmed sequences.
Another commenter brings up the work of Corridor Crew and their use of AI tools for character animation. They mention that quality animation still requires skill, and AI can assist in generating in-between frames.
A few commenters discuss the potential oversexualization of characters generated by the framework and how it can be problematic.
There are also comments regarding the limitations of the framework, such as the difficulty in animating certain characters or the lack of diverse representation in the generated animations.
There is also a discussion about the publishing of research findings on platforms like GitHub, with some commenters speculating on the reasons behind it and the accessibility of such platforms in China.

Overall, the discussion covers various aspects of the submission, including the limitations and potential implications of the framework, comparisons to existing techniques, and thoughts on the publishing of research findings.

### Interview with Viktor Lofgren from Marginalia Search

#### [Submission URL](https://nlnet.nl/news/2023/20231016-marginalia.html) | 88 points | by [luu](https://news.ycombinator.com/user?id=luu) | [18 comments](https://news.ycombinator.com/item?id=38470832)

Marginalia Search is a new search engine that aims to take users off the beaten track and introduce them to small, quality web pages that often go unnoticed by commercial search engines. In an interview with Viktor Lofgren, the creator of Marginalia Search, he explained that he was inspired to develop the search engine during the COVID-19 pandemic when he noticed that the internet seemed smaller and less diverse than it used to be. He wanted to create a search engine that resembled Google in its early days, and began building Marginalia Search as a traditional keyword search engine. What he found while crawling the web for search results were websites that were completely different from what he would find on larger search engines or social media platforms. Lofgren hopes to make the crawling data public in the future to combat censorship and offer diverse perspectives in search rankings. He also discussed the possibility of crowd-sourcing search sets, where users can contribute websites to be crawled. Lofgren believes that search engines play a critical role in helping websites and communities grow, and by offering an alternative to the dominance of search engine marketing, Marginalia Search can give smaller websites a chance to be discovered. Lofgren also mentioned the ability of Marginalia Search to penalize websites with excessive ads or tracking elements, providing a cleaner search experience for users. Overall, Marginalia Search aims to provide a fresh and diverse approach to search, offering users the opportunity to explore the less-traveled corners of the web.

The discussion on Hacker News about Marginalia Search revolves around various aspects of the search engine and its potential impact. Here are some key points that were discussed:

1. The ability of Marginalia Search to penalize websites with excessive ads or tracking elements was seen as a positive feature that would improve the search experience for users.
2. Some users highlighted the importance of search engines in helping websites and communities grow. Marginalia Search was seen as a potential alternative to search engine marketing, giving smaller websites a chance to be discovered.
3. There was a discussion about the challenges faced by search engines today, such as fighting spam, fraud, and scams. Marginalia Search's approach of crawling diverse websites and offering transparent crawling data was seen as a potential solution to these problems.
4. The concept of crowd-sourcing search sets, where users can contribute websites to be crawled, was mentioned. This feature was seen as a way to combat censorship and bring diverse perspectives to search rankings.
5. The potential use of Large Language Models (LLMs) in search engines was discussed. Some users expressed concerns about the reliability and accuracy of LLM-generated responses compared to human-generated ones.
6. The value of Marginalia Search was also highlighted as a way to discover lesser-known websites and explore the less-traveled corners of the web.

Overall, the discussion showed interest in Marginalia Search's approach to providing a fresh and diverse search experience, but also raised questions and concerns about the use of LLMs and the challenges facing search engines in general.

### Accelerating Generative AI with PyTorch II: GPT, Fast

#### [Submission URL](https://pytorch.org/blog/accelerating-generative-ai-2/) | 296 points | by [polyrand](https://news.ycombinator.com/user?id=polyrand) | [63 comments](https://news.ycombinator.com/item?id=38477197)

The PyTorch team is continuing their blog series on how to accelerate generative AI models with pure, native PyTorch. In this second part, they focus on LLM optimization, specifically for transformer inference. They demonstrate how they were able to write an LLM from scratch that is almost 10 times faster than the baseline, with no loss of accuracy, using native PyTorch optimizations. They leverage optimizations such as Torch.compile, GPU quantization, speculative decoding, and tensor parallelism. The team shares their code on GitHub for those interested in diving deeper. They also discuss reducing CPU overhead through torch.compile and a static kv-cache, overcoming challenges with the kv-cache's dynamism in text generation.

The discussion in the comments revolves around various aspects of the blog post and the topic of accelerating generative AI models using PyTorch. Some of the main points discussed are:

- The difference between Karpathy's nanoGPT GPT implementation and the one in the blog post, with the response highlighting the speed and inference performance achieved with native PyTorch optimizations.
- The support for PyTorch on Apple Silicon and the discussion on using Triton as a backend for Apple Silicon and other GPUs.
- The budget considerations for local workstations and suggestions for deals on GPUs.
- The discussion on the number of GPUs, VRAM, and technical skills required for building a multi-GPU setup.
- The energy consumption of GPUs and the possibility of choosing countries with lower energy prices for GPU-based projects.
- Various opinions on GPU testing and hardware configurations for training large models.
- Appreciation for the informative nature of the blog post and the author's other related writings.
- The discussion on matrix multiplication and the use of CuBLAS and FlashAttention for efficient computation in transformer models.
- The challenges and benefits of converting and deploying models in different formats.
- Requests for benchmark comparisons between PyTorch compile and Llvmacpp.
- The mention of LLamacpp and its potential speed benchmarks compared to PyTorch compile.
- The discussion on batching and persistent inference in serving frameworks and the emphasis on PyTorch's focus on latency and batch size 1.

Overall, the comments show a range of interests and perspectives on the blog post, with discussions covering technical details, budget considerations, hardware configurations, and deployment strategies for AI models.

### Large language models lack deep insights or a theory of mind

#### [Submission URL](https://arxiv.org/abs/2311.16093) | 267 points | by [mnode](https://news.ycombinator.com/user?id=mnode) | [247 comments](https://news.ycombinator.com/item?id=38474696)

In a recent paper titled "Have we built machines that think like people?", authors Luca M. Schulze Buschoff and colleagues evaluate the current state of vision-based large language models in terms of emulating human-like cognitive abilities. While these models demonstrate proficiency in processing and interpreting visual data, they still fall short of human capabilities in areas such as intuitive physics, causal reasoning, and intuitive psychology. The authors emphasize the need to integrate more robust mechanisms for understanding causality, physical dynamics, and social cognition into modern-day, vision-based language models. They also highlight the importance of cognitively-inspired benchmarks.

The discussion about the submission revolves around various aspects of language models and their ability to think like humans. Here are some key points made by different commenters:

- Some commenters argue that current large language models (LLMs) are limited in their ability to think like humans, as they often rely on pattern matching and lack deep insight or reasoning capabilities.
- Others suggest that human-like reflexive responses to questions are not necessarily indicative of human-level thinking, as humans have internal reasoning processes that LLMs cannot replicate.
- Some commenters emphasize the importance of implementing recursive execution and internal dialogue in LLMs to enhance their thinking abilities.
- There is a discussion about the role of memory and external interaction in developing artificial general intelligence (AGI). Commenters believe that AGI requires interaction with the external environment to learn and improve.
- The concept of "inner monologue" is mentioned, with some commenters warning that it can lead to wasteful and unproductive discussions.
- The topic of Asimov's Three Laws of Robotics is brought up, with commenters noting that these laws are not necessarily applicable to current AI systems.
- There is speculation about the extent to which LLMs possess theory of mind and whether they can truly understand human intentions or behavior.
- The potential benefits of providing more explicit instructions and prompt-guided training to LLMs are discussed.
- Some commenters point out that human thinking involves understanding functional meanings and behavioral differences, which current LLMs have not fully achieved.
- The idea of incorporating longer-term memory and context judgment into LLMs is mentioned as a way to improve their thinking capabilities.

Overall, the discussion highlights the limitations of current language models in simulating human-like cognitive abilities and explores potential directions for their improvement.

### Microsoft joins OpenAI's board with Sam Altman officially back as CEO

#### [Submission URL](https://www.theverge.com/2023/11/29/23981848/sam-altman-back-open-ai-ceo-microsoft-board) | 54 points | by [croes](https://news.ycombinator.com/user?id=croes) | [9 comments](https://news.ycombinator.com/item?id=38471728)

Microsoft is joining OpenAI's board as a non-voting observer, while Sam Altman returns as the CEO. Previously, Altman was ousted by the board but has now reached a deal to come back. With Microsoft as a major investor in OpenAI, this move gives the tech giant more insight into the company's operations without having an official vote. Altman expressed his excitement about the future and gratitude for everyone's hard work during the uncertain situation. OpenAI's new board now consists of chair Bret Taylor, Larry Summers, and Adam D'Angelo—three of the four members who fired Altman initially. Altman also spoke positively of Ilya Sutskever, co-founder and chief scientist, despite his initial participation in the board coup. Altman hopes to continue working with Sutskever in some capacity. Altman's return and Microsoft's involvement aim to strengthen OpenAI's mission and partnerships.

The discussion surrounding the submission on Hacker News covers a range of topics. 

One user points out that they wouldn't be surprised if OpenAI dropped Microsoft in a few years. Another user responds by saying that artificial intelligence (AI) is being treated as a mere business opportunity, rather than a technology with potential risks.
Another user clarifies that the non-voting observer role on the board is a common position where the observer receives detailed information about the board's decisions, methods, and approaches. They mention that Microsoft's involvement will provide valuable insights for the company.
In response to this, someone else mentions that the board's collective experience in various disciplines, including financial capital, influences decision-making. While Microsoft's vote is a significant addition, the power of board members lies in the exchange of information and spoken words during discussions.
A user comments that the discussion is becoming too focused on existential risks and sarcastically refers to the situation as a happy family. They also mention that Microsoft will bring a gentle level of oversight and accountability mechanisms.
One user brings up Larry Summers, who is on OpenAI's board, stating that the discussion shouldn't overlook his involvement in the decision-making process.
Another user simply states that Microsoft's AI division joining OpenAI's board is not surprising.

Lastly, two comments were flagged by users, but the content is not visible.

### Stable Diffusion XL Turbo can generate AI images as fast as you can type

#### [Submission URL](https://arstechnica.com/information-technology/2023/11/stable-diffusion-turbo-xl-accelerates-image-synthesis-with-one-step-generation/) | 42 points | by [Brajeshwar](https://news.ycombinator.com/user?id=Brajeshwar) | [3 comments](https://news.ycombinator.com/item?id=38473933)

Stability AI, a company specializing in AI-powered image synthesis, has launched a new model called Stable Diffusion XL Turbo. This model is capable of rapidly generating images based on written prompts, and it can even transform images from a source, such as a webcam, in real-time. The primary innovation of Stable Diffusion XL Turbo lies in its ability to produce image outputs in a single step, a significant improvement from its predecessor. This efficiency is achieved through a technique called Adversarial Diffusion Distillation (ADD), which utilizes score distillation and adversarial loss to improve the realism of the generated images. While Stable Diffusion XL Turbo is not as detailed as previous models, its speed savings are impressive. For example, it can generate a 3-step 1024x1024 image in about 4 seconds, compared to 26.4 seconds for a 20-step image with similar detail. Stability AI claims that the model can generate a 512x512 image in just 207 milliseconds on a powerful AI-tuned GPU, which could have applications in real-time generative AI video filters or video game graphics generation. Currently, Stable Diffusion XL Turbo is only available for non-commercial research purposes, but Stability AI is open to exploring commercial applications.

The discussion on Hacker News for the submission about Stability AI's new image synthesis model, Stable Diffusion XL Turbo, seems to be focused on the fact that this submission is a duplicate of a previous one. The duplicate submission had received a significant number of comments in just one day, but it appears that those comments have not been replicated in this duplicate submission. One comment suggests that the previous submission had received a large number of upvotes as well.

### Amazon's Trainium2 AI Accelerator Features 96 GB of HBM, 4x Training Performance

#### [Submission URL](https://www.anandtech.com/show/21173/amazons-trainium2-features-96-gb-hbm-quadruples-training-performance) | 43 points | by [ksec](https://news.ycombinator.com/user?id=ksec) | [29 comments](https://news.ycombinator.com/item?id=38475635)

Amazon has announced Trainium2, its new AI accelerator, which offers four times higher training performance compared to its predecessor. The Trainium2 accelerator is designed specifically for training foundation models and large language models, with up to trillions of parameters. It features 96GB of HBM memory, which is three times the amount of the original Trainium, and is built using a multi-tile system-in-package design. Although specific performance numbers have not been disclosed, Amazon claims that its Trainium2 instances can scale out to deliver up to 65 ExaFLOPS of low-precision compute performance for AI workloads.

The discussion on Hacker News revolves around various aspects of Amazon's announcement of Trainium2, its new AI accelerator. Here are the key points discussed:

- One user mentions that AMD is releasing the MI300x on December 6th, which has 192GB of HBM3 memory, fast connections, and 52TBs memory bandwidth. However, another user expresses that they are not aware of any trending discussions around the MI300x from AMD.
- There is a discussion regarding the performance numbers of the Trainium2 accelerator. One user mentions that they find the reported numbers impressive, but some practical issues with model completion and finicky behavior should be addressed. Another user notes that AMD has caught on to the AI race, but it remains to be seen how it compares to Nvidia GPUs.
- The relationship between the MI300x and MI300 is discussed, with a user pointing out that the MI300x claims to have 22 PFlops FP8 structured sparsity, which AMD is implementing.
- A user comments that there will be a large number of AI chips available in the market in the future.
- In response to the announcement, one user shares their experience with AWS Nvidia machines, mentioning that the cost of installing dependencies can be expensive. They express interest in Trainium as a faster and cheaper alternative.
- The issue of dependency installation on AWS instances is discussed, with some users sharing their frustrations about being locked into specific instances and GPUs not being ready for use.
- The cost of AWS instances is also mentioned, with one user highlighting that the cost of installation can be a small fraction compared to the training cost.
- The potential impact of Amazon's AI chips on the AI space is discussed, with one user suggesting that it could lead to a rewrite of the entire stack and lock users into Amazon for their workloads.
- There is a discussion about the compatibility and usefulness of the Trainium2 accelerator, with users mentioning the importance of compatibility with existing models and frameworks.
- The performance of Trainium2 is compared to other AI chips in terms of operations per second and precision, with some users suggesting that it is surprisingly low and narrow in terms of precision.
- The topic transitions to the development of accelerators and the need for developers and frameworks to move away from proprietary technologies like CUDA and embrace standard APIs.
- There is casual speculation about the fate of the hardware that Amazon sells or retires.
- A user suggests that the hardware market could be destroyed if AWS starts selling or destroying second-hand hardware.
- The discussion ends with a brief mention of software support for Trainium2 and disappointment with standard backends for transformers and middlewares.

Overall, the discussion touches on various aspects related to the Trainium2 announcement, including comparisons to other AI accelerators, cost considerations, compatibility, and the potential impact on the AI ecosystem.

### Let's Not Flip Sides on IP Maximalism Because of AI

#### [Submission URL](https://www.techdirt.com/2023/11/29/lets-not-flip-sides-on-ip-maximalism-because-of-ai/) | 95 points | by [rntn](https://news.ycombinator.com/user?id=rntn) | [108 comments](https://news.ycombinator.com/item?id=38472367)

In a recent article on Techdirt, author Matthew Lane discusses the importance of fair use in copyright law and its implications for AI companies. Fair use allows limited use of copyrighted material without permission, primarily for purposes such as commentary, criticism, and parody. Lane highlights how fair use has filled an important gap in social media and art, allowing us to retweet or link content without fear of copyright infringement. Moreover, many creators who make a living from streaming video games or creating "react" content rely on fair use protection. However, Lane expresses concern over public interest advocates who are willing to sacrifice fair use in order to regulate AI companies. He argues that using copyright in this way would be both unnecessary and detrimental, as it would hinder the ability of AI to analyze content and potentially lead to the exploitation of artists. Lane suggests that addressing the issues surrounding AI, such as worker exploitation, requires thoughtful policy-making rather than using copyright as a blunt instrument. He draws parallels to the fights against "on a computer" software patents, which caused problems in the past and still persist today. Lane concludes by emphasizing the need to preserve fair use and prevent its erosion in the face of new technological advancements.

Discussion:

- User "chlmrs" acknowledges that there are concerns about AI companies pushing for shorter copyright terms, but questions the need for AI models to have access to copyrighted material. They argue that AI models should follow rules that are established and applied fairly.
- User "Arainach" expresses skepticism about the existence of IP laws and believes that copyright terms should be longer. They also discuss the limited duration of patents and distinguish between the practical value of art and inventions.
- User "MightyBuzzard" argues that the purpose of copyright laws is to protect expressions of ideas and not to control access to creative content. They emphasize the importance of protecting artists and allowing them to profit from their work.
- User "wffltwr" raises concerns about the impact of AI on copyright laws and suggests that AI interfaces that improve the capabilities of human thought may challenge current copyright laws.
- User "gdy" agrees with the concerns expressed by "MightyBuzzard" and believes that AI should not have the ability to tighten copyright laws. They reference the Blurred Lines lawsuit as an example of how copyright claims can become subjective.
- User "ls612" suggests searching for Supreme Court cases related to Google and small excerpts of books in search results.
- User "dnrs" agrees with the sentiment that IP laws have been flipped to favor larger companies and that AI projects by big companies are potentially infringing on the works of smaller creators.
- User "MightyBuzzard" responds by stating that AI replicating uninspired creations is not a valid argument, as it assumes that AI scientists have replicated the human mind, which they argue is not the case.
- User "wrd" agrees with the concerns raised by "MightyBuzzard" and emphasizes the need to consider the perspective of regular people who are creating and sharing content.
- User "dnrs" argues that regular people creating and sharing content are often not adequately compensated, while the wealthy companies that control the copyright laws benefit greatly.
- User "wrd" points out that allowing AI natural access to licensed works while restricting human artists could create a problematic double standard.
- User "wffltwr" warns against accepting radical changes to copyright laws and suggests that AI and digital laws may cause unintended consequences.

Overall, the discussion revolves around the potential implications of copyright laws on AI companies, artists, and content creators. There are concerns about fair compensation for artists and the balance between protecting copyright and enabling innovation in AI technology.

