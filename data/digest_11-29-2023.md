## AI Submissions for Wed Nov 29 2023 {{ 'date': '2023-11-29T17:10:08.086Z' }}

### How to tackle unreliability of coding assistants

#### [Submission URL](https://martinfowler.com/articles/exploring-gen-ai.html#memo-08) | 152 points | by [ingve](https://news.ycombinator.com/user?id=ingve) | [144 comments](https://news.ycombinator.com/item?id=38456726)

Birgitta BÃ¶ckeler, a software developer working at Thoughtworks, has been delving into the world of generative AI, particularly Large Language Models (LLMs). In a series of memos, BÃ¶ckeler explores the toolchain of LLMs that support coding tasks. She categorizes the tools based on the type of assistance they provide, such as finding information, generating code, reasoning about code, and transforming code. BÃ¶ckeler also discusses the different interaction modes, prompt composition, properties of the model (such as what it was trained with and its size), and the origin and hosting of the tools. She provides examples of popular tools in the space, such as GitHub Copilot, ChatGPT, and Meta's CodeCompose. BÃ¶ckeler notes that the most common usage today involves chat interfaces combined with coding assistance in the code editor, and that in-line assistance is the most mature and effective approach for coding assistance. She also mentions ongoing experimentation with prompt composition tools and the future potential of larger models and more specialized training for coding assistance.

The discussion on this submission covers a few different topics. Some users point out the humorous side of LLMs and discuss their limitations, while others discuss the potential risks and challenges of developing AGI (Artificial General Intelligence). There is also a discussion about the reliability and practicality of LLMs, with some users expressing concerns about their ability to generate correct and understandable code. Some users also discuss the training and capabilities of LLMs, questioning whether they can understand programming languages and suggesting alternative approaches for program synthesis. Overall, the discussion covers a range of perspectives on the topic of generative AI and its potential applications in coding assistance.

### Extracting training data from ChatGPT

#### [Submission URL](https://not-just-memorization.github.io/extracting-training-data-from-chatgpt.html) | 238 points | by [Deeg9rie9usi](https://news.ycombinator.com/user?id=Deeg9rie9usi) | [121 comments](https://news.ycombinator.com/item?id=38458683)

A recent paper reveals a concerning vulnerability in OpenAI's language model, ChatGPT. Researchers discovered that by querying the model, they could extract portions of the dataset it was trained on, including sensitive information like email addresses and phone numbers. Unlike previous data extraction attacks, this one targets a production model, emphasizing the importance of testing base models and patching vulnerabilities. The attack, which prompts the model with a specific command, allows for the extraction of several megabytes of training data for a minimal cost. The implications extend beyond ChatGPT, raising concerns about the potential leakage of sensitive training data in other language models.

The discussion on the submission revolves around various aspects of the vulnerability in OpenAI's ChatGPT model. Some users express their surprise and interest in the finding, while others provide additional insights and comments.

One user shares a link to a thread on Reddit where the attack approach was posted several months ago. Another user mentions that it's important to conduct research and test base models for vulnerabilities before deploying them in production.

There are discussions about the shortcomings of the current peer-review journal system, with some users expressing their preference for open access and reproducible papers. The topic also shifts to the behavior of GPT models and the need to explain their actions, as well as the challenges faced by reviewers in understanding and detecting vulnerabilities.

One user provides a detailed explanation of how the attack works and suggests that OpenAI should have been more proactive in patching the vulnerability. Another user mentions that the attack works by downloading random internet data, making it difficult to prevent entirely.

A user points out the similarities between Bard, a Google model, and ChatGPT, raising questions about potential vulnerabilities in other language models. There are discussions about the difficulty of mitigating the vulnerability and the limitations of current programming.

Some users argue that the findings are not surprising and that similar attacks on other models have been attempted in the past. A user clarifies that the attack involves extracting specific portions of the training dataset and provides examples of personal information that could be extracted.

A user highlights the need to patch the vulnerability and fix the underlying issue. They advise against changing prompts randomly and suggest taking a more strategic approach.

Overall, the discussion includes different perspectives on the vulnerability in ChatGPT and its implications, with users offering insights, explanations, and opinions on the matter.

### Show HN: Hacky Meta Glasses GPT4 Vision Integration

#### [Submission URL](https://github.com/dcrebbin/meta-vision-api) | 220 points | by [devon_c](https://news.ycombinator.com/user?id=devon_c) | [88 comments](https://news.ycombinator.com/item?id=38457815)

ðŸ”¥ Hot News from Hacker News: "Hacky Meta Glasses GPT4 Vision Integration"

Devon Crebbin has developed an interesting hack that allows users to integrate GPT4 Vision into Meta Rayban Smart Glasses using voice commands. The implementation involves using Messenger Chat Observer to forward image URLs to the project's REST API, which then sends the image URLs to GPT4 Vision for processing. The integration allows users to send photos to their alternative Facebook/Messenger account using voice commands. The project requires Meta Rayban Smart Glasses, an OpenAI API key, and an alternative Facebook/Messenger account. The code for the integration is available on GitHub, along with a detailed setup guide. Devon encourages users to reach out for any issues or feature requests. Check out this exciting integration and stay tuned for updates! ðŸ‘“ðŸ”®

Read more: [GitHub - dcrebbin/meta-vision-api](https://github.com/dcrebbin/meta-vision-api)

The discussion surrounding the submission revolves around various aspects of the Meta Rayban Smart Glasses integration with GPT4 Vision. Some users express their interest in the project and appreciate the hack, while others raise concerns about privacy and camera access. There are also discussions about the quality of the recorded videos using the glasses and comparisons to other similar projects. Some users suggest integrating ChatGPT with other messaging platforms like WhatsApp, while others discuss the latency involved in using the smart glasses. The potential of future features and the need for direct integration of vision capabilities into the glasses themselves are also mentioned. Overall, the discussion includes a mix of positive feedback, technical considerations, and privacy concerns related to the project and smart glasses in general.

### What should I do if I suspect one of the journal reviews I got is AI-generated?

#### [Submission URL](https://academia.stackexchange.com/questions/204370/what-should-i-do-if-i-suspect-one-of-the-journal-reviews-i-got-is-al-generated) | 137 points | by [j2kun](https://news.ycombinator.com/user?id=j2kun) | [59 comments](https://news.ycombinator.com/item?id=38462269)

A recent post on Academia Stack Exchange raises an interesting question about the use of AI-generated journal reviews. The user explains that they suspect one of the reviews they received for their paper was generated by an AI, based on the style and content of the review. The review consists only of long questions that rephrase each line of the abstract, with no suggestions or feedback provided. Additionally, the list of suggested articles includes irrelevant papers from unrelated fields. The user has even run the text through AI-detection tools, which have consistently identified it as AI-generated. 

The user asks whether they should mention their suspicions to the journal editor, even though they can't prove the use of AI. They express concern about the ethical implications of using AI to generate reviews in academic publishing. They also worry about the potential consequences for their own article if they speak up. 

In response to the question, several answers suggest that the user should indeed contact the editor and explain their suspicions. They advise the user to outline their evidence and express their concerns about the integrity of the peer review process and the protection of their intellectual property. It's also suggested to check the journal's website for any explicit statements about the use of AI in peer review. Ultimately, the decision of how to proceed lies with the editor, and the user should be prepared to revise and resubmit their paper regardless of the outcome. 

This question brings to light an important discussion about the increasing use of AI in academia and the potential impact on the peer review process. It highlights the need for clear guidelines and policies to address this issue and ensure the integrity of academic publishing.

The discussion revolves around the suspicion of AI-generated journal reviews and the implications for the peer review process in academia. Some commenters suggest contacting the journal editor and expressing concerns about the integrity of the review process and the protection of intellectual property. Others argue that AI can be helpful in filtering out irrelevant submissions and improving the efficiency of the review process. The debate also touches on issues of trust and reliability in both human and AI-generated reviews. Some commenters express skepticism about AI's ability to replace human reviewers, while others highlight the potential benefits of AI in speeding up the review process and optimizing quantity and quality. Overall, there is a call for clear guidelines and policies to address the increasing use of AI in academic publishing.

### Stable Diffusion:Real time prompting with SDXL Turbo and ComfyUI running locally

#### [Submission URL](https://old.reddit.com/r/StableDiffusion/comments/1869cnk/real_time_prompting_with_sdxl_turbo_and_comfyui/) | 116 points | by [belltaco](https://news.ycombinator.com/user?id=belltaco) | [42 comments](https://news.ycombinator.com/item?id=38454349)

Title: Real-Time Prompting with SDXL Turbo and ComfyUI: Approaching the Singularity?

Yesterday, a mind-blowing demonstration was posted on Stable Diffusion, showcasing a workflow that allows for real-time prompting with SDXL Turbo and ComfyUI. The video, which is not sped up, shows the workflow running smoothly on a powerful 3090 TI computer. 

The technology behind this workflow represents a major milestone in the development of AI capabilities. It hints at the possibility of approaching the singularity, where AI systems reach and potentially exceed human-level intelligence. 

One commenter compared the experience to what the singularity might feel like. Others expressed astonishment at the rapid progress being made in AI. One user shared their prediction that this acceleration could indicate that we are at the start of the singularity, with 2024 being a potentially wild year of innovation. 

Another user imagined a future where scripts could be easily transformed into new movies or TV shows. They suggested that by simply inputting a script into a prompt window and typing a desired parody theme, an entirely new production could be generated within a day. 

Overall, this stunning demonstration has left many in awe of the possibilities that AI technology holds for the future. As developments continue to accelerate, it remains to be seen just how close we are to the singularity and what incredible creations lie ahead.

The discussion around the submission primarily focuses on the impressive speed and capabilities of SDXL Turbo and ComfyUI in real-time prompting. Some users express astonishment at the advancements in AI technology, with one person suggesting that we may be approaching the singularity. Others discuss the practical applications of this technology, such as easily transforming scripts into new movies or TV shows. The conversation also delves into technical details, including optimizations with different graphics cards and the compatibility of SDXL Turbo with various models. Some users mention the challenges of working with CPU models and the potential for further optimization with SDXL Turbo and OpenVino. The discussion also touches on the limitations and potential pitfalls of rapid AI generation, including the risk of generating kitsch or low-quality content.

### Show HN: Free image generation using Cloudflare Workers AI

#### [Submission URL](https://art.aiphotoshoot.com/) | 23 points | by [jackculpan](https://news.ycombinator.com/user?id=jackculpan) | [4 comments](https://news.ycombinator.com/item?id=38459481)

The AI-generated art has no boundaries! It can explore a variety of styles and techniques, from abstract to realistic, surrealistic to cubism, pop art to impressionism, expressionism to futurism, and even delve into the realms of advertisement, comics, and cartoons. Whatever sparks creativity, the AI can create it. But don't forget to also check out the amazing work already made by talented artists like @jackculpan, who have pushed the boundaries of AI-generated art and brought fresh perspectives to the digital canvas.

The discussion mainly revolves around the impressive capabilities of AI-generated art and the potential for further development. @jckclpn expresses admiration for the AI's ability to explore various artistic styles and techniques. They also mention requesting an increase in the limits of AI-generated art. Another user, @sh-shh, finds it interesting that Cloudflare charges for AI services, specifically highlighting rapid-thinking processes. @jckclpn shows interest in this aspect of AI as well. Overall, the sentiment in the discussion is positive, with an appreciation for the remarkable work done by both AI and talented artists.

### OpenAI's board needs to say something

#### [Submission URL](https://www.theverge.com/2023/11/29/23981516/openai-board-silence-sam-altman) | 34 points | by [goplayoutside](https://news.ycombinator.com/user?id=goplayoutside) | [14 comments](https://news.ycombinator.com/item?id=38465560)

OpenAI's board has been noticeably silent following the failed attempt to oust Sam Altman, leaving many to wonder what their next move will be. The board, which recently lost directors Reid Hoffman and Shivon Zilis, is now tasked with rebuilding and conducting an internal investigation into Altman's firing. Adam D'Angelo, CEO of Quora and a board member of OpenAI, has so far been the only member to survive the power struggle. It remains to be seen how the board will navigate this difficult situation and restore stability to the organization. In other news, Meta's morale is on the rebound, and there's a new AI startup making waves in the industry.

The discussion surrounding the submission revolves around various topics. There is a debate about the relevance of the recent global events, such as Ukraine, Israel-Palestine, and OpenAI's current situation. Some users argue that these topics are unrelated while others believe they are important for staying informed. There is also a discussion about experts and their involvement in board politics and governance. Some users express frustration with the lack of transparency from OpenAI's board and their interest in maintaining public messaging. Others argue that the danger lies in the company losing financial value and compare Altman's departure to a typical CEO switch. The discussion also touches on the importance of voting and the potential risks of former board members predicting sufficient attention as the biggest danger. Lastly, there is a comment mentioning the East Coast Establishment, but it lacks further context.

### Mother plucker: Steel fingers guided by AI pluck weeds rapidly and autonomously

#### [Submission URL](https://arstechnica.com/information-technology/2023/11/mother-plucker-steel-fingers-guided-by-ai-pluck-weeds-rapidly-and-autonomously/) | 24 points | by [ashitlerferad](https://news.ycombinator.com/user?id=ashitlerferad) | [5 comments](https://news.ycombinator.com/item?id=38462113)

Swedish company Ekobot AB has developed an autonomous robot that can rapidly identify and remove weeds from farmland. The Ekobot WEAI robot is battery-powered, weighs 600 kg, and can operate for 10-12 hours on a single charge. Equipped with a machine vision system powered by artificial intelligence, the robot can recognize and pluck weeds as it moves over the field. In trials, the robot allowed farmers to grow onions with 70% fewer herbicides. Ekobot has also integrated 5G mobile technology into the robot, enabling it to communicate remotely with a central server. The company has now released "5G onions" grown using this weeding method, which have an extended shelf life and improved taste. The Ekobot system is set to become available in several European countries, as well as the US and the UK, by 2030.

The discussion on the Hacker News submission revolves around the use of the Ekobot WEAI robot and its integration of 5G technology. 

One user, "rngn," compares the robot's movement to that of chickens picking, indicating that it seems to follow a simple copying motion rather than using advanced lasers. 

Another user, "the_optimist," highlights the importance of 5G technology in the robot's operation. 

A sub-thread between users "lbg" and "vntrmnn" focuses on the collaboration between Ekobot and Swedish telecommunications company Telia. They discuss how Telia's integration of 5G mobile technology allows the robot to communicate remotely with a central server and collect learning data from the field. 

User "Sabinus" comments on the article, expressing skepticism about the accuracy of collecting weed vision data. 

Overall, the discussion primarily centers around the functionality and potential of the Ekobot WEAI robot, as well as the role of 5G technology in its operation.

### Together AI raises a $102.5M Series A

#### [Submission URL](https://www.together.ai/blog/series-a) | 67 points | by [bratao](https://news.ycombinator.com/user?id=bratao) | [23 comments](https://news.ycombinator.com/item?id=38463034)

Together AI, a company focusing on open and custom AI models, has raised $102.5 million in a Series A financing round. Led by Kleiner Perkins, the round also included participation from investors such as NVIDIA and Emergence Capital. Together AI plans to use the new capital to accelerate the development of its cloud platform, with the aim of creating the fastest cloud platform for generative AI applications. The platform allows developers to integrate leading open source models or create their own models through pre-training or fine-tuning. The company believes that generative AI is a platform technology that will have a long-term impact on society, and aims to provide researchers and developers with the tools to shape the AI future.

The discussion on the submission about Together AI's $102.5 million funding round covers various topics related to the AI industry and the use of AI models:

1. Some users mention the challenges in training AI models compared to inference. They note that inference has a large market and is dominated by cloud providers, while training requires specialized knowledge and optimization. They mention Google Cloud Platform (GCP) and Amazon Web Services (AWS) as dominant players in the inference space.

2. Another user suggests that decentralized skills and specialized distributed training frameworks are necessary for competing with big cloud players. They mention CoreWeave as an example of a GPU cloud provider that specializes in distributed training frameworks.

3. The discussion also touches on the skepticism around long-term business viability in the machine learning field. One user shares their experience, stating that machine learning projects require significant effort and expertise in modeling and data quality.

4. The topic of NVIDIA's investment in Together AI is brought up, with a user questioning the return on investment from a hardware perspective. Others comment on the accounting rules and holding structures when it comes to joint ventures.

5. The discussion briefly shifts to Microsoft Azure, with one user mentioning Microsoft's high margin on Azure and another user expressing disbelief in such high margins.

6. Pricing of Together AI's models is discussed, with one user pointing out the relatively low cost and another mentioning the GPT-4 model and its potential price range. The scalability of prices based on the number of tokens is also mentioned.

7. A few users share their personal experience with inference service platforms, mentioning factors like clear and simple user interfaces, pricing, and speed.

8. The discussion ends with a brief mention of venture capital money in the FinTech industry.

Overall, the discussion covers topics such as the challenges of AI training, the dominance of cloud providers in inference, skepticism about long-term business viability, the impact of NVIDIA's investment, Azure's margins, pricing of AI models, and user experiences with inference service platforms.

