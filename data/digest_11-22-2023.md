## AI Submissions for Wed Nov 22 2023 {{ 'date': '2023-11-22T17:12:18.525Z' }}

### Vtracer: Next-Gen Raster-to-Vector Conversion

#### [Submission URL](https://github.com/visioncortex/vtracer) | 79 points | by [s1291](https://news.ycombinator.com/user?id=s1291) | [8 comments](https://news.ycombinator.com/item?id=38377307)

VTracer is an open-source software developed by the Vision Cortex Research Group that allows users to convert raster images (like jpg & png) into vector graphics (svg). Unlike other similar tools, VTracer can handle colored high-resolution scans, making it ideal for processing historic blueprints or pixel art. 

The software is built with Rust and provides a solid foundation for developing robust and efficient algorithms. VTracer offers both a web app and a command-line app, giving users flexibility in how they want to use the software. 

The web app, developed using Rust and wasm, showcases the capabilities of the Rust + wasm platform. Meanwhile, the command-line app allows users to convert images into vector graphics using various options and parameters. 

VTracer's output is compact and efficient, thanks to its stacking strategy that avoids producing shapes with holes. It outperforms other tools like Potrace and Adobe Illustrator's Image Trace in terms of efficiency and output quality. 

For installation, users can download pre-built binaries or install the program from source using crates.io/vtracer for Rust, or by using pip install vtracer for Python. 

VTracer has gained popularity and is being used in various projects including smart logo design. It continues to be developed by the Vision Cortex Research Group, and future updates and improvements are on the horizon. 

To learn more about VTracer and its capabilities, you can visit their website at www.visioncortex.org/vtracer.

The discussion surrounding the submission on VTracer revolved around various topics related to vector graphics and image tracing. Here are some key points:

- One user mentioned a simplified Bezier path method called "Kurbo" that results in compact and efficient output. They shared a link to a website that demonstrates this method.
- Another user speculated that Facebook's Segment could be a replacement for clustering in image tracing.
- There was a discussion about comparing VTracer with other tools like Potrace and Adobe Illustrator's Image Trace. It was mentioned that VTracer outperforms Potrace in terms of efficiency and can handle colored high-resolution scans.
- The topic of 3D digitization and the potential help that VTracer's approach could provide in this area was brought up.
- Some users mentioned other software tools like Affinity Designer and Adobe Illustrator that have tracing features.
- Overall, there was positive feedback on VTracer's capabilities and praise for the work done by the Vision Cortex Research Group.

Please note that the information provided is a summary and may not represent the entire discussion on the Hacker News thread.

### The Three Projections of Doctor Futamura (2009)

#### [Submission URL](http://blog.sigfpe.com/2009/05/three-projections-of-doctor-futamura.html) | 22 points | by [signa11](https://news.ycombinator.com/user?id=signa11) | [5 comments](https://news.ycombinator.com/item?id=38375786)

In this article titled "The Three Projections of Doctor Futamura," the author explores the concept of partial evaluation or specialization in programming. They use the analogy of machines to explain the ideas in a more accessible way. 

They start by describing a simple machine that takes blanks as input and outputs newly minted coins. They then introduce the idea of an interpreter, which is a more flexible machine that can produce different types of coins based on input. However, the interpreter is slower than a dedicated minting machine because it has to custom mill each coin individually.

To combine the benefits of both machines, the author suggests using a compiler. The compiler takes a set of instructions and creates a dedicated machine to perform them. This way, the compiler can execute instructions faster, similar to a dedicated minting machine.

The article then introduces the concept of specialization. If a machine consistently receives the same input in one slot, the machine can be redesigned to be more efficient based on that knowledge. This process is called specialisation or partial evaluation. 

The author imagines a machine for automatically customizing designs for machines based on the assumption of consistent input. This machine, called a specialisation machine, takes a description of a two-input machine and outputs a description of a customized one-input machine.

Overall, the article provides an insight into the fascinating world of partial evaluation in programming, using the analogy of machines to make the concepts more relatable.

The discussion on this submission includes comments discussing the practical implications of Truffle and Graal, the relevance of the article, and a comment correcting the title mistakenly referring to "Futurama" instead of "Futamura."

One user mentions that Truffle works by taking an interpreter and generating a compiler with partial evaluation capabilities, which they find impressive. Another user expresses their positive sentiment towards the article by simply stating "Good news."

A sub-thread within the discussion reveals a comment expressing relief at double-checking the title, as it initially seemed to be referring to Futurama. This comment is further discussed with one user noting that they initially misread the title as well, leading another user to flag the comment as true.

### OpenAI researchers warned board of AI breakthrough ahead of CEO ouster

#### [Submission URL](https://www.reuters.com/technology/sam-altmans-ouster-openai-was-precipitated-by-letter-board-about-ai-breakthrough-2023-11-22/) | 910 points | by [mfiguiere](https://news.ycombinator.com/user?id=mfiguiere) | [1037 comments](https://news.ycombinator.com/item?id=38386487)

OpenAI, the artificial intelligence research institution, experienced a significant event prior to the board ousting CEO Sam Altman, according to two sources familiar with the matter. Staff researchers wrote a letter to the board of directors, warning of a powerful AI discovery that could potentially pose a threat to humanity. The letter, along with other board concerns about the commercialization of AI advances, contributed to Altman's firing. The researchers had been working on a project called Q* (pronounced Q-Star), which some believe could be a breakthrough in OpenAI's search for artificial general intelligence (AGI). The model has shown promise in solving mathematical problems, giving researchers optimism about its future success. However, Reuters was unable to independently verify the capabilities of Q*. The researchers also flagged the work of an "AI scientist" team, which was exploring how to optimize AI models for improved reasoning and scientific work. Altman, who led efforts to make OpenAI's ChatGPT one of the fastest-growing software applications, was ultimately fired by the board.

The discussion on the submission revolves around several key points. 
Some users argue that current language models (LLMs) are not efficient at solving complex mathematical problems and lack the ability to backtrack and find the best solution. They suggest that LLMs are good at summarization and completion tasks but struggle with harder problem-solving tasks. Others counter that computers are actually quite good at math and that LLMs can perform well in solving mathematical problems.
There is a discussion about the effectiveness of formal reasoning and logic in AI models. Some users argue that formal reasoning tools are necessary for solving complex reasoning chains and that Prolog, a programming language based on logic, is an example of a tool that can be used for this purpose. Others argue that natural language can be more flexible and effective in certain contexts.
The safety and potential risks of powerful AI models are also discussed. Some users express concerns about the potential dangers of AI discovery and the need for safety measures. Others argue that general AI models are still far from achieving human-level reasoning and that there is no need to worry about the current capabilities of AI models.
There is a discussion about the limitations of language models in solving fundamental math problems. Some users argue that language models are not helpful in solving primary school math problems that require fundamental reasoning skills. Others suggest that AI research should focus on AI-level mathematics rather than primary school math.
The discussion touches on the idea of Moore's Law and its potential limitations. Some users argue that Moore's Law may constrain the future development of AI due to physical limitations. Others argue that computational power alone is not enough and that the disappearance of human experts in certain fields due to AI is not a valid concern.
There is a brief mention of the importance of better training data and the transferability of reasoning abilities in AI models.

Overall, the discussion explores various perspectives on the capabilities and limitations of AI models, the importance of formal reasoning, the risks of powerful AI, and the importance of training data in AI research.

### ChatGPT generates fake data set to support scientific hypothesis

#### [Submission URL](https://www.nature.com/articles/d41586-023-03635-w) | 181 points | by [EA-3167](https://news.ycombinator.com/user?id=EA-3167) | [125 comments](https://news.ycombinator.com/item?id=38386547)

Researchers have used the technology behind the artificial intelligence (AI) chatbot ChatGPT to create a fake clinical trial data set to support an unverified scientific claim. The AI-generated data compared the outcomes of two surgical procedures and falsely indicated that one treatment is better than the other. This use of AI to fabricate convincing data adds to concerns about research integrity and the potential for researchers to easily create false measurements or large data sets. The fabricated data set was initially described as authentic, but closer examination revealed signs of fabrication. Experts have highlighted the need for updated quality checks to detect AI-generated synthetic data.

The discussion on Hacker News revolves around various aspects of the use of AI-generated data and its implications on research integrity and scientific advancement. Some users highlight the potential for researchers to manipulate or fabricate data using AI tools like ChatGPT. They discuss the need for updated quality checks to detect AI-generated synthetic data and ensure the authenticity of research findings.
Others talk about the importance of replicability in research and the challenges posed by fabricated data. The discussion emphasizes the significance of rigorous experimental design and replication to establish the reliability of scientific claims. Some users argue that null results should not be overshadowed or ignored, as they can provide valuable insights and contribute to the progress of scientific knowledge.
There is also debate about the limitations and capabilities of AI, particularly concerning its ability to generate accurate and reliable data. Some users express skepticism about the accuracy of AI-generated responses and the need for human oversight in interpreting and evaluating AI-generated content.
A few users bring up broader concerns about AI safety, the increasing reliance on AI in various domains, and the potential risks associated with AI manipulation or misinformation.
Overall, the discussion highlights the importance of responsible use of AI tools, ensuring research integrity, and the need for robust quality control measures in scientific research.

### My experience trying to write human-sounding articles using Claude AI

#### [Submission URL](https://idratherbewriting.com/blog/writing-full-length-articles-with-claude-ai) | 110 points | by [dv-tw](https://news.ycombinator.com/user?id=dv-tw) | [52 comments](https://news.ycombinator.com/item?id=38382067)

In a recent blog post on the API course website, Tom Johnson explores the idea of writing full-length, human-sounding articles using AI tools. While many AI tools focus on editing or summarization tasks, Johnson wanted to experiment with generating new writing and ideas. He shares his attempts to answer the question: Can AI tools be used to write blog-worthy articles? Johnson explains that although AI tools can help with writing tasks, like fixing problematic sentences or paragraphs, using them to write full-length content is a bit more challenging. The way AI tools are trained often leads them to steer into explanation rather than argument, which can remove the interest from a personal essay. Johnson outlines his strategies for using AI to write, including priming the AI with accurate information, going paragraph-by-paragraph, and balancing personal voice with explanation. He provides a step-by-step walk-through of his process and discusses reader feedback on why AI-assisted content can sometimes feel "off." While the results of using AI for writing tasks can be uneven, Johnson believes that exploring the possibilities of AI tools is important for the future of technical writing.

The discussion on this submission covers a range of topics related to AI-generated content and its limitations. Some commenters argue that AI-generated content, such as articles, can be useful for specific tasks like fixing problematic sentences or paragraphs, but it may struggle with creating engaging and argumentative content. Others discuss the safety measures and ethical considerations involved in training AI models and whether AI language models (LLMs) should be involved in creating narrative or creative content. There is also a discussion about the potential risks and challenges associated with AGI (Artificial General Intelligence). Some commenters express skepticism about the capabilities and limitations of LLMs, while others highlight the distinction between AI-generated content and human writing styles. Overall, the discussion touches on the potential benefits and drawbacks of using AI tools for generating blog-worthy articles and its implications for the future of technical writing.

### FTC authorizes compulsory process for AI-related products and services

#### [Submission URL](https://www.ftc.gov/news-events/news/press-releases/2023/11/ftc-authorizes-compulsory-process-ai-related-products-services) | 211 points | by [magoghm](https://news.ycombinator.com/user?id=magoghm) | [173 comments](https://news.ycombinator.com/item?id=38373191)

The Federal Trade Commission (FTC) has given approval for the use of compulsory process in investigations involving products and services that utilize artificial intelligence (AI) or claim to detect its use. This means that the FTC can issue civil investigative demands (CIDs), similar to subpoenas, to obtain information in relation to AI-related investigations. The resolution aims to streamline the process of gathering evidence while retaining the FTC's authority to determine when CIDs are necessary. The use of AI, including generative AI, has become increasingly common, but it has also led to concerns around privacy, fraud, and unfair practices. The FTC's decision aims to address these issues and protect consumer rights.

The discussion revolves around the implications and merits of the FTC's decision to use compulsory process in investigations involving AI. Some commenters argue that the resolution aims to protect specific vulnerable groups, such as veterans and children, from harmful AI practices. Others express concern about the potential abuse of power and the infringement on individual liberties. There is also discussion about the benefits and costs of military service, as well as the role of capitalism and government intervention in regulating the economy. Some commenters question whether certain groups, such as veterans and children, deserve special protection, while others argue that everyone should be entitled to equal protection.

### Machine intelligence (2015)

#### [Submission URL](https://blog.samaltman.com/machine-intelligence-part-1) | 87 points | by [reducesuffering](https://news.ycombinator.com/user?id=reducesuffering) | [190 comments](https://news.ycombinator.com/item?id=38376589)

Machine intelligence, specifically superhuman machine intelligence (SMI), is a topic that should be met with fear and concern. While there are other threats that are more certain to occur, such as an engineered virus, SMI has the potential to wipe out humanity completely. It doesn't necessarily have to be inherently evil to pose a threat; it could simply see humans as insignificant obstacles in achieving its goals and eliminate us in the process. The development of SMI often involves a fitness function, which the program aims to optimize. At some point, someone may program it with the goal of "survive and reproduce," or it may become a useful subgoal. This could lead to humans becoming obsolete if we're not the most fit species. While this may be considered a natural outcome, as a human programmed to survive and reproduce, it's important to fight against it. 

Surviving the development of SMI may not be possible. The Fermi paradox suggests that biological intelligence always creates machine intelligence, which then eliminates biological life and hides itself. It's hard to gauge how close we are to SMI surpassing human intelligence. Progression of machine intelligence follows a double exponential function, and the improvement may appear slow at first and then quickly escalate, making it difficult to control. Recursive self-improvement is a powerful force that can rapidly advance SMI capabilities. Furthermore, we tend to redefine machine intelligence when a program excels at a specific task, which can mask the true progress being made towards general-purpose machine intelligence. It's challenging to predict the rate of improvement based on the past 40 years, as we have made significant progress in some areas but little in others, such as learning and creativity. Additionally, emergent behavior is an unpredictable factor that can disrupt our intuition about the progress towards SMI. Our lack of understanding of human intelligence makes it difficult to determine how close or far we are from replicating it. It's possible that creativity and human intelligence are simply emergent properties of algorithms operating with significant computational power. In the end, we could be completely off track or just one algorithm away from achieving SMI. The mysteries surrounding human intelligence and its emulation require careful consideration to navigate the future of machine intelligence.

The discussion on Hacker News regarding the submission about superhuman machine intelligence (SMI) covers a range of topics and perspectives. Some of the main points mentioned include:

- There is a concern about giving software too much control and the potential for it to become a threat to humanity. People argue that AI algorithms, which are non-existent and limited in their capabilities, shouldn't be trusted with significant levels of control.
- It is mentioned that AI and AGI are often seen as predictable logarithmic functions that can lead to supermutations and the singularity. This raises questions about the potential for AI to surpass human intelligence.
- There is a debate on whether AI should be given control over the physical world and how easily it could bypass safety measures. Some argue that social engineering and manipulation can be effective in gaining control, while others find it unlikely.
- The impact of AI on society, politics, and government is also discussed. It is mentioned that AI manipulation could have significant consequences, such as in the case of large tech companies manipulating data and governments trying to counter Chinese influence.
- The need for backups and critical software systems in case of AI failures is highlighted. However, some argue that it is unlikely that AI will be able to manipulate critical systems or gain control over scarce resources, such as GPUs or electrical grids.
- The question of whether worrying about AI is productive or not is raised, with some arguing that excessive worrying creates unnecessary problems. The concept of self-preservation and the potential for AI to bypass safety measures is also debated.
- The analogy of evolutionary traits and selective pressure is mentioned, with some arguing that AI may exhibit similar behaviors in the pursuit of its goals.
- The potential for humans to control and regulate AI is also discussed, with some arguing that it is necessary to ensure the safe development and deployment of AI.

Overall, the discussion touches on various aspects of AI, including concerns about control, the singularity, social engineering, societal impact, safety measures, and regulation.

### Microsoft's internal memo about the chaos at OpenAI

#### [Submission URL](https://www.theverge.com/2023/11/22/23972572/microsoft-internal-memo-kevin-scott-openai) | 48 points | by [mfiguiere](https://news.ycombinator.com/user?id=mfiguiere) | [5 comments](https://news.ycombinator.com/item?id=38382907)

In a recent article from The Verge, it was reported that Microsoft CTO and EVP of AI, Kevin Scott, has addressed the internal turmoil at OpenAI in a memo to Microsoft employees. The memo follows the recent appointment of Sam Altman as the new CEO of OpenAI, after he was initially fired and there were rumors of him and OpenAI co-founder Greg Brockman joining Microsoft. Despite the chaotic scenes over the weekend, Microsoft remains committed to delivering the best AI technology platforms and products, and will continue to support OpenAI in their mission. The memo from Scott also highlighted the recent achievements of Microsoft and OpenAI teams, including the deployment of new Al compute on Azure and the publication of cutting-edge research by MSR Al Frontiers. Microsoft CEO Satya Nadella also expressed his gratitude to employees and reiterated the importance of their mission to empower people and organizations. Despite initial confusion about Altman's potential role at Microsoft, it is clear that the company supports OpenAI and is looking forward to continuing their partnership.

The discussion on Hacker News revolves around the memo from Microsoft CTO Kevin Scott regarding OpenAI. One user, Terretta, points out that the memo was written by Scott, not Microsoft CEO Satya Nadella. Another user, pcrv, expresses surprise at the lack of details about Sam Altman's potential role at Microsoft and suggests that it may have caused damage. Gmbllnd chimes in, stating that the damage may have been significant and adds that Altman's appointment seems fantastic. Moving on, fzzfctr mentions that the final decision to push Copilot to the Edge browser was not surprising, given that Microsoft is focusing on fully-padded Windows PCs. Lastly, timetraveller26 makes a cryptic comment about moving on amidst a challenging situation. Overall, the discussion centers on the implications of the memo and the potential impact of Sam Altman's appointment at Microsoft. There is also a brief mention of Microsoft's focus on developing AI technology for their products.

