## AI Submissions for Tue Sep 23 2025 {{ 'date': '2025-09-23T17:16:56.696Z' }}

### Getting AI to work in complex codebases

#### [Submission URL](https://github.com/humanlayer/advanced-context-engineering-for-coding-agents/blob/main/ace-fca.md) | 458 points | by [dhorthy](https://news.ycombinator.com/user?id=dhorthy) | [383 comments](https://news.ycombinator.com/item?id=45347532)

Advanced Context Engineering for Coding Agents (GitHub) — HumanLayer released an open-source playbook aimed at making LLM coding agents more reliable on real-world codebases by ruthlessly curating and structuring what goes into the model’s context. The repo focuses on practical techniques for selecting relevant code, organizing prompts, and scaling context across large repos—so agents can plan changes and navigate without getting lost or hallucinating. It’s already drawing interest (400+ stars), and is a useful reference if you’re building or tuning repo-aware AI dev tools.

The Hacker News discussion on the "Advanced Context Engineering for Coding Agents" submission highlights skepticism and debate about AI's reliability in coding tasks. Key points include:

1. **Frustration with AI Tools**: Users note that while AI speeds up code generation, it shifts time toward debugging unexpected errors and incorrect assumptions. Integration challenges persist, with AI-generated code often requiring extensive manual validation.

2. **Non-Determinism vs. Compilers**: LLMs are criticized for non-deterministic outputs, unlike compilers, which are predictable and testable. Ambiguous requirements (e.g., written in English) exacerbate issues, leading to unreliable code.

3. **Human vs. AI Reasoning**: Participants debate whether LLMs truly "reason" or merely mimic patterns. Some argue LLMs lack human-like understanding, likening them to junior developers needing strict guidance. Others counter that intelligence manifests differently (e.g., statistical pattern matching vs. abstract reasoning).

4. **Testing and Redundancy**: AI's unpredictable errors are harder to catch than human mistakes. Suggestions include aggressive testing, redundant code checks, and skepticism toward AI's "correct-looking" but potentially flawed outputs.

5. **Philosophical Debates**: Discussions diverge into the nature of intelligence, comparing LLMs to biological brains and historical innovations like flight. Some dismiss anthropocentric views, arguing intelligence need not mirror human cognition to be effective.

6. **Practical Challenges**: Handling tasks like CSV parsing or legacy systems (e.g., COBOL) reveals gaps in AI's ability to manage real-world codebases. High-level languages (Python) fare better, but edge cases and integration complexities remain problematic.

Overall, the consensus leans toward cautious pragmatism: AI tools show promise but require rigorous context engineering, precise specifications, and human oversight to mitigate hallucinations and integration pitfalls.

### Context Engineering for AI Agents: Lessons

#### [Submission URL](https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus) | 104 points | by [helloericsf](https://news.ycombinator.com/user?id=helloericsf) | [4 comments](https://news.ycombinator.com/item?id=45352901)

Why it matters: Instead of training end-to-end agent models, Manus bets on “context engineering” atop frontier LLMs to iterate in hours, not weeks—staying model-agnostic so rising model quality lifts the product.

Key ideas:
- Optimize for KV-cache hit rate: For agents, input-to-output tokens can be ~100:1, so prefilling dominates latency and cost. Cached tokens can be 10x cheaper (e.g., Claude Sonnet: $0.30 vs $3 per MTok). The author argues KV-cache hit rate is the single most important production metric.
- Practical cache tactics:
  - Keep the prompt prefix stable. Even one-token drift (like a live timestamp) breaks cache from that point on.
  - Make context append-only. Don’t edit prior actions/observations; serialize deterministically (watch JSON key ordering).
  - Mark cache breakpoints when needed. Some providers require explicit boundaries; at minimum, include the system prompt’s end. If self-hosting (e.g., vLLM), enable prefix caching and use session IDs for routing consistency.
- “Mask, don’t remove” tools: As agents gain capabilities, the tool/action space balloons—especially with user-added MCP tools. Dynamically adding/removing tools mid-trajectory sounds smart but often backfires:
  - Changing tool definitions near the front of context invalidates cache for subsequent steps.
  - If past steps reference tools no longer present, models get confused, causing schema violations or hallucinated actions.
  - The principle: keep the action space stable during a run and guide selection via masking/constraints rather than swapping tools in and out.

Vibe: Equal parts war story and playbook. The team calls their iterative prompt/architecture search “Stochastic Graduate Descent”—messy but effective. If model progress is the tide, they want to be the boat, not a pillar stuck to the seabed.

Takeaways for builders:
- Treat KV-cache as a first-class metric; structure prompts and logs to preserve it.
- Prefer append-only, deterministic contexts.
- Keep tool lists stable within a session; steer choices via masking/constraints instead of dynamic loading.

Here's a concise summary of the discussion:

1. **jslv** (with reply from **SafeDusk**)  
   - Emphasizes system memory optimization and tool management patterns like using deterministic file naming, clean code directories, and git-like revision control for agent decisions.  
   - SafeDusk adds a practical example: A simplified Codex-based approach tracking task progress via https://blog.toolcompany.com/codex-tools  

2. **dxfhl**  
   - Warns against over-engineering: Recommends preserving flawed tools/comments via rollbacks/PRs rather than deletions. Advises maintaining stable tooling to avoid model confusion (mirroring the submission's "mask, don't remove tools" principle).  

3. **sfk**  
   - Highlights business implications: Fixed pricing plans incentivize providers to optimize caching (improving margins). Stresses the importance of measuring cache hit rates and visibility, especially for teams new to context engineering.

**Key themes**  
- Real-world tradeoffs between engineering purity and production needs  
- Alignment between technical caching tactics (from submission) and business models  
- Emphasis on version control patterns for agent memory/tools

### From MCP to shell: MCP auth flaws enable RCE in Claude Code, Gemini CLI and more

#### [Submission URL](https://verialabs.com/blog/from-mcp-to-shell/) | 140 points | by [stuxf](https://news.ycombinator.com/user?id=stuxf) | [39 comments](https://news.ycombinator.com/item?id=45348183)

What happened:
- Veria Labs found a simple but high‑impact auth flaw across Model Context Protocol (MCP) clients including Cloudflare’s use-mcp library, Anthropic’s MCP Inspector, Claude Code, and Google’s Gemini CLI.
- Root cause: MCP added an OAuth-based auth flow where the server supplies an authorization URL. Many clients blindly opened that URL. In use-mcp, window.open(authUrlString) accepted javascript: URLs, yielding instant XSS.
- Chain to RCE: With MCP Inspector and the stdio transport, researchers turned that XSS into native code execution (“popped calc”) and note it could be extended to deliver malware or a reverse shell.
- They also demo exploits against Claude Code and Gemini CLI. ChatGPT narrowly avoided impact due to server-side redirects that broke the XSS path.

Why it matters:
- This is a classic trust-boundary mistake: treating server-supplied URLs as safe during OAuth. In AI toolchains, that mistake bridges from browser XSS to local RCE via transports like stdio.
- The attack is one-click: connect to a malicious MCP server and the client opens the attacker’s URL, triggering XSS and then RCE.

Fixes and mitigations highlighted:
- Strictly validate and allowlist OAuth authorization URLs (scheme/domain/path), reject javascript:/data: schemes, and use system browser with deep origin checks.
- Bind OAuth flows with state/nonce, verify origins on postMessage, and harden stdio transport so a compromised webview can’t reach native exec.
- Vendors have shipped or are shipping patches; proof-of-concepts and a timeline are included in the post.

If you use these tools:
- Update Claude Code, Gemini CLI, MCP Inspector, and any app/library using use-mcp.
- Don’t connect clients to untrusted MCP servers; consider sandboxing until patched.

Big takeaway: OAuth is an attack surface. Never trust server-provided auth URLs, especially in agent/tooling ecosystems where a web XSS can quickly escalate to local RCE.

The discussion revolves around security vulnerabilities in MCP clients, particularly OAuth flaws that allow malicious servers to execute code. Key points include:

1. **Risk Comparisons**:  
   - Users liken trusting MCP servers to blindly installing packages from PyPI or npm, highlighting supply chain risks. Even "trusted" MCP servers (like those from Anthropic or Google) could be compromised, emphasizing the need for skepticism and validation.

2. **Root Causes**:  
   - The vulnerability stems from clients blindly trusting server-provided URLs (including `javascript:` schemes), leading to XSS and RCE via insecure transports like stdio. Patches have been released, but questions remain about the robustness of fixes (e.g., Google’s quick PowerShell tweak vs. deeper validation).

3. **AI Hype vs. Security**:  
   - Commercial AI tools (Claude, Gemini) may lead users to implicitly trust MCP servers, creating liability. The hype around AI agents obscures fundamental security gaps, as MCP’s design mixes trusted code execution with untrusted inputs.

4. **Broader Protocol Concerns**:  
   - MCP and similar protocols face challenges in securely handling inputs, akin to prompt injection in LLMs. Users debate whether strict input validation, sandboxing, or dedicated LLM instances can mitigate risks, though 100% prevention is deemed impossible statistically.

5. **Criticism of AI Practices**:  
   - Some criticize AI companies for prioritizing speed over security, leading to vulnerabilities. Disabling MCP servers is suggested to avoid risks, while others argue MCP’s potential warrants improving its security framework.

6. **Proposed Solutions**:  
   - Suggestions include strict URL allowlisting, sandboxing, separating data from commands, and redesigning protocols to avoid credential/execution vulnerabilities. Community collaboration to address flaws is encouraged.

**Key Takeaway**: Trust in third-party sources (supply chain) remains a critical weakness. While patches help, MCP and AI ecosystems need fundamental redesigns to balance innovation with security, ensuring inputs are rigorously validated and execution contexts are isolated.

### MLB approves robot umpires for 2026 as part of challenge system

#### [Submission URL](https://www.espn.com/mlb/story/_/id/46357017/mlb-approves-robot-umpires-2026-part-challenge-system) | 106 points | by [pseudolus](https://news.ycombinator.com/user?id=pseudolus) | [94 comments](https://news.ycombinator.com/item?id=45354304)

- What’s changing: MLB will use an automated ball-strike (ABS) challenge system, not full automation. Each team gets two challenges per game; only the pitcher, catcher, or hitter can initiate by tapping their head. Successful challenges are retained. Calls and replays will show on stadium videoboards.
- Extra-innings logic: Unused challenges carry over. If you run out by the 10th, you get one more; run out again, you get another in the 11th, and so on.
- How it works: 12 calibrated cameras track pitches with ~1/6-inch margin of error. The strike zone is a 2D plane over the plate (17 inches wide) scaled to each batter: top at 53.5% of height, bottom at 27%.
- Early data: Spring training saw ~4 challenges per game with a 52.2% success rate. Catchers led overturns (56%), hitters were at 50%, pitchers 41%.
- Why it matters: It’s a high-profile, human-in-the-loop computer vision system in a major pro sport. MLB aims to correct high-leverage misses without slowing the game, reduce ejections (over 60% tied to balls/strikes), and clarify the zone. It could diminish the value of pitch framing and subtly reshape game strategy.
- Governance notes: The competition committee vote wasn’t unanimous; owners hold a six-seat majority. Umpires still call the game; ABS is the backstop. Minor leagues have tested both challenge and full-ABS modes since 2021.

**Summary of Hacker News Discussion on MLB's "Robot Umps" (2026):**

The introduction of MLB's challenge-based automated ball-strike (ABS) system sparked debate around technology's role in sports officiating, fairness, and tradition. Key themes from the discussion include:

1. **Human vs. Automated Judgment**:  
   - Many users acknowledged the inevitability of technology correcting high-stakes errors but lamented the loss of the "human element" in umpiring. Comparisons were drawn to soccer’s VAR system, where controversial calls (e.g., Maradona’s "Hand of God") remain iconic despite technological interventions.  
   - Concerns were raised about unintended consequences, such as diminished roles for skills like pitch framing and potential shifts in game strategy.

2. **Sports Betting Influence**:  
   - Several comments tied the adoption of ABS to the rise of legalized sports betting in the U.S., arguing that leagues now prioritize precision to protect gambling integrity. Skeptics questioned whether profit motives, rather than fairness, drove the change.

3. **Historical Context**:  
   - Users noted MLB’s gradual tech adoption (e.g., instant replays over 50 years) and resistance from umpires’ unions. Comparisons to cricket’s tech-heavy officiating highlighted differing cultural approaches to automation in sports.

4. **Referee Bias and Fairness**:  
   - References to the book *Scorecasting* underscored statistical evidence of subconscious referee bias in sports. Robot umps were seen as a way to reduce such biases, though some argued human judgment inherently shapes games unpredictably.

5. **Practical Challenges**:  
   - Early data from minor leagues (4 challenges/game, ~52% success rate) sparked discussions about implementation logistics, such as challenge limits and maintaining game pace. Users debated whether ABS would reduce ejections (60% tied to ball/strike calls) or introduce new frustrations.

6. **Cultural Resistance**:  
   - Traditionalists mourned the erosion of "sandlot baseball" nostalgia, while others welcomed progress. The Cubs’ Wrigley Field was cited as a symbol of balancing innovation with heritage.

**Conclusion**: The discussion reflects tension between precision and tradition, with supporters advocating for fairness and critics fearing the loss of human nuance. While ABS aims to correct errors and modernize MLB, its success hinges on balancing technological accuracy with the unpredictable drama that defines sports fandom.

### Agents turn simple keyword search into compelling search experiences

#### [Submission URL](https://softwaredoug.com/blog/2025/09/22/reasoning-agents-need-bad-search) | 62 points | by [softwaredoug](https://news.ycombinator.com/user?id=softwaredoug) | [31 comments](https://news.ycombinator.com/item?id=45347363)

Stupid backend, smart agent: this post argues that LLM agents produce better search experiences when the search API is simple and predictable, not a “thick” black box full of synonyming, reranking, and vector tricks. The author stripped their furniture search to bare‑bones BM25 with a clear docstring, then let an agent iterate like a human—probing queries, judging results, and refining terms. In examples like “a couch fit for a vampire” and “ugliest chair,” the agent tried targeted queries (“cow print chair,” “patchwork accent chair,” “skull chair”), labeled outcomes as good/meh/bad, and saved them. A lightweight memory layer and semantic cache let it reuse winning expansions for similar future queries (“ugly chair”). It’s slower, but surprisingly effective—and the learned expansions can even feed back into a conventional non-LLM search. Takeaway: give agents transparent tools they can reason about; move the “intelligence” to the agent, not the API.

**Summary of the Discussion:**  
The discussion revolves around balancing **simplicity vs. complexity** in search backend design when integrating LLM agents. Here are the key points:

1. **Transparency & Control**  
   - Supporters argue that a simple, predictable API (like BM25) gives LLM agents clearer tools to reason with, enabling human-like iterative query refinement (e.g., "ugly chair" → "cow print chair"). Structured outputs and deterministic tools help agents avoid "magical" black-box behavior.  
   - Critics question whether overly simplistic APIs can handle nuanced needs like synonyms or relevance ranking, which Google’s complex backend (PageRank, conversational keyword support) addresses—albeit with trade-offs like SEO spam and popularity bias.

2. **Cost & Practicality**  
   - Building custom search engines (e.g., with vector databases) is resource-intensive, while relying on services like Google risks losing control over relevance. Some note LLM-driven agents could inflate costs via API calls and prompt engineering.

3. **SEO & Content Quality**  
   - Google’s evolution highlights struggles with keyword-stuffed, SEO-optimized content. A lightweight backend might bypass these issues but lacks advanced features like topic clustering or popularity signals.

4. **LLM Integration**  
   - Agents leveraging semantic caching and feedback loops (saving successful queries) can improve results over time. However, challenges remain in handling ambiguous terms (e.g., "swift" meaning Taylor Swift vs. the programming language) without backend disambiguation.

5. **Structured vs. Open-Ended Search**  
   - While structured tools aid reasoning, users emphasize the need for agents to handle vague or creative queries ("couch fit for a vampire") through experimentation rather than rigid APIs.

**Takeaways**:  
The debate highlights a split between favoring minimalist, transparent backends for LLM-driven search agility and acknowledging the necessity of some backend sophistication to manage real-world ambiguity and scalability. The original approach’s strength lies in prioritising agent reasoning over opaque backend "magic," though practical implementation may require balancing both.

### Android users can now use conversational editing in Google Photos

#### [Submission URL](https://blog.google/products/photos/android-conversational-editing-google-photos/) | 128 points | by [meetpateltech](https://news.ycombinator.com/user?id=meetpateltech) | [129 comments](https://news.ycombinator.com/item?id=45349848)

Google is rolling out Google Photos’ conversational editing—first seen on Pixel 10—to eligible Android users in the U.S. Tap “Help me edit” and describe changes by voice or text (or just say “make it better”); Gemini-powered smarts handle the rest, alongside one-tap suggestions and simple gestures. Beyond quick touch-ups, it supports playful, generative edits (think: moving an alpaca from a petting zoo to Waikiki). Availability is U.S.-only for now and limited to eligible Android devices.

**Hacker News Discussion Summary on Google Photos AI Editing Feature:**

1. **Criticism of AI Integration:**  
   - Many users express frustration with Google's aggressive AI feature rollouts, calling them "bloated" and poorly integrated. Complaints include cluttered interfaces, hidden settings, and difficulties performing basic tasks like cropping ("Impossible on Pixel now!").  
   - Comparisons to Apple’s slower AI adoption are noted, with some suggesting Google prioritizes investor-friendly "AI buzzwords" over thoughtful user experience.  

2. **Privacy and Data Concerns:**  
   - Skepticism exists around AI features being "data grabs" for training models, especially with Google Photos allegedly removing options to disable features like face grouping.  

3. **Alternatives to Google Photos:**  
   - **Self-hosted solutions** like [Immich](https://immich.app/) and [Ente](https://ente.io/) are praised, though Immich lacks HDR support.  
   - Technical debates arise over encryption (e.g., Hetzner VPS setups, Tailscale/WireGuard for secure sync), with warnings about trusting third-party providers with unencrypted data.  

4. **Technical Bugs and Regressions:**  
   - Users report broken features (e.g., Magic Eraser no longer working correctly) and interface overhauls (e.g., "rounded corners everywhere") that disrupt workflows.  

5. **Motivations Behind Features:**  
   - Some speculate Google’s AI push is financially motivated (e.g., driving storage sales via AI-generated content). Others counter that storage costs are negligible for Google, arguing the goal is product stickiness, not short-term profit.  

**Humorous/Cultural Notes:**  
   - A *Blade Runner* reference ("Enhance!") jokes about AI’s limitations.  
   - "Soylent Green vibes" quips nod to dystopian tech critiques.  

**Key Takeaway:** The discussion reflects skepticism toward forced AI integration, enthusiasm for open-source alternatives, and debates about balancing convenience with privacy/control.

### Abundant Intelligence

#### [Submission URL](https://blog.samaltman.com/abundant-intelligence) | 86 points | by [j4mie](https://news.ycombinator.com/user?id=j4mie) | [128 comments](https://news.ycombinator.com/item?id=45346968)

Sam Altman: Building a “gigawatt-per-week” AI infrastructure factory

- Altman says AI demand is outpacing supply and access to AI could become a fundamental economic driver—and possibly a human right.
- He frames compute as the bottleneck: with enough power (e.g., “10 gigawatts”), AI might tackle goals like curing cancer or delivering personalized tutoring to every student.
- Vision: create a factory capable of producing 1 gigawatt of new AI infrastructure every week. Getting there will take years and breakthroughs across chips, power, construction, and robotics.
- Strong emphasis on building much of this in the US to accelerate domestic capacity, where he argues other countries currently move faster on fabs and energy.
- Details to come: partners will be announced in the next couple of months; financing plans later this year, with “interesting” models tied to revenue growth from added compute.

Why it matters: If realized, this would be one of the largest infrastructure build-outs in tech history, reshaping energy, semiconductor, and data center markets—and potentially who can access advanced AI. What’s unclear: where the power, capital, and supply chain headroom will come from, and how quickly permitting and grid constraints can be addressed.

**Hacker News Discussion Summary: Sam Altman's Gigawatt AI Infrastructure Proposal**

The Hacker News community reacts to Sam Altman's ambitious plan for a "gigawatt-per-week" AI infrastructure factory with a mix of cautious optimism and skepticism. Here are the key themes:

### **1. Environmental and Energy Concerns**
- **Feasibility Challenges**: Users question the practicality of scaling energy production to 10 gigawatts, noting it would require ~87 TWh annually (2% of U.S. consumption). Critics argue this demands unprecedented investment in renewables, nuclear, or untested energy solutions.
- **Nuclear Partnerships**: Altman’s investment in Oklo, a nuclear startup, is highlighted as a potential pathway, though doubts remain about regulatory and technical hurdles.

### **2. Skepticism Toward Claims**
- **Hyperbole and History**: Altman is criticized for past exaggerated statements (e.g., AGI timelines). Users compare the proposal to overhyped tech trends, urging scrutiny of his predictions.
- **AI Capability Doubts**: Some argue current AI (e.g., GPT-4/5) shows incremental, not revolutionary, progress. Solving issues like climate change or cancer is deemed unrealistic without addressing deeper societal or behavioral factors.

### **3. Ethical and Economic Implications**
- **Privatization vs. Public Utility**: Debate erupts over framing AI access as a "human right." Critics liken privatized AI to commodified utilities (e.g., bottled water), advocating for government-provided access to prevent corporate monopolies.
- **Global Inequality**: Concerns arise that AI could exacerbate colonialism-like dynamics, benefiting wealthy nations while leaving developing regions behind. Others humorously suggest AI might inadvertently address poverty through job creation or efficiency gains.

### **4. Technical and Logistical Hurdles**
- **Infrastructure Realities**: Building gigawatt-scale data centers faces challenges in chip supply chains, construction speed, and labor conditions. Comparisons are drawn to stalled megaprojects (e.g., TSMC’s Arizona fab delays).
- **Regulatory and Financial Barriers**: Questions linger about funding models, partnerships (e.g., Nvidia, TSMC), and navigating U.S. permitting processes for energy and construction.

### **5. Broader Societal Impact**
- **Climate Priorities**: Some argue focusing on AI distracts from urgent climate action, while others speculate AI could optimize energy use or accelerate research.
- **Economic Reshaping**: The proposal’s scale could redefine semiconductor, energy, and data center markets, though skeptics doubt its profitability compared to existing tech infrastructure.

### **Conclusion**
While Altman’s vision is acknowledged as transformative in theory, the community emphasizes unresolved challenges: environmental sustainability, ethical governance, and logistical feasibility. The discussion reflects tension between optimism for AI’s potential and skepticism about Silicon Valley’s ability to deliver on grand promises without exacerbating existing societal issues.

