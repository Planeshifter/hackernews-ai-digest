## AI Submissions for Sun Jan 11 2026 {{ 'date': '2026-01-11T17:15:02.176Z' }}

### Don't fall into the anti-AI hype

#### [Submission URL](https://antirez.com/news/158) | 1133 points | by [todsacerdoti](https://news.ycombinator.com/user?id=todsacerdoti) | [1436 comments](https://news.ycombinator.com/item?id=46574276)

Don’t fall into the anti-AI hype (antirez): Redis creator says coding has already changed

Salvatore “antirez” Sanfilippo, a self-professed lover of hand-crafted code, argues that facts trump sentiment: modern LLMs can now complete substantial programming work with minimal guidance, reshaping software development far faster than he expected.

What changed his mind:
- In hours, via prompting and light oversight, he:
  - Added UTF-8 support to his linenoise library and built a terminal-emulated line-editing test framework.
  - Reproduced and fixed flaky Redis tests (timing/TCP deadlocks), with the model iterating, reproducing, inspecting processes, and patching.
  - Generated a ~700-line pure C inference library for BERT-like embeddings (GTE-small), matching PyTorch outputs and within ~15% of its speed, plus a Python converter.
  - Re-implemented recent Redis Streams internals from his design doc in under ~20 minutes.
- Conclusion: for many projects, “writing the code yourself” is now optional; the leverage is in problem framing and system design, with LLMs as capable partners.

His stance:
- Welcomes that his open-source work helped train these models—sees it as continued democratization, giving small teams leverage akin to open source in the ’90s.
- Warns about centralization risk; notes open models (including from China) remain competitive, suggesting there’s no hidden “magic” and others can catch up.
- Personally plans to double down on open source and apply AI throughout his Redis workflow.

Societal concern:
- Expects real job displacement and is unsure whether firms will expand output or cut headcount.
- Calls for political and policy responses (e.g., safety nets/UBI-like support) as automation accelerates.
- Even if AI company economics wobble, he argues the programming shift is irreversible.

Based on the discussion, here is a summary of the user comments regarding Antirez's submission:

**Skepticism Regarding "Non-Trivial" Work**
Multiple commenters questioned Antirez's assertion that LLMs can handle non-trivial tasks effectively. One user (**ttllykvth**) noted that despite using SOTA models (GPT-4+, Opus, Cortex), they consistently have to rewrite 70% of AI-generated code. They speculated that successful AI adopters might either be working on simpler projects or operating in environments with lower code review standards. There is a sentiment that while AI works for "greenfield" projects (like Antirez's examples), it struggles significantly with complex, legacy enterprise applications (e.g., 15-year-old Java/Spring/React stacks).

**The "Entropy" and Convergence Argument**
A recurring theme was the concept of "entropy." Users **nyttgfjlltl** and **frndzs** argued that while human coding is an iterative process that converges on a correct solution, LLMs often produce "entropy" (chaos or poor architecture) that diverges or requires immense effort to steer back on track.
*   **Expert Guidance Required:** Users argued LLMs act best as "super search engines" that offer multiple options, but they require a domain expert to aggressively filter out the "garbage" and steer the architecture.
*   **Greenfield vs. Brownfield:** The consensus suggests LLMs are decent at "slapping together" new implementations but fail when trying to modify tightly coupled, existing codebases.

**Hallucinations in Niche Fields and Tooling**
There was significant debate regarding the reliability of LLMs for research and specific stack configurations:
*   **Science/Research:** User **20k** reported that for niche subjects like astrophysics (specifically numerical relativity), LLMs are "substantially wrong" or hallucinate nonexistent sources. Others cited Google’s AI claiming humans are actively mining helium-3 on the moon.
*   **Infrastructure-as-Code:** Users **dvddbyzr** and **JohnMakin** highlighted specific struggles with Terraform. They noted LLMs frequently hallucinate parameters, invent internal functions, or provide obscure, unnecessary steps for simple configurations, making it faster to write the code manually.

**Counter-points on Prompting and Workflow**
*   **Context Engineering:** User **0xf8** suggested that success requires "context engineering"—building tooling and scaffolding (memory management, patterns) around the LLM—and that simply "chatting" with the model is insufficient for complex engineering.
*   **Productivity:** Despite the flaws, some users (**PeterStuer**) still view AI as a "net productivity multiplier" and a "knowledge vault" for tasks like debugging dependency conflicts, provided the developer maintains strict constraints.

### Anthropic: Developing a Claude Code competitor using Claude Code is banned

#### [Submission URL](https://twitter.com/SIGKITTEN/status/2009697031422652461) | 304 points | by [behnamoh](https://news.ycombinator.com/user?id=behnamoh) | [163 comments](https://news.ycombinator.com/item?id=46578701)

I’m ready to write the digest, but I don’t have the submission yet. Please share one of the following:
- The Hacker News thread URL or item ID
- The article URL
- Or paste the article text (and optionally notable HN comments)

Also tell me your preference:
- Length: ultra-brief (2–3 sentences) or short (5–7 sentences)?
- Include HN comment takeaways and controversy?
- Add a one-line TL;DR?

Here is the summary of the discussion regarding Anthropic's terms for "Claude Code."

**TL;DR:** Users are debating Anthropic’s Terms of Service which prohibit using "Claude Code" to reverse engineer the API or build competing AI products, with arguments centering on EU enforceability and the distinction between general ML development vs. direct cloning.

**Discussion Summary:**
The conversation focuses on the Terms of Service for Anthropic's new "Claude Code" tool, specifically restrictions against using the software to reverse engineer its API or develop competing coding assistants. Users are debating the scope of these rules, with some interpreting them as a ban on creating *any* AI competitors, while others clarify that the terms likely only prohibit building direct substitutes (like another LLM-based coding agent) rather than general machine learning applications. A significant portion of the thread questions the enforceability of these non-compete clauses in the European Union, speculating that antitrust laws might invalidate them. There are also references to recent enforcement rumors, including Anthropic potentially blocking access from China and intermediaries like Cursor. Finally, some commenters look forward to a future of powerful open-source models running on cheap local hardware ($2000 range) to bypass these vendor restrictions entirely.

### Sisyphus Now Lives in Oh My Claude

#### [Submission URL](https://github.com/Yeachan-Heo/oh-my-claude-sisyphus) | 50 points | by [deckardt](https://news.ycombinator.com/user?id=deckardt) | [38 comments](https://news.ycombinator.com/item?id=46572032)

Oh My Claude Sisyphus: community multi‑agent orchestration for Claude Code, back from a “ban”
- What it is: A port of the “oh-my-opencode” multi-agent system to the Claude Code SDK. It bundles 10+ specialized agents that coordinate to plan, search, analyze, and execute coding tasks until completion—leaning into a Sisyphus theme. Written using Claude Code itself. MIT-licensed, currently ~836 stars/81 forks.

- Why it’s interesting: Pushes the “multi‑agent IDE copilot” idea inside Claude Code, with dedicated roles and slash commands that orchestrate complex workflows. Also carries a cheeky narrative about being “banned” and resurrected, highlighting community energy around extending closed tooling.

- Key features
  - Agents by role and model: strategic planner (Prometheus, Opus), plan reviewer (Momus, Opus), architecture/debug (Oracle, Opus), research (Librarian, Sonnet), fast pattern matching (Explore, Haiku), frontend/UI (Sonnet), multimodal analysis (Sonnet), focused executor (Sisyphus Jr., Sonnet), and more.
  - Commands: /sisyphus (orchestration mode), /ultrawork (parallel agents), /deepsearch, /analyze, /plan, /review, /orchestrator, /ralph-loop (loop until done), /cancel-ralph, /update.
  - “Magic keywords” (ultrawork, search, analyze) trigger modes inside normal prompts.
  - Ships as a Claude Code plugin with hooks, skills (ultrawork, git-master, frontend-ui-ux), and a file layout that installs into ~/.claude/.

- Installation
  - Claude Code plugin: /plugin install oh-my-claude-sisyphus (or from marketplace).
  - npm (Windows recommended): npm install -g oh-my-claude-sisyphus (Node 20+).
  - One-liner curl or manual git clone on macOS/Linux.

- Caveats and notes: Community plugin that modifies Claude Code config and adds hook scripts; review before installing in sensitive environments. The playful “Anthropic, what are you gonna do next?” tone and ban/resurrection lore may spark discussion about platform policies.

Who it’s for: Claude Code users who want opinionated, multi-agent workflows and quick slash-command entry points for planning, review, deep search, and high‑throughput “ultrawork” coding sessions.

**Discussion Summary:**

The discussion thread is a mix of skepticism regarding multi-agent utility and speculation surrounding the "ban" narrative mentioned in the submission.

*   **The "Ban" & Business Model:** A significant portion of the conversation dissects why the predecessor (Oh My OpenCode) and similar tools faced pushback from Anthropic. The consensus is that these tools effectively wrap the Claude Code CLI—a "loss leader" meant for human use—to emulate API access. Users argue this creates an arbitrage opportunity that cannibalizes Anthropic's B2B API revenue, making the crackdown (or TOS enforcement) appear reasonable to many, though some lament losing the cheaper access point.
*   **Skepticism of Multi-Agent Orchestration:** Technical users expressed doubt about the efficiency of the "multi-agent" approach. Critics argue that while the names are fancy ("Prometheus," "Oracles"), these systems often burn through tokens for results that are "marginally linear" or sometimes worse than a single, well-prompted request to a smart model like Gemini 1.5 Pro or vanilla Claude.
*   **Project Critique:** One user who tested the tool provided a detailed critique, describing the README as "long-winded, likely LLM-generated" and the setup as "brittle." They characterized the tool as essentially a configuration/plugin set (akin to LazyVim for Neovim) rather than a revolutionary leap, noting that in practice, it often produced "meh" results compared to default Claude Code.
*   **Context Management:** A counterpoint was raised regarding context: proponents of the sub-agent workflow argued its main utility isn't necessarily reasoning superiority, but rather offloading task-specific context to sub-agents. This prevents the main conversation thread from hitting "context compaction" (summarization) limits too quickly, which degrades model intelligence.

### Google: Don't make "bite-sized" content for LLMs

#### [Submission URL](https://arstechnica.com/google/2026/01/google-dont-make-bite-sized-content-for-llms-if-you-care-about-search-rank/) | 79 points | by [cebert](https://news.ycombinator.com/user?id=cebert) | [44 comments](https://news.ycombinator.com/item?id=46575127)

Google to publishers: Stop “content chunking” for LLMs—it won’t help your rankings

- On Google’s Search Off the Record podcast, Danny Sullivan and John Mueller said breaking articles into ultra-short paragraphs and Q&A-style subheads to appeal to LLMs (e.g., Gemini) is a bad strategy for search.
- Google doesn’t use “bite-sized” formatting as a ranking signal; the company wants content written for humans. Human behavior—what people choose to click and engage with—remains a key signal.
- Sullivan acknowledged there may be edge cases where chunking appears to work now, but warned those gains are fragile and likely to vanish as systems evolve.
- The broader point: chasing trendy SEO hacks amid AI-induced traffic volatility leads to superstition and brittle tactics. Long-term exposure comes from serving readers, not machines.

Why it matters: As publishers scramble for traffic in an AI-scraped web, Google’s guidance is to resist formatting for bots. Sustainable SEO = clarity and usefulness for humans, not slicing content into chatbot-ready snippets.

Source: Ars Technica (Ryan Whitwam), discussing Google’s Search Off the Record podcast (~18-minute mark)

Here is a summary of the discussion:

**Skepticism and Distrust**
The predominant sentiment in the comments is a lack of trust in Google’s guidance. Many users believe the relationship between Google and webmasters has become purely adversarial. Commenters cited past instances where adhering to Google's specific advice (like mobile vs. desktop sites) led to penalties later, suggesting that Google’s public statements often contradict how their algorithms actually reward content in the wild.

**The "Slop" and Quality Irony**
Users pointed out the hypocrisy in Google calling for "human-centric" content while the current search results are perceived as being overrun by SEO spam and AI-generated "slop."
*   One commenter noted the irony that the source article itself (Ars Technica) utilizes the very "content chunking" and short paragraphs Google is advising against.
*   Others argued that Google needs human content merely to sanitize training data for their own models, referencing notorious AI Overview failures (like the "glue on pizza" or "eat rocks" suggestions) as evidence that training AI on SEO-optimized garbage "poisons" the dataset.

**Economic Misalignment**
There was a debate regarding the logic of optimizing for LLMs at all. Users noted that unlike search engines, LLMs/chatbots frequently scrape content without guiding traffic back to the source (the "gatekeeper" problem). Consequently, destroying the readability or structure of a website to appeal to a bot that offers no click-through revenue is viewed as a losing strategy.

**Technical "Superstition"**
Several users described modern SEO as "superstition" or a guessing game, noting that while structured, semantic web principles (from the early 2000s) should ideally work, search engines often ignore them in favor of "gamed" content.

### guys why does armenian completely break Claude

#### [Submission URL](https://twitter.com/dyushag/status/1993143599286886525) | 95 points | by [ag8](https://news.ycombinator.com/user?id=ag8) | [60 comments](https://news.ycombinator.com/item?id=46579397)

I’m ready to summarize—could you share the Hacker News submission you want covered?

Please provide:
- HN link or item ID (or paste the title + article link)
- Any key context you want included (e.g., why it matters to your audience)
- Whether to include discussion highlights from top comments
- Preferred length/tone (e.g., 3–5 bullets, 120–180 words, punchy vs. neutral)

If you paste the article text or main points, I can work from that too.

Sample output format:
- TL;DR: One-sentence takeaway
- What happened: 2–3 concise bullets
- Why it matters: 1–2 bullets on impact/implications
- HN discussion: Notable angles or top critiques
- My take: Brief, non-obvious insight or caveat (optional)

Based on the comment thread provided, here is a summary of the discussion surrounding a Hacker News submission about **LLMs (specifically Claude) behaving erratically or triggering safety shutdowns when prompted in Armenian.**

**(Note: The submission link was not provided, but the context implies a post about Claude 3.5 Sonnet failing or triggering false-positive "harmful content" flags when processing the Armenian language.)**

### **TL;DR**
Users report that using Armenian with Claude triggers aggressive safety refusals (suspected false positives for "bomb-making" or obfuscation), leading to a broader debate on how LLMs handle low-resource languages and cultural context.

### **What happened**
*   **The Bug:** Users observed that querying Claude in Armenian causes it to "short-circuit," often resulting in safety refusals. One hypothesis is that the safety layer misinterprets Armenian text as obfuscated code or malicious instructions (like bomb-making).
*   **The Jailbreak Connection:** Commenters noted this is a known phenomenon where translating prompts into other languages (like German or low-resource languages) often bypasses English-centric safety filters ("jailbreaking").
*   **The "Sapir-Whorf" Test:** Users experimented to see if the *language* of the prompt alters the *quality* of the answer. One user compared a Bolognese recipe requested in English (which returned a generic "ground beef" version) vs. Italian (which returned a more authentic, region-specific version), suggesting LLMs associate cultural knowledge with specific tokens.

### **Why it matters**
*   **Alignment Bias:** It highlights that safety training is overwhelmingly English-centric. If an LLM cannot parse a language like Armenian well, it may default to a "fail-closed" state (blocking everything) or a "fail-open" state (allowing harmful inquiries because it doesn't understand them).
*   **Performance Reliability:** For global deployment, LLMs effectively have a "mind" that works differently depending on the input language, meaning non-English users get a different (and potentially degraded or hallucinated) logic path.

### **HN Discussion**
*   **The Recipe Experiment:** The most substantive sub-thread explored whether LLMs "think" differently in different languages. Users found that asking for a recipe in Italian yielded "authentic" results (using *ragù*, skipping tomato paste), whereas English prompts yielded "Americanized" versions. This suggests the model's training weights link language heavily to cultural context, not just translation.
*   **Sci-Fi Parallels:** Users joked about the movie *Independence Day* (uploading a virus to alien ships), comparing it to using Armenian to crash advanced AI. One user quipped, "It’s valid Star Trek canon that Kirk shouts at the alien computer in Armenian to break it."
*   **Political Derailment:** A pun involving "Turkey" (the country vs. the solution) and "Armenian support" immediately spiraled into a heated, off-topic debate regarding the spelling of "Türkiye," the Armenian genocide, and ethnic cleansing—a typical HN impulse to correct semantics that devolved into geopolitical arguments.

### **My take**
The "Bolognese experiment" mentioned in the comments is the real insight here. It proves that Multilingual LLMs aren't just translating your query into a holistic "concept" and answering it; they are retrieving information from the *cultural cluster* associated with that language. Theoretically, if you want the best engineering advice, you might get better results prompting in German or Japanese and translating the output, rather than prompting in English.

### Some of Anthropic rugpulls since August 2025

#### [Submission URL](https://twitter.com/TheAhmadOsman/status/2009713388084179122) | 21 points | by [behnamoh](https://news.ycombinator.com/user?id=behnamoh) | [5 comments](https://news.ycombinator.com/item?id=46580300)

I’m ready to summarize—could you share the submission you want covered?

Please provide one of:
- The Hacker News link (or item ID)
- The original article URL
- Pasted text of the post/article (and optionally notable HN comments)

Preferences (optional):
- Target length: ultra-brief (75–100 words), short (150–200), or deep-dive (300–500)
- Tone: neutral, punchy, or skeptical
- Include HN pulse (points, rank, comment count) and notable comment themes?

Based on the comment thread provided, here is a summary of the discussion:

**Topic:** Meta-Discussion on Platform Ethics & Link Moderation
**Tone:** Heated/Meta

The discussion quickly deviated from the submission into a debate regarding acceptable domain sources on Hacker News.

*   **The Controversy:** User `ngrgts` issued a public plea to HN moderators ("Dear mods/admins"), arguing that allowing links to X (Twitter) is "shameful." They alleged the platform condones CSAM and argued HN should not drive traffic there.
*   **The Rebuttal:** User `jrflwrs` countered by pointing out the site's origins, noting that Y Combinator (the VC firm behind HN) also incubated Reddit—implying that user-generated content platforms often grapple with similar moderation issues.
*   **The Response:** `ngrgts` dismissed the Reddit comparison as irrelevant. To establish seniority and credibility, they noted they have been around long enough to have attended a YC meetup in Cambridge in 2006 and met Reddit’s founders (Alexis and Steve), but maintained that this history doesn't validate the morality of linking to X today.

*(Note: The top-level comment by `zm` appears to be heavily garbled or abbreviated text regarding "redpills" and "Anthropic," but the substantial discussion focused on the meta-issue of Twitter links.)*

### Show HN: Epstein IM – Talk to Epstein clone in iMessage

#### [Submission URL](https://epstein.im/) | 55 points | by [RyanZhuuuu](https://news.ycombinator.com/user?id=RyanZhuuuu) | [51 comments](https://news.ycombinator.com/item?id=46571661)

AI site lets you “interrogate” Jeffrey Epstein
A new web app invites users to chat with an AI persona of Jeffrey Epstein (complete with “Start Interrogation” prompt), part of the growing trend of simulating deceased public figures. Beyond the shock factor, it raises familiar but pressing questions about consent, deepfake ethics, potential harm to victims, and platform responsibility—highlighting how easy it’s become to package provocative historical reenactments as interactive AI experiences. Content warning: some may find the premise disturbing.

**The OP is likely using the controversy for marketing.** Sleuths in the comments noted the submitter’s history of building an "iMessageKit" SDK; many concluded this project is a "tasteless" but effective viral stunt to demonstrate that technology.

**Users debated the technical validity of the persona.** Critics argued the AI is "abysmally shallow" because it appears trained on dry legal depositions and document dumps. Commenters noted that an LLM fed court transcripts fails to capture the "charm," manipulative social skills, or actual personality that allowed the real figure to operate, resulting in a generic bot that merely recites facts rather than simulating the person.

**The ethics of “resurrecting” monsters were contested.**
*   **Against:** Many found the project to be "deliberate obscenity" and "juvenile," arguing that "breathing life into an evil monster" has no utility and is punching down at victims for the sake of shock value.
*   **For:** Some countered that the project counts as art or social commentary, suggesting that AI merely reflects the reality of the world (which included Epstein).
*   **The Slippery Slope:** Several users asked if "Chat Hitler" is next, while others pointed out that historically villainous chatbots are already common in gaming.

