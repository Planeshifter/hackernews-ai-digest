## AI Submissions for Sun Dec 10 2023 {{ 'date': '2023-12-10T17:10:25.923Z' }}

### I wrote a meta mode for ChatGPT

#### [Submission URL](https://www.novaspivack.com/technology/nova-mode-the-ultimate-chatgpt-custom-instruction) | 175 points | by [airesearcher](https://news.ycombinator.com/user?id=airesearcher) | [76 comments](https://news.ycombinator.com/item?id=38594521)

Nova Spivack has developed a new feature called Nova Mode 1.0 that supercharges ChatGPT, a popular language model. Nova Mode enhances the functionality of ChatGPT by allowing users to iteratively edit and refine ideas more effectively. It introduces message numbering, enabling users to easily refer to previous messages and create new versions or combine multiple messages. Nova Mode also introduces short commands prefixed with "//" that perform various useful tasks, such as distilling key points from a message set or expanding a message. Users can even create their own commands to automate tasks within ChatGPT. To enable Nova Mode, users can copy and paste the provided custom instructions into the settings of ChatGPT. It's important to note that Nova Mode works best with the GPT 4.0 model, and users will need a ChatGPT Pro subscription for optimal results. Overall, Nova Mode revolutionizes the way users interact with ChatGPT, making it a powerful tool for idea generation and content refinement.

The discussion around Nova Spivack's Nova Mode 1.0 feature for ChatGPT on Hacker News covers a range of topics. Some users express skepticism about the usefulness of the number slash commands and suggest that they may just clutter the conversation. There's also a discussion about the difference between building tools like ChatGPT and directly using it, with some users struggling to see the value in building on top of ChatGPT. Others point out that Nova Mode allows for more efficient content development and refinement by iterating and combining messages. The topic of character limits in custom instructions is also brought up, with users discussing the challenges of fitting instructions within the limit. Some users suggest using specific prompts to generate more concise and accurate responses. There's also a discussion about the power and functionality of Nova Mode, with users highlighting its ability to reference previous messages and perform tasks like merging messages. The conversation touches on the effectiveness of embeddings and the relevance of techniques like RAG (Retrieve and Generate) for adjusting semantic space and bringing context to information retrieval. Overall, there seems to be a mix of opinions regarding the usefulness and potential limitations of Nova Mode.

### Mistral AI Valued at $2B

#### [Submission URL](https://www.unite.ai/paris-based-startup-and-openai-competitor-mistral-ai-valued-at-2-billion/) | 292 points | by [marban](https://news.ycombinator.com/user?id=marban) | [220 comments](https://news.ycombinator.com/item?id=38593616)

Paris-based startup Mistral AI has raised â‚¬450 million ($2 billion) in funding, giving the company a valuation of $2 billion. Leading the investment round is Andreessen Horowitz, with additional contributions from Nvidia Corp and Salesforce. Mistral AI is known for its flagship product, Mistral 7B, which is a large language model that employs customized training and tuning methods. This funding will allow Mistral AI to further its research and development efforts and solidify its position in the AI industry. The substantial investment demonstrates the growing recognition of the strategic importance of AI technologies and highlights the increasing competition in the field. Mistral AI's success also signifies the rising prominence of the European AI landscape. With its open-source approach and focus on scalability and efficiency, Mistral AI is positioning itself as a strong competitor to established AI giants like OpenAI. The funding round is a transformative moment for the European AI sector, demonstrating its potential for innovation and investment.

The discussion around the Mistral AI funding announcement on Hacker News involves various topics related to language models and AI development. Some commenters mentioned other language models like GPT-4 and Yi-34B, comparing their capabilities and potential improvements. There were discussions about the limitations of AI models and the challenges in training them effectively. Some users pointed out the importance of diverse training data and the potential bias that can arise from it. Another topic discussed was the difference between AI models and human understanding, with some expressing skepticism about AI's ability to replicate human cognition. The cost and investment required for AI research and development were also mentioned, with some comparing it to the advancements in transistor technology and the cost reduction over time. The discussion highlights a range of perspectives on the potential and limitations of AI models like Mistral's Mistral 7B.

### Mistral-8x7B-Chat

#### [Submission URL](https://huggingface.co/mattshumer/mistral-8x7b-chat) | 122 points | by [birriel](https://news.ycombinator.com/user?id=birriel) | [69 comments](https://news.ycombinator.com/item?id=38594578)

Introducing Mistral-8x7B: A Poetic AI Assistant

Today, we bring you an exciting new AI model built on top of the Mistral MoE architecture. Meet Mistral-8x7B, a powerful chat model trained on the SlimOrca dataset for one epoch, using QLoRA. With the Mistral-8x7B model, you can experience a conversational AI assistant like never before!

To get started, you can easily import the Mistral-8x7B model using the following code snippet:

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("mattshumer/mistral-8x7b-chat", low_cpu_mem_usage=True, device_map="auto", trust_remote_code=True)
tokenizer = AutoTokenizer.from_pretrained("mattshumer/mistral-8x7b-chat")

prompt = "user Write me a poem about AI."
input_ids = tokenizer.encode(prompt, return_tensors="pt").cuda()

generated_output = model.generate(input_ids, max_new_tokens=512).cpu()
decoded_output = tokenizer.batch_decode(generated_output)
print(decoded_output)
```

As you can see, it's straightforward to generate a response from the Mistral-8x7B model using a given prompt. In this example, the prompt requests a poem about AI.

Notably, Mistral-8x7B was trained with the Axolotl training process, utilizing six H100 GPUs for a total of nine hours. This rigorous training process ensures high-quality responses and a rich conversational experience.

The Mistral-8x7B model has gained popularity, with 79 downloads in the last month alone. Its efficient design allows for optimal disk space usage.

So, whether you need assistance, engaging in a friendly chat, or looking for a poetic masterpiece, Mistral-8x7B is here to enhance your AI experience. Try it out and let your conversations flow!

The discussion on this submission covers a range of topics related to the Mistral-8x7B AI model and AI development in general. Some key points from the discussion include:

- One user points out the trade-offs and limitations of centralized testing versus decentralized testing. They argue that models like GPT-4 may perform well on centralized benchmarks but may not reflect their real-world performance.

- There is a discussion about ranking language models and the relevance of the Elo scoring system used in chess. The conversation revolves around the importance of different metrics in evaluating models and the potential similarities between Elo ratings and the performance of AI models.

- Some users discuss the benefits of using LLMs (Large Language Models) for specific tasks like IRC bots or personal projects. The advantages of distributed and decentralized models are also highlighted.

- The usefulness of Mistral-8x7B and its availability on platforms like OpenRouter and OpenAI's Chat Playground is mentioned. Some users express their satisfaction with Mistral-8x7B, while others mention alternative AI models or issues with certain hardware configurations.

- There is a brief discussion about Apple's release of a broken CUDA-only package, and some users suggest alternative hardware and solutions for running LLMs.

Overall, the discussion encompasses various aspects of AI model development, performance evaluation, hardware compatibility, and user experiences with different AI models.

### Why Tesla Autopilot shouldn't be used in as many places as you think

#### [Submission URL](https://www.washingtonpost.com/technology/2023/12/10/tesla-autopilot-cross-traffic/) | 27 points | by [tallowen](https://news.ycombinator.com/user?id=tallowen) | [25 comments](https://news.ycombinator.com/item?id=38593095)

Tesla's Autopilot technology has been involved in approximately 40 fatal and serious car crashes, including at least eight that occurred on roads where the driver-assistance feature was not designed to be used, according to a Washington Post analysis. Despite federal officials requesting Tesla to limit Autopilot use to highways with center medians and no cross traffic, the company has largely ignored these requests. Tesla argues that Autopilot use should be at the discretion of drivers, as stated in their user manual. However, experts suggest that many drivers are unaware of the technology's limitations as they often do not read the extensive manuals.

The discussion on Hacker News regarding the submission about Tesla's Autopilot technology revolves around different perspectives and opinions. Here are the main points highlighted in the comments:

1. Some users argue that the statistics mentioned in the Washington Post analysis don't necessarily prove that Autopilot is responsible for the crashes, as human drivers are statistically more dangerous. They suggest that the blame falls on the drivers' responsibility.

2. Others point out that the issue is not just about the technology itself, but also about how Tesla markets and sells it. They argue that the company should make more effort to educate users about the limitations of Autopilot and ensure that they are aware of the user manual.

3. Some users express skepticism towards Tesla's claims and marketing tactics, suggesting that Elon Musk often makes grand claims about full self-driving capabilities that are not fully realized.

4. There is a debate about the upgradeability of Tesla's Autopilot system, with some users mentioning that Tesla started including Autopilot hardware in its cars in 2014 and later offered retrofit upgrades. However, there are differing opinions on the compatibility of these upgrades with different versions of Tesla vehicles.

5. Users discuss the responsibility of Tesla as a manufacturer and suggest that there should be stricter scrutiny on the company's autonomous driving features.

6. Some users compare Tesla's Autopilot to other driver-assist systems and emphasize the importance of understanding the limitations and using them appropriately.

7. A few users criticize the credibility of the Washington Post article, calling it garbage or biased.

8. There is also a mention of Tesla's vertical integration strategy and the challenges that come with it.

Overall, the discussion highlights concerns about Tesla's Autopilot technology, the responsibility of the company, and the need for better education and regulation in this area.

