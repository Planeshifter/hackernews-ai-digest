## AI Submissions for Mon May 08 2023 {{ 'date': '2023-05-08T17:10:30.096Z' }}

### GPU vendor-agnostic fluid dynamics solver in Julia

#### [Submission URL](https://b-fg.github.io/2023/05/07/waterlily-on-gpu.html) | 219 points | by [moelf](https://news.ycombinator.com/user?id=moelf) | [90 comments](https://news.ycombinator.com/item?id=35861435)

WaterLily.jl, a pure Julia fluid simulator, has successfully ported its solver from a serial CPU execution to a backend-agnostic execution that includes multi-threaded CPU and GPU from different vendors (NVIDIA and AMD) thanks to KernelAbstractions.jl (KA). Using the @kernel macro from KA, the team was able to generate the divergence operator using KernelAbstractions by defining a divergence kernel and a wrapper function. To automate the generation of loops, the team defined the macro @loop, which generates loops over CartesianIndices ranges automatically. Additionally, this approach can be used to generate KA kernels for each loop in the code. The extended abstract preprint with benchmarking details regarding this port can be found on arXiv.

The comments discussed the advantages and disadvantages of Julia as a language compared to Python and other languages, as well as the challenges of programming for simulations and the benefits of using GPU acceleration. Many users expressed interest in trying out the new code and in working with Julia for machine learning and other projects.

### Early Artificial Intelligence Projects: A Student Perspective (2006)

#### [Submission URL](https://projects.csail.mit.edu/films/aifilms/AIFilms.html) | 58 points | by [onemind](https://news.ycombinator.com/user?id=onemind) | [7 comments](https://news.ycombinator.com/item?id=35857070)

This article is a retrospective look at the early days of artificial intelligence, exploring its definition, foundational concepts, and major projects at MIT and in the US. The article begins with John McCarthy's definition of AI, which is the science of making intelligent machines that can mimic human thought, feelings, and decision-making. It explains that AI has not had a linear progression, but rather has grown in many directions along the intertwining world wide web. The article then delves into the foundational concepts of AI, such as programmable machines that can solve equations, and the development of early computers. Moving along, the article explores major AI projects during the decades following the creation of the term "AI," from the 1950s to the present day, including the emergence of search engines, spell checkers, and spam filters.

One user mentions studying AI in the past, including the use of programming languages, such as Prolog and Lisp, during the 90s. Another user shares their experience of shifting their focus to neural networks and how technology changes over time. The topic of machine learning and its ability to surpass human performance in certain tasks is mentioned. The concept of intelligence being computationally attainable is also discussed. Finally, another user shares an article on an AI-generated system that demonstrates intelligence comparable to humans.

### Giving GPT “Infinite” Knowledge

#### [Submission URL](https://sudoapps.substack.com/p/giving-gpt-infinite-knowledge) | 116 points | by [sudoapps](https://news.ycombinator.com/user?id=sudoapps) | [83 comments](https://news.ycombinator.com/item?id=35864698)

Large Language Models (LLMs) like OpenAI's GPT can provide accurate responses to information retrieval questions if fed with relevant real-time data for interpretation. However, a limitation on the number of tokens for the initial prompt or response to generate results restricts LLMs from ingesting large amounts of data directly. To overcome this hurdle, data is converted into embeddings - vector representations of a string - and stored in vector databases. When a user asks a question, similarity search is done for relevant information with only the essential pieces related to the question being injected into the LLM prompt before answering the question. While there are token limitations, string compression techniques can help accommodate more data within the limit.

The discussion on this submission consists of a variety of viewpoints on the capabilities and limitations of Large Language Models (LLMs), particularly in their ability to comprehend and analyze data beyond the token limitations. Some commenters suggest that embeddings-based search is a viable solution that allows for relevant data to be retrieved within the token limit, while others express concerns about the effectiveness of such an approach. There are also discussions about the potential of training larger models with real-time data to further enhance LLMs' performance and capabilities. Additionally, there are debates about the practical application of LLMs and their scalability, with some commenters expressing skepticism about their ability to achieve true artificial general intelligence (AGI) and others suggesting that advancements in technology may make it possible in the future.

### RasaGPT: First headless LLM chatbot built on top of Rasa, Langchain and FastAPI

#### [Submission URL](https://github.com/paulpierre/RasaGPT) | 171 points | by [riter](https://news.ycombinator.com/user?id=riter) | [108 comments](https://news.ycombinator.com/item?id=35859344)

Introducing RasaGPT, the first headless LLM chatbot platform built on top of Rasa and Langchain. Rasa is a popular and easy-to-use chatbot framework with built-in NLU ML pipelines, while Langchain is an LLM library for indexing, retrieval, and context injection. RasaGPT provides a reference implementation of Rasa and Telegram utilizing LLM libraries, including Langchain, for a seamless chatbot experience. Additionally, RasaGPT offers full API documentation and features including document versioning and automatic re-training, async end-points and database models customization, and pgAdmin for database browsing.

Some users discuss their experiences implementing Langchain and offer potential solutions to common problems. There is also discussion about other language models and their applications. Additionally, there is discussion about LMQL, a language model query language. Some users provide links to relevant resources and articles about these topics.

### Alphabet plans to announce its new general-use LLM called PaLM 2 at Google I/O

#### [Submission URL](https://www.cnbc.com/2023/05/08/google-io-to-feature-ai-updates-showing-off-palm-2-llm.html) | 37 points | by [mfiguiere](https://news.ycombinator.com/user?id=mfiguiere) | [5 comments](https://news.ycombinator.com/item?id=35866435)

Google is set to unveil its latest artificial intelligence (AI) updates, including the launch of a general-use large language model (LLM) called PaLM 2, at its annual developer conference (Google I/O). The LLM, incorporating more than 100 languages, has undergone numerous creative writing, coding, and maths tests. Google will also announce advancements to its Bard and Search AI, including a new feature that allows a user to "create" an image based on entered text. The conference will also cover the company's progress with Workspace AI collaborator and image recognition tool Google Lens.

The first comment by "jggwtts" seems to be a prediction about Google's latest AI developments and their potential impact. Another user, "two_in_one", expands on the idea by stating that these developments will enable dystopian scenarios like targeted advertising and tailored content. "sekh60" adds to the discussion by pointing out that Google stopped targeting based on demographics years ago. "cynydz" offers a different perspective, suggesting that AI can be effective in completing sentences and matching patterns, but also has the potential to manipulate individuals and activities. Finally, "lphbttng" mentions the PaLM AI and describes it as advanced.

