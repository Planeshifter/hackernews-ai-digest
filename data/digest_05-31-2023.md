## AI Submissions for Wed May 31 2023 {{ 'date': '2023-05-31T17:14:11.288Z' }}

### Nvidia DGX GH200: 100 Terabyte GPU Memory System

#### [Submission URL](https://developer.nvidia.com/blog/announcing-nvidia-dgx-gh200-first-100-terabyte-gpu-memory-system/) | 532 points | by [MacsHeadroom](https://news.ycombinator.com/user?id=MacsHeadroom) | [360 comments](https://news.ycombinator.com/item?id=36133226)

NVIDIA has announced the DGX GH200, which delivers a generational leap in GPU memory performance and is designed to empower scientists to solve extraordinary challenges. The DGX GH200 pairs the NVIDIA Grace Hopper Superchip with the NVLink Switch System to unite up to 256 GPUs in a single system, providing 144 terabytes of memory that has access to the GPU shared memory programming model at high speed over NVLink. This breakthrough in GPU-accelerated computing paves the way for giant, trillion-parameter AI models that can't be solved on today's best supercomputers. The comments discuss the system's capabilities and compare it to Google's TPUv4. Some also explore the topic of artificial intelligence in general, its limitations, and the way it impacts various facets of businesses and customers.

### Japan’s government will not enforce copyrights on data used in AI training

#### [Submission URL](https://technomancers.ai/japan-goes-all-in-copyright-doesnt-apply-to-ai-training/) | 446 points | by [version_five](https://news.ycombinator.com/user?id=version_five) | [359 comments](https://news.ycombinator.com/item?id=36144241)

Japan has announced that it will not enforce copyrights on data used in AI training, allowing AI to use any data, regardless of its source or intended use. While the move has received pushback from some of Japan's artists, many in academia and the business sector are hoping to leverage the country's relaxed data laws to become a major global player in AI technology. With the lowest per-capita income in the G-7 and years of low growth, the implementation of AI could potentially boost Japan's GDP by 50% or more in a short time. The move also adds a new dynamic to the ongoing regulation debate, as the world's third-largest economy is saying it won't hinder AI research and development, and is prepared to leverage this new technology to compete directly with the West.

In the comments, there is a discussion about the implications of this decision. Some users argue that allowing AI to use copyrighted material without permission is not fair to the original authors, while others believe that the benefits of advancing AI technology outweigh the potential negative impact on copyright holders. One user suggests that the issue is not with the existing copyright law, but with regulating the industry as a whole. Others argue that humans learned and studied from existing culture and literature, and that the same should apply for AI. One user suggests that lossy training data has value, and copyright holders launching a lawsuit could prevent people from accessing valuable data. Another proposes that copyright is important and is different for machines than for humans. Overall, there are arguments made both for and against the relaxation of copyright laws for AI training purposes.

### OpenAI's plans according to sama

#### [Submission URL](https://humanloop.com/blog/open_ai_talk) | 297 points | by [razcle](https://news.ycombinator.com/user?id=razcle) | [237 comments](https://news.ycombinator.com/item?id=36141544)

Last week, OpenAI's Sam Altman met with developers to discuss the company's API and product plans. The main takeaway is that the company is heavily GPU-limited, causing delays in its short-term plans. To improve the API reliability and speed, OpenAI's top priority is cheaper and faster GPT-4. Additionally, the company will extend the finetuning API to the latest models, develop a stateful API and work on multimodality for GPT-4. OpenAI will also avoid competing with its customers and is considering open-sourcing GPT-3 while calling for regulation of future models. Finally, the scaling laws for model performance continue to hold, leading to significant implications for AGI development.

In the comments, there is discussion about OpenAI's potential open-sourcing of GPT-3, avoiding competition with customers, and considering regulations for future models. There is also debate about the benefits and drawbacks of small teams sourcing big tech, as well as discussion around the need for open-source community efforts to stabilize the diffusion of GPT models. Some commenters question the efficiency of GPUs, while others discuss the challenges of making OpenAI products accessible to developers.

### Show HN: Lance – Alternative to Parquet for ML data

#### [Submission URL](https://github.com/lancedb/lance) | 78 points | by [chop](https://news.ycombinator.com/user?id=chop) | [16 comments](https://news.ycombinator.com/item?id=36144450)

Lance, a modern columnar data format optimized for machine learning workflows and datasets, has been released. It offers faster random access, vector search, data versioning, and ecosystem integrations with PyArrow, Pandas, Polars, DuckDB, and more. Lance is suitable for building search engines, storing and querying deeply nested data, and large-scale ML training. It claims to be 100 times faster than Parquet without sacrificing scan performance. Lance also enables users to manage versions of their data without requiring extra infrastructure.

The comments discuss Lance's benefits for various applications, such as building search engines, storing and querying deeply nested data, and large-scale ML training. One user compares Lance to the Parquet format and another raises the question of Lance's compatibility with Java. There are also discussions around the Lance file directory structure, its implementation in different languages, and its compatibility with different systems. A user also points out issues with accessing Lance's Github page.

### AI21 Labs concludes largest Turing Test experiment to date

#### [Submission URL](https://www.ai21.com/blog/human-or-not-results) | 96 points | by [kennyfrc](https://news.ycombinator.com/user?id=kennyfrc) | [39 comments](https://news.ycombinator.com/item?id=36137897)

AI21 Labs has conducted the largest Turing Test in history with more than 10 million conversations having been conducted in their social Turing game "Human or Not?" in which participants guess whether they are talking to a human or a machine. The initial results found that 68% of people guessed correctly, with people more successfully differentiating between humans and AI bots when talking to humans (73%) than bots (60%). Furthermore, the data shows that younger age groups tend to have correct guesses at slightly higher rates than older age groups, and that participants made certain assumptions about AI bots, such as they don't make typos or use slang, aren't aware of current events, and that they have no personal background. The discussion includes comments about the limitations of the test and the strategies used by AI models to target weaknesses. Some participants found the test confusing or limited in its scope. Others found it entertaining and worth playing.

### AI intensifies fight against ‘paper mills’ that churn out fake research

#### [Submission URL](https://www.nature.com/articles/d41586-023-01780-w) | 189 points | by [headalgorithm](https://news.ycombinator.com/user?id=headalgorithm) | [172 comments](https://news.ycombinator.com/item?id=36139349)

Artificial intelligence (AI) is posing challenges to publishers in their efforts to tackle the problem of paper mills, which are companies that produce fake scientific papers to order. Generative AI tools, such as chatbots and image-generating software, create ways of producing paper-mill content that can be difficult to detect. Experts at a recent research-integrity summit discussed synthetic images and text as pose a similar challenge. The event was convened by the Committee on Publication Ethics and the International Association of Scientific, Technical and Medical Publishers, and brought together researchers, independent research-integrity analysts, funders and publishers.

The comments discuss various related issues, including the problems with the current scientific system, the importance and challenges of peer review, and the limitations of peer review in catching errors in experimental and statistical analyses. There is also discussion on the need for better data sharing practices and the challenges of reproducing research results.

### AI camera with no lens

#### [Submission URL](https://www.theprompt.io/p/ai-camera-no-lens) | 280 points | by [anitakirkovska](https://news.ycombinator.com/user?id=anitakirkovska) | [81 comments](https://news.ycombinator.com/item?id=36139729)

A new AI camera called Paragraphica is changing the game of photography by taking photos using GPS data instead of a lens. The camera collects various data from its location through open APIs such as current weather, time of day, address, and nearby places to create a detailed description of the current place and moment. It then converts this description into an AI-generated "photo" using Stable Diffusion. Meanwhile, Google's latest speech model, SoundStorm, is challenging other AI-generated speech models by creating 30 seconds of speech in half a second and specializing in highly realistic dialogues. Other developments in the AI world include Nvidia's recent announcement of their DGX GH200 AI supercomputer, GitHub's One-Click DeepFake, and Andrew Karpathy's latest talk at the Microsoft Build conference on the state of GPT.

However, some commenters on Hacker News pointed out that a similar idea had been introduced in the 2000s using a camera that matched GPS coordinates and time of day with Flickr images. Others discussed the merits of traditional photography vs. AI-generated images. In addition to the Paragraphica camera, Google's SoundStorm speech model, NVIDIA's DGX GH200 AI supercomputer, GitHub's One-Click DeepFake, and Andrew Karpathy's latest talk on GPT for the Microsoft Build conference were also mentioned. Some commenters expressed concerns about the potential for AI-generated sketches to perpetuate racism, while others shared their experiences with traditional sketch artists and the limitations of human memory in capturing details. One user also mentioned the potential for AI-generated sketches to enhance police sketches.

### Train Your Own Private ChatGPT Model for the Cost of a Starbucks Coffee

#### [Submission URL](https://medium.com/@ApacheDolphinScheduler/train-your-own-private-chatgpt-model-for-the-cost-of-a-starbucks-coffee-25c588f450ee) | 72 points | by [DSOfficial](https://news.ycombinator.com/user?id=DSOfficial) | [16 comments](https://news.ycombinator.com/item?id=36136318)

With just the cost of a Starbucks coffee and a couple of hours of your time, you can now train your own open-source large-scale model using Apache DolphinScheduler. This model can be fine-tuned to enhance various skills, such as medical, programming, stock trading, and love advice, thus making it more "understanding" of you. With this, you can have your personal AI assistant that understands and responds to you better. Apache DolphinScheduler provides one-click support for training, tuning, and deploying open-source large-scale models, making it accessible to anyone interested in GPT and not just AI professionals. The whole process takes only three steps and around 20 hours of running time to build your ChatGPT model that understands and responds to you better.

The discussion on this submission covers a range of topics related to training and using open-source large-scale models. There is some disagreement about the effectiveness of self-training versus proprietary methods developed by companies like Microsoft. Some commenters suggest that domain-specific knowledge is important for organizations and individuals looking to train their own models. Others discuss the technical details of building and training models. One commenter suggests that there may be privacy implications to using a personal AI assistant, and another questions the use of the term "private" in relation to ChatGPT. In the end, the main point of the submission is that it is possible to train your own model using Apache DolphinScheduler for just a small investment in time and money.

