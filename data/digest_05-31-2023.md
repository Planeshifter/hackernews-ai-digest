## AI Submissions for Wed May 31 2023 {{ 'date': '2023-05-31T17:14:11.288Z' }}

### Nvidia DGX GH200: 100 Terabyte GPU Memory System

#### [Submission URL](https://developer.nvidia.com/blog/announcing-nvidia-dgx-gh200-first-100-terabyte-gpu-memory-system/) | 532 points | by [MacsHeadroom](https://news.ycombinator.com/user?id=MacsHeadroom) | [360 comments](https://news.ycombinator.com/item?id=36133226)

NVIDIA has announced the DGX GH200, which delivers a generational leap in GPU memory performance and is designed to empower scientists to solve extraordinary challenges. The DGX GH200 pairs the NVIDIA Grace Hopper Superchip with the NVLink Switch System to unite up to 256 GPUs in a single system, providing 144 terabytes of memory that has access to the GPU shared memory programming model at high speed over NVLink. This breakthrough in GPU-accelerated computing paves the way for giant, trillion-parameter AI models that can't be solved on today's best supercomputers.

NVIDIA has unveiled the DGX GH200, a new system that combines the NVIDIA Grace Hopper Superchip with the NVLink Switch System to allow up to 256 GPUs to be united in one system, providing 144 terabytes of memory. This development is a breakthrough in GPU-accelerated computing that could pave the way for giant AI models. The comments discuss the system's capabilities and compare it to Google's TPUv4. Some also explore the topic of artificial intelligence in general, its limitations, and the way it impacts various facets of businesses and customers.

### Directly compiling Scheme to WebAssembly: lambdas, recursion, iteration

#### [Submission URL](https://spritely.institute/news/scheme-to-wasm-lambdas-recursion.html) | 139 points | by [rekado](https://news.ycombinator.com/user?id=rekado) | [22 comments](https://news.ycombinator.com/item?id=36135844)

The Spritely Institute's Guile on WebAssembly project, also known as Hoot, has successfully compiled several standard Scheme procedures directly to WebAssembly without any intermediate virtual machine. The project provides a mini WebAssembly toolkit and has its own suite of tools for parsing, assembling, disassembling, and dumping WebAssembly directly, making it a valuable tool for other languages. Though still in its early stages, Hoot's compiled programs are showing promising results in terms of speed and efficiency. With this project, the team hopes to provide more options for programmers to use their preferred programming languages in web development.

The Spritely Institute's Guile on WebAssembly project, Hoot, has compiled several standard Scheme procedures directly to WebAssembly. Though still in the early stages, the project shows promising results in terms of speed and efficiency. The team hopes it will provide more options for programmers to use their preferred programming languages in web development. The discussion revolved around the use of Scheme in web development, with some commenters suggesting Racket, Gambit, or other Lispy languages. Some commenters mentioned that historically, there has been little adoption of Scheme in production-grade environments. Others discussed their experiences using Scheme-specific libraries and their hopes for standardization. There was also a tangent on Lisp's influence on programming languages like JavaScript and Python.

### Japanâ€™s government will not enforce copyrights on data used in AI training

#### [Submission URL](https://technomancers.ai/japan-goes-all-in-copyright-doesnt-apply-to-ai-training/) | 446 points | by [version_five](https://news.ycombinator.com/user?id=version_five) | [359 comments](https://news.ycombinator.com/item?id=36144241)

Japan has announced that it will not enforce copyrights on data used in AI training, allowing AI to use any data, regardless of its source or intended use. While the move has received pushback from some of Japan's artists, many in academia and the business sector are hoping to leverage the country's relaxed data laws to become a major global player in AI technology. With the lowest per-capita income in the G-7 and years of low growth, the implementation of AI could potentially boost Japan's GDP by 50% or more in a short time. The move also adds a new dynamic to the ongoing regulation debate, as the world's third-largest economy is saying it won't hinder AI research and development, and is prepared to leverage this new technology to compete directly with the West.

The submission states that Japan is not going to enforce copyrights on data used in AI training, allowing AI to use any data regardless of its source or intended use. In the comments, there is a discussion about the implications of this decision. Some users argue that allowing AI to use copyrighted material without permission is not fair to the original authors, while others believe that the benefits of advancing AI technology outweigh the potential negative impact on copyright holders. One user suggests that the issue is not with the existing copyright law, but with regulating the industry as a whole. Others argue that humans learned and studied from existing culture and literature, and that the same should apply for AI. One user suggests that lossy training data has value, and copyright holders launching a lawsuit could prevent people from accessing valuable data. Another proposes that copyright is important and is different for machines than for humans. Overall, there are arguments made both for and against the relaxation of copyright laws for AI training purposes.

### OpenAI's plans according to sama

#### [Submission URL](https://humanloop.com/blog/open_ai_talk) | 297 points | by [razcle](https://news.ycombinator.com/user?id=razcle) | [237 comments](https://news.ycombinator.com/item?id=36141544)

Last week, OpenAI's Sam Altman met with developers to discuss the company's API and product plans. The main takeaway is that the company is heavily GPU-limited, causing delays in its short-term plans. To improve the API reliability and speed, OpenAI's top priority is cheaper and faster GPT-4. Additionally, the company will extend the finetuning API to the latest models, develop a stateful API and work on multimodality for GPT-4. OpenAI will also avoid competing with its customers and is considering open-sourcing GPT-3 while calling for regulation of future models. Finally, the scaling laws for model performance continue to hold, leading to significant implications for AGI development.

OpenAI is heavily GPU-limited, which has caused delays in its short-term plans to improve API reliability and speed. The company's top priority is to develop a cheaper and faster GPT-4. OpenAI will also extend the fine-tuning API to its latest models, develop a stateful API, and work on multimodality for GPT-4. The scaling laws for model performance continue to hold, leading to significant implications for AGI development. In the comments, there is discussion about OpenAI's potential open-sourcing of GPT-3, avoiding competition with customers, and considering regulations for future models. There is also debate about the benefits and drawbacks of small teams sourcing big tech, as well as discussion around the need for open-source community efforts to stabilize the diffusion of GPT models. Some commenters question the efficiency of GPUs, while others discuss the challenges of making OpenAI products accessible to developers.

### FTC says Ring employees illegally surveilled customers, failed to stop hackers

#### [Submission URL](https://www.ftc.gov/news-events/news/press-releases/2023/05/ftc-says-ring-employees-illegally-surveilled-customers-failed-stop-hackers-taking-control-users) | 163 points | by [miles](https://news.ycombinator.com/user?id=miles) | [35 comments](https://news.ycombinator.com/item?id=36146062)

The Federal Trade Commission (FTC) has accused home security camera company Ring of violating its customers' privacy by allowing unauthorized access to their private videos and failing to implement necessary security measures, which enabled hackers to take control of consumers' accounts. In the complaint, the FTC alleges that Ring used and reviewed customers' videos without their consent, and failed to implement safeguards to prevent employees from accessing private data. Ring has agreed to pay $5.8m in customer refunds and will be prohibited from profiting from illegal access to consumer videos.

Home security camera company Ring has agreed to pay $5.8m in customer refunds after the Federal Trade Commission accused the business of violating the privacy of its customers by allowing unauthorized access to private videos. The company is also accused of reviewing customers' videos without their consent and failing to implement necessary security measures. The comments on the article largely discuss the importance of privacy and security measures with IoT devices. Some express concerns over the power that technology companies have and whether current regulations are effective in protecting consumers from breaches of privacy and security, while others discuss the punishment for violating privacy laws.

### Show HN: Lance â€“ Alternative to Parquet for ML data

#### [Submission URL](https://github.com/lancedb/lance) | 78 points | by [chop](https://news.ycombinator.com/user?id=chop) | [16 comments](https://news.ycombinator.com/item?id=36144450)

Lance, a modern columnar data format optimized for machine learning workflows and datasets, has been released. It offers faster random access, vector search, data versioning, and ecosystem integrations with PyArrow, Pandas, Polars, DuckDB, and more. Lance is suitable for building search engines, storing and querying deeply nested data, and large-scale ML training. It claims to be 100 times faster than Parquet without sacrificing scan performance. Lance also enables users to manage versions of their data without requiring extra infrastructure.

A new data format optimized for machine learning workflows and datasets, called Lance, was released. According to the submission, it offers faster random access, vector search, and data versioning at a speed 100 times faster than Parquet. The comments discuss Lance's benefits for various applications, such as building search engines, storing and querying deeply nested data, and large-scale ML training. One user compares Lance to the Parquet format and another raises the question of Lance's compatibility with Java. There are also discussions around the Lance file directory structure, its implementation in different languages, and its compatibility with different systems. A user also points out issues with accessing Lance's Github page.

### AI21 Labs concludes largest Turing Test experiment to date

#### [Submission URL](https://www.ai21.com/blog/human-or-not-results) | 96 points | by [kennyfrc](https://news.ycombinator.com/user?id=kennyfrc) | [39 comments](https://news.ycombinator.com/item?id=36137897)

AI21 Labs has conducted the largest Turing Test in history with more than 10 million conversations having been conducted in their social Turing game "Human or Not?" in which participants guess whether they are talking to a human or a machine. The initial results found that 68% of people guessed correctly, with people more successfully differentiating between humans and AI bots when talking to humans (73%) than bots (60%). Furthermore, the data shows that younger age groups tend to have correct guesses at slightly higher rates than older age groups, and that participants made certain assumptions about AI bots, such as they don't make typos or use slang, aren't aware of current events, and that they have no personal background.

AI21 Labs conducted the largest Turing Test in history with over 10 million conversations in their "Human or Not?" social Turing game where participants guess whether they are talking to a human or a machine. The initial results showed that 68% of people guessed correctly. Participants made certain assumptions about AI bots, such as they don't make typos or use slang, aren't aware of current events, and have no personal background. The discussion includes comments about the limitations of the test and the strategies used by AI models to target weaknesses. Some participants found the test confusing or limited in its scope. Others found it entertaining and worth playing.

### AVX512 intrinsics for JDKâ€™s Arrays.sort methods

#### [Submission URL](https://github.com/openjdk/jdk/pull/14227) | 158 points | by [mfiguiere](https://news.ycombinator.com/user?id=mfiguiere) | [144 comments](https://news.ycombinator.com/item?id=36135533)

OpenJDK has unveiled a new version that uses AVX512 instructions for Arrays.sort methods, promising substantial speed-ups. The enhancement provides a ten-fold improvement for int, long, float and double arrays using x86_64 CPUs. The performance data provided by OpenJDK shows that this PR exhibits a performance boost of 13x for 32-bit datatype arrays and 8x for 64-bit datatype arrays. The new code also handles array sizes of 50 and 10, promising further improvements to its predecessor.

OpenJDK has announced a new version that uses AVX512 instructions for its Arrays.sort methods. The enhancement provides a ten-fold improvement for int, long, float and double arrays using x86_64 CPUs. The discussion features talks on processor performance, benchmarking challenges, the impact of AVX-512 on Intel and AMD consumer chips, and the difficulty of working with low-level vector assemblers. Some commenters even suggest that in general-purpose messy compute kernels, CBOR and JSON are more important than SIMD for optimizations. AVX-512 features such as the masked registers, pattern recognition, and vectorization are difficult to follow, and AVX-512's performance is challenging to optimize compared to its more straightforward predecessor. While AVX-512 has some incredible features, the discussion points out that working with its low-level vector assemblers remains elusive to many programmers.

### AI intensifies fight against â€˜paper millsâ€™ that churn out fake research

#### [Submission URL](https://www.nature.com/articles/d41586-023-01780-w) | 189 points | by [headalgorithm](https://news.ycombinator.com/user?id=headalgorithm) | [172 comments](https://news.ycombinator.com/item?id=36139349)

Artificial intelligence (AI) is posing challenges to publishers in their efforts to tackle the problem of paper mills, which are companies that produce fake scientific papers to order. Generative AI tools, such as chatbots and image-generating software, create ways of producing paper-mill content that can be difficult to detect. Experts at a recent research-integrity summit discussed synthetic images and text as pose a similar challenge. The event was convened by the Committee on Publication Ethics and the International Association of Scientific, Technical and Medical Publishers, and brought together researchers, independent research-integrity analysts, funders and publishers.

The article discusses how artificial intelligence (AI) is causing problems for publishers trying to tackle paper mills that produce fake scientific papers. The comments discuss various related issues, including the problems with the current scientific system, the importance and challenges of peer review, and the limitations of peer review in catching errors in experimental and statistical analyses. There is also discussion on the need for better data sharing practices and the challenges of reproducing research results.

### Show HN: Reddit Firehose

#### [Submission URL](https://hotgarbo.github.io/dumppit/) | 22 points | by [HotGarbage](https://news.ycombinator.com/user?id=HotGarbage) | [6 comments](https://news.ycombinator.com/item?id=36143803)

I'm sorry, I'm not sure what you mean by ðŸš½. Could you please clarify?

The submission seems to contain some cryptic language and symbols, including ðŸš½, which someone asked for clarification on. One user commented that the Reddit post may be NSFW and that disabling the NSFW filter on Reddit can lead to some unexpected content. Others discussed the inclusion of NSFW content on Reddit and the tagging system for such content. One user expressed concern that the post may contain illegal content and suggested using legal moderation services.

### Box64 and RISC-V

#### [Submission URL](https://box86.org/2023/05/box64-and-risc-v/) | 122 points | by [ekianjo](https://news.ycombinator.com/user?id=ekianjo) | [41 comments](https://news.ycombinator.com/item?id=36135190)

Box64, a Linux emulator for running x86-64 applications on ARM64 devices, is adding dynarec support for the RISC-V architecture. The emulator maps the x86-64 instructions to RISC-V instructions on a 1:1 basis, while skipping expensive computations where possible to keep the code small and fast. While x86-64 is a CISC architecture and RISC-V is a RISC architecture, dynarec support ensures compatibility between the two architectures, allowing games to be played on a RISC-V device like the VisionFive2 using Box64.

Box64, a Linux emulator for running x86-64 applications on ARM64 devices, is adding dynarec support for the RISC-V architecture. The emulator maps the x86-64 instructions to RISC-V instructions on a 1:1 basis, while skipping expensive computations where possible to keep the code small and fast. Users were discussing the speed at which x86_64 programs can run on RISC-V and how it compares with current Intel and AMD processors. Some users pointed out that current RISC-V boards run games at a quarter of the speed of Intel and AMD. However, users also stated that RISC-V boards are getting faster quickly. Some users also discussed the use of TPM security chips and open-source alternatives to them, as well as the implementation of dynarec and binary translation in box64 and other emulators.

### AI camera with no lens

#### [Submission URL](https://www.theprompt.io/p/ai-camera-no-lens) | 280 points | by [anitakirkovska](https://news.ycombinator.com/user?id=anitakirkovska) | [81 comments](https://news.ycombinator.com/item?id=36139729)

A new AI camera called Paragraphica is changing the game of photography by taking photos using GPS data instead of a lens. The camera collects various data from its location through open APIs such as current weather, time of day, address, and nearby places to create a detailed description of the current place and moment. It then converts this description into an AI-generated "photo" using Stable Diffusion. Meanwhile, Google's latest speech model, SoundStorm, is challenging other AI-generated speech models by creating 30 seconds of speech in half a second and specializing in highly realistic dialogues. Other developments in the AI world include Nvidia's recent announcement of their DGX GH200 AI supercomputer, GitHub's One-Click DeepFake, and Andrew Karpathy's latest talk at the Microsoft Build conference on the state of GPT.

A new AI camera called Paragraphica is changing the game of photography by taking photos using GPS data instead of a lens. However, some commenters on Hacker News pointed out that a similar idea had been introduced in the 2000s using a camera that matched GPS coordinates and time of day with Flickr images. Others discussed the merits of traditional photography vs. AI-generated images. In addition to the Paragraphica camera, Google's SoundStorm speech model, NVIDIA's DGX GH200 AI supercomputer, GitHub's One-Click DeepFake, and Andrew Karpathy's latest talk on GPT for the Microsoft Build conference were also mentioned. Some commenters expressed concerns about the potential for AI-generated sketches to perpetuate racism, while others shared their experiences with traditional sketch artists and the limitations of human memory in capturing details. One user also mentioned the potential for AI-generated sketches to enhance police sketches.

### The greatest risk of AI is from the people who control it, not the tech itself

#### [Submission URL](https://aisnakeoil.substack.com/p/is-avoiding-extinction-from-ai-really) | 594 points | by [nickwritesit](https://news.ycombinator.com/user?id=nickwritesit) | [467 comments](https://news.ycombinator.com/item?id=36139852)

A group of AI technologists have signed a statement claiming that "mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war." But skeptics argue that other AI risks are more urgent and that the greatest risks come not from the technology, but from the people who control it using it to accumulate power and wealth. They suggest that instead of focusing on ambiguous projections about the future of AI, we should focus on building institutions that reduce existing AI risks and put us in a robust position to address new ones.

A group of AI technologists signed a statement declaring that mitigating the risk of extinction from AI should be a global priority, but some argue that other AI risks are more pressing, such as those that come from people who control AI to accumulate power and wealth. Instead, we should focus on building institutions that reduce existing AI risks and put us in a robust position to address new ones. The discussion in the comments section revolves around whether Yudkowsky's AI safety tech companies are promoting safe AI practices or just greenwashing the industry. Some suggest that AI safety is not as urgent as other risks, such as violent extremists, greedy corporations suppressing exploration, or linguistic models inhibiting public relations. A few commenters criticize Yudkowsky's appearances, while others feel that focusing on personalities detracts from the discussion. Overall, the comments suggest that the discussion on AI safety is ongoing and contentious and that AI must be approached with caution.

### Hibernation artificially triggered in potential space travel breakthrough

#### [Submission URL](https://www.theguardian.com/science/2023/may/25/hibernation-artificially-triggered-in-potential-space-travel-breakthrough) | 22 points | by [bookofjoe](https://news.ycombinator.com/user?id=bookofjoe) | [10 comments](https://news.ycombinator.com/item?id=36146063)

Scientists have artificially triggered hibernation in rats using ultrasonic pulses, raising hopes that the same technique could be used to send humans into suspended animation. The team from Washington University in St Louis identified a group of neurons in the hypothalamus preoptic area that regulate body temperature and metabolism and showed they could be activated in rodents using helmets delivering ultrasound. The animals showed a drop in body temperature and a shift towards using fat for energy. The technique could have medical applications, including enabling extra time for treating life-threatening conditions such as heart attack and stroke.

The discussion on this submission is mostly about the implications of artificially inducing hibernation in humans. Some users express concern about the potential risks and ethical considerations of such a procedure. Others mention the parallels to science fiction and the possibility of using hibernation for space travel. One user points out that the technique could have practical medical applications for emergency situations such as heart attacks and strokes. Another user notes the similarities between hibernation and the way some animals go into a state of torpor to conserve energy. Overall, the discussion is a mix of excitement and caution about the prospects of hibernation in humans.

### Arm announces the Cortex X4 for 2024, plus a 14-core M2-fighter

#### [Submission URL](https://arstechnica.com/gadgets/2023/05/arm-announces-the-cortex-x4-for-2024-plus-a-14-core-m2-fighter/) | 82 points | by [jackalo](https://news.ycombinator.com/user?id=jackalo) | [65 comments](https://news.ycombinator.com/item?id=36136719)

Arm has unveiled its latest chips, including the Arm Cortex X4, Cortex A720, and Cortex A520, which it claims will arrive in Android phones and Windows laptops in 2024. The company said the Cortex X4 will boast 15% higher performance than this year's X3 chip, alongside a 40% improvementÂ in power efficiency. It also pledged a 20% efficiency boost for the A700 series and a 22% boost for the A500. The chips use Armv9.2 architectureÂ which features theÂ newÂ QARMA3 algorithm, designed to protect against memory vulnerabilities like buffer overflows. The improvements will allow manufacturers to reduce the CPU overhead of extra memory work to just 1% of the chip's power.

Arm has announced its latest chips, the Arm Cortex X4, Cortex A720, and Cortex A520, which are expected to become available in Android phones and Windows laptops by 2024. The Cortex X4 will offer a performance gain of 15% and a 40% power efficiency improvement over its predecessor, the X3. The A700 and A500 series will also experience a 20% and 22% efficiency boost, respectively. The chips are based on the Armv9.2 architecture, enabling manufacturers to reduce the CPU overhead of extra memory work to 1% of the chip's power to protect against memory vulnerabilities like buffer overflows. In a discussion, comparisons were drawn between the Arm, Apple M1, and the Snapdragon 8 Gen 2 chips. Some users praised the improvements while others questioned whether Arm was moving towards middleware system-on-chip vendors.

### Train Your Own Private ChatGPT Model for the Cost of a Starbucks Coffee

#### [Submission URL](https://medium.com/@ApacheDolphinScheduler/train-your-own-private-chatgpt-model-for-the-cost-of-a-starbucks-coffee-25c588f450ee) | 72 points | by [DSOfficial](https://news.ycombinator.com/user?id=DSOfficial) | [16 comments](https://news.ycombinator.com/item?id=36136318)

With just the cost of a Starbucks coffee and a couple of hours of your time, you can now train your own open-source large-scale model using Apache DolphinScheduler. This model can be fine-tuned to enhance various skills, such as medical, programming, stock trading, and love advice, thus making it more "understanding" of you. With this, you can have your personal AI assistant that understands and responds to you better. Apache DolphinScheduler provides one-click support for training, tuning, and deploying open-source large-scale models, making it accessible to anyone interested in GPT and not just AI professionals. The whole process takes only three steps and around 20 hours of running time to build your ChatGPT model that understands and responds to you better.

The discussion on this submission covers a range of topics related to training and using open-source large-scale models. There is some disagreement about the effectiveness of self-training versus proprietary methods developed by companies like Microsoft. Some commenters suggest that domain-specific knowledge is important for organizations and individuals looking to train their own models. Others discuss the technical details of building and training models. One commenter suggests that there may be privacy implications to using a personal AI assistant, and another questions the use of the term "private" in relation to ChatGPT. In the end, the main point of the submission is that it is possible to train your own model using Apache DolphinScheduler for just a small investment in time and money.

