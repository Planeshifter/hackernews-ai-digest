## AI Submissions for Sat Sep 13 2025 {{ 'date': '2025-09-13T17:13:20.285Z' }}

### Will AI be the basis of many future industrial fortunes, or a net loser?

#### [Submission URL](https://joincolossus.com/article/ai-will-not-make-you-rich/) | 182 points | by [saucymew](https://news.ycombinator.com/user?id=saucymew) | [266 comments](https://news.ycombinator.com/item?id=45235676)

Thesis: Generative AI will be massively transformative but won’t mint broad new fortunes. Like shipping containerization, much of the surplus will accrue to customers, while builders and app companies compete into thin-margin oligopolies. The smart money either gets in very early and exits fast, or focuses on incumbents that capture efficiency gains.

Key points:
- Innovation vs. value capture: Past revolutions split into two patterns—ICT (PCs/microprocessors) created outsized startup wealth; containerization spread value so widely that almost no one captured it.
- Why PCs made fortunes: Cheap, permissionless hardware (6502, Z80 price drops) unleashed bottom-up experimentation, killer apps (e.g., spreadsheets), and new firms that could sell something people learned to want.
- Why AI may rhyme with containerization: Capital intensity, rapid commoditization, and platform power tilt outcomes toward oligopolies; builders and app layers grind each other’s margins down while customers enjoy the gains.
- Investing implication: Much of today’s AI capital is aimed at the wrong layers. Profits likely accrue to adopters who embed AI to improve workflows and margins, not to undifferentiated model or app startups.
- Playbook: Assume surplus goes to users. Back distribution advantages and entrenched workflows, monetize early hype, and be willing to get out before the Red Queen’s race sets in.

Bottom line: The disruption is real; the profits are predictable—and mostly downstream.

**Summary of Discussion:**

The discussion revolves around AI's role in lowering entry barriers across domains like game development, creative work, and entrepreneurship, while questioning whether this democratization translates to sustainable economic value. Key points include:

1. **Lowered Barriers & Democratization**:  
   - Users highlight AI’s ability to simplify tasks (e.g., generating game assets, graphics, or code) that previously required specialized skills or budgets. Examples include indie developers creating games with $0 budgets using AI tools, likening this shift to how GarageBand and iMovie democratized music/video creation.  
   - However, this accessibility intensifies competition, commoditizing outputs and squeezing margins for startups and creators.

2. **Economic Impact Debates**:  
   - Some argue AI risks "destroying economic activity" by replacing high-value transactions (e.g., hiring specialists) with low-cost subscriptions, potentially distorting metrics like GDP. Critics counter this with the "broken window fallacy," noting efficiency gains (e.g., mechanized hole-digging vs. manual labor) can create surplus even if traditional metrics miss it.  
   - Concerns arise about startups leveraging AI to mimic specialized services (design, copywriting), flooding markets with "good enough" solutions that undercut professionals but may not generate significant economic value long-term.

3. **Creative vs. Mundane Tasks**:  
   - A tangent debates whether AI aids creativity or merely automates drudgery. While AI can expedite workflows (e.g., overcoming "mental hurdles" in projects), it risks homogenizing outputs (e.g., generic corporate graphics) and bypassing the nuanced, human-driven creativity seen in fields like art or design.

4. **Incumbent Advantage**:  
   - Participants speculate that entrenched companies embedding AI into existing workflows (e.g., improving margins) may capture more value than startups in oversaturated "undifferentiated" AI app layers.

**Conclusion**: The discussion reflects cautious optimism about AI’s democratizing potential but skepticism about its profit-generating capacity for new entrants. Many foresee a future where efficiency gains benefit end-users and incumbents, while creators and startups face a "Red Queen’s race" of diminishing returns—mirroring historical shifts like containerization.

### AI coding

#### [Submission URL](https://geohot.github.io//blog/jekyll/update/2025/09/12/ai-coding.html) | 383 points | by [abhaynayar](https://news.ycombinator.com/user?id=abhaynayar) | [269 comments](https://news.ycombinator.com/item?id=45230677)

AI coding is just compiling English, not programming, argues a strongly worded HN post. The author likens today’s LLMs to compilers: you supply a prompt (the “source”), they emit code (the “compiled” output). That works for common patterns but breaks down on novel tasks because English is imprecise, prompts are non‑local, and the systems are non‑deterministic—unlike compilers, which are bound to language specs.

Key points:
- If you believe compilers “code,” then sure—AI “codes.” Otherwise, LLMs are best seen as powerful autocomplete plus search/optimization over massive pattern libraries.
- The apparent success of AI coding reflects how rough today’s languages, libraries, and tooling are. Better PLs/compilers would reduce the appeal of prompting.
- Hype mirrors past bubbles (e.g., self‑driving); the author claims billions are being burned on “vibe coding” demos.
- Cites a study where users felt ~20% more productive with AI but were actually ~19% slower; argues perception is outpacing reality.
- Predicts AI will replace some programming the way compilers and spreadsheets did—by reshaping workflows—yet insists we frame it as a tool, not a replacement “doing the coding.”
- Meta: admits the post is deliberately punchy for reach; says he’s pro‑AI as a carefully applied tool and expects steady, incremental improvement, not magic.

Why it matters: The piece challenges the “AI writes software” narrative and pushes investment toward better languages, specs, and deterministic tooling—using LLMs where they’re strongest without outsourcing engineering judgment.

**Summary of Discussion:**

The discussion explores the impact of AI coding tools, weighing their benefits against potential drawbacks. Key themes include:

1. **Efficiency vs. Depth**:  
   - Many agree AI accelerates **routine tasks** (boilerplate, debugging) but risks prioritizing speed over **critical thinking** and problem-solving depth.  
   - Examples: Senior devs note AI handles "good parts" quickly but may lead to **superficial solutions** for complex issues.  

2. **Learning and Skill Development**:  
   - Concern that over-reliance on AI could **stunt junior developers’ growth**, bypassing foundational skills (e.g., understanding low-level logic, debugging).  
   - Counterpoint: Comparing AI to **modern libraries/abstractions**, which also abstract complexity but require vetting.  

3. **Cognitive Impact**:  
   - Some report **mental exhaustion** from constantly reviewing AI-generated code, likening it to "rubber-stamping" outputs.  
   - Others fear **reduced creativity** as AI encourages a "gambler’s mentality" (hoping prompts yield viable solutions vs. deep analysis).  

4. **Workplace Pressures**:  
   - Companies prioritizing **consistent, measurable progress** (e.g., sprint cycles) may favor AI’s rapid output, marginalizing harder, less predictable tasks.  
   - Risk of **burnout** as developers juggle oversight of AI and complex work.  

5. **Tool vs. Replacement**:  
   - Skepticism about AI as a true "problem-solver"—it excels at **pattern-matching** but struggles with novel tasks.  
   - Analogy: AI is akin to **advanced autocomplete**, not a replacement for engineering judgment.  

**Notable Quotes**:  
- *"AI changes the job to constantly struggling with hard problems."* (rncl)  
- *"AI disrupts the rewarding parts of coding, demotivating developers."* (smnwrds)  
- *"Relying on AI for boilerplate may strip software engineering to mere assembly work."* (skydhsh)  

**Conclusion**:  
While AI tools can enhance productivity, the consensus stresses **human oversight** and preserving core engineering principles. The debate mirrors past shifts (e.g., compilers, IDEs) but amplifies concerns about critical thinking erosion and the need for balance between efficiency and depth.

### ‘Overworked, underpaid’ humans train Google’s AI

#### [Submission URL](https://www.theguardian.com/technology/2025/sep/11/google-gemini-ai-training-humans) | 276 points | by [Brajeshwar](https://news.ycombinator.com/user?id=Brajeshwar) | [148 comments](https://news.ycombinator.com/item?id=45231239)

The Guardian spotlights the “shadow workforce” behind Google’s polished AI. Thousands of contract “raters,” hired largely through Hitachi’s GlobalLogic (plus Accenture and others), spend their days grading and moderating Gemini and AI Overviews outputs for safety and accuracy. Many were recruited under vague titles (e.g., “writing analyst”) and ended up reviewing violent or sexual content without prior warning or consent, under tight quotas (tasks in ~10 minutes) and with no mental health support.

Roles split into “generalist raters” and “super raters,” the latter organized into specialist pods (from teachers to PhDs). GlobalLogic’s team reportedly grew from about 25 super raters in 2023 to nearly 2,000, mostly U.S.-based and working in English. Pay is higher than data labelers in places like Nairobi or Bogotá, but far below Silicon Valley engineers. Workers describe anxiety, burnout, and a sense of invisibility despite being central to making models appear safe and smart. As DAIR’s Adio Dinika puts it: “AI isn’t magic; it’s a pyramid scheme of human labor.”

Google’s response: raters are supplier employees; their feedback is one of many signals and doesn’t directly shape algorithms. The piece underscores how human moderation remains critical even as Google touts progress (e.g., Gemini 2.5 Pro vs OpenAI’s O3), raising questions about consent, support, and labor standards in AI’s supply chain.

**Summary of Discussion:**

- **Contractor Experiences & Pay:**  
  Contractors (e.g., "nlnhst") report pay rates of ~$45/hour, which exceeds the U.S. median wage ($27/hour) but pales against Silicon Valley engineer salaries. However, work unpredictability, sudden project cancellations, and lack of communication from employers like Google’s subcontractors (e.g., GlobalLogic) cause stress. Some defend the pay as fair for remote roles, while others note burnout from escalating task complexity requiring advanced expertise (e.g., PhD-level problem-solving).  

- **Labor Market Dynamics:**  
  Job seekers mention platforms like DataAnnotation, Outlier, and Mercor for AI-related gigs, though skepticism exists about opaque postings and “blanket” recruitment tactics. Others highlight trends of companies outsourcing to lower-cost regions (e.g., Mexico, India) or automating roles, fueling fears of job displacement despite AI’s reliance on human input.  

- **Content Moderation Challenges:**  
  Workers exposed to violent/sexual content liken the role to “cleaning filthy toilets” — necessary but mentally taxing. Debate arises over whether such jobs should include explicit consent, mental health support, or hazard pay, with comparisons to other high-stress roles (e.g., therapists, construction workers).  

- **AI’s Hidden Labor Ecosystem:**  
  Comments underscore the vast, often opaque network of RLHF (Reinforcement Learning from Human Feedback) providers like Scale AI, Toloka, and Invisible, which power major AI firms. Transparency remains scarce, with few companies disclosing their reliance on human raters in research papers or public communications.  

- **Broader Critiques:**  
  Critics liken AI development to a “pyramid scheme,” dependent on undervalued human labor for safety and quality. Others argue this reflects broader capitalist exploitation, where corporations profit from decentralized, underpaid workforces. Meanwhile, defenders view it as a pragmatic trade-off in advancing technology.  

**Key Takeaway:**  
The discussion paints a complex picture of AI’s “shadow workforce” — a mix of opportunity and exploitation, where decent pay coexists with instability, invisibility, and ethical concerns about labor practices in the tech supply chain.

### I unified convolution and attention into a single framework

#### [Submission URL](https://zenodo.org/records/17103133) | 74 points | by [umjunsik132](https://news.ycombinator.com/user?id=umjunsik132) | [16 comments](https://news.ycombinator.com/item?id=45229960)

An independent researcher proposes the Generalized Windowed Operation (GWO), a unifying lens for neural net ops. The idea: most primitives (e.g., convolution, matrix-multiply–based layers) can be decomposed into three orthogonal pieces:
- Path: operational locality (where information flows)
- Shape: geometric structure and symmetry assumptions
- Weight: feature importance

Core claims
- Principle of Structural Alignment: models generalize best when a layer’s (P, S, W) mirrors the data’s intrinsic structure.
- This falls out of the Information Bottleneck: the best “compression” keeps structure-aligned information.
- Operational Complexity (via Kolmogorov-style complexity) should not just be minimized; how that complexity is used matters—adaptive regularization beats brute-force capacity.
- Canonical ops and modern variants emerge as IB-optimal under the right (P, S, W). Experiments reportedly show that the quality—not just quantity—of an operation’s complexity governs performance.

Why it matters
- A compact “grammar” for inventing layers and selecting inductive biases from data properties, potentially unifying how we think about convs and other matmul-based modules.
- Reframes the tuning question from “more parameters?” to “better-structured complexity?”

What to look for
- How is Kolmogorov-style complexity approximated in practice?
- Scope and rigor of experiments and benchmarks
- Whether the framework cleanly covers attention/graph ops
- Availability of code or design recipes

Link: DOI https://doi.org/10.5281/zenodo.17103133 (PDF, CC BY 4.0)

**Hacker News Discussion Summary:**

The discussion revolves around the **Generalized Windowed Operation (GWO)** framework proposed in the paper, touching on its implications, technical details, and adjacent debates about AI-generated text and research culture. Key points:  

---

### **Technical Contributions & Debates**  
1. **GWO vs. Mamba Models**:  
   - A user (**FjordWarden**) connects GWO to **Mamba models**, highlighting similarities:  
     - *Path*: Mamba’s structured state-space recurrence for long-range dependencies.  
     - *Shape*: 1D sequential processing aligns with GWO's principles.  
     - *Weight*: Dynamic input-dependent parameters enable efficient information bottlenecks.  
   - **umjunsik132** (OP) agrees, noting that Mamba is a "stellar instance" of GWO, tailored for sequential data.  

2. **GWO’s Broader Impact**:  
   - GWO is praised as a unifying "grammar" for neural ops, with experiments suggesting that **adaptive regularization** beats brute-force complexity.  
   - The framework’s ability to reframe layer design (e.g., explaining Self-Attention) is seen as promising but requires rigorous validation.  

---

### **Community Reception**  
1. **Independent Research**:  
   - **CuriouslyC** applauds the independent research but raises skepticism about reproducibility and "imposter syndrome" in solo projects.  
   - Users call for clarity on **benchmarks**, code availability, and whether GWO cleanly handles attention/graph ops.  

2. **AI-Generated Text Criticism**:  
   - Users (**dwb**, **pssmzr**) mock verbose, hyperbolically phrased responses (suspected to be AI-generated).  
   - Suggestions include simplifying technical language and improving prompts to avoid "kindergarten teacher"-style explanations.  

---

### **Side Conversations**  
1. **Humor and Meta-Debates**:  
   - A subthread jokes about AI’s struggle with sycophantic outputs ("fntstc prfct stllr") and RLHF’s limitations.  
   - References to **GPT-4o** and hand-drawing flaws spark memes about AI’s quirks.  

2. **Research Culture**:  
   - Light debates arise about balancing rigor with accessible communication, with some users criticizing overly technical jargon.  

---

### **Key Questions Remaining**  
- How is **Kolmogorov-style complexity** approximated in practice?  
- Can GWO’s framework *predict* new ops, or just retroactively explain existing ones?  
- Will independent researchers get support to validate claims at scale?  

The thread reflects excitement for GWO’s theoretical promise but highlights skepticism about execution and broader applicability.

### Chatbox app is back on the US app store

#### [Submission URL](https://github.com/chatboxai/chatbox/issues/2644) | 68 points | by [themez](https://news.ycombinator.com/user?id=themez) | [36 comments](https://news.ycombinator.com/item?id=45228766)

Chatbox app returns to U.S. App Store after court fight over “Chatbox” trademark

- The Chatbox team says a rival claimed trademark rights to the generic term “Chatbox” in April, leading Apple to pull the app on June 17 despite the rival’s USPTO application having been initially rejected.
- The developers took the dispute to federal court; on Aug 29, a judge ordered Apple to restore the app within seven days. Apple notified them about two weeks later that the app was back online.
- The team calls it a win against trademark bullying and notes they’ve used “Chatbox” for AI software since March 2023 on GitHub.
- Beyond this case, it highlights how App Store takedowns can hinge on contested IP claims—and that developers can prevail when challenging overbroad marks.

**Summary of Hacker News Discussion:**

1. **GPL Licensing Concerns**:  
   - Users debated whether the **Chatbox** app (available on the App Store) complies with the **GPLv3 license**.  
   - Critics pointed out the **closed-source commercial version** ($19.99/month) may violate GPL terms if derived from the open-source GitHub repository.  
   - Confusion arose about whether the GitHub code (regularly synced) legally obligates the App Store version to provide source access. Some argued the GPL binds redistributors, not the original copyright holder.  

2. **Technical Criticisms of Chatbox**:  
   - Labeled a **basic AI client** (e.g., ChatGPT wrapper) with limited mobile functionality. Users noted difficulty finding apps that support custom APIs or local AI models.  
   - Some switched to alternatives like **Mysty** or **T3Chat** (open-source) for better features or self-hosting options.  

3. **App Store Security Risks**:  
   - Concerns about exploitative apps on stores, even open-source ones. Users recommended trusted apps like **Anki**, **KDE Connect**, and **F-Droid** (30% of which were deemed "questionable").  
   - Debates highlighted the irony of app stores policing security while hosting risky apps.  

4. **Broader Critique of App Stores**:  
   - Frustration with **Apple/Google’s dominance**, high fees (30% cut), and restrictive policies. Some advocated for web apps to avoid store constraints.  
   - Others acknowledged mobile apps are critical for business success, despite the hurdles.  

5. **Licensing Nuances**:  
   - Users clarified GPL obligations: Redistributors must provide source code, but original copyright holders can dual-license (proprietary + GPL).  
   - Skepticism remained about whether Chatbox’s GitHub repo includes the latest App Store code.  

6. **Alternatives & Workarounds**:  
   - Suggestions to **self-host AI models** (e.g., **Ollama**) or use open-source clients like **Chatbox Community Edition**.  
   - Mentions of **T3Chat** (non-mobile) as another open-source option.  

**Key Themes**:  
- Tension between **open-source ideals** and **app store realities**.  
- **Legal ambiguity** around GPL enforcement in proprietary contexts.  
- Growing preference for **self-hosted/offline solutions** to avoid store dependencies.

### OpenAI’s latest research paper demonstrates that falsehoods are inevitable

#### [Submission URL](https://theconversation.com/why-openais-solution-to-ai-hallucinations-would-kill-chatgpt-tomorrow-265107) | 63 points | by [ricksunny](https://news.ycombinator.com/user?id=ricksunny) | [44 comments](https://news.ycombinator.com/item?id=45233589)

TL;DR: A new OpenAI paper argues hallucinations aren’t a bug but a mathematical inevitability for language models. The cleanest fix—only answering when sufficiently confident—would slash hallucinations but also make chatbots say “I don’t know” far more often, likely driving users away and raising costs.

Key points:
- Inevitability of errors: Even with perfect training data, next-word prediction accumulates mistakes across tokens. The paper shows sequence generation has at least 2x the error rate of equivalent yes/no classification.
- Data sparsity bites: Rare facts seen only once in training lead to proportionally high error rates on those queries. Example: models gave multiple confident but wrong birthdays for an author of the paper.
- The evaluation trap: Most benchmarks use binary grading that penalizes “I don’t know” the same as a wrong answer. Mathematically, this makes always guessing the optimal strategy, incentivizing confident nonsense.
- OpenAI’s proposed fix: Calibrate and enforce confidence thresholds (only answer if, say, >75% likely correct) and grade models accordingly. This would reduce hallucinations.
- The trade-off: If models abstain on a sizable fraction of questions (the article suggests ~30% as a conservative figure), user satisfaction and engagement could crater. Plus, reliable uncertainty estimates typically require extra computation (e.g., multiple samples/ensembles), driving latency and cost for high-volume systems.

Why it matters:
- The core tension isn’t just technical—it’s product and economics. Honest uncertainty improves truthfulness but degrades the seamless, always-confident UX that made chatbots popular, while also increasing compute bills.
- Benchmarks and incentives shape behavior. As long as evaluations punish abstention, models will be trained to guess.
- Expect future systems to juggle modes: fast, confident answers for casual use; slower, uncertainty-aware workflows (with retrieval/tools/human-in-the-loop) for high-stakes queries.

Source: “Why OpenAI’s solution to AI hallucinations would kill ChatGPT tomorrow” by Wei Xing (The Conversation). DOI: https://doi.org/10.64628/AB.kur93yu6h Article: https://theconversation.com/why-openais-solution-to-ai-hallucinations-would-kill-chatgpt-tomorrow-265107

**Summary of Hacker News Discussion on OpenAI’s Hallucination Fix:**

The Hacker News discussion on OpenAI’s proposed solution to reduce AI hallucinations highlights a mix of technical skepticism, practical trade-offs, and alternative proposals. Here’s a breakdown of key points:

---

### **Key Themes & Perspectives:**

1. **Technical Limitations of LLMs:**
   - Users emphasize that LLMs are fundamentally **prediction machines** trained to generate plausible-sounding text, not factual databases. This design inherently limits their ability to "know" truths or reliably abstain from guessing.
   - Skepticism arises about whether **confidence calibration** (e.g., only answering when >75% sure) can resolve hallucinations, given sparse training data and conflicting "truths" in sources like Wikipedia or 4chan.

2. **User Experience Trade-Offs:**
   - Frequent "I don’t know" responses risk frustrating users accustomed to ChatGPT’s confident tone. Commenters liken this to **human behavior**: students guessing on exams, professors accepting error margins, or people preferring quick answers over uncertainty.
   - Proposed workaround: Offer **multiple modes**, such as a "fast mode" (guesses with disclaimers) and a "slow mode" (verified, accurate answers using retrieval-augmented generation, or RAG). This mirrors how humans balance speed and accuracy.

3. **Benchmarks and Incentives:**
   - Current benchmarks and leaderboards **penalize abstentions**, incentivizing models to guess confidently even when wrong. Users suggest revising evaluation metrics to reward honesty over confidence.
   - Comparisons are drawn to industries like finance, where confidence intervals are standard, yet businesses often ignore them—implying similar challenges for AI adoption.

4. **Alternative Solutions & Comparisons:**
   - **Retrieval-Augmented Generation (RAG)** is highlighted as a practical fix, where models cite sources and verify claims, though some note it’s already being used (e.g., ChatGPT’s web searches) with mixed results.
   - A provocative analogy: Treating LLMs like **"surgeon general warnings"** for high-stakes answers, acknowledging their limitations upfront.
   - Humorous takes: Skeptics rebrand LLMs as "Large Limitations Machines" or joke that an honest chatbot would go viral as a "psychic therapist."

5. **Broader Philosophical Concerns:**
   - Some argue hallucinations are **inevitable** unless models are trained on curated "correct" data, which raises ethical and logistical challenges (e.g., who defines truth?).
   - Others critique the focus on technical fixes over **rethinking LLM design**, suggesting symbolic AI hybrids or systems that prioritize truth-seeking over next-word prediction.

---

### **Sentiment & Takeaways:**
- **Pragmatic Optimism**: Many agree the tension between accuracy and usability is solvable through hybrid approaches (e.g., RAG + user feedback) and better transparency.
- **Frustration with Trade-Offs**: Users lament the dilemma between truthful but hesitant AI and engaging but unreliable chatbots.
- **Skepticism of Quick Fixes**: Technical proposals like confidence thresholds are seen as partial solutions that fail to address core limitations of LLMs as predictive systems.

Ultimately, the discussion underscores that resolving hallucinations isn’t just a technical challenge—it’s a **product, ethical, and cultural problem** requiring shifts in user expectations, evaluation standards, and AI design.
