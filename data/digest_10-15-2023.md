## AI Submissions for Sun Oct 15 2023 {{ 'date': '2023-10-15T17:10:35.576Z' }}

### MemGPT: Towards LLMs as Operating Systems

#### [Submission URL](https://arxiv.org/abs/2310.08560) | 210 points | by [belter](https://news.ycombinator.com/user?id=belter) | [117 comments](https://news.ycombinator.com/item?id=37894403)

Researchers from various institutions have proposed a technique called virtual context management to extend the utility of large language models (LLMs), such as GPT, in tasks like extended conversations and document analysis. LLMs are often limited by their context windows, which restrict the amount of information they can process. Inspired by hierarchical memory systems in traditional operating systems, the researchers have developed MemGPT (Memory-GPT), an operating system-like system that intelligently manages different memory tiers to provide extended context within the LLM's limited context window. MemGPT uses interrupts to manage control flow between itself and the user. The researchers evaluated MemGPT in two domains: document analysis and multi-session chat, and found that MemGPT can effectively analyze large documents and create conversational agents that remember and evolve dynamically through interactions with users. The researchers have released the MemGPT code and data for further experiments.

The discussion around this submission covers a range of topics. 

- Users discuss the limitations of large language models (LLMs) and the potential benefits of extending their context windows through techniques like MemGPT. Some users share their experiences with similar projects and suggest different approaches to context management.
- Some users express their appreciation for the work and offer positive feedback to the author.
- Others discuss the nature of AI models and the challenges in their development and deployment. There are discussions about the reliability of AI models, the importance of replicability in scientific publishing, and the potential risks associated with AI technology.
- There is also a brief discussion about the application of AI in the cryptocurrency industry and the potential impact on different sectors.
- Finally, there are a few comments exploring the analogy between the AI industry and the gold rush, and a humorous exchange about selling shovels in a gold rush.

Overall, the discussion covers a range of perspectives and insights related to large language models, AI technology, and its potential implications.

### Teaching Apple Cyberdog 1.0 new tricks (featuring OpenDoc)

#### [Submission URL](http://oldvcr.blogspot.com/2023/10/teaching-apple-cyberdog-10-new-tricks.html) | 126 points | by [classichasclass](https://news.ycombinator.com/user?id=classichasclass) | [55 comments](https://news.ycombinator.com/item?id=37894030)

In a blast from the past, the author revisits Apple's Cyberdog, a web browser and internet suite that has long been forgotten. Cyberdog was unique in that it allowed developers to create their own components, such as viewers and UI elements, using Apple's OpenDoc embedding. OpenDoc was a standard compound document format that allowed for an object-oriented approach to document creation. The goal was to have reusable components that could be pulled into a document and maintain their own views and state. Cyberdog was essentially a demonstration of OpenDoc's capabilities, and it was released as part of Apple's Project Amber, which aimed to create a next-generation technology platform called Taligent. Despite Apple's efforts, OpenDoc did not gain much traction with developers or users, and it was seen as a competitor to Microsoft's Object Linking and Embedding (OLE) technology. Apple eventually released Cyberdog as an internet suite, capitalizing on the popularity of internet document creation. However, Cyberdog also faded into obscurity, and today it serves as a reminder of Apple's ambitious but unsuccessful foray into component-based document creation.

The discussion on this article covers various aspects of Apple's Cyberdog and OpenDoc technology.

- One commenter mentions that they remember the Apple Dylan IDE requiring 24MB of RAM, which was a significant amount at the time. They also mention that Cyberdog was a fascinating project but ultimately faced difficulties due to its large RAM requirements.
- Another commenter shares links to screenshots and explanations of Cyberdog, as well as a mention of the SK8 programming language.
- There is a discussion about Steve Jobs' response to OpenDoc versus Java, with a correction made that Jobs was not yet CEO at the time.
- A commenter expresses relief in reading the well-written article but admits that they still don't fully understand the supposed problem that OpenDoc was meant to solve.
- Some commenters compare OpenDoc to Microsoft's OLE technology, with one mentioning that OpenDoc aimed to improve cross-platform interoperability.
- The complexity and memory requirements of OpenDoc are mentioned, with one commenter stating that it required a significant amount of RAM to function.
- One commenter shares their personal experience with using Apple Cyberdog and praises its capabilities.
- The topic of connecting Cyberdog with Microsoft Internet Explorer is brought up, with a link shared to an archived page about it.
- There are mentions of the CI Labs and its involvement with OpenDoc.
- Commenters discuss the creativity and whimsical nature of the icons used in old Mac applications.
- The conversation touches on the history of Cyberdog and its features, as well as the advancements in instant search technology.
- A reference is made to a character named Preston from a 1995 Wallace and Gromit short film.
- The discussion briefly touches on compound documents and the challenges they presented in the '90s.
- A commenter mentions a Japanese system that recently graduated from paper documents to digital ones.
- The discussion also includes mentions of the Kantara project and the App Store.

Overall, the comments cover a range of experiences, memories, and opinions related to Apple's Cyberdog and OpenDoc technology.

### Show HN: Deep Chat â€“ AI chat component

#### [Submission URL](https://github.com/OvidijusParsiunas/deep-chat) | 65 points | by [ovisource](https://news.ycombinator.com/user?id=ovisource) | [4 comments](https://news.ycombinator.com/item?id=37889444)

Deep Chat is a customizable AI chat component that can be easily integrated into your website. It allows you to connect to popular AI APIs like OpenAI, HuggingFace, and Cohere, or even to your own custom service. With Deep Chat, you can send and receive messages, exchange files, capture photos via webcam, record audio, and even convert speech to text and vice versa. The latest update includes support for custom elements in message bubbles, allowing you to add suggestion buttons, charts, maps, or any other HTML element you desire. Deep Chat is highly customizable and can be used with any major UI framework or library. To get started, simply install the npm package and add the Deep Chat component to your markup.

In the discussion, user "vrtclbx" expressed their appreciation for the webcam and microphone functionality in the chat component and mentioned that they found it useful for capturing photos and recording audio. User "jlthln" suggested adding a feature that allows for the integration of a recommendation engine to enhance the product. User "vsrc" thanked "jlthln" for the suggestion and mentioned that they are currently calling external services to handle interactions with models. They appreciated the suggestion and said that they are planning to add functionality to host models entirely in the browser, which would greatly benefit from the recommendation engine capabilities.

### Margaret Atwood Reviews a Margaret Atwood Story by AI

#### [Submission URL](https://thewalrus.ca/margaret-atwood-ai/) | 88 points | by [goldenskye](https://news.ycombinator.com/user?id=goldenskye) | [68 comments](https://news.ycombinator.com/item?id=37894072)

In a recent article, the author dives into the anxieties surrounding generative AI and its potential impact on writers. They question whether AI chatbots will devour our literature, infiltrate our minds, and take over our jobs. However, the author provides some reassurance by highlighting the current limitations of AI chatbots, such as their inability to reflect or grasp metaphor and punctuation. To demonstrate this, the author shares two examples of literary attempts by AI chatbots, a poem and a short story. While these examples certainly have their quirks and inaccuracies, they serve as a reminder that AI chatbots are not yet ready to replace human authors. So, writers can take heart knowing that their creative skills are safe from the clutches of AI, at least for now. Ultimately, while the fear of AI may loom large in some writers' minds, it seems that human creativity still has the upper hand.

The discussion on this submission focuses on various aspects of AI-generated writing and the limitations of current AI models. One commenter notes that the 10x improvement mentioned in the article regarding ChatGPT is not adequately supported and questions the validity of such claims. Another points out that GPT-4 does not possess the intelligent reasoning capabilities that GPT-35 lacks, emphasizing the need to differentiate between different versions of AI models. Some commenters express skepticism about AI-generated content, while others argue that AI can assist human writers and bring new perspectives. The debate also touches on the potential shortcomings of AI models in replicating human memory and recall. Additionally, there are discussions about Margaret Atwood's writing style and the similarities between AI-generated content and children's stories or specific authors like H.P. Lovecraft.

### Scientists begin building AI for scientific discovery using tech behind ChatGPT

#### [Submission URL](https://techxplore.com/news/2023-10-scientists-ai-scientific-discovery-tech.html) | 38 points | by [gardenfelder](https://news.ycombinator.com/user?id=gardenfelder) | [14 comments](https://news.ycombinator.com/item?id=37890570)

An international team of scientists, including researchers from the University of Cambridge, has launched a research collaboration called Polymathic AI to develop an AI-powered tool for scientific discovery. Leveraging the technology behind ChatGPT, the team aims to build an AI that can learn from numerical data and physics simulations to assist scientists in various scientific fields. By starting with a large, pre-trained model, Polymathic AI aims to make AI and machine learning more accessible and effective in scientific research. The team includes experts in physics, astrophysics, mathematics, artificial intelligence, and neuroscience from institutions such as the University of Cambridge, Simons Foundation, New York University, Princeton University, and Lawrence Berkeley National Laboratory. Polymathic AI's goal is to connect different scientific subfields and apply multidisciplinary knowledge to solve complex scientific problems. The project will prioritize transparency and openness, aiming to democratize AI for scientific analysis across various domains.

The discussion surrounding the submission includes various viewpoints. 

One commenter expresses skepticism about the hype surrounding AI, stating that while it may have many applications, it is unlikely to replicate the scientific discoveries made by great scientists like Albert Einstein. They believe that scientific research requires tools for searching large spaces and independently verifying results.

Another commenter mentions their involvement in computational chemistry and shares a link to their GitHub repository.

There is a discussion about the BLOOM language model, where someone suggests joining the BLOOM collaboration and asks if the BLOOM model will receive funding. Another commenter indicates that they are unsure about joining the BLOOM collaboration and mentions that the BLOOM model can answer questions.

Yet another commenter argues that Language Models (LLMs) are capable of innovation and discovering new concepts. They believe that innovation can come from simplifying existing concepts and challenge the notion that discovery requires complex methods.

In response, another commenter disagrees, stating that complex physics relies on a vast number of relationships published by human knowledge.

Overall, the discussion touches on concerns regarding the potential of AI in scientific research, the importance of independent verification, the capabilities of BLOOM language model, and the role of LLMs in innovation and scientific discovery.

### 2nd law of infodynamics and its implications for simulated universe hypothesis

#### [Submission URL](https://pubs.aip.org/aip/adv/article/13/10/105308/2915332/The-second-law-of-infodynamics-and-its) | 14 points | by [imhoguy](https://news.ycombinator.com/user?id=imhoguy) | [5 comments](https://news.ycombinator.com/item?id=37893189)

In a recent article published in AIP Advances, Melvin M. Vopson explores the implications of the second law of infodynamics for the simulated universe hypothesis. The simulated universe hypothesis suggests that our entire reality is a simulated construct. While lacking concrete evidence, this idea is gaining popularity in both scientific and entertainment circles. The second law of infodynamics, discovered in 2022, further supports this possibility by providing a new framework for studying the intersection of physics and information. Vopson examines the applicability of this law to various domains, including digital information, genetic information, atomic physics, mathematical symmetries, and cosmology. By re-examining the second law of infodynamics, Vopson provides scientific evidence that appears to underpin the simulated universe hypothesis. This research opens up new avenues for understanding the nature of our reality and the role of information within it.

In the discussion, user "SideburnsOfDoom" raises a question about the second law of infodynamics, stating that it may be ignorant to assume that the second law implies the first law of infodynamics, which is a fundamental principle in science. User "az09mugen" agrees with SideburnsOfDoom, stating that the first law of infodynamics is indeed foundational. User "c22" adds that in 2022, a new fundamental law of physics was proposed and demonstrated, called the law of information dynamics, which simplifies the second law of infodynamics. They also provide a link to an article on the topic. User "gus_massa" mentions that the law may have different applications in various domains and gives an example related to DNA replication. "SideburnsOfDoom" remarks that the discussion is becoming too technical and compares it to a college-level physics class.

