## AI Submissions for Sat Jan 06 2024 {{ 'date': '2024-01-06T17:09:41.202Z' }}

### NIST identifies types of cyberattacks that manipulate behavior of AI systems

#### [Submission URL](https://www.nist.gov/news-events/news/2024/01/nist-identifies-types-cyberattacks-manipulate-behavior-ai-systems) | 117 points | by [geox](https://news.ycombinator.com/user?id=geox) | [44 comments](https://news.ycombinator.com/item?id=38893614)

The National Institute of Standards and Technology (NIST) has identified types of cyberattacks that manipulate the behavior of artificial intelligence (AI) systems. These attacks, known as "adversarial machine learning" threats, can cause AI systems to malfunction when exposed to untrustworthy data. The NIST publication, titled "Adversarial Machine Learning: A Taxonomy and Terminology of Attacks and Mitigations," outlines various types of attacks and provides mitigation strategies. However, there is currently no foolproof method for protecting AI systems from misdirection, and developers and users should be cautious of claims suggesting otherwise. The publication aims to help AI developers and users understand the potential attacks and develop better defenses in the future.

The discussion on this submission revolves around the different types of attacks that can be carried out on AI systems, such as poisoning attacks and prompt injection. Some users express concern about the potential implications of these attacks, particularly in sensitive systems like AI assistants and chatbots. Others discuss the challenges of mitigating these attacks and the need for comprehensive solutions to protect AI models. There is also a discussion on the AI community's responsibility in addressing these threats and ensuring the security of AI systems. Additionally, some users share their thoughts on the misconceptions surrounding AI and the importance of understanding the limitations and risks associated with it.

### Generative AI Has a Visual Plagiarism Problem

#### [Submission URL](https://spectrum.ieee.org/midjourney-copyright) | 17 points | by [YeGoblynQueenne](https://news.ycombinator.com/user?id=YeGoblynQueenne) | [11 comments](https://news.ycombinator.com/item?id=38896259)

Generative AI, which includes large language models (LLMs), has a visual plagiarism problem, according to a guest post on IEEE Spectrum. Recent research has shown that LLMs are capable of reproducing chunks of text from their training sets, including private information like email addresses and phone numbers. In some instances, LLMs have even produced near-verbatim outputs that can be seen as instances of plagiarism. This raises questions about how often these plagiaristic outputs occur and under what circumstances. Due to the black box nature of LLMs, researchers can only answer these questions through experimental methods. The existence of plagiaristic outputs has implications for technology, journalism, and copyright infringement laws.

The discussion about the submission covers various perspectives on the issue of visual plagiarism by language models. 
One user mentions that it would be challenging to guess the passwords to decrypt the compressed and separately encrypted copyrighted pictures, suggesting that generating plagiarized content may not necessarily infringe copyright. Another user argues that the responsibility for copyright infringement lies with the owners of the training data, not the models themselves.
Another user brings up the example of a young artist drawing a copyrighted character and suggests that drawing such characters could be seen as artistic training, rather than infringement. However, another user counters that the line between infringement and artistic expression can be blurry, especially with regards to the threshold of originality.
OpenAI's response to copyright issues is discussed, with one user suggesting that they could implement a system similar to Napster's extinction event to remove copyrighted content. However, others point out that many AI-generated works are small and have negligible impact on the original creators.
The concept of plagiarism is debated, with one user emphasizing that plagiarism involves taking credit for someone else's work. However, another user argues that training a child to draw copyrighted characters could be seen as condoning copyright infringement and refers to a hypothetical situation involving Planned Parenthood.
Overall, the discussion highlights the complexity of the issue and explores different viewpoints on the responsibility for plagiarism and copyright infringement in the context of generative AI.

### LLM Training and Inference with Intel Gaudi 2 AI Accelerators

#### [Submission URL](https://www.databricks.com/blog/llm-training-and-inference-intel-gaudi2-ai-accelerators) | 33 points | by [sailplease](https://news.ycombinator.com/user?id=sailplease) | [8 comments](https://news.ycombinator.com/item?id=38887798)

Databricks, a company that helps customers build and deploy generative AI applications, has announced support for the Intel Gaudi family of AI accelerators. The Gaudi accelerators are designed for deep learning training and inference and offer high performance and memory capacity. Databricks found that the Intel Gaudi 2 accelerator had the second-best training performance-per-chip among the platforms tested, achieving over 260 TFLOP/s/device when training MPT-7B on 8 x Gaudi 2. It also matched the NVIDIA H100 in decoding latency for LLM inference. Additionally, the Gaudi 2 showed the best training and inference performance-per-dollar compared to other popular accelerators. Databricks used SynapseAI 1.12 and BF16 mixed precision training for their testing, and they plan to explore performance improvements with SynapseAI 1.13, which supports FP8 training.

The discussion on this submission revolves around the performance comparison between Intel Gaudi 2 and NVIDIA accelerators, as well as the challenges of porting software to different hardware platforms.
One commenter points out that the Intel Developer Cloud offers 8x individual Gaudi 2 inference costs for $130/hr, which is significantly cheaper than using NVIDIA A100 or H100 accelerators with Amazon Web Services (AWS) infrastructure.
Another commenter brings up the issue of software portability, stating that while it may be a friction point, it is appreciated that Databricks built their performance claim by specifying the device, TDVcstr statements for verification, and experience with other platforms like Google's TPUs and AMD GPUs.
In response, another commenter acknowledges that there is fully supported Nvidia competition but highlights that implementations on Intel, AMD, Google TPUs, etc., are not at the same level of real-world applications yet. They mention that a lot of actual project implementations in the industry heavily depend on CUDA and that AMD ROCm is finally starting to sort things out, potentially becoming competitive in terms of software ecosystem. They mention that the commenter's satisfaction with Nvidia and CUDA is due to the significant performance and optimized software implementations for CUDA.
There is also a discussion about the level of abstraction provided by PyTorch and whether it truly matters or not. One commenter mentions that for most basic level use cases, PyTorch's hardware abstraction is not necessary, but it becomes important for more complex and specific machine learning projects.
The conversation turns to the challenges of software portability and the commenter's personal experience with AMD ROCm. They express frustration with the time and effort spent on making AMD hardware work with ROCm, compared to the smooth experience they have had with Nvidia and CUDA over the years. They argue that Nvidia's market dominance and support, especially for production-scale AI training and inference, make it the preferred choice despite the total cost of ownership.

Lastly, one commenter finds it ironic that there is no standard connector protocol like NVLink for Intel Gaudi, given that it requires larger and more complex plugs and cards compared to normal 100GbE switches.

### Ten Noteworthy AI Research Papers of 2023

#### [Submission URL](https://magazine.sebastianraschka.com/p/10-ai-research-papers-2023) | 123 points | by [danboarder](https://news.ycombinator.com/user?id=danboarder) | [19 comments](https://news.ycombinator.com/item?id=38896027)

This year has been a game-changer for machine learning and AI research, with rapid advancements and growing popularity in these fields. To wrap up 2023, Sebastian Raschka shares his top ten noteworthy research papers. The selection includes a heavier emphasis on large language models (LLMs) rather than computer vision papers. One standout paper is Pythia, which not only released 8 LLMs but also provided training details, analyses, and insights. Another paper, Llama 2, introduces open foundation and fine-tuned chat models, which are widely used and come with reinforcement learning with human feedback (RLHF) instruction-finetuned variants. The Llama 2 suite showcases consistent improvements over its iterations, making it a popular choice among researchers. Overall, 2023 has been a year of immense progress, and Raschka hopes for more transparency and detailed papers in the future.

The discussion on this submission primarily revolves around specific papers mentioned in the article. One user points out that the TinyStories paper demonstrates the ability to generate coherent text with fewer parameters. Another user shares a link to an interesting summary article on the topic. 
There is also a discussion about the BloombergGPT paper, which was published in March and launched Bloomberg's knowledge model. Another user comments on the usefulness of popular papers and conferences for getting expert opinions and advice. 
A commenter raises concerns about the peer review process, stating that some reviewers don't read papers carefully and their reviews can be arbitrary. Another user agrees and adds that conference reviews often miss good work and accept mediocre papers. The shortage of competent reviewers is also mentioned as a contributing factor to the problem.
Finally, there is a brief exchange discussing the flaws in the current conference review process and the need for improvements.

### Computers by the Millions (1979)

#### [Submission URL](https://web.stanford.edu/dept/SUL/sites/mac/primary/docs/cbm.html) | 6 points | by [rnjailamba](https://news.ycombinator.com/user?id=rnjailamba) | [4 comments](https://news.ycombinator.com/item?id=38890926)

Jef Raskin, a computer scientist and one of the key creators of the Apple Macintosh, wrote an essay in 1980 titled "Computers by the Millions," discussing the challenges and considerations involved in producing a large number of personal computers. Raskin emphasized the need for computers to become more accessible and affordable, suggesting that the average family should be able to own one. He explored the financial implications of manufacturing a million computers per year, including the costs of parts, labor, and inventory. Raskin also discussed the challenges of duplicating software, highlighting the time-consuming process of duplicating software on magnetic media and the need for mass storage media duplication companies. He mentioned the potential use of printing presses for distributing programs and the possibility of software distribution via communication channels. Raskin's essay shed light on the magnitude of the task of making computers more widely available and the various obstacles that needed to be overcome.

There was a discussion about Jef Raskin's essay "Computers by the Millions." One user pointed out that while Raskin emphasized the need for affordable computers, the new Mac Pro machines are quite expensive. Another user mentioned that Raskin was a key contributor to the early development of the Apple Macintosh project. Another user shared a link to Raskin's essay and mentioned that Raskin was enjoying positive feedback, except from Steve Jobs. Then, there was a mention that Steve Jobs shot Apple's history exhibits at a library and collections at Stanford.

### Show HN: LangCSS, the AI Assistant for Tailwind

#### [Submission URL](https://langcss.com/) | 7 points | by [mcapodici](https://news.ycombinator.com/user?id=mcapodici) | [3 comments](https://news.ycombinator.com/item?id=38889467)

Introducing Tailwind Chat, the AI assistant that helps you create stunning forms, buttons, landing pages, and more in real time. Currently in early beta, Tailwind Chat allows you to join the waitlist for free beta access. With Tailwind Chat, you can talk through your design ideas while the AI generates the necessary utility classes. It even lets you edit the HTML and continue the conversation seamlessly. You'll have access to great components from popular libraries like tailwindui, shadcn, flowbite, and daisyui. Not limited to just Tailwind, the AI can assist with other tasks as well, such as designing logos using SVG. Tailwind Chat gives you complete ownership over the code it generates, with no licensing requirements. The platform also offers a robust code editor, the Monaco Code Editor, to allow you to make edits without leaving your browser. You can easily undo, go back to previous designs, and save your chats for future reference. Tailwind Chat is built on OpenAI, but their code is designed to be compatible with other AI systems in the future. Get ready to revolutionize your design workflow with Tailwind Chat!

The discussion mainly consists of two comments:
1. User "mcpdc" mentions that they experimented with using simplicityNode.js, Express, and Postgres, specifically using the Vercel ORM. They also mention using HTMX for frontend interactions and Mithril for React-like requirements. Lastly, they mention using the Digital Ocean App Platform for $5 million.
2. User "p2hari" responds, saying that the response time and copyHTML work indication are good. They also mention that they worked nicely with the chat request.
User "mcpdc" thanks "p2hari" and expresses their agreement with their comments.

