## AI Submissions for Tue Jan 13 2026 {{ 'date': '2026-01-13T17:17:47.770Z' }}

### vLLM large scale serving: DeepSeek 2.2k tok/s/h200 with wide-ep

#### [Submission URL](https://blog.vllm.ai/2025/12/17/large-scale-serving.html) | 139 points | by [robertnishihara](https://news.ycombinator.com/user?id=robertnishihara) | [45 comments](https://news.ycombinator.com/item?id=46602737)

vLLM v0.11 completes V1 migration, pushes MoE throughput to 2.2k tok/s per H200

- What’s new: vLLM fully drops the V0 engine in 0.11.0, completing the move to its V1 architecture. Backed by 1,969 contributors and 950+ commits in the past month (as of 12/18/25), it’s now featured in SemiAnalysis’ open-source InferenceMax benchmarks and is used in production by Meta, LinkedIn, Red Hat, Mistral, and Hugging Face.

- Performance: Community runs on a CoreWeave H200 cluster (InfiniBand, ConnectX-7) report sustained 2.2k tokens/s per H200 GPU in multi-node setups, up from ~1.5k. Gains come from kernel work (silu-mul-quant fusion, Cutlass QKV, TP attention fixes) and Dual Batch Overlap (DBO) for decode—translating to fewer replicas for the same QPS and better token-per-dollar.

- DeepSeek-style serving: 
  - Wide-EP: Expert Parallelism combined with Data Parallelism improves effective KV cache and avoids MLA pitfalls of Tensor Parallelism; enable with --enable-expert-parallel. Supports DeepEP high-throughput all-to-all, Perplexity MoE kernels, and NCCL AllGather-ReduceScatter.
  - DBO: Overlaps collective comms with compute via microbatch worker threads and CUDA graphs; enable with --enable-dbo and tune with --dbo-decode-token-threshold.
  - Expert load balancing: Hierarchical and global EPLB policies (--enable-eplb) smooth real-world routing skew.

- Other optimizations: Async scheduling, disaggregated serving, CUDA graph mode, DeepGEMM default, integrated DeepEP kernels, SiLU kernel for DeepSeek-R1.

Why it matters: Open-source, production-grade throughput for sparse MoE and disaggregated serving makes frontier models cheaper and easier to run at scale.

Here is a summary of the discussion:

**Economic Implications & Pricing Sustainability**
The most active debate focused on the economics of serving LLMs given vLLM's new throughput numbers (2,200 tokens/s per H200).
*   **Cost Calculations:** Users crunched the numbers on hardware costs (~$750k for a 16xH200 system) versus throughput. Several commenters estimated the raw cost (hardware depreciation + electricity) to be between **$0.03 and $0.25 per million tokens**.
*   **DeepSeek Validation:** Many noted that these efficiency gains suggest DeepSeek’s ultra-low API pricing is actually sustainable and potentially profitable, rather than just "VC money subsidizing consumption."
*   **The "Utilization" Catch:** Skeptics pointed out that theoretical benchmarks don't match real-world serving. Factors like context length, request concurrency, and fluctuating traffic mean 100% utilization is impossible, making actual costs higher than the raw math suggests.

**Performance & Optimization**
*   **Source of Gains:** Users discussed where the 40%+ performance jump came from. While vLLM uses Python, commenters clarified that the gains are due to low-level optimizations (kernel fusion, better scheduling, JIT compilation) rather than high-level language tweaks.
*   **Future Headroom:** Several comments suggested this is just the beginning ("greenfield") of inference optimization, predicting further gains as engines target 4-bit quantization and Blackwell architecture, which could potentially drop costs to ~$0.11 per million tokens.
*   **Benchmarks:** There were calls for more transparent, direct comparisons between open-source engines (vLLM, SGLang, TRT-LLM), with users noting a lack of standardized testing for different serving configurations.

**Hardware & Compatibility**
*   **H200 vs. Cerebras:** One user noted they expected these kinds of throughput numbers from waferscale engines (Cerebras), making the H200 performance particularly impressive.
*   **AMD Support:** Users discussed support for AMD GPUs via ROCm. While vLLM works on AMD, some noted that for single-user scenarios (like on Strix Halo), it can be slower than Ollama/llama.cpp, as vLLM is optimized for high-throughput batching rather than single-stream latency.

### We can't have nice things because of AI scrapers

#### [Submission URL](https://blog.metabrainz.org/2025/12/11/we-cant-have-nice-things-because-of-ai-scrapers/) | 434 points | by [LorenDB](https://news.ycombinator.com/user?id=LorenDB) | [233 comments](https://news.ycombinator.com/item?id=46608840)

Anubis: a browser proof‑of‑work gate to blunt AI scrapers

- What it is: A lightweight, Hashcash-style proof‑of‑work (PoW) page that runs in the browser to slow down large‑scale scraping. Legitimate visitors incur a tiny, one‑off computation; mass scrapers pay a steep, cumulative cost.
- Why it exists: The site’s admin says aggressive AI scraping has caused downtime and resource strain. Anubis is positioned as a stopgap that keeps the site usable while more targeted bot detection (e.g., fingerprinting headless browsers via font rendering and similar signals) is developed.
- How it works: When you visit, a “Making sure you’re not a bot” page triggers a PoW challenge in JavaScript. At human scale, the added load is meant to be negligible; at scraper scale, it becomes expensive.
- Trade‑offs called out: It’s a compromise—some friction for real users in exchange for shifting costs to scrapers. It requires modern JavaScript and can be blocked by privacy/anti‑fingerprinting tools like JShelter; users may need to disable such plugins for the domain. The goal is to eventually avoid showing challenges to visitors who look clearly legitimate.

**Discussion Summary:**

The discussion pivoted from the specific technical implementation of Anubis to the broader economics and ethics of AI scraping.

*   **The "Data Dump" Alternative:** A prevalent counter-proposal was that instead of blocking scrapers with computation tasks, sites should offer a standardized bulk download (e.g., a gzipped tarball or torrent) of their public data. Commenters argued this converts an "adversarial" relationship into a coordination problem, saving bandwidth for both the host and the scraper.
*   **Standardization Challenges:** Users debated how to signal these bulk endpoints to bots. Suggestions included the emerging `/llms.txt` standard, specific `robots.txt` directives, or embedding instructions in HTTP 429 (Too Many Requests) headers.
*   **Skepticism of AI Actors:** Many were skeptical that AI companies would respect "polite" standards. The consensus among several users was that scrapers often ignore `robots.txt` and copyright law already, implying they are indifferent to resource consumption on the target's end.
*   **Impact on the Small Web:** The conversation highlighted the financial toll on hobbyist sites. Users shared anecdotes of hosting accounts being suspended or costs ballooning due to aggressive scraping, noting that individual webmasters pay the price for AI training data.
*   **Infrastructure Implications:** Some speculated that the inability to effectively Ip-block scrapers (due to VPNs and botnets) might force a faster migration to IPv6 or drive the web toward closed, gated communities to survive.

### Confer – End to end encrypted AI chat

#### [Submission URL](https://confer.to/) | 100 points | by [vednig](https://news.ycombinator.com/user?id=vednig) | [75 comments](https://news.ycombinator.com/item?id=46600839)

I’m ready to summarize—could you share the Hacker News submission you want covered?

Please provide any of the following:
- HN link or item title + URL
- The article text or key excerpts (helpful if paywalled)
- Any notable HN comments you want included

Preferences (optional):
- Length: ultra-brief (50–80 words), standard (120–180), or in-depth (250–350)
- Tone: neutral, punchy, or technical

I’ll return:
- TL;DR
- Key takeaways (3–5 bullets)
- Why it matters
- HN chatter highlights (if provided)

Based on the discussion provided, here is the summary of the Hacker News submission regarding **Confer**.

**Submission:** **Confer – End-to-End Encrypted AI Inference**
**Url:** `https://confer.to/blog` (Derived from context)

### TL;DR
Confer has launched an AI inference platform that claims to be "end-to-end encrypted" (E2EE). Unlike standard LLM providers (like OpenAI) that can technically read user prompts, Confer processes data inside Trusted Execution Environments (TEEs). This ensures data is encrypted on the user's device and only decrypted momentarily inside a hardware-isolated "enclave" on the server, theoretically preventing even confer—or a server intruder—from seeing the content.

### Key Takeaways
*   **The "E2EE" Claim:** The service uses TEEs (likely Intel SGX or similar) to extend the encryption boundary. Data remains encrypted until it hits the specific CPU interacting with the model.
*   **Remote Attestation:** The security model relies on users (or their client software) verifying a cryptographic hash of the code running on the server to ensure it hasn't been tampered with before sending data.
*   **Privacy Compromise:** The discussion highlights that while Fully Homomorphic Encryption (FHE)—processing data *while* it stays encrypted—is the privacy "holy grail," it is currently 100x too slow or expensive. TEEs are presented as the practical 5–10 year solution.
*   **Browser Support:** The implementation leans heavily on modern browser cryptoprimitives (Passkeys/PRF), causing some compatibility issues for users on specific setups (e.g., Firefox on Linux).

### Why It Matters
This technology attempts to solve the biggest enterprise hurdle for AI adoption: **Data Privacy.** If companies can guarantee that cloud providers *physically cannot* access their IP during inference, the barrier to using powerful remote LLMs drops significantly. This approach mirrors Apple’s recent "Private Cloud Compute" strategy and Signal's contact discovery architecture.

### HN Chatter Highlights
The comment section is technical and debating the semantics of "End-to-End":

*   **Definition Debate:** Users like **drfr** and **Stefan-H** debated if "E2EE" is the right term. In traditional messaging (Signal), E2EE is client-to-client. Here, the "recipient" is the Server/AI. The consensus leans toward it being E2EE *if* the server operator is effectively locked out via hardware.
*   **Hardware Trust:** **2bitencryption** and **binary132** voiced skepticism about TEEs, citing Moxie Marlinspike’s previous writing. They argue TEEs aren't magic; they are vulnerable to side-channel attacks and ultimately require trusting the chip manufacturer (e.g., Intel/AMD).
*   **Side Channels:** **shwnz** noted that while TEEs protect against direct memory reads, a compromised host system might still infer data via side channels, meaning it isn't a silver bullet.
*   **Comparison to TLS:** **pxys** asked how this differs from standard SSL/TLS. The answer provided was that TLS only protects data *in transit*. TEEs protect data *during execution* (in memory).

### FOSS in times of war, scarcity and (adversarial) AI [video]

#### [Submission URL](https://fosdem.org/2026/schedule/event/FE7ULY-foss-in-times-of-war-scarcity-and-ai/) | 158 points | by [maelito](https://news.ycombinator.com/user?id=maelito) | [110 comments](https://news.ycombinator.com/item?id=46598991)

FOSDEM 2026: FOSS in times of war, scarcity and (adversarial) AI

- Big idea: A FOSDEM main-track talk argues the post–Cold War conditions that helped FOSS flourish have vanished. Today, open tech sits at the center of geopolitical conflict, disinformation, and “hypercapitalist” power—raising hard questions about “any-use” freedoms and responsibility.
- Dual-use becomes any-use: While Europe tried to regulate “dual-use” tech, libre licenses enable unrestricted use. FOSS now powers everything from social platforms that polarize to authoritarian surveillance stacks, with private capital steering policy and norms.
- Scarcity and climate: With resource limits looming, the talk warns the current trajectory of energy and materials consumption is unsustainable—adding pressure to how FOSS is built, deployed, and maintained.
- AI as Trojan horse: Code-writing LLMs expand the software supply chain’s attack surface. Unlike clearly labeled binary blobs, opaque models can inject subtle, hard-to-detect errors or manipulations, whether through hallucinations or adversarial interference.
- Core question: How can the community preserve FOSS’s openness and public-good impact while hardening against wartime exploitation, political capture, and AI-driven supply-chain risks?

When/where: FOSDEM 2026, Main Track (Janson), Saturday, 10:00–10:50.

**FOSADEM 2026: FOSS in times of war, scarcity and (adversarial) AI**

This submission outlines a main-track presentation for FOSDEM 2026 regarding the existential crisis facing Free and Open Source Software (FOSS). The speaker posits that the geopolitical conditions that allowed FOSS to thrive—the post-Cold War "End of History" era of relative peace and techno-optimism—have collapsed. The talk argues that FOSS is now central to "dual-use" technologies, fueling both democratic platforms and authoritarian surveillance states, often without the regulatory guardrails applied to hardware.

Key points include:
*   **The Any-Use Dilemma:** Libre licenses allow unrestricted use, meaning private capital and hostile states can leverage open tools for polarization or warfare.
*   **Material Limits:** The era of infinite growth is ending; scarcity and climate change require a shift in how resource-intensive software is built and maintained.
*   **AI Risks:** Unlike binary blobs, AI models are "Trojan horses" capable of subtle hallucinations or injected adversarial defects that hard-code manipulation into the supply chain.
*   **The Challenge:** The community must figure out how to preserve public-good openness while hardening the ecosystem against political capture and wartime exploitation.

**Hacker News Discussion Summary**

The discussion considers the limits of software licenses in the face of physical power and state violence.

*   **The End of Techno-Optimism:** Commenters generally agreed with the premise that the 90s "End of History" optimism is over. User **Fiveplus** argued that the current ecosystem suffers from a hangover of that era, assuming good actors by default. There is skepticism that "ethical licenses" can stop bad actors; **lcptn** noted that restrictive licenses are largely irrelevant to hostile states or AI code generators that bypass attribution entirely.
*   **Physical Violence vs. Cryptography:** A significant portion of the debate focused on "XKCD 538" scenario (the $5 wrench attack). **cyber_kinetist** and **nathan_compton** emphasized that state actors use violence, imprisonment, and physical control over hardware, which mathematical encryption cannot prevent. **Espressosaurus** noted that "hard power" involves accessing fiber closets and jail time, rendering software defenses moot.
*   **Technical Mitigations:** Despite the threat of violence, **wizzwizz4** and **thwbgyd** argued for technical architectures that make coercion harder. Suggestions included geographically distributed keys (so no single person can be tortured for access), Shamir’s secret sharing, and "dead man switches." The consensus was that while you cannot make a system "violence-proof," you can organize distributed systems to be resilient against local physical force.
*   **FOSS Survival:** **FrustratedMonky** questioned if FOSS could even be invented in today's "hyper-capitalist" environment, suggesting it requires a specific historical context to flourish. Countering this, **bgyb** argued that the fundamental utility of cost-sharing and efficient distribution ensures FOSS will survive, even if it is co-opted by bad actors (e.g., North Korea using open code for missiles).
*   **Code as Politics:** There was a philosophical debate on whether code itself is political action. **positron26** argued that while text files aren't power, open source enables social mobility and bypasses institutional gatekeepers. However, **lttlstymr** warned against the buzzword "Zero Trust," arguing that trust is never eliminated, just shifted to different commercial providers or hardware.

### The insecure evangelism of LLM maximalists

#### [Submission URL](https://lewiscampbell.tech/blog/260114.html) | 239 points | by [todsacerdoti](https://news.ycombinator.com/user?id=todsacerdoti) | [248 comments](https://news.ycombinator.com/item?id=46609591)

The Insecure Evangelism of LLM Maximalists

A senior developer argues that while LLMs are great as “digital clerks” (research, docs, small code with tight specs), agentic “vibe coding” was a letdown: slow, error-prone, and high-overhead to babysit—leaving them feeling less effective as tokens ticked away. They push back on the trope that skeptics are just scared of change, noting they’d love a world where specs become working code, but today’s agents don’t deliver that for them. Their sharper claim: some loud evangelism may be projection—devs who find agents outperform their own skills insisting holdouts are threatened, rather than considering differences in baseline ability or workflow fit. The author stays open to being wrong (perhaps “not holding the agents properly”) while challenging evangelists to consider that they might simply not be strong programmers. A wry kicker: they’re available to clean up agent-generated code.

Why it matters: Captures a growing split between pragmatic LLM use and end-to-end agent workflows. Beyond hype, adoption hinges on reliability, speed, cost, and whether “agent handling” is a real, teachable skill that beats traditional workflows for experienced engineers.

**Discussion Summary:**

Commenters pushed back strongly against the author's theory that "evangelism" masks incompetence, citing high-profile technical leaders like Simon Willison (Django) and Antirez (Redis) who enthusiastically use LLMs despite having undeniable engineering skills. The conversation reframed the skepticism not as fear of change, but as a clash of engineering philosophies: one user argued that the "hacker drive" is fueled by understanding and fixing deterministic systems, whereas "vibe coding" feels like the drudgery of cleaning up "stochastic messes" (akin to an artist fixing extra fingers on AI images). Others suggested the hostility often stems from corporate pressure—such as managers mandating "minimum AI usage" quotas—rather than the technology itself, while acknowledging that internet culture inevitably polarizes tool usage into "cult-like" camps.

### Mozilla's open source AI strategy

#### [Submission URL](https://blog.mozilla.org/en/mozilla/mozilla-open-source-ai-strategy/) | 187 points | by [nalinidash](https://news.ycombinator.com/user?id=nalinidash) | [196 comments](https://news.ycombinator.com/item?id=46599897)

Mozilla: “Owners, not renters” — an open-source AI push

Raffi Krikorian argues we’re drifting toward “rented intelligence,” where closed AI platforms mediate how we think and act — and can change the rules at will. Mozilla wants to rerun its Firefox playbook: make the “user agent” work for users again, this time at AI’s emerging “Layer 8” that will negotiate, filter, and recommend on our behalf.

Key points:
- Why closed wins today: It’s a developer experience (DX) problem, not a values problem. Big providers bundle models, GPUs, guardrails, monitoring, and billing into a single API that “just works,” while open-source tooling is powerful but fragmented.
- The shift underway: 
  - Small, task-tuned models (1–8B params) are now good and run on existing hardware.
  - Economics favor self-hosting; some companies report major savings moving to open stacks.
  - Governments want sovereign control over AI supply chains.
  - Users expect instant, contextual AI that isn’t locked to one platform.
- Openness will win when it’s the better deal: cheaper, more capable, and just as easy to use — not merely more principled.
- Where cracks are forming: Mozilla sees four tipping points; first up is developer experience, because the stacks and defaults developers choose now will set the future. (The post continues from here.)

The takeaway: Mozilla plans to make open AI competitive on usability and integration so people can own, not rent, the intelligence layer that will soon mediate the web.

Based on the discussion, the core debate centers on whether Mozilla’s pivot to AI is a necessary survival strategy or another "distraction" from its core mission of building a competitive browser.

**The "Distraction" vs. "Diversification" Debate**
*   **Critics of the Pivot:** Many users argue that Mozilla continues to misstep by pouring resources into non-browser projects (VPNs, AI, etc.) while Firefox loses market share. They contend that if Mozilla focused solely on making Firefox the fastest, most reliable engine (addressing long-standing complaints about hardware acceleration on Linux and basic UI speeds), users would return.
*   **The "Innovator’s Dilemma" Defense:** Defenders point out a contradiction in user sentiment: people complain that Mozilla is too reliant on Google search royalties, yet criticize any attempt Mozilla makes to diversify revenue streams (like this AI initiative).
*   **The Threat to Search Revenue:** Several commenters note that the rise of AI threatens the search-licensing model (Google paying Mozilla). Consequently, Mozilla *must* find a new "Layer 8" application to own the user relationship, or they will go bankrupt when the search deal eventually dissolves.

**Firefox Quality and Management**
*   **Feature Velocity:** A Mozilla employee and other users noted recent shipping wins (vertical tabs, profile switchers, Rust integration), but critics feel development is too slow compared to the budget.
*   **Budget Efficiency:** Comparisons were drawn to the **Ladybird** browser project, which is building a new engine with a tiny fraction of Mozilla’s staff/budget, leading users to question the efficiency of Mozilla’s $500M+ annual spend.
*   **Endowment Strategy:** There was debate over Mozilla's financial endowment (estimated over $1B). Some view it as a safety net that should be hoarded to keep Firefox alive indefinitely; others see it as a "VC fund" that must be deployed to invent the future of the web (and revenue) before the current model collapses.

**Hypothetical Scenarios**
*   **A Cloudflare Acquisition?** A sub-thread discussed the idea of Cloudflare acquiring Firefox/Mozilla, citing the shared talent pool (heavy usage of Rust) and alignment against Google. However, others pointed out the irony, as Cloudflare’s bot protections/CAPTCHAs are notoriously hostile to Firefox users.

**Consensus**
While the community generally supports the *ideal* of open AI, there is deep skepticism that Mozilla can execute it successfully, given their perceived history of neglecting the core localized browser experience in favor of "moonshot" projects.

### Why we don’t use AI

#### [Submission URL](https://yarnspinner.dev/blog/why-we-dont-use-ai/) | 110 points | by [parisidau](https://news.ycombinator.com/user?id=parisidau) | [71 comments](https://news.ycombinator.com/item?id=46609279)

Why We Don't Use AI – Yarn Spinner team says no to generative AI, on principle

- The maintainers of Yarn Spinner, a widely used open‑source dialogue tool for games, lay out why they neither use nor integrate generative AI—and won’t accept AI‑generated contributions.
- Core claim: today’s AI products are largely designed to cut headcount or extract more work without new hires; the team doesn’t want to normalize or fund that ecosystem.
- Background: they have deep ML experience (research, books, game bots) and were once enthusiastic as tooling/GPU access improved. Around 2020, they soured as industry focus shifted to generative imagery/chatbots, mitigation work was sidelined, and critics were pushed out.
- Beyond labor: they acknowledge many other issues (bias, opacity, etc.). Even if labor harms were solved, more hurdles would remain—but they’re arguing one major point at a time.
- Product philosophy: reject “tool‑driven development” (“use AI or be left behind”) in favor of “does this help make better games?” They prefer fewer, polished features over hype integrations and will add/remove features based on real dev needs.
- No “ethical DIY AI” for now: building bespoke models would be time‑intensive, and their example could nudge others toward mainstream AI platforms they object to.
- Future stance: open to revisiting ML if the landscape changes meaningfully; until then, no generative AI in Yarn Spinner or their workflow.

Why it matters: A prominent game tooling project is drawing a clear line against generative AI on ethical and practical grounds, signaling to studios and open‑source communities that “not adopting AI” can be a deliberate, product‑focused choice rather than a lagging one.

Here is a summary of the discussion:

**Summary of Discussion**

The Yarn Spinner team's rejection of generative AI sparked a polarized debate on Hacker News, moving quickly from the specific tool to broader questions about labor, economic systems, and the utility of coding assistants.

*   **Nuance and the "Indie" Defense:** Several users felt the maintainers’ stance lacked nuance, conflating "Enterprise Scale Replacement" (firing 500 support staff) with "Assistive Tooling" for solo developers. Commenters argued that for small indie teams, AI is a vital force multiplier that unblocks tricky coding problems or generates assets they couldn't otherwise afford, drawing a distinction between corporate greed and indie survival.
*   **The Utility Debate:** A sub-thread debated whether LLMs actually help with "tricky" coding problems. Skeptics argued that LLMs fundamentally cannot solve novel architectural problems, often providing hallucinated or "junior-level" code that experienced engineers have to fix. Others pushed back, stating that regardless of perfection, the tools provide necessary efficiency for developers working alone.
*   **Automation vs. Capitalism:** The "jobs" argument triggered a philosophical discussion on luddism and automation. While users generally agreed that eliminating toil (like tractors or dishwashers) is historically positive, they argued that the current economic system makes AI a threat. The consensus among several commenters was that the technology isn't the enemy; rather, the lack of a social safety net (like UBI) turns labor-saving technology into a livelihood-destroying crisis.
*   **Virtue Signaling Accusations:** A segment of the discussion dismissed the post as "virtue signaling" or part of a culture war, suggesting that condemning the technology outright is a performative stance that ignores the practical realities of modern software development.

### Anthropic invests $1.5M in the Python Software Foundation

#### [Submission URL](https://discuss.python.org/t/anthropic-has-made-a-large-contribution-to-the-python-software-foundation-and-open-source-security/105694) | 392 points | by [ayhanfuat](https://news.ycombinator.com/user?id=ayhanfuat) | [174 comments](https://news.ycombinator.com/item?id=46601902)

Anthropic is donating $1.5M over two years to the Python Software Foundation, earmarked primarily for Python ecosystem security. The PSF says the gift will accelerate its security roadmap—particularly hardening PyPI against supply‑chain attacks—and also support core operations like the CPython Developer-in-Residence program, community grants, and running infrastructure such as PyPI.

HN chatter: One commenter notes the gift is tiny relative to big‑tech scale (e.g., 1.5M is ~0.005% of 30B), hinting at PR optics, while the PSF frames it as a landmark contribution with outsized impact for Python’s community and infrastructure.

**Governance and Corporate Influence**
A significant portion of the discussion focused on the PSF's leadership structure, sparked by a commenter questioning the optics of a Microsoft employee serving as the PSF Board Chair.
*   **The Critique:** Skeptics argued that having an employee of a major tech corporation ($15B+ revenue) leading the board of a critical non-profit creates potential conflicts of interest or "unconscious bias," drawing comparisons to a government official regulating their private sector industry.
*   **The Defense:** Community members (including PSF insiders) clarified that the Board is distinct from the **Python Steering Council** (which handles technical decisions). They noted that board positions are unpaid volunteer roles, the Chair has limited executive power compared to the Executive Director, and bylaws restrict the number of board members from any single company (max 1/3 or 2 members).
*   **Context:** Defenders emphasized that the current Chair served the Python community long before joining Microsoft and that questioning their ethics based solely on employment is unfounded.

**The value of $1.5M**
*   **"Cheap PR":** Some users felt the donation was "tiny" relative to Anthropic's valuation and the immense value they derive from the ecosystem, labeling it an inexpensive way to buy good press.
*   **"Better than nothing":** Counter-arguments highlighted that many massive industries (specifically Investment Banking and Finance) run entirely on Python but contribute $0. Users argued that criticizing a donation creates a perverse incentive; instead, the community should applaud the funding to normalize corporate support for open-source infrastructure.

**Strategic Utility**
*   Commenters noted that this is a pragmatic investment for Anthropic rather than pure charity. Because LLMs (like Claude) generate millions of lines of Python code and rely on the ecosystem's stability, hardening PyPI against supply-chain attacks directly reduces Anthropic's own risk vector.

### Signal leaders warn agentic AI is an insecure, unreliable surveillance risk

#### [Submission URL](https://coywolf.com/news/productivity/signal-president-and-vp-warn-agentic-ai-is-insecure-unreliable-and-a-surveillance-nightmare/) | 334 points | by [speckx](https://news.ycombinator.com/user?id=speckx) | [102 comments](https://news.ycombinator.com/item?id=46605553)

Signal leadership warns agentic AI is a security and privacy disaster in the making

At 39C3 in Hamburg, Signal’s Meredith Whittaker (President) and Udbhav Tiwari (VP of Strategy & Global Affairs) argued that agentic AI—especially when embedded at the OS level—creates a surveillance-friendly, malware-prone attack surface and remains too unreliable for autonomous tasks.

What they highlighted
- OS-level agents as “forensic dossiers”: Microsoft’s Recall was cited as a cautionary example—frequent screenshots, OCR, semantic tagging, and a timeline of user activity consolidated into a single local database. Malware or indirect prompt-injection could exfiltrate it, effectively sidestepping end-to-end encryption by capturing plaintext on-device. Signal added a “prevent screen capture” flag but says that’s only triage.
- Reliability breaks down fast: Agents are probabilistic. Even optimistic per-step accuracy compounds poorly—at 95% per step, a 10-step task drops to ~60% success; 30 steps to ~21%. At 90% per step, a 30-step task is ~4%. They said the best agent models still fail ~70% of the time.
- Enterprise and consumer risk: Centralized life-logs plus autonomous actions invite catastrophic data leakage, regulatory exposure, and an erosion of user trust in already overhyped tech.

Their prescriptions
- Hit pause on OS-level life-logging and plaintext, all-in-one databases that malware can grab.
- Default to opt-out for users and require explicit developer opt-ins for integrations.
- Provide radical transparency and granular auditability of how agents collect, store, and act on data.

Bottom line: Without a course correction, they warn the agent era could implode under security failures and backlash—long before it delivers on its promises.

Based on the discussion, commenters largely shifted the blame from "Agentic AI" specifically to the fundamental failures of modern Operating System (OS) security models.

**The "OS Problem" vs. The "AI Problem"**
*   **Fundamental Flaws:** Several users argued that the risks posed by Agentic AI are actually symptoms of insecure-by-design operating systems. They noted that mainstream systems (Windows, standard UNIX) were not designed to assume software is untrusted. One commenter noted that while Microsoft sells computers, they have not historically prioritized designing strict security models.
*   **Better Models Exist:** Participants pointed to niche or mobile operating systems (Plan 9, seL4, Fuschia, Qubes OS) as examples of architectures that handle untrusted processes better. iOS was frequently cited as a mainstream success story where applications are sandboxed by default and must explicitly request capabilities (e.g., access to contacts or location), unlike desktop environments where network and file access are often open by default.

**Complexity and Developer Incentives**
*   **Security vs. Usability:** There was a debate regarding why secure OSs aren't the standard. Some argued that implementing "secure-by-default" systems is a "tedious nightmare" for developers (citing difficulties with TLS and immutable filesystems). Others countered that current security measures (firewalls, patching) are merely "security theater" required to patch bad architectural design.
*   **Market Forces:** Commenters suggested that secure systems like Andrew Tanenbaum’s *Amoeba* existed decades ago but failed because the market prioritizes speed, backward compatibility, and developer ease-of-use (e.g., `npm run`) over strict security boundaries.
*   **Containerization Blame:** Some criticism was directed at container ecosystems (Docker/OCI) for encouraging bad practices, such as allowing permission binds and refusing to disable privileged flags by default, effectively bypassing OS security for convenience.

**The Changing Threat Model**
*   **Adversarial Software:** One user noted that legacy security models assumed the user controlled the software. The modern era—and specifically Agentic AI—introduces an adversarial model where software runs *against* the user's interests (telemetry, surveillance), rendering old "user permission" models insufficient.
*   **Network Assumptions:** The discussion highlighted that while UNIX was multi-user, early desktop security models did not anticipate the modern "always-connected" internet, leaving desktop clients blindly trusting local networks—a concept that Agentic AI exploits.

### Games Workshop bans staff from using AI

#### [Submission URL](https://www.ign.com/articles/warhammer-maker-games-workshop-bans-its-staff-from-using-ai-in-its-content-or-designs-says-none-of-its-senior-managers-are-currently-excited-about-the-tech) | 226 points | by [jsheard](https://news.ycombinator.com/user?id=jsheard) | [122 comments](https://news.ycombinator.com/item?id=46607681)

Games Workshop bans AI in Warhammer content and design

- CEO Kevin Rountree says staff are prohibited from using generative AI in any official content or design work, including entries to GW-run competitions. A few senior managers are allowed to explore the tech, but leadership is “not excited” about it.
- Rationale: protect IP, preserve human-made art/writing/sculpting, and reduce data/security/compliance risks as AI features creep onto devices by default.
- GW is doubling down on human creatives, expanding hiring across concept art, writing, and sculpting for its Warhammer Studio.
- Context: Warhammer’s community is highly protective of its art; even the suspicion of AI in official imagery has sparked backlash (e.g., a recent Displate controversy the company denied).
- Contrast: While publishers like EA and Square Enix tout aggressive AI adoption, GW is positioning itself as a human-first outlier in entertainment.

Why it matters: For a lore- and art-driven brand, trust in authenticity may outweigh AI’s efficiency gains. Expect stricter partner guidelines, more IP enforcement, and GW marketing its “human-made” pedigree as a differentiator.

Here is a summary of the discussion:

**Community Alignment vs. Practical Utility**
Commenters largely agreed that Games Workshop made a savvy business move, noting that the tabletop community zealously dislikes "tech bro" trends like NFTs and crypto. By taking a hardline anti-AI stance, GW avoids a "multi-year headache" with their core customer base. However, one user noted a contradiction: while non-technical board gamers hate AI art, many admitted to using AI to generate code for websites and apps because professional programmers remain too expensive to hire.

**The Double Standard: Art vs. Code**
A significant portion of the discussion focused on why the public views AI replacing artists as a tragedy, but AI replacing programmers as progress. Several theories emerged:
*   **Sympathy and Class:** Artists are generally viewed as struggling creatives, garnering public protection. Conversely, software engineers are seen as high-paid, "entitled," and "arrogant." Some users suggested the non-tech world feels a sense of *schadenfreude* at the prospect of AI "spanking" the comfortable tech labor market.
*   **Nature of the Work:** One user argued that people view programming as "assembly line work" rather than a creative endeavor. Code is often refactored, merged, and owned collectively, whereas art has a strong attachment to specific authorship and authenticity.
*   **Training Data Ethics:** There was a debate regarding the fairness of training models. While AI art generators scraped copyrighted portfolios (like ArtStation) against artists' wishes, AI coding assistants were largely trained on open-source code. However, critics countered that "Open Source" does not mean "Public Domain," and licenses like GPL still carry legal weight that AI training ignores.

**The "Revenge" of the Creatives**
A particularly heated thread highlighted the irony of the current moment. A user identifying as both an artist and developer noted that when technology crushed the livelihoods of traditional artists and manufacturing workers, the tech industry’s response was often dismissed as "adapt or die." Now that the same technology threatens white-collar coding jobs, the sudden concern from developers feels hypocritical to those who were previously told their displacement was just "progress."

