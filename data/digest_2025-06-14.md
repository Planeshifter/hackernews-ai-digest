## AI Submissions for Sat Jun 14 2025 {{ 'date': '2025-06-14T17:11:22.079Z' }}

### I have reimplemented Stable Diffusion 3.5 from scratch in pure PyTorch

#### [Submission URL](https://github.com/yousef-rafat/miniDiffusion) | 445 points | by [yousef_g](https://news.ycombinator.com/user?id=yousef_g) | [71 comments](https://news.ycombinator.com/item?id=44276476)

Today's top Hacker News story centers around a fascinating project for machine learning and AI enthusiasts: "miniDiffusion," a fresh reimplementation of Stable Diffusion 3.5 using only pure PyTorch. Gaining attention with 428 stars and 22 forks, this repository by Yousef Rafat is designed for educational, experimental, and hacking uses. miniDiffusion aims to simplify the understanding of the Stable Diffusion model by minimizing code complexity, maintaining only about 2800 lines of code.

Key components include implementations for core image generation modules like VAE, CLIP, and T5 Text Encoders, operational along with Byte-Pair & Unigram tokenizers. It also features a Multi-Modal Diffusion Transformer Model, Flow-Matching Euler Scheduler, and Joint Attention mechanisms.

This repository is perfect for those wanting to dive deeper into the technical workings of Stable Diffusion, offering scripts for both training and inference. Note that while the project is promising, it's marked with a cautionary note about its experimental status, indicating ongoing testing requirements.

Whether you're looking to learn more about AI image generation or contribute to an open-source project, miniDiffusion offers a robust platform grounded in PyTorch's capabilities—a must-see for developers and AI researchers alike!

**Summary of Discussion:**

1. **Technical Challenges & Bugs in Implementations:**  
   - Users highlighted issues in reference implementations (e.g., CLIP tokenizers, T5 encoders) causing training/inference mismatches. For example, incorrect padding tokens in SDXL and missing masking in T5 encoders lead to degraded outputs.  
   - Emphasis on the difficulty of replicating training processes accurately, stressing the importance of bug-free reference code for reproducibility.

2. **Dataset & Training Details:**  
   - The training dataset is small and fashion-focused, raising questions about model generalizability. The author clarified it’s for fine-tuning, not full training, due to hardware constraints.  
   - Debate on whether the model uses pre-trained weights (e.g., from Hugging Face) or trains from scratch, with users noting the computational expense of full training.

3. **Educational Value:**  
   - Praised as a learning resource, with recommendations for beginners to pair it with Fast.ai’s course on Stable Diffusion.  
   - Users appreciated the minimal codebase for demystifying diffusion models.

4. **Hardware & Performance:**  
   - PyTorch’s optimization for non-NVIDIA GPUs (e.g., Apple Silicon, AMD via Vulkan) was discussed, though challenges remain for full performance parity with CUDA.  
   - Some skepticism about PyTorch’s readiness for non-traditional hardware setups.

5. **Licensing & Copyright Concerns:**  
   - Debates over the legal status of model weights and code. Some argue code/architecture is copyrightable, while weights (as mathematical outputs) may not be.  
   - Uncertainty around whether using or replicating models like SD 3.5 infringes on Stability AI’s licenses.

6. **Code Optimization Suggestions:**  
   - A user proposed consolidating linear layers (`self.qkv` instead of separate `q`, `k`, `v`) for efficiency, sparking discussion on trade-offs between readability and performance.

7. **Community Reactions:**  
   - Mixed feelings: Some users admitted past struggles with “dirty” implementations, while others praised the project’s clarity despite technical hurdles.  
   - Humorous relief about avoiding proprietary CUDA code in favor of PyTorch’s abstractions.

**Key Takeaway:**  
The discussion underscores the complexity of reimplementing cutting-edge models, balancing technical accuracy with educational goals, and navigating unresolved legal/ethical questions in open-source AI.

### Unsupervised Elicitation of Language Models

#### [Submission URL](https://arxiv.org/abs/2506.10139) | 128 points | by [kordlessagain](https://news.ycombinator.com/user?id=kordlessagain) | [19 comments](https://news.ycombinator.com/item?id=44276041)

In an exciting development for the world of Artificial Intelligence and language processing, a new research paper titled "Unsupervised Elicitation of Language Models" challenges the traditional reliance on human supervision to fine-tune language models. Authored by a team of researchers including Jiaxin Wen and Jan Leike, the paper introduces an innovative method known as Internal Coherence Maximization (ICM). This unsupervised algorithm fine-tunes language models by using their own generated labels, bypassing the need for external human input entirely.

The groundbreaking aspect of this research is its ability to harness superhuman capabilities of language models far more effectively than previous human-supervised methodologies, particularly on tasks where these AI models already possess exceptional prowess. Tested on benchmarks like GSM8k-verification and TruthfulQA, ICM not only matched but, in many cases, outperformed traditional training methods reliant on human labels.

Moreover, the study highlights the potential of this method in refining advanced AI systems, such as an unsupervised reward model and a Haiku-based Claude 3.5 assistant, both surpassing their human-supervised equivalents. This leap towards autonomous model enhancement could revolutionize how AI is trained, freeing it from the constraints and biases of human intervention. As this research progresses, its impact will likely reverberate across the AI community, offering new pathways for developing more capable and independent AI systems.

The Hacker News discussion on the "Unsupervised Elicitation of Language Models" paper highlights several key debates and insights:

1. **"Superhuman" Performance Debate**:  
   Users questioned the paper’s use of the term "superhuman," arguing that while LLMs excel in breadth and speed (e.g., solving math problems faster than humans), their depth in nuanced tasks (e.g., identifying gender from blog posts) may not truly surpass human expertise. Comparisons were drawn to tools like calculators, which are "superhuman" in specific domains but limited elsewhere.

2. **Self-Generated Labels and Bias**:  
   Critics raised concerns about the ICM method’s reliance on self-generated labels. While the approach avoids human biases, some worry it could propagate the model’s existing misconceptions or "internalized patterns," especially if training data lacks ground truth. A user noted that human labels, though imperfect, still provide a critical benchmark for quality.

3. **Technical and Philosophical Implications**:  
   Commenters compared the method to "weak supervision" techniques and highlighted its potential to reduce dependency on human-labeled data in reinforcement learning (RLHF). Philosophically, some saw it as a step toward AI systems that refine their worldviews through internal consistency rather than human input.

4. **Mixed Reactions**:  
   While many praised the innovation, others expressed caution. Excitement centered on the efficiency gains and novel applications (e.g., unsupervised reward models), but fears about unintended consequences—such as entrenching model biases or opaque decision-making—were also prominent.

5. **Miscellaneous Notes**:  
   - A user pointed out the researchers’ backgrounds, including a science communicator on the team.  
   - References were made to related works, such as Max Tegmark’s "Geometry of Truth," suggesting broader interest in truth-seeking mechanisms for AI.  

Overall, the discussion reflects cautious optimism about the paper’s technical advances but underscores unresolved questions about evaluation metrics, terminology, and long-term implications for AI autonomy.

### Clinical knowledge in LLMs does not translate to human interactions

#### [Submission URL](https://arxiv.org/pdf/2504.18919) | 95 points | by [insistent](https://news.ycombinator.com/user?id=insistent) | [35 comments](https://news.ycombinator.com/item?id=44279209)

In a thought-provoking revelation on Hacker News, an intriguing research paper titled "Clinical knowledge in LLMs does not translate to human interactions" has sparked much discussion. Authored by a team of experts including Andrew M. Bean, Rebecca Payne, and others, this study delves into the limitations of Large Language Models (LLMs) when applied to real-world clinical settings.

The paper, recently published on arXiv, presents a critical analysis of how the ostensibly sophisticated clinical knowledge stored within LLMs such as GPT models, might not necessarily result in effective human interactions in medical environments. Despite their apparent prowess in data handling and knowledge simulation, these AI models stumble when interfacing in nuanced, real-world human communication, especially in healthcare scenarios that demand empathy, sensitivity, and human judgment.

The discussion surrounding the paper raises questions on the applications of AI in sensitive fields like medicine, highlighting the gap between impressive technical capabilities and actual clinical efficacy. As the technology continues to evolve, this research underscores the critical importance of integrating human-centric design and ethics into the development and deployment of AI systems in healthcare.

This discourse adds a layer of caution to the unbridled enthusiasm around AI, reminding technologists, developers, and healthcare professionals to consider the broader implications and the current limitations of their implementations.

**Summary of Hacker News Discussion:**

The discussion around the paper "Clinical knowledge in LLMs does not translate to human interactions" highlights several key points and debates:

1. **Limitations in Patient Interaction**:  
   Users note that LLMs like ChatGPT struggle to replicate the nuanced, empathetic interactions required in clinical settings. Patients often withhold sensitive or embarrassing symptoms, a challenge human doctors navigate through trust and rapport—skills LLMs lack. For example, participants in the study withheld critical information when interacting with AI, mirroring real-world patient behavior.

2. **Diagnostic Shortcomings**:  
   Several commenters shared anecdotes of LLMs misdiagnosing conditions (e.g., misidentifying rashes or failing to account for geographic-specific diseases like Hand-Foot-Mouth disease). While LLMs excel at generating standard care recommendations, they may overlook contextual factors critical to accurate diagnosis.

3. **Human Expertise vs. AI Efficiency**:  
   Debate emerged over whether LLMs could replace doctors. Some argued LLMs are useful for rapid information distillation (e.g., summarizing medical guidelines) but lack the years of training and contextual judgment that physicians bring. Others emphasized that most clinical cases are routine, where LLMs might assist, but complex cases still require human expertise.

4. **Design and Liability Concerns**:  
   Users highlighted the need for better prompting strategies and interaction design to improve LLM utility in healthcare. Concerns were raised about liability—trusting LLMs with autonomous decisions could lead to harmful outcomes, especially if errors occur. One user noted that unless healthcare providers accept liability for AI recommendations, adoption will remain limited.

5. **Human-AI Collaboration**:  
   Suggestions included using LLMs as tools to assist doctors (e.g., drafting questions or providing rapid literature summaries) rather than replacing them. A study participant proposed structuring interactions where LLMs offer 2-3 potential diagnoses, allowing doctors to make final decisions.

6. **Cautious Optimism**:  
   While some praised LLMs for matching symptom-checking tools like WebMD, the consensus leaned toward cautious optimism. The community acknowledged LLMs’ potential but stressed their current limitations in replicating human judgment, empathy, and adaptability in dynamic clinical environments.

**Key Takeaway**:  
The discussion underscores a critical gap between LLMs’ technical knowledge and their ability to function effectively in real-world healthcare settings. Ethical integration, human oversight, and improved interaction design are seen as essential next steps for leveraging AI in medicine responsibly.

### Beware the Intention Economy: Collection and Commodification of Intent via LLMs

#### [Submission URL](https://hdsr.mitpress.mit.edu/pub/ujvharkk/release/1) | 22 points | by [yoonseokang](https://news.ycombinator.com/user?id=yoonseokang) | [4 comments](https://news.ycombinator.com/item?id=44277651)

In a thought-provoking deep dive, Yaqub Chaudhary and Jonnie Penn expand on the idea of the "intention economy" in their recent publication, "Beware the Intention Economy: Collection and Commodification of Intent via Large Language Models." This concept arises from the rapid adoption of AI and Large Language Models (LLMs) and adds a new layer of complexity to the internet's current attention economy.

Where the attention economy has focused on capturing users' attention as the primary commodity, the intention economy aims to capture and manipulate users' desires and motivations as data points to be bought and sold. The authors argue that this represents a potentially troubling shift, as companies are beginning to see human intentions—ranging from choosing a vacation spot to selecting political candidates—as valuable assets ripe for commodification.

Chaudhary and Penn outline how tech giants, leveraging the immense data processing power of LLMs, are gearing up for a frontier of personalized and persuasive technologies. These systems have the potential to not only understand but predict and wield influence over user intentions by engaging in hyper-personalized interactions that feel eerily familiar.

The article outlines a critical warning: the unchecked growth of the intention economy could undermine democratic norms by surreptitiously shaping consumer behavior and societal values. It challenges the optimistic perspectives of an intention economy as liberating, suggesting instead that it demands rigorous scrutiny and regulation.

By analyzing philosophical interpretations of "intention," alongside corporate maneuvers to leverage this concept, the authors invite readers to consider the broader social implications and ethical dilemmas of an AI-driven future where our very plans and purposes might become commodified. The piece is not just a forecast but a call to action, urging policymakers, technologists, and the general public to engage with the profound implications of this emerging digital marketplace.

**Summary of Discussion:**

The discussion revolves around concerns that AI-driven personal assistants and LLMs (Large Language Models) could amplify political manipulation and erode democratic processes. Key points include:

1. **Manipulation Risks**: Users express fears that wealthy, politically connected entities might use AI to subtly influence public opinion ("personal AI assistants start subtly manipulating"), potentially altering political beliefs for 75% of users in desired directions. This raises concerns about democracy being challenged by technology that prioritizes individualized persuasion over meaningful representation.

2. **Existing Threats vs. AI Exacerbation**: A debate emerges on whether AI introduces new risks or merely intensifies existing issues. One user argues that democracy was already under threat from social media’s control over agendas ("democracy losing without LLMs"), suggesting manipulation tactics ("bullshitting") have long been effective regardless of technological sophistication. Others counter that AI’s hyper-personalized targeting represents a dangerous escalation.

3. **Privacy Erosion**: Participants highlight longstanding privacy losses, such as cell network tracking, license plate readers, and online dossiers, noting that surveillance infrastructure already exists to create detailed individual profiles ("pattern people's can't worse"). This context implies AI could deepen existing invasive practices.

4. **Skepticism and Resignation**: Some users downplay AI’s unique impact, questioning whether it meaningfully worsens pre-existing trends, while others warn of unprecedented manipulation capabilities. The tone mixes urgency about AI’s potential with resignation about entrenched systemic issues.

**Overall**: The thread reflects tension between viewing AI as a novel threat to democracy and seeing it as part of a continuum of existing technological and social challenges, with shared concerns about privacy, manipulation, and the erosion of authentic representation.

### RAG Is a Fancy, Lying Search Engine

#### [Submission URL](https://labs.stardog.ai/rag-is-a-fancy-lying-search-engine) | 31 points | by [kendallgclark](https://news.ycombinator.com/user?id=kendallgclark) | [6 comments](https://news.ycombinator.com/item?id=44277902)

Are you ready to dive into the world of RAG, the fancy search engine that’s making waves in the GenAI universe? Kendall Clark provides a candid analysis of why RAG is all the rage but may not be the best choice for high-stakes enterprise. Let's break it down.

First, what exactly is RAG? It stands for Retrieval-Augmented Generation, a GenAI application design pattern that enhances LLM prompts with additional information for better responses. It might sound promising, but Clark argues it’s mostly hype, especially for sensitive industries where reliability is critical.

1. **The Sizzle Reel Effect**: RAG delivers impressive demos, which is part of its charm. Crafting a basic RAG system is so straightforward that there are countless open-source resources to get started, and this accessibility fuels rapid adoption.

2. **The Startup Surge**: Venture capitalists can’t seem to get enough of RAG-based startups. With a flood of VC funding, the space is teeming with new RAG applications, despite the inherently shallow moat these technologies offer.

3. **A16Z’s Influence**: Venture powerhouse Andreessen Horowitz has indirectly steered early GenAI investment towards RAG-friendly architectures, amplifying its deluge in the startup world. Their influence is undisputed, and where they lead, others follow.

4. **The Illusion of Science**: The substantial increase in LLM-focused research gives a veneer of legitimacy to RAG, supported by a cascade of publications. This wave of academic output serves as a reassuring, if somewhat cynical, crutch for investors.

5. **Search Engine Fatigue**: Traditional search technologies, primarily those backed by behemoths like Google, are perceived as stale, opening the door for innovation like RAG. It may not be superior, but it certainly is fresh.

Despite the buzz, Clark warns that RAG is ill-suited for regulation-heavy sectors due to its inherent unreliability—after all, it allows LLMs the last word, often with fabricated outcomes. While RAG's popularity surges, largely due to its demo appeal and the backing of influential VCs, Clark urges caution and discernment in its application, especially where accuracy and trust matter most. 

So, as we navigate the GenAI landscape, it's worth remembering that while RAG brings some flashy new tricks, it's not 'the' answer to our search desires, at least not without a critical eye on where and how it's used.

The Hacker News discussion around Kendall Clark's critique of Retrieval-Augmented Generation (RAG) highlights mixed reactions and extended concerns:  

1. **Skepticism Toward RAG's Reliability**: Users express doubts about RAG's suitability for critical applications, noting that while it leverages LLMs, their inherent unpredictability ("unstable or stubborn" nature) undermines trust. A sub-thread also humorously points out a broken link (404 error) in Clark’s blog post, hinting at broader concerns about the robustness of technical claims.  

2. **Bias and Unintended Consequences**: One comment raises alarm about developer bias influencing how LLMs frame user queries, potentially leading to misleading outcomes. This aligns with critiques about opaque AI decision-making in sensitive domains.  

3. **Practical Implementation Woes**: A user shares firsthand experience with RAG, arguing it introduces unnecessary complexity (e.g., prompting challenges, assumptions in curation) and higher costs due to reliance on LLMs. They suggest simpler, non-RAG solutions (like direct API calls) might be more efficient.  

4. **Critique of AI-Generated Content**: A parting jab notes the submission itself, likely AI-written, is "surprisingly long with little depth," reflecting broader skepticism about AI’s ability to produce substantive analysis.  

Overall, the discussion underscores a cautious stance toward RAG’s hype, emphasizing real-world limitations, hidden costs, and the need for critical evaluation—especially where accuracy and transparency matter.

### The z80 technique reveals the source code for Atlassian's 'rovo' AI assistant

#### [Submission URL](https://ghuntley.com/atlassian-rovo-source-code/) | 7 points | by [ghuntley](https://news.ycombinator.com/user?id=ghuntley) | [3 comments](https://news.ycombinator.com/item?id=44274427)

In a fascinating reverse engineering feat, a security researcher successfully deconstructed the Atlassian Command Line Interface (ACLI), particularly focusing on the 'rovo' functionality. The process, detailed on GitHub, involved converting the binary into ASM, generating technical specifications, and extracting source code, all documented meticulously in a markdown format. This investigation revealed 'Rovo Dev' as a sophisticated AI agent integrated with the Model Context Protocol (MCP) and equipped with extensive analytics capabilities.

The journey began with a thorough binary analysis, establishing that the executable was a Mach-O 64-bit Apple Silicon binary, coded in Go. This analysis unveiled standard macOS system library dependencies and a hidden treasure of embedded content related to AI tools and system prompts.

To unearth the ‘rovo’ secrets, the researcher utilized a series of string analysis and content discovery tools, unearthing references to various AI frameworks like Anthropic and OpenAI, along with system prompts related to Atlassian products. By detecting ZIP signatures within the binary, the researcher crafted a Python script to extract these embedded archives, revealing over 100 Python source files.

The extracted components painted a comprehensive picture of 'Rovo Dev'. The AI agent, operating with an interactive terminal UI, employed a security model based on session and permission-based controls, crucial for safeguarding operations. Additionally, the reverse engineering exposé detailed extensive analytics and telemetry systems that tracked user patterns, tool execution metrics, and even code modification activities.

Among the spoils were six detailed AI system operational templates—ranging from code review automation to Atlassian product integration, demonstrating Rovo’s prowess in automating and enhancing coding workflows.

Ultimately, the repository not only showcases the technical brilliance behind this reverse engineering but also provides a compelling example of how artificial intelligence can be utilized to decode complex software ecosystems effectively. The findings offer a treasure trove of insights into Atlassian's approach to AI integration and security within their command-line tools.

The discussion revolves around confusion and clarification regarding the relevance of the **Z80** (an 8-bit microprocessor from the 1970s) and **ZX Spectrum** retrocomputing context to the reverse-engineering analysis of Atlassian's CLI tool ("rovo"). Key points:  

- A user (**z80**) initially mentioned techniques like binary-to-ASM conversion and extracting 100+ Python files but included fragmented references to Z80 assembly, sparking ambiguity.  
- Another participant (**ghntly**) linked to a blog post titled "Z80" (likely unrelated to the original submission), further muddying the connection.  
- The original poster (**0points**) clarified that the Z80/XZ Spectrum references were off-topic and arguably irrelevant to the **Rovo Dev AI analysis**, explaining that the Z80 assembly discussion "[didn't] address the questions raised" about reverse-engineering the Go/Python-based tool.  

The thread highlights a misalignment in the conversation, with the core takeaway being a call to refocus on the **technical specifics of the ACLI analysis** (Go binaries, AI integration, security models) rather than retrocomputing tangents. The Z80 mention appears to be a red herring or shorthand confusion.

