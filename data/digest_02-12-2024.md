## AI Submissions for Mon Feb 12 2024 {{ 'date': '2024-02-12T17:11:13.897Z' }}

### Home Assistant: Three years later

#### [Submission URL](https://eamonnsullivan.co.uk/posts-output/home-automation-three-years/2024-02-11-home-assistant-three-years-later/) | 234 points | by [eamonnsullivan](https://news.ycombinator.com/user?id=eamonnsullivan) | [151 comments](https://news.ycombinator.com/item?id=39345122)

In this blog post, the author reflects on their experience with Home Assistant, a popular home automation software, after using it for almost three years. They discuss what hasn't changed, such as the ability to integrate devices from different manufacturers and the use of Node-RED for automations. However, they note that their approach to automation has evolved, focusing more on subtle and seamless actions rather than flashy effects. They also emphasize the importance of local control, opting for devices that work without relying on cloud services. The author mentions their reliance on the Home Assistant Cloud service for secure remote access and expresses cautious optimism about the new local-focused standard for home automation called Matter. They also discuss the growing importance of Home Assistant in their daily life and mention their considerations for replacing their hardware setup in the future. Overall, the author highlights their journey with Home Assistant and the valuable lessons they've learned along the way.

The discussion on the submission revolves around personal experiences with Home Assistant and home automation in general. Some individuals express frustration with cloud services and the reliance on internet connectivity for functionality. Others discuss their DIY projects and the challenges they face in integrating different devices. The topic of local control and the rejection of cloud services is also brought up, with some commenters expressing concerns about privacy and potential issues with companies discontinuing support. The discussion also touches on the compatibility of different smart home devices and the importance of standardization. Some users share their preferred hardware options, such as Z-Wave switches, while others mention their experiences with mechanical and digital solutions for light switches. The conversation concludes with skepticism about the future adoption of the Matter standard and doubts about its current functionality.

### The Rise and Fall of GOFAI

#### [Submission URL](https://billwadge.com/2024/02/12/the-rise-and-fall-of-gofai/) | 47 points | by [herodotus](https://news.ycombinator.com/user?id=herodotus) | [36 comments](https://news.ycombinator.com/item?id=39344934)

Bill Wadge, an AI expert, recently wrote a blog post discussing the rise and fall of Good Old Fashioned AI (GOFAI). GOFAI, which grew out of the 1956 Dartmouth AI meeting, is based on symbolic reasoning and has had several triumphs throughout history. Wadge highlights the invention of numbers and numerals as the first triumph, as they allowed for reliable symbolic reasoning about quantities. The invention of the abacus and similar mechanisms also automated symbolic computation with numerals. Aristotle's classification of valid syllogisms and the development of calculus and mathematical notation further extended the domain of symbolic reasoning. However, GOFAI encountered significant challenges as well. Russell's Paradox dealt a blow to Frege's axiomatization of set theory, and Gödel's incompleteness theorems showed that any formal system powerful enough to formalize arithmetic is incomplete. Despite these setbacks, GOFAI continued to make progress with the introduction of the λ calculus and Turing machines. However, it became clear by the 1956 Dartmouth conference that GOFAI had its limitations.

The discussion about the submission revolves around various aspects of Good Old Fashioned AI (GOFAI). Some commenters argue that GOFAI is not relevant and that modern AI techniques, particularly those based on machine learning, have far surpassed its capabilities. They mention examples like Deep Blue, which defeated Garry Kasparov in chess using brute-force search, and suggest that GOFAI approaches like symbolic reasoning are no longer practical.
Others point out that GOFAI has been successful in certain domains, such as natural language processing, planning, and scheduling. They argue that GOFAI techniques, like those implemented in Lisp, have their own advantages and should not be dismissed entirely.
There is also a discussion about the fundamental problems of GOFAI, including Gödel's incompleteness theorems and the undecidability problem. Some commenters argue that machine learning, while based on mathematical foundations, also has its limitations and is not a complete solution to AI.
The debate continues with discussions on the definition of GOFAI, its relationship with other fields like cybernetics, and the role of logic in AI. Some commenters highlight the successes of GOFAI in areas like automated provers and expert systems, while others express skepticism and believe that GOFAI is limited in its scalability.
Overall, the discussion reflects different perspectives on the strengths and weaknesses of GOFAI compared to modern AI approaches, and the ongoing debate about the future direction of AI research and development.

### AMD funded a drop-in CUDA implementation built on ROCm: It's now open-source

#### [Submission URL](https://www.phoronix.com/review/radeon-cuda-zluda) | 1001 points | by [mfiguiere](https://news.ycombinator.com/user?id=mfiguiere) | [390 comments](https://news.ycombinator.com/item?id=39344815)

In a surprising move, AMD has quietly funded a project to bring binary compatibility between NVIDIA CUDA applications and the AMD ROCm stack. This means that many CUDA applications can now run on AMD Radeon GPUs without the need for developers to adapt the source code. The project, known as ZLUDA, was initially developed to enable CUDA support on Intel graphics, but was later adapted for use on AMD GPUs. Although it's not 100% fail-safe and some features are not supported, the implementation has been successful in running CUDA-enabled software on ROCm. The open-source code is dual-licensed under Apache 2.0 or MIT, and it leverages the Rust programming language for the Radeon implementation. This development opens up new possibilities for end-users who want to run CUDA applications on AMD GPUs without any additional effort.

The discussion on Hacker News regarding the submission about AMD funding a project for binary compatibility between NVIDIA CUDA applications and the AMD ROCm stack covers a range of topics.
One commenter points out the dominance of NVIDIA in the machine learning field due to the popularity of CUDA. They argue that if AMD can provide seamless compatibility for CUDA machine learning tasks, it could potentially eat into NVIDIA's market share.
Another commenter raises concerns about the ever-changing landscape of machine learning frameworks and the difficulty of keeping up with them. They suggest that instead of investing in low-level optimizations for AMD GPUs, it might be strategically better for AMD to invest in compilers and software tools that allow high-level languages to write efficient kernels for AMD hardware.
There is also a discussion about the business decision behind AMD's funding of the project. Some commenters express frustration at AMD's lack of support for the ROCm stack, while others speculate about AMD's long-term strategy in the machine learning market.
One commenter brings up the success of the RADV project, which is an open-source Radeon Vulkan driver developed by Red Hat and Valve. They argue that if AMD can fund and support projects like ZLUDA, it can be beneficial for the community as a whole.
Additionally, there are discussions about the control NVIDIA has over CUDA software and hardware, the misconception that deep learning frameworks are built solely on CUDA, and the potential limitations of ZLUDA compared to the official CUDNN library.

Overall, the discussion reflects a mixture of excitement, skepticism, and curiosity about AMD's move to fund the ZLUDA project and the potential impact it could have on the machine learning community.

### GeneGPT, a tool-augmented LLM for bioinformatics

#### [Submission URL](https://github.com/ncbi/GeneGPT) | 106 points | by [brianzelip](https://news.ycombinator.com/user?id=brianzelip) | [18 comments](https://news.ycombinator.com/item?id=39348902)

GeneGPT is a tool-augmented large language model (LLM) designed to address the challenges faced by LLMs in handling specialized biomedical knowledge. With the goal of improving information retrieval in this domain, GeneGPT uses NCBI Web APIs to answer questions related to biomedical information. This approach leverages in-context learning and a unique decoding algorithm to execute API calls. Experimental results show that GeneGPT outperforms previous state-of-the-art models on eight GeneTuring tasks, with an average score of 0.83. The model surpasses BioGPT and ChatGPT, achieving a significant improvement in accuracy. In addition, GeneGPT demonstrates the potential of integrating domain-specific tools with LLMs to improve access and accuracy in specialized knowledge areas. The code and data for GeneGPT are available on GitHub.

The discussion on this submission revolves around various aspects of the GeneGPT project and related topics in the field of biomedical information retrieval. 
One user points out that the initial prompt for the language model project doesn't specify whether it's using the OpenAI API or some other method. Another user provides clarification, saying that the project is using NCBI Web APIs wrapped in command-line interfaces (CLIs) to perform tasks and suggests that the GPT OpenAI Marketplace could be a suitable platform for this.
Another user mentions that they found a similar project called LLaVA-Med, which also focuses on using domain-specific tools with language models.
There is a brief comment suggesting that the name "NCBI-APIs-GPT" would be more appropriate for this project.
The discussion then shifts to the topic of genetic information. One user jokingly suggests that they will make the GPT process their 23andMe DNA data to confirm special abilities. Another user mentions a tool called LLaVA-Med, which they believe is the best modern classifier they have found.
The conversation also touches on the evaluation of language models and the importance of measuring factors like question-answering accuracy and exact match performance.
A user brings up the possibility of using language models to predict phenotypes from genotype data, highlighting the potential of genetic engineering and its impact on phenotypic traits.
There's a discussion about transfer learning and the potential for transformer models to map genotypes to phenotypes accurately. Some users mention the challenges of gene regulation and the complexity of genetic data.
One user raises concerns about data security and the risks of handling sensitive genetic information. They suggest that genetic datasets contain subtle patterns that can be exploited.

Finally, there are a couple of comments unrelated to the main topic, with one user expressing disappointment that a language model is based on Gene Rayburn from Match Game and another user confirming the nature of the post.

### A celebrated cryptography-breaking algorithm just got an upgrade

#### [Submission URL](https://www.quantamagazine.org/celebrated-cryptography-algorithm-gets-an-upgrade-20231214/) | 46 points | by [thunderbong](https://news.ycombinator.com/user?id=thunderbong) | [3 comments](https://news.ycombinator.com/item?id=39341180)

Researchers have developed a new algorithm that improves the efficiency of lattice basis reduction, a technique widely used in cryptography and mathematics. The algorithm, known as LLL-style, allows for the reduction of the bases of lattices with thousands of dimensions, significantly expanding the range of scenarios in which LLL-like approaches can be used. The technique combines multiple strategies, including a recursive structure and precise number management, to achieve its efficiency. The algorithm has the potential to enhance the security and performance of cryptographic systems.

The discussion surrounding the submission is rather limited. One user, "dfrst," shares a link to the full paper of 63 pages, providing more in-depth information on the algorithm. Another user, "dng," shares a link to another article about the algorithm upgrade, which has attracted five comments. There is a third user, "ChrisArchitect," who simply mentions the year "2023," potentially indicating that the algorithm is expected to be significant in that year. Unfortunately, without further details, it is difficult to discern the specific content of the comments or the significance of the mentioned year.
- Biological AI and the evolution of plant intelligence were mentioned, raising the idea of millions of years of closed-loop plant systems evolving into AI.

Overall, the discussion covered a wide range of topics related to AI, energy consumption, economics, and the environment.

