## AI Submissions for Fri Feb 16 2024 {{ 'date': '2024-02-16T17:10:56.550Z' }}

### Coding in Vision Pro

#### [Submission URL](https://willem.com/blog/2024-02-16_vision-pro/) | 311 points | by [willemlaurentz](https://news.ycombinator.com/user?id=willemlaurentz) | [383 comments](https://news.ycombinator.com/item?id=39403935)

Willem L. Middelkoop is on a mind-bending journey exploring Spatial Computing with Apple's Vision Pro headset, and the experience is nothing short of extraordinary. Picture a world where you can seamlessly blend virtual objects into your reality – watching movies on a colossal cinema display or immersing yourself in personal photos right at your fingertips. With the Vision Pro, the line between the digital realm and the physical world blurs, opening up a realm of possibilities.

Equipped with cutting-edge technology like advanced chips, sensors, and cameras, the Vision Pro projects virtual elements into your surroundings, offering a truly immersive experience. This isn't just a wearable projector; it's an interactive marvel that responds to your gaze and touch, redefining how we interact with technology. Middelkoop showcases how he integrates a Bluetooth keyboard and trackpad with the Vision Pro, transforming it into a full-fledged computing system with immense capabilities.

Creating a multi-monitor work setup that feels incredibly lifelike, he delves into the concept of deep work, where the Vision Pro becomes a gateway to unparalleled focus and productivity. Imagine being surrounded by virtual windows that tower over you, allowing you to touch and interact with your digital workspace in ways that feel remarkably tangible. Middelkoop's journey with the Vision Pro blurs the boundaries between the real and virtual, offering a glimpse into a future where Spatial Computing revolutionizes how we perceive and engage with technology.

The discussion on Hacker News revolves around Willem L. Middelkoop's exploration of Spatial Computing with Apple's Vision Pro headset. The comments cover various aspects of the technology, including comparisons with other display options, scaling factors, and pricing considerations. Users discuss the limitations and advantages of the Vision Pro, such as its potential applications for work setups and productivity. Additionally, there are comparisons with other products in the market, like LG's UltraFine monitors, with debates on features and pricing. Some users express concerns about the functionality and support for external displays in Apple's ecosystem. The discussion also delves into the user experience, comfort, weight, and practicality of using the Vision Pro for extended periods, with comparisons to other headsets like the Oculus Quest 3. Overall, the comments touch on a range of technical, user experience, and pricing considerations related to Spatial Computing and Apple's Vision Pro headset.

### Magika: AI powered fast and efficient file type identification

#### [Submission URL](https://opensource.googleblog.com/2024/02/magika-ai-powered-fast-and-efficient-file-type-identification.html) | 657 points | by [alphabetting](https://news.ycombinator.com/user?id=alphabetting) | [227 comments](https://news.ycombinator.com/item?id=39391688)

Google has unveiled Magika, an AI-powered file-type identification system, open-sourced for the public good. With Magika, file identification becomes a breeze, using a custom deep-learning model for lightning-fast results, even when operating on a CPU. This tool is set to revolutionize how we handle different file types, a task that has traditionally been challenging due to the unique structures and complexities of various file formats. By leveraging AI technology, Magika surpasses existing tools in accuracy and speed, making it a game-changer in the realm of file management.

Magika's performance metrics speak for themselves, outperforming other tools by 20% across a benchmark of over 1 million files, with a particular edge in identifying textual files like code and configurations. Internally at Google, Magika is already in use at scale to enhance user safety across platforms like Gmail, Drive, and Safe Browsing, boosting file identification accuracy by a significant margin. Moreover, Magika's integration with VirusTotal promises to fortify the platform's cybersecurity efforts, contributing to a safer digital landscape for all users.

By open-sourcing Magika, Google aims to empower developers and researchers to refine their file identification methods and advance their projects. Available on Github under the Apache2 License, Magika is easily accessible for installation as a utility or Python library via the pip package manager. This release marks a significant step towards improving file management processes and streamlining security protocols in the ever-evolving landscape of tech.

The discussion on Hacker News regarding Google's open-sourced AI-powered file-type identification system, Magika, covers various topics. 
1. There is a conversation between users TomNomNom and brsztn regarding crawling locally for files, challenges faced with identifying specific file types correctly, and improving the tool's automation capabilities. They exchange experiences and insights into using Magika.
2. In another thread, IvyMike and bbb discuss permissions for crawling data from Google and redistributing files, touching on the complexities and legal considerations involved in software development and copyright issues.
3. Users tmschmdt and bbb delve into fair use and copyright concerns related to posting files publicly, emphasizing the need for protection and potential legal implications.
4. The discussion expands to MIME types, file formats, cybersecurity methods, file signatures, and data management tools like PhotoRec and Kaitai Struct, shedding light on various aspects of file identification and processing.
5. Users like EnigmaFlare delve into the technical aspects of file identification, highlighting the challenges of predicting file types accurately and the differences between human classification and AI-based tools like Magika.
6. The conversation continues with insights into the unpredictability of file identification, the importance of accuracy in determining file types, and the potential limitations of Magika in handling certain file types.

Overall, the discussion provides a wealth of information and perspectives on file management, AI technology, copyright issues, and practical considerations in software development.

### LLM agents can autonomously hack websites

#### [Submission URL](https://arxiv.org/abs/2402.06664) | 70 points | by [pella](https://news.ycombinator.com/user?id=pella) | [17 comments](https://news.ycombinator.com/item?id=39403534)

The latest from the cryptography and security world: a groundbreaking paper titled "LLM Agents can Autonomously Hack Websites" sheds light on the offensive capabilities of large language models (LLMs). Authored by Richard Fang and his team, this research showcases how LLM agents, particularly GPT-4, can independently hack websites, performing tasks like blind database schema extraction and SQL injections without any human guidance. The study highlights the potential security risks posed by advanced AI agents and raises concerns about their widespread deployment. Dive into the details of this cutting-edge research to explore the implications for cybersecurity.

- **ActorNightly**: Points out that hacking website activity revolves around finding vulnerabilities, and exploiting those vulnerabilities listed in the paper due to mistakes in standard development practices. Mentions that LLMs exist and might be used for security reasons, and expresses uncertainty about LLMs being utilized for looking into Personally Identifiable Information (PII) leaks.
- **wsnks**: Agrees that LLMs could crack typical exploits found on weak websites, bringing up the OWASP10 paper that lists pages greatly cherry-picked for testing hypotheses and the breakthrough it indicates.
- **MattPalmer1086**: Confirms that the attacks' generic descriptions mentioned in the paper are fascinating and highlights the rough approachability ability to make functional calls. Mentions success rates and cost-effectiveness of attacks but also points out that attacking a human thing is a significant security risk.
- **vrptr**: Raises a significant point regarding the comparison of writing processes dedicated to running ready explanations and the concept of LLM, clarifying the program equivalent, and highlighting that security scanning is preceding full exploitation.
- **bemusedthrow75**: Shares OpenAI's IP ranges documented in links provided.
- **tyngq**: Comments on how easy it is to hack OpenAI through IP shifting, referencing a specific pattern.
- **maCDzP**: Hopes OpenAI doesn’t share large bounties.
- **pnqc**: Expresses skepticism about cybersecurity engineers safeguarding against AI.
- **g3ol4d0**: Points out a potentially automated vulnerability scanning tool.
- **your_friend**: Mentions the human tendency to hack websites casually.
- **bbr**: Concludes the discussion by emphasizing the danger of publishing vulnerabilities and how LLM capabilities might be widely available in the near future through APIs like OpenAI Assistants.

### Recording and visualising the 20k system calls it takes to "import seaborn"

#### [Submission URL](http://blog.mattstuchlik.com/2024/02/16/counting-syscalls-in-python.html) | 106 points | by [sYnfo](https://news.ycombinator.com/user?id=sYnfo) | [45 comments](https://news.ycombinator.com/item?id=39402868)

Today's top story on Hacker News delves into the world of system calls (syscalls) and how to analyze them using a new tool added to Cirron. The tool, called Tracer, allows you to see exactly what syscalls a piece of Python code is making. It works by using the strace tool to trace the system calls, capturing the output in a file for later analysis.

For example, tracing a simple print("Hello") statement reveals that it ultimately maps to a write syscall, writing the string "Hello\n" to stdout with some interesting details like the number of bytes written and the time taken for the call. 

Furthermore, the author tracks the syscalls generated by importing Seaborn library and finds that it results in around 20,000 syscalls which can be overwhelming to analyze manually. To overcome this, they introduce Perfetto Trace Viewer as a tool to visualize and analyze the syscall traces more efficiently. By converting the Tracer output to Trace Event Format, one can load the trace into Perfetto for detailed analysis.

Overall, the article provides a fascinating insight into how syscalls can be traced and analyzed using the Cirron tool, demonstrating a unique way to gain deeper understanding of the system-level operations carried out by Python code.

### Training LLMs to generate text with citations via fine-grained rewards

#### [Submission URL](https://arxiv.org/abs/2402.04315) | 165 points | by [PaulHoule](https://news.ycombinator.com/user?id=PaulHoule) | [28 comments](https://news.ycombinator.com/item?id=39399418)

The paper titled "Training Language Models to Generate Text with Citations via Fine-grained Rewards" by Chengyu Huang and collaborators addresses the limitations of Large Language Models (LLMs) in producing credible responses by lacking references to reliable sources. The authors introduce a training framework using fine-grained rewards to guide LLMs in generating supportive and relevant citations, enhancing the correctness of their responses. By conducting experiments on Question Answering datasets, the proposed method outperforms conventional practices and even surpasses GPT-3.5-turbo on LLaMA-2-7B. This work contributes to improving the quality of text generation by incorporating in-text citations within language models.

### Video generation models as world simulators

#### [Submission URL](https://openai.com/research/video-generation-models-as-world-simulators) | 353 points | by [linksbro](https://news.ycombinator.com/user?id=linksbro) | [165 comments](https://news.ycombinator.com/item?id=39391458)

Researchers have developed Sora, a cutting-edge video generation model that acts as a world simulator. By training on a vast amount of video and image data, Sora leverages a transformer architecture to generate high-fidelity videos of various durations, resolutions, and aspect ratios. This innovative model, capable of creating one minute of detailed video, signifies a significant step towards constructing all-encompassing world simulators.

Incorporating patches of visual data akin to tokens in language models, Sora transforms videos into a compressed latent space before decomposing them into spacetime patches for training and generation. This approach allows Sora to operate on diverse video and image types effectively. Leveraging a diffusion model within a transformer framework, Sora refines its predictions of clean patches from noisy inputs, showcasing remarkable scaling properties across different domains.

By training on data at its native size rather than conforming to standard dimensions, Sora gains flexibility in sampling videos of varying resolutions and aspect ratios, boosting content creation for different devices and enhancing framing and composition. Moreover, Sora benefits from training on videos with highly descriptive captions, improving text fidelity and video quality. With the capability to generate high-quality videos based on user prompts, Sora represents a significant advancement in video generation technology.

The discussion on this submission revolves around the capabilities and implications of Sora, a cutting-edge video generation model. Some users draw parallels between Sora and existing models while others discuss the potential applications and limitations of advanced AI technology such as AGI. There is also a conversation about the challenges in creating an AI for games like Civilization, with mentions of the need for improved hardware and different strategies for training the AI. Additionally, the discussion touches upon the significance of generalization in reinforcement learning and the potential for AI to predict economic models in games like Civilization. Finally, a user references Yann LeCun's work on Objective-Driven AI and the gradual progress towards achieving AGI through various breakthroughs in AI technologies.

### The fifth epoch of distributed computing

#### [Submission URL](https://cloud.google.com/blog/topics/systems/the-fifth-epoch-of-distributed-computing) | 55 points | by [simonpure](https://news.ycombinator.com/user?id=simonpure) | [42 comments](https://news.ycombinator.com/item?id=39396151)

In a recent keynote by Google Fellow Amin Vahdat, the evolution of computing from its origins to the present era was examined. Over the past fifty years, exponential growth in computing capacity has revolutionized human capabilities, enabling instant access to knowledge, real-time language translation, and advanced AI systems tackling complex challenges. These advancements have led to a need for foundational breakthroughs every decade to sustain progress. Vahdat proposes the concept of a fifth epoch of computing, focused on being data-centric, declarative, and outcome-oriented to democratize access to knowledge and opportunities. Reflecting on computing history, four epochs have already shaped our technology landscape. From the introduction of the first transistor in 1947 to the birth of the modern Internet in 1969, each era marked significant milestones in computing development. The progression through these epochs led to improvements in network communication, high-level programming languages, multi-user operating systems, and the emergence of the ARPANet, laying the groundwork for the exponential growth in subsequent epochs. The move from asynchronous to synchronous communication, the rise of personal computers, and the advent of the World Wide Web symbolize the transformative shifts in computing paradigms over the years. As we stand on the brink of a new era, the fifth epoch of computing promises to redefine how we interact with technology, driven by a data-centric approach and aimed at delivering insights proactively to users.

The discussion on Hacker News about the keynote by Google Fellow Amin Vahdat covers various perspectives on the evolution of computing and the proposed fifth epoch of computing. 

- One user expressed skepticism towards the insights of the article, criticizing its MBA-style language and expecting technical depth. 
- Another user highlighted the shift in the internet landscape over the years, mentioning concerns about consolidation, access to information, and the dangers of intrusive AI. 
- GMoromisato discussed the impact of AI on programming and user experience, emphasizing the potential of AI in simplifying complex tasks in software development. 
- There was a debate about the role of AI in programming and the difficulty of managing increasingly complex systems. 
- The importance of teaching the next generation of programmers was emphasized by one user, suggesting that engineers should focus on sharing knowledge with younger developers. 
- Some users discussed the necessity of declarative programming models focused on business logic due to the increasing complexity in computing. 

Overall, the discussion highlighted concerns, insights, and differing opinions on the future of computing and the impact of AI on software development and technology.

### V-JEPA: Video Joint Embedding Predictive Architecture (V-JEPA) Model

#### [Submission URL](https://ai.meta.com/blog/v-jepa-yann-lecun-ai-model-video-joint-embedding-predictive-architecture/) | 62 points | by [agnosticmantis](https://news.ycombinator.com/user?id=agnosticmantis) | [6 comments](https://news.ycombinator.com/item?id=39392104)

Today, the tech world is buzzing with the release of the Video Joint Embedding Predictive Architecture (V-JEPA), a significant leap towards achieving advanced machine intelligence as envisioned by Yann LeCun. This model excels at understanding detailed interactions between objects in a physical world model. Released under a Creative Commons license, V-JEPA aims to pave the way for more grounded machine intelligence.

V-JEPA operates by predicting missing parts of a video in an abstract space, enhancing training efficiency by up to 6x. The model uses self-supervised learning, pre-training solely with unlabeled data, and then adapts to specific tasks with labeled examples. By focusing on abstract representations rather than specific pixel details, V-JEPA demonstrates improved learning efficiency.

The unique masking strategy of V-JEPA ensures that the model learns complex aspects of the world by predicting masked spatio-temporal regions in videos. This approach makes the model adept at frozen evaluations, enabling efficient adaptation to new skills with minimal additional training.

Excitingly, V-JEPA outshines other video models in label efficiency, proving its effectiveness in low-shot settings. As the tech world marches towards more human-like machine intelligence, V-JEPA's release marks a significant milestone in this journey.

1. **jimmySixDOF** shared a cryptic message about Gemini 15 Sora Magic investment happening at Gemini. Another user, **sbstnnght**, referred to a previous test last year. **jimmySixDOF** then elaborated that the reference was to a benchmark test establishing baseline performance where phrases from the Gemini white paper flashed on the screen, and the model had to compare and predict the performance of Large Language Models (LLMs).
2. **cm** mentioned something about a "Magic investment," and **strmfthr** commented that the development of the Magic platform has received a $100 million investment. The CEO appears to be impressed with the progress towards using Long Context-Optimized LLMs to replace developers.
3. **btshftfcd** noted that Alpha has started training on human data to predict real-world events. They are curious if a similar approach using Large Language Models can ground real-world video prediction with minimal language abilities, suggesting a breakthrough in bootstrapping machine learning laws with physics and mathematics.

The discussion seems to revolve around advancements in AI models like LLMs, their applications in predicting real-world events, and investments in AI development.

### How much electricity does AI consume?

#### [Submission URL](https://www.theverge.com/24066646/ai-electricity-energy-watts-generative-consumption) | 102 points | by [doener](https://news.ycombinator.com/user?id=doener) | [114 comments](https://news.ycombinator.com/item?id=39397161)

Today's top story on Hacker News discusses the energy consumption of AI models, shedding light on the significant power requirements behind machine learning. The article highlights the challenges in accurately estimating the energy cost of AI due to varying configurations and the reluctance of companies to share such data. Training AI models, like GPT-3, is described as highly energy-intensive, with the electricity used equivalent to that consumed by 130 US homes annually. Furthermore, the article discusses the differences in energy usage between training and deploying AI models for inference tasks. Researchers have started to analyze the energy consumption of various AI models, providing insights into the environmental impact of AI technologies. Despite the lack of absolute figures, these studies offer comparative data on the energy costs associated with AI activities. The article raises important questions about the hidden energy expenses of AI systems and emphasizes the need for more transparency in this area.

The discussion on Hacker News regarding the energy consumption of AI models covers various aspects related to the topic. Users discussed the challenges in estimating global energy consumption due to AI and the comparison between different energy-efficient hardware solutions for AI tasks. Some users expressed concerns about the potential increase in energy consumption with the rise of AI technologies and the need for more efficient hardware and software optimizations to mitigate this issue. The conversation also delved into the energy efficiency of data centers, Bitcoin mining, and the implications of AI development on overall energy consumption. Aspects such as the efficiency of GPUs compared to custom ASICs for AI tasks and the potential environmental impact of AI models were also explored. The discussion highlighted the importance of improving energy efficiency in AI systems to address the growing energy demands of emerging technologies.
