## AI Submissions for Fri Jul 07 2023 {{ 'date': '2023-07-07T17:10:26.489Z' }}

### Owner of Symbolics Lisp machines IP is interested in a non-commercial release

#### [Submission URL](https://hachyderm.io/@gmpalter/110673993659492472) | 209 points | by [mepian](https://news.ycombinator.com/user?id=mepian) | [120 comments](https://news.ycombinator.com/item?id=36635373)

Introducing Hacker News Daily Digest: Your Go-To Source for the Latest Tech Trends and Innovations!

In today's top story, "How AI is Revolutionizing the World of Self-Driving Cars," technology behemoths are breaking new ground by integrating artificial intelligence into automotive technology. With AI at the wheel, autonomous vehicles are becoming smarter, safer, and more efficient than ever before. This groundbreaking innovation promises to transform transportation as we know it, paving the way for a future where cars can navigate the bustling city streets on their own, reducing accidents and traffic congestion.

Next up, "Blockchain for Supply Chain Management: Disrupting the Status Quo." Blockchain, the technology that underpins cryptocurrencies like Bitcoin, is now making waves in the world of supply chain management. By harnessing the power of decentralized and immutable ledgers, businesses are finding novel ways to enhance transparency, traceability, and accountability in their supply chains. This game-changing implementation of blockchain technology has the potential to revolutionize the logistics industry, streamlining processes and reducing fraud.

In a fascinating exploration of the human-computer interaction, "Natural Language Processing: From Text to Understanding," researchers are pushing the boundaries of artificial intelligence by developing language models that comprehend human language like never before. By leveraging advanced algorithms and vast datasets, these language models can not only translate text but also grasp the intricate nuances and context inherent in human dialogue. This breakthrough in natural language processing opens up exciting possibilities, from more intuitive voice assistants to enhanced language translation services.

Last but certainly not least, "The Rise of Low-Code Development: Empowering Non-Technical Professionals." With the rising demand for software development and a shortage of skilled programmers, low-code development is emerging as a game-changer. By providing simple visual interfaces and drag-and-drop functionality, non-technical professionals can now create custom applications without needing extensive coding knowledge. This democratization of software development empowers businesses to build their own tools, streamlining processes and reducing reliance on traditional development approaches.

That's all for today's Hacker News Daily Digest. Stay tuned for tomorrow's edition, packed with more exciting stories and groundbreaking tech news!

The discussion on this submission revolves around the topics mentioned in the summary. Here are some key points from the comments:

- Some users express skepticism about the value of making old software publicly available, pointing out that it may not have much commercial or practical use in today's context.
- There is a debate about the legitimacy and legal aspects of John Mallery purchasing the Symbolics IP and making it available, with conflicting opinions on whether it was a legitimate transaction.
- The possible motivations behind Mallery's actions are discussed, with some suggesting that he may be hoping for a significant return on investment and others speculating that he is driven by a desire to preserve and share historical artifacts.
- The conversation touches on the historical significance and cultural value of preserving computing history, specifically in the context of Lisp Machines and Symbolics Genera.
- The topic of open-source software and its impact on the preservation and availability of historical software is mentioned, with references to the GNU project and its efforts to create FOSS clones of systems like Genera.
- Some users discuss other related topics, such as the Blender software, the availability of Lisp programming languages, and the advantages of Genera and Interlisp-D.

### Waterwave Could Quench AIs' Thirst for GPU Memory

#### [Submission URL](https://spectrum.ieee.org/ai-training) | 43 points | by [RafelMri](https://news.ycombinator.com/user?id=RafelMri) | [5 comments](https://news.ycombinator.com/item?id=36625400)

Researchers have developed a novel approach called Waterwave to increase the efficiency of training multiple AI models simultaneously on the same GPU. Traditional methods for training AI models using GPUs often have to assess models one by one due to memory constraints, leading to time-consuming processes. Waterwave breaks models up into more manageable "sub-models" that can be processed simultaneously on the same GPU. As soon as one sub-model is finished, memory space is freed up for the next sub-model, increasing overall efficiency. The results show that Waterwave is 12 times as fast as existing spatial sharing methods and 1.49 times as fast as existing temporal memory sharing methods.

One commenter pointed out that the Waterwave approach is not necessary for training multiple models on GPUs as existing methods already allow for parallelization. They argue that the paper does not address the real-world problem of training large models on GPUs, and thus the solution proposed in the paper is practically irrelevant.

Another commenter provided a link to the source paper for those interested in further reading and citations.

In response to the discussion, another commenter explained that the paper focuses on the practical problem of training smaller models on GPUs and does not aim to solve the issue of training large models. They suggest that the problem can be solved through GPU utilization optimization and maximizing GPU utilization for training jobs, particularly for SaaS providers. They also mentioned the difficulty of handling heterogeneous jobs and the challenges of scheduling GPU resources effectively.

### Fedora considers “privacy-preserving” telemetry

#### [Submission URL](https://lwn.net/Articles/937528/) | 131 points | by [pabs3](https://news.ycombinator.com/user?id=pabs3) | [208 comments](https://news.ycombinator.com/item?id=36630032)

The Fedora project is considering adding limited, opt-out telemetry to its workstation edition. While the use of the term "telemetry" has raised some concerns, the developers of Fedora believe that collecting aggregate data on software usage can be done ethically and without compromising users' privacy. Users will have the option to disable data upload before any data is sent, and the data collection will be operated by Fedora on Fedora infrastructure, not relying on third-party services. Additionally, users can redirect the data collection to their own private metrics server. While there are objections to the opt-out nature of the telemetry, the developers insist that they will ensure compliance with European law and respect users' privacy.

The discussion on the submission revolves around the topic of telemetry and its implications for privacy. Some commenters express concerns about the potential privacy implications of data collection, while others argue that telemetry can be beneficial for improving software quality and making informed decisions. There is also a discussion about the importance of maintaining a balance between privacy and the need for data to drive product improvements. Some commenters highlight the importance of transparency and consent in data collection practices, while others discuss the potential use of AI in data collection and analysis. Other topics of discussion include the role of telemetry in safety-critical systems and the challenges of balancing user preferences and standardization in software development. Some commenters also provide examples of how telemetry has been used to improve software and user experience in the past. Overall, the discussion highlights the complex considerations surrounding the implementation of telemetry in software projects.

### ChatGPT loses users for first time, shaking faith in AI revolution

#### [Submission URL](https://www.washingtonpost.com/technology/2023/07/07/chatgpt-users-decline-future-ai-openai/) | 196 points | by [mfiguiere](https://news.ycombinator.com/user?id=mfiguiere) | [270 comments](https://news.ycombinator.com/item?id=36636039)

Consumer interest in AI chatbots and image-generators may be starting to decline as download numbers and website traffic for OpenAI's ChatGPT have fallen for the first time since its launch in November. ChatGPT gained widespread popularity and sparked an AI race among tech giants when it unveiled its capabilities to engage in complex conversations, write poetry, and pass professional exams. However, users have started to encounter the chatbot generating false information, leading to a realization that its usefulness may have been overhyped. The drop in usage could also be influenced by the bot's limitations, concerns over data leaks, the rising cost of running the bot, and the potential impact of looming regulations.

The discussion on Hacker News revolves around the decline in usage and interest in OpenAI's ChatGPT. Users point out that while ChatGPT initially garnered excitement and attention, its limitations and the realization that it can generate false information have led to a decline in its popularity. Some users express that ChatGPT is not efficient and can generate time-consuming and inaccurate responses. Others highlight the importance of context and the need for human-written responses rather than relying solely on AI-generated text. Some users also discuss their experiences with ChatGPT and share their frustrations with its inability to understand complex instructions or certain programming languages. The discussion also touches on the role of search engines, with users noting that while search engines like Google can provide helpful information, ChatGPT's ability to generate satisfactory answers quickly is appealing to some users.

### Mechanical Turk workers are using AI to automate being human

#### [Submission URL](https://techcrunch.com/2023/06/14/mechanical-turk-workers-are-using-ai-to-automate-being-human/) | 243 points | by [elsewhen](https://news.ycombinator.com/user?id=elsewhen) | [150 comments](https://news.ycombinator.com/item?id=36629777)

In a study conducted by researchers at EPFL in Switzerland, it has been found that nearly half of the workers on Amazon's Mechanical Turk platform may be utilizing AI to complete tasks that were intended for humans. Mechanical Turk allows users to divide small tasks into subtasks, paying workers a small amount of money for each completed task. These tasks often involve activities that are difficult to automate, such as CAPTCHA solving or sentiment analysis. However, the study reveals that workers are using large language models like ChatGPT to automate their work. This raises concerns about the reliability of the data collected through Mechanical Turk and highlights the growing issue of "AI training on AI-generated data." As language models continue to advance, it becomes harder to determine whether a task was actually completed by a human or AI. The researchers suggest that new measures need to be taken to ensure the integrity of human-generated data amidst this development.

Discussion Summary:

1. Some users highlighted the problem of relying on Google Translate and automated translation tools for important documents. They expressed concerns about the accuracy and reliability of machine translations.

2. Another user mentioned how workers on Mechanical Turk would often share scripts to achieve the expected results and match the majority of answers. This raised questions about the integrity of the data collected through the platform.

3. A user argued that the extraction of value is not inherently dependent on labor, but rather on the capital. They discussed the concept of extracting value from workers and the limitations of the current system.

4. The discussion touched on the issue of underpaid teachers and the importance of specialized workers in society.

5. There was a debate about the value of artificial intelligence and the extent to which it can replace human labor. Some argued that AI is fundamentally independent of labor, while others emphasized the importance of human workers.

6. Users discussed the potential consequences of AI-generated data and highlighted the challenges of verifying the authenticity of information in an AI-driven world.

7. There was a mention of the problem of circular reporting on platforms like Wikipedia and the need for reliable sources of information.

8. Users shared interesting facts about historical events and the preservation of information.

9. The discussion turned to the value of languages and the importance of linguistic diversity in conveying human experiences.

10. Users debated the concept of compression in language and its impact on understanding and communication.

11. There was a discussion about the limitations of compressed data and the potential for misunderstandings caused by condensed information.

12. Users discussed the spread of ideas and the role of language in shaping society's understanding of different concepts, emphasizing the need for critical thinking and nuance in communication.

### GPU Guide (For AI Use-Cases)

#### [Submission URL](https://gpus.llm-utils.org/the-gpu-guide/) | 43 points | by [tikkun](https://news.ycombinator.com/user?id=tikkun) | [24 comments](https://news.ycombinator.com/item?id=36632397)

The author discusses the best AI tools worth running and provides recommendations for different use cases. They suggest running stable diffusion, whisper transcription, and open language models like GPT-3.5 or GPT-4. They also provide recommendations for GPUs based on different models, whether running on cloud or locally. The article explains the difference between RTX 6000, A6000, and 6000 Ada GPUs, as well as the difference between DGX GH200, GH200, and H100 GPUs. The author also mentions Nvidia's official cloud offering, DGX Cloud, and discusses the significant upgrade H100s offer over A100s for training language models. They touch on other GPU options like AMD and Intel and provide suggestions for GPU cloud providers based on specific needs. The article concludes by recommending Runpod and their templates as the easiest GPU cloud to start with.

The discussion on this submission covers a range of topics related to AI tools, GPU recommendations, and possibilities for cloud deployment. Several commenters thank the author for their insights and provide additional recommendations. One commenter mentions the performance of the RTX 4000 cards for running Stable Diffusion, while another discusses the potential benefits of using CPU instances for large models with low VRAM GPUs. There is a conversation about the VRAM limitations of consumer graphics cards and the potential for Nvidia to make consumer-friendly options. The discussion also touches on alternative GPU options like AMD and Apple's M1/M2 chips. The topic of GPU cloud providers is brought up, with some commenters mentioning pricing differences and their own experiences. There is a recommendation to include specific pricing metrics in the article for more informed recommendations. Other topics include local GPU options, FPGA deployment, and the potential future offerings from Intel and AMD.

