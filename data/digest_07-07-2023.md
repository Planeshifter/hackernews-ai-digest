## AI Submissions for Fri Jul 07 2023 {{ 'date': '2023-07-07T17:10:26.489Z' }}

### Fedora considers “privacy-preserving” telemetry

#### [Submission URL](https://lwn.net/Articles/937528/) | 131 points | by [pabs3](https://news.ycombinator.com/user?id=pabs3) | [208 comments](https://news.ycombinator.com/item?id=36630032)

The Fedora project is considering adding limited, opt-out telemetry to its workstation edition. While the use of the term "telemetry" has raised some concerns, the developers of Fedora believe that collecting aggregate data on software usage can be done ethically and without compromising users' privacy. Users will have the option to disable data upload before any data is sent, and the data collection will be operated by Fedora on Fedora infrastructure, not relying on third-party services. Additionally, users can redirect the data collection to their own private metrics server. While there are objections to the opt-out nature of the telemetry, the developers insist that they will ensure compliance with European law and respect users' privacy.

The discussion on the submission revolves around the topic of telemetry and its implications for privacy. Some commenters express concerns about the potential privacy implications of data collection, while others argue that telemetry can be beneficial for improving software quality and making informed decisions. There is also a discussion about the importance of maintaining a balance between privacy and the need for data to drive product improvements. Some commenters highlight the importance of transparency and consent in data collection practices, while others discuss the potential use of AI in data collection and analysis. Other topics of discussion include the role of telemetry in safety-critical systems and the challenges of balancing user preferences and standardization in software development. Some commenters also provide examples of how telemetry has been used to improve software and user experience in the past. Overall, the discussion highlights the complex considerations surrounding the implementation of telemetry in software projects.

### ChatGPT loses users for first time, shaking faith in AI revolution

#### [Submission URL](https://www.washingtonpost.com/technology/2023/07/07/chatgpt-users-decline-future-ai-openai/) | 196 points | by [mfiguiere](https://news.ycombinator.com/user?id=mfiguiere) | [270 comments](https://news.ycombinator.com/item?id=36636039)

Consumer interest in AI chatbots and image-generators may be starting to decline as download numbers and website traffic for OpenAI's ChatGPT have fallen for the first time since its launch in November. ChatGPT gained widespread popularity and sparked an AI race among tech giants when it unveiled its capabilities to engage in complex conversations, write poetry, and pass professional exams. However, users have started to encounter the chatbot generating false information, leading to a realization that its usefulness may have been overhyped. The drop in usage could also be influenced by the bot's limitations, concerns over data leaks, the rising cost of running the bot, and the potential impact of looming regulations.

The discussion on Hacker News revolves around the decline in usage and interest in OpenAI's ChatGPT. Users point out that while ChatGPT initially garnered excitement and attention, its limitations and the realization that it can generate false information have led to a decline in its popularity. Some users express that ChatGPT is not efficient and can generate time-consuming and inaccurate responses. Others highlight the importance of context and the need for human-written responses rather than relying solely on AI-generated text. Some users also discuss their experiences with ChatGPT and share their frustrations with its inability to understand complex instructions or certain programming languages. The discussion also touches on the role of search engines, with users noting that while search engines like Google can provide helpful information, ChatGPT's ability to generate satisfactory answers quickly is appealing to some users.

### Mechanical Turk workers are using AI to automate being human

#### [Submission URL](https://techcrunch.com/2023/06/14/mechanical-turk-workers-are-using-ai-to-automate-being-human/) | 243 points | by [elsewhen](https://news.ycombinator.com/user?id=elsewhen) | [150 comments](https://news.ycombinator.com/item?id=36629777)

In a study conducted by researchers at EPFL in Switzerland, it has been found that nearly half of the workers on Amazon's Mechanical Turk platform may be utilizing AI to complete tasks that were intended for humans. Mechanical Turk allows users to divide small tasks into subtasks, paying workers a small amount of money for each completed task. These tasks often involve activities that are difficult to automate, such as CAPTCHA solving or sentiment analysis. However, the study reveals that workers are using large language models like ChatGPT to automate their work. This raises concerns about the reliability of the data collected through Mechanical Turk and highlights the growing issue of "AI training on AI-generated data." As language models continue to advance, it becomes harder to determine whether a task was actually completed by a human or AI. The researchers suggest that new measures need to be taken to ensure the integrity of human-generated data amidst this development.

### GPU Guide (For AI Use-Cases)

#### [Submission URL](https://gpus.llm-utils.org/the-gpu-guide/) | 43 points | by [tikkun](https://news.ycombinator.com/user?id=tikkun) | [24 comments](https://news.ycombinator.com/item?id=36632397)

The author discusses the best AI tools worth running and provides recommendations for different use cases. They suggest running stable diffusion, whisper transcription, and open language models like GPT-3.5 or GPT-4. They also provide recommendations for GPUs based on different models, whether running on cloud or locally. The article explains the difference between RTX 6000, A6000, and 6000 Ada GPUs, as well as the difference between DGX GH200, GH200, and H100 GPUs. The author also mentions Nvidia's official cloud offering, DGX Cloud, and discusses the significant upgrade H100s offer over A100s for training language models. They touch on other GPU options like AMD and Intel and provide suggestions for GPU cloud providers based on specific needs. The article concludes by recommending Runpod and their templates as the easiest GPU cloud to start with.

The discussion on this submission covers a range of topics related to AI tools, GPU recommendations, and possibilities for cloud deployment. Several commenters thank the author for their insights and provide additional recommendations. One commenter mentions the performance of the RTX 4000 cards for running Stable Diffusion, while another discusses the potential benefits of using CPU instances for large models with low VRAM GPUs. There is a conversation about the VRAM limitations of consumer graphics cards and the potential for Nvidia to make consumer-friendly options. The discussion also touches on alternative GPU options like AMD and Apple's M1/M2 chips. The topic of GPU cloud providers is brought up, with some commenters mentioning pricing differences and their own experiences. There is a recommendation to include specific pricing metrics in the article for more informed recommendations. Other topics include local GPU options, FPGA deployment, and the potential future offerings from Intel and AMD.

