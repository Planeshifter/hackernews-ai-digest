## AI Submissions for Thu Jan 01 2026 {{ 'date': '2026-01-01T17:09:10.570Z' }}

### Build a Deep Learning Library

#### [Submission URL](https://zekcrates.quarto.pub/deep-learning-library/) | 120 points | by [butanyways](https://news.ycombinator.com/user?id=butanyways) | [15 comments](https://news.ycombinator.com/item?id=46454587)

Build a Deep Learning Library from Scratch (free online book)

- Learn by building: start with a blank file and NumPy, implement your own autograd engine and a small suite of layer modules.
- By the end, you’ll train models on MNIST, plus a simple CNN and a simple ResNet—gaining a clear, under‑the‑hood understanding of modern DL stacks.
- Free to read online; pay‑what‑you‑want support via Gumroad. Questions/feedback: zekcrates@proton.me.

**Discussion Summary:**

*   **Community Implementations:** The comment section became a showcase for "learning by doing," with multiple users sharing their own custom ML libraries built from scratch in both Python and C++. One notable mention included a user who successfully replicated GPT-2 using their own NumPy-based library.
*   **Comparison to Karpathy:** When asked how this compares to Andrej Karpathy’s "Zero to Hero" series, the author explained that while Karpathy’s *Micrograd* operates on scalars, this book focuses on tensors. However, the author still recommended Karpathy’s videos as a complementary resource.
*   **Depth of Abstraction:** Some technical discussion arose regarding the use of NumPy. Critics argued that relying on NumPy hides the implementation details of tensors themselves; the author agreed, suggesting a future C++ backend might address this, while others expressed interest in a "build NumPy from scratch" guide to cover that gap.

### Building an internal agent: Code-driven vs. LLM-driven workflows

#### [Submission URL](https://lethain.com/agents-coordinators/) | 67 points | by [pavel_lishin](https://news.ycombinator.com/user?id=pavel_lishin) | [31 comments](https://news.ycombinator.com/item?id=46456682)

TL;DR: LLMs plus tools can automate complex internal workflows, but tiny error rates can break trust. Imprint now treats LLM orchestration as the fast default and “graduates” important workflows to deterministic, code-driven coordinators.

The story:
- Problem: An LLM agent auto-tagged Slack messages with :merged: by parsing PR links and checking GitHub. It worked great—until it occasionally marked unmerged PRs as merged, causing real risk (people stopped looking).
- Lesson: Determinism matters. A 1% error on internal ops can erase 99% of the value.

What they built:
- Two coordinators in the same framework:
  - coordinator: llm (default) — LLM selects and sequences tools; handler enforces limits and termination.
  - coordinator: script — a checked-in Python script gets the same tools, triggers, and “virtual files” (Slack/Jira attachments). It can optionally invoke a subagent LLM for specific steps, but control is explicit and reviewable.
- Engineers can one-shot convert working prompts into code (via Claude Code), preserving behavior while gaining reliability, speed, and code review.

Why it matters:
- Hybrid pattern: start with LLMs for exploration and simple cases; promote to code when you need guarantees on correctness, latency, or cost.
- Even as models improve, use LLMs narrowly for truly intelligent decisions; handle iterative, stateful orchestration with deterministic software.
- Framing: “progressive enhancement” for agents—LLM when sufficient, code when necessary.

**Determinism, Evals, and the "Script vs. Agent" Debate**

The discussion around Will Larson's post focused heavily on the engineering trade-offs between probabilistic LLMs and deterministic code for internal tooling.

*   **Skepticism on LLM Necessity:** Many commenters questioned the premise of using an LLM for tasks with established APIs (like checking GitHub PR status). The consensus among skeptics was that if a deterministic API exists, wrapping it in an LLM introduces unnecessary cost, latency, and "judgment" errors into what should be a binary operation.
*   **The Struggle with Non-Determinism:** A significant portion of the thread debated why LLMs cannot be perfectly deterministic even with `temperature=0`. Technical explanations cited floating-point non-determinism in GPUs and batching variances. This unpredictability frustrates iterative development, where small changes to a "spec" (prompt) can result in a completely rewritten, broken solution rather than a slight modification.
*   **"Evals" are the New Unit Tests:** To mitigate hallucinations, commenters suggested treating LLM outputs as untrusted candidates that must pass rigid, deterministic tests (Evals). For example, rather than trusting the LLM to check a PR, the LLM should write the code to check the PR, which is then verified against a mock environment.
*   **Defining the Boundary:** Users argued that the "Code vs. LLM" framing is slightly misleading. The emerging best practice is using LLMs for *intent understanding* and handling unexpected states (e.g., messy HTML, vague user requests), while handing off the *mechanical execution* to standard code.
*   **Security Risks:** A smaller sub-thread highlighted the danger of "prompt injection" in internal agents, noting that an agent with broad permissions (reading Slack, checking GitHub) is a high-value target for malicious input manipulation.

### Hierarchical Navigable Small World (HNSW) in PHP

#### [Submission URL](https://centamori.com/index.php?slug=hierarchical-navigable-small-world-hnsw-php&lang=en) | 91 points | by [centamiv](https://news.ycombinator.com/user?id=centamiv) | [15 comments](https://news.ycombinator.com/item?id=46454968)

HNSW in PHP: fast vector search without scanning everything

- The problem: Doing cosine similarity against every vector is linear-time and crawls at scale (think scanning 10M docs one by one).
- The idea: Implement HNSW (Hierarchical Navigable Small World) in PHP—a layered graph where upper levels act like “highways” and lower levels like “side streets,” letting you zoom toward the target quickly.
- How it works:
  - Greedy descent from the highest layer to Level 1: at each layer, hop to neighbors that improve similarity until no improvement, then drop a layer.
  - Precision pass at Level 0: run a best-first search with a priority queue. The ef parameter controls the candidate set size (bigger ef = higher recall, slower). M controls max connections per node (bigger M = more memory, better recall).
- Implementation notes: Uses cosine similarity, SplPriorityQueue, and a winners list to track top results; part of an open-source PHP project called Vektor for native vector search.
- Why it matters: Brings ANN-style speedups to pure PHP—no external services—while exposing clear speed/accuracy/memory trade-offs via ef and M.

Takeaway: If you’re doing semantic search in PHP, HNSW gives you near-instant queries with tunable precision instead of O(N) scans.

Here is a summary of the discussion:

The author, **cntmv**, joined the comments to explain their motivation: they wanted to deeply understand HNSW mechanics without determining them via external libraries, noting that modern PHP (8.x/JIT) is surprisingly capable of handling this workload. They positioned the library as a "drop-in solution" for PHP monoliths to add semantic search without managing external services like Qdrant or Pinecone.

Key discussion points included:

*   **Practical Use Cases:** User **hu3** asked about the feasibility of indexing 1,000–10,000 Markdown or PHP files for an LLM agent. The author confirmed the library handles 1,000 documents with millisecond search times, though they advised careful chunking when parsing code snippets.
*   **Dependency Confusion:** User **Random09** pointed out that the examples seemed to require OpenAI. The author clarified that the library is completely model-agnostic (compatible with Ollama, HuggingFace, etc.) but agreed to update the README, as the current "Hello World" example relies on OpenAI for convenience.
*   **Educational Value:** Several users praised the blog post's clarity, highlighting the use of "fantasy-based examples" (comparing programming to magic incantations) and the implementation itself, which **fthsx** described as creating "executable pseudocode" that makes complex algorithms easier to understand than lower-level implementations.

### Cycling Game (Mini Neural Net Demo)

#### [Submission URL](https://www.doc.ic.ac.uk/~ajd/Cycling/) | 21 points | by [ungreased0675](https://news.ycombinator.com/user?id=ungreased0675) | [3 comments](https://news.ycombinator.com/item?id=46458029)

Cycling Neuroevolution is a slick, interactive demo where a pack of identical cyclists are controlled by tiny neural nets that evolve race-by-race to get faster over a 2 km course with randomized terrain. You can watch tactics emerge—push on climbs, recover on descents, drafting trains, late sprinters—as selection and small weight mutations hone their behavior.

Highlights
- How it works: Each rider’s neural net takes in speed, current power, battery level (W′), short- and long-horizon gradient, gap to the rider ahead, and race progress; it outputs a per-timestep change in power (scaled by a Power Multiplier). After each race, the top 5 riders are kept; the next generation mixes exact copies with mutants (σ=1.0, 20 weights mutated).
- Physics/physiology: 87 kg rider+bike, CwA 0.32, Cr 0.004; drafting cuts drag by up to ~40%. Aerobic threshold 250 W, W′ 15 kJ that drains above threshold and recovers below; max sprint 750 W, tapering with battery state.
- What you can do: Click a rider to inspect their controller; red/blue inputs show real-time positive/negative contributions. Press space to force evolution mid-race, ‘r’ to reset the top-5 view, reload for fresh terrain and new populations.

Why it’s neat
- It’s an approachable, visual primer on neuroevolution with interpretable signals and believable cycling dynamics, showing how simple controllers plus selection pressure can yield lifelike race strategy.

By Andrew Davison (Imperial College London, 2025) — @ajddavison for ideas/suggestions.

**Discussion Summary:**
Commenters scrutinized the neural network's inputs, appreciating how separate moving averages for gradients (100m vs. 1000m) allow agents to distinguish between short rollers and sustained climbs. There was also a technical debate regarding sprint behaviors: while one user suggested a dedicated "distance to finish" input to trigger end-game bursts, another argued that the existing race progress metric should sufficiently handle late-race strategy.

### Show HN: A local-first financial auditor using IBM Granite, MCP, and SQLite

#### [Submission URL](https://github.com/simplynd/expense-ai) | 19 points | by [simplynd](https://news.ycombinator.com/user?id=simplynd) | [3 comments](https://news.ycombinator.com/item?id=46450489)

expense-ai is an open-source, local-first “Senior Auditor” for personal finance that turns raw bank statements into verified insights using the Model Context Protocol. Running entirely on your machine via Ollama, it pairs two Granite models (8B for reasoning, 2B for vendor normalization) with a FastAPI backend, an MCP server exposing SQLite tools, and a React dashboard. The LLM orchestrates SQL-backed queries to guarantee mathematically correct totals, filters out internal transfers/credit card settlements, and cleans messy merchant strings into readable vendor names—all without sending data to the cloud.

Highlights:
- Privacy by default: all processing is local (Ollama + SQLite)
- Agentic architecture: LLM chooses deterministic MCP tools; SQL handles all math
- Smart hygiene: vendor normalization and internal transfer filtering
- Practical workflow: upload text-based PDFs (no OCR yet), review/categorize, add manual fixed/cash expenses, visualize trends, then ask the “Senior Auditor” for verified analyses
- Stack: React UI, FastAPI app API, FastMCP server, Granite 8B/2B via Ollama; uses Astral’s uv for Python deps

Getting started: pull granite3.3:8b and :2b in Ollama, run the MCP server and FastAPI via uv, then npm run dev for the UI. Limitations: no OCR, auto-categorization is future work. Repo: github.com/simplynd/expense-ai (early-stage; ~23 stars).

Here is a summary of the discussion:

The creator, **smplynd**, provided a technical breakdown of the architecture, explaining that they achieved "100% mathematical accuracy" by strictly using the LLM to generate SQL queries via the Model Context Protocol (MCP) rather than letting the model perform calculations directly. They highlighted that the Granite 8B and 2B models offered the best balance of speed and consistency for local hardware. The author also reflected on the development process, noting that while AI is a great co-pilot, it requires developer oversight for "plumbing" issues like strict API types and CORS configurations.

Other comments focused on trust and security:
*   **IntelliAvatar** inquired about the execution-time security of the MCP implementation, specifically how the tool handles validation and side-effects when accessing the filesystem or network.
*   **da_grift_shift** pushed back on the reliability of the system, suggesting that the "100% accuracy" claim—and perhaps the author's own generated comment text—demonstrates that AI outputs still require a "quick manual review" before being submitted.

