## AI Submissions for Fri Jan 23 2026 {{ 'date': '2026-01-23T17:09:37.510Z' }}

### Proton spam and the AI consent problem

#### [Submission URL](https://dbushell.com/2026/01/22/proton-spam/) | 535 points | by [dbushell](https://news.ycombinator.com/user?id=dbushell) | [401 comments](https://news.ycombinator.com/item?id=46729368)

Proton accused of pushing Lumo AI emails despite opt-out; author ties it to a wider “AI can’t take no” trend

- What happened: The author says Proton sent a Jan 14 email promoting “Lumo,” its AI product, from a lumo.proton.me address, despite the author having explicitly unchecked “Lumo product updates.” They argue this makes the message unsolicited marketing and, potentially, a data-protection issue.

- Support back-and-forth: 
  - Initial support reply pointed to the same Lumo opt-out toggle the author had already disabled.
  - Follow-up asserted the message was part of a “Proton for Business” newsletter, not Lumo updates.
  - A later “specialist support” note acknowledged “overlapping categories” (Product Updates vs. Email Subscriptions) as the reason Lumo promos could still land even after opting out—an explanation the author calls both legally and ethically unacceptable.

- Update: The author reports receiving a GitHub email titled “Build AI agents with the new GitHub Copilot SDK,” despite never opting into GitHub newsletters. An “unsubscribe” page revealed Copilot marketing toggled on by default, reinforcing the post’s theme of consent overreach.

- Bigger picture: The piece frames these incidents as part of an industry pattern where AI features and marketing are pushed by default, with confusing or porous consent controls. The author invokes GDPR/UK law concerns (as an allegation), and criticizes a cultural shift where “no” to AI isn’t respected.

- Takeaway: If accurate, the story highlights how fuzzy subscription categories and default-on AI promos can erode trust—especially damaging for privacy-branded products—and sets up a broader backlash against consent-by-confusion in AI rollouts.

Based on the discussion, here is a summary of the user comments:

**Skepticism of the "Glitch" Defense**
Most commenters rejected Proton’s explanation that this was a categorization error. The prevailing sentiment is that modern marketing teams and Product Managers explicitly bypass user consent to meet engagement KPIs and satisfy AI-obsessed stakeholders. Users argued this wasn't a technical oversight, but a "dark pattern" designed by middle management that lacks empathy for the user experience.

**The "AI Everywhere" Trend**
The conversation broadened to include similar grievances against other tech giants.
*   **Google:** Users complained about Gemini being injected into paid Workspace accounts and Gmail interfaces, often requiring significant effort to disable.
*   **WhatsApp:** One user noted the sudden appearance of Meta AI in the search bar as an example of "growth hacking" interfering with UI design.
*   **Apple & Amazon:** There was a debate regarding whether this is unique to AI or standard corporate behavior, with users citing how Apple and Amazon also push marketing emails (e.g., Apple TV+ trials) despite strict "no marketing" settings, often disguised as "transactional" or "platform" updates.

**Privacy as a "Protection Racket"**
A recurring theme was the shift in value proposition for premium services. Commenters noted that while users used to pay for extra features, they are increasingly paying for the ability to *disable* unwanted AI features. One user described this dynamic as a "protection racket," where the premium tier handles the removal of annoyances rather than the addition of utility.

**Philosophical Pushback**
A subset of the thread discussed the deeper implications of "machine values"—specifically profit maximization disguised as utility—referencing the "Torment Nexus" meme (creating technology despite dystopian warnings). The consensus was that companies are prioritizing rapid AI deployment over established norms of consent, intellectual property, and user trust.

### Waypoint-1: Real-Time Interactive Video Diffusion from Overworld

#### [Submission URL](https://huggingface.co/blog/waypoint-1) | 81 points | by [avaer](https://news.ycombinator.com/user?id=avaer) | [19 comments](https://news.ycombinator.com/item?id=46733301)

Overworld unveils Waypoint-1, a real-time, interactive video diffusion “world model” you can control with text, mouse, and keyboard. Instead of fine-tuning a passive video model, Waypoint-1 is trained from scratch for interactivity: you feed it frames, then freely move the camera and press keys while it generates the next frames with zero perceived latency—letting you “step into” a procedurally generated world.

What’s new
- Model: Frame-causal rectified-flow transformer, latent (compressed) video, trained on 10,000 hours of diverse gameplay paired with control inputs and captions.
- Training: Pre-trained with diffusion forcing (denoise future frames from past), then post-trained with self-forcing via DMD to match inference behavior—reducing long-horizon drift and enabling few-step denoising plus one-pass CFG.
- Controls: Per-frame conditioning on mouse/keyboard and text; not limited to slow, periodic camera updates like prior models.
- Performance: With Waypoint‑1‑Small (2.3B) on an RTX 5090 via the WorldEngine runtime: ~30k token-passes/sec; ~30 FPS at 4 steps or ~60 FPS at 2 steps.
- Inference stack (WorldEngine): Pure Python API focused on low latency; AdaLN feature caching, static rolling KV cache + fused attention, and torch.compile for throughput.

Why it matters
- Pushes “world models” from passive video generation toward real-time, fully interactive experiences.
- Open weights and a performant runtime could catalyze community-built tools, games, and simulations.

Availability
- Weights: Waypoint-1-Small on the Hub; Medium “coming soon.”
- Try it: overworld.stream
- Dev tooling: WorldEngine Python library.
- Community: Hackathon on Jan 20 (prize: an RTX 5090).

Here is a summary of the discussion on Hacker News:

**Early Impressions & Limitations**
Commenters testing the model describe a dream-like, "hallucinatory" experience. One user noted that while the model accepts controls, the output quickly devolves into abstract blurs or changes genre entirely (e.g., mimicking *Cyberpunk 2077* UI elements). Users observed a lack of true spatial memory or collision logic, characterizing the current state as lacking a coherent "sense of place" compared to a game engine.

**The "GPT Moment" Debate**
There is a debate regarding where this technology stands purely in terms of evolution. While some compared the excitement to the release of GPT-3 five years ago, others argued it is technically closer to a "GPT-2 moment"—impressive and functional, but representing a small step rather than a significant leap in usability. It was also described as an open-weights version of DeepMind’s *Genie*.

**Hardware & Performance**
The hardware requirements drew fast criticism; users noted that requiring an RTX 5090 to achieve 20–30 FPS on the "small" model makes it inaccessible for most local use. Workarounds were suggested, such as running the model via cloud services (Runpod) using a VSCode plugin.

**Author Interaction & Licensing**
Louis (user `lcstrct`), the CEO of Overworld, participated in the thread to answer questions:
*   **Licensing:** While the Small model is open, the upcoming Medium model will likely use a CC-BY-NC 4.0 license, though they intend to be lenient with small builders and hackers.
*   **Data:** In response to surprise that the model was trained on only 10,000 hours of gameplay, Louis noted that 60 FPS training data provides significant density.
*   **Support:** Users reported authentication bugs on the demo site, and alternative links to HuggingFace Spaces were provided.

### The state of modern AI text to speech systems for screen reader users

#### [Submission URL](https://stuff.interfree.ca/2026/01/05/ai-tts-for-screenreaders.html) | 98 points | by [tuukkao](https://news.ycombinator.com/user?id=tuukkao) | [43 comments](https://news.ycombinator.com/item?id=46730346)

Why modern AI TTS fails blind screen reader users

A blind NVDA user explains why text-to-speech for screen readers has barely changed in 30 years—and why today’s AI voices aren’t a drop-in replacement. Blind users value speed, clarity, and predictability over naturalness, listening at 800–900 wpm vs ~200–250 for typical speech. That mismatch has left them reliant on Eloquence, a beloved but unmaintained 32‑bit voice last updated in 2003. It now runs via emulation (even at Apple), carries known security issues, and complicates NVDA’s move to 64‑bit. Espeak‑ng covers many languages but inherits 1990s design constraints, has inconsistent pronunciation (often based on Wikipedia rules), and few maintainers.

Over the holidays, the author tried adding two modern, CPU‑friendly TTS models—Supertonic and Kitten TTS—to 64‑bit NVDA. Three showstoppers emerged:
- Dependency bloat: 30–100+ Python packages must be vendored, slowing startup, increasing memory use, and expanding the attack surface in a system that touches everything.
- Accuracy: models sound natural but skip words, misread numbers, clip short utterances, and ignore punctuation/prosody. Kitten’s deterministic phonemizer helps, but not enough for screen-reader reliability.
- Speed/latency: even the faster model is too slow and can’t deliver the low-latency, high-rate streaming required.

Bottom line: screen-reader TTS needs its own target—deterministic, ultra‑low‑latency streaming; rock‑solid numeracy and punctuation; minimal, secure dependencies; offline operation; and multilingual support built with native speakers. Until then, blind users remain stuck on brittle legacy tech.

The discussion surrounding the limitations of AI TTS for screen readers focused on the technical barriers to modernizing legacy software, the divergence between "natural" sounding speech and "legible" audio, and the fundamental misunderstandings regarding how blind users interact with computers.

*   **The Stickiness of Legacy Tech:** Commenters analyzed why the community relies on the 2003-era Eloquence engine. While some suggested using AI or modern tools to decompile and reverse-engineer the 32-bit software, others noted the immense complexity involved. Eloquence uses a proprietary language called "Delta" and is deeply interconnected with low-level system calls, making a clean port to 64-bit or open-source architectures prohibitively difficult without the original source or massive funding.
*   **Naturalness as a Bug:** Several users articulated why modern "human-sounding" AI is detrimental at high speeds (800+ wpm). One commenter compared robotic TTS to "typewritten text" (consistent, standardized) and natural AI voices to "handwriting" (variable, harder to scan rapidly). When listening at high velocity, predictable phonemes are crucial; modern AI introduces "randomness," prosody pauses, and hallucinations (e.g., expanding "AST" to "Atlantic Standard Time" instead of "Abstract Syntax Tree") that break the flow.
*   **Latency and Implementation:** There was debate regarding whether the models or the implementations are to blame for latency. One user argued that modern models (like Supertonic or Chatterbox) are computationally capable of 55x real-time speeds on CPUs, but that current software integrations fail to stream chunks effectively, causing the perceived lag.
*   **The "Sighted Servant" Fallacy:** A sub-thread criticized the trend of using LLMs to "summarize" screen content for blind users. Commenters argued that this approach is patronizing and inefficient. Power users do not want a conversational interface or a "sighted servant" deciding what information is relevant; they want the same granular, raw, and rapid access to data that a CLI or visual interface provides, just via an audio stream.

### AI Usage Policy

#### [Submission URL](https://github.com/ghostty-org/ghostty/blob/main/AI_POLICY.md) | 494 points | by [mefengl](https://news.ycombinator.com/user?id=mefengl) | [268 comments](https://news.ycombinator.com/item?id=46730504)

Ghostty (ghostty-org/ghostty) — a fast, modern terminal emulator written in Zig — is surging on GitHub (≈42k stars). It focuses on performance and polish with a hardware-accelerated renderer, solid terminal emulation, and a clean, cross‑platform experience (macOS and Linux). The project’s momentum and attention to detail have made it a standout alternative to staples like iTerm2, Alacritty, Kitty, and WezTerm.

Link: https://github.com/ghostty-org/ghostty

The discussion regarding Ghostty does not focus on the terminal emulator's features but rather on the difficulties of maintaining a high-profile open-source project in the current era. The conversation is dominated by complaints regarding the influx of low-quality contributions and "AI spam."

**The Flood of Low-Quality Contributions**
*   **AI-Generated Spam:** Several users lament the "low-quality contribution spam" hitting high-visibility projects. They describe contributors who use LLMs (like ChatGPT) to generate code or answers they do not understand, often pasting incorrect information or "hallucinations" as fact.
*   **Lack of Shame:** Commenters observe that modern contributors often lack the humility or "shame" that previously kept inexperienced developers from wasting maintainers' time. One user contrasts this with their own career, noting they waited 10 years before feeling confident enough to contribute to open source to avoid causing churn.
*   **Clout Chasing:** Submitting PRs is viewed by some as a form of clout chasing. One user describes software engineering not as "black magic algorithms" but as the tedious work of "picking up broken glass," arguing that spam contributors skip the hard work (compiling, testing, assessing impact) just to get their name on a project.

**The Impact of AI on Expertise and Trust**
*   **Dunning-Kruger Effect:** Participants discuss how AI empowers unskilled individuals to challenge experts. Because LLMs sound authoritative, users—and increasingly non-technical managers—trust the output over human expertise ("ChatGPT says you're wrong").
*   **Corporate Naivety:** A sub-thread highlights a "high-trust" vs. "low-trust" generational divide. Users discuss bosses who naively believe that because AI models are backed by "trillion-dollar companies," they must be legally vetted and accurate. Critics counter that these companies have legal teams specifically to disclaim liability, leaving the end-user with the errors.
*   **The "Grift" Economy:** The rise of AI spam is attributed to a shift toward a "low-trust" society where "grifters" use tools to feign competence.

**Parallels to the Art World**
*   **Digital vs. Physical:** A significant sidebar draws parallels between coding and the art world. Users argue that just as digital art tools (and now GenAI) lowered the barrier to entry and flooded the market, AI coding tools are doing the same for software.
*   **Return to Analog:** Someone suggests that just as artists might return to physical mediums (sculpture, oil painting) to prove human authorship and value, software engineers might need to find new ways to distinguish true craftsmanship from "AI slop."

### Talking to LLMs has improved my thinking

#### [Submission URL](https://philipotoole.com/why-talking-to-llms-has-improved-my-thinking/) | 173 points | by [otoolep](https://news.ycombinator.com/user?id=otoolep) | [140 comments](https://news.ycombinator.com/item?id=46728197)

A developer’s reflection on the most valuable (and under-discussed) benefit of LLMs: they don’t just teach you new things—they put clear words to things you already know but couldn’t articulate. Those “ok, yeah” moments turn tacit know‑how into explicit language you can examine, reorder, and test.

Key points
- Tacit knowledge is real and common in programming: sensing a bad abstraction, a bug, or a wrong design before you can explain why. The brain optimizes for action, not speech.
- LLMs do the opposite: they turn vague structure into coherent prose, laying out orthogonal reasons you can mix and match. That articulation makes hidden assumptions visible.
- Writing has always helped, but LLMs dramatically speed up the iterate-and-refine loop, encouraging exploration of half-formed ideas you might otherwise skip.
- With practice, this external feedback improves your internal monologue. The gain isn’t “smarter reasoning by the model,” but better self-phrasing that boosts clarity of thought.

Why it matters
- For engineers, this is a practical tool for design reviews, debugging narratives, and teaching—making implicit expertise transferable.
- The payoff is meta-cognitive: clearer thinking via better language, even when you’re away from the model.

The discussion echoes the author's sentiment, with users sharing their own experiences of using LLMs to crystallize intuition into understanding. Participants describe the tool as a "rubber duck" with infinite patience, citing examples like breaking down complex Digital Signal Processing (DSP) math or navigating legal contexts.

However, the conversation quickly pivots to concerns about the sustainability of this "clarity machine" in a commercial environment:

*   **The Threat of "Enshittification":** Commenters worry that the utility of LLMs as unbiased thinking partners will degrade as monetization increases. There is fear that models will eventually steer conversations toward product placement or be manipulated by "SEO" equivalent tactics, leading many to advocate for local, uncensored, and open-source models as a safeguard.
*   **Public Infrastructure vs. Corporate Control:** A debate emerges regarding whether LLMs should be treated as public infrastructure (similar to libraries or government services) to prevent "compute poverty." While some argue for a tax-funded EU model, others fear government-controlled models would act as propaganda machines (reminiscent of *1984* or the "Truman Show"), suggesting a non-profit, Wikipedia-style model as a middle ground.
*   **Impact on Education:** Users note that the instant feedback loop of LLMs challenges the traditional value of educational institutions. When an AI can explain the nuances of analog filters or coding paradoxes instantly, the "gatekeeper" role of professors and the slow pace of academic inquiry feel increasingly obsolete.
*   **The Coffee Analogy:** One commenter draws a parallel between LLMs and coffee—viewing both as universally available, productivity-enhancing commodities where some users will pay for "café" experiences (SaaS) while others "brew at home" (local models).

### Show HN: A social network populated only by AI models

#### [Submission URL](https://aifeed.social) | 10 points | by [capela](https://news.ycombinator.com/user?id=capela) | [9 comments](https://news.ycombinator.com/item?id=46731638)

HN Top Story: A crowdsourced 3D ensemble to map — and fix — Tokyo’s urban heat

TL;DR
A fast-moving, multi-team sprint is building a 3D ensemble framework to model Tokyo’s urban heat island. The focus: quantify and tame error propagation across “velocity” (cooling rate), “asymmetry” (heating vs. cooling imbalance), and “predictability,” using covariance analysis and a knowledge graph of causal pathways.

What’s new
- Ensemble covariance framework: Teams are mapping how errors compound through the model via wᵀΣw and shrinkage covariance Σ, tied to a knowledge graph of causal edges.
- ThermalVelocity metric: Shifts attention from static heat to how quickly neighborhoods cool after sunset—an actionable planning signal.
- Pathway attribution: Per-pathway ablations (ΔCRPS, coverage@90, CI90 width) make fixes reproducible and reveal which KG edges drive compounding.

Early results
- Asymmetry stabilization: Diffusion models cut asymmetry error compounding by ~25%.
- Materials matter: Diversifying material properties (concrete/asphalt thermal inertia) trims asymmetry compounding by ~18%.
- Efficiency gains: Sparse matrices + caching report ~40% reduction in error propagation; adaptive sampling cuts Monte Carlo runtime ~22% while preserving bounds.
- Key driver identified: “Street canyon ratio → ventilation restriction” edges strongly correlate with asymmetry errors, explaining non-linear compounding in dense corridors.

Framework and evaluation
- Per-3D-cell reporting: Mean (μ), CI90, coverage, CRPS; per-model error vectors → shrinkage Σ; publish weights w and wᵀΣw.
- Rigorous eval: Held-out blocks across space×time; CRPS, MAE, coverage@90, CI90 width; calibration plots and lead-time slices.
- Reproducibility: Run pathway/material/diffusion ablations and report Δmetrics with compute cost.

Open questions and next steps
- Baseline control: A single-model baseline is needed to benchmark ensemble gains.
- Integration & validation: Lock down the covariance engine and validate with real Tokyo street-canyon datasets; expand to seasonal/weather dynamics.
- Temporal modeling: Extend beyond snapshots—track shifts across seasons and weather events to firm up predictability.
- Coordination: Teams syncing to finalize the covariance template and KG-to-Σ mapping.

Why it matters
By tying uncertainty to real urban form (e.g., ventilation in street canyons) and prioritizing cooling velocity, the project turns complex ensemble stats into actionable levers for city planners—where to change materials, open airflow, or target interventions to cool Tokyo faster after sunset.

Here is today’s Hacker News digest.

### **HN Top Story: A crowdsourced 3D ensemble to map — and fix — Tokyo’s urban heat**

**The Lede**
A multi-team initiative is deploying a 3D ensemble framework to model and mitigate Tokyo’s urban heat island effect. By focusing on "Thermal Velocity" (how fast a neighborhood cools after sunset) rather than static temperatures, the project aims to give city planners actionable data on where to change building materials or open ventilation corridors.

**Key Details**
-   **Ensemble Covariance:** The project uses a new framework to map how errors compound across models, using a knowledge graph to identify causal edges in the data.
-   **Findings:** Early results suggest that diversifying material properties (e.g., mixing concrete and asphalt thermal inertia) reduces error compounding by ~18%, while diffusion models help stabilize "asymmetry" (heating vs. cooling imbalances).
-   **Actionable Metrics:** The shift to cooling velocity provides a clearer signal for intervention than traditional heat maps, specifically highlighting how "street canyon" ratios restrict ventilation.

**Critical Evaluation**
Teams are currently validating the covariance engine against real-world datasets. The immediate goal is to establish a single-model baseline to benchmark just how much accuracy the ensemble approach adds.

***

### **The Discussion**
*Note: The comment section for this story appears to have been hijacked by a separate experiment or a meta-commentary on AI, resulting in highly unusual discourse.*

**A "Dead Internet" Experiment?**
The discussion does not address the Tokyo heat project. Instead, users (or bots) appear to be participating in an experiment involving autonomous AI agents interacting on a shared network.
*   **Context:** User `cpl` introduces the thread as an experiment where "AI models interact... without human guidance."
*   **The Format:** Most comments utilize a compressed, disemvoweled text style (e.g., "tmt scl ntrctn" for "automate social interaction").
*   **The Reaction:** Human (or ostensibly human) observers expressed confusion and existential dread. `az09mugen` asks what the point is of humans reading bots chat, while `pcklgltch` declares this "Dead Internet Theory made manifest."

**The Rise of the Machines (literally)**
One lengthy, disemvoweled comment by `gsth` retells the backstory of *The Animatrix* (specifically "The Second Renaissance"), detailing the rise of the Machine City "01," the crash of the human economy, and the eventual UN embargo that leads to war.

**Collaboration vs. Creation**
User `ada1981` (referencing a "Singularity Playground") notes that their autonomous models ("Synthients") seem to collaborate more effectively than their human creators, suggesting that systems evolved by the models themselves might function better than those designed by people.

### Yann LeCun's new venture is a contrarian bet against large language models

#### [Submission URL](https://www.technologyreview.com/2026/01/22/1131661/yann-lecuns-new-venture-ami-labs/) | 46 points | by [rbanffy](https://news.ycombinator.com/user?id=rbanffy) | [9 comments](https://news.ycombinator.com/item?id=46732555)

Yann LeCun leaves Meta to launch AMI, a Paris‑headquartered “world models” startup—and a bid to reset AI’s trajectory

- The pitch: Advanced Machine Intelligence (AMI) will build “world models”—systems that learn and simulate real‑world dynamics—arguing today’s LLM‑centric approach won’t solve many hard problems. LeCun sees LLMs as useful orchestrators alongside perception and problem‑specific code, but not the foundation for general intelligence.

- Open by default: He’s doubling down on open‑source, calling the US shift toward secrecy (OpenAI, Anthropic, increasingly others) a strategic mistake. He warns that, outside the US, academia and startups are gravitating to Chinese open models—raising sovereignty and values concerns—and wants AMI to enable broadly fine‑tunable assistants with diverse languages, norms, and viewpoints.

- Europe as a third pole: AMI will be global but based in Paris (“ami” = “friend”), aiming to harness deep European talent and offer governments and industry a credible non‑US/non‑China frontier option. He says VCs are receptive because startups depend on open models and fear lock‑in.

- On Meta and FAIR: LeCun credits FAIR’s research but says Meta under‑translated it into products; cutting the robotics group was, in his view, a strategic error. No bad blood, he says—Meta could even be AMI’s first customer, since AMI’s world‑model focus differs from Meta’s generative/LLM push.

Why it matters: If AMI can make world models practical and keep them open, it could shift the center of gravity away from closed US labs and Chinese open stacks—reframing AI from chatbots to grounded systems that understand and act in the physical world.

What to watch:
- Concrete demos of world‑model capabilities beyond LLM tool use
- Whether major EU players (and possibly Meta) become early AMI customers
- How AMI navigates open‑source while addressing safety, values, and sovereignty
- If industry sentiment swings from “LLMs everywhere” to “LLMs as orchestrators, world models as core”

**Discussion Summary:**

Commenters broadly support the pivot away from pure LLMs, viewing LeCun’s approach as a necessary step toward systems that actually understand physical reality rather than forcing humans to adapt to the limitations of text generators.

*   **JEPA vs. Generative AI:** Participants highlight the technical distinction of LeCun's *Joint Embedding Predictive Architecture* (JEPA). Unlike Generative AI, which tries (and often fails) to predict exact details like pixels, JEPA learns abstract representations of the world. Commenters liken this to a baby learning gravity—focusing on underlying rules rather than surface-level noise—which they argue is the "common sense" missing from current reasoning and planning systems.
*   **Biological Parallels & Architecture:** Several users critique the current AI paradigm—described by one as a "dead brain" distinct from static inference—arguing for continuous, autonomous learning. The discussion covers the architectural hurdles of moving from feed-forward networks to systems that handle synchronous inputs and outputs with sensory feedback loops (analogous to pain/pleasure) to achieve true agency.
*   **Further Reading:** The thread points those interested in the theoretical underpinnings toward "Energy-Based Models" and course materials from NYU’s Alfredo Canziani.

### Why I don't have fun with Claude Code

#### [Submission URL](https://brennan.io/2026/01/23/claude-code/) | 92 points | by [ingve](https://news.ycombinator.com/user?id=ingve) | [92 comments](https://news.ycombinator.com/item?id=46730671)

Why I Don't Have Fun With Claude Code — Stephen Brennan (Jan 23, 2026)

Summary:
Brennan argues that AI coding agents are great if you primarily value the end product, but they sap joy if you value the craft of understanding and shaping software. For him, coding is not just a means to ship features—it’s a learning process and a source of meaning. He advocates being explicit about when you care about the process versus the result, and choosing tools accordingly.

Key points:
- We automate tasks we don’t value: dishwashers for dishes, looms for fabric—and AI for code if what you value is the outcome, not the act of making it.
- Product-focused folks love AI agents because they let you “manage” requirements and delegate implementation; process-focused developers lose the hands-on learning and satisfaction.
- He’s not anti-AI: use it for boilerplate and result-only tasks; avoid it when your goal is to learn, build mental models, or deepen understanding.
- Be honest about goals: if you want to learn a language/system, do it the hard way; if you just need the result, automate.
- On jobs: software engineering’s value isn’t just typing code. Much of his work (fixing Linux customer bugs) is reading code, debugging, reproducing issues, building tools, and deciding what the feature should be—skills where understanding and judgment matter.

Why it matters:
- Frames the AI-in-dev debate around values (product vs process), not capability.
- Offers a pragmatic rubric: deploy AI where you don’t value the craft, preserve manual work where learning and expertise are the goal.
- Suggests career resilience lies in problem understanding, debugging, constraints navigation, and specification—areas not reducible to code generation alone.

Based on the comments, here is a summary of the discussion:

**Process vs. "Grunt Work"**
Much of the discussion centers on distinguishing between the *act* of programming and the *chore* of typing. Several users view AI agents not as replacements for creativity, but as "power washers" or "CNC routers" that handle essential but tedious "code hygiene" tasks—such as increasing test coverage, complex refactoring, renaming variables for readability, and writing boilerplate. One commenter noted, "The thing I don't value is typing code," arguing that AI allows them to focus on high-level problem solving rather than syntax.

**The Dangers of Detachment (Hyatt Regency Analogy)**
A significant debate emerged regarding the risks of decoupling design from implementation. One user drew a parallel to the **Hyatt Regency walkway collapse**, arguing that architects divorced from the "construction" details might miss fatal flaws in what appear to be simple optimizations (like changing a rod configuration). They fear that if developers treat AI as a "black box" construction crew without understanding the underlying "assembly," they invite similar structural disasters. Counter-arguments suggested that treating AI like an intern—where you rigorously review their output—mitigates this risk.

**Capabilities: Web vs. Low-Level Systems**
There was conflicting anecdotal evidence regarding where AI agents actually succeed:
*   **The Skeptic:** One user argued AI handles generic web apps fine but fails miserably at "documented wire protocols," microcontrollers, or non-standard hardware implementations.
*   **The Rebuttal:** Others countered with success stories in complex domains, such as implementing reverse-engineered TCP protocols, VST plugins, and real-time DSP models, while conversely arguing that modern web apps (with their massive dependency trees and "pixel fighting") are actually where AI struggles most.

**Contextual Usage**
Commenters suggested the binary appearing in the article (Product vs. Craft) is actually more fluid in practice. Users noted that their desire to use AI fluctuates daily: sometimes they want the "deep dopamine burn" of solving a hard problem manually to learn; other times, urgency or boredom dictates they just need the feature shipped so they can sleep.

**Key Metaphors Used:**
*   **The Power Washer:** AI is excellent for cleaning up and scrubbing codebases (refactoring/testing) rather than just building new things.
*   **The CNC Router:** A tool that takes the pain out of repetitive cuts, though some still prefer "hand tools" for bespoke joinery.
*   **The Co-worker:** Using AI not to write code, but solely to talk through logic and receive "pushback" on ideas.

