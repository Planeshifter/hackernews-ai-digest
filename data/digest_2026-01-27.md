## AI Submissions for Tue Jan 27 2026 {{ 'date': '2026-01-27T17:19:27.379Z' }}

### AI2: Open Coding Agents

#### [Submission URL](https://allenai.org/blog/open-coding-agents) | 225 points | by [publicmatt](https://news.ycombinator.com/user?id=publicmatt) | [40 comments](https://news.ycombinator.com/item?id=46783017)

Open Coding Agents (AI2): SERA open models + a cheap, reproducible recipe to adapt coding agents to any repo

- What’s new: AI2 released SERA (Soft-verified Efficient Repository Agents) and a fully open pipeline—models, training data, recipes, and a one-line launcher/CLI (PyPI)—to build and fine‑tune coding agents for your own codebase. Works out of the box with Claude Code.

- Performance: SERA-32B solves 54.2% on SWE-Bench Verified, edging prior open-source peers of similar size/context. Trained in ~40 GPU-days; they say reproducing prior open SOTA costs ~$400, and ~$12k can rival top industry models of the same size.

- Key idea: “Soft-verified generation” (SVG) for synthetic data—patches don’t need to be fully correct to be useful—plus a “bug-type menu” to scale/diversify data. They report SOTA open-source results at a fraction of typical costs.

- Efficiency claims: Matches SWE-smith at ~57× lower cost and SkyRL at ~26× lower cost. Fine-tuning on private code reportedly lets SERA-32B surpass its 110B teacher (GLM-4.5-Air) on repos like Django/Sympy after ~8k samples (~$1.3k).

- Inference throughput (NVIDIA-optimized): ~1,950 tok/s (BF16, 4×H100), ~3,700 tok/s (FP8), and ~8,600 tok/s on next‑gen 4×B200 (NVFP4), with minimal accuracy drop at lower precision.

- Why it matters: Puts strong, repo-aware coding agents within reach for small teams—no large-scale RL stack required—while keeping the models and data open for inspection and iteration.

Notes/caveats:
- Results center on SWE-Bench Verified; real-world repo adaptation and privacy/process for generating synthetic data from private code merit scrutiny.
- Cost/speed numbers depend on specific NVIDIA hardware and settings.

Here is a summary of the discussion:

**Comparisons and Performance Claims**
The most active debate concerned whether SERA truly reclaims the open-source SOTA title. Users pointed to Meta’s CWM models, which reportedly achieve higher scores (65% on SWE-bench Verified) when using Test-Time Selection (TTS). A discussion participant (likely a paper author) pushed back, arguing that TTS adds significant latency and cost, making it impractical for local deployment. They emphasized that SERA is optimized for efficiency and lower context lengths (32k/64k) compared to the hardware-intensive requirements of rival models.

**Openness and Licensing**
Commenters distinguished between "open weights" and "open science." While models like Mistral Small 2 and Meta’s CWM have open weights, users noted that Meta’s license restricts commercial use and does not disclose training data. In contrast, AI2 was praised for releasing the full pipeline, including training data and the recipe for synthetic data generation, allowing for genuine reproducibility and commercial application.

**Terminology: "Agent" vs. "LLM"**
There was significant semantic pushback regarding the use of the term "Agent." Users argued that an LLM itself is not an agent; rather, an agent is the combination of an LLM plus a scaffolding/harness (a loop to execute tasks). Others suggested a distinction between "Agentic LLMs" (models fine-tuned for reasoning and tool-calling) and the broader systems that utilize them.

**Fine-Tuning vs. Context Window**
Users debated the practical utility of fine-tuning a 32B model on a specific repository versus using RAG (Retrieval-Augmented Generation) with a massive context window on a frontier model (like GPT-4 or Claude 3.5).
*   **Skeptics** argued that intelligent context management with a smarter model is usually superior to fine-tuning a "dumber" model.
*   **Proponents** (one claiming to work on the "world's largest codebase") countered that fine-tuning is essential for proprietary internal libraries and syntax that general models cannot infer, even with large context windows.

**Miscellaneous**
*   **Claude Code Integration:** Some users were confused by the claim that the system requires Claude Code to run; others clarified that Claude Code is simply the harness/CLI being supported out of the box, while other open harnesses (like OpenCode or Cline) could also be used.
*   **Speed:** The inference speed (up to ~8,600 tok/s on B200s) was highlighted as a major advantage, though some questioned the specific hardware dependencies.
*   **Technique:** The synthetic data generation method—extracting tests from PR diffs and having the model reconstruct the patch—was noted as a clever approach to scaling training data.

### Management as AI superpower: Thriving in a world of agentic AI

#### [Submission URL](https://www.oneusefulthing.org/p/management-as-ai-superpower) | 94 points | by [swolpers](https://news.ycombinator.com/user?id=swolpers) | [87 comments](https://news.ycombinator.com/item?id=46782811)

Ethan Mollick recounts an experiment at Wharton: executive MBA students used agentic AI tools (Claude Code, Google Antigravity, ChatGPT, Claude, Gemini) to go from zero to working startup prototypes in four days. Results were roughly a semester’s worth of progress pre‑AI: real core features, sharper market analyses, and easier pivots. Example demos included Ticket Passport (verified ticket resale), Revenue Resilience (at‑risk revenue detection with agentic remediation), a parenting activity matcher, and Invive (blood sugar prediction).

The bigger point: management—clear goals, constraints, and evaluation—has become the AI superpower. Mollick offers a mental model for deciding when to hand work to AI:
- Human Baseline Time: how long you’d take to do the task yourself
- Probability of Success: chance the AI meets your quality bar per attempt
- AI Process Time: time to prompt, wait, and verify each AI output

You’re trading “doing the whole task” vs. “paying overhead” potentially multiple times. If the task is long, AI is fast and cheap, and the success probability is high enough, delegate. If checking takes long and success is low, just do it yourself. He ties this to OpenAI’s GDPval study: experts took ~7 hours; AI was minutes to generate but ~1 hour to verify—so the tipping point depends on model quality and your acceptance bar.

Why HN cares
- Practical rubric for real work: when AI accelerates vs. wastes time
- Emphasis on evaluation as the scarce skill, not prompting tricks
- Lower pivot costs enable broader exploration and parallel bets
- Caveats: jagged frontier unpredictability, verification overhead, domain/regulatory risk, and the danger of over‑delegation without subject‑matter judgment

**Is writing code still the bottleneck?**
A contentious debate emerged regarding the author’s premise that code generation is no longer the limiting factor in software development. While some agreed that the bottleneck has shifted to "deciding what to build," specification, and review, many argued that the true constraint remains visualizing complex systems, debugging, and managing architecture in medium-to-large codebases—tasks where AI agents frequently fail due to context window limitations and a lack of holistic understanding.

**The "Mental Model" Deficit**
A recurring critique focused on the trade-off between speed and comprehension. Commenters noted that writing code is how engineers build a mental model of the system. By delegating implementation to AI, developers risk losing the deep understanding required to debug subtle failures or make architectural decisions later. This led to concerns that AI speed is illusory, merely shifting time saved on typing toward "paying overhead" on reading, validating, and fixing "myopic" AI error corrections that introduce technical debt.

**The shift from "Builder" to "Reviewer"**
Discussion highlighted the psychological toll of this shift. Several users argued that "managing" AI strips away the creative, satisfying parts of engineering (building), leaving humans with the "grind" of tedious verification, cleanup, and orchestration—work described by some as mind-numbing. Others pointed out that unlike managing human juniors, managing AI lacks the rewarding aspect of mentorship and teaching soft skills.

**Enforcement and Verification**
Participants noted that if AI is treated as a "junior" or a force multiplier, the reliance on automated enforcement (linting, tests, strict architectural boundaries) must increase drastically. Because AI does not "learn" best practices deeply and can hallucinate valid-looking but broken structures, human reviewers must implement rigorous automated checks to prevent a collapse in code quality.

### Kimi Released Kimi K2.5, Open-Source Visual SOTA-Agentic Model

#### [Submission URL](https://www.kimi.com/blog/kimi-k2-5.html) | 483 points | by [nekofneko](https://news.ycombinator.com/user?id=nekofneko) | [227 comments](https://news.ycombinator.com/item?id=46775961)

What’s new
- Native multimodal model trained on ~15T mixed vision/text tokens; pitched as the most powerful open-source model to date.
- Strong coding + vision focus: image/video-to-code, visual debugging, and reasoning over visual puzzles. Demos include reconstructing sites from video and generating animated front‑end UIs from a single prompt.
- “Agent Swarm” paradigm: K2.5 can self-orchestrate up to 100 sub‑agents and ~1,500 tool calls in parallel, claiming up to 4.5x faster execution vs single‑agent runs. No predefined roles; the model decomposes and schedules work itself.

How the swarm works (research preview)
- Trained via Parallel-Agent Reinforcement Learning (PARL) with a learned orchestrator and frozen sub‑agents.
- Uses staged reward shaping to avoid “serial collapse” (falling back to single‑agent). Introduces a “Critical Steps” metric to optimize the critical path rather than raw step count.
- Reported gains on complex, parallelizable tasks and strong scores on agentic benchmarks (HLE, BrowseComp, SWE‑Verified) at lower cost.

Coding with vision
- Emphasis on front‑end generation and visual reasoning to lower the gap from mockups/video to working code.
- Autonomous visual debugging: the model inspects its own outputs, consults docs, and iterates without handholding.
- Internal “Kimi Code Bench” shows step‑up over K2 on build/debug/refactor/test tasks across languages.

Ecosystem and availability
- Access via Kimi.com, Kimi App, API, and Kimi Code.
- Four modes: K2.5 Instant, Thinking, Agent, and Agent Swarm (Beta). Swarm is in beta on Kimi.com with free credits for higher‑tier paid users.
- Kimi Code: open‑source terminal/IDE tooling (VSCode, Cursor, Zed, etc.), supports image/video inputs, and auto‑discovers/migrates existing “skills” and MCPs into your setup.

Why it matters
- Pushes the frontier on practical multimodal coding and parallel agent execution—two areas with big latency and productivity payoffs for real software work.
- If the “self‑directed swarm” generalizes, it could make agent workflows faster and less brittle than hand‑crafted role trees.

Caveats to watch
- Benchmarks and demos can overfit; real‑world reliability, tool integration quirks, and cost at scale remain to be seen.
- “Open‑source” claims often hinge on licensing/weights availability—expect scrutiny on what exactly is released and under what terms.
- Swarm benefits depend on tasks that truly parallelize; sequential or tightly coupled tasks won’t see the same speedups.

How to try
- Experiment with K2.5 Agent/Swarm on Kimi.com (beta) and wire up Kimi Code in your terminal/IDE for image/video‑to‑code and autonomous debugging workflows.

Based on the discussion, here is a summary of the comments regarding the Kimi K2.5 submission:

**Hardware Feasibility & Requirements**
The primary topic of debate is the feasibility of running a 1-trillion parameter model (even with only 32B active parameters) on local hardware.
*   **The VRAM Bottleneck:** Users noted that even with Int4 quantization, a 1T parameter model requires approximately 500GB of VRAM, which is prohibitively expensive for most consumers.
*   **MoE Architecture:** Defenders of the "local" potential argued that because it is a Mixture of Experts (MoE) model, compute requirements are lower (only 32B active parameters per token). However, the total VRAM capacity remains the hard constraint.
*   **High-End Consumer Gear:** Some suggested that high-end consumer hardware (like Mac Studios with unified memory or PCs with the upcoming Strix Halo) might handle the *active* parameter load, but storing the full model remains a challenge without massive memory pools.

**Quantization & Model Quality**
There was significant skepticism regarding how much the model must be compressed to be usable.
*   **Quantization Trade-offs:** Users debated whether a massive model heavily quantized (to 4-bit or 2-bit) performs better than a smaller model running at higher precision. Some reported that while benchmarks might survive quantization, real-world usage often suffers from "death spirals" (repetitive loops) or logic failures.
*   **BitNet & Future Tech:** The discussion touched on recent research (like BitNet/1.58-bit models) and whether Kimi uses post-training quantization vs. quantization-aware training.
*   **Hardware Support:** It was noted that running Int4 effectively requires hardware that natively supports the format; otherwise, the hardware wastes throughput unpacking the data.

**System Architecture & OS Limitations**
*   **Memory Offloading:** The viability of keeping the model on SSDs and swapping "experts" into RAM was debated. Experts argued that because expert activation is often random (low locality), SSD latency would make inference unacceptably slow.
*   **Windows vs. Linux:** One user argued that for consumer Nvidia cards (e.g., RTX 3000 series), Windows currently handles shared memory (using system RAM as VRAM overflow) better than Linux drivers, which were described as unstable or difficult to configure for this specific use case.

**Defining "Local"**
A philosophical disagreement emerged regarding what "running locally" means.
*   Some users feel "local LLM" implies standard consumer hardware (gaming PCs/laptops).
*   Others argued that any model running on on-premise hardware (even a $20k server cluster) counts as local, distinguishing it from API-only services.

### LLM-as-a-Courtroom

#### [Submission URL](https://falconer.com/notes/llm-as-a-courtroom/) | 67 points | by [jmtulloss](https://news.ycombinator.com/user?id=jmtulloss) | [29 comments](https://news.ycombinator.com/item?id=46784210)

Falconer: an “LLM-as-a-Courtroom” to fight documentation rot

Falconer is tackling the classic “documentation rot” problem—code evolves, docs don’t—by auto-proposing and updating internal docs when PRs merge. The hard part isn’t search; it’s trust: knowing which documents truly need updates, for which audiences, and why. After finding that simple categorical scoring (e.g., relevance 7/10) produced inconsistent, unjustified decisions, the team built a courtroom-style judgment engine: one agent prosecutes (argues to update), another defends (argues to skip), a jury deliberates, and a judge rules—creating a reasoned, auditable trail.

Why it matters:
- Turns LLMs from unreliable “raters” into structured debaters that provide evidence and rationale.
- Handles cross-functional nuance (what’s critical for support may be irrelevant for engineering).
- Scales to tens of thousands of PRs daily for enterprise teams.
- Improves trust and maintainability by coupling automation with explainability, not just findability.

The discussion around Falconer focused on the necessity of its complex architecture, the economics of running multi-agent systems, and philosophical debates regarding LLM "understanding."

**The "Courtroom" vs. Simple Scoring**
Several users questioned whether a complex adversarial system was necessary, suggesting that Occam's Razor favors simpler metrics, binary log probabilities, or standard human review. The authors responded that they initially tried simple 1–10 relevance scoring but found the results inconsistent. They argued that LLMs perform better when tasked with arguing a specific position (prosecution/defense) rather than assigning abstract numerical values to nuanced documentation changes that are rarely strictly "true" or "false."

**Cost, Latency, and the "Funnel"**
Commenters expressed concern about the token costs and latency of running multiple agents (prosecutor, defense, five jurors, judge) for every Pull Request. The creators clarified that the "courtroom" is the final step of a funnel, not the default path:
*   **65%** of PRs are filtered out by simple heuristics before review begins.
*   **95%** of the remaining are decided by single agents (prosecutors deciding whether to file charges).
*   Only **1–2%** of ambiguous cases trigger the full, expensive adversarial pipeline.

**Philosophy vs. Utility**
A segment of the discussion devolved into the "Chinese Room" argument. Skeptics argued that LLMs cannot effectively judge context because they lack human experience and true understanding, merely processing symbols. Pragmatists pushed back, noting that if the system achieves the claimed **83% success rate**, it is practically useful regardless of whether the model possesses philosophical "understanding."

**Other Notes**
*   One user shared their own experiment with a "mediation" framework, noting that while litigation seeks truth/justice, mediation seeks resolution—a subtle but interesting difference in agent goal-setting.
*   The thread contained several humorous "courtroom transcript" parodies involving LLM jailbreaks and the "Chewbacca defense."

