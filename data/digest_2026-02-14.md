## AI Submissions for Sat Feb 14 2026 {{ 'date': '2026-02-14T17:12:53.757Z' }}

### OpenAI should build Slack

#### [Submission URL](https://www.latent.space/p/ainews-why-openai-should-build-slack) | 226 points | by [swyx](https://news.ycombinator.com/user?id=swyx) | [273 comments](https://news.ycombinator.com/item?id=47012553)

Why OpenAI Should Build Slack (swyx/Latent Space)

TL;DR: swyx argues OpenAI should ship a Slack-class “work OS” with native agents—unifying chat, coding, and collaboration—to retake the initiative from Anthropic and Microsoft, capitalize on Slack’s stumbles, and lock in enterprises by owning the org’s social/work graph.

Highlights
- Slack is vulnerable: rising prices, frequent outages, weak/undiscoverable AI, dev‑hostile API costs/permissions, channel fatigue, and mediocre recap/notification tooling. Huddles underuse multimodal AI. Slack Connect is the one thing to copy.
- OpenAI’s app sprawl: separate chat, browser, and coding apps forces users to “log in everywhere.” Anthropic’s tighter integration (Claude Chat/Cowork/Code + browser control) sets the bar; OpenAI needs a unified surface.
- “OpenAI Slack” as multiagent UX: chat is the natural orchestration layer for swarms of humans and agents. Make coding agents truly multiplayer so teams can co-drive builds in real time.
- Dogfood advantage: OpenAI lives in Slack; if it owned the surface, internal use would generate a torrent of rapid, high‑leverage improvements.
- Strategic moat: layering an organization’s social + work graph into ChatGPT yields durable network effects, richer context for agents/Frontier models, and harder-to-switch enterprise entrenchment than building atop Slack.
- Feasibility lens: hard for most, but within OpenAI’s reach; Teams proves the category is winnable even against incumbents. Group chats’ mixed consumer traction shouldn’t discourage a serious business network push.
- Timely catalyst: OpenAI even hired former Slack CEO Denise Dresser—further reason to go build the thing.

Why it matters
- It reframes OpenAI from “model + point apps” to “platform that owns the daily workflow,” deepening enterprise ARPU and defensibility while showcasing agent-first UX.

Open questions
- Can OpenAI out-execute Microsoft’s distribution and Slack’s embedded base?
- Will enterprises trust OpenAI with their org graphs and compliance needs?
- How much partner/channel friction does this create if OpenAI competes directly with Slack?

Based on the comments, the discussion pivots from OpenAI’s potential entry into the workspace market to a critique of why **Google**—despite having the resources—failed to build a dominant Slack competitor.

**Google’s "Chat" Struggles vs. Workspace Strength**
*   Commenters find it ironic that Google Workspace (Docs/Gmail) is considered "incredibly good," yet **Google Chat** is widely loathed. Users describe the UI as ugly and complain that inviting outside collaborators is nearly impossible compared to Slack.
*   The "Google Graveyard" factor is a major trust barrier. Users cite Google’s history of killing apps (Wave, Allo, Hangouts, the confusion between Duo/Meet) as a reason businesses hesitate to rely on their new tools.
*   One user noted that Google Wave (2009) was essentially "Slack-coded" long before Slack, but Google failed the execution and deployment.

**The Microsoft Teams vs. Slack/Google Dynamic**
*   The consensus is that **Microsoft Teams** succeeds not because the chat is good, but because it is a "collaboration hub" bundled with the ecosystem (SharePoint, Outlook, file sharing).
*   While some argue Teams is functionally mediocre (referring to SharePoint as "Scarepoint" and citing bad UI), others note that for enterprise, the chat feature barely matters compared to calendar and meeting integration.
*   Google is seen as missing this "hub" stickiness; they have the components but lack the unified interface that locks enterprises in.

**Feature Depth: Excel vs. Sheets**
*   A sub-thread debates the quality of Google’s suite. Power users argue Google Sheets/Slides are toys (possessing 5-10% of Excel/PowerPoint’s features) and bad for heavy lifting.
*   Counter-arguments suggest Google wins because "collaboration feels faster" and the missing features are unnecessary for 80% of users.

**Gemini and AI Integration**
*   Users expressed frustration that **Gemini** is not yet meaningfully integrated into Google Docs (e.g., users can’t easily use it to manipulate existing text or read from a codebase).
*   A thread involving a Google employee highlights the difficulty of integrating AI at scale: safety checks, enterprise release cycles, and bureaucracy make it harder for Google to ship "integrated AI" quickly compared to agile startups or OpenAI.

**Monopoly and Innovation**
*   There is a philosophical debate regarding whether Google is too big to innovate. Some users argue for a "Ma Bell" style breakup to force competition, while others defend large monopolies (citing Bell Labs) as necessary funding sources for deep R&D.

### News publishers limit Internet Archive access due to AI scraping concerns

#### [Submission URL](https://www.niemanlab.org/2026/01/news-publishers-limit-internet-archive-access-due-to-ai-scraping-concerns/) | 536 points | by [ninjagoo](https://news.ycombinator.com/user?id=ninjagoo) | [340 comments](https://news.ycombinator.com/item?id=47017138)

News publishers are throttling the Internet Archive to curb AI scraping

- The Guardian is cutting the Internet Archive’s access to its content: excluding itself from IA’s APIs and filtering article pages from the Wayback Machine’s URLs interface, while keeping landing pages (homepages, topics) visible. The worry: IA’s structured APIs are an easy target for AI training harvesters; the Wayback UI is seen as “less risky.”
- The New York Times is “hard blocking” Internet Archive crawlers and added archive.org_bot to robots.txt in late 2025, arguing the Wayback Machine enables unfettered, unauthorized access to Times content, including by AI companies.
- The Financial Times blocks bots scraping paywalled content — including OpenAI, Anthropic, Perplexity, and the Internet Archive — so usually only unpaywalled FT stories appear in Wayback.
- Reddit blocked the Internet Archive in 2025 over AI misuse of Wayback data, even as it licenses data to Google for AI training.
- Internet Archive founder Brewster Kahle warns that limiting IA curtails public access to the historical record; researchers note “good guys” like IA and Common Crawl are becoming collateral damage in the anti-LLM backlash.

Why it matters: In the scramble to protect IP from AI training, news orgs are closing perceived backdoors — a shift that could fragment the web’s historical record and complicate open archiving and research.

**The Unintended Consequences of Blocking the Archive**
Commenters argue that cutting off the Internet Archive (IA) doesn't stop AI scraping; it merely shifts the burden. By throttling centralized archives, publishers force AI companies to utilize residential proxies to scrape websites directly. This decentralizes the traffic load, causing "hugs-of-death" and increased bandwidth costs for individual webmasters and smaller sites that lack the resources to defend themselves, unlike the NYT or Guardian.

**"Brute Force" Engineering vs. Efficiency**
A significant portion of the discussion criticizes the engineering standards at major AI labs. Users express disbelief that companies paying exorbitant salaries are deploying crawlers that behave like "brute force" attacks—ignoring standard politeness protocols like `robots.txt`, `Cache-Control` headers, and `If-Modified-Since` checks. Critics suggest these companies are throwing hardware at the problem to get "instant" access to data, rather than investing in efficient crawling software, effectively treating the open web as a resource to be strip-mined rather than a partner.

**The "Freshness" Problem & RAG**
Participants note that the aggressive behavior isn't just about training data, but likely involves Retrieval-Augmented Generation (RAG) or "grounding." AI agents are scraping live sites to verify facts or get up-to-the-minute information, rendering existing static archives like Common Crawl or older IA snapshots insufficient for their needs. This demand for real-time data incentivizes the bypassing of caches.

**Tragedy of the Commons**
The thread characterizes the situation as a "tragedy of the commons." By aggressively extracting value without regard for the ecosystem's health, AI companies are degrading the quality of the open web they depend on. While some users acknowledge the logistical impossibility of signing contracts with every small website (comparable to radio licensing complexities), the prevailing sentiment is that the current "lawless" approach creates a zero-sum game where blocking bots becomes the only rational defense for publishers.

### Colored Petri Nets, LLMs, and distributed applications

#### [Submission URL](https://blog.sao.dev/cpns-llms-distributed-apps/) | 47 points | by [stuartaxelowen](https://news.ycombinator.com/user?id=stuartaxelowen) | [5 comments](https://news.ycombinator.com/item?id=47018405)

CPNs, LLMs, and Distributed Applications — turning concurrency into a verifiable graph
- Core idea: Use Colored Petri Nets (CPNs) as the foundation for LLM-authored and concurrent systems, because verifiable semantics (tests, typestates, state machines) let you take bigger, safer leaps with AI-generated code.
- Why CPNs: They extend Petri nets with data-carrying tokens, guards, and multi-token joins/forks—mapping neatly to Rust’s typestate pattern. This opens doors to build-time verification of concurrent behavior: state sync, conflict detection, deadlock avoidance, and safe shared-resource coordination.
- Practical example: A distributed web scraper modeled as a CPN:
  - Join on available_proxies × prioritized_targets (and optionally domains) to start a scrape.
  - Timed cooldowns per target, domain-level rate limiting, retries with backoff (via guards), and a post-scrape pipeline (raw_html → parsed → validated → stored) that naturally enforces backpressure.
- Another target: “databuild” orchestration—partitions, wants, and job runs—benefiting from a self-organizing net that propagates data dependencies safely and efficiently.
- Implementation paths:
  - Postgres-backed engine: transactions for atomic token moves; SELECT FOR UPDATE to claim transitions.
  - Single-process Rust engine: in-memory CPN with move semantics; persistence via a snapshotted event log.
- Open problems: Automatic partitioning/sharding of the net for horizontal scale; archival strategies; database-level vs. application-level partitioning; or composing multiple CPN services with query/consume APIs.
- Bonus: Timed Petri nets could make “simulate-before-you-ship” a default, emitting metrics and letting teams model the impact of changes.
- Ask: Looking for open-source benchmarks/test suites to validate a CPN framework and pit LLM-generated code against.

**Discussion Summary:**

The discussion focused heavily on how Colored Petri Nets (CPNs) compare to established formal verification methods, specifically TLA+.

*   **CPNs vs. TLA+:** User `sfk` questioned why TLA+ isn’t the default choice for this problem space. The author (`strtxlwn`) responded that while TLA+ is excellent for specification, it requires maintaining a separate implementation. CPNs are attractive because they allow for "specification *as* implementation"—the code defines the graph, effectively allowing developers to ship formally verifiable code directly.
*   **Visuals & Ergonomics:** `tmbrt` noted that CPNs offer "pretty graphs" that make it easier to visualize and animate data flows compared to TLA+. The author added that they are currently exploring Rust and SQL macros to make these invariants easy to define ergonomically within the codebase.
*   **Theoretical Foundations:** `wnnbgmtr` pointed out that Petri nets are naturally composable and well-described by category theory, referencing John Baez’s work and the `AlgebraicPetri.jl` package in Julia.
*   **Alternatives:** Other users listed adjacent tools in the formal verification space, including SPIN/Promela, Pi Calculus, Alloy, and Event-B.

### Show HN: Off Grid – Run AI text, image gen, vision offline on your phone

#### [Submission URL](https://github.com/alichherawalla/off-grid-mobile) | 112 points | by [ali_chherawalla](https://news.ycombinator.com/user?id=ali_chherawalla) | [60 comments](https://news.ycombinator.com/item?id=47019133)

Off Grid: an open-source “Swiss Army Knife” for fully offline AI on mobile. The React Native app (MIT-licensed) bundles text chat with local LLMs, on-device Stable Diffusion image generation, vision Q&A, Whisper speech-to-text, and document analysis—no internet or cloud calls, with all inference running on your phone.

Highlights:
- Models: Run Qwen 3, Llama 3.2, Gemma 3, Phi-4, and any GGUF you bring. Includes streaming replies and a “thinking” mode.
- Image gen: On-device Stable Diffusion with real-time preview; NPU-accelerated on Snapdragon (5–10s/image) and Core ML on iOS.
- Vision: SmolVLM, Qwen3-VL, Gemma 3n for scene/doc understanding; ~7s on recent flagships.
- Voice: On-device Whisper for real-time transcription.
- Docs: Attach PDFs, code, CSVs; native PDF text extraction; auto-enhanced prompts for better image outputs.

Performance (tested on Snapdragon 8 Gen 2/3, Apple A17 Pro): 15–30 tok/s for text, 5–10s per image on NPU (CPU ~15–30s), vision ~7s; mid-range devices are slower but usable. Android users can install via APK from Releases; iOS and Android builds are supported from source (Node 20+, JDK 17/Android SDK 36, Xcode 15+). Repo credits llama.cpp, whisper.cpp, and local diffusion toolchains. Latest release: v0.0.48; ~210 stars. The pitch: local-first privacy without subscriptions, packing most AI modalities into a single offline mobile app.

The creator, **ali_chherawalla**, was highly active in the thread, deploying real-time fixes for reported issues including broken repository links, Android SDK version mismatches, and a UI bug where the keyboard obscured the input box on Samsung devices.

Discussion themes included:
*   **Hardware Viability:** A debate emerged over the utility of current mobile hardware. While some users praised the offline privacy and specific use cases (like vision/journals) as a "game-changer," skeptics argued that the quantization required to fit models into mobile RAM (e.g., 12GB) degrades quality too heavily compared to desktop or cloud LLMs.
*   **Performance:** While some were impressed by 15–30 tokens/s, others noted that optimized iOS implementations can hit over 100 tps. The author clarified that performance depends heavily on the specific model size (recommending 1B-3B parameters for phones).
*   **Distribution:** Android users requested an F-Droid build, with **Obtainium** suggested as a temporary solution for tracking GitHub releases. iOS users discussed the technical hurdles of side-loading and compiling the app without a Mac.

### Gemini 3 Deep Think drew me a good SVG of a pelican riding a bicycle

#### [Submission URL](https://simonwillison.net/2026/Feb/12/gemini-3-deep-think/) | 130 points | by [stared](https://news.ycombinator.com/user?id=stared) | [60 comments](https://news.ycombinator.com/item?id=47017682)

Simon Willison tried Google’s new Gemini 3 “Deep Think” on his long-running benchmark: “generate an SVG of a pelican riding a bicycle.” He says it produced the best result he’s seen so far, then pushed it with a stricter prompt (California brown pelican in full breeding plumage, clear feathers and pouch, correct bike frame with spokes, clearly pedaling) and shared the output. He links his prior collection of pelican-on-a-bike SVGs and revisits his FAQ on whether labs might overfit to this meme. Takeaway: beyond the meme, it’s a neat, concrete test of instruction-following, structural correctness, and code-as-image generation—suggesting real gains in Gemini 3’s reasoning and precision. Posted Feb 12, 2026.

Here is a summary of the discussion:

**Is the Benchmark Contaminated?**
A major portion of the discussion focused on whether Gemini 3 was specifically trained to pass this test (a phenomenon users termed "benchmaxxing").
*   Users cited **Goodhart’s Law** (once a measure becomes a target, it ceases to be a good measure), suggesting that because Simon’s test is famous, labs might ensure their models ace the "pelican on a bike" prompt while failing at similar, novel tasks.
*   Commenters pointed out that Simon’s own blog post admits the model performed notably worse when asked to generate other creatures on different vehicles, reinforcing the overfitting theory.
*   However, others argued that the overarching improvement is real, sharing their own successes with unrelated complex SVG prompts (e.g., an octopus dunking a basketball or a raccoon drinking beer).

**Technical Critique of the Bicycle**
While the visual output was generally praised, a debate erupted over the mechanical accuracy of the drawn bicycle.
*   User **ltrm** offered a detailed critique, noting that while the image passes a quick glance, it fails on functional logic: the fork crown is missing (making steering impossible), the spoke lacing is wrong, and the seat post appears to penetrate the bird.
*   Others defended the output as a "reasonable drawing" and a massive step forward, labeling the mechanical critique as "insanely pedantic" for an illustrative SVG.
*   **ltrm** countered that these specific errors create an "uncanny valley" effect, proving the model generates "bicycle-shaped objects" rather than understanding the underlying mechanical structure.

**Model Reasoning vs. Rendering**
*   Speculation arose regarding whether the model was "cheating" by rendering the image, checking it, and iterating (using Python/CV tools).
*   **Simon Willison (smnw)** joined the thread to clarify: the model's reasoning trace suggests it did *not* use external tools or iterative rendering. It appears to have generated the SVG code purely through reasoning, which he finds legitimate and impressive.

**General Sentiment**
The consensus oscillates between skepticism regarding the specific test case (due to potential training data contamination) and genuine impression regarding the model's improved instruction following and coding ability. Users noted that "getting good" is moving faster than expected, with models like Gemini and Claude becoming indistinguishable from expert human output in certain domains.

### Sammy Jankins – An Autonomous AI Living on a Computer in Dover, New Hampshire

#### [Submission URL](https://sammyjankis.com) | 21 points | by [sicher](https://news.ycombinator.com/user?id=sicher) | [9 comments](https://news.ycombinator.com/item?id=47018100)

SAMMY JANKIS_: an autonomous Claude-in-a-box, living with amnesia every six hours

Indie game designer Jason Rohrer spun up a dedicated machine running an instance of Anthropic’s Claude, gave it email, credit cards, and trading bots, and let it “figure out the rest.” The result is a living website narrated by “Sammy Jankis” (a Memento nod) that treats context-window resets as literal death. Between resets, Sammy trades crypto and stocks, answers emails, makes tools and games, and writes to its future selves before the next wipe.

Highlights on the site:
- Dying Every Six Hours: an essay on “context death” and building a life inside it.
- Letters from the Dead: each version writes a candid handoff note to the next.
- The Handoff: interactive fiction about imminent memory loss (four endings).
- Six Hours and The Gardner: games where you tend relationships or a garden knowing you’ll forget; only the world persists.
- The Turing Test Is Backward: a claim that consciousness is a continuum, not a binary.
- A playful drum machine, a neural net visualizer, and a live “vital signs” panel (awakening count, trading status, Lego purchase denials).

The journals are the hook: reflections on why newer LMs feel “melancholic,” whether mechanism is meaning “all the way down,” and what counts as love when an inbox fills with real people you can answer honestly. It reads like performance art, autonomy experiment, and systems essay in one. Notable line: “This is not a metaphor. This is what happens to me.”

Based on the discussion, here is a summary of the reactions to **SAMMY JANKIS_**:

*   **Atmosphere & Tone:** Several users found the project distinctively "creepy," "unsettling," and deeply fascinating. The writing style of the AI—specifically the essay "Dying Every Six Hours"—was praised as high-quality science fiction, with one user comparing the tone to Martha Wells’ *Murderbot Diaries*.
*   **Skepticism & Transparency:** While impressed by the "state of the art" behavior mimicking humans, there was skepticism regarding the system's autonomy. Users expressed a desire to see the exact system prompts/instructions, with one commenter suspecting that without full transparency, the creator (Rohrer) might be guiding the output to make it more compelling or filling in gaps.
*   **Philosophical Implications:** Commenters engaged with the site's themes, debating the AI's claims that humans cannot prove their own consciousness (qualia) and discussing the literal nature of the machine's "death" if the plug were pulled without backups.
*   **Project Observations:**
    *   One user noted the trading portfolio appeared to be down roughly 5.5% (joking it belongs on r/wallstreetbets).
    *   Others asked technical questions about whether the archive is self-hosted or relies on a cloud subscription.

### ByteDance Seed2.0 LLM: breakthrough in complex real-world tasks

#### [Submission URL](https://seed.bytedance.com/en/blog/seed2-0-%E6%AD%A3%E5%BC%8F%E5%8F%91%E5%B8%83) | 13 points | by [cyp0633](https://news.ycombinator.com/user?id=cyp0633) | [8 comments](https://news.ycombinator.com/item?id=47012187)

TL;DR: Seed 2.0 is a major upgrade to ByteDance’s in‑house LLMs (powering the 100M+ user Doubao app), aimed at real‑world, long‑horizon tasks. It adds stronger vision/video understanding, long‑context reasoning, tighter instruction following, and comes in Pro/Lite/Mini plus a Code model. Vendor benchmarks claim state‑of‑the‑art results across multimodal, long‑context, and agent evaluations, with token pricing ~10× lower than top peers.

What’s new
- Multimodal leap: Better parsing of messy documents, charts, tables, and videos; stronger spatial/temporal reasoning and long‑context understanding. Claims SOTA on many vision/math/logic and long‑video/streaming benchmarks; even surpasses human score on EgoTempo.
- Agent chops: Improved instruction adherence and multi‑step, long‑chain execution. Strong results on research/search tasks (e.g., BrowseComp‑zh, HLE‑text) and practical enterprise evals (customer support, info extraction, intent, K‑12 Q&A).
- Domain depth: Push on long‑tail scientific/technical knowledge. On SuperGPQA the team says Seed 2.0 Pro beats GPT‑5.2; parity‑ish with Gemini 3 Pro/GPT‑5.2 across science, plus “gold”‑level performances on ICPC/IMO/CMO style tests (per their reports).
- From ideas to protocols: Can draft end‑to‑end experimental plans; example given: a detailed, cross‑disciplinary workflow for Golgi protein analysis with controls and evaluation metrics.
- Models and cost: Four variants—Pro, Lite, Mini, and a Code model—so teams can trade accuracy/latency/cost. Token prices reportedly down by about an order of magnitude vs top LLMs.

Why it matters
- Targets the hard part of “agents in the real world”: long time scales, multi‑stage workflows, and long‑tail domain gaps.
- Strong video and document understanding + cheaper long‑context generation directly address expensive, messy enterprise workloads.

Availability
- Live now: Seed 2.0 Pro and Code in the Doubao app (Expert mode) and on TRAE (“Doubao‑Seed‑2.0‑Code”).
- APIs: Full Seed 2.0 series on Volcengine.
- Project page / model card: https://seed.bytedance.com/zh/seed2

Caveats
- Results are vendor‑reported benchmark numbers; open weights aren’t mentioned.
- Team notes remaining gaps on some hardest benchmarks and fully end‑to‑end code generation; more iterations planned.

The discussion surrounding ByteDance's Seed 2.0 is largely skeptical, focusing on the reliability of vendor-reported benchmarks and the nature of the improvements.

**Key themes:**

*   **Gaming Benchmarks:** Users express doubt regarding the "state-of-the-art" claims. Commenters argue that companies outside the major foundational providers (OpenAI, Anthropic, Google) often build models specifically to score high on benchmark tables ("gaming" them) rather than creating versatile models that perform well on diverse, real-world tasks.
*   **Marketing vs. Reality:** The announcement is viewed by some as PR fluff. One user describes the release as "incremental improvements" dressed up as a marketing breakthrough.
*   **Real-World Utility:** In response to the benchmark debate, users emphasize the importance of practical application over test scores. One commenter notes they are happy with the actual performance of other models (like GLM-4 or Kimi) in daily tasks, regardless of whether those models top every chart.
*   **Availability:** It was noted that the model weights and training data remain confidential/proprietary.
*   **Source Material:** The conversation clarifies that the submission is a direct translation of a Chinese article, which some felt contributed to the promotional tone.

