## AI Submissions for Sun Dec 28 2025 {{ 'date': '2025-12-28T17:09:50.466Z' }}

### Designing Predictable LLM-Verifier Systems for Formal Method Guarantee

#### [Submission URL](https://arxiv.org/abs/2512.02080) | 58 points | by [PaulHoule](https://news.ycombinator.com/user?id=PaulHoule) | [11 comments](https://news.ycombinator.com/item?id=46411539)

The 4/δ Bound: a provable stopwatch for LLM + formal verifier loops

- What’s new: The authors model an LLM-assisted verification pipeline as a sequential absorbing Markov chain with four stages—CodeGen → Compilation → InvariantSynth → SMTSolving—and prove two big claims:
  1) If every stage has a non-zero chance of success (δ > 0), the system reaches Verified almost surely (no infinite loops).
  2) The expected number of iterations to verification is tightly bounded by 4/δ.

- Why it matters: Today’s LLM-verifier “refine until it works” loops can oscillate or stall, making resource planning guessy. This paper replaces heuristics with a formal convergence theorem and a latency bound you can budget against—useful for safety-critical software, CI stability, and cost forecasting.

- How it works:
  - Models the pipeline as a sequential absorbing Markov chain with Verified as the absorbing state.
  - Uses the minimum per-stage success probability δ to derive termination and the latency bound E[n] ≤ 4/δ.
  - Because the pipeline is strictly sequential, the constant “4” comes from the four stages; the framework generalizes conceptually with more stages.

- Evidence: ~90,000 trials stress-test the theory. Every run verified, and the empirical convergence factor clustered around 1.0, suggesting the 4/δ bound tracks reality closely rather than serving as a loose upper bound.

- Engineering takeaways:
  - Predictable budgeting: Given an estimate of δ, you can set timeouts, GPU/CPU quotas, and CI limits. Example: if δ ≈ 5%, expect ≤ 80 iterations on average.
  - Focus the fix: The weakest stage (lowest success probability) dominates δ; improving that stage yields the biggest win in end-to-end latency.
  - Operations modes: They identify marginal, practical, and high-performance zones and propose dynamic calibration to adapt δ as conditions drift (model updates, dataset shifts, toolchain changes).

- Open questions to watch:
  - Estimating δ robustly online and per-domain.
  - Handling non-sequential or branching pipelines, retries with memory, or correlated failures.
  - How the bound behaves with adversarial specs, highly sparse invariants, or flaky SMT/toolchains.

Authors: Pierre Dantas, Lucas Cordeiro, Youcheng Sun, Waldir Junior. Subjects: AI, formal methods, ML, and software engineering.

**Discussion Summary:**

Technical discussion focused on the paper’s methodology and the practical implications of its mathematical claims.

*   **Critique of Methodology:** Some users suspected the paper did not conduct experiments with actual LLMs writing code. Instead, commenters argued the experiments likely consisted of simulating the simplified Markov chain model itself. This led to criticism that the paper merely validates standard probability theory rather than real-world LLM behavior, with one user calling the mathematical content "thin" and the title "LLM-Verifier Convergence Theorem" grandiose for what amounts to a standard 5-state Markov chain proof.
*   **"Almost Surely" vs. Reality:** A significant portion of the thread debated the definition and utility of the term "almost surely" (probability = 1). While the paper claims the system reaches a verified state "almost surely," users pointed out that this guarantees success over infinite time, which is distinct from failing in a finite context.
*   **Practical Constraints:** Commenters noted that "eventually" reaching a halting state is less useful if the convergence takes an impractical amount of time (e.g., a century), drawing comparisons to P vs NP limitations. Others noted that assuming a strictly non-zero probability for success ($P > 0$) in the real world is a heavy assumption, and even statistically rare events (like hash collisions) remain relevant engineering concerns.

### CEOs are hugely expensive. Why not automate them? (2021)

#### [Submission URL](https://www.newstatesman.com/business/companies/2023/05/ceos-salaries-expensive-automate-robots) | 230 points | by [nis0s](https://news.ycombinator.com/user?id=nis0s) | [276 comments](https://news.ycombinator.com/item?id=46415488)

Executive pay is back in the spotlight — and so is the question: do firms even need a CEO?

- With AGM season underway, boards at BAE Systems, AstraZeneca, Glencore, Flutter, and the LSE face potential shareholder revolts over pay. The timing stings: many firms were propped up by government stimulus during Covid, yet exec rewards kept flowing.
- Example: 40% of Foxtons shareholders voted against a near-£1m bonus for the CEO while the company took ~£7m in state support. Meanwhile, Ocado’s Tim Steiner made £58.7m in 2019 — 2,605× his median employee — and the average FTSE 100 CEO clears £15k per day.
- The High Pay Centre argues companies could protect jobs by trimming compensation among the highest earners, not just the CEO.
- A viral thread from tech CEO Christine Carrillo (crediting her Philippines-based EA with handling “most” of her CEO tasks and saving 60% of her time) raises a harsher question: if much of a CEO’s work can be outsourced, could it be automated?
- Past AI misfires (Microsoft’s automated news curation, Amazon’s biased recruiting tool, a GPT-3 medical chatbot’s unsafe response) show risks in low-oversight tasks. But proponents argue top-level strategy might be a better candidate: decisions are debated, human bias is costly, and software could enforce more rational trade-offs.
- The provocation for boards and investors: if automation is inevitable, should it start at the top? Or is leadership’s value precisely the un-automatable judgment we’re paying for?

**Discussion Summary:**

While the article questioned the necessity of high-paid CEOs broadly, the discussion immediately zeroed in on **Elon Musk** as a polarizing case study for executive value, sparking debates on leadership theory, compensation, and the definition of the role.

*   **The "Tweet vs. Work" Paradox:** Users debated whether Musk's ability to serve as CEO for multiple companies (Tesla, SpaceX, xAI, Twitter) while tweeting incessantly proves the CEO role is less rigorous than claimed. Skeptics argued that if strategic decisions can be automated or widely delegated, exorbitant pay packages are unjustified.
*   **Vision vs. Operations (COO efficacy):** A central thread argued that Musk's companies succeed because of strong COOs (specifically citing Gwynne Shotwell at SpaceX) running day-to-day operations. This led to a semantic debate:
    *   Some argued the CEO’s true value is setting a "grand vision" and maximizing valuation (stock price) rather than operations.
    *   Others countered that "vision" is the Board's responsibility, and a Chief *Executive* Officer's job should be executing that mandate, not just being a figurehead.
*   **The "Great Man" Theory vs. Technological Inevitability:** A lengthy tangent explored whether leaders like Musk and Steve Jobs actually force innovation or simply capitalize on technology that is ready to emerge (e.g., reusing detailed arguments about NASA’s prior work on VTOL/reusable rockets vs. SpaceX’s engineering). One user described Musk’s utility as "bad-person strategy," where over-promising (or lying about) timelines forces organizational breakthroughs that nice leaders wouldn't achieve.
*   **Talent Magnetism:** Participants disagreed on whether top engineering talent joins Musk's companies *because* of him or *in spite* of him. While some claimed his personal brand attracts money and talent, others argued that engineers are drawn to the specific industry challenges (rockets, EVs) and lack comparable prestigious options, tolerating the leadership rather than seeking it.

### 'PromptQuest' is the worst game of 2025 (trying to make chatbots work)

#### [Submission URL](https://www.theregister.com/2025/12/26/ai_is_like_adventure_games/) | 40 points | by [dijksterhuis](https://news.ycombinator.com/user?id=dijksterhuis) | [26 comments](https://news.ycombinator.com/item?id=46411040)

- The Register’s Simon Sharwood riffs on Microsoft open-sourcing Zork to argue that using modern chatbots—especially Copilot—often feels like playing a “guess-the-verb” text adventure: you keep trying “Hit/Kill/Stab Goblin” until syntax lucks out.
- His gripe isn’t just mistakes, but inconsistency and opacity: identical prompts yield different outputs across days, Copilot in Office vs desktop behaves differently, and silent model swaps break previously reliable prompts.
- Anecdote: asking Copilot to fetch online data and deliver a downloadable spreadsheet yielded a Python script, repeated false assurances that the job was done, and even a generated “progress bar”—but never an actual file.
- Bottom line: this “PromptQuest” turns work into cave-crawling in the dark while being sold as productivity. The implicit ask: stable versions, visible model changes, deterministic modes, and reliable artifact delivery.

The discussion around *The Register’s* critique of "PromptQuest" is polarized, splitting between those who view the author's struggles as user error (a "skill issue") and those who agree that non-deterministic tools are fundamentally flawed for reliable work.

**User Error vs. Tool limitations**
*   **"Old Man Yelling at Cloud":** Several commenters dismissed the article as typical *Register* cynicism or a "skill issue," arguing that the author is treating a raw text engine like a finished product. They suggest that 90% of LLM complaints stem from users refusing to learn how to properly guide the model.
*   **The Problem with Indeterminism:** Supporters of the article countered that the criticism is valid because LLMs are not just difficult, but inconsistent. One user noted the "flavors of non-determinism"—from varying outputs based on the input channel (API vs. Chat) to statistical hallucinations—creating a "cursed middle" where answers are plausible but hard to verify.
*   **The "Zork" Analogy:** Users expanded on the text adventure comparison. Unlike tools like Photoshop or VS Code which present finite menus of valid options, LLMs lack a UI for capabilities, forcing users into "open-ended exploration" where they must guess what the software can actually do.

**Management and Workflow**
*   **Forced Productivity:** A specific frustration emerged regarding management forcing AI adoption. Commenters noted that "LLM managers" are now blaming humans for failing to meet "alleged productivity results" calculated by the very AI that isn't working properly.
*   **Gemini vs. Copilot:** While the article complained that Copilot *wouldn't* generate files, a commenter noted the opposite problem with Google Gemini, which annoyingly insists on inserting itself into Google Workspace spreadsheets even when explicitly told not to.

**Cultural Observations**
*   **"Vibe Coding":** There was marked skepticism regarding the current hype cycle ("vibe coding"), with users calling it a delusion of VCs who hope to replace engineers with service-endpoint query tools.
*   **The "Yes Man":** One commenter drew a parallel between ChatGPT’s refusal to acknowledge failure and the generic, cheery demeanor of the "Yes Man" robot from *Fallout: New Vegas*—a helper that sounds polite while being functionally useless or dangerous.

