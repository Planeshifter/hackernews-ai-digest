## AI Submissions for Sat Sep 06 2025 {{ 'date': '2025-09-06T17:12:49.610Z' }}

### AI surveillance should be banned while there is still time

#### [Submission URL](https://gabrielweinberg.com/p/ai-surveillance-should-be-banned) | 560 points | by [mustaphah](https://news.ycombinator.com/user?id=mustaphah) | [208 comments](https://news.ycombinator.com/item?id=45149281)

TL;DR: Weinberg argues chatbot data collection supercharges the privacy harms of web tracking and opens the door to personalized manipulation. He calls for a federal ban on AI surveillance and for “protected chats” to be the default.

Key points:
- Why AI is worse than search: Chatbot conversations are longer, more intimate, and reveal thought processes and communication styles—fuel for highly tailored persuasion, both commercial and political. Memory features can fine‑tune models on your triggers, making influence subtler than ads.
- Rising risks, recent examples: He cites reports of Grok leaking hundreds of thousands of “private” chats, a Perplexity agent vulnerability exposing personal data, OpenAI’s vision for a “super assistant” that tracks everything (even offline), and Anthropic shifting to train on user chats by default.
- Feasibility: DuckDuckGo launched Duck.ai for protected chats and anonymous AI-assisted answers to show privacy-respecting AI is possible.
- Policy ask: Congress should move fast on AI-specific privacy legislation to make protected chats standard and ban AI surveillance. The U.S. still lacks a general online privacy law; every day without rules entrenches bad practices.
- Bottom line: Stop AI tracking before it repeats the worst of web surveillance; in the meantime, privacy-first AI services can mitigate harm.

**Summary of Discussion:**

The discussion revolves around concerns over AI's role in surveillance, moderation, and decision-making, highlighting risks and real-world examples. Key points include:

1. **AI in Healthcare & Insurance:**
   - Users criticize AI's undemocratic influence in critical areas, notably health insurance. Examples include Cigna allegedly using AI to deny claims automatically, leading to prolonged disputes and unapproved treatments. This raises ethical concerns about profit-driven AI decisions affecting patient care.

2. **Platform Moderation Issues:**
   - Reddit’s AI moderation tools are lambasted for low-quality enforcement, including unjust bans (e.g., users banned for scripting in *Dwarf Fortress* or criticizing traffic policies). Critics argue AI lacks contextual understanding, leading to arbitrary censorship and eroded trust.
   - Subreddits like AITA are flooded with AI-generated posts, creating spam and degrading content quality. Users fear platforms prioritize engagement metrics over genuine interaction.

3. **Broader Societal Impact:**
   - Comparisons are drawn to Facebook’s unchecked growth, with warnings that AI surveillance could replicate or worsen existing privacy harms. Concerns extend to corporate control over essential services (housing, healthcare) via AI, risking democratic accountability.
   - Literacy and critical thinking declines are cited as enabling AI manipulation, with calls for education and regulation to counter disinformation.

4. **Calls for Regulation:**
   - Participants echo the article’s demand for federal bans on AI surveillance and stricter privacy laws. Examples like DuckDuckGo’s privacy-focused Duck.ai demonstrate viable alternatives, emphasizing the need for legislative action before harmful practices become entrenched.

5. **Platform Decline & Alternatives:**
   - Reddit’s perceived decline, driven by bot traffic and advertiser reliance, sparks debate about the future of social media. Some users advocate migrating to decentralized platforms, while others lament the loss of community-driven content moderation.

**Conclusion:** The discussion underscores a pervasive distrust in AI’s ethical deployment, urging immediate regulatory intervention to prevent corporate overreach and protect user autonomy. Parallels to historical tech failures (e.g., Facebook’s privacy issues) highlight the urgency of addressing AI’s societal risks proactively.

### Qwen3 30B A3B Hits 13 token/s on 4xRaspberry Pi 5

#### [Submission URL](https://github.com/b4rtaz/distributed-llama/discussions/255) | 333 points | by [b4rtazz](https://news.ycombinator.com/user?id=b4rtazz) | [148 comments](https://news.ycombinator.com/item?id=45148237)

A neat result from the distributed-llama project: the maintainer ran Qwen3 30B (Mixture-of-Experts) with A3B Q40 quantization across four Raspberry Pi 5 (8GB) boards and got desktop-ish inference speeds—without a GPU.

Highlights
- Hardware: 4× Raspberry Pi 5 (8GB) on a TP-Link LS1008G 1 GbE switch; one root + three workers
- Model: Qwen3 30B MoE, A3B Q40 quant (nExperts=128, nActiveExperts=8)
- Software: distributed-llama v0.16.0
- Throughput: 14.33 tok/s (eval), 13.04 tok/s (generation) over 109 tokens
- Memory: ~5.5 GB required per node; fits in 8 GB
- CPU path: NEON dotprod FP16; network in non-blocking mode
- Networking: ~0.6 MB sent and ~1.1 MB received per token; ~14 MB/s at 13 tok/s—well within 1 GbE

Why it’s interesting
- Demonstrates a 30B-class MoE model running interactively on sub-$400 of commodity ARM SBCs.
- MoE + quantization make the footprint small enough to fit on 8 GB devices, while distributed-llama handles sharding/communication efficiently.
- Shows that a simple unmanaged switch can handle the per-token traffic with headroom.

Caveats
- A tokenizer/model vocab mismatch warning appeared but didn’t block inference.
- Sample output quality was iffy (a geography slip), reminding that quantization/MoE/training all impact accuracy.

Link: b4rtaz/distributed-llama (v0.16.0) GitHub issue “Qwen3 30B A3B Q40 on 4× Raspberry Pi 5 8GB”

The Hacker News discussion revolves around running large language models (LLMs) locally on consumer hardware, inspired by a post demonstrating a 30B MoE model on a Raspberry Pi 5 cluster. Key themes include:

### **Hardware Comparisons**
- **Apple Silicon (M1/M3/M4 Macs)**: Praised for high memory bandwidth (e.g., M1 Max at 400 GB/s) and efficiency, with macOS frameworks like MLX enabling optimized performance. Users highlight refurbished Macs as cost-effective options for local AI inference.
- **AMD Ryzen AI Max+ and Strix Halo**: AMD’s upcoming APUs (e.g., Ryzen AI 395 Strix Halo with 128GB shared RAM and 256-bit LPDDR5X) are seen as competitive for integrated graphics and memory bandwidth (~200+ GB/s), but still lag behind Apple’s unified memory architecture.
- **Discrete GPUs**: Criticized for high cost (Nvidia’s pricing strategies) and limited VRAM (e.g., 16GB cards). Energy consumption and context-window limitations (e.g., handling 1M tokens) are noted as drawbacks.
- **Raspberry Pi 5 Cluster**: Highlighted as a novel, budget-friendly approach ($400 for 4 nodes) for smaller models, though constrained by network latency and ARM compatibility.

### **Technical Challenges**
- **Memory Bandwidth vs. VRAM**: Apple’s unified memory (e.g., M4 Ultra at 800 GB/s) outperforms AMD/Nvidia in bandwidth-critical tasks, while discrete GPUs suffer from "memory starvation" for large models.
- **Quantization and Model Parallelism**: Discussed as necessary to fit models into limited memory (e.g., Qwen3 30B MoE quantized to 5.5GB/node). Distributed computing (e.g., Kubernetes clusters) is suggested for scaling.
- **Hallucinations**: Users debate fixes like filtering outputs, RAG (Retrieval-Augmented Generation), and uncertainty-aware models. One user links to a post advocating for "admitting uncertainty" in LLMs to reduce errors.

### **Broader Implications**
- **Privacy and Local AI**: Some argue local processing is critical for privacy, pushing demand for consumer hardware capable of running LLMs without cloud dependency.
- **Cost vs. Performance**: Raspberry Pi setups are seen as viable for hobbyists, but professionals lean towards Apple/AMD hardware for larger models. Nvidia’s dominance is questioned due to pricing and proprietary ecosystems.
- **Future Outlook**: Skepticism about "AI PC" marketing (e.g., Microsoft’s Copilot+) vs. actual user demand. Some predict Apple’s continued leadership in consumer AI hardware, while others advocate for open-source Linux solutions.

### **Notable Quotes**
- *“Macs are half the price of AI workstations with similar specs.”*  
- *“The Raspberry Pi cluster shows you don’t need $10k GPUs to run decent models interactively.”*  
- *“Fixing hallucinations isn’t about eliminating uncertainty—it’s about teaching models to admit when they’re unsure.”*  

### **Conclusion**
The thread reflects enthusiasm for democratizing LLM inference through affordable hardware and distributed computing, tempered by technical hurdles (memory, quantization) and debates over optimal platforms. Raspberry Pi clusters symbolize accessibility, while Apple and AMD chips represent the high end for serious users prioritizing speed and scalability.

### A Software Development Methodology for Disciplined LLM Collaboration

#### [Submission URL](https://github.com/Varietyz/Disciplined-AI-Software-Development) | 91 points | by [jay-baleine](https://news.ycombinator.com/user?id=jay-baleine) | [38 comments](https://news.ycombinator.com/item?id=45148180)

Disciplined AI Software Development is a practical methodology for treating AI like a focused collaborator, not a one-shot code generator. It tackles recurring pain points—code bloat, repeated logic, architectural drift, and context loss—by forcing small, validated steps and data-driven decisions.

Highlights:
- Four-stage workflow: 
  1) Configure the AI via AI-PREFERENCES.md (with explicit uncertainty flagging),
  2) Co-plan with METHODOLOGY.md (scope, dependencies, phases, measurable tasks),
  3) Implement one component per request with strict ≤150-line files and validate/benchmark each step,
  4) Iterate using performance data instead of guesswork.
- Build the benchmarking suite first (Phase 0) so optimization is guided by measurements throughout.
- Systematic constraints: architectural checkpoints, dependency gates, file-size limits, and duplication audits enforce consistency and prevent drift.
- Tooling: a project extraction script produces structured snapshots for reviews and sharing.
- Example applications show the method at scale: a production-ready Discord bot template (<150 lines/file), a multi-module language runtime (PhiCode), and a Go-based CI/CD regression detector (PhiPipe).
- Licensed CC BY-SA 4.0. Repo: Varietyz/Disciplined-AI-Software-Development (banes-lab.com).

The Hacker News discussion around the "Disciplined AI Software Development" methodology highlights a mix of cautious optimism and practical challenges faced when integrating AI into software workflows. Here's a concise summary:

### Key Themes:
1. **Context and Specificity Struggles**:
   - Users like **CuriouslyC** and **MndlshnDscpl** shared frustrations with AI tools (e.g., Claude, Gemini) struggling to maintain context or follow detailed specifications, leading to wasted time correcting outputs. Even explicit instructions in files like `Claudemd` were sometimes ignored, highlighting reliability issues.
   - **rpnd** noted difficulties with technical nuances (e.g., PostgreSQL `JSONB` vs. SQL `NULL` types), emphasizing the need for rigorous testing to catch such edge cases.

2. **Documentation Overhead**:
   - While structured specs (e.g., `AI-PREFERENCES.md`) are advocated, participants debated the practicality of creating hyper-detailed documentation. **nbckng** and **grrk** highlighted organizational challenges, noting that even thorough documentation is often ignored unless supported by cultural shifts toward collaboration and review.

3. **Human Oversight Remains Critical**:
   - Skepticism emerged around full automation. **mhdbl** stressed that AI-generated code requires human review to prevent drift, bias, and errors. Others echoed that AI should augment—not replace—developers, particularly for complex tasks.

4. **Methodology Benefits**:
   - Modular workflows, phased planning, and validation checkpoints were praised for breaking down tasks (e.g., Discord bot in <150-line files) and improving accountability. Users like **jemiluv8** found value in using AI for smaller components or brainstorming but warned against over-reliance.

5. **Cultural and Historical Parallels**:
   - Comparisons to COBOL-era role separation (analysts vs. programmers) and Agile’s shortcomings surfaced, with **rscr** and **prryg** noting that process maturity—not just tools—drives success. The discussion underscored empathy and communication as irreplaceable human skills.

### Notable Tools & Practices:
- **Tooling**: `Claudemd` files, benchmarking suites, and extraction scripts for project snapshots.
- **Debated Strategies**: AI-driven code reviews vs. human-led validation, iterative planning, and "Phase 0" benchmarking.
- **Sentiment**: Users acknowledged AI’s potential but emphasized disciplined workflows, mitigation of context loss, and human accountability as non-negotiable for credible outcomes.

### GLM 4.5 with Claude Code

#### [Submission URL](https://docs.z.ai/guides/llm/glm-4.5) | 199 points | by [vincirufus](https://news.ycombinator.com/user?id=vincirufus) | [81 comments](https://news.ycombinator.com/item?id=45145457)

Zhipu’s GLM-4.5 series pitches an “agent-first” upgrade to its GLM family, centering on reasoning, coding, and reliable tool use at unusually low costs.

- Models: Flagship GLM-4.5 (Mixture-of-Experts, 355B total params, 32B active) and GLM-4.5-Air (106B total, 12B active), plus faster/lighter X, AirX, and a free Flash tier.
- Context and modes: 128K context window, up to 96K output tokens, and a toggleable “Thinking Mode” for deeper reasoning vs instant replies.
- Agent features: Built-in function calling, web/tool invocation, streaming outputs, structured JSON, and context caching—aimed at coding agents and general tool-using assistants.
- Training and focus: Pretrained on ~15T tokens, then fine-tuned for code/reasoning/agent tasks; optimized for software engineering and front-end workflows.
- Performance claims: Docs say GLM-4.5 ranks near the top across 12 benchmarks (e.g., MMLU Pro, AIME24, SWE-Bench, GPQA), with strong parameter efficiency; Air reportedly beats larger rivals on some reasoning tests.
- Real-world eval: In 52 multi-turn coding tasks run in containers, the team reports higher tool-invocation reliability and task completion than open-source peers, and “largely comparable” behavior to Claude 4 Sonnet (with room to improve).
- Cost and speed: API pricing as low as $0.20/M input tokens and $1.10/M output tokens; high-speed variants claim >100 tokens/sec, targeting low-latency, high-concurrency use.

Why it matters: If these claims hold up outside the vendor’s tests, GLM-4.5 could pressure incumbents on price/performance for agentic coding and tool-use workloads, especially where long context and structured outputs are critical.

**Summary of Hacker News Discussion on Zhipu’s GLM-4.5:**

1. **Performance Comparisons**:  
   - Users compared GLM-4.5’s coding/tool-use capabilities to models like **Claude Sonnet**, **Qwen**, and **GPT-4/5**. Some found GLM-4.5 competitive but noted Claude Sonnet and GPT-5 still lead in benchmarks like SWE-Bench.  
   - Mixed experiences: One user reported GLM-4.5 Air ran well on a MacBook Pro, while others saw inconsistent results compared to Qwen 3 Coder (480B) via API providers.  

2. **Quantization Concerns**:  
   - Skepticism arose about API providers (e.g., OpenRouter, DeepInfra) using **FP4/FP8 quantization**, which may degrade performance. Users cited discrepancies between quantized and full-precision models.  
   - Transparency issues: Doubts persist about providers disclosing quantization practices, with some alleging hidden compromises in model quality.  

3. **API Provider Reliability**:  
   - **Cerebras** and **OpenRouter** were highlighted as cost-effective alternatives, though latency and reliability varied.  
   - Reverse-engineering attempts (e.g., GitHub repos like `claude-cd-reverse`) reflect interest in understanding proprietary models like Claude.  

4. **Cultural/Localization Factors**:  
   - Payment systems and captchas in China were discussed, with users noting challenges for Western users (e.g., complex payment flows, harder captchas).  

5. **Developer Workflows**:  
   - Some praised Claude’s structured outputs for coding agents, while others criticized GPT-5’s tool-handling inefficiency.  
   - Frustration with “agent-first” models requiring extensive prompt engineering to match closed-source alternatives.  

6. **Cost vs. Performance**:  
   - GLM-4.5’s low cost ($0.20/M input tokens) appealed to developers, but concerns lingered about hidden trade-offs. Qwen 3 Coder via Cerebras (~$50/month) emerged as a cheaper, competitive option.  

**Key Takeaway**: The community views GLM-4.5 as promising for agentic tasks but seeks independent validation of performance claims. Transparency around quantization and provider practices remains a pain point, with developers balancing cost against reliability in coding workflows.

### OpenAI set to start mass production of its own AI chips with Broadcom

#### [Submission URL](https://www.ft.com/content/e8cc6d99-d06e-4e9b-a54f-29317fa68d6f) | 29 points | by [gniting](https://news.ycombinator.com/user?id=gniting) | [4 comments](https://news.ycombinator.com/item?id=45152767)

OpenAI set to start mass production of its own AI chips with Broadcom (Financial Times)

- FT reports OpenAI is moving ahead with mass-producing custom AI accelerators in partnership with Broadcom, signaling a push to reduce reliance on Nvidia and tailor silicon to its workloads.
- Why it matters: In-house chips can cut training/inference costs, improve performance per watt, and ease supply constraints. It also aligns OpenAI with other hyperscalers pursuing custom silicon (Google TPU, Amazon Trainium, Microsoft Maia).
- What to watch: real-world performance vs Nvidia’s latest, software stack maturity and ecosystem support, deployment timing and scale, integration with Microsoft/Azure, and whether OpenAI offers the chips beyond its own datacenters.
- HN angle: feasibility and timelines for first-gen silicon, Broadcom’s role (ASIC design/packaging), TSMC capacity constraints, power/cooling challenges, and implications for Nvidia’s dominance.

Here’s a concise summary of the Hacker News discussion:

### Key Discussion Points:
1. **Broadcom’s Role Clarified**:  
   Users noted that Broadcom specializes in designing custom silicon (ASICs) for tech giants like Google, Microsoft, and Amazon. This positions them as a critical partner for OpenAI’s chip ambitions, contrasting with Nvidia’s historical dominance in AI hardware. Broadcom and Marvell are seen as leaders in the custom silicon design space.

2. **Challenges to Nvidia’s Dominance**:  
   A major theme was whether OpenAI’s custom chips could disrupt Nvidia’s ecosystem, particularly its **CUDA software stack**. Commenters highlighted that hardware alone may not suffice—software maturity and developer adoption are critical hurdles. One user succinctly remarked, “Beat CUDA,” emphasizing the difficulty of displacing Nvidia’s entrenched software advantage.

3. **Feasibility Concerns**:  
   Skepticism arose around timelines, supply chain constraints (e.g., TSMC’s manufacturing capacity), and whether OpenAI’s chips could match the performance of Nvidia’s latest GPUs. Others questioned if OpenAI would eventually offer these chips to third parties or keep them exclusive to its own infrastructure.

4. **Market Context**:  
   Comparisons were drawn to hyperscalers like Google (TPU) and Amazon (Trainium), which developed custom chips but still rely heavily on Nvidia. The discussion underscored the broader industry trend toward vertical integration in AI hardware.

### Summary:  
The community views OpenAI’s move as a logical step toward cost and performance optimization but remains cautious about technical execution, software challenges, and Nvidia’s enduring ecosystem lock-in. Broadcom’s expertise is acknowledged, but success hinges on overcoming CUDA’s dominance and scaling production amid industry-wide constraints.

### Let us git rid of it, angry GitHub users say of forced Copilot features

#### [Submission URL](https://www.theregister.com/2025/09/05/github_copilot_complaints/) | 408 points | by [latexr](https://news.ycombinator.com/user?id=latexr) | [294 comments](https://news.ycombinator.com/item?id=45148167)

GitHub’s Copilot push triggers backlash, nudges devs toward Codeberg/Forgejo

- What happened: The two most upvoted GitHub Community threads of the past year ask for ways to block Copilot from generating issues/PRs and to disable Copilot code reviews. Both remain unanswered, fueling frustration over “forced” AI features.

- Who’s driving the pushback: Developer Andi McClure has repeatedly asked Microsoft/GitHub to let users opt out, citing Copilot prompts reappearing in VS Code even after uninstalling the extension. Her posts have lately drawn growing support.

- Why devs are upset: Complaints span AI “slop” and hallucinations, liability disclaimers if Copilot reproduces licensed code, copyright/attribution risks, and ethics. Several projects ban AI-generated code, including Servo, GNOME’s Loupe, FreeBSD, Gentoo, NetBSD, and QEMU.

- Microsoft’s stance: Despite criticism, Microsoft says Copilot is surging—20 million users, with Copilot Enterprise up 75% QoQ. GitHub’s alignment under Microsoft’s CoreAI group reinforced perceptions that AI is now unavoidable on the platform.

- The shift: Some maintainers say they’ll leave if they can’t fully opt out of AI on their repos. McClure and others report increased migration chatter to Codeberg or self‑hosted Forgejo. The Software Freedom Conservancy is renewing its call to ditch GitHub.

Why it matters: There’s a widening gap between platform-level AI mandates and open-source community norms around consent, licensing, and code quality. If GitHub doesn’t provide robust, repo-level AI opt-outs, it risks an exodus of projects to AI-free forges.

**Summary of Hacker News Discussion:**

The discussion expands on frustrations with GitHub Copilot’s forced integration and explores broader implications for developers and open-source projects:

1. **Spam and Low-Quality Contributions**  
   - Users report an influx of **AI-generated spam PRs/issues**, with projects like Curl offering bounties to combat this. Events like Hacktoberfest exacerbate the problem, attracting low-effort contributions (e.g., trivial typo fixes) from users leveraging AI tools like Claude-Code or Cursor.
   - Maintainers of popular projects note **irrelevant Copilot-generated PRs and comments**, forcing them to waste time filtering noise. Some argue GitHub’s incentives (e.g., user job profiles, "cheap" PRs for resume padding) conflict with project health.

2. **Technical and Support Challenges**  
   - Disabling Copilot remains difficult: users highlight **persistent UI elements in VS Code** even after uninstalling extensions. One user details a months-long GitHub support ticket with no resolution, citing opaque JSON-based settings that bypass user preferences.
   - Skepticism grows about GitHub’s commitment to addressing concerns, with accusations of prioritizing Copilot adoption over developer needs.

3. **Migration to Alternatives**  
   - **Codeberg** and **self-hosted Forgejo** gain traction as GitHub alternatives, praised for ethical stances and simplicity. However, **network effects** (e.g., discoverability, CI/CD integrations like GitHub Actions) and inertia keep many projects on GitHub.
   - GitLab is seen as a contender but faces distrust for potentially adding similar AI features. Some advocate for decentralized hosting (e.g., Gitea) to avoid platform lock-in.

4. **Debates on AI’s Value**  
   - Critics liken Copilot to past overhyped tools, questioning its long-term productivity gains. References to **Dan Luu’s blog post** highlight how companies neglecting fundamentals (e.g., version control) for "magic" solutions often face regressions.
   - Others argue resistance to AI mirrors historical pushback against progress (e.g., compilers replacing assembly), but emphasize **consent** and **code quality** as non-negotiable for maintainers.

5. **GitHub’s "Secret Weapons"**  
   - Free CI/CD, Docker Hub integration, and GitHub’s status as a de facto portfolio platform make migration costly. Smaller projects note difficulty attracting contributors off GitHub due to visibility loss.

**Key Takeaway**: The backlash reflects a clash between GitHub’s AI-driven growth strategy and community values of control and quality. While alternatives exist, GitHub’s ecosystem dominance and developer inertia complicate exodus efforts—but sustained pressure could tip the scales if opt-out options aren’t provided.

