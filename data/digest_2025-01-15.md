## AI Submissions for Wed Jan 15 2025 {{ 'date': '2025-01-15T17:13:14.466Z' }}

### Google is making AI in Gmail and Docs free, but raising the price of Workspace

#### [Submission URL](https://www.theverge.com/2025/1/15/24343794/google-workspace-ai-features-free) | 301 points | by [lars_francke](https://news.ycombinator.com/user?id=lars_francke) | [402 comments](https://news.ycombinator.com/item?id=42710978)

In a bold move within the burgeoning B2B AI market, Google is shaking up its pricing strategy by making its AI features in Gmail and Docs accessible for free. Previously available only through the Gemini Business plan at $20 per month per user, these capabilities—including automated note-taking, email summaries, and the advanced NotebookLM research assistant—will now come standard with Google Workspace. However, there's a catch: Google is simultaneously raising the base subscription price by approximately $2, bringing it from $12 to around $14 per user per month.

This strategy aims to boost adoption of Google’s comprehensive AI suite as it competes with Microsoft and OpenAI in crafting the future of productivity tools. Jerry Dischler, Google’s president of cloud applications, emphasized that cost has often deterred companies from fully integrating AI, and by eliminating the extra fee, Google hopes to encourage more users to leverage its advanced tools.

This shift aligns with actions taken by competitors; Microsoft has also begun to fold its AI features into standard subscriptions for Microsoft 365 in a bid to capture user engagement. As the AI race intensifies, both tech giants are betting that expanding access to their tools will prove invaluable in attracting new customers and enhancing overall user experience in their ecosystems.

The Hacker News discussion surrounding Google's recent announcement on AI features in its services, such as Gmail and Docs, reveals varied perspectives from users. 

Several users expressed skepticism about the utility of AI-generated content, particularly in email summaries and note-taking, suggesting that AI may not effectively address real-world communication problems. Some users highlighted instances where AI-generated responses feel formulaic and uninspired, drawing comparisons to spam or overly generic replies.

A recurring theme among commenters is the concern about AI's impact on human communication and creativity, with some feeling that reliance on AI tools could diminish authentic interaction. Other users acknowledged the potential advantages of AI in increasing efficiency but emphasized the importance of human nuance in conveying information.

There were also discussions about the implications for workplace email dynamics and the potential frustration users may face with AI-generated content cluttering their inboxes. Critics pointed out that although AI can assist with tasks, it often falls short in understanding context or crafting high-quality responses.

In conclusion, the community is divided; while some see the move towards free AI tools as a positive step towards enhancing productivity, others remain wary of AI's limitations and its effects on meaningful communication. As companies like Google and Microsoft integrate AI features into their offerings, the ongoing conversation reflects a broader apprehension regarding the balance between technology and genuine human interaction.

### Sky-scanning complete for Gaia

#### [Submission URL](https://www.esa.int/ESA_Multimedia/Images/2025/01/Sky-scanning_complete_for_Gaia) | 171 points | by [sohkamyung](https://news.ycombinator.com/user?id=sohkamyung) | [64 comments](https://news.ycombinator.com/item?id=42709105)

The European Space Agency's Gaia mission has reached a monumental milestone by completing its sky-scanning phase, having amassed over three trillion observations of approximately two billion celestial objects over the last decade. Launched on December 19, 2013, Gaia's ambitious goal was to revolutionize our understanding of the Milky Way and its cosmic neighborhood.

As the spacecraft runs low on fuel, using just a few grams of cold gas daily to maintain its precise orientation, it has successfully performed 15,300 maneuvers. The ongoing data collection has resulted in a rich catalogue that includes information on stars, asteroids, exoplanets, and galaxies, garnering over 580 million accesses and leading to the publication of more than 13,000 scientific papers since the first data release in 2016.

Despite this achievement, Gaia's work isn't finished—two significant data drops are still on the horizon, promising to further enhance our cosmic insights. As this remarkable mission continues to unfold, it solidifies Gaia's role as a critical tool for astronomers worldwide, enabling groundbreaking discoveries about our universe.

The discussion surrounding the significant achievements of the European Space Agency's Gaia mission on Hacker News spans various topics related to its technical aspects and implications for astronomy. 

Many comments emphasized Gaia's impressive cataloging of cosmic objects, with over three trillion observations from its decade-long operation. Users discussed how Gaia’s precise measurements allow for the construction of three-dimensional models of celestial objects, offering insight into their positions and distances—this is seen as revolutionary for understanding the structure of the Milky Way.

Some commenters delved into technical discussions about the nature of galaxies, dark matter, and gravitational effects, speculating on the fate of celestial bodies and the evolution of the universe, including concepts like the "Big Crunch." The role of black holes at the centers of galaxies and their contributions to gravitational dynamics were also explored in depth.

Additionally, there were conversations about the artistic representations of data produced by Gaia, with some users sharing impressions and critiques regarding the aesthetics and interpretations of cosmic imagery.

Technical nuances regarding Gaia's instruments, measurement accuracy, and data processing speeds were discussed, highlighting the sophisticated technology involved in its observations. Users appreciated the mission’s advancements in measuring stellar positions with unprecedented accuracy, emphasizing the impact of Gaia's data on ongoing and future astronomical research.

The overarching sentiment was one of amazement at Gaia's contributions to science and the prospect of further discoveries with upcoming data releases.

### Generate audiobooks from E-books with Kokoro-82M

#### [Submission URL](https://claudio.uk/posts/epub-to-audiobook.html) | 408 points | by [csantini](https://news.ycombinator.com/user?id=csantini) | [235 comments](https://news.ycombinator.com/item?id=42708773)

The latest development in transforming reading habits comes in the form of Kokoro v0.19, a groundbreaking text-to-speech model with just 82 million parameters, giving users the ability to convert e-books into high-quality audiobooks. Designed by Claudio Santini, this tool can produce audiobooks in American and British English, French, Korean, Japanese, and Mandarin, all with impressive voice options.

Introducing "Audiblez," Santini’s companion tool makes it even easier to create audiobooks from .epub files. For example, it took approximately two hours on an M2 MacBook Pro to convert Richard Dawkins' *The Selfish Gene*, around 100,000 words, into mp3 format—a feat that opens the door for avid readers to finally enjoy their entire e-book libraries in audio form. 

Getting started is as simple as installing the tool with pip and downloading the necessary files. Once set up, users can seamlessly transform their e-books into audio files, making the process of chapter detection and conversion accessible, if not perfectly polished. Future improvements could enhance chapter navigation and even integrate text-to-speech for images.

For enthusiasts looking to breathe life into their e-book collections, Kokoro and Audiblez present an exciting opportunity to experience literature in a new auditory way. The project is openly available on GitHub for those wishing to dive deeper and contribute to its evolution.

The discussion around the Kokoro text-to-speech model submission reflects a mix of enthusiasm and skepticism regarding the future of audiobook production using AI technologies. Many commenters expressed concerns about the quality and reliability of AI-generated audiobooks, particularly compared to traditional narrations. There were observations about the potential for AI to automate processes, elevating concerns that it could diminish the artistry and emotional engagement provided by human narrators.

Several users highlighted how the advent of AI in text-to-speech could revolutionize access to literature, making vast libraries of content available to those who may prefer audio formats. However, others pointed out the risk of homogenization and the loss of individual voices in narration. Comments also discussed implications for various professions within the publishing industry, with some anticipating job displacement and others advocating for AI as a complement to human creativity rather than a replacement.

Issues surrounding copyright and the implications of generating derivative works were also raised, reflecting a broader concern within the community about the ethics of using AI for transforming literary content. The dialogue overall reveals a community grappling with the prospects and challenges that technologies like Kokoro and Audiblez present for the future of reading and audiobooks.

### Transformer^2: Self-Adaptive LLMs

#### [Submission URL](https://sakana.ai/transformer-squared/) | 147 points | by [hardmaru](https://news.ycombinator.com/user?id=hardmaru) | [47 comments](https://news.ycombinator.com/item?id=42705935)

In a groundbreaking exploration of AI adaptability, researchers have introduced a novel machine learning system, Transformer², which mimics nature's remarkable ability to adapt. Just like how an octopus blends into its environment or how the human brain reorganizes itself after an injury, Transformer² empowers AI models to dynamically adjust their internal weights for optimal performance across diverse tasks.

At the heart of this innovation is a two-step process: the system first analyzes the incoming task, then it applies task-specific adaptations based on its findings. By incorporating techniques like Singular Value Decomposition (SVD) and reinforcement learning, the model can selectively enhance or suppress different components of its "brain" — essentially its weight matrices — to better address the challenges of a new task.

Transformer² outperforms traditional static methods by significantly improving efficiency and performance while using fewer parameters. This advancement promises a future where AI systems evolve continuously, adapting in real-time to the complexities they encounter. The researchers envision a world where AI and adaptability go hand in hand, forever transforming the way we engage with intelligent systems.

The discussion on the submission regarding Transformer² focuses on various aspects of the innovation and its implications for AI. Key points include:

1. **Model Evaluation**: Several commenters are curious about how Transformer² compares to existing models such as Llama 70B and Mistral 7B, especially in terms of task adaptability and efficiency.

2. **Methodology Insights**: Experts discuss the mechanics of mixture of experts (MoE) models, highlighting the significance of techniques like Singular Value Decomposition (SVD) and reinforcement learning in enhancing the model’s adaptability.

3. **Real-Time Adaptation**: Commenters praise the potential of Transformer²'s real-time weight modification for enabling continuous learning in AI systems, a critical step toward achieving Artificial General Intelligence (AGI).

4. **Practical Implications**: There's excitement about how this research might facilitate better performance in future AI applications, with discussions revolving around its practical applications in fields requiring dynamic task handling.

5. **Community Engagement**: Enthusiasm also revolves around the implications of the researchers’ findings for the broader AI community, and there are mentions of relevant academic and industry collaborations that could result from this work.

In the backdrop of these discussions, some commenters express cautious optimism, reflecting on the limitations and the need for further experimentation and validation of this approach. Overall, the discourse acknowledges the transformative potential of Transformer² while recognizing the challenges ahead.

### OpenAI fails to deliver opt-out system for photographers

#### [Submission URL](https://petapixel.com/2025/01/06/openai-fails-to-deliver-opt-out-system-for-photographers/) | 196 points | by [onetokeoverthe](https://news.ycombinator.com/user?id=onetokeoverthe) | [137 comments](https://news.ycombinator.com/item?id=42718850)

OpenAI has quietly sidestepped its ambitious 2025 deadline for the much-anticipated Media Manager tool, designed to help photographers exclude their work from the company's training data. Announced back in May, the tool aimed to address ongoing copyright concerns, yet there has been a troubling lack of updates or priority given to its development since then. A former OpenAI employee revealed to TechCrunch that there’s little momentum behind the project, stating, “I don’t remember anyone working on it.”

Initially, OpenAI had promised a process that would allow creators to easily opt-out of AI training, but the current method requires cumbersome submissions for each individual work. Critics, including Ed Newton-Rex, founder of Fairly Trade, argue that such a system is fundamentally unfair and will not reach the majority of creators, thus enabling the continued exploitation of their work. 

As conversations around AI's use of copyrighted content intensify, this stagnation raises questions about OpenAI’s commitment to creator rights, especially against a backdrop where similar platforms struggle with copyright compliance. The last mention of Media Manager came in August, when a spokesperson confirmed it was still in development, leaving many to wonder if the tool will ever see the light of day.

In the discussion surrounding OpenAI's seemingly stalled development of the Media Manager tool, users express various concerns about copyright issues and how they relate to AI training. A post from "tddmry" points out the potential legal complexities for creators to exclude their work from AI training, noting that the current process is cumbersome and resembles past peer-to-peer sharing disputes, drawing parallels to Napster.

Others, like "DaiPlusPlus," highlight past failures in music sharing platforms that failed to protect creative rights and allow individual creators adequate control over their work. The sentiment of frustration over the inability of independent creators like photographers to opt-out effectively resonates throughout the comments, with "dylan604" criticizing the lack of a straightforward method for creators to submit their work for exclusion.

The conversation also touches upon a broader industry critique regarding the perceived double standards in copyright infringement—highlighting how machines may replicate creative works without consequence, while human artists face more scrutiny. "llm_trw" reflects on the inherent differences between human and machine-generated content, emphasizing that AI systems should not infringe on human copyrights.

Overall, there’s a shared feeling of disappointment regarding OpenAI's commitment to resolving these issues and fear that creators’ rights will be sidelined. Many commenters express skepticism about whether the Media Manager tool will materialize or adequately protect creators in the evolving landscape of AI-generated content.

### OpenAI revises policy doc to remove reference to 'politically unbiased' AI

#### [Submission URL](https://techcrunch.com/2025/01/14/openai-quietly-revises-policy-doc-to-remove-reference-to-politically-unbiased-ai/) | 19 points | by [ivanleoncz](https://news.ycombinator.com/user?id=ivanleoncz) | [10 comments](https://news.ycombinator.com/item?id=42717586)

OpenAI has made a noteworthy revision to its policy documents, removing the phrase advocating for "politically unbiased" AI from its recent economic blueprint. This adjustment comes after mounting political scrutiny, particularly from allies of President-elect Donald Trump, who have accused AI platforms like ChatGPT of harboring liberal biases. Elon Musk and David Sacks, vocal critics of perceived censorship in AI, have argued that such biases reflect the "woke" culture prevalent in San Francisco tech circles. In a bid to streamline its messaging, OpenAI maintains that biases in its models are unintentional flaws rather than design features. This move highlights the complexities of bias in AI, a persistent challenge that firms like Musk's xAI are also grappling with. As debates surrounding AI ethics and political impartiality heat up, OpenAI's tactic indicates a strategic maneuver in navigating an increasingly charged political landscape.

The discussion on Hacker News revolves around the implications of OpenAI's policy revision on AI applications, specifically regarding biases in their language models (LLMs) like ChatGPT. Participants express concern about how these models might affect decision-making in public funding and departmental budgeting, suggesting that they could inadvertently reinforce biases or lead to irrational outcomes when used as decision-making tools. Comments highlight the absurdity of relying on LLMs for such significant tasks without substantial historical data to back their predictions.

Some commenters argue that employing LLMs to make decisions around funding could negatively impact communities and favor certain biases prevalent in tech culture. Critics stress the importance of using objective data and analysis rather than relying on models that may lack the appropriate context or nuance required for sensitive policy decisions. The conversation underscores a broader debate about the relationship between AI, bias, and political implications, reflecting the complexities stemming from OpenAI's latest policy changes.

### Working with The Associated Press to provide fresh results for the Gemini app

#### [Submission URL](https://blog.google/products/news/associated-press-gemini-app/) | 87 points | by [alexrustic](https://news.ycombinator.com/user?id=alexrustic) | [63 comments](https://news.ycombinator.com/item?id=42716204)

In a significant development for news delivery and AI integration, Google has teamed up with The Associated Press to enhance the Gemini app with real-time information feeds. This partnership aims to bolster the app’s ability to provide users with timely and accurate content, aligning with Google's longstanding commitment to fostering journalism. As AP's Senior VP Kristin Heitmann highlights, this collaboration underscores the importance of accurate and nonpartisan reporting in the realm of AI products. The move is part of Google’s broader strategy to innovate and empower the journalism landscape, exemplified by initiatives like the JournalismAI Innovation Challenge. This partnership not only benefits consumers seeking up-to-date news but also strengthens the support for local journalism worldwide, marking a new chapter in the synergy between technology and media.

Google's collaboration with The Associated Press aims to enhance its Gemini app by providing real-time news feeds. This initiative focuses on delivering accurate, timely information, reinforcing Google's commitment to supporting journalism and local news outlets. The partnership is part of a wider strategy to integrate AI into news delivery while maintaining nonpartisan and factual reporting standards.

The comments reflect a mix of excitement and skepticism regarding the partnership. Some users express optimism about the potential for improved real-time information through models that can grasp current events, though concerns about "hallucinations" from AI models persist. Others highlight the challenges inherent in ensuring that AI-generated content maintains journalistic integrity, especially given that past experiences with AI summaries sometimes resulted in inaccuracies.

There are discussions about the real-time updating capabilities of the models and their effectiveness in addressing current news. Some users also underline the potential for misinformation and biases in AI reporting, stressing the necessity for careful oversight. Despite divergent viewpoints, there is a consensus that the integration has potential benefits for news dissemination but needs careful execution to avoid pitfalls that have plagued similar initiatives in the past. 

Critics of AI in journalism warn about the implications it has on trust in news sources, while proponents believe that partnerships like this could eventually lead to improved standards in news reporting and consumption, particularly in the age of misinformation. 

Overall, the discourse reflects a cautious yet hopeful perspective on the evolution of AI in journalism, balancing technological advancements with the essential need for accuracy and public trust in news.

