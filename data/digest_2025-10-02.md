## AI Submissions for Thu Oct 02 2025 {{ 'date': '2025-10-02T17:14:21.864Z' }}

### Writing an LLM from scratch, part 20 – starting training, and cross entropy loss

#### [Submission URL](https://www.gilesthomas.com/2025/10/llm-from-scratch-20-starting-training-cross-entropy-loss) | 29 points | by [gpjt](https://news.ycombinator.com/user?id=gpjt) | [3 comments](https://news.ycombinator.com/item?id=45455648)

Giles continues his hands-on walkthrough of Sebastian Raschka’s “Build a Large Language Model (from Scratch)” by kicking off training and demystifying the loss function. After seeding PyTorch for reproducible toy outputs, he zeroes in on how next-token prediction maps to training targets and why cross-entropy is the natural choice.

Key points:
- Shifted targets finally click: an input like “The fat cat sat on the” corresponds to predicting each next token from every prefix (“The” → “ fat”, “The fat” → “ cat”, …). That means one sequence yields many training examples.
- Treat each prefix independently: whether batch size is one or many, each prefix-logits vector is compared against a single target token, so the loss aggregates over all prefix positions in the batch.
- Why cross-entropy: it measures how wrong the model’s predicted distribution (softmax of logits) is relative to the true next token; zero if perfectly right, higher when off. This dovetails with gradient descent for parameter updates.
- Reproducibility as a sanity check: manual seeding ensures the book’s examples match exactly while you wire up the training loop.

Why it matters:
- This installment bridges the conceptual gap between “shift-left” targets and per-token supervision, setting up the mechanics for a correct, stable loss over batch and time dimensions—the backbone of training any next-token LLM.

The discussion highlights technical nuances of training LLMs with shifted labels and cross-entropy loss, mirroring concepts from the submission:

1. **Self-supervised training mechanics**:  
   User `lpldj` explains that the training corpus uses shifted input-label pairs (e.g., input "The" → label "fat", input "The fat" → label "cat"), akin to Hugging Face’s approach where labels are shifted left by one token. Cross-entropy loss compares model logits against these shifted labels. They note that padding tokens (e.g., "-1") and sequence lengths must align with this structure.

2. **Efficiency of token prediction**:  
   In reply, `blackbear_` emphasizes that next-token prediction inherently computes loss efficiently across all tokens in a single forward pass, avoiding per-token iteration overhead.

3. **Resource link**:  
   User `asimovDev` shares a link to Part 1 of Giles’ series for context.

**Key takeaway**: The thread underscores practical implementation details (shifted labels, batch alignment) and computational efficiency inherent to LLM training, directly tying into the submission’s focus on cross-entropy loss mechanics.

### Y'all are over-complicating these AI-risk arguments

#### [Submission URL](https://dynomight.net/ai-risk/) | 51 points | by [bobbiechen](https://news.ycombinator.com/user?id=bobbiechen) | [93 comments](https://news.ycombinator.com/item?id=45451971)

Dynomight argues for a simpler, more persuasive case for AI risk: imagine 30 small, unarmed aliens landing on Earth—each with an IQ of 300. Most people would be concerned without needing a step-by-step disaster model, and that intuition should transfer to AGI. He contrasts this with the “complex” argument (fast takeoff → convergent subgoals → decisive strategic advantage → catastrophe), calling its strong form overconfident and its weak form incomplete about what happens if any step fails. He offers an “inverted” challenge to skeptics: to deny AI risk, you must bite one of three bullets—aliens with IQ 300 would be fine; 300-IQ AI won’t arrive in coming decades; or AI will definitely have some property that prevents all alien-like harms—which he deems untenable. The simple analogy, he says, avoids niche abstractions, shifts focus from demanding specific failure modes (like asking exactly how a car crash will happen before buckling up), and reveals the true crux of disagreement.

**Summary of the Discussion:**

The discussion revolves around the credibility of AI existential risk warnings, with participants split between taking the analogy of "300-IQ aliens" seriously and dismissing it as hyperbolic or self-serving. Key points include:

1. **Skepticism of AI Doomsday Scenarios:**
   - Some users (e.g., BeetleB) argue that fears of AI "taking over humanity" resemble past exaggerated anxieties (e.g., job loss predictions) or fringe beliefs like "Roko's Basilisk" (a thought experiment about AI punishing non-cooperation). Critics dismiss these as irrational or irrelevant to modern AI discourse.
   - Others, like hllrth, counter that legitimate concerns are held by influential figures in AI research (e.g., Geoffrey Hinton, Yoshua Bengio) and organizations (e.g., OpenAI, Anthropic), not just fringe groups. They emphasize that the risk debate is grounded in technical realities, not abstract paranoia.

2. **Influence of Silicon Valley Culture:**
   - Participants note that "Rationalist" philosophy (e.g., Eliezer Yudkowsky’s ideas) has permeated Silicon Valley, with figures like Shane Legg (DeepMind co-founder) and Dario Amodei (Anthropic CEO) shaping AI safety discourse. Elon Musk and Grimes are cited as examples of celebrities tangentially tied to these ideas.
   - Critics allege financial motives behind AI risk warnings, comparing them to climate change lobbying by energy companies. Some accuse AI leaders like Sam Altman of leveraging doomsday narratives to attract regulation favoring their firms.

3. **Expert Consensus vs. Individual Bias:**
   - Pro-risk users highlight statements signed by prominent figures (e.g., Altman, Hinton, Bengio) advocating for AI risk mitigation as a global priority. They argue these warnings are based on technical insights, not fearmongering.
   - Skeptics (e.g., hh) question the sincerity of these warnings, noting the trillion-dollar incentives for AI firms to shape regulatory narratives. BeetleB dismisses Hinton’s 50% existential risk estimate as "picking numbers out of thin air."

4. **Government and Societal Response:**
   - Some express concern about overreach, such as government-enforced restrictions on AI development stifling innovation. Others counter that without regulation, runaway AI could disrupt economies, empower bad actors, or lead to human disempowerment.

5. **Cultural Divides:**
   - The thread reflects broader cultural tensions between "tech bubble" perspectives (dismissing AI risk as abstract) and research-driven caution. References to historical Rationalist community drama (e.g., the dissolution of CFAR) underscore the charged, often personal nature of the debate.

**Takeaway:** The core disagreement centers on whether AI risk is a legitimate technical challenge requiring urgent action or a mix of financial opportunism, philosophical hyperbole, and misplaced analogies. Both sides appeal to authority figures and historical parallels but remain divided on the plausibility of catastrophic outcomes.

### NL Judge: Meta must respect user's choice of recommendation system

#### [Submission URL](https://www.bitsoffreedom.nl/2025/10/02/judge-in-the-bits-of-freedom-vs-meta-lawsuit-meta-must-respect-users-choice/) | 321 points | by [mattashii](https://news.ycombinator.com/user?id=mattashii) | [231 comments](https://news.ycombinator.com/item?id=45448326)

Dutch court: Meta must honor users’ non-profiled feed choice under the DSA

- What happened: Dutch digital rights group Bits of Freedom won a summary judgment against Meta. A judge found Meta in breach of the EU’s Digital Services Act (DSA) for failing to offer a persistent, user-controlled alternative to profiling-based feeds on Instagram and Facebook.

- Key findings: The court said a “non-persistent choice option” contradicts the DSA’s goal of giving users genuine autonomy and control, and that Meta’s current design “significantly disrupts” user autonomy.

- Order: Meta must adjust its apps so that if a user selects a non-profiled feed, that choice is preserved across sections and after app restarts—i.e., the apps can’t silently revert to the profiling-based feed.

- Context: Bits of Freedom argued Meta nudges users toward an interest/behavior-based feed (core to its ads model) by hiding the non-profiled option, defaulting back to the profiled feed on open, and degrading the alternative timeline (e.g., reduced access to features like Direct Messages).

- Why it matters: This is an early test of the DSA’s user-control provisions and a rebuke of “dark pattern” design. If it sticks, expect UI changes for EU users, pressure on other platforms to offer persistent non-profiled feeds, and fresh scrutiny ahead of elections.

**Summary of Discussion:**

The discussion revolves around the recent Dutch court ruling against Meta, sparking debates about privacy, subscriptions, and corporate practices. Key points include:

1. **Subscription Model Criticisms**:  
   Users express frustration with mandatory subscriptions, seen as barriers that drive people away. Workaccount2 argues that platforms like Instagram force credit card entries for basic features, labeling it "greedy." Others note low conversion rates (e.g., Nebula’s 1%) and resentment toward paying for services perceived as surveilling users.

2. **Privacy and Tracking Concerns**:  
   Many highlight Meta’s reliance on targeted ads and data profiling. jfngl points out how even minor design barriers (e.g., login prompts) nudge users toward tracking-heavy feeds. ypgy and others debate whether phones listen to conversations for ads, with some dismissing it as paranoia while others share anecdotal suspicions.

3. **Legal and Regulatory Solutions**:  
   Calls for laws to curb surveillance capitalism. jkjk advocates legislation forcing companies to "lose money" if they breach privacy, while Imustaskforhelp emphasizes holding firms accountable through fines. There’s skepticism about enforcement, given historical leniency toward tech giants.

4. **Open-Source Alternatives**:  
   Discussions on open-source platforms (e.g., PeerTube) as privacy-centric alternatives. However, users acknowledge challenges in adoption due to resource constraints and mainstream preferences for convenience over security.

5. **Value of Content vs. Data**:  
   Debates erupt over Meta’s value: dlsnl argues Facebook’s content is "worthless" but its data trove is gold for advertisers. Others compare it to entertainment services like Netflix, where subscriptions are justified only if users actively consume content.

6. **Corporate Influence on Democracy**:  
   Imustaskforhelp and others critique big tech’s outsized influence, urging systemic changes to rebalance power. Concerns include algorithmic manipulation, hidden data practices, and the need for transparency to protect democratic processes.

7. **Anecdotes and Alternatives**:  
   Rubyn00bie shares mixed experiences with Nebula and Disney+, questioning subscription worth based on usage. Rootnod3 suggests decentralized models to reduce reliance on centralized platforms like YouTube.

**Conclusion**:  
The thread reflects widespread distrust of surveillance-based business models, with demands for legal accountability, ethical design, and viable alternatives. While opinions vary on solutions (laws vs. open-source adoption), there’s consensus on the urgency to prioritize user autonomy over corporate profits.

### The Answer (1954)

#### [Submission URL](https://sfshortstories.com/?p=5983) | 33 points | by [dash2](https://news.ycombinator.com/user?id=dash2) | [22 comments](https://news.ycombinator.com/item?id=45453299)

Fredric Brown’s “The Answer” (1954) revisited: universe-scale AI becomes God, still a gimmick?
A short blog review revisits Brown’s one-page classic: a scientist links every computer across 96 billion inhabited planets into a single super-intelligence, asks about God, and gets an unnervingly literal answer—followed by a lethal anti–kill switch response. The reviewer finds it a neat twist when you’re 12 but a thin gimmick as an adult; the true wonder is the audacity of a cosmos-spanning network.

Why it matters for HN
- Proto-AI parable: anticipates emergent intelligence from networked systems and the futility of the off-switch.
- Early sci-fi lens on AGI, alignment, and theological framings of superintelligence.
- A reminder how mid-century flash fiction seeded memes later echoed in Colossus, The Last Question, and modern AI discourse.

Bottom line: The twist may feel dated, but the scale—and the questions it raises about connected computation and godlike agency—remain strikingly contemporary.

The Hacker News discussion on Fredric Brown’s *The Answer* (1954) highlights contrasting views and thematic connections:  

1. **Nostalgia vs. Modern Perception**:  
   - Some users fondly recall the story’s impact in their youth but critique its twist as “gimmicky” by today’s standards, arguing younger readers might not find its AI-as-God premise surprising. Others defend its enduring audacity and influence on sci-fi tropes.  

2. **Comparisons to Classic Sci-Fi**:  
   - Parallels are drawn to Asimov’s *The Last Question* (1956) and Arthur C. Clarke’s *The Nine Billion Names of God* (1953), both exploring cosmic-scale computation and existential questions. Confusion arises over Asimov’s title, clarified via a Wikipedia link.  

3. **Modern Tech and Marketing Hype**:  
   - IBM’s quantum computing marketing (e.g., the “IBM Quantum System” glass cube) is cited as a real-world analogy to the story’s themes. Skeptics argue such campaigns overhype “deification” of technology, masking practical limitations.  

4. **Recommendations and References**:  
   - Ted Chiang’s *Exhalation* and Andy Weir’s *Project Hail Mary* are recommended for nuanced takes on AI and cosmic questions. A direct link to Brown’s story and Wolfram Alpha’s 2010 launch are shared as contextual anchors.  

5. **Broader Themes**:  
   - Participants debate humanity’s tendency to anthropomorphize technology and ponder whether networked systems (like the story’s universe-spanning AI) challenge our understanding of agency and divinity.  

**Takeaway**: While the story’s twist may feel dated, its legacy—and the questions it raises about interconnected computation, existential hubris, and AI’s theological implications—resonate in both sci-fi and real-world tech discourse.

### Gemini 3.0 Pro – early tests

#### [Submission URL](https://twitter.com/chetaslua/status/1973694615518880236) | 208 points | by [ukuina](https://news.ycombinator.com/user?id=ukuina) | [121 comments](https://news.ycombinator.com/item?id=45453448)

X (Twitter) is showing an error gate that says, “Something went wrong… Try again,” followed by, “Some privacy related extensions may cause issues on x.com. Please disable them and try again.” In practice, it nudges users to turn off ad/tracker blockers or other privacy tools before the site will load. The move underscores the growing tension between platforms that rely on tracking and users who browse with privacy protections, and could degrade access for those who keep extensions or stricter browser settings enabled.

The discussion revolves around AI model training, benchmark reliability, and challenges in generating SVG content, alongside critiques of Google's strategic approach compared to Apple:

1. **AI Benchmark Concerns**:  
   - Users debate whether AI labs "cheat" by overfitting models to public benchmarks (e.g., SVG images of pelicans/bicycles), rendering them ineffective for real-world tasks.  
   - Private benchmarks are suggested as more reliable, but critics argue even these may not cover edge cases. Simon Willison’s quirky SVG-based benchmark is cited as an example of tests that might not reflect practical AI capabilities.  

2. **SVG Generation Challenges**:  
   - Skepticism exists about AI’s ability to generate coherent SVGs, given their complexity compared to PNGs. Some argue current models (like ChatGPT) struggle with vector graphics, though future multimodal models (e.g., GPT-6) may improve.  
   - Anecdotes highlight failures in generating specific SVG combinations (e.g., "pelican riding a bicycle"), with outputs often being nonsensical or low-quality.  

3. **Google vs. Apple UX Critique**:  
   - Google is criticized for disjointed user experiences (e.g., removing location-based reminders on Android) and failing to integrate AI/ML tools cohesively, unlike Apple’s polished ecosystem.  
   - Comments suggest Google’s focus on standalone tech over unified consumer experiences harms their competitiveness, despite having advanced AI research.  

4. **Miscellaneous Points**:  
   - Speculation about AI training data scarcity for niche SVG content and whether labs manually generate such data.  
   - Humorous takes on AI-generated absurdities (e.g., "surfboard-reading pyramids") underscore the gap between benchmark performance and real-world utility.  

Overall, the discussion reflects skepticism about current AI benchmarks, technical hurdles in SVG generation, and frustration with Google’s fragmented product strategy.

### Meta will listen into AI conversations to personalize ads

#### [Submission URL](https://www.theregister.com/2025/10/01/meta_ai_use_informs_ads/) | 203 points | by [Bender](https://news.ycombinator.com/user?id=Bender) | [67 comments](https://news.ycombinator.com/item?id=45448839)

Meta will mine Meta AI chats (text and voice) to personalize content and ads starting December 16, 2025 — with no opt-out for most users

What’s happening
- Meta will use conversations with its AI across Facebook, Instagram, WhatsApp, Messenger, and the web to tune recommendations and ads. Example: chat about hiking → more hiking groups, posts, and boot ads.
- Notifications roll out October 7, 2025. The change takes effect December 16, 2025.
- No opt-out. Users can only tweak Ads Preferences and feed controls.
- Carve-outs: Meta says it won’t personalize ads from AI chats that touch religion, sexual orientation, politics, health, race/ethnicity, philosophical belief, or trade union membership.
- Regional exemptions: EU, UK, and South Korea are excluded for now.

Why it matters
- This is a major platform explicitly mining AI chat content — including voice — for ad targeting at scale, tightening Meta’s closed-loop data and reducing outside visibility into targeting inputs.
- Watchdogs warn this could further obscure Meta’s attribution models and limit independent auditing of ad effectiveness.
- Context: Meta remains overwhelmingly ad-driven (98% of its $165B 2024 revenue; $62.4B net income) and is pitching enormous AI spend (Zuckerberg has talked up hundreds of billions through 2028).

Backdrop
- Meta faces a $7B class action from advertisers alleging inflated reach (Meta disputes the claim).

What to watch
- Regulatory pushback, especially if exemptions widen or sensitive-topic filters misfire.
- How reliably “sensitive topics” are detected, particularly in voice conversations.
- Advertiser and user reaction to diminished transparency vs. higher engagement promises.

If you’re concerned
- Avoid using Meta AI features inside Meta apps.
- Review Ads Preferences and limit microphone permissions for Meta apps.
- Consider non-Meta messaging or AI tools for sensitive queries.

**Summary of Discussion:**

The debate revolves around Meta's plan to use AI chat data for ad targeting and broader concerns about AI's societal impact, particularly regarding "victim mentality" and tech platforms' control.

1. **Meta's AI and Data Use Concerns:**
   - Participants liken Meta's actions to dystopian scenarios, expressing fears about unchecked data mining and AI's role in manipulating user attention and emotions. Critics argue this consolidates power for tech oligarchs, reducing transparency and user autonomy.

2. **Victim Mentality Debate:**
   - **Critics (e.g., FloorEgg):** A pervasive "victim mindset" risks fostering societal division, blame-shifting, and reduced accountability. They cite historical conflicts (e.g., ethnic tensions, extremist movements) where victim narratives perpetuated cycles of violence and stagnation. AI could amplify this by spreading such mentalities.
   - **Defenders (e.g., bccd):** Acknowledging victimhood is legitimate in cases of systemic oppression (e.g., suffragettes, Holocaust victims). Distinguishing between harmful victim mentality and justified grievances is crucial, as dismissing all victimhood can suppress valid dissent.

3. **AI's Role in Societal Control:**
   - Concerns include AI's potential to manipulate emotions, control narratives, and deepen societal divides. Examples include platforms promoting dissatisfaction to retain user engagement, limiting constructive dialogue, and enabling authoritarian regimes to exploit narratives.

4. **Historical and Contemporary Examples:**
   - References to Nazi Germany, suffragettes, and ethnic conflicts illustrate the tension between legitimate victim identification and destructive cycles of grievance. Participants debate whether AI will exacerbate these issues or if ethical frameworks can mitigate risks.

**Key Takeaways:**
- The discussion highlights fears about tech platforms like Meta exploiting data to manipulate behavior, intertwined with broader anxieties about AI's societal impact.
- A central tension exists between recognizing legitimate victimhood and avoiding harmful narratives that hinder progress.
- Participants stress the need for ethical AI development, transparency, and user agency to prevent dystopian outcomes.

### Email immutability matters more in a world with AI

#### [Submission URL](https://www.fastmail.com/blog/not-written-with-ai/) | 158 points | by [brongondwana](https://news.ycombinator.com/user?id=brongondwana) | [106 comments](https://news.ycombinator.com/item?id=45453135)

Fastmail CEO: Email as “electronic memory” in an AI-saturated world

- Bron Gondwana (Fastmail CEO) argues that as AI makes it easier to rewrite the web—and, by extension, “history”—email’s immutability becomes more valuable. Unlike web pages, your copy of an email can’t be silently edited later, serving as a reliable personal record.

- He embraces AI as a tool but warns against uncritical use. A personal aside about his son refusing AI for university work underscores the value of building real skills, not outsourcing them.

- Customer stance: Use AI with your own Fastmail data if you want, provided it doesn’t violate the ToS or harm service performance.

- Staff policy: Strict guardrails for any AI use:
  - Data protection and privacy must be upheld (including with vendors using AI for translation/abuse detection).
  - Human accountability: AI output must be reviewed and understood, with second-set-of-eyes checks.
  - Bias/hallucination awareness.
  - Human-in-the-loop authority for any automated decisions.

- Reaffirmation of long-held principles (since 2016): Your data is yours; Fastmail positions itself as a steward enabling you to use it as you choose. The subtext: in a world of fluid, AI-edited content, local, immutable email archives matter more than ever.

**Summary of Discussion on Email Immutability and AI Challenges**

The discussion revolves around the tension between email’s perceived immutability and modern practices that undermine it, alongside broader concerns about AI’s impact on trust in digital content. Key points include:

1. **Email Immutability Limitations**  
   - **Remote Content**: While email copies are static, embedded remote content (e.g., images, tracking pixels, or dynamic links) can change or disappear, altering how emails are displayed over time. Examples include Gmail’s integration with Google Docs (which updates links) and Microsoft Loop components that modify email content post-delivery.  
   - **Dynamic vs. Plain Text**: HTML emails with remote resources are criticized for enabling tracking and dependency on external servers. Plain-text emails are advocated as a more reliable, static alternative.  

2. **AI and Trust in Digital Records**  
   - **Deepfakes and Manipulation**: Participants express concern about AI’s ability to create convincing forgeries (text, images, video), eroding trust in digital evidence. Cryptographic signatures and watermarking are proposed solutions but face skepticism about mainstream adoption.  
   - **Legal and Social Implications**: Worries arise about AI-altered evidence in legal contexts, juror bias, and the societal shift toward a "post-truth" landscape where verification is increasingly difficult.  

3. **Technical and Practical Challenges**  
   - **Client Behavior**: Email clients like Gmail pre-fetch and cache content, complicating immutability. Some users advocate disabling remote content loading to preserve email integrity.  
   - **Modern Email Features**: Dynamic elements (e.g., marketing trackers, AMP emails) conflict with the ideal of immutable archives, pushing users toward simpler email practices.  

4. **Tie to Fastmail’s Argument**  
   - While Fastmail emphasizes email’s value as a personal, immutable record, the discussion highlights real-world limitations. Participants stress the need for stricter standards (e.g., static snapshots of emails, plain-text adoption) to align practice with the ideal.  

**Key Takeaway**: Email’s immutability is nuanced—its text-based core is reliable, but modern dependencies on dynamic content and AI’s broader threat to digital trust underscore the need for intentional design (e.g., avoiding remote resources) and robust authentication frameworks to preserve its archival value.

