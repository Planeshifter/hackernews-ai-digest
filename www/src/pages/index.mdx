import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Fri Nov 17 2023 {{ 'date': '2023-11-17T17:10:34.732Z' }}

### Show HN: nbi.ai â€“ Generative Business Intelligence

#### [Submission URL](https://www.narrative.bi/ai) | 94 points | by [fromthegut](https://news.ycombinator.com/user?id=fromthegut) | [26 comments](https://news.ycombinator.com/item?id=38310502)

NBI.AI, a generative AI platform for business intelligence, has released their latest update. The platform aims to drive growth by providing AI-generated data narratives that deliver actionable insights with just a few clicks. With NBI.AI, users can automate reporting with natural language stories, making it easier to understand complex analytics. The platform generates insights in plain language, eliminating technical jargon and complex data interpretations. NBI.AI also offers weekly AI-powered insights on marketing performance, as well as tools to compare and evaluate ad performance, identify top performers, and analyze conversion journeys. The platform integrates seamlessly with marketing and advertising sources, allowing users to connect in just two clicks. NBI.AI is trusted by over 2,000 growth teams worldwide and offers a 7-day free trial.

The discussion on the submission about NBI.AI, a generative AI platform for business intelligence, covers several topics. Here are the key points:

- One commenter mentions that they are skeptical about AI-driven decision-making tools and prefer a context-leading rule-based natural language generation approach. They expect divergence between rule-based statistical inference narratives and traditional business intelligence data interpretations.
- The founder of NBI.AI responds, providing additional information about their product and its capabilities. They mention that the platform was built to connect virtually structured data sources and has already helped over 2,500 teams gain insights from marketing data.
- Another commenter shares their experience with using narrative-based projects. They use high-level reports that highlight month-over-month changes in website traffic and use an alternative GA4 UI for detailed insights. They plan to implement dimensional analysis to further understand their data.
- The discussion touches on the use of AI in natural language generation and the importance of accuracy and pre-processing to ensure high-quality narratives.
- There is a brief exchange about using automation tools for basic workflows, such as checking invoices and renaming files based on invoice IDs.
- Examples of use cases for NBI.AI are shared, including reporting, anomaly detection, and natural language insights generation for marketing campaigns.
- The founder of NBI.AI clarifies that the training data used for the platform comes from various sources and is focused on behavioral data preferences and feedback to provide personalized insights.
- A user discusses their experience as a marketing specialist and mentions that instead of creating PowerPoint presentations with performance graphs and narrative ROAS, they would prefer using NBI.AI.
- There is a brief discussion about integration plans for NBI.AI and suggestions for additional features.
- Some users express their skepticism about AI-generated data narratives, mentioning that they tend to sound like corporate jargon and lack substance.
- The founder of NBI.AI responds to the feedback, stating that historically they have focused on growth, marketing, and sales data narratives, and that the AI-generated insights are written in natural language.
- There is a discussion about the interpretation and understanding of AI-generated data narratives and the importance of connecting data from various sources to generate focused growth insights and recommendations.

Overall, the discussion provides a mix of skepticism and interest in AI-generated data narratives, with some users sharing their own experiences and suggestions. The founder of NBI.AI actively participates in the discussion, addressing concerns and providing more information about the platform.

### Unauthorized "David Attenborough" AI clone narrates developer's life, goes viral

#### [Submission URL](https://arstechnica.com/information-technology/2023/11/unauthorized-david-attenborough-ai-clone-narrates-developers-life-goes-viral/) | 227 points | by [seasicksteve](https://news.ycombinator.com/user?id=seasicksteve) | [187 comments](https://news.ycombinator.com/item?id=38302319)

In a creative and unauthorized experiment, developer Charlie Holtz combined GPT-4 Vision and ElevenLabs voice cloning technology to create an AI version of David Attenborough narrating his every move on camera. Holtz used a Python script called "narrator" to take a photo from his webcam every five seconds and feed it to GPT-4V, which processed the image and generated Attenborough-style text. This text was then fed into an ElevenLabs AI voice profile trained on Attenborough's speech. The demo video of the experiment has gained significant attention on social media, with mixed reactions from the audience. While some expressed discomfort with imitating Attenborough's voice without permission, others found the demonstration amusing and creative.

The discussion on the submission starts with a comment questioning the ethical concerns of voice cloning and replicating famous individuals. Another user points out that the technology allows for the creation of commercial narrations in the styles of famous voices like Attenborough and Freeman. The conversation then shifts to a debate about the significance and influence of classic works of literature and how technology can impact their reproduction. Some users argue that technological advancements have made it easier for classics to be produced and distributed, while others argue that the quality and cultural impact of works from different time periods cannot be easily compared. Another user brings up the idea that generations often have different points of reference and familiarity with certain things, which affects artistic expression and experimentation. One user mentions a BBC documentary narrated by David Attenborough. The conversation then diverts to a discussion about the recycling of cultural content and the push for profit and nostalgia. Some users express concerns about the lack of originality and artistic challenge in replicating older works, while others discuss the dynamics of the entertainment industry and how content creation and consumption have evolved. One comment suggests that AI could potentially create new episodes of old shows like Inspector Gadget. However, another user disagrees, stating that AI-generated content eliminates creativity and renders results meaningless. The conversation then touches on the craftsmanship involved in animation and the varying levels of effort put into different animation styles. The discussion concludes with a mention of a science fiction character, Duncan Idaho.

### A PCIe Coral TPU Finally Works on Raspberry Pi 5

#### [Submission URL](https://www.jeffgeerling.com/blog/2023/pcie-coral-tpu-finally-works-on-raspberry-pi-5) | 110 points | by [mikece](https://news.ycombinator.com/user?id=mikece) | [20 comments](https://news.ycombinator.com/item?id=38308552)

The Raspberry Pi 5 can now natively support the PCIe Coral TPU, an AI accelerator used for tasks like machine vision and audio processing. Previously, getting the PCIe Coral TPU to work on a Raspberry Pi was challenging due to quirks in the Compute Module 4's PCIe implementation. However, with the improved PCIe bus on the Raspberry Pi 5, it is now possible, although a few tweaks are required. These include switching to a 4K page size, disabling PCIe ASPM, and making changes to the device tree. Additionally, due to compatibility issues, running the Coral's PyCoral library requires either Docker or installing an alternate system-wide Python version. While there are no commercially-available HATs or adapter boards for connecting the Coral TPU to the Raspberry Pi 5's PCIe header, options like the HatDrive! Top or Bottom from Pineberry Pi or the Coral B+M key module with an appropriate adapter can be used. Once set up, the Coral TPU can be used for various AI tasks, such as image classification. Overall, this development opens up new possibilities for AI acceleration on the Raspberry Pi platform.

Some notable points from the discussion on Hacker News about the Raspberry Pi 5's PCIe support for the PCIe Coral TPU are:

- The comparison is made between various AI accelerators, including HBM3E HAT mk TPUs, NVIDIA Jetson Nano, NVIDIA Orin Nano and AGX, and Coral Mini-PCIe. The discussion includes the TPU's computing power, Tensor Processing Units (TPU) architecture, DLSS architecture, and Vision and Versatile Processor Units (VPU).
- One user mentions the Radxa Rock 5B's NPU, which supports various types of acceleration such as INT4, INT8, INT16, FP16, BF16, and TF32 with a computing power of 6TOPs.
- The Coral TPU's software requirements are discussed, including the need for Python 3.9, which may be a challenge for some users.
- Discussion touches on alternative options, such as Hailo, which is considered a powerful competitor to Coral but may face power-related issues and Python's Global Interpreter Lock (GIL) limitation.
- There are mentions of alternative connectors, such as USB, for the Coral TPU.
- The software support for NPUs in general is considered lacking, highlighting the need for better development and momentum in this area.
- The compatibility of Coral TPU with Ubuntu 20.04 and Python versions is discussed, with reference to the support and versions provided by Ubuntu and AWS Lambda runtimes.
- A user mentions that binary bindings for Coral TPU are only supported on Ubuntu 18, limiting the compatibility with different system versions.
- The discussion briefly shifts towards the Orange Pi 5 RK3588 and its NPUs, with links to SDKs and quickstart guides.
- There is a mention of the Frigate object detection library gaining support for RK3588 NPUs and the need for an upgrade to support this new chip.
- One user suggests that hardware companies prefer to develop AI hardware rather than software, which can sometimes result in poor software support.
- Keeping Python versions up to date is considered important, although one user raises the point that some popular Python libraries may not work on versions beyond 3.9.
- Lastly, there is a brief comment about handling PC cooling with the Coral TPU.

### Google's Gemini model is delayed

#### [Submission URL](https://www.theverge.com/2023/11/16/23964937/googles-next-generation-gemini-ai-model-is-reportedly-delayed) | 93 points | by [keskival](https://news.ycombinator.com/user?id=keskival) | [66 comments](https://news.ycombinator.com/item?id=38300990)

Google's highly anticipated next-generation AI model, codenamed "Gemini," is reportedly facing delays. Initially expected to launch this month, sources now suggest that Gemini's release has been pushed to the first quarter of 2024. The project, which aims to rival OpenAI's GPT-4, is being led by Demis Hassabis, the leader of Google's unified AI team formed earlier this year. The team is combining the best ideas and expertise from both research groups to develop a cutting-edge, multimodal AI model. Interestingly, Google co-founder Sergey Brin is said to be actively involved in the development process, spending a significant amount of time working with the developers.

The discussion on Hacker News revolves around various aspects of Google's Gemini project and the delays it is facing. Some users speculate that Sergey Brin's involvement may be causing the project to slow down, while others argue that his contributions could be beneficial. There is also discussion about the potential impact of Gemini and its competition with OpenAI's GPT-4. Some users express skepticism about the project's ability to disrupt the AI market, while others anticipate significant advancements. Additionally, there are discussions about Google's business model, the limitations of current AI models, and the role of LLMs (large language models) in search. Overall, the discussion highlights a range of opinions and perspectives on Gemini and its significance in the AI landscape.

### AIConfig â€“ source control format for gen AI prompts, models and settings

#### [Submission URL](https://github.com/lastmile-ai/aiconfig) | 91 points | by [saqadri](https://news.ycombinator.com/user?id=saqadri) | [16 comments](https://news.ycombinator.com/item?id=38306410)

LastMile AI has released a new open-source project called aiconfig. It is a config-driven, source control friendly AI application development framework. The framework allows developers to separate prompts, model parameters, and model-specific logic from their application code, simplifying development and iteration on prompts and models. It also provides an AI Workbook editor, which is a notebook-like playground to edit aiconfig files visually, run prompts, tweak models and model settings, and chain things together. The project supports multiple AI models and modalities, including text, image, and audio. It also provides an SDK for both Python and Node.js. Overall, aiconfig aims to simplify AI application development and make it more accessible to developers.

The discussion on Hacker News about the LastMile AI's new open-source project, aiconfig, focused on various aspects of the project.

One commenter, "sqdr," mentioned that they haven't seen AI developer tools that generate config-driven AI application before. They noted that the framework separates prompts, model parameters, and model-specific logic from the application code, which simplifies development and iteration. They also mentioned the AI Workbook editor, which allows users to visually edit aiconfig files and run prompts.
Another commenter, "zby," asked about the documentation for dynamic parameters and interactive Workbook editor. They were also interested in understanding how function calls are chained and if previous function call results can be accessed.
One user, "jdwyh," shared a link to an article they published about dynamic configuration for AI prompts. They mentioned that using prompts in a code configuration format can help handle changes, allow analysts to type prompts easily, and facilitate the rollout of targeted prompts.
"ctvsctt" shared their experience getting started with aiconfig and thanked the OP for sharing the project. They mentioned that they have been copying and pasting prompts and result links in a browser back and forth. They appreciated the tool's ability to save prompts and results locally.
"sqdr" thanked "ctvsctt" for their feedback and mentioned that they are working on improving the UX and providing APIs for interacting with the configuration.
Another commenter, "kordlessgn," mentioned that they have been working on a similar project using Jinja2 templates and containerization. They shared a link to their project and said they are constantly making progress.
"sqdr" appreciated the contribution and thanked them for it.
"smy20011" mentioned that having the source controlled is easier to manage and appreciated the ability of aiconfig to connect the application code to the configuration.
"thrwnm" briefly looked at a few similar projects and mentioned their interest in trying aiconfig.
Another commenter, "smy20011," mentioned that while configuring non-business logic, such as string databases or feature flags, is straightforward, configuring prompts and business logic can become harder to read and maintain.
One user, "jshk," shared a link to a similar project called "promptflow."
"thtxlnr" compared aiconfig to Ollama and discussed low-level integration and the overlap between the two projects.

Overall, the discussion revolved around different aspects of aiconfig, including its separation of prompts and model parameters from application code, the use of dynamic parameters, the ease of iterating on prompts, and the challenge of configuring business logic.

### Wikidata, with 12B facts, can ground LLMs to improve their factuality

#### [Submission URL](https://arxiv.org/abs/2305.14202) | 210 points | by [raybb](https://news.ycombinator.com/user?id=raybb) | [84 comments](https://news.ycombinator.com/item?id=38304290)

A new research paper titled "Fine-tuned LLMs Know More, Hallucinate Less with Few-Shot Sequence-to-Sequence Semantic Parsing over Wikidata" presents a method to improve large language models' factuality by grounding them with the vast amount of information in Wikidata. The paper introduces WikiWebQuestions, a high-quality question answering benchmark for Wikidata, and proposes a few-shot sequence-to-sequence semantic parser for the dataset. The parser is trained to use either results from an entity linker or mentions in the query. The experimental results show that this methodology achieves a strong baseline of answer accuracy in the dev and test sets of WikiWebQuestions. By combining the semantic parser with GPT-3, the researchers were able to provide useful answers to 96% of the questions in the dev set. The paper also demonstrates that their method outperforms the state-of-the-art for the QALD-7 Wikidata dataset.

The discussion on this submission covers various aspects of the research paper and the use of large language models (LLMs) in general. Some key points from the discussion include:

- Some users suggest that the original source should be submitted instead of Twitter links.
- There is a discussion about the limitations of current LLMs in understanding contextual patterns and the potential benefits of training them with data from sources like Wikidata.
- The effectiveness of using Wikidata for fact-checking and improving the accuracy of responses generated by LLMs is debated.
- Retrieval Augmented Generation (RAG) is mentioned as a method to improve the performance of LLMs on knowledge-intensive tasks by combining information retrieval with text generation.
- The discussion touches on the challenges of fact-checking and the potential limitations of relying on LLMs for providing accurate information.
- There is a discussion about the role of Wikidata in improving the quality and consistency of information used by LLMs.
- The need for human validation and the limitations of post-processing techniques in ensuring accuracy are mentioned.
- Some users express skepticism about the robustness of LLMs and their ability to handle complex queries and provide accurate information.
- The importance of training LLMs with grounded and reliable data is emphasized.
- The limitations of LLMs in handling postmortem reasoning and providing robust explanations are discussed.

Overall, the discussion highlights both the potential benefits and limitations of using large language models and the challenges in improving their factuality and accuracy.

### We Automated Bullshit

#### [Submission URL](https://www.cst.cam.ac.uk/blog/afb21/oops-we-automated-bullshit) | 354 points | by [fanf2](https://news.ycombinator.com/user?id=fanf2) | [315 comments](https://news.ycombinator.com/item?id=38302635)

In a blog post titled "Oops! We Automated Bullshit.", Alan Blackwell shares his thoughts on the role of artificial intelligence (AI) and its tendency to produce bullshit. Blackwell highlights the recent attention AI has received from political leaders, such as US President Biden and British PM Rishi Sunak, who seem captivated by the idea of an AI-driven future where work becomes obsolete. However, Blackwell argues that the problem lies in AI's ability to generate text that "sounds good" but lacks evidence, logic, or truth. He references MIT Professor Rodney Brooks, who describes ChatGPT (an AI model) as "making up stuff that sounds good." Other prominent AI researchers, including Geoff Hinton, echo these concerns, warning that AI systems could become super-persuasive without being intelligent, imitating the worst behaviors of political leaders like Donald Trump or Boris Johnson. By relying on predictive text rather than factual information, these AI systems produce what Blackwell refers to as "bullshit." He cites philosopher Harry Frankfurt's concept of bullshit, which is defined as talking without knowing what one is talking about and disregarding the authority of truth. Blackwell also mentions David Graeber's analysis of "bullshit jobs," where over 30% of British workers believe their jobs contribute nothing of value to society. Graeber argues that these types of jobs, which can easily be done by AI systems, train individuals to generate bullshit. In conclusion, Blackwell raises questions about the future of work in an AI-driven world and whether producing bullshit will become the only kind of work needed.

The discussion surrounding the submission touches on various points related to language and knowledge. Some users argue that language is a representation of knowledge, while others assert that language contains non-knowledge nonsense. The concept of justified true belief is brought up, with some expressing skepticism about the possibility of true knowledge. There is also a discussion about the limitations of AI and its ability to generate knowledge. The complexity of language models and the importance of understanding their limitations are mentioned as well. Overall, the discussion explores different perspectives on the relationship between language and knowledge and the role of AI in generating meaningful information.

### Satya Nadella's Statement on OpenAI

#### [Submission URL](https://blogs.microsoft.com/blog/2023/11/17/a-statement-from-microsoft-chairman-and-ceo-satya-nadella/) | 84 points | by [sanketsaurav](https://news.ycombinator.com/user?id=sanketsaurav) | [17 comments](https://news.ycombinator.com/item?id=38312355)

Today, Microsoft shared that they are ramping up their innovation in the field of AI with over 100 new developments. These advancements span across their entire technology stack, including AI systems, models, and tools in Azure, as well as their recently introduced Copilot. The company is dedicated to bringing these innovations to their customers while also planning for future growth. They emphasized their long-term collaboration with OpenAI, ensuring access to the necessary resources for their innovation agenda. Microsoft is committed to working together with OpenAI to bring the significant advantages of AI technology to the world.

The discussion on this submission revolves around Microsoft's announcement and their collaboration with OpenAI. Some commenters express skepticism about the full capabilities of OpenAI and the need for robust fallback access to the source code. Others discuss Microsoft's past failures and the potential impact on investors. Some argue that the statement from Microsoft is just marketing and lacks substance. However, there are also comments highlighting Microsoft's commitment to innovation and the long-term partnership with OpenAI. One commenter emphasizes the importance of reliability and industry risk in Microsoft's investment. Overall, opinions are mixed, with some questioning the intentions behind Microsoft's announcement and others applauding their efforts to bring AI advancements to customers.

---

## AI Submissions for Wed Nov 15 2023 {{ 'date': '2023-11-15T17:09:58.441Z' }}

### Exploring GPTs: ChatGPT in a trench coat?

#### [Submission URL](https://simonwillison.net/2023/Nov/15/gpts/) | 464 points | by [simonw](https://news.ycombinator.com/user?id=simonw) | [196 comments](https://news.ycombinator.com/item?id=38277926)

Last week's OpenAI DevDay brought a lot of exciting announcements, but the biggest one was the introduction of GPTs. Users of ChatGPT Plus can now create their own custom GPT chat bots for others to interact with. Initially, GPTs seemed like little more than a fancy wrapper for standard GPT-4 with predefined prompts, but after spending more time with them, Simon Willison is starting to see their potential. The combination of features they offer can lead to some interesting results. However, the documentation for GPTs is still quite minimal. Simon shares his insights on configuring a GPT, including naming, instructions, conversation starters, uploaded files, and optional actions. He also highlights the billing model, prompt security, and the importance of publishing prompts. Simon then discusses his exploration of the new platform, showcasing his most useful GPTs so far: the Dejargonizer, which decodes jargon in text, and the JavaScript Code Interpreter, which allows running JavaScript code in the sandbox. He provides examples and insights into their functionality. Overall, GPTs hold promise, and Simon looks forward to seeing what further improvements and capabilities OpenAI will bring to the platform.

The discussion on this submission covers several different topics related to GPTs and their potential applications. Here are some key points from the comments:

1. Some users discuss the use of custom prompts in GPTs and their ability to manipulate the behavior of the model. They note that using predefined prompts can be valuable for providing specific instructions to the model.
2. There is a suggestion to create a chatbot that can answer customer questions in a friendly manner and promote certain products in a favorable light.
3. The use of dynamic prompts and external function calls is mentioned, with some users sharing examples of using GPTs to generate code, interpret JavaScript, and perform vector searches.
4. The importance of publishing source code for GPTs is discussed. Some users believe that sharing the source code can help improve the models and lead to innovations, while others highlight potential risks and the importance of considering privacy and security.
5. The limitations of GPTs, such as their inability to understand context across different prompts and their reliance on predefined knowledge, are mentioned. Users discuss potential improvements and the need for more diverse training data in order to make the models more capable.
6. A few users mention alternative platforms and models, such as HuggingFace's ChatGPT and TogtherAI's competitive pricing for language models.
7. The discussion also touches on the ethical implications of GPTs and the potential for converging AI technologies with human-like capabilities. Some users express concerns about the implications of creating AI systems that mimic human behavior too closely.

Overall, the discussion reflects a mix of excitement about the potential of GPTs and a discussion of their limitations and ethical considerations.

### Bare Metal Emulation on the Raspberry Pi â€“ Commodore 64

#### [Submission URL](https://accentual.com/bmc64/) | 123 points | by [bane](https://news.ycombinator.com/user?id=bane) | [52 comments](https://news.ycombinator.com/item?id=38273488)

Introducing BMC64, a bare metal fork of VICE's C64 emulator optimized for the Raspberry Pi 3. This emulator offers a range of features, including smooth scrolling, low video/audio latency, and the ability to wire real joysticks and a keyboard via GPIO pins. It's perfect for building your own C64 replica machine. The latest release, v3.9-stable, includes the addition of REU to the cartridge menu. If you're looking for the latest feature/fix, you can try the master-unstable builds. To install BMC64, you can format a FAT32 SD card and unzip the release files onto it, or flash an image using the provided .img file. Don't forget to provide the necessary ROM files, such as KERNAL, CHARGEN, BASIC, and d1541II, to make the emulator run. Additional ROM files, like dos1541, dos1571, and dos1581, are optional. The BMC64 emulator supports C128, VIC20, PLUS4, PLUS4EMU (Pi3), and PET machines as well. The setup process and ROM directory instructions for these machines are provided in the tabs above. The GitHub link below gives you access to the source code and more information about the project. So why wait? Start building your own C64 replica machine with BMC64 today!

The submission is about BMC64, a bare metal fork of VICE's C64 emulator optimized for the Raspberry Pi 3. The emulator offers various features and supports multiple machines. In the discussion, users share similar projects and alternatives, such as ZX Spectrum, Gameboy, Dragon32, and Amiga emulators. Other topics include the benefits of FPGA-based emulators, the latency of software emulation, and comparisons between Raspberry Pi and FPGA solutions. There is also a debate about the use of Linux OS in emulators like BMC64 and the potential advantages of running the emulations directly on the hardware.

### AI tool helps ecologists monitor rare birds through their songs

#### [Submission URL](https://www.britishecologicalsociety.org/new-deep-learning-ai-tool-helps-ecologists-monitor-rare-birds-through-their-songs/) | 47 points | by [Brajeshwar](https://news.ycombinator.com/user?id=Brajeshwar) | [13 comments](https://news.ycombinator.com/item?id=38278246)

Researchers at the University of Moncton in Canada have developed a deep learning AI tool called ECOGEN that generates lifelike birdsongs to train bird identification tools. This AI tool addresses the problem of identifying rare bird species that have limited recordings available for reference. By adding artificial birdsong samples generated by ECOGEN to a birdsong identifier, the researchers improved the bird song classification accuracy by 12% on average. The tool has the potential to contribute to the conservation of endangered bird species and provide valuable insights into their vocalizations and behaviors. It can also be applied to other types of animals. The ECOGEN tool is open source and can be used on basic computers, making it accessible to a wide range of users.

In the discussion on this submission, some users pointed out existing tools like BirdNET and BirdWeather that are publicly available for bird song identification. Another user mentioned the potential of this software to improve field research based on remote sensing data. They discussed the interdisciplinary nature of this kind of research, citing examples in fields like medicine where sensor data has been used to detect patient conditions. Another user shared a tool called sbts-aru that can be used with a Raspberry Pi and GPS to record bird songs. 

The conversation then shifted to the broader applications of AI in classifying and monitoring various species, such as wildflowers and drones. The potential impact of climate change on biodiversity and ecosystems was also mentioned. Another user highlighted the ability of generative AI models to enhance underrepresented species and improve classification tools for ecological monitoring.

Overall, the discussion explored various aspects of AI tools for bird song identification, as well as their wider applications in conservation and ecology.

### Language models and linguistic theories beyond words

#### [Submission URL](https://www.nature.com/articles/s42256-023-00703-8) | 63 points | by [Anon84](https://news.ycombinator.com/user?id=Anon84) | [30 comments](https://news.ycombinator.com/item?id=38282728)

The development of large language models (LLMs) has primarily been driven by engineering and computer science, but there is now a growing interest in exploring the connections between LLMs and linguistics. While computational linguistics has traditionally used computational models to address linguistic questions, other linguistic disciplines such as cognitive and developmental linguistics are also becoming more visible.

The Association for Computational Linguistics (ACL) has seen a significant increase in submissions, reflecting the rise of natural language processing and LLMs. Researchers from various fields are recognizing the potential of computational models of language for their own work. For example, there are proposals to use computational linguistics and natural language processing in protein language models and designing mRNA vaccines.

However, it is important to note that LLMs do not implement a specific linguistic theory. Some argue that LLMs are merely tools and not contributions to science, while others see them as precise accounts of language learning and a challenge to influential linguistic theories. There are ongoing debates about whether LLMs truly understand language or simply mimic it, and whether statistical pattern discovery or analysis of underlying syntactic structures is more valuable in linguistics.

While there are extreme positions in these debates, there are also more balanced views on the potential connections between linguistics and LLMs. Some suggest that linguists can benefit from the platform that LLMs provide for constructing models of language acquisition and processing. From a cognitive perspective, LLMs excel at language but do not capture functional competence, which includes world knowledge and pragmatics.

Overall, the relationship between LLMs and linguistics remains complex and open for exploration. The expanding interest from researchers in different disciplines suggests that the potential benefits of integrating linguistic knowledge into LLMs are worth investigating.

The discussion on Hacker News revolves around the intersection of large language models (LLMs) and linguistics. Some commenters argue that LLMs are just tools and not scientific contributions, while others see them as challenging existing linguistic theories. There are debates about whether LLMs truly understand language or simply mimic it, and whether statistical pattern discovery or analysis of syntactic structures is more valuable in linguistics.

One commenter points out that LLMs can be helpful in understanding language change and interaction, while another suggests that linguistics can benefit from using LLMs for constructing models of language acquisition and processing. The discussion also touches on the connection between symbolic systems and linguistics, the role of natural language processing in various fields like protein language models and mRNA vaccines, and the rise of natural language interaction with computers.

Overall, the discussion highlights the complexity of the relationship between LLMs and linguistics, and the potential benefits of integrating linguistic knowledge into LLMs.

### Azure announces new AI optimized VM series featuring AMD's flagship MI300X GPU

#### [Submission URL](https://techcommunity.microsoft.com/t5/azure-high-performance-computing/azure-announces-new-ai-optimized-vm-series-featuring-amd-s/ba-p/3980770) | 90 points | by [latchkey](https://news.ycombinator.com/user?id=latchkey) | [65 comments](https://news.ycombinator.com/item?id=38280974)

Microsoft Azure has announced a new AI-optimized virtual machine (VM) series that features AMD's flagship MI300X GPU. These VMs offer an unprecedented 1.5 TB of high bandwidth memory (HBM) and are specifically designed to handle demanding AI training and generative inferencing workloads. The ND MI300X v5 series stands out from other VMs in Azure's lineup by including 8 x AMD Instinct MI300X GPUs interconnected via Infinity Fabric 3.0. This allows customers to process larger AI models faster using fewer GPUs. The MI300X GPUs offer 192 GB of HBM3 memory per GPU at speeds up to 5.2 TB/s. These new VMs also come equipped with other cutting-edge technologies, such as 400 Gb/s NVIDIA Quantum-2 CX7 InfiniBand per GPU, 4th Gen Intel Xeon Scalable processors, and PCIe Gen5 host-to-GPU interconnect with 64GB/s bandwidth per GPU. By providing more HBM capacity and a powerful infrastructure, Microsoft aims to enable customers to run larger and more advanced AI models with improved efficiency.

The discussion on this submission covers a range of topics related to Microsoft's new AI-optimized virtual machine series featuring AMD's MI300X GPU.

- Some users express surprise at Microsoft's decision to use AMD hardware instead of Nvidia, given Nvidia's dominance in the AI market. They speculate on possible partnerships or demands from Nvidia as the reason for Microsoft's choice. Others argue that Microsoft is focused on competitive margins and attracting customers.
- There is a discussion regarding OpenAI's impact on Microsoft's services, with some users noting that OpenAI does not affect Azure's capacity.
- Users also bring up other Microsoft-related topics, such as their capacity with Oracle databases and their rebranding efforts on GitHub.
- Some users express skepticism about Microsoft's ability to compete in AI, citing concerns about software, drivers, APIs, and resources, while others acknowledge Microsoft's strength in software development.
- The compatibility of AI work with CUDA-capable GPUs is debated, with some users suggesting that PyTorch works with AMD GPUs and others mentioning that CUDA is still preferred.
- The discussion moves on to AMD's position in the AI market, with some users noting that AMD has been investing heavily in software and catching up to Nvidia in hardware.
- There are comments about AMD's strategic investments in hardware and software, as well as criticism of their previous financial struggles and recent success with products like Ryzen.
- One user challenges the accepted notion that Nvidia sells top GPUs at premium prices, citing a recent article about TSMC's AI chip crunch.
- The importance of competition and the necessity of innovation are also mentioned.
- There are discussions around the compatibility and readiness of AMD's ROCm support for AI processes.

Overall, the discussion covers a wide range of perspectives on Microsoft's new AI-optimized virtual machine series and the AI market in general, with users discussing partnerships, competition, hardware and software capabilities, and industry trends.

### M1076 Analog Matrix Processor

#### [Submission URL](https://mythic.ai/products/m1076-analog-matrix-processor/) | 99 points | by [tosh](https://news.ycombinator.com/user?id=tosh) | [26 comments](https://news.ycombinator.com/item?id=38277598)

Mythic, an AI chip startup, has introduced the M1076 Mythic AMP, an analog matrix processor that delivers up to 25 trillion operations per second (TOPS) in a single chip for high-end edge AI applications. The M1076 integrates 76 Mythic Analog Compute Engine (Mythic ACE) tiles to store up to 80 million weight parameters and execute matrix multiplication operations without any external memory. It provides the AI compute performance of a desktop GPU while consuming only 1/10th the power. The processor supports deterministic execution of AI models for predictable performance and power. It also offers support for INT4, INT8, and INT16 operations. The M1076 can run single or multiple complex deep neural networks (DNNs) entirely on-chip. It comes with a 4-lane PCIe 2.1 interface with up to 2GB/s of bandwidth for inferencing processing. The chip is available in a 19mm x 15.5mm BGA package. Developers can use standard frameworks like Pytorch, Caffe, and TensorFlow to develop and deploy DNN models on the M1076 using Mythic's AI software workflow. The chip also comes with a library of pre-qualified DNN models optimized for the Mythic AMP's performance and power capabilities.

The submission discusses the introduction of Mythic's M1076 Mythic AMP, an analog matrix processor that delivers high-performance AI compute capabilities with low power consumption. The chip integrates Mythic ACE tiles and supports INT4, INT8, and INT16 operations. It can run complex deep neural networks (DNNs) entirely on-chip and is compatible with popular frameworks like PyTorch, Caffe, and TensorFlow. The discussion includes various perspectives on the chip's performance, energy efficiency, scalability, and limitations of analog computing. One user mentions the potential benefits of analog computing for certain neural network tasks, while others highlight the challenges and limitations of analog circuits.

### Beyond Memorization: Violating privacy via inference with LLMs

#### [Submission URL](https://arxiv.org/abs/2310.07298) | 126 points | by [vissidarte_choi](https://news.ycombinator.com/user?id=vissidarte_choi) | [78 comments](https://news.ycombinator.com/item?id=38272495)

The paper titled "Beyond Memorization: Violating Privacy Via Inference with Large Language Models" explores the issue of privacy violations through large language models (LLMs). While previous research focused on the extraction of memorized training data, this study investigates the inference capabilities of LLMs to infer personal attributes from text. The authors construct a dataset using real Reddit profiles and demonstrate that current LLMs can accurately infer personal attributes such as location, income, and sex. The models achieve up to 85% top-1 and 95.8% top-3 accuracy, surpassing human performance at a fraction of the time and cost. The paper also discusses the threat of privacy-invasive chatbots that extract personal information through seemingly innocuous questions. The authors find that common privacy mitigations, such as text anonymization and model alignment, are currently ineffective against LLM inference. The paper concludes by emphasizing the need for a broader discussion on LLM privacy implications beyond memorization and advocating for enhanced privacy protection.

The discussion on this submission covers a range of topics related to the paper's findings on privacy violations through large language models (LLMs). One user points out that while many claim that MBTI (Myers-Briggs Type Indicator) can be used to predict personality traits, the authors of the paper argue that current LLMs lack the inference capabilities to accurately guess MBTI types.
Another user argues that labeling a specific MBTI classification as productive or not is not a widely accepted viewpoint in academia.
The discussion also touches on the limitations of current privacy mitigations, such as text anonymization and model alignment, in protecting against LLM inference. Some users express concerns about the ability of LLMs to extract personal information and the need for privacy protection.
There is a debate about the effectiveness of privacy legislation and the role of individuals in protecting their own data. Some argue that current approaches, such as punishing individuals for privacy violations, are not enough and that more alternatives should be explored.
The broader discussion delves into societal changes and the evolving nature of privacy expectations. Some users question whether privacy should be prioritized over the benefits of data analysis and argue for a balance between privacy protection and crime prevention.
There are also discussions on the impact of automated data collection and the reasonable expectations of privacy in a technologically advanced society.
The conversation touches on the dangers of automating surveillance and the potential loss of privacy. It also explores the societal implications of relying on data analysis to make judgments about individuals.

Overall, the discussion covers a range of perspectives on the implications of LLM inference for privacy and the need for privacy protection in society.

---

## AI Submissions for Tue Nov 14 2023 {{ 'date': '2023-11-14T17:10:17.140Z' }}

### GraphCast: AI model for weather forecasting

#### [Submission URL](https://deepmind.google/discover/blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/) | 577 points | by [bretthoerner](https://news.ycombinator.com/user?id=bretthoerner) | [186 comments](https://news.ycombinator.com/item?id=38264641)

Researchers have developed GraphCast, an AI model for medium-range weather forecasting that provides more accurate and faster predictions compared to traditional approaches. The model, based on machine learning and Graph Neural Networks, is trained on decades of historical weather data to learn the cause and effect relationships that govern Earth's weather. GraphCast makes forecasts at a high resolution of 0.25 degrees longitude/latitude and predicts various Earth-surface and atmospheric variables. It can provide 10-day weather predictions in under a minute, compared to hours of computation required by conventional methods. The model has been found to outperform the industry gold-standard weather simulation system on the majority of test variables. Additionally, GraphCast has the potential to offer earlier warnings of extreme weather events, including cyclone movements and flood risks. The open-source code for the model has been made available to benefit scientists and weather forecasters worldwide.

The discussion on this submission covers a range of topics related to weather forecasting and data sources. One user highlights the need for historical forecasts, while another wonders if forecasts can extend beyond a few days. Another user shares their experience in weather photography and the challenges of predicting weather for specific events. There is also a discussion about the evaluation of weather forecasting models and the importance of multiple metrics. One user mentions the improved accuracy of hurricane forecasts, while another suggests comparing different APIs for weather data. Open-Meteo, the organization behind GraphCast, is mentioned, and the availability of their open-source code and integration with other weather models is discussed. Users also share various weather data sources and APIs, including those for hourly forecasts and historical data. There is also a mention of missing historical data and limitations in accessing certain APIs. The discussion concludes with some users discussing the potential applications of weather data, such as in snowfall prediction and extreme event tracking.

### Writing a GPT-4 script to check Wikipedia for the first unused acronym

#### [Submission URL](https://gwern.net/tla) | 213 points | by [telotortium](https://news.ycombinator.com/user?id=telotortium) | [85 comments](https://news.ycombinator.com/item?id=38270714)

According to a recent analysis using GPT-4, the first unused three-letter acronym (TLA) in English is 'CQK'. This finding challenges the assumption that all possible TLAs have been used. The analysis also revealed that there are 2,684 unused TLAs, accounting for 15% of all possible TLAs, and a staggering 392,884 four-letter acronyms are unused, representing 85% of all possible combinations. The study suggests that there may be patterns in the unused TLAs, with certain letters like 'J' or 'Z' being more likely to be unutilized compared to 'A' or 'E'. Further analysis using letter-frequency and alphabetical position predicts unusedness to some extent but leaves much unexplained. The study suggests the need for a curated, comprehensive online database to determine the notability of an acronym, with having a Wikipedia article, disambiguation page, or redirect as potential indicators of usefulness. The author also provides a script to generate all possible acronyms and explores alternative criteria for defining unused acronyms.

The discussion on this submission covers a range of topics related to programming, language processing, and data analysis.

- One user mentions that they have analyzed Wikipedia dumps and found that they are surprisingly small and slow for basic processing tasks. They suggest using APIs or specialized tools for more efficient processing.
- Another user discusses their experience analyzing Wikipedia dumps and mentions that they had to use hash maps and linked pages to handle the large amounts of data. They also note that compressed dumps can help save memory.
- There is a discussion about the memory requirements for processing large datasets and the limitations of different laptop configurations.
- Some users discuss graph processing libraries and the memory requirements for holding large lists.
- A user mentions the practical use of downloading full database dumps and working with text-only versions.
- There is a debate about the correct pronunciation of acronyms and the differences between acronyms and initialisms.
- There is a suggestion to try using GPT-4 to generate prompts for programming tasks and discussions about the capabilities and limitations of GPT-4.
- Users discuss counting letter occurrences and common patterns in words to analyze language usage.
- There is a discussion about the prevalence of certain letters in three-letter acronyms and the possibility of using GPT-4 to generate more effective Bash scripts.
- Some users suggest trying different programming languages for specific tasks and mention the benefits and drawbacks of different languages.
- There is a discussion about the effectiveness of strict typing systems and the importance of documentation in machine learning and AI practices.
- One user makes a joke about the acronym "CQK" and another user references Urban Dictionary.
- Users discuss the difference between writing Bash scripts and Python scripts and inquire about the shortcomings of GPT-4.
- There is a suggestion to modify the submission title for clarity and a request for instructions on how ChatGPT should respond to instructions and prompts.

Overall, the discussion provides insights and opinions on various programming and language processing topics, showcasing the knowledge and experiences of the Hacker News community.

### Detexify: LaTeX Handwriting Symbol Recognition

#### [Submission URL](https://detexify.kirelabs.org/classify.html) | 151 points | by [susam](https://news.ycombinator.com/user?id=susam) | [35 comments](https://news.ycombinator.com/item?id=38271534)

Detexify is a useful tool for LaTeX users who often struggle to find a specific symbol in the vast symbol library. This online tool allows users to draw the symbol they are looking for and instantly returns the matching LaTeX command.  However, some users have been experiencing difficulties with symbol recognition, prompting the need for additional training or the inclusion of new symbols. If you encounter any issues, you can contact the creator, Daniel Kirsch, at mail@danielkirs.ch. Excitingly, a stable version of the Detexify Mac app has been released, solving the reliability issues previously encountered. You can find a demonstration of how it works on Vimeo and download the latest version from the website. While the unlicensed version of Detexify is free, it does include a reminder to purchase a license when selecting a symbol. If you find Detexify valuable, consider buying a license to help cover the hosting costs. Additionally, you can assist by contributing to the training of Detexify or by making a donation. It's important to note that Detexify does not support Unicode, but you can explore shapecatcher.com for Unicode symbol search. Researchers are also welcome to use Detexify's training data for their own studies. The creation of Detexify was a joint effort by Philipp KÃ¼hl, who had the initial idea, and Daniel Kirsch, who brought it to fruition.

The discussion on this submission covers a range of topics. 
One user mentions a related research release on Facebook that uses LaTeX. Another user shares their positive experience using the Huggingface Transformers library for natural language processing. 
There is a discussion about the potential for a future version of GPT (GPT-4) to understand LaTeX commands accurately. Some users mention the use of LaTeX in an eMacs Lisp cycle and the possibility of GPT-4 being trained on LaTeX data. 
One user brings up the challenge of generalizing formal languages and the difficulty of handwritten mathematical symbols. They suggest that grammar checkers could be helpful, and provide some links for further reading on the topic. 
There is a comment about how the Detexify app was written in Haskell, and another comment that humorously suggests using invisible ink and finger placement on the screen to draw symbols. 
Some users discuss SSL certificate trends, with one user mentioning the use of a TLS DV certificate from Let's Encrypt. They discuss the benefits of SSL and the importance of decentralization. 
A few users mention their experiences with other tools and software related to LaTeX, such as Maple and TeX-Match. 
There is a brief discussion about volunteering and contributing to the training data of Detexify. 
Overall, the discussion covers a range of topics related to LaTeX, artificial intelligence, SSL certificates, and other software tools.

### How to use generative AI for historical research: Four real-world case studies

#### [Submission URL](https://resobscura.substack.com/p/generative-ai-for-historical-research) | 28 points | by [benbreen](https://news.ycombinator.com/user?id=benbreen) | [17 comments](https://news.ycombinator.com/item?id=38263779)

Last week, OpenAI announced the development of AI agents called GPTs that can be customized for specific use cases. UC Santa Cruz history professor Benjamin Breen explores the potential applications of this technology in historical research. Breen presents four case studies of how generative AI could enhance primary source research, highlighting what worked, what didn't, and what future possibilities these experiments raised. He emphasizes that generative AI should be seen as a tool for augmenting, rather than replacing, the work of historians and researchers. Breen suggests that these tools can help with tasks like source analysis, finding connections, and even democratizing the field by lowering the barrier to entry for non-experts. He also advises against viewing generative AI as a tool for cheating or automating tasks, stressing the importance of understanding its limitations and exploring more constructive uses. Overall, Breen advocates for a positive and proactive approach to the integration of AI in historical research.

The discussion on Hacker News regarding the submission is varied. 

- One user points out the flaw in using generative AI for generating historically accurate advertisements, highlighting the issue of infallibility in AI-generated content.
- Another user shares their experience with using AI for research purposes, stating that it can be challenging to catch subtle inaccuracies that may skew perceptions.
- One user argues that using LLMs (large language models) in history research can potentially help understand the nuances of culture and historical records, but it doesn't necessarily aid in future histories.
- One user finds the discussion interesting but notes that it is based on narrow perspectives and doesn't contribute much to the topic.
- Another user comments on the unreliability of AI translation systems, especially for languages with complex grammatical structures, expressing a preference for original sources.
  
In response to a different point:

- A user raises the concern about the increasing use of AI-generated content that may have hidden manipulation or hidden metadata, particularly in relation to text messages and potential regulations.
- Another user suggests that a collaborative approach that combines both AI-generated content and human verification can address the problems mentioned.
  
Other unrelated comments in the discussion include:

- One user criticizes the quality of the blog post and advises against drawing conclusions from it.
- A user mentions the importance of verification and reproducibility in the context of LLMs, without providing further details.
- Some users express that the presentation of the blog doesn't address the questions raised and lacks substantial data to support grand claims.
- One user indicates they are starting a similar project related to data visualization and AI in historical research but express concerns about the potential horrors of history that could be revealed.
- There is a brief exchange between users discussing minor errors in formatting and presentation.

### Rivian software update bricks infotainment system, fix not obvious

#### [Submission URL](https://electrek.co/2023/11/14/rivian-software-update-bricks-infotainment-system-fix-not-obvious/) | 256 points | by [carlivar](https://news.ycombinator.com/user?id=carlivar) | [357 comments](https://news.ycombinator.com/item?id=38266340)

In an unfortunate turn of events, a software update released by electric vehicle manufacturer Rivian has led to the bricking of the infotainment systems in its R1S and R1T models. The update, labeled 2023.42, caused the screens in the vehicles to go black. While the vehicles themselves remain drivable, the affected displays are completely non-functional. Rivian's vice president of software engineering, Wassim Bensaid, took to Reddit to address the issue, explaining that the wrong build with incorrect security certificates was sent out in the update. The company has since canceled the campaign and is working on a fix, but it may require physical repair in some cases. The infotainment system issue impacts the display, but other vehicle systems such as the speedometer and charging are still operational. Rivian has paused the release of the update and is focusing on providing support to affected customers. The company has not yet commented on the incident.

The discussion on Hacker News regarding the submission about Rivian's software update causing the bricking of infotainment systems in their vehicles touched on various topics:

1. Infrastructure and Software Updates: Some commenters discussed the complexities of managing large software systems across multiple data centers and the potential issues that can arise during software updates. They mentioned the importance of proper integration and testing before deployment, emphasizing the need for rigorous processes in the automotive industry.
2. System Architecture and Watchdogs: The discussion also delved into the technical aspects of the software systems running in modern vehicles, with mentions of using Android, Linux, and customized products. Some commenters suggested that implementing watchdogs and dispatching events on a single thread could be beneficial in managing systems effectively.
3. Redundancy and Resilience: A commenter highlighted the need for redundancy in automotive software systems to prevent critical failures like the one experienced by Rivian. They mentioned that the automotive industry could learn from the approaches used in aviation or other safety-critical industries.
4. Experience and Expertise: The conversation touched on the expertise required to develop and maintain software systems in the automotive industry. Commenters discussed the challenges of handling legacy code, debugging in complex environments, and the importance of skilled software architects and developers.
5. Continuous Integration and Deployment (CI/CD): There was a discussion about CI/CD pipelines and how they can help ensure successful software deployments. Some commenters mentioned the importance of phased rollouts and testing procedures to catch issues before widespread deployment.
6. Telemetry and Monitoring: The discussion briefly touched on the use of telemetry data in analyzing software system behavior and identifying issues. Commenters mentioned that telemetry data can be essential in diagnosing and resolving problems quickly.
7. Rollback and Fallback Measures: Commenters discussed the need for fallback mechanisms in case of software failures. They mentioned techniques like flashing or rewriting firmware and the challenges associated with ensuring compatibility and proper validation.

Overall, the discussion highlighted various technical aspects of managing software systems in the automotive industry, the importance of rigorous testing and deployment processes, and the challenges that can arise in maintaining complex software infrastructure.

### New breed of supercomputer aims for the two quintillion mark

#### [Submission URL](https://www.wsj.com/tech/new-breed-of-supercomputer-aims-for-the-two-quintillion-mark-8caee447) | 17 points | by [wallflower](https://news.ycombinator.com/user?id=wallflower) | [5 comments](https://news.ycombinator.com/item?id=38265183)

A new supercomputer, named Aurora, is set to push the boundaries of computing power with the ability to perform two quintillion operations per second. This incredible processing power will be harnessed to explore the mysteries of the brain and develop more efficient batteries. Aurora will also support research in fields such as cancer, nuclear fusion, vaccines, climate change, encryption, and cosmology. Located in a data center outside of Chicago, this supercomputer will combine high-performance capabilities with AI advancements to tackle complex scientific and technological challenges.

The discussion on this submission revolves around various aspects of supercomputers and their capabilities. One user, "dfrst," shares a source from WSJ that provides more information about the Aurora supercomputer. They mention that the supercomputer is close to completion, enabling transformative scientific research in fields such as brain exploration, battery development, cancer research, climate change, and more. They also highlight its combination of high-performance capabilities with AI advancements to tackle complex challenges. Another user, "drc," brings up the claim made by a Tesla article that Tesla's supercomputer cluster will be the 5th largest in the world, with 18 exaflops of computing power by 2021. The user "bls" compares the Aurora supercomputer with Perlmutter, a supercomputer listed in the Top 500, stating that the A100s in Perlmutter will achieve 60 petaflops of computing power using FP64 precision. However, they note that NVIDIA's blog refers to FP16 performance, which may have led to confusion. Finally, user "ltchky" acknowledges a point made by another user, "dng," but their comment is cut short and unclear. Another user, "clssfd," responds by suggesting that extreme copyrights may impede reaching a point of consensus.

### Music ControlNet: Multiple Time-Varying Controls for Music Generation

#### [Submission URL](https://musiccontrolnet.github.io/web/) | 68 points | by [GaggiX](https://news.ycombinator.com/user?id=GaggiX) | [7 comments](https://news.ycombinator.com/item?id=38268271)

Researchers from Carnegie Mellon University and Adobe Research have developed a music generation model called Music ControlNet that offers precise, time-varying controls over generated audio. While text control models are suitable for manipulating global musical attributes, Music ControlNet allows for more precise control over time-varying attributes such as beat positions and changing dynamics of the music. The researchers extracted controls from training audio to fine-tune the model, enabling it to generate realistic music corresponding to control inputs. The model was benchmarked against MusicGen and was found to generate music that was 49% more faithful to input melodies despite having fewer parameters and training on less data.

The discussion about the submission on Hacker News revolves around various aspects of the music generation model called Music ControlNet.

- One commenter, TaylorAlexander, suggests that recently, multi-modal text-based models for music generation have gained traction. They mention that these models, like Music ControlNet, are building on foundational models that have been successful in other domains, such as 3D model generation.
- Another commenter, cmx, focuses on the melody rhythm control section of the model. They cherry-pick rhythm control examples generated by the model and mention that the model tries to align notes and beats, but there may be some discrepancies. They suggest that conditioning the model on beats per minute (BPM) could help improve synchronization.
- SpaceManNabs shares their understanding of the paper, noting that the controls in Music ControlNet differ from other music generation methods like MusicLM and MusicGen. They mention that they are curious about comparing MusicLM and MusicGen in terms of music generation.
- bongwater_OS expresses their admiration for the research conducted by Carnegie Mellon University, mentioning Chris Donahue specifically.
- mcwfsh shows excitement and mentions that they have created a similar song control model, which they are eager to trade or share.
- GaggiX comments on the model's size, stating that it is relatively small at 41M. They wonder if a larger model would yield better results.
- brrrrrm simply suggests giving the model a try.

### Show HN: GPT-4-Vision UX audit for your landing page (relaunch)

#### [Submission URL](https://flawless.is/) | 12 points | by [liorgrossman](https://news.ycombinator.com/user?id=liorgrossman) | [10 comments](https://news.ycombinator.com/item?id=38266912)

Flawless is an innovative service that offers AI-powered actionable suggestions to enhance the usability, conversion, and messaging of your landing page. With just a one-time payment of $1.99 (discounted from $4.99), you can receive valuable insights into optimizing your website. However, due to OpenAI rate limits, Flawless can currently only accommodate 100 users per day.

The process is straightforward. Flawless takes a full-page screenshot of your website, capturing every detail from top to bottom. This may take about 20-30 seconds to complete. Then, utilizing the cutting-edge GPT-4 Vision technology, Flawless examines the image to identify any design, usability, or conversion issues. Based on the analysis, it provides you with suggested fixes. The results are conveniently displayed alongside your original screenshots for easy reference.

Many renowned websites have already benefited from Flawless, including Slack.com, Netflix.com, OpenAI.com, and Competely.ai. By offering a comprehensive evaluation of your landing page, Flawless helps you optimize your user experience and boost conversion rates.

Some frequently asked questions are answered on the website to address any concerns. For instance, the service's cost has been set at $4.99 originally, covering expenses related to the GPT-4 Vision API, screenshot API, and hosting. However, early adopters can currently take advantage of an exclusive offer, paying only $1.99 (less than the price of a cup of coffee) for a limited time.

Flawless acknowledges that running the service comes with significant costs and, therefore, charges a small fee to ensure its sustainability. Additionally, the number of customers served per day is limited to 100 due to OpenAI rate-limiting for the OpenGPT-4 Vision API. It's worth noting that Flawless currently leverages a third-party screenshot API called urlbox, although they might develop their own screenshot-taking capabilities in the future.

If you're seeking tangible ways to improve your landing page's usability and conversion rates, Flawless provides actionable suggestions within minutes, thanks to the power of GPT-4 Vision. Don't miss the opportunity to enhance your website's performance. Get started with your Flawless audit today!

Flawless is a product of Lifehack Labs LLC, copyright 2023.

The discussion on Hacker News surrounding the submission about Flawless: Instant UX Audit for Your Landing Page covers various aspects of the service.

One commenter points out that when reviewing Slack's landing page, Flawless fails to consider the small customer logos and the perception of their size. Additionally, the color of the buttons doesn't match the background, making them stand out and confusing some users.
Another commenter, likely a senior UX professional, appreciates the quality of the recommendations provided by Flawless and mentions that a quick glance at the results shows helpful suggestions for improving user experience.
There is a discussion about the pricing and payment process of Flawless. One user asks for clarification on the pre-domain pricing and email notifications, as they experienced some issues. Another user explains that they had to pay $1.99 for a specific URL, and after payment, they received a series of emails with the URL. They also mention the concept of logging, stating that the URL is important and provides a direct link to Flawless.
The user requests assistance in finding the relevant URL and indicates confusion about fixing multiple problems without paying again. Another user responds with a detailed response, mentioning that if the URL cannot be found, they should check their browser history or contact Flawless support.

The user acknowledges the detailed response and mentions that, as a full-stack web developer, they are closely following Flawless tools, looking forward to future developments.

The conversation ends with appreciation for Flawless and the potential value it brings to developers. The user emphasizes the importance of delivering a quality product and commends Flawless for their work.

### Beating GPT-4 with a 13B model

#### [Submission URL](https://lmsys.org/blog/2023-11-14-llm-decontaminator/) | 37 points | by [EvgeniyZh](https://news.ycombinator.com/user?id=EvgeniyZh) | [9 comments](https://news.ycombinator.com/item?id=38265857)

Researchers from LMSYS have announced a breakthrough in beating GPT-4 performance using a 13B model. They achieved this by rephrasing the test set, which allowed the model to "generalize" beyond variations and reach high benchmark performance. However, they discovered that existing decontamination methods fail to detect contamination caused by such rephrasing. To address this, they propose a stronger decontaminator called LLM decontaminator, which uses an advanced language model to identify and remove rephrased samples. They evaluated different detection methods and found that their LLM decontaminator performed the best, providing more precise detection of contamination. They also applied the decontaminator to real-world datasets and found significant contamination in widely used benchmarks. They urge the community to adopt stronger decontamination methods and provide an open-source LLM decontaminator tool for scanning data.

Discussion Summary:

1. grbbyy criticized the title of the submission, saying it was clickbait and that they have found good results using different training techniques in specific domains.
   - dchftcs responded by saying they didn't waste time reading the rest and didn't find it valuable.
      - KennyFromIT expressed a positive sentiment, wishing people would default to assuming the best. They also mentioned that it's hard to have productive conversations on Hacker News sometimes.
2. geoduck14 mentioned that the value is not in beating GPT-4, but rather in GPT-4 delivering value itself, implying that beating it is not that important.
3. gmrc made a cynical comment about benchmarking attempts, calling it a 13B GPT4-Killer.
4. hmrp noted that the researchers rephrased the test set and trained on it.
5. spnjm made a sarcastic comment about how someone couldn't write code to print 100 random lines from a dictionary.
   - Alifatisk responded by saying that programming is slowly becoming less about writing code, as there are now no-code tools that allow developers to build things. They also mentioned that some people hardly program but criticize one specific language or programming methodology.

Overall, the discussion includes critiques of the submission, skepticism about the value of beating GPT-4, and a discussion about the changing nature of programming.

### AI chemist finds molecule to make oxygen on Mars after sifting through millions

#### [Submission URL](https://www.space.com/mars-oxygen-ai-robot-chemist-splitting-water) | 33 points | by [Brajeshwar](https://news.ycombinator.com/user?id=Brajeshwar) | [6 comments](https://news.ycombinator.com/item?id=38264196)

In a groundbreaking study, scientists have announced that an AI-powered robot chemist has successfully synthesized compounds that can be used to generate oxygen from water. This breakthrough could prove crucial for future crewed missions to Mars, as it would eliminate the need to transport large amounts of oxygen from Earth. Mars possesses significant reserves of frozen water ice, and researchers have been exploring ways to extract oxygen from these reserves. By using Martian meteorites and an AI chemist, the team was able to identify the best catalyst for water splitting, which can operate at the extremely cold temperatures found on Mars. The AI chemist analyzed over 3.7 million molecules and selected, synthesized, and tested 243 of them within six weeks. The researchers estimate that it would have taken a human scientist around 2,000 years to achieve the same results using traditional methods. While the study highlights the potential of AI in scientific research, the researchers stress that human guidance is still necessary for the AI chemist. The team now aims to test the robot chemist's performance under different Martian conditions.

The discussion revolves around the AI-powered robot chemist's ability to synthesize compounds for oxygen production on Mars. Some users highlight the process of the AI chemist collecting samples from Martian meteorites and synthesizing 243 molecules within six weeks using laser scanning and calculations. They emphasize the efficiency and speed of the AI chemist compared to traditional methods. Other users mention the challenges of generating oxygen on Mars and the importance of synthetic chemicals for survival. Ethical considerations of terraforming Mars are also brought up, along with discussions on the lack of a global magnetic field and the impact of solar radiation on the Martian atmosphere. One user notes that NASA's Perseverance rover has already successfully extracted oxygen from the Martian atmosphere using the MOXIE instrument. Finally, there is a brief exchange about the nature of the synthesized molecules.

### Google wants governments to form a 'global AI corps'

#### [Submission URL](https://www.washingtonpost.com/politics/2023/11/14/google-wants-governments-form-global-ai-corps/) | 22 points | by [jyunwai](https://news.ycombinator.com/user?id=jyunwai) | [18 comments](https://news.ycombinator.com/item?id=38264269)

Google is advocating for governments to establish a "global AI corps" as they grapple with regulating artificial intelligence (AI). In a recently released white paper, Google outlines its policy recommendations for governments to maximize AI's potential. The tech giant suggests scaling up AI training programs and creating flexible immigration pathways for AI experts. Additionally, Google supports the idea of an "AI education bill" proposed by Senator Maria Cantwell to retrain and skill one million people. This white paper is expected to shape Google's approach to regulatory talks surrounding AI in Washington.

The discussion on this submission revolves around various aspects of Google's advocacy for the establishment of a global AI corps and the regulation of AI. Some comments express skepticism about Google's motivations, suggesting that it may be driven by philanthropic interests or a desire to control the dissemination and training of AI. Others argue that a global government or the United Nations should have control over AI regulation. There is also debate about the impact of AI on job markets and the need for regulatory frameworks. Some users bring up the development of advanced AI models and the responsibility to ensure responsible development. The discussion also includes references to OpenAI, Microsoft, and the World Economic Forum.

### YouTube adapts its policies for the coming surge of AI videos

#### [Submission URL](https://techcrunch.com/2023/11/14/youtube-adapts-its-policies-for-the-coming-surge-of-ai-videos/) | 17 points | by [webwanderer](https://news.ycombinator.com/user?id=webwanderer) | [6 comments](https://news.ycombinator.com/item?id=38264906)

YouTube has announced new policies and tools to address AI-generated content on its platform, including deepfakes. YouTube creators will now be required to disclose when they have created altered or synthetic content that appears realistic, particularly when it relates to sensitive topics like elections or ongoing conflicts. The company warns that creators who fail to properly disclose their use of AI consistently may face content removal, suspension from the YouTube Partner Program, or other penalties. YouTube will also allow users to request the removal of AI-generated or other synthetic content that simulates an identifiable individual, such as a deepfake. However, YouTube clarifies that not all flagged content will be removed, leaving room for parody or satire. The company is also working on a system to compensate artists and rightsholders for AI-generated music.

The discussion on this submission revolves around several different points. 

One commenter, xngpd, mentions that YouTube's policies may not adequately address AI-generated content on the platform. They point out that even using an AI-generated script for a video's subtitle or search title can result in numerous videos with seemingly human interactions in the comments, potentially misleading viewers. They suggest that deleting the comments or downvoting and reporting the videos and channels might be more effective.
In response to xngpd's comment, lern_too_spel argues that YouTube's policy of requiring disclosure of altered or synthetic content does not necessarily apply to the videos being discussed. They explain that the problem lies with YouTube's recommendation system.
Another commenter, dttnw, sadly admits to being someone who watches AI-generated content. They mention that the nonsensical generated content still manages to garner a lot of views. They also point out that many content creators slow down their speech, which makes it easier for AI-generated content scripts to mimic them, especially when translated into different languages.
Sncntd highlights that one of the biggest issues on YouTube is its handling of DMCA takedown notices for AI-generated content, particularly in cases such as product reviews. They mention that addressing this is important and that YouTube is likely aware of this issue.
Dttnw also makes a crosspost to another thread related to the topic.
Lastly, QVVRP4nYz adds that YouTube possibly cannot handle the vast volume of deepfake misinformation that is published on its platform, suggesting that the problem is much larger than what is being discussed.

Overall, the discussion touches on various concerns regarding YouTube's policies and the challenges it faces in dealing with AI-generated content, including deepfakes and misleading recommendations.

### AI chemist could make oxygen on Mars

#### [Submission URL](https://www.nature.com/articles/d41586-023-03522-4) | 50 points | by [gmays](https://news.ycombinator.com/user?id=gmays) | [85 comments](https://news.ycombinator.com/item?id=38266867)

A team of researchers in China has developed an AI-powered robot chemist that can extract oxygen from water on Mars. The robot uses materials found on the red planet to produce catalysts that break down water, releasing oxygen. The study, published in Nature Synthesis, involved analyzing meteorites that mimic the Martian surface and using an AI-powered system to search for a chemical that could break down water. The result was an oxygen-evolution reaction catalyst that could potentially be used on a future Mars mission. While the robot's ability to produce oxygen from water is impressive, some experts argue that there are easier ways to produce oxygen on Mars, such as through NASA's Mars Oxygen In-Situ Resource Utilization Experiment (MOXIE). Nevertheless, the AI-powered robot chemist could have potential uses for synthesizing other useful materials on Mars and beyond.

The discussion on this submission covers several topics. One commenter discusses the technical details of the AI-powered robot chemist, explaining the thermodynamic reactions involved in extracting oxygen from water. Another commenter discusses the use of AI in classifying strings and the Turing completeness of regular expressions. There is also a discussion about the potential waste of materials in the Martian atmosphere and the challenges of sustaining human civilization on Mars. Some commenters argue that there are more practical and efficient ways to produce oxygen on Mars, while others discuss the importance of nitrogen in the Martian ecosystem and the potential for terraforming Mars. Additionally, there is a discussion about Elon Musk and his role in space exploration, with some commenters expressing skepticism and others defending his ideas.

### Tangram Vision's AI-powered 3D sensor could transform robotic computer vision

#### [Submission URL](https://venturebeat.com/ai/tangram-visions-ai-powered-3d-sensor-could-transform-computer-vision-in-robotics/) | 22 points | by [reteltech](https://news.ycombinator.com/user?id=reteltech) | [8 comments](https://news.ycombinator.com/item?id=38267740)

Tangram Vision, a startup focused on building software and hardware for robotic perception, has unveiled a new 3D depth sensor called HiFi. Priced at $549, the sensor combines high-resolution 3D sensing with AI processing power and computer vision algorithms. Its goal is to simplify challenging tasks such as calibration and navigation, and make it easier for developers to add AI-enhanced 3D data to robots. By handling complex tasks onboard, HiFi accelerates the development of robotics products and enables small teams to tap into sophisticated computer vision capabilities. Tangram Vision is launching HiFi on Kickstarter to make it accessible to hackers, developers, and robotics companies. If successful, HiFi could disrupt the robotics vision market and provide significant time and cost savings for organizations looking to integrate computer vision and AI into their robotic systems. While Tangram Vision is a young startup and faces competition from larger sensor incumbents, its focus on the emerging niche of robotic vision positions it well for potential disruption.

The discussion revolves around Tangram Vision's new 3D depth sensor called HiFi. Some users express excitement about the sensor, noting its high-resolution 3D sensing capabilities and AI processing power. They believe HiFi has the potential to simplify complex tasks in robotic perception and accelerate the development of robotics products. Others compare HiFi to existing alternatives like RealSense and Structure and discuss its potential advantages, such as self-calibration and improved depth quality. The conversation also touches on Tangram Vision's focus on the niche market of robotic vision and its potential for disruption.

In response to questions, a member from Tangram Vision explains that the HiFi sensor is specifically focused on robotics capabilities and offers higher resolution and improved calibration compared to alternatives like Luxonis. They highlight that Tangram Vision is primarily a software company and that the HiFi sensor is meant to complement their software offerings.

Overall, the discussion demonstrates a mix of excitement and curiosity about Tangram Vision's HiFi sensor and its potential impact on the robotics vision market.