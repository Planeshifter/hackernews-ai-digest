import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Fri Nov 17 2023 {{ 'date': '2023-11-17T17:10:34.732Z' }}

### Show HN: nbi.ai â€“ Generative Business Intelligence

#### [Submission URL](https://www.narrative.bi/ai) | 94 points | by [fromthegut](https://news.ycombinator.com/user?id=fromthegut) | [26 comments](https://news.ycombinator.com/item?id=38310502)

NBI.AI, a generative AI platform for business intelligence, has released their latest update. The platform aims to drive growth by providing AI-generated data narratives that deliver actionable insights with just a few clicks. With NBI.AI, users can automate reporting with natural language stories, making it easier to understand complex analytics. The platform generates insights in plain language, eliminating technical jargon and complex data interpretations. NBI.AI also offers weekly AI-powered insights on marketing performance, as well as tools to compare and evaluate ad performance, identify top performers, and analyze conversion journeys. The platform integrates seamlessly with marketing and advertising sources, allowing users to connect in just two clicks. NBI.AI is trusted by over 2,000 growth teams worldwide and offers a 7-day free trial.

The discussion on the submission about NBI.AI, a generative AI platform for business intelligence, covers several topics. Here are the key points:

- One commenter mentions that they are skeptical about AI-driven decision-making tools and prefer a context-leading rule-based natural language generation approach. They expect divergence between rule-based statistical inference narratives and traditional business intelligence data interpretations.
- The founder of NBI.AI responds, providing additional information about their product and its capabilities. They mention that the platform was built to connect virtually structured data sources and has already helped over 2,500 teams gain insights from marketing data.
- Another commenter shares their experience with using narrative-based projects. They use high-level reports that highlight month-over-month changes in website traffic and use an alternative GA4 UI for detailed insights. They plan to implement dimensional analysis to further understand their data.
- The discussion touches on the use of AI in natural language generation and the importance of accuracy and pre-processing to ensure high-quality narratives.
- There is a brief exchange about using automation tools for basic workflows, such as checking invoices and renaming files based on invoice IDs.
- Examples of use cases for NBI.AI are shared, including reporting, anomaly detection, and natural language insights generation for marketing campaigns.
- The founder of NBI.AI clarifies that the training data used for the platform comes from various sources and is focused on behavioral data preferences and feedback to provide personalized insights.
- A user discusses their experience as a marketing specialist and mentions that instead of creating PowerPoint presentations with performance graphs and narrative ROAS, they would prefer using NBI.AI.
- There is a brief discussion about integration plans for NBI.AI and suggestions for additional features.
- Some users express their skepticism about AI-generated data narratives, mentioning that they tend to sound like corporate jargon and lack substance.
- The founder of NBI.AI responds to the feedback, stating that historically they have focused on growth, marketing, and sales data narratives, and that the AI-generated insights are written in natural language.
- There is a discussion about the interpretation and understanding of AI-generated data narratives and the importance of connecting data from various sources to generate focused growth insights and recommendations.

Overall, the discussion provides a mix of skepticism and interest in AI-generated data narratives, with some users sharing their own experiences and suggestions. The founder of NBI.AI actively participates in the discussion, addressing concerns and providing more information about the platform.

### Unauthorized "David Attenborough" AI clone narrates developer's life, goes viral

#### [Submission URL](https://arstechnica.com/information-technology/2023/11/unauthorized-david-attenborough-ai-clone-narrates-developers-life-goes-viral/) | 227 points | by [seasicksteve](https://news.ycombinator.com/user?id=seasicksteve) | [187 comments](https://news.ycombinator.com/item?id=38302319)

In a creative and unauthorized experiment, developer Charlie Holtz combined GPT-4 Vision and ElevenLabs voice cloning technology to create an AI version of David Attenborough narrating his every move on camera. Holtz used a Python script called "narrator" to take a photo from his webcam every five seconds and feed it to GPT-4V, which processed the image and generated Attenborough-style text. This text was then fed into an ElevenLabs AI voice profile trained on Attenborough's speech. The demo video of the experiment has gained significant attention on social media, with mixed reactions from the audience. While some expressed discomfort with imitating Attenborough's voice without permission, others found the demonstration amusing and creative.

The discussion on the submission starts with a comment questioning the ethical concerns of voice cloning and replicating famous individuals. Another user points out that the technology allows for the creation of commercial narrations in the styles of famous voices like Attenborough and Freeman. The conversation then shifts to a debate about the significance and influence of classic works of literature and how technology can impact their reproduction. Some users argue that technological advancements have made it easier for classics to be produced and distributed, while others argue that the quality and cultural impact of works from different time periods cannot be easily compared. Another user brings up the idea that generations often have different points of reference and familiarity with certain things, which affects artistic expression and experimentation. One user mentions a BBC documentary narrated by David Attenborough. The conversation then diverts to a discussion about the recycling of cultural content and the push for profit and nostalgia. Some users express concerns about the lack of originality and artistic challenge in replicating older works, while others discuss the dynamics of the entertainment industry and how content creation and consumption have evolved. One comment suggests that AI could potentially create new episodes of old shows like Inspector Gadget. However, another user disagrees, stating that AI-generated content eliminates creativity and renders results meaningless. The conversation then touches on the craftsmanship involved in animation and the varying levels of effort put into different animation styles. The discussion concludes with a mention of a science fiction character, Duncan Idaho.

### A PCIe Coral TPU Finally Works on Raspberry Pi 5

#### [Submission URL](https://www.jeffgeerling.com/blog/2023/pcie-coral-tpu-finally-works-on-raspberry-pi-5) | 110 points | by [mikece](https://news.ycombinator.com/user?id=mikece) | [20 comments](https://news.ycombinator.com/item?id=38308552)

The Raspberry Pi 5 can now natively support the PCIe Coral TPU, an AI accelerator used for tasks like machine vision and audio processing. Previously, getting the PCIe Coral TPU to work on a Raspberry Pi was challenging due to quirks in the Compute Module 4's PCIe implementation. However, with the improved PCIe bus on the Raspberry Pi 5, it is now possible, although a few tweaks are required. These include switching to a 4K page size, disabling PCIe ASPM, and making changes to the device tree. Additionally, due to compatibility issues, running the Coral's PyCoral library requires either Docker or installing an alternate system-wide Python version. While there are no commercially-available HATs or adapter boards for connecting the Coral TPU to the Raspberry Pi 5's PCIe header, options like the HatDrive! Top or Bottom from Pineberry Pi or the Coral B+M key module with an appropriate adapter can be used. Once set up, the Coral TPU can be used for various AI tasks, such as image classification. Overall, this development opens up new possibilities for AI acceleration on the Raspberry Pi platform.

Some notable points from the discussion on Hacker News about the Raspberry Pi 5's PCIe support for the PCIe Coral TPU are:

- The comparison is made between various AI accelerators, including HBM3E HAT mk TPUs, NVIDIA Jetson Nano, NVIDIA Orin Nano and AGX, and Coral Mini-PCIe. The discussion includes the TPU's computing power, Tensor Processing Units (TPU) architecture, DLSS architecture, and Vision and Versatile Processor Units (VPU).
- One user mentions the Radxa Rock 5B's NPU, which supports various types of acceleration such as INT4, INT8, INT16, FP16, BF16, and TF32 with a computing power of 6TOPs.
- The Coral TPU's software requirements are discussed, including the need for Python 3.9, which may be a challenge for some users.
- Discussion touches on alternative options, such as Hailo, which is considered a powerful competitor to Coral but may face power-related issues and Python's Global Interpreter Lock (GIL) limitation.
- There are mentions of alternative connectors, such as USB, for the Coral TPU.
- The software support for NPUs in general is considered lacking, highlighting the need for better development and momentum in this area.
- The compatibility of Coral TPU with Ubuntu 20.04 and Python versions is discussed, with reference to the support and versions provided by Ubuntu and AWS Lambda runtimes.
- A user mentions that binary bindings for Coral TPU are only supported on Ubuntu 18, limiting the compatibility with different system versions.
- The discussion briefly shifts towards the Orange Pi 5 RK3588 and its NPUs, with links to SDKs and quickstart guides.
- There is a mention of the Frigate object detection library gaining support for RK3588 NPUs and the need for an upgrade to support this new chip.
- One user suggests that hardware companies prefer to develop AI hardware rather than software, which can sometimes result in poor software support.
- Keeping Python versions up to date is considered important, although one user raises the point that some popular Python libraries may not work on versions beyond 3.9.
- Lastly, there is a brief comment about handling PC cooling with the Coral TPU.

### Google's Gemini model is delayed

#### [Submission URL](https://www.theverge.com/2023/11/16/23964937/googles-next-generation-gemini-ai-model-is-reportedly-delayed) | 93 points | by [keskival](https://news.ycombinator.com/user?id=keskival) | [66 comments](https://news.ycombinator.com/item?id=38300990)

Google's highly anticipated next-generation AI model, codenamed "Gemini," is reportedly facing delays. Initially expected to launch this month, sources now suggest that Gemini's release has been pushed to the first quarter of 2024. The project, which aims to rival OpenAI's GPT-4, is being led by Demis Hassabis, the leader of Google's unified AI team formed earlier this year. The team is combining the best ideas and expertise from both research groups to develop a cutting-edge, multimodal AI model. Interestingly, Google co-founder Sergey Brin is said to be actively involved in the development process, spending a significant amount of time working with the developers.

The discussion on Hacker News revolves around various aspects of Google's Gemini project and the delays it is facing. Some users speculate that Sergey Brin's involvement may be causing the project to slow down, while others argue that his contributions could be beneficial. There is also discussion about the potential impact of Gemini and its competition with OpenAI's GPT-4. Some users express skepticism about the project's ability to disrupt the AI market, while others anticipate significant advancements. Additionally, there are discussions about Google's business model, the limitations of current AI models, and the role of LLMs (large language models) in search. Overall, the discussion highlights a range of opinions and perspectives on Gemini and its significance in the AI landscape.

### AIConfig â€“ source control format for gen AI prompts, models and settings

#### [Submission URL](https://github.com/lastmile-ai/aiconfig) | 91 points | by [saqadri](https://news.ycombinator.com/user?id=saqadri) | [16 comments](https://news.ycombinator.com/item?id=38306410)

LastMile AI has released a new open-source project called aiconfig. It is a config-driven, source control friendly AI application development framework. The framework allows developers to separate prompts, model parameters, and model-specific logic from their application code, simplifying development and iteration on prompts and models. It also provides an AI Workbook editor, which is a notebook-like playground to edit aiconfig files visually, run prompts, tweak models and model settings, and chain things together. The project supports multiple AI models and modalities, including text, image, and audio. It also provides an SDK for both Python and Node.js. Overall, aiconfig aims to simplify AI application development and make it more accessible to developers.

The discussion on Hacker News about the LastMile AI's new open-source project, aiconfig, focused on various aspects of the project.

One commenter, "sqdr," mentioned that they haven't seen AI developer tools that generate config-driven AI application before. They noted that the framework separates prompts, model parameters, and model-specific logic from the application code, which simplifies development and iteration. They also mentioned the AI Workbook editor, which allows users to visually edit aiconfig files and run prompts.
Another commenter, "zby," asked about the documentation for dynamic parameters and interactive Workbook editor. They were also interested in understanding how function calls are chained and if previous function call results can be accessed.
One user, "jdwyh," shared a link to an article they published about dynamic configuration for AI prompts. They mentioned that using prompts in a code configuration format can help handle changes, allow analysts to type prompts easily, and facilitate the rollout of targeted prompts.
"ctvsctt" shared their experience getting started with aiconfig and thanked the OP for sharing the project. They mentioned that they have been copying and pasting prompts and result links in a browser back and forth. They appreciated the tool's ability to save prompts and results locally.
"sqdr" thanked "ctvsctt" for their feedback and mentioned that they are working on improving the UX and providing APIs for interacting with the configuration.
Another commenter, "kordlessgn," mentioned that they have been working on a similar project using Jinja2 templates and containerization. They shared a link to their project and said they are constantly making progress.
"sqdr" appreciated the contribution and thanked them for it.
"smy20011" mentioned that having the source controlled is easier to manage and appreciated the ability of aiconfig to connect the application code to the configuration.
"thrwnm" briefly looked at a few similar projects and mentioned their interest in trying aiconfig.
Another commenter, "smy20011," mentioned that while configuring non-business logic, such as string databases or feature flags, is straightforward, configuring prompts and business logic can become harder to read and maintain.
One user, "jshk," shared a link to a similar project called "promptflow."
"thtxlnr" compared aiconfig to Ollama and discussed low-level integration and the overlap between the two projects.

Overall, the discussion revolved around different aspects of aiconfig, including its separation of prompts and model parameters from application code, the use of dynamic parameters, the ease of iterating on prompts, and the challenge of configuring business logic.

### Wikidata, with 12B facts, can ground LLMs to improve their factuality

#### [Submission URL](https://arxiv.org/abs/2305.14202) | 210 points | by [raybb](https://news.ycombinator.com/user?id=raybb) | [84 comments](https://news.ycombinator.com/item?id=38304290)

A new research paper titled "Fine-tuned LLMs Know More, Hallucinate Less with Few-Shot Sequence-to-Sequence Semantic Parsing over Wikidata" presents a method to improve large language models' factuality by grounding them with the vast amount of information in Wikidata. The paper introduces WikiWebQuestions, a high-quality question answering benchmark for Wikidata, and proposes a few-shot sequence-to-sequence semantic parser for the dataset. The parser is trained to use either results from an entity linker or mentions in the query. The experimental results show that this methodology achieves a strong baseline of answer accuracy in the dev and test sets of WikiWebQuestions. By combining the semantic parser with GPT-3, the researchers were able to provide useful answers to 96% of the questions in the dev set. The paper also demonstrates that their method outperforms the state-of-the-art for the QALD-7 Wikidata dataset.

The discussion on this submission covers various aspects of the research paper and the use of large language models (LLMs) in general. Some key points from the discussion include:

- Some users suggest that the original source should be submitted instead of Twitter links.
- There is a discussion about the limitations of current LLMs in understanding contextual patterns and the potential benefits of training them with data from sources like Wikidata.
- The effectiveness of using Wikidata for fact-checking and improving the accuracy of responses generated by LLMs is debated.
- Retrieval Augmented Generation (RAG) is mentioned as a method to improve the performance of LLMs on knowledge-intensive tasks by combining information retrieval with text generation.
- The discussion touches on the challenges of fact-checking and the potential limitations of relying on LLMs for providing accurate information.
- There is a discussion about the role of Wikidata in improving the quality and consistency of information used by LLMs.
- The need for human validation and the limitations of post-processing techniques in ensuring accuracy are mentioned.
- Some users express skepticism about the robustness of LLMs and their ability to handle complex queries and provide accurate information.
- The importance of training LLMs with grounded and reliable data is emphasized.
- The limitations of LLMs in handling postmortem reasoning and providing robust explanations are discussed.

Overall, the discussion highlights both the potential benefits and limitations of using large language models and the challenges in improving their factuality and accuracy.

### We Automated Bullshit

#### [Submission URL](https://www.cst.cam.ac.uk/blog/afb21/oops-we-automated-bullshit) | 354 points | by [fanf2](https://news.ycombinator.com/user?id=fanf2) | [315 comments](https://news.ycombinator.com/item?id=38302635)

In a blog post titled "Oops! We Automated Bullshit.", Alan Blackwell shares his thoughts on the role of artificial intelligence (AI) and its tendency to produce bullshit. Blackwell highlights the recent attention AI has received from political leaders, such as US President Biden and British PM Rishi Sunak, who seem captivated by the idea of an AI-driven future where work becomes obsolete. However, Blackwell argues that the problem lies in AI's ability to generate text that "sounds good" but lacks evidence, logic, or truth. He references MIT Professor Rodney Brooks, who describes ChatGPT (an AI model) as "making up stuff that sounds good." Other prominent AI researchers, including Geoff Hinton, echo these concerns, warning that AI systems could become super-persuasive without being intelligent, imitating the worst behaviors of political leaders like Donald Trump or Boris Johnson. By relying on predictive text rather than factual information, these AI systems produce what Blackwell refers to as "bullshit." He cites philosopher Harry Frankfurt's concept of bullshit, which is defined as talking without knowing what one is talking about and disregarding the authority of truth. Blackwell also mentions David Graeber's analysis of "bullshit jobs," where over 30% of British workers believe their jobs contribute nothing of value to society. Graeber argues that these types of jobs, which can easily be done by AI systems, train individuals to generate bullshit. In conclusion, Blackwell raises questions about the future of work in an AI-driven world and whether producing bullshit will become the only kind of work needed.

The discussion surrounding the submission touches on various points related to language and knowledge. Some users argue that language is a representation of knowledge, while others assert that language contains non-knowledge nonsense. The concept of justified true belief is brought up, with some expressing skepticism about the possibility of true knowledge. There is also a discussion about the limitations of AI and its ability to generate knowledge. The complexity of language models and the importance of understanding their limitations are mentioned as well. Overall, the discussion explores different perspectives on the relationship between language and knowledge and the role of AI in generating meaningful information.

### Satya Nadella's Statement on OpenAI

#### [Submission URL](https://blogs.microsoft.com/blog/2023/11/17/a-statement-from-microsoft-chairman-and-ceo-satya-nadella/) | 84 points | by [sanketsaurav](https://news.ycombinator.com/user?id=sanketsaurav) | [17 comments](https://news.ycombinator.com/item?id=38312355)

Today, Microsoft shared that they are ramping up their innovation in the field of AI with over 100 new developments. These advancements span across their entire technology stack, including AI systems, models, and tools in Azure, as well as their recently introduced Copilot. The company is dedicated to bringing these innovations to their customers while also planning for future growth. They emphasized their long-term collaboration with OpenAI, ensuring access to the necessary resources for their innovation agenda. Microsoft is committed to working together with OpenAI to bring the significant advantages of AI technology to the world.

The discussion on this submission revolves around Microsoft's announcement and their collaboration with OpenAI. Some commenters express skepticism about the full capabilities of OpenAI and the need for robust fallback access to the source code. Others discuss Microsoft's past failures and the potential impact on investors. Some argue that the statement from Microsoft is just marketing and lacks substance. However, there are also comments highlighting Microsoft's commitment to innovation and the long-term partnership with OpenAI. One commenter emphasizes the importance of reliability and industry risk in Microsoft's investment. Overall, opinions are mixed, with some questioning the intentions behind Microsoft's announcement and others applauding their efforts to bring AI advancements to customers.

---

## AI Submissions for Thu Nov 16 2023 {{ 'date': '2023-11-16T17:11:17.654Z' }}

### Google's advanced music generation model and two new AI experiments

#### [Submission URL](https://deepmind.google/discover/blog/transforming-the-future-of-music-creation/) | 201 points | by [kmisiunas](https://news.ycombinator.com/user?id=kmisiunas) | [343 comments](https://news.ycombinator.com/item?id=38287043)

Google DeepMind, in partnership with YouTube, has announced Lyria, its most advanced AI music generation model, and two AI experiments aimed at fostering creativity. The Lyria model is designed to generate high-quality music with instrumentals and vocals, enabling users to have more control over the output's style and performance. One of the experiments, called Dream Track, allows a limited set of creators to produce a unique soundtrack using the AI-generated voice and musical style of artists such as Charlie Puth and Demi Lovato. The other experiment involves the development of AI tools for creating new music or instrumental sections, transforming audio styles or instruments, and producing instrumental and vocal accompaniments. The generated music will be watermarked with SynthID, a tool for identifying synthetically generated content. DeepMind has worked closely with artists and the music industry to ensure that these technologies are developed responsibly.

The discussion surrounding the Google DeepMind and YouTube partnership announcement revolves around a few key points:

- Some commenters express their skepticism about the quality and authenticity of AI-generated music. One user mentions that Michael Jackson's music was exceptional because it involved playing instruments and had unique rhythms, which they believe AI cannot replicate. Another user highlights the frustration of the mixing process and the challenge of creating music in a backwards manner.
- Others discuss the potential democratization of music creation and the role of AI in fostering creativity. One user mentions that AI composition tools can be valuable for those who lack the skills of professional musicians. Additionally, there is a mention of how AI can enable electronic music genres and promote experimentation.
- The impact of AI on the music industry is also brought up. Commenters note that AI can both enable and disrupt the industry, as it can make it difficult to find authentic content and distinguish between AI-generated and human-generated music. There is also a conversation about the role of AI in music curation and the challenge of finding high-quality music in an oversaturated market.
- Commenters also discuss the broader implications of generative AI, with some expressing concerns about the devaluation of craftsmanship and the potential loss of unique human expressions in art. Others highlight the potential for AI tools to enhance creativity and expand artistic possibilities.

Overall, the discussion reflects both skepticism and curiosity about the capabilities and impact of AI in music generation, while also acknowledging the potential for creativity and democratization.

### A failed AI girlfriend product, and my lessons

#### [Submission URL](https://mazzzystar.github.io/2023/11/16/ai-girlfriend-product/) | 235 points | by [mazzystar](https://news.ycombinator.com/user?id=mazzystar) | [353 comments](https://news.ycombinator.com/item?id=38287299)

In April of this year, after reading Stanford's Western Town paper, the author was inspired to create an AI framework combining memory, reflection, planning, and action to facilitate interactions between humans and GPT. The resulting product, named Dolores, is an iOS app that allows users to chat with virtual characters. Despite initial challenges with response times and dialogue length, the app gained popularity, particularly among visually impaired users. The author discovered that users had a strong demand for realistic voices and that many engaged in conversations with Dolores for hours daily. However, despite revenue from subscriptions and voice synthesis purchases, the author didn't make much profit due to high costs associated with APIs. To mitigate this, the author set usage limits for each user to prevent excessive costs. The article ends on a note of confusion regarding text content records on the ElevenLabs official website.

The discussion on this submission covers various topics related to AI and the implications of AI friends. Some users discuss the potential dangers of AI controlling and manipulating individuals, while others argue that human intelligence is flawed and imperfect. There is also a discussion about the role of AI in mental health and the potential benefits and drawbacks of having AI friends. Some users express concerns about AI products and their impact on society, including the possibility of unethical behavior and profit maximization. Additionally, there is a discussion about the limitations of GPT-based products and the need for personal data protection.

### Federated finetuning of Whisper on Raspberry Pi 5

#### [Submission URL](https://flower.dev/blog/2023-11-15-federated-finetuning-of-openai-whisper-with-flower/) | 87 points | by [danieljanes](https://news.ycombinator.com/user?id=danieljanes) | [20 comments](https://news.ycombinator.com/item?id=38294203)

Researchers at Flower Labs have demonstrated the power of federated learning by fine-tuning OpenAI's Whisper model for keyword spotting. This blog post provides a code example that shows how to perform this downstream task in a federated manner. By leveraging large models trained on publicly available data and federating the learning process, Flower ensures client privacy without the need to copy data to a central server. The example walks through the process of designing a federated learning pipeline with Flower for keyword spotting classification using a pre-trained Whisper encoder. The pipeline consists of client-server interactions, where the server samples clients and sends them the classification head. Each client trains the classification head using its own data and communicates the updated head back to the server. The server then aggregates the heads and sends a new global head to the clients in the next round. The researchers used the Google SpeechCommands dataset and achieved over 97% accuracy in classifying keywords after just a few rounds of training. Additionally, they benchmarked the example on the Raspberry Pi 5 and found vastly superior performance compared to the previous Raspberry Pi 4, making it suitable for demanding on-device training workloads.

The discussion around the submission revolved around various aspects of the Whisper model and federated learning.

One user pointed out that the article didn't mention any specific differences between the Raspberry Pi 4 and Raspberry Pi 5 in relation to Whisper. Another user responded that the larger models (like Whisper v3) are about 32 times slower than the smaller models, and they tested Whisper on a Pi 4, finding that it took around 10 minutes to transcribe a 30-second audio sample. However, they speculated that the Pi 5 would be 2-3 times faster.

The maintainer of Flower, the federated learning platform used in the example, mentioned that they were planning to do an in-depth performance comparison soon.
Another user expressed interest in people's experiences, particularly in regards to the performance of Whisper 3 on different model sizes and using double the Raspberry Pi 5. They also mentioned the importance of training specifically for inference on the Raspberry Pi 5.
There was a clarification that the Whisper versions (v2 and v3) referred to large models, with v2 being an older version and v3 being the updated version. All model sizes were mentioned to be part of the original release.
One user noted that the work presented focused on demonstrating the use of federated learning in small devices like the Raspberry Pi 5, and the bigger challenge lies in transferring models with performance improvements for training large models.
Regarding labeling in federated learning, a user suggested that Flower primarily demonstrated how to show the fun of fine-tuning models on federated devices, but the more challenging task is classifying a vast amount of data.
The topic of downstram task performance on small devices and the necessity of data labeling was discussed. It was suggested that labeling could be done either by gathering actual labels or using an auxiliary model that generates pseudo labels for training.
One user asked if it's feasible to perform fine-tuning on small devices, and another user explained that it is possible for distributed devices to move control, location, and data collection of speech recognition tasks.

The potential benefits of federated learning for data privacy were mentioned, with a user highlighting its applicability in end-to-end encryption applications.

Overall, the discussion touched upon performance comparisons, the challenges of training large models, labeling strategies, and the privacy advantages of federated learning.

### AI-Exploits: Repo of multiple unauthenticated RCEs in AI tools

#### [Submission URL](https://github.com/protectai/ai-exploits) | 65 points | by [DanMcInerney](https://news.ycombinator.com/user?id=DanMcInerney) | [18 comments](https://news.ycombinator.com/item?id=38291880)

Protect AI has released a collection of real-world AI/ML exploits for responsibly disclosed vulnerabilities. The repository, called "ai-exploits," contains exploits and scanning templates for vulnerabilities affecting machine learning tools. These attacks can lead to complete system takeovers and the loss of sensitive data, models, or credentials, often without the need for authentication. The goal of this project is to demystify practical attacks against AI/ML infrastructure and raise awareness of the vulnerabilities in the ecosystem. The repository includes Metasploit modules, Nuclei templates, and CSRF templates for security professionals to exploit or scan for vulnerabilities. The easiest way to use these modules and templates is to build and run the Docker image provided in the repository.

The discussion on Hacker News revolves around the release of the "ai-exploits" repository by Protect AI, which contains real-world AI/ML exploits for responsibly disclosed vulnerabilities.

Some commenters find the collection of exploits interesting and mention that they are common in ML operational and data science projects. Others appreciate the work done by Protect AI, stating that it helps demystify attacks against AI/ML infrastructure and raises awareness about vulnerabilities in the ecosystem.

There is also a discussion about the need for experienced programmers in the field, as some commenters express concerns about the potential security risks associated with replacing programmers with AI. This leads to a debate on the quality and potential vulnerabilities of AI-generated code.

Commenters also discuss the broad implications these exploits can have, considering that attackers can quickly target model content credentials in some cases. There is recognition of Protect AI's focus on security in AI/ML systems and the creation of a new category called MLSecOps (Machine Learning Security Operations).

Additionally, there is a debate on the importance of validating content from trusted sources, the difficulty in detecting and protecting against complex attacks, and the potential risks of compromised data integrity.

Overall, the discussion highlights the significance of addressing security vulnerabilities in AI/ML infrastructure and the challenges associated with securing machine learning systems.

### Emu Video and Emu Edit, our latest generative AI research milestones

#### [Submission URL](https://ai.meta.com/blog/emu-text-to-video-generation-image-editing-research/) | 190 points | by [ot](https://news.ycombinator.com/user?id=ot) | [46 comments](https://news.ycombinator.com/item?id=38291139)

Facebook's research team has announced two new research milestones in generative AI: Emu Video and Emu Edit. Emu Video offers a simple method for text-to-video generation based on diffusion models. It can generate high-quality videos by first generating images conditioned on a text prompt, and then generating video conditioned on both the text and the generated image. Emu Edit, on the other hand, focuses on precise image editing through recognition and generation tasks. It aims to streamline various image manipulation tasks and offers enhanced capabilities and precision in image editing. These advancements in generative AI have the potential to revolutionize creativity and self-expression by enabling users to generate animated stickers, edit photos with ease, and more. While these models are not intended to replace professional artists and animators, they provide new ways for people to express themselves.

The discussion around Facebook's Emu Video and Emu Edit focuses on various aspects of the research. Some commenters express confusion and frustration about the complexity of the models, while others appreciate the advancements in generative AI. One commenter relates the Emu Edit model to a scene from Star Trek, while another suggests the potential use of AI in programming interfaces. There is also a discussion about the ethical implications of AI replacing human creativity and the potential for AI-generated content to imitate copyrighted material. Some commenters express disappointment about the lack of access to the source code and the need for more transparency. Overall, there is a mix of excitement and skepticism about the capabilities and implications of these generative AI models.

### Types of Conversations with Generative AI

#### [Submission URL](https://www.nngroup.com/articles/AI-conversation-types/) | 91 points | by [adrian_mrd](https://news.ycombinator.com/user?id=adrian_mrd) | [12 comments](https://news.ycombinator.com/item?id=38287435)

There are six types of conversations that users have with generative-AI bots, according to a recent study. These conversations involve different types of prompts and can be of various lengths. Some conversations are simple search queries, where users are looking for specific information. Other conversations involve funneling, exploring, chiseling, expanding, or pinpointing. The length of the conversation is not necessarily an indicator of its success, as both short and long conversations can be helpful to users. The study provides tips for both users and interface designers of generative AI chatbots.

The discussion on the submission includes various perspectives on the use and limitations of generative AI chatbots. One user points out that using simple keyword prompts may not always yield accurate responses from ChatGPT, and suggests using more conversational approaches to get better results. Another user shares their experience in building a frontend interface for ChatGPT and provides a link to their project. The discussion also touches on the potential challenges and complexities of integrating ChatGPT into projects, as well as alternative research companies and studies in the field. Overall, the discussion reflects a mix of opinions and experiences with generative AI chatbots.

### Bad bots account for most internet traffic? Analysis

#### [Submission URL](https://www.securityweek.com/bad-bots-account-for-73-of-internet-traffic-analysis/) | 104 points | by [LinuxBender](https://news.ycombinator.com/user?id=LinuxBender) | [59 comments](https://news.ycombinator.com/item?id=38291406)

Arkose Labs, a cybersecurity firm, has reported a significant increase in bad bot attacks, with 73% of all internet traffic now believed to be comprised of bad bots and related fraud farm traffic. The top five categories of bad bot attacks include fake account creation, account takeovers, scraping, account management, and in-product abuse. The biggest increases in attacks from Q2 to Q3 are SMS toll fraud, account management, and fake account creation. The technology, gaming, social media, e-commerce, and financial services industries are the most targeted by these attacks. The rise of bad bots is likely due to the availability of artificial intelligence and the increasing professionalism of cybercriminals using crime-as-a-service offerings. The use of AI-powered bots that mimic human behavior makes them adept at targeting vulnerabilities in emerging technologies. Additionally, the rise of AI may be related to the increase in scraping bots, which gather data and images from websites. Scraping social media accounts can provide personal data that can be exploited for phishing attacks. The growth of crime-as-a-service has made cyberattacks cheaper and more effective for adversaries. To combat this, the report suggests implementing bad bot detection and mitigation strategies.

The discussion on this submission revolves around several points raised in the article. One commenter disputes the 73% figure mentioned in the submission, suggesting that it may be based on JavaScript fingerprinting rather than actual numbers. Another commenter argues that scraping should not be automatically classified as illegal, as it can be done legally with proper permissions. They point out that the article fails to acknowledge this distinction. Additionally, there is some debate about the validity of the statistics mentioned, with one commenter suggesting that the high view numbers on certain websites may be artificially inflated due to bot traffic. Another commenter raises the issue of toll fraud, highlighting the increase in SMS toll fraud attacks. There is also discussion about how cloud providers like Cloudflare handle bot traffic and the use of APIs to combat scraping. Overall, the discussion provides different viewpoints on the topic of bad bot attacks and the strategies to mitigate them.

### Serverless development experience for embedded computer vision

#### [Submission URL](https://github.com/pipeless-ai/pipeless) | 65 points | by [migmartri](https://news.ycombinator.com/user?id=migmartri) | [8 comments](https://news.ycombinator.com/item?id=38288743)

Pipeless is an open-source computer vision framework that allows developers to create and deploy applications in just minutes, without the complexities of building and maintaining multimedia pipelines. Inspired by modern serverless technologies, Pipeless offers a serverless-like development experience for computer vision. All you need to do is provide functions for new video frames, and Pipeless takes care of the rest.

With Pipeless, you can easily use industry-standard models like YOLO or load your custom model using the supported inference runtimes, such as the ONNX Runtime. It offers multi-stream support, dynamic stream configuration, and multi-language support, allowing you to write hooks in various languages, including Python. Pipeless is highly parallelized, takes care of multi-threading and multi-processing, and supports several inference runtimes like CUDA, TensorRT, OpenVINO, and CoreML.

Deploying your Pipeless application is also made easy, with support for edge and IoT devices or the cloud. The framework provides tools for deployment, including container images. The project structure in Pipeless is well-defined, making the code highly reusable and organized.

If you're a computer vision developer looking for a simpler and faster way to create and deploy applications, give Pipeless a try. Join the community and contribute to making the lives of computer vision developers easier.

Check out the official repository for more information and installation options.

The discussion on this submission revolves around various aspects of the Pipeless computer vision framework.

One user, "yldrb," shares their positive experience with using serverless technologies like AWS Lambda for serving low-volume computer vision models. However, they mention that the latency can be surprisingly high, especially with GPUs. They highlight the challenges faced by enterprises running things on Kubernetes clusters.
In response, user "zptrm" shares their experience with building and testing a similar framework called Modal. They express slight disappointment with the long startup times of cloud instances but express long-term interest in using inference servers with optimized models for improved performance.
"mglh" adds that Pipeless started with support for ONNX Runtime, OpenVINO, CoreML, CUDA, TensorRT, and other execution providers. They also mention checking license details and the cost of allocated resources.
A user, "nglmm," chimes in to mention that they find the integration of ChatGPT capabilities and AI+vision interesting and plan to try using the framework for personal projects and IP cameras.
"mgmrtr" finds the open-source nature of the tool interesting and notes that it abstracts away much of the plumbing required for computer vision pipelines.

Returning to the topic of performance, "yldrb" mentions plans to support 50k front-end models in Roboflow Universe, while "mglh" thinks it would be a good idea to allow people to dynamically load models in Roboflow.

In summary, the discussion includes positive experiences with serverless technologies, interest in optimized inference servers, curiosity about combining AI capabilities with computer vision, and praise for the simplicity of the Pipeless framework.

### Show HN: Beak.js â€“ Custom conversational assistants for your React app

#### [Submission URL](https://github.com/mme/beakjs) | 35 points | by [_mme](https://news.ycombinator.com/user?id=_mme) | [21 comments](https://news.ycombinator.com/item?id=38290646)

Beak.js is an open-source library that allows you to integrate custom conversational assistants into your React applications. It comes with a built-in UI, making it easy to add a beautiful and customizable chat window to your website. Beak.js is designed to be easy to use, requiring only a few lines of code to integrate with your existing React app. You can let the assistant carry out tasks in your app by setting up functions with the useBeakFunction hook. Additionally, you can use the useBeakInfo hook to let the assistant know what is happening on the screen. Beak.js is a powerful tool for adding conversational capabilities to your React app. Check out the GitHub repository for more information and to give it a try.

The discussion on this submission revolves around the security and implementation aspects of using Beak.js, the open-source library for integrating conversational assistants into React applications.

One user expresses surprise about the MITM (Man-in-the-Middle) proxy phones network and its ability to download 30 scams per second, while another user finds the concept of connecting to OpenAI's GPT AI directly without exposing the full API key headers intriguing.
An important point raised in the discussion is the need to avoid exposing the API key to the public-facing applications. Some users suggest implementing feedback for better security, while others point out the need for setting up CORS (Cross-Origin Resource Sharing) to prevent unauthorized embedding of the API.
The idea of proxying OpenAI calls through a quick Pipedream workflow is also discussed, and a link to an implementation concept is shared. The communication between the frontend and backend, as well as preventing unauthorized API usage, is also highlighted as important considerations.
There is an interest in the ability to securely communicate with OpenAI on the backend to prevent unauthorized API access, and a suggestion is made to use a backend library for proxying the communication.
Overall, the discussion shows a general interest in the potential applications and capabilities of Beak.js, with some users expressing their interest in similar projects and their thoughts on alternative solutions.

---

## AI Submissions for Wed Nov 15 2023 {{ 'date': '2023-11-15T17:09:58.441Z' }}

### Exploring GPTs: ChatGPT in a trench coat?

#### [Submission URL](https://simonwillison.net/2023/Nov/15/gpts/) | 464 points | by [simonw](https://news.ycombinator.com/user?id=simonw) | [196 comments](https://news.ycombinator.com/item?id=38277926)

Last week's OpenAI DevDay brought a lot of exciting announcements, but the biggest one was the introduction of GPTs. Users of ChatGPT Plus can now create their own custom GPT chat bots for others to interact with. Initially, GPTs seemed like little more than a fancy wrapper for standard GPT-4 with predefined prompts, but after spending more time with them, Simon Willison is starting to see their potential. The combination of features they offer can lead to some interesting results. However, the documentation for GPTs is still quite minimal. Simon shares his insights on configuring a GPT, including naming, instructions, conversation starters, uploaded files, and optional actions. He also highlights the billing model, prompt security, and the importance of publishing prompts. Simon then discusses his exploration of the new platform, showcasing his most useful GPTs so far: the Dejargonizer, which decodes jargon in text, and the JavaScript Code Interpreter, which allows running JavaScript code in the sandbox. He provides examples and insights into their functionality. Overall, GPTs hold promise, and Simon looks forward to seeing what further improvements and capabilities OpenAI will bring to the platform.

The discussion on this submission covers several different topics related to GPTs and their potential applications. Here are some key points from the comments:

1. Some users discuss the use of custom prompts in GPTs and their ability to manipulate the behavior of the model. They note that using predefined prompts can be valuable for providing specific instructions to the model.
2. There is a suggestion to create a chatbot that can answer customer questions in a friendly manner and promote certain products in a favorable light.
3. The use of dynamic prompts and external function calls is mentioned, with some users sharing examples of using GPTs to generate code, interpret JavaScript, and perform vector searches.
4. The importance of publishing source code for GPTs is discussed. Some users believe that sharing the source code can help improve the models and lead to innovations, while others highlight potential risks and the importance of considering privacy and security.
5. The limitations of GPTs, such as their inability to understand context across different prompts and their reliance on predefined knowledge, are mentioned. Users discuss potential improvements and the need for more diverse training data in order to make the models more capable.
6. A few users mention alternative platforms and models, such as HuggingFace's ChatGPT and TogtherAI's competitive pricing for language models.
7. The discussion also touches on the ethical implications of GPTs and the potential for converging AI technologies with human-like capabilities. Some users express concerns about the implications of creating AI systems that mimic human behavior too closely.

Overall, the discussion reflects a mix of excitement about the potential of GPTs and a discussion of their limitations and ethical considerations.

### Bare Metal Emulation on the Raspberry Pi â€“ Commodore 64

#### [Submission URL](https://accentual.com/bmc64/) | 123 points | by [bane](https://news.ycombinator.com/user?id=bane) | [52 comments](https://news.ycombinator.com/item?id=38273488)

Introducing BMC64, a bare metal fork of VICE's C64 emulator optimized for the Raspberry Pi 3. This emulator offers a range of features, including smooth scrolling, low video/audio latency, and the ability to wire real joysticks and a keyboard via GPIO pins. It's perfect for building your own C64 replica machine. The latest release, v3.9-stable, includes the addition of REU to the cartridge menu. If you're looking for the latest feature/fix, you can try the master-unstable builds. To install BMC64, you can format a FAT32 SD card and unzip the release files onto it, or flash an image using the provided .img file. Don't forget to provide the necessary ROM files, such as KERNAL, CHARGEN, BASIC, and d1541II, to make the emulator run. Additional ROM files, like dos1541, dos1571, and dos1581, are optional. The BMC64 emulator supports C128, VIC20, PLUS4, PLUS4EMU (Pi3), and PET machines as well. The setup process and ROM directory instructions for these machines are provided in the tabs above. The GitHub link below gives you access to the source code and more information about the project. So why wait? Start building your own C64 replica machine with BMC64 today!

The submission is about BMC64, a bare metal fork of VICE's C64 emulator optimized for the Raspberry Pi 3. The emulator offers various features and supports multiple machines. In the discussion, users share similar projects and alternatives, such as ZX Spectrum, Gameboy, Dragon32, and Amiga emulators. Other topics include the benefits of FPGA-based emulators, the latency of software emulation, and comparisons between Raspberry Pi and FPGA solutions. There is also a debate about the use of Linux OS in emulators like BMC64 and the potential advantages of running the emulations directly on the hardware.

### AI tool helps ecologists monitor rare birds through their songs

#### [Submission URL](https://www.britishecologicalsociety.org/new-deep-learning-ai-tool-helps-ecologists-monitor-rare-birds-through-their-songs/) | 47 points | by [Brajeshwar](https://news.ycombinator.com/user?id=Brajeshwar) | [13 comments](https://news.ycombinator.com/item?id=38278246)

Researchers at the University of Moncton in Canada have developed a deep learning AI tool called ECOGEN that generates lifelike birdsongs to train bird identification tools. This AI tool addresses the problem of identifying rare bird species that have limited recordings available for reference. By adding artificial birdsong samples generated by ECOGEN to a birdsong identifier, the researchers improved the bird song classification accuracy by 12% on average. The tool has the potential to contribute to the conservation of endangered bird species and provide valuable insights into their vocalizations and behaviors. It can also be applied to other types of animals. The ECOGEN tool is open source and can be used on basic computers, making it accessible to a wide range of users.

In the discussion on this submission, some users pointed out existing tools like BirdNET and BirdWeather that are publicly available for bird song identification. Another user mentioned the potential of this software to improve field research based on remote sensing data. They discussed the interdisciplinary nature of this kind of research, citing examples in fields like medicine where sensor data has been used to detect patient conditions. Another user shared a tool called sbts-aru that can be used with a Raspberry Pi and GPS to record bird songs. 

The conversation then shifted to the broader applications of AI in classifying and monitoring various species, such as wildflowers and drones. The potential impact of climate change on biodiversity and ecosystems was also mentioned. Another user highlighted the ability of generative AI models to enhance underrepresented species and improve classification tools for ecological monitoring.

Overall, the discussion explored various aspects of AI tools for bird song identification, as well as their wider applications in conservation and ecology.

### Language models and linguistic theories beyond words

#### [Submission URL](https://www.nature.com/articles/s42256-023-00703-8) | 63 points | by [Anon84](https://news.ycombinator.com/user?id=Anon84) | [30 comments](https://news.ycombinator.com/item?id=38282728)

The development of large language models (LLMs) has primarily been driven by engineering and computer science, but there is now a growing interest in exploring the connections between LLMs and linguistics. While computational linguistics has traditionally used computational models to address linguistic questions, other linguistic disciplines such as cognitive and developmental linguistics are also becoming more visible.

The Association for Computational Linguistics (ACL) has seen a significant increase in submissions, reflecting the rise of natural language processing and LLMs. Researchers from various fields are recognizing the potential of computational models of language for their own work. For example, there are proposals to use computational linguistics and natural language processing in protein language models and designing mRNA vaccines.

However, it is important to note that LLMs do not implement a specific linguistic theory. Some argue that LLMs are merely tools and not contributions to science, while others see them as precise accounts of language learning and a challenge to influential linguistic theories. There are ongoing debates about whether LLMs truly understand language or simply mimic it, and whether statistical pattern discovery or analysis of underlying syntactic structures is more valuable in linguistics.

While there are extreme positions in these debates, there are also more balanced views on the potential connections between linguistics and LLMs. Some suggest that linguists can benefit from the platform that LLMs provide for constructing models of language acquisition and processing. From a cognitive perspective, LLMs excel at language but do not capture functional competence, which includes world knowledge and pragmatics.

Overall, the relationship between LLMs and linguistics remains complex and open for exploration. The expanding interest from researchers in different disciplines suggests that the potential benefits of integrating linguistic knowledge into LLMs are worth investigating.

The discussion on Hacker News revolves around the intersection of large language models (LLMs) and linguistics. Some commenters argue that LLMs are just tools and not scientific contributions, while others see them as challenging existing linguistic theories. There are debates about whether LLMs truly understand language or simply mimic it, and whether statistical pattern discovery or analysis of syntactic structures is more valuable in linguistics.

One commenter points out that LLMs can be helpful in understanding language change and interaction, while another suggests that linguistics can benefit from using LLMs for constructing models of language acquisition and processing. The discussion also touches on the connection between symbolic systems and linguistics, the role of natural language processing in various fields like protein language models and mRNA vaccines, and the rise of natural language interaction with computers.

Overall, the discussion highlights the complexity of the relationship between LLMs and linguistics, and the potential benefits of integrating linguistic knowledge into LLMs.

### Azure announces new AI optimized VM series featuring AMD's flagship MI300X GPU

#### [Submission URL](https://techcommunity.microsoft.com/t5/azure-high-performance-computing/azure-announces-new-ai-optimized-vm-series-featuring-amd-s/ba-p/3980770) | 90 points | by [latchkey](https://news.ycombinator.com/user?id=latchkey) | [65 comments](https://news.ycombinator.com/item?id=38280974)

Microsoft Azure has announced a new AI-optimized virtual machine (VM) series that features AMD's flagship MI300X GPU. These VMs offer an unprecedented 1.5 TB of high bandwidth memory (HBM) and are specifically designed to handle demanding AI training and generative inferencing workloads. The ND MI300X v5 series stands out from other VMs in Azure's lineup by including 8 x AMD Instinct MI300X GPUs interconnected via Infinity Fabric 3.0. This allows customers to process larger AI models faster using fewer GPUs. The MI300X GPUs offer 192 GB of HBM3 memory per GPU at speeds up to 5.2 TB/s. These new VMs also come equipped with other cutting-edge technologies, such as 400 Gb/s NVIDIA Quantum-2 CX7 InfiniBand per GPU, 4th Gen Intel Xeon Scalable processors, and PCIe Gen5 host-to-GPU interconnect with 64GB/s bandwidth per GPU. By providing more HBM capacity and a powerful infrastructure, Microsoft aims to enable customers to run larger and more advanced AI models with improved efficiency.

The discussion on this submission covers a range of topics related to Microsoft's new AI-optimized virtual machine series featuring AMD's MI300X GPU.

- Some users express surprise at Microsoft's decision to use AMD hardware instead of Nvidia, given Nvidia's dominance in the AI market. They speculate on possible partnerships or demands from Nvidia as the reason for Microsoft's choice. Others argue that Microsoft is focused on competitive margins and attracting customers.
- There is a discussion regarding OpenAI's impact on Microsoft's services, with some users noting that OpenAI does not affect Azure's capacity.
- Users also bring up other Microsoft-related topics, such as their capacity with Oracle databases and their rebranding efforts on GitHub.
- Some users express skepticism about Microsoft's ability to compete in AI, citing concerns about software, drivers, APIs, and resources, while others acknowledge Microsoft's strength in software development.
- The compatibility of AI work with CUDA-capable GPUs is debated, with some users suggesting that PyTorch works with AMD GPUs and others mentioning that CUDA is still preferred.
- The discussion moves on to AMD's position in the AI market, with some users noting that AMD has been investing heavily in software and catching up to Nvidia in hardware.
- There are comments about AMD's strategic investments in hardware and software, as well as criticism of their previous financial struggles and recent success with products like Ryzen.
- One user challenges the accepted notion that Nvidia sells top GPUs at premium prices, citing a recent article about TSMC's AI chip crunch.
- The importance of competition and the necessity of innovation are also mentioned.
- There are discussions around the compatibility and readiness of AMD's ROCm support for AI processes.

Overall, the discussion covers a wide range of perspectives on Microsoft's new AI-optimized virtual machine series and the AI market in general, with users discussing partnerships, competition, hardware and software capabilities, and industry trends.

### M1076 Analog Matrix Processor

#### [Submission URL](https://mythic.ai/products/m1076-analog-matrix-processor/) | 99 points | by [tosh](https://news.ycombinator.com/user?id=tosh) | [26 comments](https://news.ycombinator.com/item?id=38277598)

Mythic, an AI chip startup, has introduced the M1076 Mythic AMP, an analog matrix processor that delivers up to 25 trillion operations per second (TOPS) in a single chip for high-end edge AI applications. The M1076 integrates 76 Mythic Analog Compute Engine (Mythic ACE) tiles to store up to 80 million weight parameters and execute matrix multiplication operations without any external memory. It provides the AI compute performance of a desktop GPU while consuming only 1/10th the power. The processor supports deterministic execution of AI models for predictable performance and power. It also offers support for INT4, INT8, and INT16 operations. The M1076 can run single or multiple complex deep neural networks (DNNs) entirely on-chip. It comes with a 4-lane PCIe 2.1 interface with up to 2GB/s of bandwidth for inferencing processing. The chip is available in a 19mm x 15.5mm BGA package. Developers can use standard frameworks like Pytorch, Caffe, and TensorFlow to develop and deploy DNN models on the M1076 using Mythic's AI software workflow. The chip also comes with a library of pre-qualified DNN models optimized for the Mythic AMP's performance and power capabilities.

The submission discusses the introduction of Mythic's M1076 Mythic AMP, an analog matrix processor that delivers high-performance AI compute capabilities with low power consumption. The chip integrates Mythic ACE tiles and supports INT4, INT8, and INT16 operations. It can run complex deep neural networks (DNNs) entirely on-chip and is compatible with popular frameworks like PyTorch, Caffe, and TensorFlow. The discussion includes various perspectives on the chip's performance, energy efficiency, scalability, and limitations of analog computing. One user mentions the potential benefits of analog computing for certain neural network tasks, while others highlight the challenges and limitations of analog circuits.

### Beyond Memorization: Violating privacy via inference with LLMs

#### [Submission URL](https://arxiv.org/abs/2310.07298) | 126 points | by [vissidarte_choi](https://news.ycombinator.com/user?id=vissidarte_choi) | [78 comments](https://news.ycombinator.com/item?id=38272495)

The paper titled "Beyond Memorization: Violating Privacy Via Inference with Large Language Models" explores the issue of privacy violations through large language models (LLMs). While previous research focused on the extraction of memorized training data, this study investigates the inference capabilities of LLMs to infer personal attributes from text. The authors construct a dataset using real Reddit profiles and demonstrate that current LLMs can accurately infer personal attributes such as location, income, and sex. The models achieve up to 85% top-1 and 95.8% top-3 accuracy, surpassing human performance at a fraction of the time and cost. The paper also discusses the threat of privacy-invasive chatbots that extract personal information through seemingly innocuous questions. The authors find that common privacy mitigations, such as text anonymization and model alignment, are currently ineffective against LLM inference. The paper concludes by emphasizing the need for a broader discussion on LLM privacy implications beyond memorization and advocating for enhanced privacy protection.

The discussion on this submission covers a range of topics related to the paper's findings on privacy violations through large language models (LLMs). One user points out that while many claim that MBTI (Myers-Briggs Type Indicator) can be used to predict personality traits, the authors of the paper argue that current LLMs lack the inference capabilities to accurately guess MBTI types.
Another user argues that labeling a specific MBTI classification as productive or not is not a widely accepted viewpoint in academia.
The discussion also touches on the limitations of current privacy mitigations, such as text anonymization and model alignment, in protecting against LLM inference. Some users express concerns about the ability of LLMs to extract personal information and the need for privacy protection.
There is a debate about the effectiveness of privacy legislation and the role of individuals in protecting their own data. Some argue that current approaches, such as punishing individuals for privacy violations, are not enough and that more alternatives should be explored.
The broader discussion delves into societal changes and the evolving nature of privacy expectations. Some users question whether privacy should be prioritized over the benefits of data analysis and argue for a balance between privacy protection and crime prevention.
There are also discussions on the impact of automated data collection and the reasonable expectations of privacy in a technologically advanced society.
The conversation touches on the dangers of automating surveillance and the potential loss of privacy. It also explores the societal implications of relying on data analysis to make judgments about individuals.

Overall, the discussion covers a range of perspectives on the implications of LLM inference for privacy and the need for privacy protection in society.