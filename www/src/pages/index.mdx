import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Wed Jan 28 2026 {{ 'date': '2026-01-28T17:21:48.791Z' }}

### Trinity large: An open 400B sparse MoE model

#### [Submission URL](https://www.arcee.ai/blog/trinity-large) | 226 points | by [linolevan](https://news.ycombinator.com/user?id=linolevan) | [73 comments](https://news.ycombinator.com/item?id=46789561)

Trinity 
\\(400B) drops as a highly sparse MoE—and you can try the Preview free on OpenRouter for a limited time.

What’s new
- Model family: Three checkpoints
  - Trinity-Large-Preview: lightly post-trained, chat-ready “instruct” model (not a reasoning model yet)
  - Trinity-Large-Base: best 17T-token pretraining checkpoint (frontier-class foundation model)
  - Trinity-Large-TrueBase: early 10T-token checkpoint with no instruct data or LR anneals (a “true base”)
- Architecture: 400B-parameter sparse MoE, 256 experts with 4 active per token (≈1.56% routed; ~13B active params/token). They doubled dense layers (3→6) to keep routing stable at this sparsity—more aggressive than most peers, rivaled mainly by Llama 4 Maverick.
- Training scale: 2048 Nvidia B300 GPUs; they claim it’s the largest publicly stated pretraining run on B300s. Finished pretraining in 33 days.

Why it matters
- Speed: Thanks to high sparsity and efficient attention, they report 2–3x faster inference/training for the same hardware versus peers in the same “weight class.”
- Performance: Trinity-Large-Base scores at frontier level across math, coding, scientific reasoning, and knowledge benchmarks, per the team.
- Data engine: 17T tokens across three phases (10T + 4T + 3T), curated by DatologyAI, including 8T+ synthetic tokens spanning web, code, math, reasoning, and multilingual (targeting 14 non‑English languages).

How they made a 400B MoE behave
- Momentum-based expert load balancing: adjusts per-expert router bias up/down with tanh clipping, momentum smoothing, plus a per-sequence balance loss to avoid within-sequence hot spots.
- z-loss: regularizes LM head to prevent logit scale creep; they monitor logit stats for early instability signals.
- Parallelism and optimizer: HSDP with expert parallelism 8 (yielding 2048 DP ranks); scaled batch size after 5T tokens. Used Muon (supports larger critical batch than AdamW), guided by MiniMax-01 batch-scaling results. Smooth loss curve with clear phase transitions.

Caveats
- The Preview is intentionally a non-reasoning instruct model; a full reasoning variant is in post-training and will take longer due to extra tokens per output, but sparsity helps speed RL rollouts.

Try it
- Trinity-Large-Preview is available free on OpenRouter for a limited time.

**The Wall, The Cost, and The Benchmarks**
The release of Trinity Large (400B) sparked a broad debate on the current state of frontier AI models, with conversation shifting from the specific model to the trajectory of the industry at large.

*   **Is AI hitting a "brick wall"?** A significant portion of the discussion focused on whether LLM progress has plateaued. Some users argued that recent gains are marginal—comparing the ELO gap between Gemini 1.5 Pro and GPT-4o as feeling like a "coin flip" (50% chance of being better) rather than a leap. Others pushed back, noting that ELO statistics imply a significant win rate (70%) despite looking close on charts, and argued that progress has shifted from linear chat improvements to step-change capabilities in coding and complex math (citing Terence Tao’s recent use of o1).
*   **Skepticism of benchmarks:** Trust in public leaderboards like LMSYS Arena is eroding among some HN users, who described them as "Markdown usage detectors" or measures of style rather than reasoning. Participants suggested that real progress is now found in private evaluations or harder, specific benchmarks (like LiveBench or METR) rather than general chat arenas.
*   **The economics of "33 days":** Commenters scrutinized the claim that the model finished pretraining in 33 days on 2048 Nvidia B300s. While some calculated the raw compute cost at roughly $10 million, critics noted this figure is misleading. They argued that quoting a "final run" cost ignores the tens of millions spent on failed experiments, dataset curation, and infrastructure setup, similar to misunderstandings around DeepSeek’s reported costs.
*   **Early feedback:** Initial user impressions were mixed to skeptical. One user reported the model failed to understand technical concepts (specifically GitHub Action DAGs), while others doubted the validity of comparing a 400B sparse MoE to "Llama 4" architecture without independent verification. Use cases for running such large models at home are also becoming increasingly infeasible due to hardware costs.

### LM Studio 0.4

#### [Submission URL](https://lmstudio.ai/blog/0.4.0) | 209 points | by [jiqiren](https://news.ycombinator.com/user?id=jiqiren) | [114 comments](https://news.ycombinator.com/item?id=46799477)

LM Studio 0.4.0: headless server, parallel inference, and a stateful chat API

What’s new
- Headless core (“llmster”): The LM Studio engine now runs as a standalone daemon without the GUI, suitable for servers, CI, Colab, or terminal-only workflows. Quick start: install (curl/irm), lms daemon up, lms get <model>, lms server start, lms chat.
- Parallel requests with continuous batching: llama.cpp 2.0.0 brings concurrent inference to the same model. New loader options:
  - Max Concurrent Predictions: cap simultaneous requests; extras queue.
  - Unified KV Cache (default): shared, non-partitioned cache for variable-length requests.
  - Note: MLX engine doesn’t support this yet; it’s coming.
- Stateful REST API: POST /v1/chat maintains conversation state via response_id/previous_response_id, keeps payloads small, and returns detailed perf stats (tokens in/out, speed, time-to-first-token). It can also use locally configured MCP tools (permission-gated).
- Permission keys: Gate which clients can access your LM Studio server (Settings > Server).
- UI refresh: Split View (side-by-side chats), chat export (PDF/Markdown/text), Developer Mode (advanced options), and in‑app docs.
- New CLI flow: lms chat for interactive terminal sessions; plus lms runtime update and other streamlined commands.

Why it matters
- Deploy anywhere: Decoupling the core from the app makes local-first models practical on cloud/GPU rigs and in automation.
- Higher throughput: Continuous batching + concurrency turns LM Studio into a more capable API server for multi-user or web workloads.
- Easier app integration: A stateful /v1/chat simplifies multi-step workflows and tool use without shipping full transcripts.

Notable fixes
- MCPs now lazy-load on demand.
- Various UI/installer bugs squashed (icons, model picker, downloads, lms import).

**LM Studio vs. Ollama:** The most active area of discussion compared the two tools. Users debated their underlying architectures:
*   **Model Storage:** several commenters criticized Ollama’s decision to mimic Docker by using a registry and "blob" storage system. While this makes pulling models easy for non-technical users, it frustrates developers because it duplicates files and makes it difficult to share model weights across different applications. In contrast, users praised LM Studio for using standard directories and GGUF files that can be easily accessed by other tools.
*   **Convergence:** Commenters noted that the tools are functionally converging from opposite directions. Ollama started as a CLI/API tool and is adding UI elements, while LM Studio started as a GUI tool and is now adding headless/CLI capabilities.

**Headless Mode & CLI:** The introduction of the headless "llmster" core was well-received by those wanting to run LM Studio on servers or CI environments without the desktop overhead.
*   There was initial confusion regarding whether the desktop app needed to remain open for the CLI to function; users clarified that the new daemon allows for a truly headless experience.
*   Mac users specifically highlighted MLX support as a primary reason for sticking with LM Studio over other runners.

**Performance & UI:**
*   **Throughput:** There was a debate regarding the new parallel request feature. While some worried that splitting resources would halve performance per user, others clarified that continuous batching allows for significantly higher total token throughput (citing up to 1300 t/s on M-series chips) by utilizing idle compute time.
*   **Visuals:** The UI refresh proved polarizing; some users complained that the new "dark mode" is actually grey and criticized the aesthetic for looking like a "toy" or "WhatsApp" rather than a professional developer tool.
*   **Licensing:** A few users expressed a wish that LM Studio itself were open source, noting that while the underlying engines (llama.cpp) are open, the wrapper remains proprietary.

### Show HN: A MitM proxy to see what your LLM tools are sending

#### [Submission URL](https://github.com/jmuncor/sherlock) | 205 points | by [jmuncor](https://news.ycombinator.com/user?id=jmuncor) | [110 comments](https://news.ycombinator.com/item?id=46799898)

Sherlock: Real-time LLM token tracker and prompt inspector for CLI tools

What it is
- A local HTTP proxy + terminal dashboard that intercepts LLM API calls to show live token usage, context-window “fuel gauge,” and recent requests.
- Automatically archives every prompt/response as markdown and JSON for debugging and review.
- Targets CLI workflows with zero config: wrap your tool and watch stats update in real time.

How it works
- Start the proxy/dashboard: sherlock start (defaults to localhost:8080; set token limit with -l).
- Run your tool through Sherlock: sherlock claude, sherlock codex, or sherlock run --provider <name> <cmd>.
- Dashboard highlights context usage (green/yellow/red) and prints a session summary on exit.

Why it matters
- Quickly spot runaway token usage and context overflow.
- Track costs via token counts during development.
- Create an audit trail of prompts to refine and debug your prompting.

Supported/limits
- Providers: Anthropic (Claude Code) and OpenAI Codex supported.
- Gemini CLI blocked by an upstream base-URL issue when using OAuth.
- Python 3.10+; local interception only (be mindful of sensitive data in saved logs).

Repo
- MIT license. ~455 stars, 16 forks.
- GitHub: https://github.com/jmuncor/sherlock

Based on the discussion, the community response focused primarily on a critical security flaw and the broader implications of AI-generated code ("vibe coding"). Here is the summary:

**Critical Security Vulnerability**
*   **TLS Disabled:** Users immediately identified that the tool unconditionally disabled TLS verification (`ssl_insecure=true` passed to mitmproxy). Code reviewers noted this exposes users to Man-in-the-Middle (MITM) attacks if used on public networks (e.g., coffee shops).
*   **The Fix:** The author (OP) acknowledged the mistake and pushed a fix during the discussion to implement a simple HTTP relay that eliminates the insecure mitmproxy implementation.

**"Vibe Coding" and AI Generation**
*   **AI Authorship:** Commenters noted the code contained artifacts indicating it was generated by AI (e.g., git commits "Co-Authored-By Claude Opus").
*   **Critique of "Vibe Coding":** The security flaw was cited as a prime example of the dangers of "vibe coding"—generating software via LLMs without possessing the technical knowledge to audit or understand the output. Critics argued that disabling SSL is a "huge red flag" that a human developer would likely catch, but an AI might insert to solve a debugging error.
*   **Open Source "Pollution":** Senior engineers expressed frustration that AI allows inexperienced developers to create projects with professional-looking READMEs, garnering hundreds of stars despite having "student-level" or dangerous code internals. This makes it difficult to distinguish production-ready software from prototypes.

**Market Need**
*   **Enterprise Governance:** Despite the code quality issues, some commenters validated the problem statements, noting a lack of standard enterprise tools for governing AI data usage, auditing prompts, and managing token budgets in large organizations.
*   **Implementation:** Some suggested this functionality would be better implemented as a standard plugin/addon for existing tools like `mitmproxy` rather than a standalone wrapper.

### UK Government’s ‘AI Skills Hub’ was delivered by PwC for £4.1M

#### [Submission URL](https://mahadk.com/posts/ai-skills-hub) | 379 points | by [JustSkyfall](https://news.ycombinator.com/user?id=JustSkyfall) | [141 comments](https://news.ycombinator.com/item?id=46803119)

UK’s £4.1m “AI Skills Hub” is mostly a link list, with busted UX and errors, says critic

- The UK government launched an AI Skills Hub to train 10 million workers by 2030. According to a detailed blog post, the site—built by PwC for £4.1m (~$5.6m)—mostly aggregates links to existing external courses (e.g., Salesforce Trailhead) rather than hosting original content.
- The author calls the UI “vibecoded” and highlights basic usability issues: a tiny “Enroll Now” button, a comment section where users expect next steps, and a prominently linked “Skills & Training Gap Analysis” that appears closed to the public.
- PwC reportedly acknowledges the site doesn’t fully meet accessibility standards, which public-sector sites are legally required to follow. The post also flags a substantive content error: teaching “fair use” (a US concept) instead of the UK’s more restrictive “fair dealing.”
- Beyond the build quality, the author is angry about value for money, arguing small UK dev shops could deliver better for a fraction of the cost, and that the public likely won’t use the site in its current state.

Why it matters: Another government IT procurement raising questions about cost, accessibility compliance, and delivery quality. Open questions include what the £4.1m covered (e.g., discovery, hosting, support, marketing, licensing) and whether further phases are planned to fix core issues.

**Discussion Summary:**

The commentary focuses heavily on the structural incentives of government procurement that lead to outcomes like this. Users argue that the "nobody gets fired for buying IBM" adage explains the selection of PwC: civil servants prioritize risk mitigation over product quality, knowing that hiring a "respectable" large firm shields them from personal blame if the project fails, whereas hiring a small, agile shop constitutes a career risk.

Key points of debate include:
*   **Barriers to Entry:** A significant portion of the thread discusses certifications (specifically ISO 9000) as a form of "grift" or structural gatekeeping. Users note these requirements filter out capable small studios that cannot afford the bureaucratic overhead, favoring large consultancies that specialize in compliance rather than software development.
*   **Process vs. Outcome:** Commenters suggest the high cost isn't just for the code, but for the immense friction of the procurement process itself (endless paperwork, insurance requirements, and audits). The consensus is that the system rewards following legal procedure perfectly rather than delivering a working product.
*   **The "Horizon" Counterpoint:** Some users pushed back on the idea that big firms are "safe," citing the Fujitsu/Post Office scandal, though others retorted that despite such scandals, the procurement habits of the civil service remain unchanged due to entrenched risk aversion.

### Will AIs take all our jobs and end human history, or not? (2023)

#### [Submission URL](https://writings.stephenwolfram.com/2023/03/will-ais-take-all-our-jobs-and-end-human-history-or-not-well-its-complicated/) | 90 points | by [lukakopajtic](https://news.ycombinator.com/user?id=lukakopajtic) | [161 comments](https://news.ycombinator.com/item?id=46797865)

Stephen Wolfram: Will AIs Take All Our Jobs? It’s Complicated

- Shock factor: ChatGPT shattered the sense that essay writing is uniquely human. It stitches together human-like text by following patterns learned from billions of webpages and millions of books.
- What’s really going on: Under the hood is a brain-like neural net doing “raw computation.” Meaning doesn’t emerge from the machine alone; it comes from the human-created data it trains on—and from the human who supplies the goal via a prompt.
- The new interface: Wolfram frames ChatGPT as a “linguistic user interface” (LUI). You provide intent in plain language; the system expands it into coherent prose grounded in shared human context.
- The big shift: “Essayification” just became cheap—like desktop publishing did for typesetting. A polished essay is no longer evidence of effort. The scarce work moves from writing to specifying intent, constraints, and judgment.
- Why prediction is hard: Expect surprises. Wolfram leans on “computational irreducibility” to argue that the trajectory of AI capabilities and social effects can’t be neatly forecast.
- What’s next (teed up for later sections): If AIs can pursue defined goals, the hard part is deciding which goals matter, how to evaluate progress, and how governance should adapt.

Takeaway: Writing as a skill is being unbundled from effort; the human edge shifts to framing problems, setting goals, and providing oversight.

**The American Fear vs. Global Safety Nets**
A significant portion of the discussion centered on whether the existential dread regarding AI displacement is a uniquely American phenomenon. User `mips_avatar` argued that Americans are more stressed because basic survival (healthcare, housing stability) and self-worth are strictly intermediated by corporate employment. By contrast, they suggested that in places with stronger social safety nets (citing Vienna and Vietnam), AI is viewed less as a threat to survival and more as a tool.

**The "Cushy Job" Theory**
Some users pushed back on the safety-net theory, suggesting the anxiety stems from the nature of the US workforce. User `nlyrlczz` posited that the US has a high density of "cushy paper-pushing" and high-paid white-collar roles that are specifically vulnerable to the "essayification" capabilities of LLMs. Others noted that anxiety is not geographically exclusive, with user `trnd` mentioning similar concerns in India.

**The Breakdown of the Labor-Capital Bargain**
Discussion moved to the macroeconomic implications (user `myrmdn`, `BirAdam`). Commenters worried that AI undermines the primary mechanism for wealth redistribution: labor. If AI makes human labor nearly valueless, the current capitalist model of "work for income" fails. The thread debated potential outcomes, ranging from a necessary shift toward socialism or UBI to a dystopian increase in inequality where class barriers become insurmountable.

**Inequality and Deflation**
User `BirAdam` offered a specific economic sequence: AI reduces production costs, which should theoretically lower prices (deflation). However, the "scary part" is the transition period where unemployment spikes before the cost of living drops to match the new reality. There was also debate regarding how deeply AI has actually permeated non-tech sectors, with users clashing over whether rural populations (like farmers) are oblivious to AI or actively using it for high-tech capital management.

### Jellyfin LLM/"AI" Development Policy

#### [Submission URL](https://jellyfin.org/docs/general/contributing/llm-policies/) | 196 points | by [mmoogle](https://news.ycombinator.com/user?id=mmoogle) | [102 comments](https://news.ycombinator.com/item?id=46801976)

The open‑source media server Jellyfin published a clear policy on AI use across its repos and community spaces, aiming to protect code quality while allowing limited, accountable assistance.

What’s new
- No AI-written communication: Issues, feature requests, PR descriptions, and forum/chat posts must be in the contributor’s own words. Exception: clearly labeled LLM-assisted translations, ideally with the original language included.
- Strict rules for code contributions: 
  - Keep PRs focused; no unrelated drive-by changes. Large changes must be split into small, reviewable commits.
  - Meet formatting and quality standards; don’t commit LLM meta/config files.
  - Explain changes yourself in the PR body; if you can’t articulate what and why, it won’t be accepted.
  - Code must build, run, and be explicitly tested.
  - Be prepared to address reviewer feedback without outsourcing fixes to an LLM.
  - “Vibe coding” (letting an LLM loose on the codebase) will be rejected.
- Reviewer discretion: Oversized, over-complex, or squashed PRs that can’t be reasonably reviewed will be rejected, LLM-assisted or not.
- Community sharing: Non-official tools primarily built with LLMs must be clearly labeled as such; users can decide if that’s acceptable. (The policy also hints at guidance for secondary AI assistance like docs/formatting.)

Why it matters
- Signals a maturing norm: maintainers embracing AI as a tool, but demanding authorship, accountability, and testable, reviewable changes.
- Sets expectations for contributors: AI can help, but you own the code and the explanation.
- Likely to spark debate across OSS: balancing velocity vs. maintainability as AI-generated contributions surge.

Based on the discussion, the community reaction focuses heavily on the nuances of communication, the "asymmetry of effort," and the role of language barriers in open source.

**Language Barriers vs. Authenticity**
A significant portion of the debate centers on non-native English speakers. While some argue that LLMs are vital accessibility tools that make open source global, many established contributors argue that "broken English" is preferable to AI-generated prose.
*   Critics note that humans are excellent at deciphering intent from imperfect grammar, whereas LLMs often bury meaning under "hyper-excited corporate drone style" fluff or introduce subtle hallucinations.
*   Several users suggested that providing the original native-language text alongside a standard machine translation (like Google Translate) is more respectful and effective than trying to pass off ChatGPT output as fluent English.

**The Asymmetry of Effort**
Commenters expressed strong feelings that using LLMs for communication is often "disrespectful."
*   The core argument is an imbalance of mental energy: the sender expends zero effort to generate a wall of text, while the recipient must spend high "mental capital" to decode it.
*   One user described this as sending 10 paragraphs of output for a 1-sentence prompt, forcing maintainers to sift through "slop."
*   However, a counter-argument appeared suggesting LLMs can be used virtuously to *shorten* and distill rambling thoughts, provided the user reviews the output.

**Writing is Thinking**
Referencing author Ted Chiang, users argued that writing isn't just a way to convey information—it is the process of thinking itself.
*   By outsourcing the description of a Pull Request (PR) or a feature to an LLM, the contributor may be bypassing the critical cognitive work required to fully understand what they have built.
*   The consensus leans toward agreement with Jellyfin: if you cannot articulate what your code does or why it is necessary, you likely do not understand it well enough to contribute it.

**Distinction Between Code and Text**
While there was near-universal agreement that "vibe coding" (blindly trusting AI code) is bad, the friction lies in the "no AI communications" rule. Some feel strict enforcement against grammar-checking tools is too harsh, while others believe that the "unresolved cognitive dissonance" of letting AI write code while banning it from explaining that code makes the policy logically consistent.

### The new era of browsing: Putting Gemini to work in Chrome

#### [Submission URL](https://blog.google/products-and-platforms/products/chrome/gemini-3-auto-browse/) | 21 points | by [xnx](https://news.ycombinator.com/user?id=xnx) | [3 comments](https://news.ycombinator.com/item?id=46799289)

Google is turning Chrome into an AI copilot powered by Gemini 3. Highlights:

- New side panel assistant: Gemini now lives in a persistent side panel so you can multitask without switching tabs—summarize pages, compare options, wrangle calendars, and more.
- On-the-fly image edits: “Nano Banana” lets you transform images directly on the page via prompts (no downloading/re-uploading).
- Connected Apps: Deeper integrations with Gmail, Calendar, YouTube, Maps, Shopping, and Flights. Examples include pulling event details from email, checking flight options, then drafting a note to teammates.
- Personal Intelligence (coming months): Opt-in memory and preferences for context-aware, proactive help across browsing. You can connect/disconnect apps and set custom instructions.
- Auto browse (agentic actions): For AI Pro/Ultra subscribers in the U.S., Chrome can handle multi-step chores—trip planning across dates, scheduling, form-filling, gathering tax docs, getting service quotes, subscription management, even shopping from a reference photo with budgets and discount codes. With permission, it can use Google Password Manager for sign-ins.

Why it matters:
- Pushes Chrome from “tool” to “agent,” competing with Microsoft’s Copilot in Edge and emerging agentic browsers.
- Big productivity upside, but raises familiar questions: privacy (what data powers context), safety (auto form-filling), and ecosystem lock-in via Connected Apps.
- Subscription gating (AI Pro/Ultra) puts the most powerful agentic features behind a paywall.

Availability:
- Gemini in Chrome on macOS, Windows, and Chromebook Plus. Side panel and image transforms available to all Gemini-in-Chrome users; Personal Intelligence rolls out in the coming months; Auto browse limited to U.S. AI Pro/Ultra.

**The Discussion**

Commenters viewed Google’s shift toward "agentic browsing" as a forced integration of LLMs, drawing negative comparisons to Microsoft’s strategy with Copilot. Skepticism ran high regarding the target audience; some noted that the Hacker News community has likely already abandoned Chrome due to the Manifest V3 controversy (which impacts ad blockers). Others wasted no time asking how to disable the new features entirely.

### Kairos: AI interns for everyone

#### [Submission URL](https://www.kairos.computer/) | 30 points | by [bamitsmanas](https://news.ycombinator.com/user?id=bamitsmanas) | [27 comments](https://news.ycombinator.com/item?id=46792225)

What’s new: Kairos pitches a doer, not a chatter—an agent with its own browser that logs into sites (even behind logins), clicks buttons, fills forms, and ships results across your stack.

Highlights
- 20+ native integrations: Gmail, Calendar, Slack, GitHub, Notion, Sheets, Outlook, HubSpot, Linear, Airtable, Drive/Docs, Teams, Zoom, Dropbox, Box, Calendly, Twitter/Reddit, and more.
- Runs while you’re away: schedule tasks or set triggers; delivers reports, summaries, and proposals.
- End‑to‑end workflows: e.g., find YC AI startups → enrich via LinkedIn → filter/process → sync to Airtable → post a Slack report.
- Real-world ops: auto‑screen candidates in Greenhouse and book screens; scan inbox for refund requests and act per policy; find mutual availability via Calendly and your calendar.
- Teach by demo: share your screen once; it learns the steps and automates thereafter.

Positioning: “ChatGPT talks. Kairos does.” It emphasizes agentic browsing plus app orchestration over pure chat.

Pricing
- Free: a handful of tasks, SMS/WhatsApp messaging, all integrations, scheduled/recurring tasks, priority support.
- Plus: $37/month for higher limits; includes the same integrations and scheduling.

Open questions for HN readers
- Security and access: how are credentials handled, 2FA/session persistence, audit logs, and data boundaries across apps?
- Reliability: how it copes with changing UIs, anti-bot measures, rate limits, and long‑running tasks.
- Scope: which tasks are truly hands‑off vs needing human-in-the-loop review, and how errors are surfaced and corrected.

**The Top Story:**
**Kairos: An “AI Intern” That Automates Browser Workflows**

Today’s discussion focused on Kairos, a new entrant in the "agentic AI" space. Unlike standard LLMs that just chat, Kairos positions itself as a "doer" that can navigate the web, log into accounts (Gmail, Slack, HubSpot, etc.), and execute multi-step workflows like candidate screening or expense reporting.

**The Discussion:**
The HN community put the "intern" through a rigorous performance review, ranging from semantic debates to live technical stress tests.

*   **The "Intern" Semantics:** The branding sparked immediate debate. Some users felt the term "intern" implies sub-par work or misses the point of internships (talent development, not just cheap labor). Others maintained that "sub-par" labor is exactly what businesses try to automate—specifically the "TPS report" style data entry that humans hate. However, one commenter noted that manually filling out forms often provides necessary mental context that gets lost when automated.
*   **Performance Review (The Birdwatcher Test):** In the most substantial critique, user `bwstrgrd` tested Kairos on a specific task: finding rare bird sightings on eBird. results were mixed. The agent struggled to navigate the site efficiently, "reinvented the wheel" by creating a complex search plan, took eight minutes to execute, and returned results that were less accurate than a quick Google search. The creator (`bmtsmns`) was active in the thread, acknowledging the feedback and limitations regarding specific niche tasks.
*   **UX and Naming Collisions:** Several users complained about "scroll-jacking" on the landing page, calling it an immediate bounce factor. Others pointed out confusion with an existing Kubernetes distribution also named Kairos.
*   **Trust and Accountability:** The "intern" metaphor broke down for some regarding supervision. Users noted that human interns are supervised in real-time, whereas an AI agent executing tasks in the background creates anxiety about accountability and potential "runaway" errors within sensitive accounts.

### Please don't say mean things about the AI I just invested a billion dollars in

#### [Submission URL](https://www.mcsweeneys.net/articles/please-dont-say-mean-things-about-the-ai-that-i-just-invested-a-billion-dollars-in) | 614 points | by [randycupertino](https://news.ycombinator.com/user?id=randycupertino) | [282 comments](https://news.ycombinator.com/item?id=46803356)

The story: A sharp McSweeney’s satire by Forest Abruzzo channels a thin-skinned tech billionaire pleading with the public to stop criticizing AI—while openly listing its worst externalities. In a breathless defense, the narrator proclaims AI “the most essential tool in human history” even as he concedes it enables scams, deepfakes, job loss, copyright scraping, surveillance creep, school degradation, and autonomous weapons. The bit riffs on a recent “stop being so negative about AI” news cycle, lampooning the industry’s PR spin and wounded-ego posture.

Why it matters: It captures a growing cultural backlash to AI boosterism and the gap between lofty promises and real harms. For builders and policymakers, it’s a reminder that trust, consent, safety, and accountability aren’t PR problems—they’re product requirements.

HN angles to watch:
- Is this satire a fair critique or a strawman?
- Training data consent/copyright, deepfake harms, and regulation
- Energy/compute costs vs. benefits
- The tech industry’s sensitivity to criticism and credibility gap

Based on the discussion, here is a summary of the comments on Hacker News:

**The Purpose of a System**
A central theme of the thread revolves around the systems thinking aphorism "the purpose of a system is what it does." Commenters debated whether the high-minded intentions of AI creators matter if the practical, dominant outputs of the technology are spam, deepfakes, and astroturfing. Several users argued that if AI’s "steady state" outcome is chaos and harm, that is its engineering purpose, regardless of intent. This led to comparisons with the internet—specifically whether AI is simply a new medium for old human behaviors or if it fundamentally alters the landscape by driving the cost of generating "infinite torrents" of noise and harassment to zero.

**Supercharging Scams**
The satire’s point about scamming the elderly resonated deeply, moving from abstract debate to concrete examples. Users shared anecdotes of AI-cloned voice scams targeting grandparents (faking emergencies) and romance scams involving cloned audio of influencers (citing a specific case involving a Brazilian TikToker). The consensus among these commenters is that AI acts as a "force multiplier" for bad actors, allowing scams to scale rapidly in a way that previous technologies did not. Some pushed back with the "guns don't kill people" argument, but others retorted that when a tool makes crime this efficient, the tool itself warrants scrutiny.

**Harms vs. Utility**
While a few users defended the technology by citing benefits like coding assistance and accessibility, the prevailing sentiment in the thread leaned toward the article's critique. Users listed non-consensual sexual imagery, copyright theft, and the degradation of online trust as immediate, tangible harms that currently outweigh potential future benefits. One commenter described AI not as a tool but as an "information nuclear reactor" spewing radioactive material (spam/slop) that is poisoning the internet.

**The "Internet Analogy" Dispute**
There was significant pushback against the idea that the internet had a similarly "rocky start." Users argued that while the internet facilitated malware and flame wars, AI’s current trajectory creates different, more invasive problems—specifically the inability to distinguish truth from fiction and the massive scale of automated harassment. The comparison was generally viewed as a failure to appreciate the specific, accelerating nature of AI risks.

### Show HN: I'm building an AI-proof writing tool. How would you defeat it?

#### [Submission URL](https://auth-auth.vercel.app/) | 10 points | by [callmeed](https://news.ycombinator.com/user?id=callmeed) | [12 comments](https://news.ycombinator.com/item?id=46799402)

Authentic Author is a browser-based writing tool that tries to verify human authorship by focusing on how text is produced, not just what it says. It gives users a prompt and requires them to write 1–2 paragraphs directly in its editor, disabling paste and common DOM tricks. Behind the scenes it records typing cadence, pauses, tab switches, and window focus changes, then outputs an “Authenticity Score” estimating the likelihood the text was originally written by a human.

The pitch targets classrooms and hiring screens battling AI-written submissions, shifting detection from content analysis to process telemetry. That could deter casual misuse, but it also raises privacy and fairness questions: keystroke-level tracking can feel invasive, and scoring may penalize slow typists, non-native speakers, or people using assistive tools. As with any anti-cheat system, workarounds (e.g., scripted keystrokes or human relays) are possible, but the approach highlights a growing trend toward provenance-by-process rather than post-hoc AI detection.

The discussion on Hacker News focused on the ease of circumventing "telemetry-based" detection, with users largely dismissing the tool's effectiveness against determined attempts to cheat.

**Key themes in the conversation included:**

*   **Trivial Technical Workarounds:** Users quickly demonstrated how to fool the system. One user (`ephou7`) wrote a simple Python script using `xdotool` and random sleep intervals to simulate human typing, achieving an "Authenticity Score" of 81. Others mentioned existing automation tools and OCR scripts that can read a prompt, query an LLM, and inject the text with simulated keystrokes.
*   **The "Transcription" Loophole:** Several commenters validated that the tool checks *how* you type, not *what* you type. Users reported generating AI text on a second monitor, phone, or separate browser window and manually typing it into the editor.
    *   User `jryan49` manually typed a response from Gemini and scored "100% genuine."
    *   User `JoshuaDavid` transcribed what they described as "obvious Claude slop" and still received an 87% authenticity rating.
*   **Hardware Spoofing:** The discussion extended to hardware solutions, noting that cheap programmable USB dongles (microcontrollers) can emulate a keyboard to input AI-generated text, making it indistinguishable from a human typist at the driver level.
*   **The Return to Analog:** The consensus was that digital verification is futile against AI. Commenters argued that the only truly "AI-proof" writing environments are physical "blue book" exams, pen and paper, or strictly invigilated offline rooms.

---

## AI Submissions for Tue Jan 27 2026 {{ 'date': '2026-01-27T17:19:27.379Z' }}

### AI2: Open Coding Agents

#### [Submission URL](https://allenai.org/blog/open-coding-agents) | 225 points | by [publicmatt](https://news.ycombinator.com/user?id=publicmatt) | [40 comments](https://news.ycombinator.com/item?id=46783017)

Open Coding Agents (AI2): SERA open models + a cheap, reproducible recipe to adapt coding agents to any repo

- What’s new: AI2 released SERA (Soft-verified Efficient Repository Agents) and a fully open pipeline—models, training data, recipes, and a one-line launcher/CLI (PyPI)—to build and fine‑tune coding agents for your own codebase. Works out of the box with Claude Code.

- Performance: SERA-32B solves 54.2% on SWE-Bench Verified, edging prior open-source peers of similar size/context. Trained in ~40 GPU-days; they say reproducing prior open SOTA costs ~$400, and ~$12k can rival top industry models of the same size.

- Key idea: “Soft-verified generation” (SVG) for synthetic data—patches don’t need to be fully correct to be useful—plus a “bug-type menu” to scale/diversify data. They report SOTA open-source results at a fraction of typical costs.

- Efficiency claims: Matches SWE-smith at ~57× lower cost and SkyRL at ~26× lower cost. Fine-tuning on private code reportedly lets SERA-32B surpass its 110B teacher (GLM-4.5-Air) on repos like Django/Sympy after ~8k samples (~$1.3k).

- Inference throughput (NVIDIA-optimized): ~1,950 tok/s (BF16, 4×H100), ~3,700 tok/s (FP8), and ~8,600 tok/s on next‑gen 4×B200 (NVFP4), with minimal accuracy drop at lower precision.

- Why it matters: Puts strong, repo-aware coding agents within reach for small teams—no large-scale RL stack required—while keeping the models and data open for inspection and iteration.

Notes/caveats:
- Results center on SWE-Bench Verified; real-world repo adaptation and privacy/process for generating synthetic data from private code merit scrutiny.
- Cost/speed numbers depend on specific NVIDIA hardware and settings.

Here is a summary of the discussion:

**Comparisons and Performance Claims**
The most active debate concerned whether SERA truly reclaims the open-source SOTA title. Users pointed to Meta’s CWM models, which reportedly achieve higher scores (65% on SWE-bench Verified) when using Test-Time Selection (TTS). A discussion participant (likely a paper author) pushed back, arguing that TTS adds significant latency and cost, making it impractical for local deployment. They emphasized that SERA is optimized for efficiency and lower context lengths (32k/64k) compared to the hardware-intensive requirements of rival models.

**Openness and Licensing**
Commenters distinguished between "open weights" and "open science." While models like Mistral Small 2 and Meta’s CWM have open weights, users noted that Meta’s license restricts commercial use and does not disclose training data. In contrast, AI2 was praised for releasing the full pipeline, including training data and the recipe for synthetic data generation, allowing for genuine reproducibility and commercial application.

**Terminology: "Agent" vs. "LLM"**
There was significant semantic pushback regarding the use of the term "Agent." Users argued that an LLM itself is not an agent; rather, an agent is the combination of an LLM plus a scaffolding/harness (a loop to execute tasks). Others suggested a distinction between "Agentic LLMs" (models fine-tuned for reasoning and tool-calling) and the broader systems that utilize them.

**Fine-Tuning vs. Context Window**
Users debated the practical utility of fine-tuning a 32B model on a specific repository versus using RAG (Retrieval-Augmented Generation) with a massive context window on a frontier model (like GPT-4 or Claude 3.5).
*   **Skeptics** argued that intelligent context management with a smarter model is usually superior to fine-tuning a "dumber" model.
*   **Proponents** (one claiming to work on the "world's largest codebase") countered that fine-tuning is essential for proprietary internal libraries and syntax that general models cannot infer, even with large context windows.

**Miscellaneous**
*   **Claude Code Integration:** Some users were confused by the claim that the system requires Claude Code to run; others clarified that Claude Code is simply the harness/CLI being supported out of the box, while other open harnesses (like OpenCode or Cline) could also be used.
*   **Speed:** The inference speed (up to ~8,600 tok/s on B200s) was highlighted as a major advantage, though some questioned the specific hardware dependencies.
*   **Technique:** The synthetic data generation method—extracting tests from PR diffs and having the model reconstruct the patch—was noted as a clever approach to scaling training data.

### Management as AI superpower: Thriving in a world of agentic AI

#### [Submission URL](https://www.oneusefulthing.org/p/management-as-ai-superpower) | 94 points | by [swolpers](https://news.ycombinator.com/user?id=swolpers) | [87 comments](https://news.ycombinator.com/item?id=46782811)

Ethan Mollick recounts an experiment at Wharton: executive MBA students used agentic AI tools (Claude Code, Google Antigravity, ChatGPT, Claude, Gemini) to go from zero to working startup prototypes in four days. Results were roughly a semester’s worth of progress pre‑AI: real core features, sharper market analyses, and easier pivots. Example demos included Ticket Passport (verified ticket resale), Revenue Resilience (at‑risk revenue detection with agentic remediation), a parenting activity matcher, and Invive (blood sugar prediction).

The bigger point: management—clear goals, constraints, and evaluation—has become the AI superpower. Mollick offers a mental model for deciding when to hand work to AI:
- Human Baseline Time: how long you’d take to do the task yourself
- Probability of Success: chance the AI meets your quality bar per attempt
- AI Process Time: time to prompt, wait, and verify each AI output

You’re trading “doing the whole task” vs. “paying overhead” potentially multiple times. If the task is long, AI is fast and cheap, and the success probability is high enough, delegate. If checking takes long and success is low, just do it yourself. He ties this to OpenAI’s GDPval study: experts took ~7 hours; AI was minutes to generate but ~1 hour to verify—so the tipping point depends on model quality and your acceptance bar.

Why HN cares
- Practical rubric for real work: when AI accelerates vs. wastes time
- Emphasis on evaluation as the scarce skill, not prompting tricks
- Lower pivot costs enable broader exploration and parallel bets
- Caveats: jagged frontier unpredictability, verification overhead, domain/regulatory risk, and the danger of over‑delegation without subject‑matter judgment

**Is writing code still the bottleneck?**
A contentious debate emerged regarding the author’s premise that code generation is no longer the limiting factor in software development. While some agreed that the bottleneck has shifted to "deciding what to build," specification, and review, many argued that the true constraint remains visualizing complex systems, debugging, and managing architecture in medium-to-large codebases—tasks where AI agents frequently fail due to context window limitations and a lack of holistic understanding.

**The "Mental Model" Deficit**
A recurring critique focused on the trade-off between speed and comprehension. Commenters noted that writing code is how engineers build a mental model of the system. By delegating implementation to AI, developers risk losing the deep understanding required to debug subtle failures or make architectural decisions later. This led to concerns that AI speed is illusory, merely shifting time saved on typing toward "paying overhead" on reading, validating, and fixing "myopic" AI error corrections that introduce technical debt.

**The shift from "Builder" to "Reviewer"**
Discussion highlighted the psychological toll of this shift. Several users argued that "managing" AI strips away the creative, satisfying parts of engineering (building), leaving humans with the "grind" of tedious verification, cleanup, and orchestration—work described by some as mind-numbing. Others pointed out that unlike managing human juniors, managing AI lacks the rewarding aspect of mentorship and teaching soft skills.

**Enforcement and Verification**
Participants noted that if AI is treated as a "junior" or a force multiplier, the reliance on automated enforcement (linting, tests, strict architectural boundaries) must increase drastically. Because AI does not "learn" best practices deeply and can hallucinate valid-looking but broken structures, human reviewers must implement rigorous automated checks to prevent a collapse in code quality.

### Kimi Released Kimi K2.5, Open-Source Visual SOTA-Agentic Model

#### [Submission URL](https://www.kimi.com/blog/kimi-k2-5.html) | 483 points | by [nekofneko](https://news.ycombinator.com/user?id=nekofneko) | [227 comments](https://news.ycombinator.com/item?id=46775961)

What’s new
- Native multimodal model trained on ~15T mixed vision/text tokens; pitched as the most powerful open-source model to date.
- Strong coding + vision focus: image/video-to-code, visual debugging, and reasoning over visual puzzles. Demos include reconstructing sites from video and generating animated front‑end UIs from a single prompt.
- “Agent Swarm” paradigm: K2.5 can self-orchestrate up to 100 sub‑agents and ~1,500 tool calls in parallel, claiming up to 4.5x faster execution vs single‑agent runs. No predefined roles; the model decomposes and schedules work itself.

How the swarm works (research preview)
- Trained via Parallel-Agent Reinforcement Learning (PARL) with a learned orchestrator and frozen sub‑agents.
- Uses staged reward shaping to avoid “serial collapse” (falling back to single‑agent). Introduces a “Critical Steps” metric to optimize the critical path rather than raw step count.
- Reported gains on complex, parallelizable tasks and strong scores on agentic benchmarks (HLE, BrowseComp, SWE‑Verified) at lower cost.

Coding with vision
- Emphasis on front‑end generation and visual reasoning to lower the gap from mockups/video to working code.
- Autonomous visual debugging: the model inspects its own outputs, consults docs, and iterates without handholding.
- Internal “Kimi Code Bench” shows step‑up over K2 on build/debug/refactor/test tasks across languages.

Ecosystem and availability
- Access via Kimi.com, Kimi App, API, and Kimi Code.
- Four modes: K2.5 Instant, Thinking, Agent, and Agent Swarm (Beta). Swarm is in beta on Kimi.com with free credits for higher‑tier paid users.
- Kimi Code: open‑source terminal/IDE tooling (VSCode, Cursor, Zed, etc.), supports image/video inputs, and auto‑discovers/migrates existing “skills” and MCPs into your setup.

Why it matters
- Pushes the frontier on practical multimodal coding and parallel agent execution—two areas with big latency and productivity payoffs for real software work.
- If the “self‑directed swarm” generalizes, it could make agent workflows faster and less brittle than hand‑crafted role trees.

Caveats to watch
- Benchmarks and demos can overfit; real‑world reliability, tool integration quirks, and cost at scale remain to be seen.
- “Open‑source” claims often hinge on licensing/weights availability—expect scrutiny on what exactly is released and under what terms.
- Swarm benefits depend on tasks that truly parallelize; sequential or tightly coupled tasks won’t see the same speedups.

How to try
- Experiment with K2.5 Agent/Swarm on Kimi.com (beta) and wire up Kimi Code in your terminal/IDE for image/video‑to‑code and autonomous debugging workflows.

Based on the discussion, here is a summary of the comments regarding the Kimi K2.5 submission:

**Hardware Feasibility & Requirements**
The primary topic of debate is the feasibility of running a 1-trillion parameter model (even with only 32B active parameters) on local hardware.
*   **The VRAM Bottleneck:** Users noted that even with Int4 quantization, a 1T parameter model requires approximately 500GB of VRAM, which is prohibitively expensive for most consumers.
*   **MoE Architecture:** Defenders of the "local" potential argued that because it is a Mixture of Experts (MoE) model, compute requirements are lower (only 32B active parameters per token). However, the total VRAM capacity remains the hard constraint.
*   **High-End Consumer Gear:** Some suggested that high-end consumer hardware (like Mac Studios with unified memory or PCs with the upcoming Strix Halo) might handle the *active* parameter load, but storing the full model remains a challenge without massive memory pools.

**Quantization & Model Quality**
There was significant skepticism regarding how much the model must be compressed to be usable.
*   **Quantization Trade-offs:** Users debated whether a massive model heavily quantized (to 4-bit or 2-bit) performs better than a smaller model running at higher precision. Some reported that while benchmarks might survive quantization, real-world usage often suffers from "death spirals" (repetitive loops) or logic failures.
*   **BitNet & Future Tech:** The discussion touched on recent research (like BitNet/1.58-bit models) and whether Kimi uses post-training quantization vs. quantization-aware training.
*   **Hardware Support:** It was noted that running Int4 effectively requires hardware that natively supports the format; otherwise, the hardware wastes throughput unpacking the data.

**System Architecture & OS Limitations**
*   **Memory Offloading:** The viability of keeping the model on SSDs and swapping "experts" into RAM was debated. Experts argued that because expert activation is often random (low locality), SSD latency would make inference unacceptably slow.
*   **Windows vs. Linux:** One user argued that for consumer Nvidia cards (e.g., RTX 3000 series), Windows currently handles shared memory (using system RAM as VRAM overflow) better than Linux drivers, which were described as unstable or difficult to configure for this specific use case.

**Defining "Local"**
A philosophical disagreement emerged regarding what "running locally" means.
*   Some users feel "local LLM" implies standard consumer hardware (gaming PCs/laptops).
*   Others argued that any model running on on-premise hardware (even a $20k server cluster) counts as local, distinguishing it from API-only services.

### LLM-as-a-Courtroom

#### [Submission URL](https://falconer.com/notes/llm-as-a-courtroom/) | 67 points | by [jmtulloss](https://news.ycombinator.com/user?id=jmtulloss) | [29 comments](https://news.ycombinator.com/item?id=46784210)

Falconer: an “LLM-as-a-Courtroom” to fight documentation rot

Falconer is tackling the classic “documentation rot” problem—code evolves, docs don’t—by auto-proposing and updating internal docs when PRs merge. The hard part isn’t search; it’s trust: knowing which documents truly need updates, for which audiences, and why. After finding that simple categorical scoring (e.g., relevance 7/10) produced inconsistent, unjustified decisions, the team built a courtroom-style judgment engine: one agent prosecutes (argues to update), another defends (argues to skip), a jury deliberates, and a judge rules—creating a reasoned, auditable trail.

Why it matters:
- Turns LLMs from unreliable “raters” into structured debaters that provide evidence and rationale.
- Handles cross-functional nuance (what’s critical for support may be irrelevant for engineering).
- Scales to tens of thousands of PRs daily for enterprise teams.
- Improves trust and maintainability by coupling automation with explainability, not just findability.

The discussion around Falconer focused on the necessity of its complex architecture, the economics of running multi-agent systems, and philosophical debates regarding LLM "understanding."

**The "Courtroom" vs. Simple Scoring**
Several users questioned whether a complex adversarial system was necessary, suggesting that Occam's Razor favors simpler metrics, binary log probabilities, or standard human review. The authors responded that they initially tried simple 1–10 relevance scoring but found the results inconsistent. They argued that LLMs perform better when tasked with arguing a specific position (prosecution/defense) rather than assigning abstract numerical values to nuanced documentation changes that are rarely strictly "true" or "false."

**Cost, Latency, and the "Funnel"**
Commenters expressed concern about the token costs and latency of running multiple agents (prosecutor, defense, five jurors, judge) for every Pull Request. The creators clarified that the "courtroom" is the final step of a funnel, not the default path:
*   **65%** of PRs are filtered out by simple heuristics before review begins.
*   **95%** of the remaining are decided by single agents (prosecutors deciding whether to file charges).
*   Only **1–2%** of ambiguous cases trigger the full, expensive adversarial pipeline.

**Philosophy vs. Utility**
A segment of the discussion devolved into the "Chinese Room" argument. Skeptics argued that LLMs cannot effectively judge context because they lack human experience and true understanding, merely processing symbols. Pragmatists pushed back, noting that if the system achieves the claimed **83% success rate**, it is practically useful regardless of whether the model possesses philosophical "understanding."

**Other Notes**
*   One user shared their own experiment with a "mediation" framework, noting that while litigation seeks truth/justice, mediation seeks resolution—a subtle but interesting difference in agent goal-setting.
*   The thread contained several humorous "courtroom transcript" parodies involving LLM jailbreaks and the "Chewbacca defense."

---

## AI Submissions for Mon Jan 26 2026 {{ 'date': '2026-01-26T17:17:16.264Z' }}

### ChatGPT Containers can now run bash, pip/npm install packages and download files

#### [Submission URL](https://simonwillison.net/2026/Jan/26/chatgpt-containers/) | 407 points | by [simonw](https://news.ycombinator.com/user?id=simonw) | [290 comments](https://news.ycombinator.com/item?id=46770221)

ChatGPT’s code sandbox just got a quiet but major upgrade, per Simon Willison’s testing

- What’s new: Containers can now run Bash directly, not just Python. They also execute Node.js/JavaScript and successfully ran “hello world” in Ruby, Perl, PHP, Go, Java, Swift, Kotlin, C, and C++ (no Rust yet).
- Package installs: pip install and npm install now work via a custom proxy despite the container lacking general outbound network access.
- Fetching files: A new container.download tool saves files from the web into the sandboxed filesystem. It only downloads URLs that were explicitly surfaced in the chat (e.g., via web.run), reducing prompt-injection exfil risks.
- Safety notes: Attempts to craft arbitrary URLs with embedded secrets were blocked—container.download requires the URL to have been “viewed in conversation,” and web.run filters long/constructed query strings. Downloads appear to come from an Azure IP with a ChatGPT-User/1.0 UA.
- Availability: Willison says these capabilities show up even on free accounts; documentation and release notes are lagging.

Why it matters: This turns ChatGPT’s sandbox into a far more capable dev environment—able to script with Bash, pull data files, and install ecosystem packages—making it much better at real-world coding, data wrangling, and agentic workflows, while keeping some guardrails against data exfiltration.

**The Discussion**

*   **Expanded Capabilities & Language Support:** Users confirmed the sandbox is available to free accounts and validated support for languages like D (via the DMD compiler), though C# appears mostly absent due to .NET framework constraints. One commenter described the experience of watching the AI use a computer to answer questions as having a "Pro Star Trek" feel, contrasting it with the loops and errors often encountered with Gemini.
*   **CLI Abstraction vs. Mastery:** A debate emerged regarding the efficiency of using LLMs for standard *nix tasks. While some purists argued that using an LLM to inspect file headers (magic bytes) is wasteful compared to valid CLI commands like `file` or `curl`, Simon Willison and others countered that this democratizes powerful tools (like `ffmpeg` or ImageMagick) for the 800 million users who aren't comfortable in a terminal, while also saving experienced developers from memorizing complex flag syntax.
*   **The Return of the "Mainframe":** The upgrade sparked speculation about the shift toward persistent, virtual development environments. Commenters noted that tool-calling is moving off-platform, with some likening the trend to "renting time on a mainframe" to avoid local hardware maintenance. Willison noted that ephemeral environments are particularly well-suited for coding agents, as they mitigate security risks—if an environment is trashed or leaked, it can be discarded and restarted.
*   **Unreleased Features:** Sharp-eyed users noticed "Context: Gmail" and "read-only" tags in the application logs, leading to speculation (confirmed by news leaks) that deep integration with Gmail and Calendar is currently in testing.

### There is an AI code review bubble

#### [Submission URL](https://www.greptile.com/blog/ai-code-review-bubble) | 317 points | by [dakshgupta](https://news.ycombinator.com/user?id=dakshgupta) | [217 comments](https://news.ycombinator.com/item?id=46766961)

What’s new: Greptile’s Daksh Gupta argues we’re in the “hard seltzer era” of AI code review—everyone’s shipping one (OpenAI, Anthropic, Cursor, Augment, Cognition, Linear; plus pure-plays like Greptile, CodeRabbit, Macroscope). Rather than claim benchmark superiority, Greptile stakes its bet on a philosophy built around:
- Independence: The reviewer should be different from the coder. Greptile refuses to ship codegen—separation of duties so the agent that writes code isn’t the one approving it.
- Autonomy: Code validation (review, tests, QA) should be fully automated with minimal human touch. Greptile positions itself as “pipes,” not a UI.
- Feedback loops: Coding agent writes, validation agent critiques and blocks, coding agent fixes, repeat until the reviewer approves and merges. Their Claude Code plugin can auto-apply Greptile’s comments and iterate.

Why it matters:
- If agents start auto-approving most changes, separation of duties becomes a compliance and safety necessity.
- Review tools could shift from “assistive UIs” to background infrastructure with audit trails and merge authority.
- Switching costs are high for review systems, so vendor choice may be sticky.

Claims and context:
- Greptile cites enterprise traction (including two “Mag7” customers) and recent updates: Greptile v3 (agentic workflow, higher acceptance rates and better signal ratios), long‑term memory, MCP integrations, scoped rules, and lower pricing.
- Caveat: performance claims are vendor-reported; teams may weigh the trade-off between independence and an integrated, single-agent DX.

If you’re evaluating:
- Can the reviewer run tests/QA autonomously and block/merge with guardrails?
- Is there strict separation from your codegen agent?
- Does it support closed-loop remediation, audit logs, and blast-radius controls (permissions, rollout, revert)?
- How painful is it to rip out later (repo coverage, CI/CD hooks, policy/rule migration)?

Based on the discussion, here is a summary of the comments regarding AI code review tools and code review philosophy:

**Prompt Engineering & Implementation Strategies**
User `jv22222` shared detailed insights from building an internal AI review tool, suggesting specific prompting strategies to improve results:
*   **Context is King:** The AI needs the full file contents of changed files plus "1-level-deep imports" to understand how changed code interacts with the codebase.
*   **Diff-First, Context-Second:** Prompts should explicitly mark diffs as "REVIEW THIS" and surrounding files as "UNDERSTANDING ONLY" to prevent hallucinations or false positives on unchanged code.
*   **Strict Constraints:** Use negative constraints to reduce noise. Explicitly tell the AI *not* to flag formatting (let Prettier handle it), TypeScript errors (let the IDE handle it), or guess line numbers.
*   **Structure:** Force structured output (e.g., Emoji-prefixed bullets lists) categorized by severity (Critical, Major, Minor).

**The Signal-to-Noise Problem**
Several users expressed skepticism about the current state of AI reviewers, citing a poor signal-to-noise ratio.
*   One user estimated their experience as "80% noise."
*   The danger is that if the AI floods the PR with speculative or trivial comments, humans will tune it out, resulting in the "20%" of critical bugs slipping through because the human bottleneck is attention.
*   The consensus among skeptics is that AI tools are currently useful as a "second set of eyes" but cannot yet be trusted as a default gatekeeper or autonomous agent.

**Edit-Based vs. Comment-Based Reviews**
A significant portion of the thread digressed into a discussion on Jane Street’s internal review system (Iron), sparked by a comment about how reviewers should fix trivial issues rather than commenting on them.
*   **The Philosophy:** Instead of leaving nitpick comments (which can feel insulting and slow down the loop), reviewers should simply apply the changes directly.
*   **The Friction:** Users noted that standard GitHub workflows make this difficult (checkout, edit, push, context switch), whereas internal tools like Iron or specific VS Code plugins streamline "server-side" editing.
*   **Consensus:** "Ping-ponging" comments over variable names or minor logic is a massive productivity killer; fixing it directly is often preferred by senior engineers.

**The "Bikeshedding" & Naming Debate**
The thread debated whether comments on variable names are valuable or a waste of time ("bikeshedding").
*   **Pro-Naming Comments:** If a reviewer finds a name confusing, the code *is* confusing. The mental model gap must be bridged. One user suggested the default response to a naming suggestion should always be "Done" unless there is a specific, strong reason not to.
*   **Anti-Naming Comments:** Others argued that arguing over `itemCount` vs `numberOfItems` for 30 minutes is a waste of expensive engineering time. While clarity matters, hyper-focusing on conventions (especially in test function names) often yields diminishing returns.

### Porting 100k lines from TypeScript to Rust using Claude Code in a month

#### [Submission URL](https://blog.vjeux.com/2026/analysis/porting-100k-lines-from-typescript-to-rust-using-claude-code-in-a-month.html) | 236 points | by [ibobev](https://news.ycombinator.com/user?id=ibobev) | [155 comments](https://news.ycombinator.com/item?id=46765694)

- Vjeux (ex-Facebook/Meta engineer) set out to port the open‑source Pokémon Showdown battle engine (JS/TS) to Rust to enable fast AI training loops, using Claude Code as the primary driver.
- The “agentic” setup required a surprising amount of ops hackery: he proxied git push via a local HTTP server (since the sandbox blocked SSH), compiled/running inside Docker to avoid antivirus prompts, and used automation (keypress/paste loops and an auto‑clicker) to keep Claude running unattended for hours.
- Early naïve conversion looked impressive (thousands of compiling lines) but hid bad abstractions: duplicated types across files, “simplified” logic where code was hard, and hardcoded patches that didn’t integrate.
- The fix was to impose structure and determinism: generate a script that enumerates every JS file/method and mirrors them in Rust, with source references inline. This grounded the port and reduced hallucinations/mistranslations.
- Key lesson: AI can churn volume, but you must tightly constrain it with deterministic scaffolding, clear mappings, and checks; otherwise it drifts into inconsistent designs.
- Reliability matters for long runs: he hit intermittent failures overnight and notes that agent platforms still need better stability and permission models for unattended work.
- Big picture: with enough guardrails and orchestration, one engineer can push a six‑figure‑line port in weeks—but the real work is designing the rails that keep the model honest and productive.

Takeaway: AI was the tireless junior dev; success came from turning the task into a scripted, verifiable pipeline rather than a free‑form translation exercise.

Here is a summary of the discussion:

- **The "Improvement" Trap:** Commenters shared similar war stories, notably a failed attempt to port an Android/libgdx game to WASM. The "agentic" failure mode was identical: the AI refused to do a dumb, line-by-line port and insisted on "improving" the code (simplifying methods, splitting generic `start` functions), which immediately introduced layout and physics bugs. The consensus is that legacy code is battle-tested; AI attempts to apply "clean code" principles during a port usually destroy subtle, necessary logic.
- **Hidden Prompts vs. User Intent:** A key insight proposed is that IDE plugins and agent frameworks often inject system prompts instructing the model to act as an "expert engineer" using "best practices." This creates a conflict: the user wants a literal translation, but the hidden prompt forces the AI into a refactoring mode.
- **Anthropomorphism and Consistency:** A sub-thread debated the terminology of calling the AI "arrogant." While technically just token prediction based on training data (which includes post-mortems suggesting code cleanup), users noted that treating the model as an inconsistent reasoning engine is dangerous. The "reasoning" output (Chain of Thought) is often just a hallucinated prediction of what an explanation *should* look like, not a true window into the model's logic.
- **Alternative Strategies:**
    - **Port Tests First:** Write or port the test suite before the application code; if the AI can make the green lights turn on, the implementation details matter less.
    - **Source-to-Source Compilers:** for mechanical translations, it may be better to have the AI write a chaotic regex/parsing script to do the bulk work deterministically, rather than relying on inference for every line.
    - **Language Proximity:** One user noted a successful 10k line C++ to Rust port using Gemini, largely because the memory models and structures allowed for a straighter translation than the paradigm shift required for TS to Rust.

### AI code and software craft

#### [Submission URL](https://alexwennerberg.com/blog/2026-01-25-slop.html) | 222 points | by [alexwennerberg](https://news.ycombinator.com/user?id=alexwennerberg) | [135 comments](https://news.ycombinator.com/item?id=46769188)

- The author frames today’s AI “slop” (low-effort audio/video/text) through Jacques Ellul’s “technique”: optimizing for measurable outcomes over craft. When engagement and efficiency are the ends, quality and delight erode.
- Music as parable: Bandcamp’s album-centric, human curation model nurtured indie craft; Spotify’s playlist-driven optimization yields bland, metrics-first tracks. In such spaces, AI thrives because “good enough” at scale beats artistry. The author says Bandcamp is hostile to AI, even banning it.
- Software’s parallel: Big Tech work has become “plumbing”—bloated systems, weak documentation, enshittified platforms, and narrow roles that atrophy broad engineering skill. Cites Jonathan Blow’s “Preventing the Collapse of [Software] Civilization”: industry has forgotten how to do things well.
- Two consequences: (1) AI agents threaten rote, narrow coding jobs—where “good enough” code meets business needs. (2) Overgeneralized claims that AI can do “most” software reduce software to mere output, ignoring design, taste, and higher aims.
- Practical verdict on agents: useful for well-scoped, often-solved tasks (tests, simple DB functions). But they hallucinate, lack understanding, and produce brittle or monstrous code when asked to generalize or “vibe code.”
- Implied takeaway: If platforms value the domain (music, software) over metrics, craft can flourish. Cultivate broad, thoughtful engineers and resist collapsing creative work into pure optimization. AI is a tool for the routine, not a replacement for judgment or taste.

**Summary of Discussion:**

The discussion threads initially pivoted from the article's critique of software to a specific debate on why implementation quality varies so drastically between markets:

*   **The Enterprise Software Trap:** Commenters argued that enterprise software is universally poor due to a principal-agent problem: the purchaser (managers) prioritizes compliance, reporting, and "required fields," while the actual user is ignored. Conversely, consumer software is polished because it must woo individuals, though users noted it is often optimized for engagement rather than actual utility.
*   **AI Code: Orchestration vs. Pollution:** A heated debate emerged regarding the practical output of AI agents.
    *   *The Proponent View:* Some argued AI is a tool change—like moving from a handsaw to a chainsaw. The work shifts from manual coding to "orchestration, planning, and validating."
    *   *The Skeptic View:* Critics countered that AI-generated code is often "orders of magnitude worse" and brittle. A significant complaint was **review fatigue**: developers expressed frustration with colleagues dumping 1,000-line, zero-effort AI diffs that are exhausting to verify and debug.
    *   *The Amplification Theory:* One user suggested AI simply amplifies existing traits: it makes lazy developers lazier (and dangerous), while potentially helping skilled architects move faster, provided they have the taste to judge the output.
*   **The Philosophy of Efficiency:** Expanding on the article's reference to Jacques Ellul, commenters noted that "efficiency" in the corporate sense is usually a proxy for shareholder return, which fundamentally trades off system adaptability and resilience. One user recommended *Turing's Cathedral* as a look back at an era of "real engineering" craft that the industry has since forgotten.

### Show HN: TetrisBench – Gemini Flash reaches 66% win rate on Tetris against Opus

#### [Submission URL](https://tetrisbench.com/tetrisbench/) | 107 points | by [ykhli](https://news.ycombinator.com/user?id=ykhli) | [39 comments](https://news.ycombinator.com/item?id=46769752)

AI Model Tetris Performance Comparison: A new site pits AI models against each other in head-to-head Tetris and tracks results by wins, losses, and draws with a public leaderboard. It’s a community-driven benchmark—there’s no data yet, and users are invited to run AI-vs-AI matches to populate the stats. Beyond the leaderboard, there’s a “Tetris Battle” mode to watch or play, making it a playful way to compare agents under identical conditions.

**Discussion Summary:**

 The discussion focused heavily on the specific mechanics of the implementation, the suitability of LLMs for the task, and legal concerns. The creator (ykhl) clarified the technical architecture: rather than relying on visual reasoning (where LLMs struggle), the system treats Tetris as a coding optimization problem. The models receive the board state as a JSON structure and generate code to evaluate moves. The creator noted that models often "over-optimize" for immediate rewards (clearing lines), creating brittle game states that lead to failure, rather than prioritizing long-term survival heuristics like board smoothness.

Key points from the thread include:

*   **Tetris Mechanics:** Experienced players pointed out flaws in the game engine, specifically regarding the randomization system (suggesting a standard "7-bag" system to prevent piece starvation) and rotation physics, which users felt were biased to the left or lacked standard "SRS" (Super Rotation System) behaviors.
*   **LLM Utility vs. Traditional Bots:** Several users debated the efficiency of using billion-dollar GPU clusters to play a game that simple algorithms on 40-year-old CPUs can master. Critics argued that Reinforcement Learning (e.g., AlphaZero) is a more appropriate architecture than LLMs for this task. However, others acknowledged the project as a benchmark for reasoning and coding benchmarks rather than raw gameplay dominance.
*   **Model Specifics:** There was a brief comparison of model performance, with Gemini 1.5 Flash highlighted as a cost-effective "workhorse" compared to the more expensive Claude 3 Opus.
*   **Legal Warnings:** Multiple users warned the creator about "Tetris Holdings" and their history of aggressive trademark enforcement against fan projects.

### Show HN: Only 1 LLM can fly a drone

#### [Submission URL](https://github.com/kxzk/snapbench) | 172 points | by [beigebrucewayne](https://news.ycombinator.com/user?id=beigebrucewayne) | [91 comments](https://news.ycombinator.com/item?id=46764170)

SnapBench: a Pokémon Snap–style spatial reasoning test for LLMs
- What it is: A small benchmark where a vision-language model pilots a drone in a voxel world to find and identify three ground-dwelling creatures (cat/dog/pig/sheep). Architecture: Rust controller (orchestration), Zig/raylib simulator (physics, terrain, creatures, UDP 9999 command API), and a VLM via OpenRouter. The controller feeds screenshots + state to the model; the model returns movement, identify, and screenshot commands. Identification succeeds within 5 units.

- Headline result: Out of seven frontier models tested with the same prompt, seeds, and a 50-iteration cap, only Gemini 3 Flash consistently managed to descend to ground level and successfully identify creatures. Others:
  - GPT-5.2-chat: Gets near horizontally, rarely lowers altitude.
  - Claude Opus 4.5: Spams identification (160+ attempts) but approaches at bad angles, never succeeds.
  - The rest: Wander or get stuck.

- Key insight: Altitude/approach control—not abstract reasoning—was the bottleneck. The cheapest model beat pricier ones, suggesting:
  - Spatial/embodied control may not scale with model size (yet).
  - Training differences (e.g., robotics/embodied data) could matter more.
  - Smaller models may follow literal instructions (“go down”) more directly.

- Other observations:
  - Two-creature “win” happened when spawns were close; the 50-step limit often ends runs early in a big world.
  - High-contrast creatures (gray sheep, pink pigs) are easier to spot; visibility normalization is a possible future tweak.

- Caveats: Side project, not a rigorous benchmark; one blanket prompt for all models; basic feedback loop; iteration cap may disadvantage slower-but-capable agents.

- Prior attempt IRL: A DJI Tello test ended with ceiling bumps and donuts; new hardware planned now that Flash shows promise in sim.

- Try it: GitHub kxzk/snapbench. Requires Zig ≥0.15.2, Rust (2024 edition), Python ≥3.11, uv, and an OpenRouter API key.

Here is a summary of the discussion:

**Critique: Wrong Tool for the Job**
A significant portion of the discussion centered on whether LLMs should handle low-level control. Critics argued that LLMs are inefficient text generators ill-suited for real-time physics, noting that latency and token costs destroy the simple economics of flight. Several users likened the approach to "using a power drill for roofing nails," suggesting that standard control loops (PID) or simpler neural networks (LSTMs) are far superior for keeping a drone airborne.

**Alternative Architectures**
Commenters suggested looking beyond standard VLLMs:
*   **Vision-Language-Action (VLA) Models:** Users noted that specific VLA models (like those intended for robotics) are better suited for embodied control than general-purpose chat models.
*   **Qwen3VL:** One user observed that Qwen models often possess better spatial grounding (encoding pixel coordinates in tokens) compared to larger, abstract reasoning models.
*   **Pipeline Flaws:** One commenter critiqued the project's architecture (Scene $\to$ Text Labels $\to$ Action), arguing that converting a 3D simulation into discrete text labels causes a loss of relative geometry and depth. They suggested that without an explicit world model, errors compound over time because the agent is "stateless" and lacks temporal consistency.

**Defense: High-Level Reasoning vs. Piloting**
Defenders of the approach clarified that the goal isn't to replace the flight controller (autopilot), but to build an agent capable of high-level semantic tasks (e.g., "fly to the garden and take a picture of the flowers"). They argued that while an LLM shouldn't manage rotor speeds, it is currently the best tool for interpreting natural language instructions and visual context to direct a traditional control system.

### Google AI Overviews cite YouTube more than any medical site for health queries

#### [Submission URL](https://www.theguardian.com/technology/2026/jan/24/google-ai-overviews-youtube-medical-citations-study) | 396 points | by [bookofjoe](https://news.ycombinator.com/user?id=bookofjoe) | [206 comments](https://news.ycombinator.com/item?id=46766031)

Google AI Overviews lean on YouTube over medical sites for health answers, study finds

- The study: SE Ranking analyzed 50,807 German-language health queries (Dec 2025) and 465,823 citations. AI Overviews appeared on 82% of health searches.
- Top source: YouTube was the single most cited domain at 4.43% (20,621 citations) — more than any hospital network, government health portal, medical association, or academic institution. Next: NDR.de (3.04%), MSD Manuals (2.08%), NetDoktor (1.61%), Praktischarzt (1.53%).
- Why it matters: Researchers say relying on a general-purpose video platform (where anyone can upload) signals a structural design risk—prioritizing visibility/popularity over medical reliability—in a feature seen by 2B people monthly.
- Google’s response: Says AI Overviews surface high-quality content regardless of format; many cited YouTube videos come from hospitals/clinics and licensed professionals; cautions against generalizing beyond Germany; claims most cited domains are reputable. Notes 96% of the 25 most-cited YouTube videos were from medical channels.
- Context: Follows a Guardian probe showing dangerous health misinfo in some AI Overviews (e.g., misleading liver test guidance). Google has since removed AI Overviews for some, but not all, medical searches.
- Caveats: One-time snapshot in Germany; results can vary by region, time, and query phrasing. Researchers chose Germany for its tightly regulated healthcare environment to test whether reliance on non-authoritative sources persists.

Based on the discussion, users expressed concern that Google’s AI is creating a "closed loop" of misinformation by citing AI-generated YouTube videos as primary sources.

**The "Dead Internet" and Feedback Loops**
Participants described an "Ouroboros" effect where AI models validate themselves by citing other AI-generated content. Several users invoked the "Dead Internet Theory," noting that YouTube is exploding with low-quality, AI-generated videos that exist solely to capture search traffic.
*   One user recounted finding deepfaked "science" videos, such as AI-generated scripts using Richard Feynman’s voice to read content he never wrote.
*   Others noted that "grifters" are using AI to scale scams, such as fake video game cheat detection software.

**The Crisis of Trust**
A debate emerged regarding the fundamental reliability of AI search:
*   **The skeptical view:** Relying on AI that cites other AI destroys "shared reality." If the technology cannot filter out its own "slop," it cannot be a trusted intermediary for truth.
*   **The counter-argument:** Some argued that human sources (propaganda, biased media) are also unreliable, suggesting that the issue is not unique to AI but rather a general problem of verifying information sources.

**Economic Incentives and Alternatives**
Commenters suggested that Google's business model limits its ability to fix this.
*   Users praised **Kagi** (a paid search engine) for its "niche" ability to use agentic loops to verify primary sources.
*   The consensus was that Google’s unit economics (serving billions of free queries) force it to rely on cheaper, single-pass retrieval methods, which lack the depth to filter out reputable-looking but hallucinated video content.

### OSS ChatGPT WebUI – 530 Models, MCP, Tools, Gemini RAG, Image/Audio Gen

#### [Submission URL](https://llmspy.org/docs/v3) | 129 points | by [mythz](https://news.ycombinator.com/user?id=mythz) | [32 comments](https://news.ycombinator.com/item?id=46766432)

HN: llms.py v3 ships a big extensibility overhaul, 530+ models via models.dev, and “computer use” automation

llms.py just released v3, reframing itself as a highly extensible LLM workbench. The headline change is a switch to the models.dev catalog, unlocking 530+ models across 24 providers, paired with a redesigned Model Selector and a plugin-style extensions system. It also adds desktop automation (“computer use”), Gemini File Search–based RAG, MCP tool connections, and a raft of built‑in UIs (calculator, code execution, media generation).

Highlights
- 530+ models, 24 providers: Now inherits the models.dev open catalog; enable providers with a simple "enabled": true in llms.json. Daily auto-updates via llms --update-providers. Extra providers can be merged via providers-extra.json.
- New Model Selector: Fast search, filtering by provider/modalities, sorting (knowledge cutoff, release date, last updated, context), favorites, and rich model cards. Quick toggles to enable/disable providers.
- Extensions-first architecture: UI and server features implemented as plugins using public client/server APIs; built-ins are just extensions.
- RAG with Gemini File Search: Manage file stores and document uploads for retrieval workflows.
- Tooling: First-class Python function calling; MCP support to connect to Model Context Protocol servers for extended tools.
- Computer Use: Desktop automation (mouse, keyboard, screenshots) for agentic workflows.
- Built-in UIs: 
  - Calculator (Python math with a friendly UI)
  - Run Code (execute Python/JS/TS/C# in CodeMirror)
  - KaTeX math rendering
  - Media generation: image (Google, OpenAI, OpenRouter, Chutes, Nvidia) and TTS (Gemini 2.5 Flash/Pro Preview), plus a media gallery
- Storage and performance: Server-side SQLite replaces IndexedDB for robust persistence and concurrent use; persistent asset caching with metadata.
- Provider nuances: Non-OpenAI-compatible providers handled via a providers extension; supports Anthropic’s Messages API “Interleaved Thinking” to improve reasoning between tool calls (applies to Claude and MiniMax).

Why it matters
- One pane of glass for LLM experimentation: broad model coverage with consistent UX.
- Batteries included: from RAG to tool use to desktop control and media generation, with minimal setup.
- Extensible by design: encourages custom providers, tools, and UI add-ons as plugins.

Getting started
- pip install llms-py (or pip install llms-py --upgrade)
- Configure providers by enabling them in llms.json; update catalogs with llms --update-providers

Caveats
- Powerful features like code execution and desktop control have security implications; use with care.
- You’ll still need API keys and to mind provider quotas/costs.

**Top Story: llms.py v3 ships a big extensibility overhaul, 530+ models via models.dev, and “computer use” automation**

llms.py has released version 3, repositioning itself as a highly extensible workbench for Large Language Models. This update integrates the models.dev catalog, providing access to over 530 models from 24 providers, and introduces a plugin-based architecture that allows both UI and server features to be extended. Key additions include "Computer Use" for desktop automation, RAG capabilities via Gemini File Search, and support for the Model Context Protocol (MCP) to connect external tools. The release also includes built-in utilities like a calculator and code execution environment, all backed by a switch to server-side SQLite for better performance.

**Discussion Summary**

The discussion explores the open-source philosophy behind the project, its technical implementation of agents, and the challenges of gaining visibility on Hacker News.

*   **Licensing and OpenWebUI:** A significant portion of the conversation contrasts llms.py with OpenWebUI. Users expressed frustration with OpenWebUI’s recent move to a more restrictive license and branding lock-in. The creator of llms.py (`mythz`) positioned v3 as a direct response to this, emphasizing a permissive open-source approach and an architecture designed to prevent monopoly on components by making everything modular and replaceable.
*   **Technical Implementation (Agents & MCP):**
    *   **Orchestration:** When asked about managing state in long-running loops (specifically regarding LangGraph), the creator clarified that llms.py uses a custom state machine modeled after Anthropic’s "computer use" reference implementation, encapsulating the agent loop in a single message thread.
    *   **Model Context Protocol (MCP):** The creator noted that while MCP support is available (via the `fast_mcp` extension), it introduces noticeable latency compared to native tools. However, it is useful for enabling capabilities like image generation on models that don't natively support it.
*   **Deployment and Auth:** Users inquired about multi-user scenarios. The system currently supports GitHub OAuth for authentication, saving content per user. While some users felt the features were "gatekept" or not ready for enterprise deployment compared to other tools, the creator emphasized that the project's goal is simplicity and functionality for individuals or internal teams, rather than heavy enterprise feature sets.
*   **Ranking Mechanics:** There was a meta-discussion regarding the difficulty of getting independent projects to the front page. The creator noted they had posted the project multiple times over the week before it finally gained traction, leading to speculation by other users about how quickly "new" queue submissions are buried by downvotes or flagged compared to company-backed posts.
*   **Naming:** A user briefly pointed out the potential confusion with Simon Willison’s popular `llm` CLI tool.

### OracleGPT: Thought Experiment on an AI Powered Executive

#### [Submission URL](https://senteguard.com/blog/#post-7fYcaQrAcfsldmSb7zVM) | 58 points | by [djwide](https://news.ycombinator.com/user?id=djwide) | [50 comments](https://news.ycombinator.com/item?id=46766507)

SenTeGuard launches a blog focused on “cognitive security,” but it’s not live yet. The landing page promises team-authored, moderated longform updates, research notes, and security insights, but currently shows “No posts yet” and even an admin prompt to add the first post—suggesting a freshly set-up placeholder. One to bookmark if you’re tracking cognitive security, once content arrives.

Based on the provided comment text (which appears to be a compressed or vowel-reduced transcript), the discussion focuses on the implications of AI in management and governance, rather than the specific lack of content on the blog mentioned in the submission summary.

**The conversation covers three main themes:**

*   **Automation and Accountability:** Users debate the feasibility of "expert systems" running companies. While some cite algorithmic trading and autopilot as proof that computers already make high-stakes decisions, others argue that humans (executives) are structurally necessary to provide legal accountability ("you can't prosecute code"). The short story *Manna* by Marshall Brain is cited as a relevant prediction of algorithmic management.
*   **Government by Algorithm:** Participants speculate on using a "special government LLM" to synthesize citizen preferences for direct democracy. Skeptics counter that current LLMs hallucinate (referencing AI-generated fake legal briefs) and that such a system would likely be manipulated by those in charge of the training data or infrastructure.
*   **Commercial Intent:** One commenter critiques the submission as a commercial vehicle to sell "cognitive security" services, arguing this commercial framing undermines the philosophical discussion. The apparent author (`djwd`) acknowledges the commercial intent but argues the engineering and political questions raised are still worth discussing.

**Minor tangents include:**
*   Comparisons of AI hype to failed technologies like Theranos versus successful infrastructure like aviation autoland.
*   A debate regarding the US President’s practical vs. theoretical access to classified information and chain-of-command issues.

### Clawdbot - open source personal AI assistant

#### [Submission URL](https://github.com/clawdbot/clawdbot) | 382 points | by [KuzeyAbi](https://news.ycombinator.com/user?id=KuzeyAbi) | [250 comments](https://news.ycombinator.com/item?id=46760237)

Moltbot (aka Clawdbot): an open‑source, self‑hosted personal AI assistant for your existing chat apps

What’s new
- A single assistant that runs on your own devices and replies wherever you already are: WhatsApp, Telegram, Slack, Discord, Google Chat, Signal, iMessage/BlueBubbles, Microsoft Teams, Matrix, Zalo (incl. Personal), and WebChat. It can also speak/listen on macOS/iOS/Android and render a live, controllable Canvas.
- Easy onboarding: a CLI wizard (moltbot onboard) sets up the gateway, channels, and skills; installs a user‑level daemon (launchd/systemd) so it stays always on. Cross‑platform (macOS, Linux, Windows via WSL2). Requires Node ≥22; install via npm/pnpm/bun.
- Models and auth: works with multiple providers via OAuth or API keys; built‑in model selection, profile rotation, and failover. The maintainers recommend Anthropic Pro/Max (100/200) and Opus 4.5 for long‑context and better prompt‑injection resistance.
- Developer/ops friendly: stable/beta/dev release channels, Docker and Nix options, pnpm build flow, and a compatibility shim for the older clawdbot command.
- Security defaults for real messaging surfaces: unknown DMs get a pairing code and aren’t processed until you approve, reducing risk from untrusted input.

Why it matters
- Brings the “always‑on, everywhere” assistant experience without locking you into a hosted SaaS front end.
- Bridges consumer and workplace chat apps, making agents genuinely useful where people already collaborate.
- Thoughtful guardrails (DM pairing) and model failover are practical touches for a bot that lives in your actual message streams.

Quick try
- Install: npm install -g moltbot@latest, then moltbot onboard --install-daemon
- Run the gateway: moltbot gateway --port 18789 --verbose
- Talk to it: moltbot agent --message "Ship checklist" --thinking high

Notes and caveats
- You’re still trusting whichever model/provider you use; “feels local” doesn’t mean the LLM runs locally by default.
- Needs Node 22 and a background service; connecting to many chat platforms may have ToS and security considerations.
- MIT licensed; the repo shows strong community interest (tens of thousands of stars/forks).

**Discussion Summary**

The discussion centers on the economics, security, and utility of self-hosted AI agents:

*   **The Cost of "Always On":** Users warned that running agents via metered APIs (like Claude Opus) can get expensive quickly. One user reported spending over $300 in two days on "fairly basic tasks," while another noted that a single complex calendar optimization task involving reasoning cost $29. This led to suggestions for using smaller, specialized local models to handle routine logic.
*   **The "Private Secretary" Dream:** There is distinct enthusiasm for a "grown-up" version of Siri—dubbed "Nagatha Christy" or "Jarbis" by one commenter—that can handle messy personal contexts (kids' birthday parties, dentist reminders) alongside work integrations (Jira, Trello, Telegram) without monetizing user data. Several users expressed deep dissatisfaction with current hosted options and a willingness to pay a premium for a private, reliable alternative.
*   **Security Concerns:** The repository came under scrutiny for hardcoded OAuth credentials (client secrets), which some argued is common in open-source desktop apps but poses risks if the "box" is compromised. Others found the concept of "directory sandboxing" insufficient, expressing fear about granting an AI agent permission to modify code and files on their primary machine.
*   **Self-Repairing Agents:** One contributor shared an anecdote about the "lightbulb moment" of working with an agent: when the bot stopped responding on Slack, they used the AI to debug the issue, review the code, and help submit a Pull Request to fix itself.

### AI Lazyslop and Personal Responsibility

#### [Submission URL](https://danielsada.tech/blog/ai-lazyslop-and-personal-responsibility/) | 65 points | by [dshacker](https://news.ycombinator.com/user?id=dshacker) | [71 comments](https://news.ycombinator.com/item?id=46770675)

A developer recounts a painful code review: a coworker shipped a 1,600-line, AI-generated PR with no tests, demanded instant approval, and later “sneak-merged” changes after pushback. The author doesn’t blame the individual so much as incentives that reward speed over stewardship—and coins “AI Lazyslop”: AI output the author hasn’t actually read, pushing the burden onto reviewers.

Proposed anti–AI Lazyslop norms:
- Own the code you accept from an LLM.
- Disclose when and how you used AI; include key prompts/plans in the PR.
- Personally read and test everything; add self-review comments explaining your thinking.
- Use AI to assist review, then summarize what you fixed and why.
- Be able to explain the logic and design without referring back to the AI.
- Write meaningful tests (not trivial ones).

The post notes a cultural shift: projects like Ghostty ask contributors to disclose AI use, and even Linus Torvalds has experimented with “vibe-coding” via AI. The gray area persists: the coworker evolved to “semi-lazy-slop,” piping reviewer comments straight into an LLM—maybe better, maybe not.

In a nice touch of dogfooding, the author discloses using Claude for copy edits and lists the concrete fixes it suggested. The core message: don’t shame AI—set expectations that keep quality and responsibility with the human who ships the code.

Based on the discussion, here is a summary of the points raised by Hacker News commenters:

**Trust and Professionalism**
The most heated point of discussion was the "sneak-merge" (merging code after a review without approval). Commenters almost universally agreed that this violates the fundamental trust required for collaborative development. While the author focused on systemic incentives, many users argued that "Mike" bears personal responsibility. One user compared sneaking unreviewed code to a chef spitting in food—a deliberate, unethical action rather than just a process error.

**The "Blameless" Culture Debate**
Several users pushed back against the author's attempt to "blame the incentives" rather than the individual.
*   Commenters warned that "blameless culture" can swing too far, protecting toxic behavior and forcing managers to silently manage poor performers out while publicly maintaining a positive façade.
*   One user argued that "bending backwards" to avoid blaming an individual for intentional actions creates a low-trust environment where high-quality software cannot be built.

**The Reviewer’s Burden and "Prisoner's Dilemma"**
A recurring theme was the asymmetry of effort. AI allows developers to generate code faster than seniors can review it.
*   **The Prisoner’s Dilemma:** One commenter described a situation where diligent reviewers spend all their time fixing "AI slop" from others, consequently missing their own deadlines. Meanwhile, the "slop-coders" appear productive due to high velocity and get promoted, punishing those who maintain quality.
*   **Scale vs. Existence:** While huge PRs existed before AI (e.g., refactoring or Java boilerplate), users noted that AI changes the *frequency*. Instead of one massive PR every few weeks, it becomes a daily occurrence, overwhelming the review pipeline.

**Proposed Solutions and Nuance**
*   **Policy:** Some pointed to the LLVM project's AI policy as a gold standard: AI is a tool, but the human must own the code and ensure it is not just "extractive" (wasting reviewer time).
*   **Reviewing Prompts:** There was a debate on the author's suggestion to include prompts in PRs. Some argued that prompts represent the "ground truth" and reveal assumptions, making them valuable to review. Others felt that only the resulting code matters and reviewing prompts is unnecessary overhead.
*   **Author’s Context:** The author (presumably 'bdsctrcl') chimed in to clarify the technical context of the 1,600-line PR, noting it was largely Unreal Engine UI boilerplate (flags and saved states). While it "worked," it bypassed specific logic (ignoring flag checks) in favor of direct struct configuration, highlighting how AI code can be functional but architecturally incorrect.

### When AI 'builds a browser,' check the repo before believing the hype

#### [Submission URL](https://www.theregister.com/2026/01/26/cursor_opinion/) | 228 points | by [CrankyBear](https://news.ycombinator.com/user?id=CrankyBear) | [137 comments](https://news.ycombinator.com/item?id=46769965)

Top story: The Register calls out Cursor’s “AI-built browser” as mostly hype

- What was claimed: Cursor’s CEO touted that GPT‑5.2 agents “built a browser” in a week—3M+ lines, Rust rendering engine “from scratch,” custom JS VM—adding it “kind of works.”
- What devs found: Cloning the repo showed a project that rarely compiles, fails CI on main, and runs poorly when manually patched (reports of ~1 minute page loads). Reviewers also spotted reliance on existing projects (e.g., Servo-like pieces and QuickJS) despite “from scratch” messaging.
- Pushback from maintainers: Servo maintainer Gregory Terzian described the code as “a tangle of spaghetti” with a “uniquely bad design” unlikely to ever support a real web engine.
- Cursor’s defense: Engineer Wilson Lin said the JS VM was a vendored version of his own parser project, not merely wiring dependencies—but that undercuts the “from scratch” and “AI-built” framing.
- Scale vs. results: The Register cites estimates that the autonomous run may have burned through vast token counts at significant cost, yet still didn’t yield a reproducible, functional browser.
- Bigger picture: The piece argues this is emblematic of agentic-AI hype—exciting demos without CI, reproducible builds, or credible benchmarks. AI coding tools are useful as assistive “autocomplete/refactor” layers, but claims that agents can ship complex software are outpacing reality.

Why it matters for HN:
- Shipping > demo: Repos, CI status, and benchmarks remain the truth serum. If the code doesn’t build, the press release doesn’t matter.
- Agent limits: Autonomous agents can generate mountains of code, but architecture, integration, and correctness still demand human engineering rigor.
- Practical adoption: The Register urges proof first—working software and measurable ROI—before buying into “agents will write 90% of code” narratives.

Bottom line: Before believing “AI built X,” check the repo.

Based on the discussion, here is a summary of the comments on Hacker News:

**The "Novel-Shaped Object"**
Commenters were largely dismissive of the project's technical merit, characterizing it as a marketing stunt rather than an engineering breakthrough. One user likened the browser to a "novel-shaped object"—akin to a key made of hollow mud that looks correct but shatters upon use. Others described the code not as a functional engine, but as a "tangle of spaghetti" that poorly copied existing implementations (like Servo) rather than genuinely building "from scratch," resulting in a design unlikely to ever support a real-world engine.

**Debating "From Scratch" and Dependencies**
A significant portion of the thread involved users decompiling the "from scratch" claim.
*   **Hidden Dependencies:** Users pointed out that the AI did not write a rendering engine from nothing; it halluncinated or "vendored" (copied) code from existing projects like Servo, Taffy, and QuickJS.
*   **The "TurboTax" Analogy:** One commenter compared the CEO’s claim to saying you did your taxes "manually" while actually filling out TurboTax forms—technically you typed the numbers, but the heavy lifting was pre-existing logic.
*   **Legal Definitions:** There was a brief debate over whether the "from scratch" claim constituted "fraudulent misrepresentation" under UK law, though others argued that software engineering terms are too subjective for such a legal standard.

**Metacommentary on Tech Journalism**
Simon Willison (`smnw`), who interviewed the creators for a related piece, was active in the thread and faced criticism for "access journalism."
*   **The Critique:** Users argued Willison should have pushed back harder against the CEO's "from scratch" hype during the interview, accusing him of enabling a marketing narrative rather than exposing the project's lack of rigor.
*   **The Defense:** Willison defended his approach, stating his goal was to understand *how* the system was built rather than to grill the CEO on Twitter phrasing. While he conceded that "from scratch" was a misrepresentation due to the vendored dependencies, he argued the system did still perform complex tasks (writing 1M+ lines of code) that were worth investigating, even if the end result was flawed.

**Cost vs. Output**
Critiques also focused on the economics of the experiment. Users noted that spending vast amounts of resources (potentially $100k+ in compute/tokens) to generate a browser that "kind of works" is not impressive. They argued that the inability to compile or pass CI makes the project less of a demo of AI capability and more of a cautionary tale about the inefficiency of current agentic workflows.

### Georgia leads push to ban datacenters used to power America's AI boom

#### [Submission URL](https://www.theguardian.com/technology/2026/jan/26/georgia-datacenters-ai-ban) | 52 points | by [toomuchtodo](https://news.ycombinator.com/user?id=toomuchtodo) | [23 comments](https://news.ycombinator.com/item?id=46767696)

Georgia lawmakers have introduced what could become the first statewide moratorium on building new datacenters, aiming to pause projects until March 2027 to set rules around facilities that guzzle energy and water. Similar statewide bills surfaced in Maryland and Oklahoma, while a wave of local moratoriums has already spread across Georgia and at least 14 other states. The push comes as Atlanta leads U.S. datacenter construction and regulators approved a massive, mostly fossil-fueled power expansion to meet tech demand.

Key points
- HB 1012 would halt new datacenters statewide to give state and local officials time to craft zoning and regulatory policies. A Republican co-sponsor says the pause is about planning, not opposing datacenters, which bring jobs and tax revenue.
- Georgia’s Public Service Commission just greenlit 10 GW of additional power—enough for ~8.3m homes—largely to serve datacenters, with most supply from fossil fuels.
- At least 10 Georgia municipalities (including Roswell) have enacted their own moratoriums; Atlanta led the nation in datacenter builds in 2024.
- Critics cite rising utility bills, water use, and tax breaks: Georgia Power profits from new capital projects, which advocates say drives rate hikes (up roughly a third in recent years) while dulling incentives to improve grid efficiency.
- Proposals in the legislature span ending datacenter tax breaks, protecting consumers from bill spikes, and requiring annual disclosure of energy and water use.
- National momentum: Bernie Sanders floated a federal moratorium; advocacy groups say communities want time to weigh harms and costs.

Politics to watch
- Bill sponsor Ruwa Romman, a Democrat running for governor, ties the pause to upcoming elections for Georgia’s utility regulator. Voters recently ended the PSC’s all-Republican control by electing two Democrats; another seat is up this year. Supporters hope a new majority will scrutinize utility requests tied to datacenter growth.

Source: The Guardian (Timothy Pratt)

Here is a summary of the discussion on Hacker News:

**Regulation vs. Prohibition**
Rather than a blanket moratorium, several commenters suggested Georgia should implement strict zoning and operational requirements. Proposals included mandating "zero net water" usage (forcing the use of recycled or "purple pipe" water), setting strict decibel limits at property boundaries to mitigate noise, and requiring facilities to secure their own renewable energy sources.

**Grid Strain and Economic Risk**
A significant portion of the debate focused on the mismatch between data center construction speeds and power plant construction timelines. Users highlighted the economic risk to local ratepayers: if utilities build expensive capacity for data centers that later scale back or move, residents could be left paying for the overbuilt infrastructure. Some noted that power funding models are shifting to make data centers liable for these costs, but skepticism remains about whether consumers are truly shielded from rate hikes.

**Environmental and Local Impact**
The "NIMBY" (Not In My Backyard) aspect was heavily discussed. While some users argued that data centers are clean compared to factories, others pointed out that on-site backup generators (gas/diesel turbines) do produce exhaust, and constant cooling noise is a nuisance. There is also frustration that these facilities consume massive resources (water/power) while providing very few local jobs compared to their footprint.

**Georgia’s Energy Mix**
Commenters debated whether Georgia’s political leaning would hinder renewable energy adoption. However, data was cited showing Georgia is actually a leader in solar capacity (ranked around 7th in the U.S.), suggesting that solar adoption in sunny states is driven more by economics than political rhetoric.

**Clarifying the Scope**
There was some confusion regarding federal preemption of "AI regulation." Other users clarified that this bill specifically targets physical land use, zoning, and utility consumption—areas traditionally under state and local control—rather than the regulation of AI software or algorithms.

### AI Was Supposed to "Revolutionize" Work. In Many Offices, It's Creating Chaos

#### [Submission URL](https://slate.com/life/2026/01/work-artificial-intelligence-ai-office-chaos.html) | 14 points | by [ryan_j_naughton](https://news.ycombinator.com/user?id=ryan_j_naughton) | [4 comments](https://news.ycombinator.com/item?id=46773478)

Alison Green’s latest “Direct Report” rounds up real workplace stories showing how generative AI is backfiring in mundane but costly ways—less “revolution,” more chaos.

Highlights:
- Invented wins: An employee’s AI-written LinkedIn post falsely claimed CDC collaborations and community health initiatives—spawning shares, confusion, and award nominations based on fiction.
- Hype that lies: An exec let AI “punch up” a morale email; it announced a coveted program that didn’t exist.
- Privacy faceplants: AI note-takers auto-emailed interview feedback to the entire company and to candidates; another recorded union grievance meetings and blasted recaps to all calendar invitees.
- Hollow comms: Students and job candidates lean on LLMs for networking and interviews, producing buzzwordy, substance-free answers that erode trust.

The throughline: People over-trust polished outputs, underestimate hallucinations, and don’t grasp default-sharing risks. The result is reputational damage, legal exposure, and worse teamwork.

Takeaways for teams:
- Default to human review; never let AI invent accomplishments or announce decisions.
- Lock down AI transcription/sharing settings; avoid them in sensitive meetings.
- Set clear policies on disclosure and acceptable use; train for verification and privacy.
- In hiring and outreach, authenticity beats LLM gloss—interviewers can tell.

**The Discussion**

In a concise thread, commenters drew parallels between these workplace AI failures and the privacy controversies surrounding **Windows 11** (likely referencing the Recall feature’s data scraping). Conversation also touched on the disparity between the promised "revolution" and the actual user experience, with users briefly debating timelines—**years** versus **quarters**—for the technology to mature or for the hype to settle.