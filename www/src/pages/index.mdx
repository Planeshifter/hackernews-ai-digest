import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Mon Jul 10 2023 {{ 'date': '2023-07-10T17:09:55.954Z' }}

### Hybrid Insect Micro-Electro-Mechanical Systems (Hi-MEMS)

#### [Submission URL](https://en.wikipedia.org/wiki/Hybrid_Insect_Micro-Electro-Mechanical_Systems) | 29 points | by [pyinstallwoes](https://news.ycombinator.com/user?id=pyinstallwoes) | [6 comments](https://news.ycombinator.com/item?id=36664372)

The Pentagon is working on a project called Hybrid Insect Micro-Electro-Mechanical Systems (HI-MEMS), which involves implanting micro-mechanical systems into insects during their early stages of metamorphosis. The goal is to create "insect cyborgs" that can be controlled remotely through electrical impulses sent to their muscles. The primary application for these cyborg insects is surveillance. The project aims to develop insects that can reach a target located 100 meters away, starting from a distance of 5 meters. So far, researchers have demonstrated the successful implantation of electronic probes into tobacco hornworms and the flight capabilities of a cyborg unicorn beetle. With this technology, the military could potentially deploy these insect spies for reconnaissance purposes.

The comments on Hacker News start with some initial reactions to the news. One user expresses their surprise about the project, while another makes a joke about the integration of bugs into the military. Another user provides an alternative acronym for the project, suggesting that the "MEMS" part could stand for "Micro-Electro-Mechanical-Bio-Entomo Robot MEMBER," adding a humorous twist to the discussion.  One user makes a somewhat sarcastic comment pointing out how interesting it is that the military is funding such projects. They go on to mention that the idea seems straight out of a science fiction novel.  Another user joins in with the humor, mentioning the Fifth Element movie which includes a scene with robotic cockroaches. A final comment with a pun is made about the "Multipass" concept from the same movie.  Overall, the discussion mainly revolves around amusement and jokes related to the concept of insect cyborgs being used for military purposes.

### Apple VisionOS Simulator streaming wirelessly to Meta Quest headset

#### [Submission URL](https://github.com/zhuowei/VisionOSStereoScreenshots/tree/alvr) | 395 points | by [ozten](https://news.ycombinator.com/user?id=ozten) | [224 comments](https://news.ycombinator.com/item?id=36668732)

The top story on Hacker News today is about a project called VisionOSStereoScreenshots. Created by a developer named Zhuowei, this project allows users to take 3D stereoscopic screenshots in the visionOS emulator. It has gained a lot of attention, with 259 stars and 9 forks on GitHub. The project involves streaming the visionOS Simulator wirelessly to a Meta Quest headset using ALVR (a cross-platform VR streaming system). While the project is still a work in progress, it has already made significant progress in enabling wireless streaming and is looking to add features such as passthrough and support for Quest controllers in the future. The developer credits @ShinyQuagsire for pioneering the wired Quest Link version of the tool and helping with the wireless port, as well as @JJTech and @keithahern for their contributions in figuring out input handling in the visionOS Simulator. This project is an exciting development for VR enthusiasts and developers alike.

### OpenAI says it could ‘cease operating’ in the EU

#### [Submission URL](https://www.theverge.com/2023/5/25/23737116/openai-ai-regulation-eu-ai-act-cease-operating) | 24 points | by [pimeys](https://news.ycombinator.com/user?id=pimeys) | [6 comments](https://news.ycombinator.com/item?id=36674187)

OpenAI has stated that it may stop operating in the European Union (EU) if it cannot comply with future AI regulations. The EU is currently finalizing new legislation called the AI Act, which would require companies like OpenAI to disclose information about their training methods and data sources. OpenAI CEO Sam Altman has expressed concerns about the Act, particularly the designation of systems like OpenAI's ChatGPT as "high risk", which would subject them to safety and transparency requirements. Altman later clarified that OpenAI has no plans to leave Europe, but the company does have concerns about meeting the requirements of the AI Act. In addition to technical challenges, the Act also poses potential business threats to OpenAI, such as the disclosure of copyrighted data used for training its AI models. Altman's comments shed light on OpenAI's perspective on regulation and its desire for it to primarily apply to future AI systems.

### That Google memo about having ‘no moat’ in AI was real

#### [Submission URL](https://www.theverge.com/2023/7/10/23790132/google-memo-moat-ai-leak-demis-hassabis) | 29 points | by [mfiguiere](https://news.ycombinator.com/user?id=mfiguiere) | [11 comments](https://news.ycombinator.com/item?id=36672274)

A leaked memo from a Google researcher suggested that the company has "no moat" in the AI industry, stating that open-source AI models are outpacing Google. In an interview with The Verge, Demis Hassabis, CEO of Google's DeepMind, confirmed the authenticity of the memo but disagreed with its conclusions. Hassabis expressed confidence in Google's future in AI, citing the competitive nature of the company's researchers and the potential for breakthroughs from the newly merged Google Brain and Google DeepMind teams.

The discussion around the leaked memo from a Google researcher and Demis Hassabis' response to it centers on whether Google truly lacks a competitive advantage in the AI industry. Some users argue that Google's dominance in products like Search, YouTube, and Gmail does give them an edge, while others believe that open-source AI models are rapidly advancing and pose a threat to Google's position. Some commenters also discuss the role of default settings in shaping user behavior, with one user mentioning that Google's default search engine on popular web browsers gives the company a significant advantage.

Others point out that Google's control over Android through Google Play Services also contributes to their ability to maintain influence in the AI landscape. One user highlights the importance of ongoing testing and improvement to stay competitive, particularly in tasks like creative writing and inductive reasoning. They mention the success of OpenAI's ChatGPT and the potential of Google developing an AI chatbot interface.

In response to a user suggesting trying out OpenAI's Bard chatbot, another commenter questions the true endorsement of Bard, pointing out that Google is also well-poised in this area. Overall, the discussion involves a mix of perspectives, ranging from those who believe Google has a clear advantage to those who express skepticism and highlight potential rivals.

---

## AI Submissions for Sun Jul 09 2023 {{ 'date': '2023-07-09T17:09:32.746Z' }}

### New York City’s AI hiring law takes effect

#### [Submission URL](https://qz.com/americas-first-law-regulating-ai-bias-in-hiring-takes-e-1850602243) | 99 points | by [donohoe](https://news.ycombinator.com/user?id=donohoe) | [127 comments](https://news.ycombinator.com/item?id=36654420)

New York City is set to enforce a groundbreaking law aimed at tackling bias in AI hiring tools. The law, first passed in 2021, requires employers to be transparent about their use of AI and algorithmic-based tools in the hiring process. Companies must inform candidates of their use of such tools and disclose what personal data is being collected. Additionally, the law mandates that companies undergo annual audits to identify potential bias within their AI systems. While the law is a step towards transparency, critics argue that it may not go far enough in protecting job candidates from bias. They argue that developers can find loopholes to pass or bypass audits, rendering them less effective in detecting bias. Nonetheless, the enforcement of this law is a significant move towards promoting fairness and accountability in the AI hiring process.

### Rio, a new GPU-accelerated terminat that can run natively and in the browser

#### [Submission URL](https://medium.com/@raphamorim/rio-terminal-a-native-and-web-terminal-application-powered-by-rust-webgpu-and-webassembly-76d03a8c99ed) | 33 points | by [rogerwilson](https://news.ycombinator.com/user?id=rogerwilson) | [10 comments](https://news.ycombinator.com/item?id=36656443)

Introducing Rio Terminal: a powerful native and web terminal application that is powered by Rust, WebGPU, and WebAssembly. Rio is known for its speed, thanks to its built-in Rust language and the use of the Alacritty terminal's VTE for ANSI handling and parsing. The terminal also features a minimal tabs design and supports multi-window features on various platforms including Windows, MacOS, and Linux. What sets Rio apart is its ability to run in browsers and desktop environments, making it ideal for creating plugins for cross-platform architectures. The WebAssembly version of Rio is still in progress but promises to bring the same functionality to the web. With its impressive rendering capabilities through its Sugarloaf renderer, Rio is a versatile tool for developers and users alike.

---

## AI Submissions for Sat Jul 08 2023 {{ 'date': '2023-07-08T17:10:12.878Z' }}

### The away team model at Amazon (2022)

#### [Submission URL](https://pedrodelgallego.github.io/blog/amazon/operating-model/away-team-model/) | 67 points | by [softwaredoug](https://news.ycombinator.com/user?id=softwaredoug) | [60 comments](https://news.ycombinator.com/item?id=36645306)

Amazon has developed a model called the "away team model" to address dependencies and avoid inter-team roadblocks. The away team model involves a self-sufficient engineering team working on code owned by another team (the host team) to deliver features. This allows teams to work independently and helps accelerate software delivery by addressing roadmap dependencies. The away team model works best when the host team and away team coordinate and collaborate on implementing and verifying changes. However, it is important to note that the away team model can be inefficient and should only be used when necessary. Teams should first try to align features and timeframes with host teams before resorting to the away team model. Overall, the away team model is a powerful mechanism for removing dependencies between teams and keeping organizations nimble.

The discussion surrounding the submission on Hacker News is mixed. Some commenters express frustration and dissatisfaction with the "away team model" at Amazon, citing issues such as wasted time, unproductive meetings, and a lack of communication and visibility. Others share similar experiences with the model, both at Amazon and other companies, highlighting the challenges of working on codebases owned by other teams. Some commenters defend the model, suggesting that it can be successful when properly implemented and emphasizing the importance of aligning features and timeframes with the host team before resorting to the away team model. There is discussion about the benefits and drawbacks of different team structures and project management approaches, with comparisons made to companies like Microsoft and Boeing. Overall, the discussion highlights the complexity and potential inefficiencies of managing dependencies between teams in large organizations.

### Train an AI model once and deploy on any cloud

#### [Submission URL](https://developer.nvidia.com/blog/train-your-ai-model-once-and-deploy-on-any-cloud-with-nvidia-and-runai/) | 189 points | by [GavCo](https://news.ycombinator.com/user?id=GavCo) | [106 comments](https://news.ycombinator.com/item?id=36642315)

NVIDIA is aiming to make it easier for engineers to operationalize AI applications across different platforms with the introduction of the NVIDIA Cloud Native Stack Virtual Machine Image (VMI). This GPU-accelerated image comes pre-installed with the Cloud Native Stack, which includes Kubernetes and the NVIDIA GPU Operator. The GPU Operator automates the management of software needed to expose GPUs on Kubernetes, ensuring better performance and utilization. The Cloud Native Stack VMI is certified and validated for compatibility with leading Kubernetes solutions and is available on AWS, Azure, and GCP. Additionally, NVIDIA is offering enterprise support for the Cloud Native Stack VMI and GPU Operator through NVIDIA AI Enterprise, providing users with access to NVIDIA AI experts, service-level agreements, and control over upgrade and maintenance schedules. The compute orchestration platform Run:ai has also certified NVIDIA AI Enterprise, allowing enterprises to streamline their data science pipeline and accelerate development and deployment of AI models. Run:ai's platform simplifies GPU access, management, and utilization, with capabilities for automating the orchestration and virtualization of hardware resources. With the NVIDIA Cloud Native Stack VMI, users can add cloud instances as GPU-powered worker nodes to their Kubernetes clusters. Overall, these offerings from NVIDIA aim to make it easier for organizations to leverage GPUs for AI applications across various platforms.

The discussion on this submission revolves around the complexity of Kubernetes (K8s) and its benefits and challenges in cloud deployments. Some users argue that learning Kubernetes is essential for managing cloud infrastructure, while others find it complicated and believe that it adds additional overhead. Some users provide resources and strategies for learning Kubernetes, while others share their experiences with managing large-scale deployments across different cloud providers. There is a debate about whether Kubernetes is a suitable solution for everyone. Some argue that Kubernetes is a fantastic method for abstracting away the complexities of hosting and non-preemptive versus preemptive cloud providers. On the other hand, some users highlight the difficulties in managing Kubernetes in multi-cloud environments and express that it may not be the right choice for every use case. The discussion also touches on the challenges of scaling containerized applications, the differences in behavior between cloud providers, and the benefits of using Kubernetes-specific cloud services versus general cloud resources. There is a consensus that Kubernetes offers benefits in managing resources and scalability, but it may require a significant learning curve and understanding of the underlying infrastructure.

### If PEP 703 is accepted, Meta can commit three engineer-years to no-GIL CPython

#### [Submission URL](https://discuss.python.org/t/a-fast-free-threading-python/27903/99) | 618 points | by [bratao](https://news.ycombinator.com/user?id=bratao) | [419 comments](https://news.ycombinator.com/item?id=36643670)

Lie Ryan has proposed a solution to the threading challenges faced by Python developers. He believes that if free threading is possible, even developers who only work with single threads will still be affected by threading issues. This is because libraries can start background threads, causing threading problems in code that never expected them. However, Ryan suggests implementing a voluntary lock for threads in order to avoid this issue. Other users on Hacker News have expressed their opinions on the matter, with some highlighting the need for specific examples before considering it a significant problem. This ongoing discussion explores the implications of free threading in Python and its potential impact on developers.

The discussion on Hacker News about Lie Ryan's proposed solution to threading challenges in Python covers a range of opinions and perspectives. One user points out that removing the Global Interpreter Lock (GIL) in Python would require significant changes, including rewriting existing C-API extensions. They argue that the effort involved in rewriting and maintaining these extensions is one of the reasons why Python remains popular, despite the GIL limitations. Another user mentions that they have little experience with multithreading in Python and suggests looking up examples online. They express that multithreading is not a common requirement for their personal use cases. Overall, the discussion on Hacker News reflects a range of opinions regarding the proposed solution to threading challenges in Python, with varying perspectives on the need for GIL removal and the potential impacts it may have on the Python ecosystem.

### Machine Unlearning Challenge

#### [Submission URL](https://ai.googleblog.com/2023/06/announcing-first-machine-unlearning.html) | 162 points | by [Anon84](https://news.ycombinator.com/user?id=Anon84) | [29 comments](https://news.ycombinator.com/item?id=36649710)

Unlearning is an emerging field in machine learning that focuses on removing the influence of specific training examples from a trained model. Google, along with a group of academic and industrial researchers, has organized the first Machine Unlearning Challenge to further advance this field. The competition, which will be hosted on Kaggle, will require participants to develop efficient and effective unlearning algorithms that can remove a subset of training images without compromising the model's utility. This challenge aims to address the challenges of unlearning, such as maintaining accuracy while removing data and evaluating the effectiveness of different unlearning methods. Machine unlearning has various applications, including protecting user privacy, erasing inaccurate information, and correcting unfair biases in models.

### CrunchGPT: A ChatGPT assisted framework for scientific machine learning

#### [Submission URL](https://arxiv.org/abs/2306.15551) | 74 points | by [occamschainsaw](https://news.ycombinator.com/user?id=occamschainsaw) | [3 comments](https://news.ycombinator.com/item?id=36644933)

Researchers have developed a framework called CrunchGPT that integrates various stages of Scientific Machine Learning (SciML) using the ChatGPT language model. SciML aims to seamlessly integrate data and physics without the need for complex data assimilation methods. However, preprocessing, problem formulation, code generation, postprocessing, and analysis are still time-consuming. CrunchGPT acts as a conductor, orchestrating the workflow of SciML based on simple prompts from the user. The framework has been demonstrated in optimizing airfoils in aerodynamics and obtaining flow fields in various geometries in interactive mode. The researchers also created a webapp with a guided user interface and options for a comprehensive summary report. The ultimate goal is to expand CrunchGPT's capabilities to handle diverse problems in computational mechanics, design, optimization, controls, and general scientific computing tasks. Future versions of CrunchGPT may target other fields like solid mechanics, materials science, geophysics, systems biology, and bioinformatics.

The discussion mainly revolves around one user, malux85, who shares their experience and interest in working on a chemistry hobby project. They mention using a framework called Atomic Tessellator and providing a link to a prototype they are working on. They express the need for help with letter writing, hypothesis generation, experimental design, execution, simulations, and result analysis. Another user, bjctv, offers assistance and suggests integrating feedback. Malux85 shares their enthusiasm for their project and mentions working on distributed computing and parallel simulations. They also mention their connections in the industry and enjoying working in the field. Frggychrs compliments malux85's work and expresses interest in ClimateTech and working on climate-related projects. Malux85 thanks them for their appreciation.

---

## AI Submissions for Fri Jul 07 2023 {{ 'date': '2023-07-07T17:10:26.489Z' }}

### Fedora considers “privacy-preserving” telemetry

#### [Submission URL](https://lwn.net/Articles/937528/) | 131 points | by [pabs3](https://news.ycombinator.com/user?id=pabs3) | [208 comments](https://news.ycombinator.com/item?id=36630032)

The Fedora project is considering adding limited, opt-out telemetry to its workstation edition. While the use of the term "telemetry" has raised some concerns, the developers of Fedora believe that collecting aggregate data on software usage can be done ethically and without compromising users' privacy. Users will have the option to disable data upload before any data is sent, and the data collection will be operated by Fedora on Fedora infrastructure, not relying on third-party services. Additionally, users can redirect the data collection to their own private metrics server. While there are objections to the opt-out nature of the telemetry, the developers insist that they will ensure compliance with European law and respect users' privacy.

The discussion on the submission revolves around the topic of telemetry and its implications for privacy. Some commenters express concerns about the potential privacy implications of data collection, while others argue that telemetry can be beneficial for improving software quality and making informed decisions. There is also a discussion about the importance of maintaining a balance between privacy and the need for data to drive product improvements. Some commenters highlight the importance of transparency and consent in data collection practices, while others discuss the potential use of AI in data collection and analysis. Other topics of discussion include the role of telemetry in safety-critical systems and the challenges of balancing user preferences and standardization in software development. Some commenters also provide examples of how telemetry has been used to improve software and user experience in the past. Overall, the discussion highlights the complex considerations surrounding the implementation of telemetry in software projects.

### ChatGPT loses users for first time, shaking faith in AI revolution

#### [Submission URL](https://www.washingtonpost.com/technology/2023/07/07/chatgpt-users-decline-future-ai-openai/) | 196 points | by [mfiguiere](https://news.ycombinator.com/user?id=mfiguiere) | [270 comments](https://news.ycombinator.com/item?id=36636039)

Consumer interest in AI chatbots and image-generators may be starting to decline as download numbers and website traffic for OpenAI's ChatGPT have fallen for the first time since its launch in November. ChatGPT gained widespread popularity and sparked an AI race among tech giants when it unveiled its capabilities to engage in complex conversations, write poetry, and pass professional exams. However, users have started to encounter the chatbot generating false information, leading to a realization that its usefulness may have been overhyped. The drop in usage could also be influenced by the bot's limitations, concerns over data leaks, the rising cost of running the bot, and the potential impact of looming regulations.

The discussion on Hacker News revolves around the decline in usage and interest in OpenAI's ChatGPT. Users point out that while ChatGPT initially garnered excitement and attention, its limitations and the realization that it can generate false information have led to a decline in its popularity. Some users express that ChatGPT is not efficient and can generate time-consuming and inaccurate responses. Others highlight the importance of context and the need for human-written responses rather than relying solely on AI-generated text. Some users also discuss their experiences with ChatGPT and share their frustrations with its inability to understand complex instructions or certain programming languages. The discussion also touches on the role of search engines, with users noting that while search engines like Google can provide helpful information, ChatGPT's ability to generate satisfactory answers quickly is appealing to some users.

### Mechanical Turk workers are using AI to automate being human

#### [Submission URL](https://techcrunch.com/2023/06/14/mechanical-turk-workers-are-using-ai-to-automate-being-human/) | 243 points | by [elsewhen](https://news.ycombinator.com/user?id=elsewhen) | [150 comments](https://news.ycombinator.com/item?id=36629777)

In a study conducted by researchers at EPFL in Switzerland, it has been found that nearly half of the workers on Amazon's Mechanical Turk platform may be utilizing AI to complete tasks that were intended for humans. Mechanical Turk allows users to divide small tasks into subtasks, paying workers a small amount of money for each completed task. These tasks often involve activities that are difficult to automate, such as CAPTCHA solving or sentiment analysis. However, the study reveals that workers are using large language models like ChatGPT to automate their work. This raises concerns about the reliability of the data collected through Mechanical Turk and highlights the growing issue of "AI training on AI-generated data." As language models continue to advance, it becomes harder to determine whether a task was actually completed by a human or AI. The researchers suggest that new measures need to be taken to ensure the integrity of human-generated data amidst this development.

### GPU Guide (For AI Use-Cases)

#### [Submission URL](https://gpus.llm-utils.org/the-gpu-guide/) | 43 points | by [tikkun](https://news.ycombinator.com/user?id=tikkun) | [24 comments](https://news.ycombinator.com/item?id=36632397)

The author discusses the best AI tools worth running and provides recommendations for different use cases. They suggest running stable diffusion, whisper transcription, and open language models like GPT-3.5 or GPT-4. They also provide recommendations for GPUs based on different models, whether running on cloud or locally. The article explains the difference between RTX 6000, A6000, and 6000 Ada GPUs, as well as the difference between DGX GH200, GH200, and H100 GPUs. The author also mentions Nvidia's official cloud offering, DGX Cloud, and discusses the significant upgrade H100s offer over A100s for training language models. They touch on other GPU options like AMD and Intel and provide suggestions for GPU cloud providers based on specific needs. The article concludes by recommending Runpod and their templates as the easiest GPU cloud to start with.

The discussion on this submission covers a range of topics related to AI tools, GPU recommendations, and possibilities for cloud deployment. Several commenters thank the author for their insights and provide additional recommendations. One commenter mentions the performance of the RTX 4000 cards for running Stable Diffusion, while another discusses the potential benefits of using CPU instances for large models with low VRAM GPUs. There is a conversation about the VRAM limitations of consumer graphics cards and the potential for Nvidia to make consumer-friendly options. The discussion also touches on alternative GPU options like AMD and Apple's M1/M2 chips. The topic of GPU cloud providers is brought up, with some commenters mentioning pricing differences and their own experiences. There is a recommendation to include specific pricing metrics in the article for more informed recommendations. Other topics include local GPU options, FPGA deployment, and the potential future offerings from Intel and AMD.

---

## AI Submissions for Thu Jul 06 2023 {{ 'date': '2023-07-06T17:11:21.096Z' }}

### AI agents that “self-reflect” perform better in changing environments

#### [Submission URL](https://hai.stanford.edu/news/ai-agents-self-reflect-perform-better-changing-environments) | 204 points | by [chdoyle](https://news.ycombinator.com/user?id=chdoyle) | [40 comments](https://news.ycombinator.com/item?id=36622959)

Researchers at Stanford have developed a training method called "curious replay" that helps AI agents explore and adapt to new environments. The method is based on the way animals naturally excel at exploring and adapting to their surroundings. In experiments, an AI agent and a mouse were put in separate environments with a red ball. The mouse quickly approached and interacted with the ball, while the AI agent didn't notice it. Adding the "curious replay" training method improved the AI agent's performance and ability to engage with the ball. The method has potential applications in various fields, from robotics to personalized learning tools.

The discussion on this submission has covered a range of topics related to the "curious replay" training method for AI agents. Some users have discussed the details and potential applications of the method, while others have expressed skepticism or confusion about certain aspects. One user questioned the efficiency of AI processing and commented on the need for AI to replicate Eureka moments. Another user mentioned the importance of exploration in reinforcement learning and the trade-offs involved. The concept of "curious replay" and its benefits were also discussed. Some users debated the use of provocative or misleading titles in news articles, while others shared relevant links and resources. The discussion also veered into topics such as self-reflection, rationality, and the similarities between AI and human cognitive processes.

### GPT-4 API General Availability

#### [Submission URL](https://openai.com/blog/gpt-4-api-general-availability) | 719 points | by [mfiguiere](https://news.ycombinator.com/user?id=mfiguiere) | [517 comments](https://news.ycombinator.com/item?id=36621120)

OpenAI has announced the general availability of the GPT-4 API for all paying customers. This highly capable model has been eagerly anticipated by developers, and millions of requests for access have been made since March. Alongside GPT-4, OpenAI is also making the GPT-3.5 Turbo, DALL·E, and Whisper APIs generally available. However, OpenAI is now recommending that users transition from the older Completions API to the newer Chat Completions API, as it provides better results with a more structured prompt interface. The Chat Completions API accounts for 97% of API usage and enables developers to build conversational experiences and a wide range of completion tasks. To optimize compute capacity and focus on the Chat Completions API, OpenAI plans to retire older models in the Completions API beginning in January 2024. Users will be required to upgrade their integration to the recommended models or specify the new models in API requests.

### My small, no name company has lost its mind with AI

#### [Submission URL](https://www.teamblind.com/post/My-small-no-name-company-has-completely-lost-its-mind-with-AI-nfqEDfSi) | 98 points | by [donsupreme](https://news.ycombinator.com/user?id=donsupreme) | [93 comments](https://news.ycombinator.com/item?id=36611356)

Story: A software engineer at a small, unknown company shared their frustration on Blind about their CEO's obsession with AI. The company's CEO views AI as a magical solution that will fix all their problems, while the managers have jumped on the hype train to please him. The engineer described various misguided attempts to implement AI, including using ChatGPT to write JIRA tickets and acceptance criteria, generating HR documents without legal checks, and feeding proprietary information into ChatGPT without concerns. The engineer believes that the company's reliance on AI is a desperate attempt to solve long-standing issues with documentation, code quality, and requirements clarity. They also expressed concerns about the future if even small companies are behaving this way.

The discussion on Hacker News mainly revolves around the limitations and potential dangers of relying too heavily on AI. Some users point out that the CEO's decision to implement AI in various aspects of the company's operations seems to be based on hype rather than sound decision-making. Others highlight the importance of human input and specialized skills in solving complex problems, suggesting that AI should be seen as a tool rather than a magical solution. The discussion also touches on the impact of AI on jobs and the cyclical nature of AI hype. Some users express concerns about the quality and reliability of AI-generated content, particularly in the context of search engines and SEO. Additionally, there are discussions about the usefulness of AI tools like ChatGPT and the challenges of implementing AI in practical applications. Overall, the discussion raises important questions about the responsible use of AI and the need to balance its potential benefits with careful consideration of its limitations.

### Responsibly empowering developers with AI on MDN

#### [Submission URL](https://blog.mozilla.org/en/products/mdn/responsibly-empowering-developers-with-ai-on-mdn/) | 14 points | by [deviantintegral](https://news.ycombinator.com/user?id=deviantintegral) | [3 comments](https://news.ycombinator.com/item?id=36624590)

Mozilla has launched AI integrations with its web developer reference documentation, MDN, to provide developers with AI-driven helpers. The AI Help feature allows developers to ask questions and receive concise answers with related MDN articles for contextual help, while the AI Explain feature enables readers to explore and understand code blocks in MDN documentation. These tools aim to save developers time and provide learning resources, particularly for early-stage developers. Although there have been instances where the AI tools provide incorrect information, the MDN team is working to improve their accuracy and encourages users to provide feedback. The goal is to make MDN more accessible and useful without compromising its role as a high-quality reference source. The discussion on Hacker News includes a comment by user "klysm" who argues that responsible AI models should not be deployed without proper context, as they may produce incorrect information. Another user, "mndcrm", provides a link to a related issue and suggests that the AI Help button should include good links to existing resources.

---

## AI Submissions for Wed Jul 05 2023 {{ 'date': '2023-07-05T17:10:12.974Z' }}

### Cicada: Private on-chain voting using time-lock puzzles

#### [Submission URL](https://a16zcrypto.com/posts/article/building-cicada-private-on-chain-voting-using-time-lock-puzzles/) | 38 points | by [subsequent](https://news.ycombinator.com/user?id=subsequent) | [27 comments](https://news.ycombinator.com/item?id=36607081)

A new open-source Solidity library called Cicada has been released, aimed at providing privacy for on-chain voting on platforms such as Ethereum. Cicada utilizes time-lock puzzles and zero-knowledge proofs to ensure private voting, addressing the drawbacks of current on-chain voting protocols that lack privacy. By securing the secrecy of individual ballots, vote tallies, and voter identities, Cicada aims to prevent manipulation and encourage democratic outcomes in decentralized organizations. Developers are encouraged to explore Cicada's GitHub repository and consider its potential for different voting schemes and features. The library offers running tally privacy, which can be combined with zero-knowledge group membership proofs to achieve voter anonymity and ballot privacy.

The discussion on the submission about the new open-source Solidity library, Cicada, revolves around various aspects of its privacy features and the challenges associated with on-chain voting. One user points out that the privacy aspect of on-chain voting is an interesting concept, but they find it difficult to fully understand how it would work in practice. Another user adds that while the library offers privacy, the final tally of the votes can still be deduced, albeit reasonably small.

The discussion also touches on the notion of blockchain technology and its reputation for providing security and trust. Some users express skepticism about implementing transparent, blockchain-backed voting systems, as they believe they could still be subject to conspiracy and fraud. One user shares a relevant XKCD comic that highlights the issue of misinformation and the challenges of reinventing the entire voting system. The conversation further delves into the feasibility and trustworthiness of different voting systems, with references to specific regions and their respective experiences. Overall, the discussion highlights the complexities and challenges associated with implementing secure and private voting systems, while also questioning the effectiveness and trustworthiness of existing solutions.

### The many ways that digital minds can know – A better way to think about LLMs

#### [Submission URL](https://moultano.wordpress.com/2023/06/28/the-many-ways-that-digital-minds-can-know/) | 112 points | by [moultano](https://news.ycombinator.com/user?id=moultano) | [14 comments](https://news.ycombinator.com/item?id=36603573)

LLMs, or large language models, have been the subject of intense debate among proponents and critics. However, the author of this post argues that LLMs actually fulfill the claims of both sides simultaneously. They possess both the ability to generate AGI-like output and to learn the complex functions of multivariable calculus. The author introduces new terminology to discuss these phenomena: search index size and memorization. They draw an analogy between search engine index size and LLMs' ability to "memorize" information. A larger search index allows search engines to provide more accurate results to specific queries, even for obscure topics. Similarly, LLMs can generate relevant responses by inferring meaning from queries, even if the exact wording is not present in the training data. The post explores this analogy further, highlighting the importance of both memorization and generalization in LLMs' capabilities.

The discussion surrounding the submission is varied. One user discusses the similarities between large language models (LLMs) and the concept of Relational Frame Theory (RFT), highlighting how both involve identifying and manipulating relationships and contextual understanding. Another user disagrees, stating that LLMs do not truly understand text but instead analyze and reproduce statistical patterns. The conversation shifts to the understanding and interpretation capabilities of LLMs compared to humans, with one user highlighting their conversations with ChatGPT and suggesting that it has some level of understanding, albeit different from humans. There are also discussions about cognitive context, the limitations of LLMs, and the role of numerical computation in LLMs. Another user comments on the power and limitations of LLMs, while others question the balance of views and parameters in LLMs and the concept of digital minds.

### ChatGPT's explosive growth shows first decline in traffic since launch

#### [Submission URL](https://www.reuters.com/technology/booming-traffic-openais-chatgpt-posts-first-ever-monthly-dip-june-similarweb-2023-07-05/) | 24 points | by [mfiguiere](https://news.ycombinator.com/user?id=mfiguiere) | [13 comments](https://news.ycombinator.com/item?id=36606754)

OpenAI's AI chatbot, ChatGPT, experienced a decline in monthly traffic and unique visitors in June, marking the first time this has occurred since its launch in November 2022. According to analytics firm Similarweb, desktop and mobile traffic to the ChatGPT website dropped by 9.7%, while unique visitors decreased by 5.7%. The amount of time spent on the website also saw an 8.5% decline. Similarweb's Senior Insights Manager David Carr attributes the decreasing traffic to the novelty of the chatbot wearing off, while RBC Capital Markets analyst Rishi Jaluria suggests a greater demand for generative AI with real-time information. Despite the drop in website traffic, ChatGPT remains the fastest-growing consumer application ever and currently boasts over 1.5 billion monthly visits, placing it among the top 20 websites globally. OpenAI has not yet commented on the recent decline in traffic.

---

## AI Submissions for Tue Jul 04 2023 {{ 'date': '2023-07-04T17:10:15.539Z' }}

### The Lone Banana Problem in AI

#### [Submission URL](https://www.digital-science.com/tldr/article/the-lone-banana-problem-or-the-new-programming-speaking-ai/) | 139 points | by [JohnHammersley](https://news.ycombinator.com/user?id=JohnHammersley) | [96 comments](https://news.ycombinator.com/item?id=36582937)

In this article, Daniel Hook, CEO of Digital Science, explores the potential biases present in Large Language Models (LLMs) and their impact on AI-generated content. He coined the phrase "Lone Banana Problem" to describe the subtle biases that can be difficult to detect. To illustrate this, Hook used an AI program called Midjourney to generate an image of a single banana casting a shadow on a grey background. However, even after refining his prompt, the AI consistently produced images with multiple bananas. This led Hook to question the biases embedded in the AI's training data and the need for a deeper understanding of these technologies. Despite the amusing nature of the "Lone Banana Problem," it raises important considerations about the limitations and potential pitfalls of AI language models.

### Google's updated privacy policy states it can use public data to train its AI

#### [Submission URL](https://www.engadget.com/googles-updated-privacy-policy-states-it-can-use-public-data-to-train-its-ai-models-095541684.html) | 173 points | by [firstSpeaker](https://news.ycombinator.com/user?id=firstSpeaker) | [91 comments](https://news.ycombinator.com/item?id=36586170)

Google has recently updated its privacy policy to clarify that it can use publicly available data to train its AI models. The updated policy specifies that this data can be used to not only build features but also full products like Google Translate, Bard, and Cloud AI capabilities. By making this change, Google is notifying users that anything they publicly post online could be used to train its AI systems. This comes as critics have raised concerns about companies using personal data from the internet without consent to train their language models for generative AI use. OpenAI, for example, is facing a proposed class action lawsuit for allegedly scraping personal data without consent. As more companies develop generative AI products, similar lawsuits are likely to emerge in the future. In response to data scraping concerns, websites like Reddit have started charging access to their API, while Twitter has limited the number of tweets a user can see per day.

The discussion on this submission revolves around the topic of public versus private information and the legality of taking and publishing photos in public spaces. Some users argue that taking photos in public places is legally allowed and does not require explicit consent from individuals in the background. Others mention that there are restrictions in certain countries, such as Germany, where taking photos of vulnerable individuals or private locations without permission is prohibited.  There is also a discussion about the privacy laws in different countries, particularly in the European Union (EU) and the United States (US). Users point out that there are substantial differences between the two regions, with some arguing that EU laws prioritize individual privacy rights more than the US. The debate expands to include factors such as population size, democratic systems, and economic considerations. Another topic discussed is the responsibilities of companies like Google in handling public data and complying with legal requirements. One user highlights the case of Wikileaks, raising questions about the legality of accessing and processing public data without authorization. It is noted that if information remains classified or requires security clearances, accessing or disseminating it would be illegal.

### Chat-based Large Language Models replicate the mechanisms of a psychic’s con

#### [Submission URL](https://softwarecrisis.dev/letters/llmentalist/) | 24 points | by [EventH-](https://news.ycombinator.com/user?id=EventH-) | [10 comments](https://news.ycombinator.com/item?id=36586540)

In a thought-provoking article, Baldur Bjarnason investigates the phenomenon of language models, specifically chat-based large language models (LLMs), being perceived as intelligent. Bjarnason argues that LLMs are not capable of reasoning or thinking like humans do, as they are merely mathematical models of language tokens. He presents two possible explanations for the intelligence illusion: either the tech industry has unintentionally developed a completely new kind of mind, or the illusion lies in the mind of the user. Bjarnason aligns himself with the latter camp, drawing a parallel between the intelligence illusion and the tactics used by psychics in cold reading. By employing validation statements and statistically probable guesses, both chatbots and psychics create the illusion of intelligence and specificity. Bjarnason suggests that the rise of chat-based LLMs has unintentionally resulted in an automation of the psychic con, where users are tricked into perceiving an intelligence that doesn't truly exist.

The discussion in the comments starts with a user expressing frustration with system administrators and suggesting that GPT-4, a large language model (LLM), might help solve system problems. Another user points out that defining intelligence is a subjective and fluff-filled discussion, and that minimizing attention given to doubts and definitions can hinder understanding. Another commenter is impressed by LLMs' ability to understand instructions to some extent, giving an example of using LLMs for debugging. However, they argue that LLMs' capabilities are limited to statistical matching rather than true intelligence. A user counters this argument by suggesting that intelligence can be learned through training data, mentioning the concept of predictive coding and the recursive identification of models within the human brain. They propose that LLMs could potentially reach a level of intelligence similar to humans. Someone else brings up the objective view that LLMs simply produce text based on probabilistic matrices and do not actually possess intelligence. They argue that LLMs mimic structures but lack the complexity for reasoning and consciousness. Another user agrees with this view, stating that the development of LLMs has not led to the invention of a new kind of mind, but rather the tech industry stumbled upon unknown principles and processes. They assert that intelligence lies in the mind of the user rather than the LLM itself. Finally, there is a mention of the challenges scientists face in understanding the intricacies of the human brain. A user sarcastically remarks that people impressed with LLMs should reconsider granting machines rights, hinting at potential issues surrounding artificial intelligence and its impact on society. One user concludes the discussion by referring to "scientism," suggesting that there is an overreliance on science as an ideology.

### A human just defeated an AI in Go. Here's why that matters

#### [Submission URL](https://www.zmescience.com/future/a-human-just-defeated-an-ai-in-go-heres-why-that-matters/) | 64 points | by [amadeuspagel](https://news.ycombinator.com/user?id=amadeuspagel) | [21 comments](https://news.ycombinator.com/item?id=36590242)

In a surprising turn of events, a human has defeated an AI in the complex game of Go. Go is considered one of the most intricate games ever created, with an almost unfathomable number of possible moves. AI has proven to be a formidable opponent in the past, but humans have now learned to exploit its weaknesses. Researchers trained their own AI opponents to trick the reigning AI champion, KataGo, and amateur player Kellin Pelrine managed to beat KataGo 14 out of 15 times. This outcome highlights an important lesson for the future of artificial intelligence: high performance doesn't always guarantee robustness. Even the most advanced AI systems can have blind spots or vulnerabilities, which is crucial to consider as AI technology becomes increasingly integrated into real-world applications. By studying these flaws and exploits in game-playing AI, we can gain insights into how these algorithms behave and better understand the potential risks and limitations of AI in the real world.

The discussion surrounding the submission revolves around various topics related to the defeat of an AI by a human in the game of Go. Some comments draw parallels to other games like chess and CSGO, highlighting the importance of strategy and skill level. There is also a mention of how memory retention can impact gameplay. The discussion then delves into the vulnerabilities and flaws of AI systems, with references to Murphy's Law and the greater threat posed by human cognition. The exploit found by Kellin Pelrine in defeating KataGo is noted, along with comparisons to previous AI victories. The article's emphasis on the significance of AI weaknesses and the potential risks and limitations of AI in real-world applications is also acknowledged. Some comments explore the nature of AI training and the need for a deeper understanding of specific tactics. There is also a discussion about AI's ability to extrapolate and the possibility of AI making legally binding decisions. Overall, the discussion highlights the complexities of AI and the need for further exploration and understanding.

### GPT-4 is great at infuriating telemarketing scammers

#### [Submission URL](https://www.theregister.com/2023/07/03/jolly_roger_telephone_company/) | 139 points | by [isaacfrond](https://news.ycombinator.com/user?id=isaacfrond) | [87 comments](https://news.ycombinator.com/item?id=36583969)

In a refreshing twist on AI implementation, a California man has created a business that uses chatbots to frustrate telemarketing scammers. The Jolly Roger Telephone Company offers customers the ability to merge their calls with chatbots that engage the scammers in bizarre and nonsensical conversations, ultimately wasting their time. The company has a range of bots available, each with their own unique voice and quirks. Not only does this business provide entertainment for those annoyed by telemarketers, it also serves as an effective tool against scammers. The Jolly Roger Telephone Company has thousands of subscribers paying $23.80 a year for the service.

The discussion on the Hacker News submission revolves around various aspects of telemarketing scams and the use of chatbots to counter them. Some commenters discuss the technical implementation and efficacy of using chatbots to frustrate scammers, while others highlight the potential ethical and legal concerns.  One commenter suggests using machine learning or AI-powered call blockers instead of chatbots, mentioning that the computational resources wasted on engaging with scammers could be better used elsewhere. Another commenter argues that relying on external networks and third-party services for call-blocking could pose security risks and may not be ideal from a privacy standpoint.  There is also a discussion on the terminology used, particularly the term "telemarketing," with some pointing out that it traditionally refers to phone calls and not online advertising or billboards. The conversation diverges into debates about free speech, the regulation of advertising, and the banning of billboards in certain states.  Commenters express concerns about the limitations and potential abuse of AI-powered systems, such as flooding emergency services with fake calls or spamming local businesses with negative reviews. Some also discuss the challenges of identity verification and the potential for AI-powered bots to handle sensitive information in the future. The discussion ends with a couple of comments highlighting examples of AI implementation in other contexts and imagining the potential consequences of widespread adoption of AI.

---

## AI Submissions for Mon Jul 03 2023 {{ 'date': '2023-07-03T17:11:17.405Z' }}

### The industry behind the industry behind AI

#### [Submission URL](https://restofworld.org/2023/exporter-industry-behind-ai/) | 36 points | by [marban](https://news.ycombinator.com/user?id=marban) | [12 comments](https://news.ycombinator.com/item?id=36573813)

The hidden labor behind artificial intelligence (AI) is brought to light in a recent feature by The Verge. The article focuses on a Remotasks office in Nairobi which is a subsidiary of Scale AI, where workers perform annotation tasks to improve AI algorithms. These tasks range from identifying human or robotic voices in audio clips to rating the sexual provocation of online ads. The article highlights the reliance of generative AI on human labor and the low wages that workers in this field often receive. The similarities between AI annotation work and moderation contractors—who clean up platforms like Facebook and YouTube—are noted, as many of these companies operate in both areas. The article discusses the industry of business process outsourcing (BPO), which includes call centers and various types of work. While annotation work may not be traumatic like moderation, the lack of better pay and visibility for workers remains a concern.

### Nvidia’s H100: Funny L2, and Tons of Bandwidth

#### [Submission URL](https://chipsandcheese.com/2023/07/02/nvidias-h100-funny-l2-and-tons-of-bandwidth/) | 129 points | by [picture](https://news.ycombinator.com/user?id=picture) | [48 comments](https://news.ycombinator.com/item?id=36569044)

Nvidia has released its latest compute-oriented GPU, the H100, built on the Hopper architecture. The GPU features 144 Streaming Multiprocessors, 60 MB of L2 cache, and 12 512-bit HBM memory controllers. The PCIe version of the H100, tested on Lambda Cloud, offers 114 SMs, 50 MB of L2 cache, and 10 HBM2 memory controllers. The SXM form factor H100, on the other hand, can draw up to 700W and has 132 SMs enabled, along with HBM3 memory for additional bandwidth. The H100 boasts higher boost clock speeds than its predecessor, the A100, but sometimes drops to 80% of its maximum boost clock during microbenchmarking. The H100 features larger L1/Shared Memory capacity and a 50 MB L2 cache, with access to the "far" L2 partition taking nearly twice as long. Overall, the H100 represents a significant improvement over the A100 in terms of cache capacity and latency.

### Tesla is valuing Full Self-Driving high only when it’s convenient

#### [Submission URL](https://electrek.co/2023/07/03/tesla-valuing-full-self-driving-high-when-convenient/) | 39 points | by [mfiguiere](https://news.ycombinator.com/user?id=mfiguiere) | [12 comments](https://news.ycombinator.com/item?id=36578225)

Tesla's valuation of its Full Self-Driving (FSD) package appears to be inconsistent, with the company valuing it high when it's convenient and lower for trade-ins. Tesla CEO Elon Musk has previously claimed that vehicles equipped with the FSD package would be "appreciating assets" as the package improved through software updates. However, trade-in estimates for Tesla vehicles with the FSD package seem to be lower than the package's actual price. This has led to frustration among Tesla owners who feel that their FSD package is being devalued, despite Tesla not delivering on its promises of full autonomy. Some users have suggested that Tesla should allow the transfer of the FSD software to new vehicles to incentivize current owners to upgrade. This would help increase sales and create goodwill around the FSD package.

The discussion on Hacker News regarding the submission revolves around frustration with Tesla's valuation and handling of its Full Self-Driving (FSD) package. Some users express frustration with Elon Musk's claims about the FSD package being an appreciating asset despite the package not delivering on its promises of full autonomy. Others suggest that allowing the transfer of the FSD software to new vehicles would incentivize current owners to upgrade, increase sales, and create goodwill. Some users argue that Tesla doesn't value the FSD package highly because it isn't willing to help sales by creating goodwill around it. There is also mention of the frustration over not being able to trade-in software and the misunderstanding that Tesla doesn't allow it, with clarification that it's about not being able to transfer the software to third-party buyers. The discussion also touches on skepticism about Musk's claims and connects it to potential stock market dynamics. Overall, the discussion highlights concerns and dissatisfaction with Tesla's valuation and handling of the FSD package.

### Self-driving cars are surveillance cameras on wheels

#### [Submission URL](https://www.schneier.com/blog/archives/2023/07/self-driving-cars-are-surveillance-cameras-on-wheels.html) | 261 points | by [activiation](https://news.ycombinator.com/user?id=activiation) | [270 comments](https://news.ycombinator.com/item?id=36572401)

Self-driving cars may be convenient for commuting, but they're also becoming a new tool for law enforcement surveillance. Police are increasingly using footage from self-driving cars as video evidence in criminal investigations. While security cameras are already common in cities, self-driving cars offer a new level of access and coverage. They capture a wider range of footage as they navigate the city, making it easier for law enforcement to turn to one company with a large repository of videos instead of reaching out to multiple businesses with their own security systems. However, this raises concerns about privacy and the erosion of personal freedom. Advocates argue that individuals should be able to go about their daily lives without constant surveillance, unless they are suspected of a crime. As self-driving cars become more prevalent, it's likely that video evidence will play a larger role in criminal cases.

The discussion on Hacker News revolves around the topic of self-driving cars and their use in law enforcement surveillance. Some users express concerns about the erosion of privacy and personal freedom, while others argue that video evidence from self-driving cars can be helpful in criminal investigations. There is also a discussion about the limitations and potential abuses of surveillance systems, as well as the role of speed cameras and the impact of government-owned cameras. Additionally, there are debates about the importance of privacy and the potential benefits and drawbacks of data collection by connected cars. Some users highlight the need for regulation and accountability in the use of surveillance technology.

### Valve responds to claims it has banned AI-generated games from Steam

#### [Submission URL](https://techcrunch.com/2023/07/03/valve-responds-to-claims-it-has-banned-ai-generated-games-from-steam/) | 23 points | by [lsllc](https://news.ycombinator.com/user?id=lsllc) | [12 comments](https://news.ycombinator.com/item?id=36580344)

Valve, the developer of the Half-Life series and operator of the Steam games store, has clarified its policy on games with AI-generated assets. This comes after rumors spread that Valve was rejecting games utilizing AI-generated content. The company stated that its policy is not a stand against AI, but rather an evolving approach to content approval. Valve's rules on content can be unclear until developers test them with unique cases. One developer had their game rejected due to having AI-generated assets that potentially infringed on intellectual property rights. Valve cited unclear legal ownership of such assets as the reason for rejecting the game. The use of AI as a game development tool is not controversial, with major developers like Ubisoft embracing the technology. However, the issue arises when AI-generated content involves unpaid artists. It remains unclear who bears liability for the generated art. Valve clarified that its review process is based on current copyright laws, not personal opinion. In cases where this policy decision influences game rejection, Valve will refund the app submission fee. While some developers may utilize AI-generated content for quick profits, as the use of AI tools becomes more widespread and sophisticated, the matter becomes less straightforward.

The discussion on the submission primarily revolves around the implications of using AI-generated assets in games and the legal and ethical challenges associated with it. Some users express concerns about the potential low quality and lack of originality in AI-generated content, suggesting that it may lead to an influx of low-quality games on platforms like Steam. Others raise issues of copyright infringement and the difficulty in determining ownership of AI-generated assets.  There is also a discussion about the regulations and policies surrounding AI-generated games. Some users reference Google's policies regarding AI-generated content and the potential legal battles that could arise from such regulations. One user argues that AI-generated assets should be allowed under Creative Commons licenses, while another highlights the challenges posed by AI-generated textures and the need for greater compensation for content creators. One user brings up the topic of attributing AI-generated content, noting that it can be challenging to trace the original source of such content. Another user argues that using AI for content generation is similar to using tools like Photoshop and should be seen in the same light. Overall, the discussion highlights the complex legal, ethical, and quality-related considerations surrounding AI-generated assets in games.

---

## AI Submissions for Sun Jul 02 2023 {{ 'date': '2023-07-02T17:09:42.147Z' }}

### AI and the Automation of Work

#### [Submission URL](https://www.ben-evans.com/benedictevans/2023/7/2/working-with-ai) | 196 points | by [CharlesW](https://news.ycombinator.com/user?id=CharlesW) | [214 comments](https://news.ycombinator.com/item?id=36565854)

In a recent blog post, Benedict Evans discusses the impact of generative AI, Large Language Models (LLMs), and ChatGPT on the automation of work. He acknowledges that while there is agreement in the tech industry about the transformative power of these technologies, there is much debate about the implications and future consequences.

Evans points out that automation has been happening for the past 200 years, and each wave of automation has eliminated certain jobs but also created new ones. However, when facing automation in our own generation, it's natural to worry that the new jobs won't materialize. While historical evidence suggests otherwise, it's hard to predict what new jobs will emerge. To address this concern, Evans refers to the "Lump of Labour" fallacy, which assumes that there is a fixed amount of work to be done and automation reduces job opportunities. He argues that when automation makes things cheaper, it leads to increased consumption and the creation of new jobs. The ripple effect through the economy generates prosperity and employment.

One criticism of this model is that automation has been progressively moving up the scale of human capabilities. From physical labor to white-collar jobs, if we automate white-collar work, what's left? To counter this, Evans introduces the concept of the Jevons Paradox, which explains that as technology becomes more efficient, it's used more extensively, leading to increased resource consumption. In the case of white-collar work, automation has historically created new opportunities rather than eliminating jobs.

Evans gives examples from history, such as the impact of typewriters and adding machines on clerical employment. Although these technologies reduced the number of clerks required for certain tasks, they also increased productivity and enabled new forms of work. Similarly, he argues that automation can lead to more analysis, improved inventory management, and the creation of businesses that can only exist because of automation.

In conclusion, while concerns about job displacement due to automation are valid, historical evidence suggests that new jobs will emerge. Automation has consistently led to increased productivity and economic growth. Although we can't predict the exact nature of future jobs, Evans remains optimistic that automation will continue to present new opportunities for prosperity.

The discussion on Hacker News revolves around the capabilities and potential dangers of AI taking over various jobs. Some users express concerns about security issues and the potential for AI machines to misinterpret human intentions, leading to fatal mistakes. Others argue that AI reporting machines can be useful in certain situations but should not replace human judgment entirely. The discussion also touches on the impact of automation on the military and law enforcement sectors, with some users pointing out the risks and limitations of relying too heavily on AI in those fields. There is a debate about whether AI will truly replace human jobs or if it will primarily enhance them by taking over tasks that are repetitive or require specific expertise. Overall, the discussion raises valid concerns about the implications of AI in the workforce while also acknowledging its potential benefits.

### Automated CPU Design with AI

#### [Submission URL](https://arxiv.org/abs/2306.12456) | 89 points | by [skilled](https://news.ycombinator.com/user?id=skilled) | [23 comments](https://news.ycombinator.com/item?id=36565671)

Researchers from the field of Artificial Intelligence (AI) have made a significant breakthrough in the realm of machine design. In a recent publication, titled "Pushing the Limits of Machine Design: Automated CPU Design with AI," a team of 18 authors led by Shuyao Cheng introduces a revolutionary approach to automatically designing a central processing unit (CPU) using AI techniques. CPUs are considered one of the most complex devices ever created by humans, making the successful application of AI in their design a remarkable achievement.

In their research, the team developed a method that allows machines to design a CPU solely based on external input-output observations, rather than relying on formal program code. The AI approach generates the circuit logic of the CPU design using a graph structure called Binary Speculation Diagram (BSD), ensuring both accuracy and efficiency. To demonstrate the capability of their method, the researchers explored an unprecedentedly large search space of 10 to the power of 10 to the power of 540 possibilities, which is considered the largest of all machine-designed objects to date.

After only five hours of computation, the team's approach successfully generated an industrial-scale RISC-V CPU, which was able to run the Linux operating system and perform comparably to the human-designed Intel 80486SX CPU. This breakthrough not only significantly reduces the design cycle in the semiconductor industry but also has the potential to reform it by enabling machines to learn the von Neumann architecture autonomously.

The research paper, totaling 28 pages, was submitted to the arXiv preprint server under the category of Artificial Intelligence (cs.AI) and Hardware Architecture (cs.AR). The authors provide extensive technical details and analysis of their methodology, making it a valuable resource for researchers in the field. This groundbreaking work represents a significant leap forward in machine design, opening up new possibilities for AI systems to tackle increasingly complex tasks in the future.

### It's 2023 and memory overwrite bugs are not just a thing theyre still number one

#### [Submission URL](https://www.theregister.com/2023/06/29/cwe_top_25_2023/) | 111 points | by [LinuxBender](https://news.ycombinator.com/user?id=LinuxBender) | [50 comments](https://news.ycombinator.com/item?id=36562727)

Memory overwrite bugs continue to be the most dangerous type of software bug, according to MITRE. These bugs, also known as out-of-bounds write bugs, are responsible for 70 vulnerabilities on the US government's list of known vulnerabilities that are under active attack. Out-of-bounds write bugs occur when software or hardware alters memory it shouldn't, causing unexpected changes or crashes. Exploit code can trigger these bugs to take control of the software. MITRE recommends using memory-safe languages like Rust to prevent these bugs. Cross-site scripting bugs and SQL injection flaws are the second and third most dangerous bugs, respectively. CISA has added eight more vulnerabilities to its Known Exploited Vulnerabilities Catalog, including flaws in D-Link and Samsung devices. The list of the Top 25 most dangerous software weaknesses for 2023 remains the same as last year. MITRE will publish reports to help organizations effectively use the Top 25 list.

### The open-source AI boom is built on Big Tech’s handouts. How long will it last?

#### [Submission URL](https://www.technologyreview.com/2023/05/12/1072950/open-source-ai-google-openai-eleuther-meta/) | 32 points | by [AnhTho_FR](https://news.ycombinator.com/user?id=AnhTho_FR) | [27 comments](https://news.ycombinator.com/item?id=36560473)

The rise of open-source large language models is threatening the dominance of Big Tech in the field of artificial intelligence (AI), according to a leaked memo by a senior engineer at Google. These freely available alternatives to Google's Bard or OpenAI's ChatGPT offer researchers and app developers the ability to study, modify, and build upon them. While this increased accessibility has driven innovation and democratized AI, it also poses risks. Many of these models rely on the work of big firms like Meta AI and OpenAI, which could choose to restrict access in the future. Closing down access would not only stifle the open-source community but also consolidate AI breakthroughs in the hands of the largest AI labs. However, some argue that opening up code for a limited period can drive innovation while still protecting the company's interests. The future of AI development and usage hangs in the balance as the industry grapples with the implications of open-source models.

The discussion on the Hacker News submission revolves around the rise of open-source large language models and its impact on the dominance of big tech companies in the AI field. Some commenters express skepticism about the benefits of open-source projects, stating that companies like Google, Meta, and Microsoft do not benefit from open sourcing their projects. However, others argue that open-source models offer alternatives and drive innovation. There is a debate about the control of AI technology, with some expressing concerns about big tech companies gaining a monopoly. However, others argue that big tech companies do not rely solely on open-source projects and have internal versions that are not shared with the world. The discussion also touches on the importance of open-source software in the AI industry, with examples such as PyTorch and TensorFlow being mentioned. Additionally, there is a discussion about the role of open source in the survival and growth of big tech companies. Some commenters bring up historical examples, highlighting the impact of open-source software in the past, such as the case of Sun and Linux. Overall, the discussion delves into the benefits and limitations of open-source models and their potential implications for the future of AI development.

---

## AI Submissions for Sat Jul 01 2023 {{ 'date': '2023-07-01T17:09:24.801Z' }}

### Vector support in PostgreSQL services to power AI-enabled applications

#### [Submission URL](https://cloud.google.com/blog/products/databases/announcing-vector-support-in-postgresql-services-to-power-ai-enabled-applications) | 70 points | by [srameshc](https://news.ycombinator.com/user?id=srameshc) | [21 comments](https://news.ycombinator.com/item?id=36551936)

Google Cloud Databases has announced the addition of vector support in PostgreSQL services. This new feature allows developers to store and efficiently query vectors in Cloud SQL for PostgreSQL and AlloyDB for PostgreSQL, enabling the use of generative AI in applications. With vector support, developers can store and index vector embeddings generated by large language models (LLMs) and perform similarity searches. This can be useful in various applications, such as providing product recommendations based on user preferences or simulating long-term memory in chatbot conversations. The integration of vector support in PostgreSQL services provides an easy and familiar way for developers to leverage AI capabilities in their applications. Additionally, the Cloud SQL and AlloyDB databases offer enterprise-grade features and tight integration with operational data, making it easier to create AI-enabled experiences that utilize real-time transactional data. The vector support can be combined with Vertex AI services, such as pre-trained models and custom model integration, to further enhance AI capabilities in applications.

### Workers with less experience gain the most from generative AI

#### [Submission URL](https://mitsloan.mit.edu/ideas-made-to-matter/workers-less-experience-gain-most-generative-ai) | 147 points | by [diskmuncher](https://news.ycombinator.com/user?id=diskmuncher) | [109 comments](https://news.ycombinator.com/item?id=36553987)

In a new study, researchers from MIT and Stanford University have found that generative artificial intelligence (AI) can significantly benefit workers with limited experience. The study focused on contact center agents who had access to a conversational AI assistant. The researchers discovered that these agents saw a 14% boost in productivity, with the largest gains observed among new or low-skilled workers. The generative AI technology helped to upskill the workers rather than replace them. This finding highlights the potential for generative AI to decrease inequality in productivity, providing opportunities for less-experienced workers to improve at their jobs more quickly. The study also revealed that the use of generative AI led to efficiency gains, with workers experiencing an increase in the number of customer chats resolved per hour, improved customer sentiment, and fewer requests to speak to a manager. Overall, the research suggests that generative AI can have a positive impact on the workforce, particularly for those with limited experience.

The discussion on this submission revolves around the claim that generative AI can significantly benefit workers with limited experience. Some users express skepticism about the specific claims of a 10x or 100x improvement in productivity, suggesting that a 5-10% improvement seems more reasonable. Others point out that while generative AI can be helpful for tasks like searching for information or writing small scripts, it may not be as effective for more complex programming tasks that require a deeper understanding of systems and coding. Some users share their experiences with generative AI models like GPT-4, noting that they have been helpful in generating SQL queries and providing detailed explanations. Overall, the discussion focuses on the potential limitations and benefits of generative AI in the workforce.

### AMD's AI chips could match Nvidia's offerings, software firm says

#### [Submission URL](https://www.reuters.com/technology/amds-ai-chips-could-match-nvidias-offerings-software-firm-says-2023-06-30/) | 40 points | by [dbcooper](https://news.ycombinator.com/user?id=dbcooper) | [9 comments](https://news.ycombinator.com/item?id=36549392)

AI chips from Advanced Micro Devices (AMD) are showing promise as a strong challenger to Nvidia's dominant position in the market, according to a report by AI software firm MosaicML. The report states that AMD's chips are currently about 80% as fast as Nvidia's, with a future path to matching their performance. This comes at a time when tech companies are looking for alternatives to Nvidia due to a shortage of its chips. MosaicML conducted a test comparing AMD's MI250 chip to Nvidia's A100, and found that AMD's chip was able to achieve 80% of the performance of Nvidia's, thanks to recent software updates and improvements. MosaicML believes that further software updates from AMD will help its chip match the performance of Nvidia's flagship chip. This report highlights the growing competition in the AI chip market and the potential for AMD to gain market share.

The discussion revolves around different aspects of AMD's AI chips and their competition with Nvidia. Some users express skepticism about AMD's software support, noting issues with previous GPU software and drivers. There are also mentions of AMD's slow community support and concerns about the company's ability to address software problems. One user believes that AMD's success with its Zen architecture has made people forget about the company's previous troubles. Another user mentions that developers are experimenting with rented virtual machines and are not using AMD's Radeon cards. Support for AMD's RDNA2 graphics cards and the performance metric in graphics competition are also discussed. There is mention of Nvidia's decision to implement CUDA support and a comparison to AMD's GPU software. The challenges for AMD are seen as brand name recognition and software support. One user emphasizes that performance is a significant metric, while another highlights the importance of driver support and software for desktop and casual gamers.