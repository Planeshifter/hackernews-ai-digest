import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Tue Dec 26 2023 {{ 'date': '2023-12-26T17:09:21.134Z' }}

### Show HN: I made a GPU VRAM calculator for transformer-based models

#### [Submission URL](https://vram.asmirnov.xyz/) | 128 points | by [furiousteabag](https://news.ycombinator.com/user?id=furiousteabag) | [35 comments](https://news.ycombinator.com/item?id=38774026)

The VRAM Estimator is a handy tool that allows you to estimate the memory usage of transformer-based models on GPUs. Whether you're running inference or training, this tool can give you insights into how much VRAM your model will consume.
To make use of the estimator, you'll need to input various parameters such as precision, optimizer, sequence length, batch size, number of GPUs, model parameters, number of layers, vocab size, hidden size, number of attention heads, intermediate size, and number of key-value heads. The tool then provides you with an estimation of the VRAM usage.
For example, if you input the parameters and run the estimator, it might tell you that the total VRAM usage is 27836 MiB. It breaks down the usage into different components such as CUDA kernels, parameters, activations, gradients, first moments, and output tensors, giving you a detailed understanding of how each component contributes to the overall VRAM usage.
It's important to note that these estimates might not be 100% precise, but they provide a good understanding of the VRAM requirements based on the current understanding of the topic. If you want a more thorough explanation and access to the calculation code, you can check out the detailed post and the source repository. And if you spot any issues or have suggestions, you can reach out to the developer via email or create an issue/PR in the repository.
So if you're working with transformer-based models and need to estimate VRAM usage, give the VRAM Estimator a try and optimize your GPU memory usage. Cheers!

The discussion on this submission revolves around several topics related to GPU VRAM, transformer-based models, and optimization techniques. Here are some notable points from the discussion:
1. One user mentions that consumer-grade GPUs like Nvidia's 3090 and 4090 have high VRAM capacity (24GB) but are more affordable compared to enterprise-grade GPUs used for training large models like gpt-2 and gpt-13.
2. Another user suggests that Nvidia's workstation cards, such as the RTX 6000 with 48GB VRAM and RTX 5000 with 32GB VRAM, offer more RAM than consumer cards and are suitable for distributed training on multiple GPUs.
3. There is a discussion about power consumption, where one user suggests using multiple consumer-grade cards in a workstation setup as an alternative to a single high-end GPU. Another user shares their experience with water cooling and a dedicated power source to mitigate thermal and power constraints.
4. The discussion briefly touches on GPU pricing and availability, with one user mentioning that the prices of Nvidia 3090 GPUs have dropped in Europe due to the downturn in cryptocurrency mining.
5. A user mentions an upcoming M2 Ultra Mac Studio with 192GB RAM, and others express interest or share their experiences with various GPU configurations and performance optimizations.
6. Some users discuss the performance impact of using lower precision models (such as 16-bit) for inference and the efficiency of quantized models in terms of memory usage.
7. There is a mention of the poor functioning of some links related to GPU modeling and training techniques, and users express their preference for Python-based libraries and ease of use.

Overall, the discussion provides insights into GPU VRAM requirements, cost optimization strategies, and the trade-offs involved in selecting different GPUs and precision modes for transformer-based models.

### DreamCoder: Growing Generalizable, Interpretable Knowledge

#### [Submission URL](https://arxiv.org/abs/2006.08381) | 80 points | by [fbodz](https://news.ycombinator.com/user?id=fbodz) | [7 comments](https://news.ycombinator.com/item?id=38767815)

A group of researchers has developed a system called DreamCoder that learns to solve problems by writing programs. The system creates programming languages and neural networks to guide the search for programs within these languages. Using a "wake-sleep" learning algorithm, the system extends the language with new symbolic abstractions and trains the neural network on imagined and replayed problems. DreamCoder is capable of solving both classic inductive programming tasks and creative tasks such as drawing pictures and building scenes. It has even rediscovered concepts such as modern functional programming, vector algebra, and classical physics. The system's concepts are built compositionally from those learned earlier, resulting in interpretable and transferrable symbolic representations. DreamCoder demonstrates the potential of growing generalizable and interpretable knowledge through wake-sleep Bayesian program learning.

The discussion surrounding the submission revolves around several different topics. 
One user, "nmsk," expresses their opinion that building hierarchical abstractions is essential for true artificial intelligence that can go beyond complete tasks. They believe that DreamCoder demonstrates the potential of hierarchical abstractions and has the ability to create complex descriptions of tasks.
In response to this, user "rmrm" questions the existence of hierarchical coding in AI systems, suggesting that it is difficult to achieve due to the limitations of current programming languages and libraries.
User "Nevermark" shares their belief that coding models should aim to make humans more capable and that AI abstractions can be useful. They also mention that simpler tasks are easier to tackle, while more complex tasks are more competitively challenging.
Another user, "mrkrsn," adds that DreamCoder is an AI system that creates abstractions.
In a separate comment, user "mdnl" shares their disappointment that some supplementary materials related to DreamCoder are not accessible, but they provide links to GitHub repositories.
User "ndnfrth" simply states "2020," without providing further context or explanation.

Lastly, user "yrprfct" asks about the exploration-exploitation pattern in DreamCoder's wake-sleep algorithm, comparing it to deep reinforcement learning.

### Burger King turns hangovers into discounts with facial recognition

#### [Submission URL](https://www.marketingdive.com/news/burger-king-hangover-facial-recognition-app-campaign/703187/) | 19 points | by [rntn](https://news.ycombinator.com/user?id=rntn) | [13 comments](https://news.ycombinator.com/item?id=38775400)

Burger King in Brazil has launched a campaign called "Hangover Whopper" that provides discounts on the brand's sandwiches to hungover consumers. The campaign uses facial recognition technology to scan a consumer's face and detect the degree of their hangover, offering a corresponding combo coupon for the Whopper Jr. Double, Whopper, or Whopper Double. The selfies taken during the process can also be shared on social media. Burger King has a history of using creative mobile technology campaigns to drive engagement and increase traffic to its app and restaurants. This campaign, created in partnership with agency DM9, runs through January 2nd and is available only for BK Delivery.

The discussion surrounding Burger King's "Hangover Whopper" campaign on Hacker News covers a range of topics. 
One user points out that Burger King has faced challenges in Russia and Ukraine, with hundreds of its major international franchises having to leave these countries. Another user shares a personal anecdote about their experience with Burger King's breakfast promotion. They recall being disappointed after walking into Burger King for a breakfast burger only to be told they had to wait until 9 am to order one. Eventually, they were offered a 2-for-1 happy hour deal.
The discussion then moves towards the marketing tactics of Burger King, with one user sarcastically commenting on the "new ways" they find to exploit consumers. Another user feels that the campaign is not realistic and points out that facial recognition technology is not always accurate.
Further comments criticize the campaign for incentivizing people to drink excessively and then consume fast food. One user mentions that this campaign is not the best idea considering it runs until January 2nd, right after New Year's Day, which is known for its hangovers. Another user jokingly suggests not serving people who are hungover.
Lastly, there are a few unrelated comments; one user mentions that they are in Brazil, where the campaign is being run, and another user simply states, "I'm flying to Brazil."

### OpenAI Pissed Off Developers by Phasing Out Plugins for GPTs

#### [Submission URL](https://gizmodo.com/openai-pissed-off-developers-by-phasing-out-plugins-for-1851124124) | 49 points | by [rntn](https://news.ycombinator.com/user?id=rntn) | [9 comments](https://news.ycombinator.com/item?id=38773147)

OpenAI's recent email to plugin developers has caused some controversy. The company asked developers to switch from building plugins to building GPTs, which has upset many developers who prefer the functionality of plugins. Although OpenAI claims that plugins are not going away yet, they anticipate that most developers will eventually move to GPTs once the GPT Store goes live. The developer community feels that plugins are crucial for the development and progress of AI applications. OpenAI's move towards more consumer-friendly GPTs may ostracize the developer community, which is essential for building good GPTs. This is not the first time OpenAI has upset developers, as an earlier update to ChatGPT rendered many developers' products useless. OpenAI's focus on GPTs may be due to security concerns surrounding plugins. However, there are limitations with GPTs that make developers less enthusiastic about building them. OpenAI's approach seems to prioritize appealing to everyday consumers rather than developers, similar to Apple's approach of offering cleaner products with fewer ports. However, OpenAI needs developers to build useful AI applications to succeed.

Discussion Summary:
- One commenter mentioned that OpenAI's recent decision to migrate from plugins to GPTs has caused frustration among developers. They highlighted the importance of plugins for AI applications and expressed concerns about the limitations of GPTs.
- Another commenter pointed out that OpenAI's terms of service for plugins were not very clear, and some developers were unsure about what was allowed. They suggested that OpenAI should have encouraged custom system prompts and sandboxing with GPTs to improve functionality.
- A user mentioned that the sentiment toward plugins has been negative in recent months due to issues with latency and discovery. They argued that OpenAI's focus on the API and functional calls with GPTs may be a more efficient approach.
- One commenter noted that they have heard little about developers liking plugins and have seen many posts from companies praising OpenAI's decision. They speculate that developers may not be the target audience for OpenAI's products.
- Another user disagreed, stating that GPTs do not offer the same functionality as plugins, such as custom instructions and file handling. They argued that plugins could work alongside GPTs.
- One commenter explained that the reason OpenAI created GPTs was to generate synthetic data for training models, which leads to better quality results. They provided an estimate of the amount of data used and its impact on training.
- A user suggested that custom system prompts and sandboxing should be considered as external APIs and not be built externally.
- Lastly, a commenter expressed their dislike for negativity in the discussion.

---

## AI Submissions for Mon Dec 25 2023 {{ 'date': '2023-12-25T17:10:17.622Z' }}

### Evolving Reservoirs for Meta Reinforcement Learning

#### [Submission URL](https://arxiv.org/abs/2312.06695) | 54 points | by [PaulHoule](https://news.ycombinator.com/user?id=PaulHoule) | [4 comments](https://news.ycombinator.com/item?id=38766381)

Researchers have proposed a computational model for studying the mechanism of adaptation in animals. The model, called Evolving Reservoirs for Meta Reinforcement Learning, combines evolution and development to enable lifetime learning in artificial agents. At the evolutionary scale, reservoirs, a type of recurrent neural networks, are evolved with optimized hyperparameters that control macro-level properties such as memory and dynamics. These evolved reservoirs are then used at the developmental scale to facilitate the learning of behavioral policies through reinforcement learning. The researchers demonstrated the effectiveness of this approach in diverse challenging tasks, including solving tasks with partial observability, generating oscillatory dynamics for locomotion tasks, and facilitating the generalization of learned behaviors to new tasks. The results highlight the potential of combining reservoirs and reinforcement learning for improving learning capabilities in artificial agents.

The discussion on this submission seems to be quite divided. 
One user with the username "p1esk" questions the need for using randomly initialized transformer blocks as reservoirs in the researched computational model, suggesting that it may not be necessary. 
Another user with the username "dy" expresses difficulty in understanding the paper and criticizes the unclear objectives and experiments presented. 
However, a user with the username "smstv" provides a comprehensive explanation of how the computational model works by combining evolutionary and developmental approaches. They also draw an interesting comparison with Monarch butterflies' multi-generational migration, highlighting the potential of the model in learning and adaptation. 
In response to "smstv's" comment, another user questions if external triggers, such as the presence of specific patterns in the reservoir's genome, could enhance contextual understanding and faster learning. The user also suggests that this approach could be beneficial for GPT-style models. 
Additionally, "smstv" discusses the training of Ferret, a multimodal large language model capable of understanding spatial references with high accuracy. They highlight the model's ability to comprehend spatially complex tasks. 
Lastly, a user shares a link to an image gallery titled "The DHS Department of Entangled Agent Technologies Humanity," which appears to be related to the topic but is not further discussed in the comments.

### 2023: The Year of AI

#### [Submission URL](https://journal.everypixel.com/2023-the-year-of-ai) | 118 points | by [talboren](https://news.ycombinator.com/user?id=talboren) | [70 comments](https://news.ycombinator.com/item?id=38765027)

2023 was a transformative year for AI, with various advancements in image generation, video generation, and text generation. Adobe Firefly and Generative Fill democratized AI, allowing users to create diverse visual content. Midjourney and DALL·E 3 improved image generation capabilities, while Stability AI and HeyGen paved the way for generative video creation. Runway's Gen-2 model enabled easy video generation, and Meta's Pixel Codec Avatars brought us closer to photorealistic telepresence. In text generation, Google's Bard and Gemini added human-like emotion and sentiment to chatbots, and xAI's Grok showcased rebelliousness and real-time knowledge. OpenAI's GPT-4 now handles image input, generates captions, and supports real-time web browsing. Mistral AI challenged GPT-4 with Mistral 7B and Mixtral 8x7B, emphasizing an open technology approach. These advancements mark an intermediate stage towards Artificial General Intelligence (AGI), setting the stage for even more powerful innovations in the future.

The discussion on this submission covers a range of topics related to AI and the mentioned advancements. Some users discuss technical aspects, such as the command line options and compatibility of different AI models. Others raise concerns about privacy and the transmission of data to external servers. Some users express their appreciation for the progress in AI, while others caution against the hype and emphasize the need for careful evaluation of models. There is also a discussion about different model architectures and their effectiveness in various applications. Overall, the discussion provides different perspectives and insights into the advancements and implications of AI technology.

### Matter, set to fix smart home standards in 2023, stumbled in the real market

#### [Submission URL](https://arstechnica.com/gadgets/2023/12/matter-was-more-of-a-nice-smart-home-concept-than-useful-reality-in-2023/) | 128 points | by [LinuxBender](https://news.ycombinator.com/user?id=LinuxBender) | [154 comments](https://news.ycombinator.com/item?id=38763743)

Matter, a smart home standard aimed at reducing fragmentation and improving device compatibility, has been met with mixed reviews. While it promises to make owning a smart home easier and more secure, critics have found that setting up Matter devices and making them work across different home systems can be challenging. Jennifer Pattison Tuohy from The Verge expressed frustration with Matter devices, stating that after a year of testing, she has not been able to get a single Matter-based device to work reliably in her home. Even popular devices like Philips Hue lights have become less usable since being moved to Matter. Additionally, many Matter-compatible devices still require users to download the manufacturer's specific app for full functionality. While the potential of Matter is promising, it seems there is still work to be done to improve its performance and ease of use.

The discussion on Hacker News about Matter, the smart home standard, touches on several points. One user mentions that recurring revenue from closed systems and subscription-based business models could be a problem. Another user praises the support for Home Assistant, which allows direct integration with various sensors and platforms like Google Home and Apple HomeKit. Some users express interest in AirGradient and Air Things Wave+ as alternative devices. The discussion also addresses the confusion surrounding Matter as a standard and the concerns about data collection and privacy in smart homes. Overall, there is a mix of skepticism, praise for certain platforms, and a desire for more user-friendly and open-source solutions.

---

## AI Submissions for Sun Dec 24 2023 {{ 'date': '2023-12-24T17:09:45.751Z' }}

### "Attention", "Transformers", in Neural Network "Large Language Models"

#### [Submission URL](http://bactra.org/notebooks/nn-attention-and-transformers.html) | 251 points | by [macleginn](https://news.ycombinator.com/user?id=macleginn) | [62 comments](https://news.ycombinator.com/item?id=38756888)

In this submission, the author expresses their frustration with the literature on "Attention" and "Transformers" in Neural Network "Large Language Models". They admit that they find it difficult to understand and wrap their head around, but acknowledge the need to learn about it to stay relevant in the field of AI. The author discusses their thoughts on different concepts related to attention, kernel smoothing, and the use of matrix algebra in these models. They also criticize the use of the term "attention" in the context of neural networks, suggesting that it is not truly representative of human attention. The author acknowledges that their opinions may contain errors and that they are open to learning from others.

The discussion on this submission revolves around various aspects of attention in neural network models. Some users provide additional resources and papers to further understand the concept, while others express their frustration with the literature surrounding attention and the lack of clear explanations. One user mentions the importance of understanding the background papers on attention, while another points out that some academic papers are poorly written and fail to provide clear definitions. The discussion also touches on the terminology and terminology used in the field, with some users suggesting that it can be confusing for newcomers. Overall, the discussion highlights the complexity and ongoing research in the field of attention in neural networks.

### Meilisearch expands search power with Arroy's filtered disk ANN

#### [Submission URL](https://blog.kerollmops.com/meilisearch-expands-search-power-with-arroy-s-filtered-disk-ann) | 71 points | by [Kerollmops](https://news.ycombinator.com/user?id=Kerollmops) | [23 comments](https://news.ycombinator.com/item?id=38752060)

Meilisearch, a full-text search engine, is expanding its search capabilities with Arroy's filtered disk. In order to support filtering and selecting subsets of documents, Meilisearch needed to develop a filtering system that can handle large datasets and provide scalability and responsiveness. One of their clients required the ability to filter through over 100 million YouTube video metadata and associated image embeddings to select videos released within specific time frames. 

Previously, Meilisearch ranked only the subset of filtered documents, but now with Arroy's filtered disk, they needed to implement a more efficient method. They were using an in-memory HNSW (Hierarchical Navigable Small World) data structure, but it was inefficient. They had to deserialize the whole data structure in memory, which took a lot of time and memory. Additionally, Meilisearch supports multiple vectors by document, so they needed to look up every vector they were iterating, which was not ideal.

With Arroy, Meilisearch was able to integrate the new vector store more quickly through mob programming, where the team codes together at the same time. Arroy provided a smarter search engine that can determine the exact number of results to return, even with filters to consider.

Arroy's internal data structure consists of item nodes (original vectors), normal nodes (split planes), and descendant nodes (tree leaves composed of item IDs). During a search, the algorithm pops the nearest item from a binary heap associated with an infinite distance. The modified Arroy stores the list of descendants in RoaringBitmaps, which are more efficient and allow for easier intersection with the filtered subset of documents.

One challenge Meilisearch faced was that vector IDs are not the same as document IDs, and Meilisearch only knows about the documents after executing the filters. Iterating on a lookup table to construct the final bitmap with all the vector IDs corresponding to the filtered documents would not be efficient, especially when many documents are part of the subset. To address this, Meilisearch used multiple indexes for efficient filtering.

Overall, the integration of Arroy in Meilisearch improved their search tool and highlighted the importance of collaboration and teamwork when facing technical challenges.

There are several discussions taking place in the comments section of the submission about Meilisearch's integration of Arroy's filtered disk. Here is a summary of the key points:

- One commenter suggests that Meilisearch should provide more information about their indexing process, and another mentions that they should discuss it on Discord. It is noted that Meilisearch has improved its indexing speed in version 16, and links to blog posts and tweets are shared to provide more details.
- Another commenter mentions that they recently re-indexed comments in Meilisearch using PHP and synchronized the data with MYSQL. They also mention upgrading to the latest version and mention the indexing speed.
- There is a discussion about the use of RoaringBitmaps in Meilisearch. One commenter finds it interesting as it offers benefits such as memory traversal, while another commenter raises questions about the design choices and suggests considering benchmarks.
- The expansion of Meilisearch's hybrid search capabilities is mentioned, and one commenter agrees that it's a great addition. They also suggest that Meilisearch should consider introducing replication clusters for high availability.
- A commenter shares their plan to use Markdown Astro and Seveltkit SSG to generate an index for searching with Meilisearch.
- A brief discussion about the disk space improvement in Meilisearch is mentioned.
- The topic of product placement in the blog post is brought up by a commenter, to which the original poster replies that Meilisearch works hard and mentions the various search features in different versions.
- There is a suggestion to use bundled libraries like SQLite or PostgreSQL for Meilisearch's storage needs, and another commenter mentions running a local bundled app.

Overall, the comments discuss various aspects of Meilisearch's integration of Arroy's filtered disk, including indexing speed, design choices, hybrid search capabilities, storage solutions, and future developments.

### AI Employe: Reliable Browser Agent, an Open-Source Alternative to Adept.ai

#### [Submission URL](https://aiemploye.com) | 7 points | by [vignesh_warar](https://news.ycombinator.com/user?id=vignesh_warar) | [7 comments](https://news.ycombinator.com/item?id=38753052)

AI Employe is a groundbreaking browser automation tool that aims to save you time and effort in your daily tasks. With its ability to automate email-to-CRM/ERP data transfers and perform tasks requiring human-like intelligence, such as understanding emails, receipts, and invoices, it promises to give you hours back every week. If you find yourself spending too much time logging your budget from emails to your expense tracker or manually entering details from receipts into your expense tracker, AI Employe can automate these processes for you. Its AI-powered capabilities can accurately interpret the information in your emails and receipts, and automatically log them into your chosen tracking system.

But AI Employe doesn't stop there. It also offers features like workflow creation, research assistance, and insights extraction. You can easily create workflows by outlining and demonstrating your tasks in the browser, just as you would explain them to a human. The tool records browser changes without capturing your screen, microphone, or camera, ensuring your privacy. Furthermore, AI Employe can help you gain insights from graphs, intricate tables, and image-based OCR (optical character recognition). With its ability to understand and interpret visual information, it can analyze and extract valuable insights, saving you even more time in data analysis.

AI Employe is open source, allowing users to contribute and improve the tool. If you're interested in trying it out, you can go to their website and sign up. They are currently offering a lifetime deal, so you don't want to miss out on the opportunity to reclaim your week with this innovative automation tool. And don't forget to star them on GitHub to show your support! The discussion on the Hacker News submission focuses on two main points: privacy concerns and the naming of the tool. One user, stvncr, raises privacy concerns by questioning how the data sent from the browser to the server is handled. The OP, vignesh_warar, responds by providing a link to the privacy page, where users can find information about how their data is protected. Another user, trtlycht, points out that the correct spelling of the tool should be "Employee" instead of "Employe." vignesh_warar acknowledges the mistake and provides a link to a source explaining the alternative spelling of the word. Additionally, grghll adds a link to an external website that provides information about the term "Employee." trtlycht thanks them for the link and mentions that the submitted URL should be updated.

Overall, the discussion mainly revolves around privacy concerns and clarification regarding the naming of the tool.