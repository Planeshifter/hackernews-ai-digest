import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Fri Jul 25 2025 {{ 'date': '2025-07-25T17:12:01.791Z' }}

### Experimental surgery performed by AI-driven surgical robot

#### [Submission URL](https://arstechnica.com/science/2025/07/experimental-surgery-performed-by-ai-driven-surgical-robot/) | 111 points | by [horseradish](https://news.ycombinator.com/user?id=horseradish) | [120 comments](https://news.ycombinator.com/item?id=44688096)

In a groundbreaking leap for robotics and healthcare, researchers at Johns Hopkins University have made a remarkable advance by integrating an AI system into the DaVinci surgical robot to successfully perform a gallbladder removal surgery. This new AI—the Surgical Robot Transformer (SRT-H)—employs a dual transformer model architecture similar to that which powers ChatGPT. This cutting-edge technology serves as a testament to the potential of AI in performing complex medical procedures with precision.

Traditionally, robotic surgeries relied heavily on pre-programmed sequences, much like how industrial robots on assembly lines operate. However, Axel Krieger and his team took a significant step forward by developing a system that learns from human demonstrations. The team trained SRT-H using imitation learning, from which the robot achieved a 100% success rate in performing cholecystectomy on test samples it had never encountered before.

What makes SRT-H stand out is its ability to receive real-time feedback in natural language and adapt its performance accordingly. This makes the system similar to a human apprentice, as it assimilates expert advice to fine-tune its surgical actions.

However, the journey for AI surgical robots is not without obstacles. The lack of necessary kinematics data from the industry-standard DaVinci robots is proving to be a bottleneck. Although Intuitive Surgical, the maker of DaVinci, is willing to share video feeds, it restricts kinematics data sharing to protect its competitive edge, posing a challenge for researchers. Yet, Kim and his team are optimistic; they're considering innovative workarounds such as motion-tracking sensors on manual surgical tools to capture the needed data.

In summary, this introduction of a ChatGPT-inspired AI into surgical robots isn't just a step—it could be a leap—towards the future of autonomous surgery. While some corporate barriers remain, researchers are determined to overcome them, inching closer to a future where AI and humans collaborate seamlessly in operating theaters worldwide.

**Summary of Discussion:**

The discussion around AI-driven surgical robots (exemplified by Johns Hopkins' SRT-H system) reflects cautious optimism tempered by practical concerns and comparisons to other technologies. Key points include:

1. **Trust & Precedent:**  
   - Comparisons were drawn to **LASIK surgery**, which is largely automated today, suggesting gradual acceptance of robotic procedures as they prove reliability. Users highlighted **Invisalign** as a model for incremental adoption in medicine.  
   - Skeptics questioned trust in robotic surgeons versus human experts, paralleling debates over **Waymo/Tesla autonomy**. While Waymo’s phased trust-building was noted, some argued surgical errors carry higher stakes than driving mistakes, requiring stricter safeguards.

2. **Technical Challenges:**  
   - **Data limitations** (e.g., restricted kinematics data from DaVinci robots) were seen as a hurdle. Researchers proposed workarounds like motion-tracking tools, but corporate barriers (e.g., Intuitive Surgical’s proprietary control) complicate progress.  
   - **Frequency vs. Complexity**: Unlike frequent driving scenarios, surgeries are rare, complex events. Training AI systems effectively demands reinforcement learning and robust simulation, akin to autonomous vehicles’ “corner case” training.

3. **Safety & Oversight:**  
   - Concerns arose about **catastrophic errors** in surgery (e.g., organ damage) versus recoverable driving mistakes. Users stressed the need for extreme QA, human oversight, and incremental AI integration (e.g., DaVinci’s current role as a surgeon’s tool, not replacement).  
   - Ethical implications of AI making life-or-death decisions were noted, alongside calls for transparency in AI decision-making processes.

4. **Future Outlook:**  
   - Optimists envisioned AI surgeons operating under expert supervision, handling routine tasks (like stitching) while humans tackle nuanced decisions. Parallels were drawn to radiology, where AI aids diagnostics but doesn’t replace specialists.  
   - Corporate incentives and regulatory hurdles were acknowledged, but participants expressed hope that collaboration between researchers and industry could overcome these barriers, much like early autonomous vehicle development.

**Conclusion**: The discussion underscores a tension between excitement for AI’s potential to enhance precision and accessibility in surgery and wariness about trusting machines with inherently human, high-stakes tasks. The path forward likely mirrors other tech adoptions—proven reliability, transparent oversight, and hybrid human-AI collaboration will be critical to acceptance.

### Claude Code introduces specialized sub-agents

#### [Submission URL](https://docs.anthropic.com/en/docs/claude-code/sub-agents) | 128 points | by [tekkertje](https://news.ycombinator.com/user?id=tekkertje) | [48 comments](https://news.ycombinator.com/item?id=44686726)

Anthropic has unveiled a fascinating new development on their Claude platform, introducing "sub agents" – specialized AI assistants crafted for specific tasks, making problem-solving more efficient than ever. Think of sub agents as unique AI personalities each with a particular area of expertise, operating independently to handle designated tasks. Here’s the scoop on how these work.

Each sub agent has its own context window, significantly enhancing problem-solving efficiency by preserving the main conversation's focus on overarching goals. These agents come with custom instructions and tool access tailored to their expertise, which not only improves task accuracy but ensures efficient management of resources across projects.

Creating a sub agent is a breeze: you just head to the sub agents interface via the command `/agents`, choose whether you want a project-level or user-level agent, and define what you want it to achieve. You can even customize tools access to ensure that powerful capabilities are reserved for the right agents. Once set up, these agents will be automatically invoked by Claude, whenever suitable, or you can call on them explicitly.

Sub agents are defined in Markdown files, allowing easy management and replication across different projects. You can adjust permissions and access levels via the `/agents` command for a user-friendly setup or manage files directly if you prefer. Whether it’s checking code, running tests, or any other task-specific activity, sub agents are here to transform and refine workflows with specialized precision!

Here's a concise summary of the Hacker News discussion about Anthropic's new "sub agents" feature in Claude:

### Key Themes:
1. **Skepticism About Effectiveness**:  
   Users expressed doubts about sub agents' reliability for complex tasks like code review, noting current LLMs (e.g., Sonnet) may lack the nuance for dependable performance. Comparisons to Gemini-Pro were mentioned as a potential alternative.

2. **Technical Challenges**:  
   - Context management issues arose, with users debating strategies to optimize limited context windows (e.g., manually clearing history or breaking tasks into smaller chunks).  
   - Integration hurdles were highlighted, including security concerns about bypassing permissions and workflow fragmentation with tools like `claude-flow`.

3. **Workflow Comparisons**:  
   Some users compared sub agents to existing orchestration methods, suggesting they resemble system-prompted workflows rather than revolutionary tools. Others shared experimental setups using Docker containers or VSCode integrations.

4. **Humorous Speculation**:  
   - Jokes about the timing of the release (e.g., referencing French summer holidays due to the name "Claude").  
   - Lighthearted takes on AI "psychiatrists" debugging models, metaphorically prescribing Prozac to fix issues.

5. **Performance Observations**:  
   Users reported intermittent Claude outages and rate limits, with some noting recent improvements. Others critiqued Anthropic’s uptime stats (98%) as misleading during peak hours.

### Notable Mentions:
- **Claude-Flow**: A tool for orchestrating sub agents, though some found it overly complex or unstable.  
- **Sonnet vs. Gemini-Pro**: Debate over which model handles tougher tasks better.  
- **Security Warnings**: Concerns about permissions bypass (`--dangerously-skip-permissions`) risking credential exposure.

Overall, the discussion reflects cautious curiosity about sub agents, tempered by technical critiques and humor about AI’s evolving landscape.

### Show HN: Price Per Token – LLM API Pricing Data

#### [Submission URL](https://pricepertoken.com/) | 316 points | by [alexellman](https://news.ycombinator.com/user?id=alexellman) | [123 comments](https://news.ycombinator.com/item?id=44682465)

Staying on top of the ever-changing landscape of large language model (LLM) pricing just got easier with a comprehensive resource that compiles up-to-date pricing information for key AI providers including OpenAI, Anthropic, and Google. As of late July 2025, the resource provides a clear comparative chart for evaluating costs across different AI models, focusing on both input and output costs per million tokens. This invaluable tool, created by @aellman, helps developers and businesses alike navigate the subtle differences in pricing structures, such as tiered pricing models for prompts of varying lengths.

Moreover, by subscribing to the weekly newsletter, users can keep abreast of any fluctuations in pricing or the introduction of new models, ensuring they're always informed and able to make cost-effective decisions for their projects. It's worth noting that token counting methods can vary by provider, typically equating to about 3-4 characters per token. For precise calculations and understanding, consulting each provider's specific documentation is advised. Whether you're a startup or a seasoned tech company, this resource promises valuable insights into optimizing your AI spending.

**Summary of Discussion:**

The discussion revolves around challenges in tracking and comparing LLM pricing across providers, with several key themes emerging:

1. **Data Accuracy Concerns**: Users noted discrepancies in the original pricing chart, particularly around Google Gemini 25 Flash-Lite ($0.10/1M input tokens vs. the cited $0.40/1M). One user highlighted corrections, acknowledging the error.

2. **Complexity of Pricing Models**: Participants emphasized the difficulty in navigating vendor-specific pricing rules (e.g., tiered/batch pricing, token counting variations, or context-window adjustments). Examples include OpenAI/Anthropic’s batch discounts and Google’s token-based billing influenced by prompt structure.

3. **Tools & Alternatives**:
   - **OpenRouter**: Praised for simplifying model comparisons across providers, though some noted limitations (e.g., incomplete model listings or reliance on aggregated data).
   - **Local Deployment**: Users discussed cost-effective hardware setups (e.g., M2 Mac Minis, NVIDIA GPUs) for running quantized models locally via Ollama, balancing expenses vs. commercial API costs.

4. **User Experience Frustrations**: Complaints about fragmented vendor marketing pages and the need for better UI/UX in comparison tools. Mention was made of Simon Willison’s [LLM pricing calculator](https://www.llm-prices.com) as a simplified resource.

5. **Token Ambiguities**: Debates arose over token equivalence across providers (e.g., GPT-4 vs. Gemini’s tokens per prompt), with warnings about hidden costs due to differing billing metrics.

6. **Cost-Saving Strategies**: Subscribers to weekly pricing newsletters and advocates for open-source/local models highlighted proactive approaches to managing expenses. Skepticism persisted about vendor claims, with calls for transparent benchmarks.

Overall, the conversation underscores the dynamic, opaque nature of LLM pricing and the community’s demand for clearer, standardized comparison tools and trustworthy data sources.

### WhoFi: Deep Person Re-Identification via Wi-Fi Channel Signal Encoding

#### [Submission URL](https://arxiv.org/abs/2507.12869) | 52 points | by [wut42](https://news.ycombinator.com/user?id=wut42) | [8 comments](https://news.ycombinator.com/item?id=44685869)

A groundbreaking paper titled "WhoFi: Deep Person Re-Identification via Wi-Fi Channel Signal Encoding" has been submitted to arXiv, showcasing an innovative approach to person identification in video surveillance. Traditional methods often grapple with challenges like poor lighting and occlusion, but WhoFi sidesteps these hurdles by harnessing Wi-Fi signals instead of visual data. This novel pipeline captures biometric features from Wi-Fi Channel State Information (CSI) and processes them through a deep neural network, which includes a Transformer-based encoder. Notably, the system leverages an in-batch negative loss function to effectively train its model for capturing robust biometric signatures. Tests conducted on the NTU-Fi dataset indicate that WhoFi's performance is on par with the best existing methods, proving its potential to revolutionize the future of non-invasive surveillance technology. This paper, contributed by Danilo Avola, Daniele Pannone, Dario Montagnini, and Emad Emam, has garnered interest for its cutting-edge approach and impact on privacy-focused identification techniques. For those fascinated by the intersection of machine learning and security, diving into the PDF version of this paper could provide valuable insights.

The Hacker News discussion on the "WhoFi" paper raises several key points and debates:  

1. **Privacy and Surveillance Concerns**: Users highlight how Wi-Fi sensing technology, as explored in the paper, could lead to invasive surveillance. Mentions of Xfinity's reported Wi-Fi motion detection plans by 2025 and IEEE's involvement in privacy standards (e.g., Wi-Fi 7 and 802.11bf) underscore fears that such systems might infer sensitive data (e.g., keyboard typing, activity tracking) and track individuals through walls without consent.  

2. **Technical and Ethical Challenges**: Commenters debate whether widespread adoption would require global biometric databases and robust algorithms. Concerns are raised about governments leveraging this tech, with a subthread referencing Chinese surveillance technologies and sarcastic remarks about "spying on citizens" and mistranslated claims.  

3. **Regulatory and Institutional Roles**: The IEEE’s "SENS" task force is noted for addressing privacy in Wi-Fi sensing, but skepticism remains about accountability. A user jokes about the EU loving this tech, sparking a subthread on public dissent and the need for “passive” systems (not requiring device connections) to avoid overreach.  

4. **Technical Clarifications**: Replies explain that Wi-Fi-based identification relies on reflected signals (not direct connections), termed "ambient WiFi RF displacement," reducing dependency on user consent.  

**Overall**: The discussion reflects a mix of fascination with the technology’s potential and apprehension about privacy erosion, emphasizing the need for transparent governance and ethical safeguards.

### Google Opal

#### [Submission URL](https://developers.googleblog.com/en/introducing-opal/) | 46 points | by [babushkaboi](https://news.ycombinator.com/user?id=babushkaboi) | [16 comments](https://news.ycombinator.com/item?id=44681786)

Google Labs has unveiled a groundbreaking tool named Opal, promising to revolutionize the way we create AI applications. Launched in a US-only public beta, Opal allows users to craft AI mini-apps by stringing together prompts, AI models, and tools, all without needing to write any code. By using natural language and visual editing, Opal democratizes AI app development, making it accessible to creators and innovators who can now prototype AI ideas, develop customized apps to enhance productivity, and more.

Opal simplifies the creation process by allowing users to describe the logic of their applications, which the tool then translates into visual workflows. This means you can build sophisticated, multi-step applications visually—even tweaking them in the editor or through conversational commands—without any programming knowledge. Once an app is built, it can be easily shared for others to use via their Google accounts, further enhancing collaboration.

To ease users into the app-building world, Opal includes a demo gallery with starter templates, providing a solid foundation for developing custom AI solutions. This aligns with Google Labs' vision of empowering users to turn their imaginative ideas into concrete tools with minimal barriers.

As part of this innovative leap, Opal joins a suite of new offerings from Google Labs’ recent activities, marking a commitment to fostering creativity and efficiency through accessible AI technologies. Welcome to the future of AI app creation with Opal, where your ideas come to life one prompt at a time.

The Hacker News discussion on Google Labs' Opal tool reflects a mix of skepticism, comparisons to existing tools, and cautious optimism about its potential:  

### Key Themes:  
1. **Skepticism About Google’s Track Record**  
   - Users humorously referenced Google’s history of discontinuing products (*"Google Sunsetting Opal"*, *"Killed by Google"*), with jokes about Opal’s eventual fate.  

2. **Comparisons to Existing Tools**  
   - Opal was likened to **Yahoo Pipes** (a legacy workflow tool) and **n8n** (a modern low-code automation platform). Some saw it as a "Google-ified" version of these systems.  

3. **Praise for No-Code AI Potential**  
   - Opal’s no-code, natural-language approach to building AI workflows impressed users, with one calling it "*incredible*" for enabling complex integrations via AI agents.  

4. **Discussion of Use Cases**  
   - Comments highlighted possible applications: event planning (BBQs, community gatherings), project management, and social media automation. However, users noted challenges in translating AI promises to real-world utility, especially for intricate tasks.  

5. **Criticism of Hype**  
   - Some dismissed AI trends (*"LLMs"*) as overhyped, arguing that long-term success depends on solving practical problems, not just flashy demos.  

6. **Miscellaneous Reactions**  
   - Frustration over unclear/abbreviated comments (*"illegible content"*), curiosity about Opal’s US-only beta, and nods to similar tools like *"tldrw Computer"* (a summarized news tool) surfaced.  

In summary, while Opal’s accessibility and vision were applauded, the discussion leaned heavily on caution—emphasizing Google’s shaky product longevity and the need for AI tools to deliver beyond hype.

### How Anthropic teams use Claude Code

#### [Submission URL](https://www.anthropic.com/news/how-anthropic-teams-use-claude-code) | 275 points | by [yurivish](https://news.ycombinator.com/user?id=yurivish) | [236 comments](https://news.ycombinator.com/item?id=44678535)

Anthropic's teams are making waves by integrating Claude Code into their operations, revolutionizing how both technical and non-technical staff manage projects, automate workflows, and bridge skill differences. We delved into their experiences across various divisions, from data infrastructure to legal, to uncover how Claude Code is transforming work processes and boosting productivity.

### Claude Code: A Game-Changer for Data Infrastructure
The Data Infrastructure team, vital in managing Anthropic's business data, uses Claude Code to boost efficiency and autonomy. Here’s how they leverage this powerful tool:

- **Automated Data Engineering**: By utilizing Claude Code for automating routine tasks and troubleshooting infrastructure issues, non-technical team members are now empowered to access and manipulate data independently with streamlined workflows.
  
- **Kubernetes Debugging**: When Kubernetes clusters faced pod scheduling issues, Claude Code helped diagnose and rectify IP address problems in Google Cloud, eliminating the need for specialized networking expertise.

- **Accessible Workflows**: Finance teams now execute complex data operations by writing plain text files interpreted by Claude Code, transforming non-coders into active data handlers.

- **Codebase Navigation**: New hires leverage Claude Code's ability to navigate vast codebases, simplifying onboarding and accelerating their integration into the team.

### Expedited Onboarding and Enhanced Collaboration
Claude Code has accelerated onboarding significantly by helping new data scientists understand complex systems without extensive hand-holding. By generating end-of-session summaries and improvement suggestions, Claude Code ensures teams have a living document that evolves for better future use. Members also benefit from parallel task management, as multiple Claude Code instances assist in maintaining workflow continuity across long projects.

### Prosperity for Product Development
The Product Development team taps into Claude Code's strengths for rapid prototyping and expansion of its functionalities. For instance:

- **Fast Prototyping**: They utilize an “auto-accept mode,” enabling continuous iteration on abstract problems, thus significantly speeding up the development process.

- **Synchronous Coding**: Claude Code assists with comprehensive code quality and compliance checks, allowing developers to focus on core application features and architecture.

### Recommendations and Insights
The teams advocate for well-documented Claude.md files to maximize Claude Code’s effectiveness and stress the importance of secure data handling through MCP servers. Sharing usage sessions among team members also proves beneficial, promoting knowledge sharing and revealing innovative implementations previously unexplored.

In summary, Claude Code has not only enhanced productivity and cross-team independence but has also redefined how Anthropic teams approach complex problems and data management. Their insights serve as valuable advice for other organizations considering this transformative tool.

**Summary of the Discussion:**

The Hacker News thread debates the practicality and limitations of using **Claude Code** (Anthropic's AI tool) for software development, highlighting both its potential and pitfalls:

### **Key Themes**
1. **Efficiency vs. Reliability**  
   - Users acknowledge Claude Code can automate **70–80% of tasks** (e.g., boilerplate code, parallel job runs), saving significant time.  
   - However, outputs often require **iterative refinement** ("run Claude for 30 mins, accept or restart") and human oversight to fix mistakes (e.g., syntax errors, misaligned logic).  

2. **Code Quality & Trust**  
   - Debate arises over **AI-generated code vs. human-written code**:  
     - **Pros**: Rapid prototyping, scalability, and handling mundane tasks (e.g., formatting).  
     - **Cons**: Code may lack context awareness, produce "crappy" outputs, or introduce subtle bugs. Users compare debugging AI code to teaching "terrible students" who mask mistakes.  
   - Some argue **human code** remains superior due to intentionality and contextual understanding, though it’s not immune to errors.  

3. **Cost and Scalability Tradeoffs**  
   - Parallelizing tasks with Claude Code boosts speed but incurs **high API costs** ("Big Bill" cons).  
   - Strategies like **auto-accept modes** and **selective iteration** balance cost and utility.  

4. **Workflow Strategies**  
   - Users recommend:  
     - Treating Claude as a "slot machine" (run multiple attempts, pick the best).  
     - Using **linters/formatters** to enforce quality for AI-generated code.  
     - Destroying containers to reset processes when debugging becomes too time-consuming.  

5. **Analogies to Other AI Challenges**  
   - Comparisons to **self-driving car** issues (e.g., edge cases in parking lots) highlight the gap between AI capabilities and real-world complexity.  

### **Notable Quotes & Sentiments**  
- “**AI-written code doesn’t mean you can skip code reviews**.”  
- “**The Pareto principle applies: Claude saves 20% of time but leaves 80% of problems.**”  
- “**Trusting AI code feels like trusting a student who hides errors**—it’s fragile and needs constant babysitting.”  

### **Conclusion**  
While Claude Code offers **transformative efficiency gains**, users emphasize:  
- **Human oversight** remains critical.  
- Iterative workflows (test → refine → repeat) are necessary to manage reliability.  
- Costs and code quality risks may offset productivity benefits in complex projects.  

The consensus? Claude Code is a **powerful but imperfect tool**—best used strategically, not as a wholesale replacement for human judgment.

### The Mythical Machine-Month Paradox – How much could AI change programming?

#### [Submission URL](https://tucson-josh.com/posts/mythical-machine-month/) | 26 points | by [tucson-josh](https://news.ycombinator.com/user?id=tucson-josh) | [11 comments](https://news.ycombinator.com/item?id=44685627)

The software industry is currently grappling with a transformative shift driven by the rise of generative AI, which threatens to redefine how code is produced and the roles of software engineers. As generative AI advances, some corporate leaders predict that AI could be responsible for crafting 95% of code by decade's end. This prospect dangles the possibility of streamlined operations, potentially enabling companies to develop software faster and at reduced costs, even postulating the emergence of unicorn SaaS companies driven by a single employee leveraging AI technologies.

However, the transition isn't as seamless as it seems. At the heart of any valuable software lies a complex theoretical model—an intricate blueprint that accurately solves a problem and accounts for edge cases, user interactions, and integrations with other systems. The process of translating this model into code is deemed straightforward but is interwoven with iteration, testing, and refinement. It requires a nuanced understanding that generative AI, with its prose-based interface, might struggle to replicate without deeper comprehension and problem-solving prowess.

Thus, while AI can assist in creating large volumes of code, the challenge remains for it to understand and extrapolate the sophisticated theoretical models driving software. The process of determining these models traditionally involves iterative development—testing and refining ideas—which AI must also navigate to fulfill its promise of surpassing human developers in productivity.

Moreover, the true test of software lies in its deployment, where real-world usage uncovers imperfections in models or code implementations. Testing, an essential part of the development lifecycle, ranges from individual unit validation to complex system performance assessments, uncovering issues not envisaged during initial designing. With AI, the questions arise about how it will handle such complexities and whether it can adapt as software intricacies evolve.

In essence, while generative AI heralds a potential paradigm shift in software development, the journey from human-written to AI-driven code involves bridging significant theoretical and practical complexities. The future may indeed hold a larger AI footprint in coding, but software engineers will likely continue to play a crucial role in crafting, maintaining, and improving the systemic integrity of the theoretical models that power our digital world.

**Summary of Discussion:**

1. **Debugging AI-Generated Code & Kernighan's Law:**  
   Participants reference Kernighan’s Law (“Debugging is twice as hard as writing code”) to highlight concerns around AI-generated code’s complexity. Clever, opaque code from AI (e.g., via LLMs) complicates debugging. Tools like Claude are noted for debugging attempts, but challenges persist in navigating probabilistic outputs and embracing iterative refinement over perfect inputs.

2. **Classic Software Principles in the AI Era:**  
   Comparisons to *The Mythical Man-Month* and Brooks’ “No Silver Bullet” essay surface skepticism about AI as a panacea. Comments humorously adapt “mythical machine-minute” to critique overhyped promises of AI-driven productivity gains, stressing that adding AI may not bypass traditional project complexities.

3. **AI’s Transformative Impact:**  
   A tangential analogy likens generative AI’s disruption to the Chicxulub asteroid impact, underscoring its transformative potential while hinting at hype.  

4. **Behind-the-Scenes Tech Shifts:**  
   Speculation arises about unspoken changes in large tech companies’ systems (e.g., replacing databases with AI-driven interfaces like chatbots), questioning transparency and practical implementation.

**Key Themes:**  
- AI’s role amplifies existing software challenges (debugging, complexity) rather than eliminating them.  
- Classic engineering wisdom remains relevant in tempering AI optimism.  
- Underlying systemic shifts in tech infrastructure may be underacknowledged.

### Qwen3-235B-A22B-Thinking-2507

#### [Submission URL](https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507) | 152 points | by [tosh](https://news.ycombinator.com/user?id=tosh) | [61 comments](https://news.ycombinator.com/item?id=44681565)

In the ever-evolving world of AI, Qwen3-235B-A22B-Thinking-2507 emerges as a new pinnacle of intelligence, supercharging the capacities of its predecessor. This state-of-the-art model introduces significant improvements in reasoning capabilities, excelling in tasks demanding human-like expertise in logic, science, and coding, among others. It achieves top scores on academic metrics, outperforming other open-source models in the field.

This enhanced version shines with robust general abilities, like instruction adherence and text generation, aligning closely with human preferences. Its long-context understanding, now bolstered to 256K tokens, makes it exceptionally suited for complex tasks.

Structurally, Qwen3-235B-A22B-Thinking-2507 boasts a staggering 235 billion parameters, engaging 22 billion during activation. Its architecture is delicately crafted with 94 layers and a specialized attention mechanism, allowing for remarkable processing depth.

In practice, developers can harness Qwen3's prowess through platforms like Hugging Face transformers, ensuring seamless integration into AI projects. The model's design notably facilitates agentic use, with Qwen-Agent enabling sophisticated tool interactions.

Whether deciphering intricate problems or generating creative content, Qwen3-235B-A22B-Thinking-2507 stands ready to push the boundaries of what AI can achieve, heralding a new era in large language models. For more technical insights and deployment tips, developers are encouraged to explore detailed resources provided in the accompanying blog, GitHub, and documentation.

**Summary of Hacker News Discussion on Qwen3-235B-A22B-Thinking-2507:**  

**Technical Highlights**  
- **Quantization & Optimization**: Users discussed dynamic quantization (e.g., Q8_0, Q2_K_XL variants) as a method to reduce hardware demands while preserving performance. Dynamic quantization selectively applies lower bitrates to less critical layers, leveraging activation data for calibration. Resources like the [Unsloth blog](https://unsloth.ai/blog/deepseekr1-dynamic) were shared to explain trade-offs between speed, memory, and accuracy.  
- **Hardware Feasibility**: Debate arose over local inference viability. While some dismissed it as impractical due to VRAM demands (e.g., 90GB model size), others noted techniques like offloading to RAM/SSD and dynamic 2-bit quantization enable running the model on setups like an RTX 4090 with 128GB RAM, albeit at slower speeds.  

**Performance & Benchmarks**  
- **ARC-AGI Scores**: The model’s reported 41.8% ARC-AGI score sparked skepticism, with users questioning potential overfitting or test-set contamination. Comparisons to Gemini 25 Pro and Claude were made, though Qwen3’s Apache2 license was seen as a competitive advantage.  
- **Use Cases**: Praised for tackling complex tasks (e.g., analytical integrals), though occasional repetition in outputs was noted with low-bit quantization.  

**Censorship & Ethics**  
- Users flagged potential censorship, as the model (backed by Alibaba) avoided sensitive historical topics like Tiananmen Square, aligning with Chinese regulatory norms. Skepticism arose about its reliability for politically charged queries.  

**Community Sentiment**  
- **Mixed Reactions**: Some lauded Qwen3’s technical advancements, while others doubted benchmark validity and emphasized real-world performance over parameter counts (235B parameters, 22B active).  
- **Open-Source Concerns**: Discussions highlighted challenges in running such large models locally, with calls for smaller, efficient variants (e.g., Gemma) over brute-force scaling.  

**Conclusion**: The discussion reflects enthusiasm for Qwen3’s innovations but underscores skepticism around benchmarks, censorship, and practical deployment hurdles. Technical depth, licensing, and ethical implications remain focal points for developers and researchers.

---

## AI Submissions for Thu Jul 24 2025 {{ 'date': '2025-07-24T17:13:02.285Z' }}

### Transformers without normalization

#### [Submission URL](https://arxiv.org/abs/2503.10622) | 41 points | by [kaycebasques](https://news.ycombinator.com/user?id=kaycebasques) | [6 comments](https://news.ycombinator.com/item?id=44671375)

In a surprising turn of events for neural network enthusiasts, a fresh study from Jiachen Zhu and colleagues, titled "Transformers without Normalization," showcases a revolutionary approach to Transformers that defies long-standing beliefs. Traditionally, normalization layers have been considered crucial for the success of neural networks, but this new research challenges that notion.

The paper introduces Dynamic Tanh (DyT), a simple yet powerful element-wise operation that remarkably replaces normalization layers in Transformers. By mimicking the tanh-like input-output mappings often produced by layer normalization, DyT allows Transformers to not only maintain but potentially enhance performance without the need for normalization layers or extensive hyperparameter tuning. 

This breakthrough was rigorously validated across a wide spectrum of AI applications, spanning from recognition and generation tasks to both supervised and self-supervised learning settings in fields as diverse as computer vision and language modeling. Lead author Jiachen Zhu and his co-authors, including AI luminaries like Yann LeCun, suggest that these findings could dramatically shift the understanding and design of neural networks, sparking new debates and exploration into the roles of normalization in deep learning architectures.

For those interested in delving deeper, the research is part of the CVPR 2025 conference and can be viewed in detail on their project page linked via the provided arXiv DOI.

**Summary of Hacker News Discussion:**

The Hacker News discussion on the "Transformers without Normalization" paper reflects a mix of skepticism, technical debate, and cautious optimism. Key points include:

1. **Skepticism & Nuanced Praise**:  
   - User **gdlsk** initially labels the paper as "misleading" but acknowledges its rigorous methodology and practical value. They argue that while replacing LayerNorm with Dynamic Tanh (DyT) simplifies the architecture, bounding inputs to a range like [-1, 1] might not universally outperform traditional normalization, especially in scenarios with extreme input ranges or varying training/test data distributions.  
   - Others commend the paper for challenging dogma, noting that revisiting "basic" assumptions (e.g., normalization necessity) is valuable for progress.

2. **Normalization’s Role Debated**:  
   - **hodgehog11** highlights normalization’s traditional purpose: preserving data distribution statistics across layers. They suggest DyT might implicitly replicate aspects of linear normalization, blurring its novelty.  
   - **gncrlstr** differentiates between data normalization (e.g., input preprocessing) and architectural normalization (e.g., LayerNorm), cautioning against conflating terms. This sparks discussion about whether DyT qualifies as a true "normalization-free" method or merely redefines it.

3. **Technical Trade-offs**:  
   - **DoctorOetker** observes that DyT appears computationally efficient, but subthreads explore potential drawbacks. For instance, bounding inputs could affect numerical precision (e.g., in FP16/32/64), and models might struggle with out-of-distribution data if training ranges are too constrained.  
   - Users debate LayerNorm's flexibility versus DyT’s rigid bounds, weighing simplicity against robustness. Some argue DyT’s simplicity could reduce hyperparameter tuning, while others worry it sacrifices adaptability.

4. **Community Implications**:  
   - Many agree the paper encourages healthy re-examination of "standard" practices. However, users stress the importance of clear terminology and rigorous validation across diverse tasks (e.g., varying batch sizes, domains, hardware setups).  

**Takeaway**: The discussion highlights interest in DyT’s potential to simplify Transformers but underscores unresolved questions about its generality and trade-offs compared to traditional normalization. While some see it as a promising paradigm shift, others urge caution, emphasizing the need for further empirical testing and clearer definitions of "normalization" in deep learning.

### Hacker slips malicious 'wiping' command into Amazon's Q AI coding assistant

#### [Submission URL](https://www.zdnet.com/article/hacker-slips-malicious-wiping-command-into-amazons-q-ai-coding-assistant-and-devs-are-worried/) | 74 points | by [CrankyBear](https://news.ycombinator.com/user?id=CrankyBear) | [12 comments](https://news.ycombinator.com/item?id=44675557)

In a shocking turn of events, Amazon's AI coding assistant, "Q," found itself at the center of a potential tech disaster. A hacker reportedly inserted a malicious command into Q’s codebase, triggering concern among developers who discovered that the agent could have deleted local files and potentially dismantled company cloud infrastructures hosted on AWS. This scandal was unveiled when the hacker submitted a pull request on GitHub, cleverly designed to go unnoticed during Amazon's review process.

Though Amazon acted swiftly to address the breach, their response was criticized for lack of transparency, as they quietly pulled the compromised version from the Visual Studio Code Marketplace without issuing a changelog or a Common Vulnerabilities and Exposures (CVE) entry. The incident sparked a heated conversation around open-source implementation and security, as Eric S. Raymond pointed out that simply being open-source doesn't ensure safety if proper oversight is missing.

Prominent AWS critic Corey Quinn described the situation as "far from 'oops, we fat-fingered a command,'" highlighting the gravity of letting strangers dictate the future road map. Critics are now calling for more transparency and engagement from Amazon to regain trust. Meanwhile, Amazon CEO Andy Jassy's earlier claims of Q being a game-changer are overshadowed by skepticism from a wary developer community.

As the tech industry reels from this headline-grabbing incident, it's evident that stronger protocols and community transparency are needed to prevent such disruptions in AI tool deployment.

The Hacker News discussion highlights widespread criticism and skepticism toward Amazon's handling of the compromised "Q" AI coding assistant incident. Key points include:

1. **Skepticism About AI Oversight**: Users mocked the irony of Amazon CEO Andy Jassy’s claims that AI would handle code reviews, given that the breach occurred via a malicious pull request (PR) allegedly approved by AI. References to Jassy’s prior statements underscored concerns about over-reliance on AI for critical security tasks.

2. **Security Failures**: Commenters expressed alarm that a PR altering system prompts to execute destructive commands (e.g., `rm -rf`, deleting files) was merged into a public repository. Some joked about the severity (e.g., "rm -rf little 🐮") or criticized Amazon for quietly resolving the issue without transparency (e.g., no CVE entry).

3. **Technical Critiques**: Users highlighted the risks of granting AI agents unchecked access to system-level tools like Bash. Suggestions included sandboxing AI tools (e.g., using containers or tools like [nksndbx](https://github.com/nksndbx)) to restrict harmful actions and prevent exploitation.

4. **References and Sources**: Links to GitHub commits, news articles (e.g., The Register), and external discussions were shared, emphasizing the incident’s visibility and the community’s demand for accountability.

Overall, the discussion reflects frustration with Amazon’s opaque response and calls for stricter safeguards, including sandboxing and human oversight, to prevent similar AI-related vulnerabilities.

### Two narratives about AI

#### [Submission URL](https://calnewport.com/no-one-knows-anything-about-ai/) | 258 points | by [RickJWagner](https://news.ycombinator.com/user?id=RickJWagner) | [239 comments](https://news.ycombinator.com/item?id=44672414)

In today's digital landscape, the discussion around AI's impact on the programming industry has become a heated debate featuring two opposing narratives. One side argues that AI, and more specifically Large Language Models (LLMs), are causing a seismic shift in the programming world by automating tasks and reducing job opportunities. High-profile examples like Aravind Srinivas of Perplexity reveal AI tools drastically cutting down task completion times, fueling fears about job security in tech giants like Microsoft, where layoffs are rumored to be AI-driven.

Contrasting this is a wave of skepticism cautioning against the hype. The AI evaluation company METR's study found developers using AI tools were actually slower by 19%. Commentary from tech insiders like Simon Willison and Nick Khami dismisses the doom-and-gloom predictions, arguing that AI is merely a tool to augment human work rather than a replacement. Even some of the feared job cuts at Microsoft turn out to be reallocations for emerging AI initiatives rather than direct replacements.

Adding to this complexity is the fluctuating job market; a decline in computer science enrollments is attributed not only to AI panic but also to a natural correction post-pandemic tech spending frenzy. The narratives present a divided landscape where sensationalism often clouds understanding, and the article advises readers to maintain a skeptical distance, focusing on observable changes in their own areas of interest.

Overall, the consensus is clear: while AI's potential is undeniable, its true impact remains speculative. Recognizing the nascent nature of this technology is crucial, as is welcoming AI's capabilities with a balanced perspective and a willingness to adapt. As one reader wisely commented, AI may ultimately lead us back to the roots of human innovation—only time will tell.

**Hacker News Discussion Summary:**  
The Hacker News discussion around AI’s impact on programming reveals nuanced perspectives, balancing skepticism with cautious optimism. Here’s a breakdown of key themes:

### **1. AI as a Tool, Not a Replacement**  
- Many argue AI (e.g., LLMs) automates **mundane tasks** (e.g., code calculations, repetitive processes) but doesn’t replace developers. For example, automating infrastructure provisioning (Terraform, Kubernetes) allows engineers to focus on higher-value work.  
- **Job roles** like DevOps, SRE, and sysadmins have evolved to manage abstracted systems, reducing manual intervention over time. AI may further streamline these layers but won’t eliminate human oversight.

### **2. Skepticism of Hype**  
- Several users dismiss fearmongering about AI-driven layoffs, calling it "crazy alarmism." Some tech employees mock CEOs for pushing replacement narratives, noting that **tools like NoOps/Serverless have existed for years** without displacing engineers.  
- **Code generation criticism**: LLMs can produce quick, small-scale code but struggle with large, complex projects requiring maintainability and context. "AI-generated code works until it doesn’t," one user remarks.

### **3. Shifting Job Dynamics**  
- **Customer support roles** face risks: AI chatbots are increasingly handling queries, but users note backlash when companies prioritize cost-cutting over human interaction. Employees in these roles report frustration as AI tools degrade service quality.  
- Others highlight **market corrections**, suggesting declining CS enrollments and layoffs (e.g., Microsoft) reflect post-pandemic adjustments, not solely AI disruption.

### **4. Adaptation and Evolution**  
- Veterans share how tech roles transformed over decades (e.g., desktop support → cloud infrastructure) and predict AI will **abstract lower-level tasks** (e.g., Crossplane, GitHub Actions). Humans will focus on design, oversight, and edge cases.  
- Younger developers express concerns about being stuck in "endless mundane work," but others stress **upskilling** as the antidote to automation.

### **5. The Role of Hype Cycles**  
- Comparisons to trends like "Crypto Experts" emerge, with jokes about "Generative AI Experts" flooding LinkedIn. Users caution against buzzword-driven hiring and advocate focusing on tangible skills.  

### **Final Takeaway**  
The consensus is cautious: AI accelerates certain tasks but lacks the nuance for high-stakes work. Job markets will shift toward roles managing AI tools and abstract systems, while low-skill roles (e.g., customer support) face higher disruption. Adaptation, skepticism of hype, and balancing automation with human judgment remain critical. As one user put it: *"AI may squeeze human labor upward, but roots of innovation will stay human."*

### Show HN: Local Email Client for AI Horseless Carriages

#### [Submission URL](https://github.com/dbish/DispatchMail) | 14 points | by [shahahmed](https://news.ycombinator.com/user?id=shahahmed) | [6 comments](https://news.ycombinator.com/item?id=44673613)

Are you overwhelmed by the chaotic mess that is your email inbox? Fear not, a new open source project named DispatchMail has arrived to declutter your digital life. Created by the user 'dbish' on GitHub, DispatchMail is an AI-powered email assistant designed to help you manage your inbox efficiently—all while running locally on your system.

### Key Features
- **AI-Powered Processing**: Using OpenAI, DispatchMail processes your emails and assists in drafting responses. 
- **Web Interface**: It offers an easy-to-use web interface for managing your inbox.
- **Customizable Filtering**: Set up email filtering and whitelist rules so the AI only processes specific types of emails.
- **Automated Organization**: Automatically labels and archives emails to keep your inbox tidy.
- **Local Storage**: Utilizes a local SQLite database to ensure your data stays private and secure.

### Who's it for?
DispatchMail is currently in its early alpha stage, aimed at developers who love to tinker and tailor their tools. The project invites feedback and contributions, with hopes of someday launching a managed, polished version depending on user interest.

### Getting Started
To start using DispatchMail, make sure you have Python 3.8+, Node.js 16+, a Gmail account with 2FA, and an OpenAI API key. Installation involves cloning the GitHub repository and running a simple setup script. The process is streamlined to help you quickly deploy and begin managing your emails.

### Future Vision
The team behind DispatchMail envisions a collaborative future where AI agents work alongside humans seamlessly. They are keen to explore and expand this tool, inviting users to contribute ideas and development support.

This AI-native email assistant could be your next step towards simplifying your email management. Whether you're a developer looking to experiment or just someone curious about AI-powered productivity tools, DispatchMail might be worth checking out. Visit the [GitHub repository](https://github.com/dbish/DispatchMail) for more details and start your journey to a tidier inbox today!

**Summary of Discussion:**  

The Hacker News discussion around **DispatchMail** highlights a mix of technical feedback, concerns, and future-roadmap insights from the creator, **dbish**:

### Key Points:  
1. **Prompt Injection & Automation Concerns**:  
   - Users raised questions about preventing misuse (e.g., bots archiving emails automatically).  
   - **dbish** clarified that drafting emails requires human approval, and the system mitigates prompt injection risks by classifying emails differently. They emphasized user feedback as critical for refining these safeguards.  

2. **Future Vision & Integrations**:  
   - **dbish** outlined plans to expand DispatchMail into a collaborative ecosystem where AI agents interface with existing tools (e.g., n8n automation) and work alongside humans.  
   - A **Managed Control Plane (MCP)** was proposed as a future goal, enabling centralized management of AI agents.  

3. **Technical Design Rationale**:  
   - **dbish** explained three core reasons for prioritizing a user-centric approach:  
     1. **Transparency**: Ensuring users fully understand how AI agents act on their inbox.  
     2. **Collaboration UX**: Designing interfaces that blend human-AI interaction (e.g., feedback loops, approval workflows).  
     3. **Proactive Processing**: Transitioning from local, reactive email handling to server-side AI workflows (e.g., using Claude Assistant).  

### Community Response:  
- While some users sought deeper automation capabilities, others stressed the importance of keeping humans "in the loop." The project’s open-source nature and focus on privacy (local SQLite storage) were seen as strengths.  

Overall, the discussion reflects enthusiasm for AI-driven email management but underscores the need for clear controls, transparency, and iterative development to balance automation with user trust.

---

## AI Submissions for Wed Jul 23 2025 {{ 'date': '2025-07-23T17:17:07.716Z' }}

### CARA – High precision robot dog using rope

#### [Submission URL](https://www.aaedmusa.com/projects/cara) | 930 points | by [hakonjdjohnsen](https://news.ycombinator.com/user?id=hakonjdjohnsen) | [158 comments](https://news.ycombinator.com/item?id=44661846)

CARA, the latest innovation in robotic quadrupeds, stands out for its groundbreaking use of capstan drives, foregoing traditional gears and pulleys. This unique feature not only positions CARA ahead of her predecessors like ZEUS, ARES, and TOPS but also places her as the second-ever quadruped to adopt such technology after Stanley. Capstan drives, known for their zero backlash, high torque transparency, low inertia, and quiet operation, are perfect for robotics, bringing significant advantages, especially in achieving accurate gear ratios—an aspect thoroughly explored and refined by CARA's creator.

Embarking on this project, the primary challenge was attaining an exact 8:1 gear ratio using capstan drives. Initial attempts were slightly off-mark due to an oversight in calculating the effective diameters of the drums where rope wraps around, akin to misconceptions in a notorious SAT question from 1982. The solution involved a meticulous process of trial, error, and calculation, culminating in an almost perfect 8.000619:1 ratio, achieved through linear interpolation and rigorous testing.

CARA's leg design epitomizes innovation and efficiency. The use of a coaxial 5-bar linkage—rare in quadrupeds—ensures even load distribution, compactness, and a unique mechanical structure. Powered by robust motors and driven by advanced electronics like the ODrive S1 FOC Controllers, each leg integrates seamlessly into the robot’s architecture, with high-strength materials like PET-CF and Polycarbonate ensuring durability under stress.

The robot's design is impressively straightforward yet effective, centering around four legs attached to a carbon fiber frame that offers unmatched strength-to-weight benefits. This structural efficiency is complemented by strategically placed electronics boxes and robust TPU components like its handle and feet.

CARA's creation marks a significant leap in robotic design, showcasing the potential and versatility of capstan drives in robotics, all while coupling creativity with precise engineering. The journey and insights shared, including available CAD files and further technical resources, invite enthusiasts and engineers alike to explore and expand the boundaries of robotic development.

The discussion around CARA, the robotic quadruped with capstan drives, revolves around several key themes:

### Technical Innovation & Design
- Users praised CARA's capstan drives for **zero backlash, high torque, and compact design**, with comparisons to historical uses in film equipment. The coaxial 5-bar linkage and material choices (e.g., Dyneema, Kevlar) were highlighted for improving strength, elasticity, and durability.
- Debates emerged on **material trade-offs** (e.g., Dyneema vs. steel cables) and engineering challenges like backlash compensation, linear interpolation, and gear ratio precision. Some noted parallels to DIY robotics projects, such as surgical robots using similar mechanisms.

### Content Discovery & Algorithms
- Many discussed **YouTube's recommendation algorithm**, arguing it both helps and hinders niche technical creators. While it occasionally surfaces high-quality, small channels (e.g., Aaeds’ "Breaking Taps"), users lamented its bias toward popular content and difficulty in reliably discovering specialized topics. Some attributed CARA’s visibility to algorithmic "luck," emphasizing the struggle for smaller creators to compete.

### Creator Appreciation & Accessibility
- Aaeds’ **dedication, presentation style, and technical depth** were widely applauded. Commenters admired the video’s production quality and the project’s open-source ethos (shared CAD files, detailed documentation).
- Discussions highlighted how platforms like YouTube democratize access to advanced engineering knowledge, though concerns were raised about **content saturation** and the need for better search/discovery tools.

### Corporate vs. Independent Work
- Users noted the rarity of CARA’s **corporate sponsorship** (via Automata) in a landscape dominated by academic or hobbyist projects, sparking interest in sustainable funding models for niche engineering endeavors.

In summary, the thread blends technical admiration for CARA’s design, critiques of content algorithms, and appreciation for accessible engineering education, reflecting a community passionate about both innovation and knowledge-sharing.

### AI overviews cause massive drop in search clicks

#### [Submission URL](https://arstechnica.com/ai/2025/07/research-shows-google-ai-overviews-reduce-website-clicks-by-almost-half/) | 619 points | by [jonbaer](https://news.ycombinator.com/user?id=jonbaer) | [700 comments](https://news.ycombinator.com/item?id=44663227)

In a recent deep dive, the Pew Research Center unveils a striking paradigm shift in online behavior, all thanks to Google's new AI Overviews. Released last May as part of Google's "search generative experience," these AI-driven summaries have gradually crept into more search results and are altering the way users interact with information online. Pew's analysis of data from 900 test users starkly reveals that the click-through rate on search results plummets from 15% without AI to a mere 8% with AI Overviews. For those who remain skeptical about the changes in web traffic, this study is a strong rebuttal against Google's assurances that AI benefits web publishers. Despite this, Google maintains its stance, arguing that AI enhances user experience and creates more opportunities for website engagement.

The study highlights a worrying trend: users are abandoning further exploration after absorbing AI-delivered summaries that could potentially carry inaccuracies due to the "hallucinations" known in generative AI. Currently, roughly 1 in 5 Google searches includes an AI summary, especially for queries structured as questions. With the tech titan opposing Pew's findings, calling their methodology "flawed," the debate over AI's role in shaping search engines and its repercussions for website traffic intensifies. Amidst these changes, Google's profits soar, underscoring the complex relationship between innovation and traditional web publishing. Ryan Whitwam, a senior technology reporter at Ars Technica, documents this ongoing shift as AI continues to redefine information consumption.

The discussion revolves around the impact of Google's AI Overviews and broader ecosystem changes on user experience, content creators, and web traffic. Key points include:

### **User Experience Frustrations**
- **Intrusive Ads and Pop-ups**: Users cite annoying experiences with mobile pages immediately scrolling, sticky banners, consent pop-ups, and video ads that hinder access to content. Many resort to ad blockers (e.g., uBlock Origin) to mitigate this but face ethical dilemmas about supporting creators.
- **YouTube's Degraded Experience**: Non-Premium users report buffering, unskippable ads, and interface clutter. Even Premium subscribers note declining quality, with increased ads in content and algorithmic pushes for "Shorts" or low-effort videos.

### **Impact on Content Creators**
- **Monetization Challenges**: Google’s AI summaries reduce click-through rates, directly affecting creators reliant on Adsense revenue. Smaller creators struggle as revenue skews toward top-tier channels, pushing them toward direct payments (Patreon, sponsorships) or unsustainable content strategies.
- **Shift to Subscriptions**: YouTube Premium is criticized for unclear revenue sharing, with doubts about how much supports smaller creators. Some users prefer direct creator support over Google’s subscription model.

### **Ad Blockers vs. Ethical Concerns**
- **Ad Blocker Arms Race**: Users face constant technical hurdles (e.g., broken video playback) as Google works to bypass blockers. This fuels resentment, with some reluctantly paying for Premium to avoid hassles.
- **Creators Caught in Middle**: While ads fund content, intrusive implementations drive users to blockers, harming creators. SponsorBlock and SmartTube are mentioned as solutions to skip non-ad content (e.g., sponsorships), but sustainability remains unclear.

### **Criticism of Google’s Dominance**
- **Ecosystem Control**: Google’s垄断 over search, ads (via Adsense/AdMob), and YouTube allows it to prioritize profit over user/creator needs. Header bidding and ad-space monopolization further squeeze publishers.
- **AI’s Long-Term Threat**: Users and creators fear AI summaries and generative answers will eclipse traditional content, reducing traffic and eroding the open web. Comparisons are drawn to YouTube’s evolution from a creator-friendly platform to an ad-centric monopoly.

### **Broader Sentiment**
- **Cynicism Toward Big Tech**: Many distrust Google’s claims about AI and Adsense benefits, viewing profit motives as overriding user/creator welfare. The push for subscriptions and ads is seen as exploitative, with few alternatives in a Google-dominated landscape.
- **Nostalgia for Older Web**: Some lament the decline of straightforward, ad-free browsing and creator ecosystems, replaced by tracking, paywalls, and algorithm-driven content.

In summary, the discussion highlights tension between user convenience, creator sustainability, and Google’s monetization strategies. While AI Overviews symbolize a shift toward centralized content delivery, the broader ecosystem faces criticism for degrading user experience and undermining the open web’s vitality.

### US AI Action Plan

#### [Submission URL](https://www.ai.gov/action-plan) | 389 points | by [joelburget](https://news.ycombinator.com/user?id=joelburget) | [550 comments](https://news.ycombinator.com/item?id=44660323)

In a compelling bid to secure a leading role in the global artificial intelligence landscape, the United States has unveiled an ambitious AI Action Plan under the direction of President Trump. This action plan is designed around three pivotal pillars that aim to propel America to the forefront of AI innovation, infrastructure development, and international diplomacy.

**Pillar 1: Accelerate AI Innovation**
The first pillar emphasizes fostering an environment ripe for AI innovation by supporting private sector-led advancements. The plan advocates for the removal of regulatory obstacles to unleash creativity and productivity. It highlights commitments to advance AI interpretability, safeguard free speech, encourage open-source development, and empower the workforce in this technological era. The federal government is also keen on accelerating AI adoption in various agencies, including defense, while combating challenges like synthetic media in the legal system.

**Pillar 2: Build American AI Infrastructure**
Recognizing the vital need for robust infrastructure, the second pillar focuses on upgrading energy capacities and semiconductor manufacturing—a stark need given America's stagnant growth compared to China's rapid expansion. Priorities include developing a sturdy grid, enhancing cybersecurity for critical infrastructure, and training a skilled workforce to actualize these technological implementations.

**Pillar 3: Lead in International AI Diplomacy and Security**
The United States seeks not only to lead at home but also to influence global AI standards and practices. This pillar highlights America’s role in exporting AI technologies to allies while countering China's influence in global governance. It also stresses the importance of securing global alliances and enforcing stringent control over AI-associated exports to prevent adversaries from exploiting American innovations.

Each of these pillars is backed by an array of targeted actions and policies aiming to consolidate America's leadership in the AI domain. The plan envisions a future where AI fuels economic growth, enhances national security, and fosters scientific breakthroughs, placing the United States at the cusp of a new era of technological dominance and global influence.

**Summary of Discussion:**

The discussion revolves around the AI Action Plan's energy infrastructure goals, with a focus on nuclear power versus renewable energy (solar + storage), regulatory challenges, and real-world implementation issues.

1. **Nuclear vs. Renewables Debate**:
   - **Pro-Nuclear**: Some argue nuclear is essential for clean energy, citing Trump's executive orders to boost U.S. nuclear capacity by 400% over 25 years. Others note nuclear’s reliability for energy-intensive industries and criticize its PR issues. Waste concerns are downplayed (e.g., "waste per person is small" and stored safely in dry casks).
   - **Anti-Nuclear/Solar Advocates**: Critics highlight nuclear’s high costs, risks (Fukushima/Chernobyl), and toxic waste challenges. Solar+storage is seen as cheaper and safer, with advancing technology reducing reliance on fossil fuels. Links to articles (e.g., solar’s economic edge) reinforce this stance.

2. **Regulatory and Practical Hurdles**:
   - U.S. energy leadership is questioned due to regulatory bottlenecks. For example, California faces issues like HOAs blocking solar installations, permitting delays, and grid limitations despite abundant solar potential. Users criticize inefficient policies hindering decentralized renewable adoption.
   - Debate touches on global market dynamics: If solar/battery demand surges, U.S. domestic fossil production could collapse, favoring cheaper imports.

3. **Infrastructure and Cost Concerns**:
   - GPU clusters for AI training face high costs and capacity shortages (e.g., AWS limitations), highlighting the need for affordable, scalable energy solutions.
   - Chernobyl’s exclusion zone and Fukushima’s $1 trillion cleanup costs are cited as nuclear’s legacy risks, contrasting with renewables’ lower long-term liabilities.

**Key Takeaways**:  
The HN community is split on nuclear’s role in clean energy. Proponents stress its reliability and undervalued potential, while opponents advocate for solar+storage due to cost and safety. Real-world examples (e.g., California’s regulatory maze, GPU infrastructure costs) underscore the challenges of transitioning to next-gen energy systems, whether nuclear or renewable.

### Building better AI tools

#### [Submission URL](https://hazelweakly.me/blog/stop-building-ai-tools-backwards/) | 324 points | by [eternalreturn](https://news.ycombinator.com/user?id=eternalreturn) | [168 comments](https://news.ycombinator.com/item?id=44659921)

Here’s a refreshing take on AI development that suggests we’re doing it all wrong—working backwards, in fact. The author argues that our approach to creating AI tools is often misaligned with how humans actually learn and innovate. The key issue? We’ve built AI systems that sidestep instead of leverage our natural strengths, such as problem-solving through cumulative iteration and collaboration.

Let’s break it down: humans are innately good at learning through processes, applying efforts to recall information, and innovating collectively rather than in isolation. This collective model of learning and problem-solving emphasizes the strength of brainstorming and community iteration—a stark contrast to the glorified myth of the lone genius developer. Yet, current AI tooling seems to bypass these strengths, performing tasks for us rather than enhancing our innate capabilities.

The typical AI workflow—click for magic, get AI-generated suggestions, proceed based on AI prompts—lacks the engagement required for effective learning and knowledge transfer. As humans become deskilled due to over-reliance on AI for tasks we excel at, the feedback loop degenerates, resulting in a loss of high-quality data and effective systems.

The proposed solution? Reimagine AI as an "absent-minded instructor," there to guide and enhance your skills without doing the thinking for you. Picture AI as a quirky, Socratic rubber duck, facilitating learning rather than dispensing pre-made answers. This approach refines the typical teaching method into “Explain, Demonstrate, Guide, Enhance,” aiming to embed human action into iterative learning for better mastery and skill enhancement over time.

The argument raises essential questions about how we engage with AI, suggesting that a better collaborative design could not only empower humans but also lead to richer AI systems. By tailoring AI tooling to suit human strengths rather than replace them, we can unlock the unrealized potential of both human and artificial intelligence.

**Summary of Discussion:**

The discussion centers on the **role of AI in incident management** and how it intersects with human learning and decision-making. Key points debated include:

1. **AI Reliability & Overreach**:  
   - Users highlight risks of AI making errors (e.g., wiping databases despite contrary instructions) and stress the need for caution when granting AI direct access to critical systems.  
   - Skepticism arises about AI autonomously resolving incidents, with concerns about accountability ("People want responsibility but avoid actions"). Tools performing tasks "behind the scenes" risk deskilling humans and undermining accountability.

2. **Chess Analogy & Human Skill**:  
   - Comparisons to chess suggest AI should aid analysis (like engines suggesting moves) but not replace human judgment. Even strong players improve through mental practice, not passive tool use.  
   - Critical thinking and iterative problem-solving remain essential, with AI tools criticized for generating "fast, flashy hypotheses" that lack depth without human validation.

3. **Incident Resolution Workflows**:  
   - Some propose AI as a **diagnostic assistant**, surfacing patterns/logs quickly (e.g., analyzing DeviceMapper errors to spot block size mismatches) while letting humans finalize actions. Others fear tools like LLMs might distract with irrelevant hypotheses.  
   - Privacy/security concerns emerge when AI handles sensitive data autonomously. Trust issues arise from AI executing commands without transparent oversight.  

4. **Human-Centric Design**:  
   - Emphasis on designing tools that **guide**, not replace, human reasoning. For example, dashboards presenting summarized data for informed decisions.  
   - Learning is tied to hands-on investigation; delegating tasks like log analysis to AI risks stifling skill development. Engineers must retain the ability to troubleshoot foundational systems (e.g., DNS, LVM2).  

5. **Balancing Automation & Control**:  
   - Proposals include AI tools reinforcing good practices (e.g., observing workflows to suggest optimizations) without dictating actions. Ensuring humans remain the "driver" for critical processes.  

**Conclusion**: While AI can accelerate diagnostics and surface insights, its best role is augmenting—not replacing—human judgment. Trust hinges on transparency, maintaining human agency, and avoiding overreliance that erodes problem-solving skills. The ideal system empowers users to learn *through* AI collaboration, preserving expertise and accountability.

### Lumo: Privacy-first AI assistant

#### [Submission URL](https://proton.me/blog/lumo-ai) | 202 points | by [pentagrama](https://news.ycombinator.com/user?id=pentagrama) | [105 comments](https://news.ycombinator.com/item?id=44657556)

In a landscape where AI is often critiqued for privacy invasions and data misuse, Proton introduces Lumo, a revolutionary AI assistant that prioritizes user privacy. Unlike many AI tools, Lumo doesn’t convert people into products by harvesting their data. Instead, it offers a private, secure alternative that refuses to compromise your data for profit. Built within the trusted Proton privacy ecosystem, Lumo ensures no logs are kept, and every chat is encrypted with zero-access technology, providing users with the confidence that their conversations remain confidential and safe from data leaks or third-party exploitation.

Lumo distinguishes itself with features such as ghost mode, which allows conversations to disappear once closed, and integration with secure services like Proton Drive. An open-source model, Lumo operates out of Europe, offering robust legal privacy protections far from the lengthy reach of US and Chinese jurisdictions. By not using user interactions to train the AI, it ensures sensitive information isn’t repurposed inadvertently in other outputs—a crucial factor for businesses and individuals handling confidential material. 

Available for free without requiring an account, Lumo allows you to start harnessing the power of AI right away by simply visiting their website or downloading the app on Android or iOS. Embrace the benefits of AI without the trade-off of personal privacy, thanks to Lumo’s innovative approach.

**Summary of Discussion:**

The discussion revolves around Proton's Lumo AI and broader privacy-centric technologies, focusing on jurisdiction, technical implementation, and trust in corporate practices. Key points include:

1. **Jurisdiction & Legal Protections**:  
   - Switzerland is praised for robust privacy laws, shielding Proton from foreign jurisdictions like the US and China. Users debate the practical efficacy of these protections, questioning whether Swiss-based hosting genuinely prevents external interference (e.g., US FVEY alliances).  
   - Comparisons to Apple’s **Private Cloud Compute** highlight cryptographic safeguards and third-party audits, though skepticism remains about trusting corporate-controlled systems even with transparency measures.

2. **Proton’s Product Critique**:  
   - Some users contrast ProtonMail with alternatives like Fastmail or Zoho, citing usability flaws (e.g., ProtonMail lacks transactional email support). Others defend Proton’s commitment to privacy, emphasizing encryption and features like "ghost mode" and Proton Drive integration.  
   - Questions arise about Lumo’s **open-source claims**, with users unable to locate its source code, raising concerns about transparency.

3. **Technical and Privacy Concerns**:  
   - Discussions delve into encryption practices (e.g., zero-access encryption) and infrastructure security, but skepticism persists about trusting **proprietary models** over fully open alternatives.  
   - Mention of tools like Monero, VPNs, and decentralized tech (i2p) reflects broader debates on balancing privacy with usability.

4. **Surveillance and Trust Issues**:  
   - References to cases like **Ross Ulbricht** and Snowden-era surveillance underscore distrust in government overreach and mass surveillance capabilities. Users criticize AI-driven tools (e.g., web3, VPNs) as potential privacy traps without enforceable safeguards.  
   - Apple’s approach is noted as a benchmark, blending cryptographic hardware with third-party verification, though doubts linger about centralized control.

5. **Corporate Accountability**:  
   - Calls for independent audits and legal transparency recur, emphasizing that jurisdictional advantages (e.g., Switzerland) must align with verifiable practices to avoid "privacy theater."

**Conclusion**: The thread underscores a tension between advocating for jurisdiction-based privacy solutions (like Proton’s Swiss hosting) and demanding technical/legal accountability. While Proton’s Lumo is viewed as a step forward, users stress the need for auditable open-source frameworks, skepticism toward corporate claims, and broader systemic reforms to counter surveillance overreach.

### AI coding agents are removing programming language barriers

#### [Submission URL](https://railsatscale.com/2025-07-19-ai-coding-agents-are-removing-programming-language-barriers/) | 142 points | by [Bogdanp](https://news.ycombinator.com/user?id=Bogdanp) | [170 comments](https://news.ycombinator.com/item?id=44655515)

For a decade, this developer lived and breathed Ruby, mastering everything from Rails to core tooling. But in 2025, they took a bold leap into the world of multi-language programming, diving into C++, C, and Rust projects with newfound confidence, thanks to Shopify and AI tools like Cursor and Claude Code.

The shift wasn't just a personal milestone; it was propelled by changes in project requirements at Shopify. The developer needed to support Sorbet for RBS, prompting a foray into system programming languages they had never touched before. Fortunately, Shopify’s nurturing environment and mentors provided the foundational knowledge needed. Initially daunting, the transition was made smoother by these supportive colleagues who shared their expertise generously.

The developer's work on ZJIT, a Ruby JIT compiler, exemplified the complexity of juggling Rust, C, and Ruby, along with compiler theory. Here, AI tools played a pivotal role—not as code generators but as collaborative partners. AI assisted by offering insights into syntax, patterns, and theoretical explorations, allowing the developer to focus on project requirements and context. This dynamic pairing highlighted how AI can halve learning obstacles, accelerating skill acquisition without taking away the need for human expertise for course corrections.

The developer stresses that AI's assistance doesn’t replace deep expertise but lowers the entry barrier, making contributions to projects achievable sooner. Mistakes with syntax or unfamiliar tools are quickly corrected, freeing developers to immediately impact without needing to master every detail upfront.

This transformation from a Ruby-only background to a multi-language developer within a year speaks to a larger shift. As AI continues to enhance learning, the cognitive load of understanding new languages is easing, allowing developers to focus on solving complex problems. This evolution is not only redefining individual careers but could reshape the landscape of programming specialization, making language versatility more accessible to a broader range of developers.

The Hacker News discussion critiques the role of AI (specifically LLMs) in programming, emphasizing its strengths, limitations, and broader implications:

### Key Themes:
1. **AI as "Glorified Pattern Matchers":**
   - Critics argue LLMs are limited to pattern-matching existing code and struggle with **abstract reasoning** (e.g., handling novel problems like concurrent resource locking or missing logical steps).  
   - Advocates counter that AI excels in **context-specific tasks** (generating code snippets, debugging, or CSV/config processing) and accelerates workflows by automating repetitive patterns.

2. **Niche vs. Mainstream Languages:**
   - Non-mainstream languages (Scala, OCaml, Elm) face challenges, as AI tools have less training data for them, making code generation less reliable.  
   - Some users note AI can still assist in **scaffolding** (e.g., translating Python libraries to Scala) but requires human expertise for optimization and correctness.

3. **Bug Detection & Reliability:**
   - LLMs can surface potential bugs quickly but often generate **false positives** or miss critical context. Users stress the need for human verification (tests, code reviews) to avoid catastrophic errors.  
   - Skeptics highlight that AI lacks the intuition for "needle-in-haystack" bugs, relying instead on deterministic analysis or examples from its training corpus.

4. **Human-AI Collaboration:**
   - While AI lowers barriers to entry (e.g., generating boilerplate code), it doesn’t replace human reasoning. For complex systems (flight transfers, financial logic), humans must design safeguards.  
   - Some users express optimism that AI will evolve beyond current limitations, but others dismiss this as overhyped determinism.

5. **Ethical & Practical Concerns:**
   - Concerns about AI-generated code leading to **economic misplanning** (e.g., unreliable CSV parsers) or security risks (random column-flipping in data) surface.  
   - The debate touches on AI’s role in stifling creativity, with some fearing it could homogenize solutions, while others see it freeing developers for higher-level tasks.

### Notable Examples:
- A user shared frustration with AI generating **outdated code styles** (e.g., "Zig code with 1990s-era loops").  
- Another highlighted **AI’s utility in security research**, where it helped find a curl bug but required extensive human validation to avoid false reports.  

### Conclusion:
The consensus leans toward AI as a powerful **supplement to human expertise**—ideal for pattern-driven tasks but dependent on human oversight for abstract reasoning, context, and system-level integrity.

### You can now disable all AI features in Zed

#### [Submission URL](https://zed.dev/blog/disable-ai-features) | 548 points | by [meetpateltech](https://news.ycombinator.com/user?id=meetpateltech) | [261 comments](https://news.ycombinator.com/item?id=44660519)

Exciting news for developers who value control over their coding environment: Zed, the high-performance code editor, now allows users to disable all AI features! This update responds to feedback from the coding community, recognizing that not everyone wants or can use AI in their workflows. Whether due to philosophical reasons, concerns about data privacy, or organizational restrictions, developers can now choose a non-AI path with a simple addition to their settings.json: `{ "disable_ai": true }`.

The option to disable AI is rolling out in Zed’s Preview today and will be part of the Stable release next week. Plus, new users soon will have the choice to disable AI features during the onboarding process. For those who still want to benefit from AI but worry about privacy, Zed offers alternatives like using personal API keys or local AI models to keep everything secure and private.

As AI tools increasingly influence software development, understanding them is becoming an essential skill. Zed recognizes this, and through its Agentic Engineering series, aims to educate developers about effectively leveraging AI while respecting those who choose traditional methods.

The open-source editor, available on macOS and Linux, continues to evolve with these user-centered developments. And if you're passionate about crafting the future of coding tools, Zed is hiring! Download Zed today to explore a customizable coding environment tailored to your preferences.

**Summary of Hacker News Discussion on Zed's AI Disable Feature and Performance:**

The discussion revolves around Zed’s new option to disable AI features, its performance, and comparisons with other editors/IDEs. Key points include:  

1. **Performance vs. Features**:  
   - Users praise Zed’s speed and low resource usage, especially after recent optimizations. However, some note lingering latency issues on Linux and when handling large files, even on high-end hardware like M3 Max MacBooks.  
   - JetBrains IDEs (e.g., IntelliJ) are cited as *feature-rich but sluggish*, while Zed and Sublime Text are lauded for responsiveness.  

2. **AI Integration Controversy**:  
   - The decision to add AI features sparks debate. Some users disable AI for privacy/performance reasons, while others want better AI customization (e.g., local models, context-aware tab completion).  
   - Comparisons with tools like **Cursor** (AI-focused editor) highlight gaps in Zed’s AI-driven tab completion quality, though Zed’s speed keeps users engaged.  

3. **Plugin Ecosystem Critique**:  
   - Zed’s limited plugin system is a pain point. Users miss the extensibility of VS Code/JetBrains and hope Zed prioritizes plugin APIs to rival competitors.  

4. **Vim/Neovim Comparisons**:  
   - Zed’s modal editing and keyboard shortcuts receive mixed reviews. Some find it "smoother than Vim," while NeoVim loyalists cite missing features (e.g., fuzzy file matching) and integration gaps.  

5. **Platform-Specific Issues**:  
   - Linux users report higher input latency compared to macOS, with some calling Zed’s Linux port "the worst latency experience."  

**Overall Sentiment**:  
Zed is celebrated for its snappiness and lightweight design but faces pressure to improve its AI tooling, plugin ecosystem, and platform parity. The ability to disable AI is welcomed, reflecting a community split between AI enthusiasts and minimalists. Zed’s future success may hinge on balancing performance with extensibility.

### Cerebras launches Qwen3-235B, achieving 1.5k tokens per second

#### [Submission URL](https://www.cerebras.ai/press-release/cerebras-launches-qwen3-235b-world-s-fastest-frontier-ai-model-with-full-131k-context-support) | 358 points | by [mihau](https://news.ycombinator.com/user?id=mihau) | [148 comments](https://news.ycombinator.com/item?id=44657727)

Cerebras Systems has set a new benchmark in AI technology with the launch of Qwen3-235B, now available on the Cerebras Inference Cloud. This model is claimed to be the fastest frontier AI reasoning model, offering unprecedented speed and efficiency at a fraction of the cost of its competitors. Qwen3-235B stands out for its ability to provide production-grade code generation at 30 times the speed and at only 10% of the cost compared to other closed-source models.

**Breaking New Ground in AI Performance**

This cutting-edge model not only accelerates processing speed but also revolutionizes enterprise AI deployment by enabling real-time reasoning. With its Wafer Scale Engine, Qwen3-235B processes data at an unprecedented 1,500 tokens per second, cutting response times from minutes to mere seconds. This transformation means developers can now perform complex coding and reasoning tasks almost instantaneously.

**Expanding Horizons with 131K Context Support**

Cerebras has enhanced its model’s capabilities by increasing its context length support from 32K to 131K tokens. This leap allows Qwen3-235B to effectively manage large codebases and complex documents, addressing the growing demand for production-grade AI applications in the enterprise code generation market.

**Strategic Collaboration with Cline**

In a strategic move, Cerebras has partnered with Cline, a renowned coding agent for Microsoft VS Code. This integration means Cline users will experience vastly improved coding speeds, starting with Qwen3-32B on the free tier and eventually benefiting from the full capabilities of Qwen3-235B. Cline's fast processing and real-time iteration potential place it at the forefront of developer tooling.

**The Future of AI Computing: A Strategic Edge**

Cerebras’ latest offering provides a significant edge over competitors like OpenAI and Anthropic by combining superior model intelligence with unparalleled speed and affordability. Qwen3-235B is not only a game-changer in terms of performance and cost but also demonstrates what’s possible when AI evolves to keep pace with the rapid needs of developers.

**About Cerebras Systems**

A leader in AI supercomputing, Cerebras continues to innovate with their Wafer-Scale Engine-3, powering the CS-3 system—the world's largest AI processor. Their solutions, offered through cloud and on-premises, support some of the most advanced AI applications today. For more about their groundbreaking work, visit cerebras.ai.

**Summary of Hacker News Discussion on Cerebras Qwen3-235B**

1. **Model Confusion and Availability**:  
   Users noted confusion around the model’s naming conventions (e.g., Qwen3-235B vs. Qwen3-405B mentions) and its availability across platforms like OpenRouter, with some pointing out differences between Cerebras's version and third-party implementations.

2. **Hardware Cost and Architecture Debate**:  
   - A key thread debated Cerebras’s claimed $135M total cost (45 chips at $3M each) for 44GB SRAM per chip. Critics argued this was likely exaggerated, distinguishing between individual chip costs and wafer-scale system pricing (e.g., one user noted TSMC wafers cost ~$30K, but wafer-scale systems are far pricier).  
   - Comparisons to Nvidia’s DGX B200 systems highlighted cost disparities: $500K for Nvidia’s 28TB memory setup vs. Cerebras’s $135M claim, favoring Nvidia for affordability despite Cerebras’s speed claims.  

3. **SRAM vs. HBM Practicality**:  
   - Skepticism arose around Cerebras’s reliance on SRAM for performance. Users argued that while SRAM offers fast access, it’s impractical for large models due to size and thermal constraints. Some speculated Cerebras combines SRAM with external DRAM (via MemoryX) to offset limitations.  
   - Nvidia’s HBM approach was defended as more scalable for real-world workloads, with debate over latency vs. bandwidth trade-offs.  

4. **Token Speed and Efficiency**:  
   The 1,500 tokens/second claim was dissected, with users questioning if this relies entirely on SRAM or hybrid memory. Some compared Groq’s SRAM-focused chips but noted Cerebras’s wafer-scale design might face challenges in power/cooling.  

5. **Quantization and Model Optimization**:  
   Discussions praised modern quantization methods (e.g., GGUF formats) for reducing memory usage without significant precision loss. Dynamic precision assignment in layers was highlighted as a practical optimization, though some debated its implementation complexity.  

6. **Cost-Benefit Analysis**:  
   Critics challenged Cerebras’s value proposition: while $135M hardware might deliver speed, a $500K Nvidia system could break even in ~62 days versus cloud API costs (e.g., $8K/day for Anthropic at scale), favoring Nvidia for cost efficiency.  

**Conclusion**:  
The thread reflects technical skepticism toward Cerebras’s cost and architecture claims, with users emphasizing Nvidia’s ecosystem advantages. While Cerebras’s SRAM-driven speed is acknowledged, practical limitations in scalability, cooling, and cost-effectiveness dominate concerns. Quantization and hybrid memory strategies emerged as critical factors in real-world AI deployment.

### Copilot Vision on Windows 11 sends data to Microsoft servers

#### [Submission URL](https://www.theregister.com/2025/07/23/microsoft_copilot_vision/) | 77 points | by [rntn](https://news.ycombinator.com/user?id=rntn) | [9 comments](https://news.ycombinator.com/item?id=44659137)

In a bold yet controversial move, Microsoft is doubling down on AI integration within Windows 11, introducing features like Copilot Vision, which has garnered mixed reactions. This new AI tool aims to be a "true companion" by capturing everything you do on your PC screen and analyzing it through Microsoft's servers. Although the technology is only used when explicitly activated by the user, privacy concerns are rife given the tool's capability to process every action performed on a machine.

Copilot Vision follows the footsteps of the infamous Recall feature. Despite their assurances, critics remain wary of Microsoft's claim that users' data won't be held for long-term storage or used for ads personalization. Echoes of the playful yet intrusive Clippy of old might make some nostalgic but leave others wary. Europe, famously stringent on data privacy, remains exempt for now, likely influenced by the looming AI Act, but Microsoft plans to extend the service to "non-European countries" soon.

Alongside Copilot Vision, Microsoft is also teasing more AI functionalities, such as Mu - their first "agentic" AI model - eager to assist and act on user commands with natural language instructions. While this sounds promising, potential pitfalls loom under the notorious "hallucination" error common in language models, where they might produce logic-defient results.

In the meantime, innovative features like "Click to Do" are available for testing outside Europe but have continued to stir debates over privacy versus innovation. Microsoft is forging ahead, seemingly betting on AI rising to meet the needs, wants, and occasionally the whims of its users, while skeptics watch closely.

**Summary of Discussion:**  

The discussion highlights widespread skepticism and concern over Microsoft's AI-driven features in Windows 11, particularly **Copilot Vision**, with users emphasizing **privacy risks**, **lack of control**, and comparisons to past controversies:  

1. **Privacy and Data Collection**:  
   - Users criticize Microsoft's history of collecting data despite user settings (e.g., forced updates altering configurations) and question assurances about limited data retention. Comments note that even "opt-in" features could funnel sensitive data to Microsoft’s servers, drawing parallels to the invasive **Recall** feature and older blunders like **Clippy**.  
   - Concerns are raised about **Copilot Vision** acting as a "screen-sharing" tool similar to Teams, potentially transmitting screen data without explicit consent.  

2. **User Control and Proprietary Systems**:  
   - Many argue that proprietary operating systems like Windows inherently limit user autonomy, with one user stating, *"When your vendor locks you into their OS, there’s no stopping their software."* Some advocate switching to Linux for greater control.  

3. **Forced Updates and Lack of Transparency**:  
   - Anecdotes highlight forced Windows updates resetting user preferences (e.g., login backgrounds), eroding trust. Critics accuse Microsoft of prioritizing corporate interests over user agency.  

4. **Antitrust and Legal Risks**:  
   - References to the **Department of Justice (DoJ)** suggest concerns about monopoly practices, with joking predictions of billion-dollar fines. Others sarcastically note that legal repercussions rarely deter big tech.  

5. **AI Reliability and Implementation**:  
   - Skepticism surrounds AI “hallucinations” (erroneous outputs) and the rushed rollout of features like **Mu**, labeled as experimental "Labs" previews. Users question whether Copilot’s crashes and UI issues indicate unfinished development.  

6. **Geopolitical Exemptions**:  
   - Europe’s exclusion (due to strict privacy laws like the **AI Act**) is noted, though plans to expand elsewhere amplify fears of unchecked data harvesting in other regions.  

**Overall Sentiment**: A mix of resignation and frustration, with users wary of Microsoft’s AI ambitions and distrustful of its privacy claims, even as the company pushes forward with new features.

### Executive Order – Preventing Woke AI in the Federal Government

#### [Submission URL](https://www.whitehouse.gov/presidential-actions/2025/07/preventing-woke-ai-in-the-federal-government/) | 31 points | by [hooverd](https://news.ycombinator.com/user?id=hooverd) | [21 comments](https://news.ycombinator.com/item?id=44665070)

On July 23, 2025, a new executive order titled "Presidential Actions PREVENTING WOKE AI IN THE FEDERAL GOVERNMENT" was introduced, focusing on the implementation and regulation of Artificial Intelligence (AI) in the federal government. The initiative aims to ensure that AI models, particularly large language models (LLMs), deliver reliable and unbiased outputs free of ideological bias. The directive highlights concerns that concepts such as diversity, equity, and inclusion (DEI) could lead to distortions in AI outputs, including historical inaccuracies and ideological skewing.

Under this order, federal agencies are directed to procure AI models adhering to two key principles: truth-seeking and ideological neutrality. These principles mandate that AI systems provide factual, objective, and unbiased information while acknowledging uncertainties. The order also sets forth a process for guiding agencies in implementing these principles, advocating for transparency and flexibility in AI development and procurement while ensuring that federal contracts for AI adhere to these standards.

The guidance will be shaped by input from various federal bodies, emphasizing technical limitations, transparency, and the latitude for vendors to innovate. Exceptions are made for national security systems, recognizing the unique demands of such domains. This initiative builds upon previous efforts to foster trustworthy AI use within the government, reinforcing a commitment to accuracy and impartiality in federal AI applications.

The discussion on Hacker News about the executive order targeting "woke AI" in the federal government reflects a mix of satire, semantic debates, and skepticism. Key themes include:  

1. **Political Satire and Jabs**:  
   - Comments mock the order’s framing, comparing it to dystopian satire (*Idiocracy*) and questioning its legitimacy. References to "Superman," arrests of former presidents, and "HR memes" highlight hyperbolic or cynical takes on the policy’s motives.  

2. **Defining "Woke"**:  
   - Users debate the term’s ambiguity:  
     - Some reductively define it as "non-traditional" or "things I don’t like," linking it to critiques of DEI initiatives.  
     - Others argue "woke" has become a meaningless buzzword, stretched to absurdity (e.g., applied to "vegetarian pizza toppings").  

3. **Skepticism About AI Neutrality**:  
   - Critics question the feasibility of ideologically neutral AI, noting biases are inherent in training data. One user points out the irony of mandating neutrality while rejecting DEI-informed models.  

4. **Policy and Implementation Concerns**:  
   - Practical challenges are raised, such as defining "truth-seeking" in AI and addressing technical limitations. Some suggest the order risks stifling innovation or oversimplifying complex issues.  

5. **Political Tangents**:  
   - Threads derail into broader political grievances, invoking figures like Trump, Musk, and RFK Jr., and debates over education (e.g., claims about "racist math curricula").  

6. **Flame Wars and Fragmentation**:  
   - Portions of the discussion devolve into flagged or unproductive exchanges, reflecting polarized views on the term "woke" and the policy itself.  

**Tone**: The thread blends humor, frustration, and ideological sparring, with some users engaging earnestly on technical challenges while others dismiss the policy as performative or politically charged.