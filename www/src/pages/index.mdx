import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Tue Oct 17 2023 {{ 'date': '2023-10-17T17:11:30.501Z' }}

### Llemma: An Open Language Model for Mathematics

#### [Submission URL](https://arxiv.org/abs/2310.10631) | 245 points | by [AlphaWeaver](https://news.ycombinator.com/user?id=AlphaWeaver) | [44 comments](https://news.ycombinator.com/item?id=37918327)

Researchers have developed a large language model for mathematics called Llemma. The model was trained on a mixture of scientific papers, web data containing mathematics, and mathematical code. Llemma outperforms all known open base models on the MATH benchmark and is even capable of tool use and formal theorem proving without any further fine-tuning. The researchers have openly released all artifacts, including the models, the dataset used for training, and the code to replicate the experiments. This development has the potential to significantly advance the field of mathematical language understanding.

The discussion on Hacker News about the submission on the development of Llemma, a large language model for mathematics, covered several topics. One user noted that specialized provers like Proverbot9001 showed better results in formal theorem proving compared to Llemma. Another user mentioned that Llemma is not meant to replace specialized tools but rather focuses on non-formal proof generation. There was also a discussion about the potential of translating formal proofs into natural language and the combination of different approaches to mathematical language understanding. Some users engaged in wordplay and discussions about the pronunciation of "Llemma." Another user mentioned the training of Llemma on the RoPE dataset and its potential impact in advancing the field of mathematical language understanding.

The discussion also touched on benchmark results comparing Llemma to other models like WizardMath1 and its commercial applications. There was a mention of downloading test prompts and validation libraries related to Llemma. A user questioned the need for proprietary licenses in the model and whether the source code was publicly available. Another user highlighted the importance of clear and accurate naming in scientific projects. Additional comments included discussions on the practical applications of Llemma in mathematics and programming, as well as opinions on naming conventions and the use of catchy marketing names.

### Making CRDTs More Efficient

#### [Submission URL](https://jakelazaroff.com/words/making-crdts-98-percent-more-efficient/) | 259 points | by [jakelazaroff](https://news.ycombinator.com/user?id=jakelazaroff) | [55 comments](https://news.ycombinator.com/item?id=37915934)

In the third part of his blog post series, Jake Lazaroff demonstrates how he reduces the state size of a collaborative pixel art editor that uses state-based CRDTs. The initial state size is around 648kb for a 100x100 image, but Jake aims to decrease it to around 14kb. To achieve this, he implements several optimizations. First, he changes the color representation from RGB tuples to hex codes, reducing the size by 6%. Next, he stores the UUIDs in a separate table instead of repeating them for each pixel, resulting in a 63% reduction in size. Finally, he applies the same technique to colors and creates a palette table, further reducing the size. By the end, the state size is reduced to 236kb, which is almost 98% smaller than the initial size.

The discussion on this submission revolved around different ways to further optimize the size of the state in a collaborative pixel art editor that uses state-based CRDTs. Some commenters suggested alternative compression techniques, such as implementing RLE-based compression or using general-purpose compressors like zstd. Others mentioned the possibility of using different data representations, like JSON or BSON, and the potential benefits of using faster UUID generation methods. The discussion also touched on the trade-offs between storing UUIDs and integers and the effectiveness of integer compression techniques. Some commenters recommended exploring advanced compression techniques like FastPFOR or Roaring Bitmaps. Overall, the conversation highlighted different perspectives on optimizing the state size and offered additional suggestions for further improvement.

### The Meta glassholes have arrived

#### [Submission URL](https://www.theverge.com/23920102/meta-quest-3-in-public-privacy-recording-glassholes) | 32 points | by [ent101](https://news.ycombinator.com/user?id=ent101) | [28 comments](https://news.ycombinator.com/item?id=37923184)

The latest version of Meta's virtual reality headset, the Meta Quest 3, has already sparked controversy as some owners have started posting videos of themselves using the device in public spaces. These individuals, dubbed "Meta glassholes," are capturing footage of everyday activities such as ordering coffee, cooking, and even waiting for an elevator. While the videos range from amusing to impressive, they have also raised concerns about privacy and social etiquette. The incident highlights the evolving opinions on wearable technology in public spaces, which have changed considerably since Google Glass gained notoriety a decade ago. However, Meta's lack of published guidelines for the Quest 3 and the device's discreet recording indicators have raised questions about the company's preparedness for such incidents. It remains to be seen whether Meta will take action to address these concerns.

The discussion on Hacker News regarding the submission about Meta's Meta Quest 3 virtual reality headset mainly revolves around the concerns of recording and privacy in public spaces. Some users argue that there is nothing inherently wrong with posting videos of everyday activities, while others express worries about privacy invasion and the need for guidelines from Meta. Some users compare the situation to the Google Glass controversy a decade ago and discuss the varying norms and laws regarding public recording in different countries. 

Additionally, there is a brief discussion about the benefits and drawbacks of wearable technology, such as VR headsets, in public spaces. Some users believe that public recording has already been normalized through the use of smartphones, while others express concerns about the surveillance capabilities of Meta's device.

There are also a few comments about unrelated topics, such as forthcoming translation features for the Meta Quest 3 and the long-term web tracking of smartphones.

One user finds the article's description of "Meta glassholes" confusing and incomplete, suggesting that it fell short of providing a polished critique. Another user points out that the term "glassholes" was popularized years ago in relation to Google Glass. The term is used to describe people who record videos in public spaces without notifying others.

Overall, the discussion touches on a range of topics related to the impact of wearable technology on privacy, social norms, and public discourse.

### Antibiotic Identified by AI

#### [Submission URL](https://www.nature.com/articles/s41589-023-01448-6) | 175 points | by [bookofjoe](https://news.ycombinator.com/user?id=bookofjoe) | [50 comments](https://news.ycombinator.com/item?id=37909433)

A study published in Nature Chemical Biology describes how researchers have used machine learning to discover a new antibiotic called abaucin, which targets the bacterial pathogen Acinetobacter baumannii. Traditionally, antibiotics have been discovered through the screening of soil microorganisms, but this approach is time-consuming and expensive. By incorporating artificial intelligence and other computational approaches, researchers have been able to accelerate the identification of new drugs. In this study, the machine learning algorithm was used to predict the antimicrobial activity of molecules against Acinetobacter baumannii, a cause of hospital-derived infections. The discovery of abaucin showcases the potential of machine learning in the field of drug discovery.

The discussion on this submission consists of various comments from users with different backgrounds and perspectives. Here are the main points discussed:

- Some users express skepticism about the validity and effectiveness of the machine learning algorithm used in the study, questioning the quality of the data and the reliability of the predictions made.
- Others mention the long-standing use of computational techniques, such as high-throughput screening, in the pharmaceutical industry for discovering potential drug compounds.
- One user highlights the importance of incorporating machine learning into drug discovery but also mentions the challenges of optimizing the models for different tasks.
- Another user recommends checking out resources, such as blogs and articles by experts in the field, for more insights into the practices and techniques used in drug discovery.
- There is a discussion about the limitations of using QSAR (Quantitative Structure-Activity Relationship) models and the complexities involved in optimizing compounds for various desired properties.
- Some users share concerns about antibiotic resistance and the need for responsible use of antibiotics.
- There are a few comments discussing the challenges of accurately interpreting scientific articles and the availability of non-paywalled versions.
- The topic of the role of artificial intelligence in scientific advancements is briefly touched upon, with some users expressing appreciation for the potential of AI.
- The risks and consequences of antibiotic resistance are discussed, with some suggesting better regulations and restrictions on antibiotic use.

Overall, the discussion touches upon various aspects of the study, from the reliability of the machine learning algorithm to the broader implications of antibiotic discovery and resistance.

### Interviews in the Age of AI: Ditch Leetcode – Try Code Reviews Instead

#### [Submission URL](https://chrlschn.dev/blog/2023/07/interviews-age-of-ai-ditch-leetcode-try-code-reviews-instead/) | 182 points | by [CharlieDigital](https://news.ycombinator.com/user?id=CharlieDigital) | [263 comments](https://news.ycombinator.com/item?id=37913506)

In a recent Medium article titled "Interviews in the Age of AI: Ditch Leetcode - Try Code Reviews Instead," Charles Chen argues that traditional coding exercises, such as those found on Leetcode, may not be the best way to evaluate software engineering candidates. Chen believes that code reviews offer a more realistic and insightful evaluation process. 

Chen points out that most developers don't spend their coding time on algorithmically complex problems. Instead, they rely on resources like StackOverflow, documentation, and online tools for assistance. Additionally, developers often work in isolation and without time constraints, which is unlike high-stakes coding interviews. 

By shifting the focus to code reviews, Chen believes that teams can evaluate a candidate's ability to read and understand code, identify defects, provide quality feedback, and collaborate effectively. Code reviews also provide a more accurate picture of how a candidate would fit into a team and their depth of experience. Moreover, code reviews are not easily "cheatable" through AI-generated code or studying for specific problems. 

Chen suggests several strategies for implementing code reviews in the interview process, such as using relevant parts of an existing codebase or real problems the team has been working on. Overall, he emphasizes that code reviews offer a more comprehensive and practical evaluation of technical candidates.

The discussion surrounding the submission revolves around the topic of background checks during the hiring process. Some commenters express concerns about the legal implications and privacy concerns of conducting background checks. Others share their experiences with background checks and the different practices they have encountered, highlighting the variations in different regions and industries.

The conversation also touches on the idea of showcasing personal projects during interviews as an alternative evaluation method. Some commenters argue that personal projects can be a good indicator of a candidate's skills and commitment, while others caution that not all candidates have the resources or time to work on personal projects.

There is also a discussion about the use of code reviews as a better evaluation method for software engineering candidates compared to traditional coding exercises. Commenters agree that code reviews offer a more realistic evaluation of a candidate's abilities, as they focus on reading and understanding code, providing feedback, and collaborating effectively. Some commenters share their positive experiences with implementing code reviews in their hiring process.

Overall, the discussion highlights the importance of finding alternative evaluation methods that provide a more comprehensive and practical view of a candidate's skills and fit within a team. Background checks and code reviews are explored as potential solutions to this challenge.

### Lumalabs AI

#### [Submission URL](https://lumalabs.ai) | 66 points | by [downboots](https://news.ycombinator.com/user?id=downboots) | [14 comments](https://news.ycombinator.com/item?id=37920678)

Luma AI is revolutionizing the world of visual effects with its cutting-edge technology. Their latest innovation, Luma AI Interactive Scenes, allows users to capture lifelike 3D environments with unmatched photorealism, reflections, and details. The future of VFX is now accessible to everyone! The company offers an iOS app called My Captures, which enables users to create stunning flythroughs of their 3D scenes. With just a few taps, you can generate high-quality, photorealistic assets and environments in minutes using the Luma API. And if you're looking for even more advanced features, they have a pro version available, so you can take your creations to the next level.

Behind the scenes, Luma AI has assembled an impressive team of experts, including Ian Curtis, a seasoned professional in the field. Together, they're pushing the boundaries of what's possible in the world of visual effects. The Luma AI website provides more information on their innovative products and services. They also have a dedicated Discord community where users can connect, share their work, and collaborate with fellow creatives. It's no wonder Luma AI is making waves in the industry. Their technology is unlocking the potential for anyone to create stunning 3D visual effects. So, join Luma AI and be a part of the future of VFX!

The discussion on the submission revolves around various aspects of Luma AI's technology and its potential applications.

- mkc mentions that Luma AI's product is based on NeRF-bsd 3D-Gaussians and is impressed with the implementation as it follows a research paper in a polished manner.
- xnx is familiar with a similar technology called Polycam KIRI Engine but mentions that Luma AI's app does not support Android, which is disappointing.
- tmhlx reflects on how fast technology is advancing and mentions that some incredible things that were once considered impossible are now becoming normal.
- IanCal asks about the current version of the product and expresses concerns about the running UI and incomplete functionality, along with issues related to Gaussian splatting.
   - thschw believes that Gaussian splatting was covered by Inria in a research paper.
- hstrlh comments on the high fidelity of Luma AI's product, making it a compelling offering.
- syntxng brings up the topic of 3D printing a 3D Gaussian-based scene.
   - coder543 provides a short answer, saying that it is not possible currently, but there may be potential for it in the future by using cross-referencing and other AI techniques.
      - vln supports this idea, suggesting that it might be feasible in the future but would require significant manual work.
         - coder543 disagrees, saying it is an entirely impractical and time-consuming question.
- blvscff comments that the geometry produced by Luma AI's product looks great, but the underlying geometry might be messy.
- m3kw9 mentions using iPhone LiDAR for 3D scanning but acknowledges that the results may not have the same level of fidelity as Luma AI's product.
   - vln confirms having used LiDAR on an iPhone for 3D scanning and notes that the results are not at the same level as Luma AI's scans.
   - jnplcktt agrees and states that Luma AI's scenes look like LiDAR scans.

### An AI Which Imitates Humans Can Beat Humans

#### [Submission URL](https://tecunningham.github.io/posts/2023-09-05-model-of-ai-imitation.html) | 17 points | by [laurex](https://news.ycombinator.com/user?id=laurex) | [4 comments](https://news.ycombinator.com/item?id=37908597)

In a recent blog post, the author discusses whether AI systems trained to imitate human behaviors can eventually surpass human capabilities. The author explores five mechanisms through which imitative AI could potentially outperform humans. The first mechanism is noise. Different humans have different answers to the same question, so if an AI model can consistently provide the average answer, it would outperform the average human. The second mechanism is specialization. Humans tend to write about what they know, so an AI model that learns to predict typical answers to specific questions could sound like a specialist in various areas. It could answer questions about water like a hydrologist and questions about bugs like an entomologist. The third mechanism is interpolation. AI models can interpolate responses from different humans, which can be functionally equivalent to inference. This means that AI models may be able to answer questions that no human can answer reliably. The fourth mechanism is priors. If an AI model has different priors than a human, it could uncover hidden structures that humans are unaware of. For example, an AI model trained on human observations of astronomical events could potentially discover cycles in those events and make more accurate predictions than humans. The fifth mechanism is tacit knowledge. Most human knowledge is tacit, meaning it is used in forming judgments without conscious awareness. If AI models can accurately predict human judgments, then the weights in those models effectively contain that tacit knowledge. These models can be re-engineered to use that knowledge in ways that humans cannot. 

Although the author provides a theoretical framework for these mechanisms, there is limited evidence of superhuman performance by AI systems. Many benchmarks used to evaluate machine learning models have human labels as the ground truth, making it difficult to determine when computers surpass humans. The blog post includes a graphical argument illustrating the five mechanisms and a deeper discussion of each mechanism. It also explores the AI-human gap in various tasks, discusses applications and related literature, and presents a simple formal model derived from the five mechanisms.

The discussion on this submission includes a comment from "hltst" who references articles that discuss the increasing accuracy of AI models in solving mathematical problems. They mention that the accuracy remains relatively low compared to human norms, and increasing the model parameters does not necessarily improve mathematical reasoning. In response, "K0balt" makes a sarcastic comment, saying they can't help but smirk at highly intelligent computers struggling with fundamental math. There is another comment from "mchlhny" who adds that the content mentioned in the submission is also available on a GitHub page. Overall, the discussion seems to revolve around the limitations of AI models in terms of mathematical reasoning and the availability of additional content related to the submission.

### Stable Diffusion Gets a Major Boost with RTX Acceleration

#### [Submission URL](https://www.nvidia.com/en-us/geforce/news/game-ready-driver-dlss-3-naraka-vermintide-rtx-vsr/) | 101 points | by [ortusdux](https://news.ycombinator.com/user?id=ortusdux) | [26 comments](https://news.ycombinator.com/item?id=37921661)

NVIDIA has released a new Game Ready driver that brings several enhancements to gaming performance. The driver introduces DLSS 3 support for NARAKA: BLADEPOINT and Warhammer: Vermintide 2, allowing GeForce RTX gamers to experience improved frame rates. Additionally, the RTX Video Super Resolution feature has been updated to version 1.5, bringing improved quality and support for GeForce RTX 20 Series GPUs. The update also includes faster performance for Stable Diffusion, a generative AI tool for image generation. Furthermore, GeForce Experience now supports optimal settings for 14 new games, including Counter-Strike 2 and Forza Motorsport. To download the new driver, head to the Drivers tab of GeForce Experience or GeForce.com.

The discussion on this submission covers various topics related to the NVIDIA Game Ready driver update.

- Some users discuss limitations and issues with the update, such as the lack of support for LoRA and the need for conversion of base models.
- Others share their experiences with the driver update, mentioning noticeable improvements in gaming performance on their RTX graphics cards.
- There is a link provided for support pages related to the update.
- Users discuss the memory and performance aspects of the driver update, with some noting that it works below 8GB of VRAM and others mentioning the difficulty in obtaining worsened training data.
- One user mentions running the update on an 8GB card and experiencing a potential memory swap issue.
- Another user points out that the update does not reduce memory usage for 32-bit floating-point VAE decoding.
- The discussion also includes a mention of reaching high frame rates using interpolation and the usefulness of fast generation in treating faster motion.
- There is a link provided to the extension referenced in the discussion.
- Some users discuss the limited support and feature set provided by TensorRT, with one mentioning other libraries that offer more support.
- A few users complain about the lack of instructions for Linux and Mac users and express anticipation for faster generation times.
- One user humorously mentions that their RTX JK card is not supported.
- A couple of off-topic comments eventually lead to a discussion about Mr. Miyagi and the RTX graphics card.

Overall, the discussion mainly revolves around technical aspects, limitations, and user experiences with the NVIDIA Game Ready driver update, with some tangential discussions as well.

---

## AI Submissions for Mon Oct 16 2023 {{ 'date': '2023-10-16T17:10:35.045Z' }}

### MemGPT – LLMs with self-editing memory for unbounded context

#### [Submission URL](https://github.com/cpacker/MemGPT) | 330 points | by [shishirpatil](https://news.ycombinator.com/user?id=shishirpatil) | [78 comments](https://news.ycombinator.com/item?id=37901902)

MemGPT is a system that allows you to create perpetual chatbots with self-editing memory. It intelligently manages different memory tiers within the model to provide extended context and enable continuous conversations. The system knows when to push critical information to a vector database and when to retrieve it later in the chat. It supports various types of data sources, including SQL databases, local files, and documents.

You can try out MemGPT on Discord by messaging the MemGPT bot in the #memgpt channel. To run MemGPT locally, you need to install the dependencies and add your OpenAI API key to the environment variables. Then you can run the main.py file. You can also create new starter users or personas by adding .txt files in the appropriate folders.

MemGPT CLI provides various commands for interacting with the chatbot, such as saving and loading checkpoints, viewing message logs, and managing memory. Additionally, MemGPT's archival memory feature allows you to load your SQL database and have conversations with it. The system includes a toy example using a test database.

MemGPT is a powerful tool for creating chatbots that can have meaningful and ongoing conversations by leveraging memory management techniques. Check out the repository for more information and examples.

The discussion on this submission covers a range of topics related to MemGPT and memory management in chatbots.

- One user mentions that they have experimented with a similar approach of managing memory in a limited context window, where the chatbot generates memories and retrieves them later. They also mention that there are multiple ways to handle memory in chatbots, including implicit and explicit memory management.
- Another user finds the approach interesting and mentions that they are working on a similar feedback loop for rewriting history and transactional data in a conversational context. They discuss the potential of using structured data to extract context and generate embeddings for building vector databases.
- Someone points out that multi-agent systems could be implemented with confidence levels and entropy to make conversations more worthwhile.
- Another user suggests that the same approach could be applied to ChatGPT, a chatbot that they have used which degrades in performance when long chat histories are included. They speculate that recursive summarization could be a fundamental feature to solve this issue.
- The limitations of Llama (a similar project) are brought up, including difficulties in generating correct function calls and grammatically correct sampling.
- In response, it is mentioned that grammar-based sampling is not a perfect fit for MemGPT experiments, as the main impact is with incorrect function parameters, not the function call itself.
- The discussion also touches on the potential of using total chat change as a prompt and how conversations with context windows could retain important information.
- Some users discuss the potential of a middle language model and the vanishing gradient problem in long-context models. Resources related to this topic are shared, including papers on long-context language models and the vanishing gradient problem.
- Finally, the discussion briefly mentions regularization techniques for mitigating the vanishing gradient problem in neural networks.

Overall, the discussion provides insights and ideas related to memory management, chatbot design, and the challenges associated with long-context models.

### Actively exploited Cisco 0day with maximum severity gives full network control

#### [Submission URL](https://arstechnica.com/security/2023/10/actively-exploited-cisco-0-day-with-maximum-10-severity-gives-full-network-control/) | 114 points | by [AdmiralAsshat](https://news.ycombinator.com/user?id=AdmiralAsshat) | [6 comments](https://news.ycombinator.com/item?id=37906156)

Cisco has warned its customers about a critical zero-day vulnerability that is actively being exploited by threat actors. The vulnerability, tracked as CVE-2023-20198, allows attackers to gain full administrative control over Cisco devices, including switches, routers, and wireless LAN controllers running IOS XE software with the HTTP or HTTPS Server feature enabled and exposed to the Internet. Cisco's Talos security team discovered that an unknown threat actor has been exploiting the zero-day since September 18, creating an authorized user account and deploying a malicious implant that allows for the execution of arbitrary commands. Cisco has advised affected entities to implement the necessary steps outlined in its advisory to protect their devices.

The discussion around the submission revolves around various points:

1. User "jpc0" mentions that Cisco's advisory is related to the vulnerability being accessible through the HTTP or HTTPS server feature enabled on Internet-facing systems. They also highlight the importance of following established practices for securing critical hardware and management access.
2. User "cdh" criticizes Cisco, suggesting that they compromise software in a sneaky manner and fail to contact victims. They also make negative remarks about CEOs, network teams, and inexperienced internet entry-level staff.
3. User "jcqsm" finds it interesting how the headline implies that Cisco's response to the situation is different from their previous actions in similar incidents.
4. User "crs" simply states that it is noteworthy that Cisco has a zero-day vulnerability.
5. User "mltynss" contributes to the discussion by mentioning Security Technical Implementation Guides (STIGs), which provide guidelines for secure configuration and operation of various systems. They provide links to STIGs related to Cisco IOS XE switch and router configurations. Another user, "ThePowerOfFuet," identifies the acronym STIG and confirms its meaning.

Overall, the discussion contains mixed sentiments, ranging from technical insights to criticisms of Cisco's handling of the situation.

### PaLI-3 Vision Language Models

#### [Submission URL](https://arxiv.org/abs/2310.09199) | 173 points | by [maccaw](https://news.ycombinator.com/user?id=maccaw) | [22 comments](https://news.ycombinator.com/item?id=37895601)

A team of researchers from various institutions has introduced a smaller, faster, and stronger vision language model (VLM) called PaLI-3. Compared to larger models, PaLI-3 achieves comparable performance while using only a fraction of the parameters. The researchers achieved this by comparing pretrained models using classification objectives to contrastively pretrained ones. While PaLI-3 slightly underperforms on image classification benchmarks, it outperforms on various multimodal benchmarks, particularly in localization and visually-situated text understanding. The team scaled up the model's image encoder to 2 billion parameters, setting a new state-of-the-art on multilingual cross-modal retrieval. With PaLI-3, the researchers hope to encourage further research on fundamental aspects of complex VLMs and pave the way for scaled-up models in the future.

The discussion on this submission covers various aspects of the research and its implications. Some users express skepticism about the realism of the training data used for the model and argue that it may not reflect real-world scenarios. Others point out that benchmarking is a common practice in machine learning but note that the benchmarks may not fully represent complex real-world tasks. One user shares a link to another paper for further comparison. 

There is also a discussion about the technical details of the model, including the visual tokens and the projection of visual tokens in the PaLI-3 model. Users provide explanations and comparisons with other implementations such as ViT.

Some users raise concerns about the limitations of models like PaLI-3 in handling tasks such as pixel-wise segmentation masks. Others discuss potential applications of vision-language models for tasks like OCR and image categorization.

A few comments comment on the rivalry between different companies in the field of AI research, particularly mentioning Google, OpenAI, and Facebook. There is a debate surrounding the capabilities and performance of their respective models, with some users criticizing the boastful nature of their submissions.

The discussion also touches on copyright infringement and the potential misuse of AI models.

### AI pioneers LeCun, Bengio clash in intense online AI safety, governance debate

#### [Submission URL](https://venturebeat.com/ai/ai-pioneers-yann-lecun-and-yoshua-bengio-clash-in-an-intense-online-debate-over-ai-safety-and-governance/) | 31 points | by [rntn](https://news.ycombinator.com/user?id=rntn) | [17 comments](https://news.ycombinator.com/item?id=37902248)

Two prominent figures in the field of artificial intelligence (AI), Yann LeCun and Yoshua Bengio, engaged in a fiery debate over the weekend regarding the potential risks and safety concerns associated with AI. LeCun, Meta's chief AI scientist, initiated the debate on Facebook, calling on AI scientists and engineers who believe in the power of AI to voice their opinions. Bengio, founder of Element AI and a professor at the University of Montreal, responded by challenging LeCun's perspective on AI safety and raising concerns about the risks of open-source AI platforms. The debate also involved input from Jason Eisner, director of research at Microsoft Semantic Machines and professor at Johns Hopkins University. Despite their past collaboration and shared recognition for contributions to the field, the debate highlights the considerable disagreement among esteemed researchers regarding the risks and safety measures associated with AI. This ongoing debate reflects the growing concern surrounding the implications of AI as it becomes increasingly embedded in daily life. As AI technology continues to advance, the need for informed discussions on its implications becomes more urgent.

The discussion on the submission revolves around different perspectives on AI safety and its potential risks. Some commenters argue that AI should be designed with strong safety measures in place to avoid dangerous consequences, comparing it to the responsible design of weapons. Others disagree with this comparison, stating that AI is intended to enhance human intelligence and not cause harm. 

One commenter brings up the topic of the American Medical Association (AMA) and its control over the supply of medical providers, arguing that restrictions on the number of doctors allowed to practice exacerbate the medical crisis. Another commenter responds by emphasizing that the commenter's comment is condescending and that people facing the medical crisis deserve better understanding.

The debate continues with discussions on the affordability of medical care, the role of AI in replacing certain professions, and the potential economic impact of such changes. There are also comments suggesting alternate solutions and expressing skepticism about the role of radiologists.

Some commenters argue that AI should be prioritized for applications that can achieve positive results and ensure safety, rather than wasting time on potentially harmful endeavors. Others argue that the responsibility for any negative consequences lies with society, not solely with AI researchers or developers.

Overall, the discussion reflects differing opinions on the importance of AI safety measures and the potential risks associated with AI technology.

### Ultra-efficient machine learning transistor cuts AI energy use by 99%

#### [Submission URL](https://newatlas.com/technology/ai-machine-learning-transistor/) | 27 points | by [0xa2](https://news.ycombinator.com/user?id=0xa2) | [9 comments](https://news.ycombinator.com/item?id=37899129)

Researchers at Northwestern University have developed a microtransistor that could significantly reduce the energy consumption of AI machine learning tasks. The microtransistor, built from two-dimensional sheets of molybdenum disulfide and carbon nanotubes, is 100 times more efficient than current technology, and can perform classification tasks at just 1% of the energy consumption. This breakthrough could enable the deployment of AI directly in wearable electronics, leading to real-time detection and data processing in health emergencies. The new transistor's high tunability and low energy consumption make it ideal for sophisticated classification algorithms with a small footprint. The researchers demonstrated its capabilities by correctly classifying abnormal heartbeats with 95% accuracy using just two of these microtransistors. Once this technology is brought to production, mobile devices will be able to run machine learning AI on their own sensor data, providing quicker results and keeping personal data local and secure. It remains to be seen if this technology can extend beyond portable devices and be applied to larger AI equipment, potentially revolutionizing large model training by drastically reducing energy consumption.

The discussion surrounding the submission revolves around the concept of weight valuation in AI neural networks and the potential impact of the new microtransistor technology. One user suggests that understanding how neural networks work, particularly backpropagation and forward pass, can help optimize weight valuation and save energy. They also mention that analog computation could introduce errors from noise and thermal variations. Another user acknowledges the importance of energy-efficient training but mentions that inferences on low-power devices with pre-trained models are more common in practice. Another user suggests that it might be possible to create AI chips with programmable architectures that can handle both training and inference tasks with lower power consumption. A different perspective is brought up, stating that regardless of the advancements in energy efficiency, digital neural networks still require high bandwidth due to data correction and machine learning algorithms that are not tolerant to noise. Overall, the discussion focuses on various aspects of AI, neural networks, and the potential benefits and challenges of the new microtransistor technology.

---

## AI Submissions for Sun Oct 15 2023 {{ 'date': '2023-10-15T17:10:35.576Z' }}

### MemGPT: Towards LLMs as Operating Systems

#### [Submission URL](https://arxiv.org/abs/2310.08560) | 210 points | by [belter](https://news.ycombinator.com/user?id=belter) | [117 comments](https://news.ycombinator.com/item?id=37894403)

Researchers from various institutions have proposed a technique called virtual context management to extend the utility of large language models (LLMs), such as GPT, in tasks like extended conversations and document analysis. LLMs are often limited by their context windows, which restrict the amount of information they can process. Inspired by hierarchical memory systems in traditional operating systems, the researchers have developed MemGPT (Memory-GPT), an operating system-like system that intelligently manages different memory tiers to provide extended context within the LLM's limited context window. MemGPT uses interrupts to manage control flow between itself and the user. The researchers evaluated MemGPT in two domains: document analysis and multi-session chat, and found that MemGPT can effectively analyze large documents and create conversational agents that remember and evolve dynamically through interactions with users. The researchers have released the MemGPT code and data for further experiments.

The discussion around this submission covers a range of topics. 

- Users discuss the limitations of large language models (LLMs) and the potential benefits of extending their context windows through techniques like MemGPT. Some users share their experiences with similar projects and suggest different approaches to context management.
- Some users express their appreciation for the work and offer positive feedback to the author.
- Others discuss the nature of AI models and the challenges in their development and deployment. There are discussions about the reliability of AI models, the importance of replicability in scientific publishing, and the potential risks associated with AI technology.
- There is also a brief discussion about the application of AI in the cryptocurrency industry and the potential impact on different sectors.
- Finally, there are a few comments exploring the analogy between the AI industry and the gold rush, and a humorous exchange about selling shovels in a gold rush.

Overall, the discussion covers a range of perspectives and insights related to large language models, AI technology, and its potential implications.

### Teaching Apple Cyberdog 1.0 new tricks (featuring OpenDoc)

#### [Submission URL](http://oldvcr.blogspot.com/2023/10/teaching-apple-cyberdog-10-new-tricks.html) | 126 points | by [classichasclass](https://news.ycombinator.com/user?id=classichasclass) | [55 comments](https://news.ycombinator.com/item?id=37894030)

In a blast from the past, the author revisits Apple's Cyberdog, a web browser and internet suite that has long been forgotten. Cyberdog was unique in that it allowed developers to create their own components, such as viewers and UI elements, using Apple's OpenDoc embedding. OpenDoc was a standard compound document format that allowed for an object-oriented approach to document creation. The goal was to have reusable components that could be pulled into a document and maintain their own views and state. Cyberdog was essentially a demonstration of OpenDoc's capabilities, and it was released as part of Apple's Project Amber, which aimed to create a next-generation technology platform called Taligent. Despite Apple's efforts, OpenDoc did not gain much traction with developers or users, and it was seen as a competitor to Microsoft's Object Linking and Embedding (OLE) technology. Apple eventually released Cyberdog as an internet suite, capitalizing on the popularity of internet document creation. However, Cyberdog also faded into obscurity, and today it serves as a reminder of Apple's ambitious but unsuccessful foray into component-based document creation.

The discussion on this article covers various aspects of Apple's Cyberdog and OpenDoc technology.

- One commenter mentions that they remember the Apple Dylan IDE requiring 24MB of RAM, which was a significant amount at the time. They also mention that Cyberdog was a fascinating project but ultimately faced difficulties due to its large RAM requirements.
- Another commenter shares links to screenshots and explanations of Cyberdog, as well as a mention of the SK8 programming language.
- There is a discussion about Steve Jobs' response to OpenDoc versus Java, with a correction made that Jobs was not yet CEO at the time.
- A commenter expresses relief in reading the well-written article but admits that they still don't fully understand the supposed problem that OpenDoc was meant to solve.
- Some commenters compare OpenDoc to Microsoft's OLE technology, with one mentioning that OpenDoc aimed to improve cross-platform interoperability.
- The complexity and memory requirements of OpenDoc are mentioned, with one commenter stating that it required a significant amount of RAM to function.
- One commenter shares their personal experience with using Apple Cyberdog and praises its capabilities.
- The topic of connecting Cyberdog with Microsoft Internet Explorer is brought up, with a link shared to an archived page about it.
- There are mentions of the CI Labs and its involvement with OpenDoc.
- Commenters discuss the creativity and whimsical nature of the icons used in old Mac applications.
- The conversation touches on the history of Cyberdog and its features, as well as the advancements in instant search technology.
- A reference is made to a character named Preston from a 1995 Wallace and Gromit short film.
- The discussion briefly touches on compound documents and the challenges they presented in the '90s.
- A commenter mentions a Japanese system that recently graduated from paper documents to digital ones.
- The discussion also includes mentions of the Kantara project and the App Store.

Overall, the comments cover a range of experiences, memories, and opinions related to Apple's Cyberdog and OpenDoc technology.

### Show HN: Deep Chat – AI chat component

#### [Submission URL](https://github.com/OvidijusParsiunas/deep-chat) | 65 points | by [ovisource](https://news.ycombinator.com/user?id=ovisource) | [4 comments](https://news.ycombinator.com/item?id=37889444)

Deep Chat is a customizable AI chat component that can be easily integrated into your website. It allows you to connect to popular AI APIs like OpenAI, HuggingFace, and Cohere, or even to your own custom service. With Deep Chat, you can send and receive messages, exchange files, capture photos via webcam, record audio, and even convert speech to text and vice versa. The latest update includes support for custom elements in message bubbles, allowing you to add suggestion buttons, charts, maps, or any other HTML element you desire. Deep Chat is highly customizable and can be used with any major UI framework or library. To get started, simply install the npm package and add the Deep Chat component to your markup.

In the discussion, user "vrtclbx" expressed their appreciation for the webcam and microphone functionality in the chat component and mentioned that they found it useful for capturing photos and recording audio. User "jlthln" suggested adding a feature that allows for the integration of a recommendation engine to enhance the product. User "vsrc" thanked "jlthln" for the suggestion and mentioned that they are currently calling external services to handle interactions with models. They appreciated the suggestion and said that they are planning to add functionality to host models entirely in the browser, which would greatly benefit from the recommendation engine capabilities.

### Margaret Atwood Reviews a Margaret Atwood Story by AI

#### [Submission URL](https://thewalrus.ca/margaret-atwood-ai/) | 88 points | by [goldenskye](https://news.ycombinator.com/user?id=goldenskye) | [68 comments](https://news.ycombinator.com/item?id=37894072)

In a recent article, the author dives into the anxieties surrounding generative AI and its potential impact on writers. They question whether AI chatbots will devour our literature, infiltrate our minds, and take over our jobs. However, the author provides some reassurance by highlighting the current limitations of AI chatbots, such as their inability to reflect or grasp metaphor and punctuation. To demonstrate this, the author shares two examples of literary attempts by AI chatbots, a poem and a short story. While these examples certainly have their quirks and inaccuracies, they serve as a reminder that AI chatbots are not yet ready to replace human authors. So, writers can take heart knowing that their creative skills are safe from the clutches of AI, at least for now. Ultimately, while the fear of AI may loom large in some writers' minds, it seems that human creativity still has the upper hand.

The discussion on this submission focuses on various aspects of AI-generated writing and the limitations of current AI models. One commenter notes that the 10x improvement mentioned in the article regarding ChatGPT is not adequately supported and questions the validity of such claims. Another points out that GPT-4 does not possess the intelligent reasoning capabilities that GPT-35 lacks, emphasizing the need to differentiate between different versions of AI models. Some commenters express skepticism about AI-generated content, while others argue that AI can assist human writers and bring new perspectives. The debate also touches on the potential shortcomings of AI models in replicating human memory and recall. Additionally, there are discussions about Margaret Atwood's writing style and the similarities between AI-generated content and children's stories or specific authors like H.P. Lovecraft.

### Scientists begin building AI for scientific discovery using tech behind ChatGPT

#### [Submission URL](https://techxplore.com/news/2023-10-scientists-ai-scientific-discovery-tech.html) | 38 points | by [gardenfelder](https://news.ycombinator.com/user?id=gardenfelder) | [14 comments](https://news.ycombinator.com/item?id=37890570)

An international team of scientists, including researchers from the University of Cambridge, has launched a research collaboration called Polymathic AI to develop an AI-powered tool for scientific discovery. Leveraging the technology behind ChatGPT, the team aims to build an AI that can learn from numerical data and physics simulations to assist scientists in various scientific fields. By starting with a large, pre-trained model, Polymathic AI aims to make AI and machine learning more accessible and effective in scientific research. The team includes experts in physics, astrophysics, mathematics, artificial intelligence, and neuroscience from institutions such as the University of Cambridge, Simons Foundation, New York University, Princeton University, and Lawrence Berkeley National Laboratory. Polymathic AI's goal is to connect different scientific subfields and apply multidisciplinary knowledge to solve complex scientific problems. The project will prioritize transparency and openness, aiming to democratize AI for scientific analysis across various domains.

The discussion surrounding the submission includes various viewpoints. 

One commenter expresses skepticism about the hype surrounding AI, stating that while it may have many applications, it is unlikely to replicate the scientific discoveries made by great scientists like Albert Einstein. They believe that scientific research requires tools for searching large spaces and independently verifying results.

Another commenter mentions their involvement in computational chemistry and shares a link to their GitHub repository.

There is a discussion about the BLOOM language model, where someone suggests joining the BLOOM collaboration and asks if the BLOOM model will receive funding. Another commenter indicates that they are unsure about joining the BLOOM collaboration and mentions that the BLOOM model can answer questions.

Yet another commenter argues that Language Models (LLMs) are capable of innovation and discovering new concepts. They believe that innovation can come from simplifying existing concepts and challenge the notion that discovery requires complex methods.

In response, another commenter disagrees, stating that complex physics relies on a vast number of relationships published by human knowledge.

Overall, the discussion touches on concerns regarding the potential of AI in scientific research, the importance of independent verification, the capabilities of BLOOM language model, and the role of LLMs in innovation and scientific discovery.

### 2nd law of infodynamics and its implications for simulated universe hypothesis

#### [Submission URL](https://pubs.aip.org/aip/adv/article/13/10/105308/2915332/The-second-law-of-infodynamics-and-its) | 14 points | by [imhoguy](https://news.ycombinator.com/user?id=imhoguy) | [5 comments](https://news.ycombinator.com/item?id=37893189)

In a recent article published in AIP Advances, Melvin M. Vopson explores the implications of the second law of infodynamics for the simulated universe hypothesis. The simulated universe hypothesis suggests that our entire reality is a simulated construct. While lacking concrete evidence, this idea is gaining popularity in both scientific and entertainment circles. The second law of infodynamics, discovered in 2022, further supports this possibility by providing a new framework for studying the intersection of physics and information. Vopson examines the applicability of this law to various domains, including digital information, genetic information, atomic physics, mathematical symmetries, and cosmology. By re-examining the second law of infodynamics, Vopson provides scientific evidence that appears to underpin the simulated universe hypothesis. This research opens up new avenues for understanding the nature of our reality and the role of information within it.

In the discussion, user "SideburnsOfDoom" raises a question about the second law of infodynamics, stating that it may be ignorant to assume that the second law implies the first law of infodynamics, which is a fundamental principle in science. User "az09mugen" agrees with SideburnsOfDoom, stating that the first law of infodynamics is indeed foundational. User "c22" adds that in 2022, a new fundamental law of physics was proposed and demonstrated, called the law of information dynamics, which simplifies the second law of infodynamics. They also provide a link to an article on the topic. User "gus_massa" mentions that the law may have different applications in various domains and gives an example related to DNA replication. "SideburnsOfDoom" remarks that the discussion is becoming too technical and compares it to a college-level physics class.