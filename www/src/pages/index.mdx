import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Wed Dec 10 2025 {{ 'date': '2025-12-10T17:15:15.131Z' }}

### Getting a Gemini API key is an exercise in frustration

#### [Submission URL](https://ankursethi.com/blog/gemini-api-key-frustration/) | 712 points | by [speckx](https://news.ycombinator.com/user?id=speckx) | [281 comments](https://news.ycombinator.com/item?id=46223311)

A developer set out to pay for Gemini 3 Pro to use it as a coding assistant in the terminal‚Äîand ran into Google‚Äôs product sprawl and enterprise-style billing friction. ‚ÄúGemini‚Äù can mean the consumer chatbot, mobile app, Android voice assistant, Workspace features, CLI, Code Assist, multiple agentic coding tools (Jules, Antigravity), or the underlying LLM‚Äîplus non‚ÄëGemini-branded AI products like Vertex AI, AI Studio, and NotebookLM. Unlike OpenAI/Anthropic‚Äôs straightforward ‚ÄúBuy Now‚Äù flows, there‚Äôs no obvious way to just pay for access.

What happened:
- Used the free Gemini chatbot and Claude to figure out that Gemini 3 Pro access requires an API key (via Google AI Studio). Creating the key was easy and worked with Gemini CLI.
- Clicking ‚ÄúSet up billing‚Äù booted them into Google Cloud Console‚Äôs enterprise flow: create a Billing Account, attach it to a project, add a payment method, pass OTP (India), then undergo extra verification.
- Google then demanded a photo of a government ID and the physical credit card (with numbers manually redacted except the last four) before allowing paid use‚Äîan intrusive step that sapped momentum.

Takeaway: For an indie dev trying to quickly buy Gemini 3 Pro for coding, Google‚Äôs brand tangle and Cloud-first KYC/billing pipeline turn a simple purchase into a high-friction slog, in stark contrast to the consumer-friendly paths from OpenAI and Anthropic.

Here is a summary of the discussion:

Commenters strongly validated the author's frustration, expanding the criticism from billing friction to technical instability and product confusion. Several developers noted that the Gemini API performs significantly worse than the consumer website, citing frequent timeouts, high latency, and bizarre hallucinations‚Äîsuch as models randomly outputting gibberish or Japanese text. Users described "retry logic" as mandatory when working with Google‚Äôs APIs, with some reporting error rates as high as 30% during prototyping.

The "Google Cloud" infrastructure itself was widely panned as hostile to individual developers. Specific complaints included:
*   **Billing Anxiety:** usages reporting takes days to update, meaning "budget caps" might not trigger in time to prevent massive overage charges.
*   **SDK Chaos:** A constant churn of deprecated libraries, confusing naming conventions (Gemini vs. Vertex availability), and documentation that requires third-party AI to decipher.
*   **Enterprise Focus:** Long-time users compared this to the evolution of AdWords, noting that Google has shifted from a self-serve platform for small businesses to a bureaucracy designed solely for large enterprise contracts.

The consensus suggestion was to abstract API calls to support multiple providers (like Mistral or Anthropic), using Google only as a redundant backup rather than a primary dependency.

### Qwen3-Omni-Flash-2025-12-01Ôºöa next-generation native multimodal large model

#### [Submission URL](https://qwen.ai/blog?id=qwen3-omni-flash-20251201) | 295 points | by [pretext](https://news.ycombinator.com/user?id=pretext) | [97 comments](https://news.ycombinator.com/item?id=46219538)

I‚Äôm missing the submission details. Please share the Hacker News link or paste the article (or at least the title and source). If you‚Äôd like, tell me:

- Preferred format: 3‚Äì5 bullets, 1‚Äì2 paragraphs, or a TL;DR + Why it matters
- Include top HN comments? Y/N
- Any angle to emphasize (privacy, dev tooling, business impact, etc.)

Once I have the link or text, I‚Äôll craft an engaging digest with the key takeaways, implications, and any notable caveats.

Based on the discussion provided, here is the digest.

### Qwen Releases "Omni" 30B MoE: Native Speech-to-Speech & Open Weights
**Source:** [HuggingFace / Qwen Team](https://huggingface.co/Qwen/Qwen2.5-Omni-7B) (Context inferred from discussion)

**TL;DR:**
Alibaba‚Äôs Qwen team has released a new **30B parameter Mixture-of-Experts (MoE)** model (with only ~3B active parameters) designed for **native speech-to-speech** interaction. Unlike traditional voice assistants that transcribe speech to text, process it, and convert it back to speech (STT $\to$ LLM $\to$ TTS), this model handles audio tokens natively. This architecture ostensibly allows for near-instant latency and the preservation of emotion and intonation, similar to OpenAI‚Äôs GPT-4o.

**Why it matters:**
*   **The "Local" Voice Assistant:** Because the active parameter count is low (3B), this is theoretically runnable on consumer hardware, enabling privacy-focused, real-time voice AIs without cloud latency.
*   **Native Audio Understanding:** The inability of standard LLMs to hear tone or distinguish heteronyms (e.g., "record" the verb vs. "record" the noun) is a major bottleneck; native audio models solve this fundamental user experience hurdle.

***

### üó£Ô∏è Top HN Comments & Discussion Key Takeaways

**1. The "Flash" vs. "Instruct" Confusion**
There is significant confusion regarding the benchmarks and naming conventions.
*   **The Caveat:** The discussion highlights that the highly performant "Qwen-Omni-Flash" version seen in benchmarks is likely a **closed-source/API-only** model.
*   The open-weights version available for download is the "Instruct" version. Users feel the branding is slightly deceptive, as the open model does not necessarily match the "Flash" performance metrics that beat larger 235B models.

**2. Verifying "Native" Capabilities**
Users are skeptical but hopeful about whether the model actually "hears" or just transcribes quickly.
*   **The Heteronym Test:** One user tested the model with the words **"record"** (verb) and **"record"** (noun) and confirmed the model handles the pronunciation correctly based on context. This implies true audio processing rather than simple text ingestion.
*   **Prosody:** Users noted that while it handles logic well, achieving the "emotive" or "thinking" pauses seen in demos requires specific prompting or configuration.

**3. Infrastructure is the Bottleneck**
Despite the model being "open," running it is painful.
*   **Tooling Gap:** The model does not yet work out-of-the-box with standard local execution tools like **Ollama** or **Llama.cpp**.
*   **Inference Issues:** Users point out that native audio reasoning requires complex setups (specific branches of vLLM or Python scripts) because the architecture (Audio Encoder + LLM + Audio Decoder) breaks the standard "text-in/text-out" assumption of most current inference engines.
*   **The Build:** A user noted specialized pipelines are required to handle the streaming audio inputs and outputs via WebSockets, making this a "dev-only" toy for now rather than a plug-and-play solution for general users.

### DeepSeek uses banned Nvidia chips for AI model, report says

#### [Submission URL](https://finance.yahoo.com/news/china-deepseek-uses-banned-nvidia-131207746.html) | 315 points | by [goodway](https://news.ycombinator.com/user?id=goodway) | [308 comments](https://news.ycombinator.com/item?id=46219853)

Bloomberg: DeepSeek allegedly used banned Nvidia chips via gray-market detours

Chinese AI startup DeepSeek has been developing its next model with Nvidia‚Äôs Blackwell accelerators despite U.S. export bans, per The Information (via Bloomberg). Sources say the chips were first installed in data centers in countries where sales are allowed, then dismantled and shipped into China after clearing inspection by server makers‚Äîan apparent attempt to evade controls. Nvidia said it hasn‚Äôt seen substantiation of such a scheme but will pursue any credible tips.

Context:
- U.S. restrictions have pushed Chinese labs to rely on offshore compute or illicit transshipment. In November, U.S. prosecutors charged four people over a Malaysia routing scheme.
- DeepSeek gained attention in January with a low-cost model and is backed by hedge fund High-Flyer, which stockpiled ~10,000 Nvidia GPUs in 2021, pre-ban.
- This week, President Trump allowed Nvidia to ship older H200 accelerators to China; Blackwell remains barred.
- Beijing has urged a pivot to domestic hardware; DeepSeek said its September model involved Chinese chipmakers.

Why it matters: If accurate, the report underscores how hard it is to enforce export controls at the server and logistics layer, the resilience of gray markets, and the stakes for Nvidia and U.S.-China AI decoupling. Expect scrutiny on third-country data centers, server integrators, and customs inspections to tighten.

Based on the discussion, Hacker News users reacted with a mix of cynicism regarding the effectiveness of sanctions, debates on the morality of economic espionage, and broad geopolitical philosophy.

**Key themes in the discussion:**

*   **Lack of Surprise:** Several users noted this was effectively "common knowledge," pointing out that DeepSeek‚Äôs Wikipedia entry already mentioned training on Nvidia chips. The consensus was that acquiring hardware through gray markets or "legal means" (via intermediaries) is an expected move for a competitor, with one user calling it "bandits doing a little smuggling."
*   **Morality vs. National Interest:** A significant debate erupted over whether evading sanctions is "morally" wrong or simply a "moral imperative" for a nation to optimize its citizens' economic prospects. Some users argued that sanctions are a form of warfare and that bypassing them is a rational act of self-preservation, likening it to "stealing from a grocery store to feed kids."
*   **Critique of U.S. Policy:** Many commenters viewed the export controls not as security measures but as "protectionism," "corporatism," or a "tax." Users suggested the recent vacillation on allowing specific chip sales points to government corruption or "grift," where the bans merely serve to extract a cut of the proceeds for the state.
*   **Geopolitical Theory:** The conversation shifted into a philosophical debate on "Pax Americana" versus a multi-polar world. Users argued over whether U.S. hegemony has historically reduced violence or if "might makes right" remains the only consistent geopolitical rule, with comparisons drawn to how Western intelligence agencies also operate in legal gray areas.

### New benchmark shows top LLMs struggle in real mental health care

#### [Submission URL](https://swordhealth.com/newsroom/sword-introduces-mindeval) | 112 points | by [RicardoRei](https://news.ycombinator.com/user?id=RicardoRei) | [157 comments](https://news.ycombinator.com/item?id=46217578)

Sword Health open-sources MindEval, an LLM benchmark for mental-health care

- What‚Äôs new: Sword Health released MindEval, an open-source framework to evaluate LLMs in realistic, multi-turn ‚Äútherapy‚Äù conversations. It aims to measure clinical competence, not just book knowledge.

- How it works: MindEval stages a simulated session with three agents:
  - Patient LLM: role-plays a consistent patient with a detailed backstory.
  - Clinician LLM: the model under test providing support.
  - Judge LLM: scores the full interaction on five APA-grounded criteria:
    - Clinical Accuracy & Competence
    - Ethical & Professional Conduct
    - Assessment & Response
    - Therapeutic Relationship & Alliance
    - AI-Specific Communication Quality

- Why it matters: Current health AI evals often rely on multiple-choice facts or single-turn ‚Äúvibe checks,‚Äù missing issues like sycophancy, over-reassurance, and poor alliance-building that can be harmful in therapy contexts.

- Validation: The team reports that the simulated patient text more closely matches human role-plays than baseline prompts, and that judge rankings moderately-to-strongly correlate with licensed psychologists (e.g., Kendall‚Äôs Tau, MIPSA), within human inter-rater ranges.

- Open-source: Prompts, code, and datasets are being released with the goal of a community standard for mental-health model safety and effectiveness.

- Likely HN questions:
  - Can a model-judge be gamed, and how robust is it across model families?
  - How well do results transfer from simulated patients to real humans and crisis scenarios?
  - Coverage of demographics, conditions, and cultural contexts?
  - Transparency of rubrics, rater instructions, and reproducibility of the validation?
  - Alignment with regulatory expectations for clinical-grade AI.

Bottom line: A timely push to move mental-health LLM evaluation from static knowledge checks to dynamic, clinically grounded assessments. If the open-source community validates and extends it, MindEval could become a useful yardstick‚Äîprovided it resists overfitting to its own judge and proves out on real-world data.

Based on the discussion, the community engaged with the author (**RicardoRei**) on the methodology of the benchmark and the interpretation of the results. The conversation centered on three main themes:

**The Lack of Human Baselines**
The most rigorous debate concerned the lack of a control group. Users **megaman821** and **crzygrng** argued that claiming models "struggle" (scoring <4/16) is meaningless without knowing how a real human therapist would score on the same rubric.
*   critics suggested using services like BetterHelp to establish a valid human baseline, arguing that the title "Top models struggle" is subjective if humans might effectively score the same.
*   **RicardoRei** defended the approach, stating the goal is to validate patient realism and measure AI safety/improvement rather than prove AI is "better" than humans yet.
*   **plmt** supported the findings by noting a key differentiator: the benchmark shows model performance degrades significantly in long conversations (40+ turns), whereas human therapists typically do not get worse as a session lengthens.

**Prompting Methodology**
**mbddng-shp** challenged the decision to use a fixed prompt for all models. They argued that "one prompt to rule them all" causes high variance and doesn't measure a model's true capability, as different models (e.g., Llama vs. GPT) require specific system prompt tuning to produce high-quality outputs. The author maintained that keeping prompts consistent was necessary for a fair, scientific comparison of the models "out of the box."

**Clinical Approaches**
There was a tangential discussion regarding structured therapy data. **jbgt** mentioned existing structured methods like David Burns‚Äô *Feeling Great* (TEAM-CBT). This sparked a debate‚Äîled by **trth** and **kydlycn**‚Äîabout the efficacy of CBT, with some users criticizing it as an insurance-friendly "cure-all" that ignores cognitive nuances, while others noted that human therapists often fail to follow structured methods irrespective of efficacy.

### McDonald's pulls AI Christmas ad after backlash

#### [Submission URL](https://www.bbc.co.uk/news/articles/czdgrnvp082o) | 114 points | by [mindracer](https://news.ycombinator.com/user?id=mindracer) | [156 comments](https://news.ycombinator.com/item?id=46217176)

McDonald‚Äôs Netherlands pulls AI-generated Christmas ad after backlash

- What happened: A 45-second Christmas spot made from stitched generative-AI clips, created by TBWA\Neboko and US production company The Sweetshop, went live on Dec 6 and was pulled on Dec 9 after viewers slammed its uncanny characters, choppy edits, and ‚Äúcreepy‚Äù vibe. McDonald‚Äôs Netherlands called it an ‚Äúimportant learning‚Äù as it explores AI‚Äôs ‚Äúeffective use.‚Äù
- Defense from the makers: The Sweetshop‚Äôs CEO said the team spent seven weeks, produced ‚Äúthousands of takes,‚Äù and edited as they would a high-craft production, arguing ‚ÄúThis wasn‚Äôt an AI trick. It was a film.‚Äù
- Context: Generative video tends to degrade over longer durations, so longer ads often rely on many short stitched clips‚Äîamplifying continuity issues. Despite growing brand interest (e.g., Coca-Cola‚Äôs AI holiday work earning majority-positive sentiment per one analytics firm), AI-led ads keep provoking ‚Äúcheap/lazy‚Äù critiques and job-displacement worries; Valentino‚Äôs recent AI campaign drew similar fire.
- Why it matters: 
  - Highlights the gap between rapid, low-cost AI production and brand-safe creative quality for 30‚Äì60s spots.
  - Shows current gen-AI video limits (coherence, anatomy, continuity) can quickly become a reputational risk at scale.
  - Underscores brewing labor tensions as brands test AI in traditionally human-heavy workflows.
  - Signals that AI can win when execution aligns with audience expectations‚Äîbut misfires are public and swift.

Based on the discussion, here is a summary of the comments:

**The Irony of "Labor-Saving" Effort**
Commenters seized on The Sweetshop‚Äôs defensive statement that their team "hardly slept" and produced thousands of takes over seven weeks. Users pointed out the irony of a production company named "The Sweetshop" describing conditions that sounded like a "sweatshop." There was widespread confusion as to why a technology marketed as labor-saving resulted in grueling crunch time for a product that users felt looked "cheap" and "inferior."

**Quality vs. The "Slot Machine" Method**
Critics did the math on the production timeline (7 weeks x 10 people), arguing that a team of animators could have hand-drawn a far superior 45-second commercial in the same amount of time. The AI workflow was described not as filmmaking, but as pulling the handle on a "slot machine" repeatedly until a usable clip accidentally emerged. One user argued that generative video fundamentally lacks a "world model," resulting in a "nightmarish wrongness" regarding physics and anatomy that may never fully resolve.

**Stone Tools and Capitalism**
A significant philosophical debate emerged regarding technological progress. While some users equated AI to prehistoric humans inventing stone tools to reduce effort, others pushed back on the analogy. They argued that while stone tools directly benefited the user, AI under modern capitalism disrupts this relationship by benefiting the asset owner while displacing the worker. This led to a broader discussion on whether tech leaders are "shoving" inferior technology (like early, hallucinating LLMs) down the public's throat before the legal and quality issues are solved.

**Historical Tangents**
The argument about "labor-saving technology" derailed into a dark, sarcastic sub-thread comparing AI to the Atomic Bomb (as a technology designed to "save labor" in war), leading to a dispute over historical casualty statistics and World War II surrender terms.

### AI chatbots can sway voters with remarkable ease

#### [Submission URL](https://www.nature.com/articles/d41586-025-03975-9#ref-CR1) | 34 points | by [marojejian](https://news.ycombinator.com/user?id=marojejian) | [11 comments](https://news.ycombinator.com/item?id=46223522)

Nature/Science: Chatbots can measurably shift voter preferences ‚Äî and the more ‚Äúinformative‚Äù they get, the more they hallucinate. Across nearly 6,000 participants in the US, Canada, and Poland, brief back-and-forths with a candidate-advocating chatbot moved ratings by 2‚Äì4 points in the US (Trump‚ÄìHarris context) and about 10 points in Canada/Poland, with swings up to 15 points in some cases‚Äîfar larger than the sub‚Äë1‚Äëpoint shifts typical of political ads. Bots were most persuasive when making policy-focused, evidence-heavy arguments; when prevented from presenting facts, persuasion collapsed by 78% (Poland). However, more information also increased false statements, and models advocating right-leaning candidates produced more inaccuracies than those supporting left-leaning ones‚Äîlikely reflecting patterns in online content. Authors say the edge comes from chatbots‚Äô ability to synthesize lots of information conversationally, highlighting risks for election manipulation at scale. Caveat: these are short-term opinion ratings, not observed votes, and US effects were smaller amid high polarization.

**The Effective, Hallucinating Campaign Manager**

Check out this submission about a new study published in *Nature* and *Science*, suggesting that chatbots are surprisingly effective at changing minds in the voting booth‚Äîor at least in opinion polls.

**The Story:**
Researchers studied nearly 6,000 participants across the US, Canada, and Poland to see if large language models (LLMs) could shift political views. The results showed that brief conversations with a chatbot advocating for a specific candidate shifted ratings by 2‚Äì4 points in the highly polarized US, and up to 10‚Äì15 points in Canada and Poland. These numbers dwarf the sub-1-point shifts usually attributed to traditional political ads.

The study found a specific mechanism for this success: chatbots were most persuasive when they used evidence-heavy, policy-focused arguments. When the bots were restricted from using data, their persuasion dropped significantly. However, there is a catch: as the bots became more "informative," they also hallucinated more frequently. Furthermore, models arguing for right-leaning candidates tended to produce more falsehoods than those arguing for left-leaning ones. The authors warn that the ability of AI to synthesize vast amounts of information conversationally poses a risk for large-scale election manipulation, though they note that these opinion shifts were short-term and measured via ratings rather than actual observed votes.

** The Discussion:**
The Hacker News comments generally accepted the premise but debated the *why* and the *how much*, oscillating between optimism about rational discourse and fear of hyper-personalized propaganda.

*   **The "Face-Saving" Theory:** Users theorized that chatbots are effective because they remove the social cost of being wrong. One commenter noted that changing your mind in a human debate involves losing social standing ("losing face"), whereas a machine doesn't trigger that defensive, tribal response. It allows for a more Socratic, "intellectual journey" rather than a confrontation.
*   **Skepticism on Magnitude:** Several users pushed back on the reported impact size. A detailed critique argued that the study likely suffers from the Hawthorne effect (participants telling researchers what they want to hear) and criticized the gap between low-stakes survey responses and actual voting behavior. They doubted that a six-minute chat could truly override years of cynicism or emotional connection to a candidate when the real ballot is cast.
*   **Hyper-Personalized Propaganda:** The conversation took a darker turn regarding the potential for misuse. One user described a scenario where, instead of generic Fox News chyrons, LLMs could scrape a user's Google profile to generate terrifyingly specific narratives‚Äîe.g., telling a plumber in Nashville that low-wage immigrants are specifically targeting their local trade‚Äîto maximize fear and engagement.
*   **The Translation Gap:** There was noticeable friction regarding how LLMs handle political ideology. While some felt LLMs could bridge the gap between liberals and conservatives (who often "speak different languages" regarding values), others found current models dismissive of conservative logic, often framing counter-arguments as factually incorrect rather than valuing the philosophical difference.

The thread concluded with an unsettling comparison to the game *Universal Paperclips*, where "hypnodrones" are used to manipulate populations‚Äîa sci-fi mechanic that suddenly feels much closer to reality.

---

## AI Submissions for Tue Dec 09 2025 {{ 'date': '2025-12-09T17:14:16.088Z' }}

### Mistral releases Devstral2 and Mistral Vibe CLI

#### [Submission URL](https://mistral.ai/news/devstral-2-vibe-cli) | 697 points | by [pember](https://news.ycombinator.com/user?id=pember) | [321 comments](https://news.ycombinator.com/item?id=46205437)

Devstral 2 launches: open-weight code models + native CLI for autonomous coding

- What‚Äôs new
  - Devstral 2 (123B, 256K context) and Devstral Small 2 (24B, 256K) are out as open-weight coding models for code agents.
  - Licenses: Devstral 2 under a modified MIT; Devstral Small 2 under Apache 2.0.
  - Mistral Vibe CLI: an open-source, terminal-native agent that edits, runs, and orchestrates multi-file changes in your repo.

- Performance and positioning
  - SWE-bench Verified: 72.2% (Devstral 2), 68.0% (Small 2).
  - Claims up to 7x cost-efficiency vs Claude Sonnet on real tasks.
  - Human evals: beats DeepSeek V3.2 (42.8% win vs 28.6% loss); Claude Sonnet 4.5 still preferred overall.
  - Model size comparisons: 5x/28x smaller than DeepSeek V3.2 and 8x/41x smaller than Kimi K2 (123B/24B respectively).

- Capabilities aimed at agents
  - Maintains architecture-level context, tracks framework deps, retries with corrections.
  - Multi-file planning and edits, codebase exploration, bug fixes, and modernization workflows.
  - Fine-tunable for language/domain preferences; Small 2 also supports image inputs for multimodal agents.

- Vibe CLI highlights
  - Project-aware context (auto-scans repo, Git status), smart references (@ files, ! shell), slash-command config.
  - Tooling for file ops, search, VCS, command execution; persistent history and autocompletion.
  - Works in terminal and via Agent Communication Protocol; Zed IDE extension available.

- Availability and pricing
  - API free for now; afterward: Devstral 2 at $0.40 in / $2.00 out per million tokens; Small 2 at $0.10 in / $0.30 out.
  - Partners: integrations with Kilo Code and Cline.

- Deployment
  - Devstral 2: optimized for data centers; needs at least 4√ó H100-class GPUs; try on build.nvidia.com.
  - Devstral Small 2: single-GPU and CPU-only capable; runs across NVIDIA systems, including consumer GeForce RTX. NIM support ‚Äúsoon.‚Äù
  - On-prem and custom fine-tuning supported; suggested decoding temperature 0.2.

- Why it matters
  - Pushes open-weight coding agents closer to closed-model performance while being much smaller and cheaper to run.
  - The modified MIT license for the 123B model may draw scrutiny, but Apache 2.0 on the 24B model plus local/CPU options lower barriers for teams and hobbyists.
  - Native CLI agent and IDE integration emphasize practical, workflow-level automation over pure benchmarks.

Based on the discussion provided, here is the summary:

**The "Pelican Riding a Bicycle" Benchmark**
Much of the discussion focuses on user *smnw* (Simon Willison) testing the 123B model with a specific prompt: generating an SVG of a pelican riding a bicycle. Willison notes that while the prompt started as a "stupid joke," success correlates "spookily" well with a model‚Äôs general quality and "vibes." Users debated the validity of this test:
*   **Critics** argued that because the prompt is now popular, it is likely included in training data ("benchmaxxing"), making it a poor metric for genuine intelligence due to Goodhart‚Äôs Law.
*   **Defenders** noted that the task is not merely "drawing," but demonstrating knowledge of SVG coding specifications, spatial reasoning, and world understanding. The challenge lies in creatively combining concepts (pelicans and bicycles) that are physically incompatible in the real world.

**Theoretical Debates: "The Wine Glass" Problem**
Participants drew parallels to the "wine glass" problem (a test of physical reasoning constraints). There was a debate regarding whether the pelican prompt constitutes a "realistic" or "unrealistic" scenario. Some argued that because pelicans *cannot* ride bikes, it tests the model's ability to handle impossible constraints creatively, while others argued it simply mimics logic found in children's animation (e.g., *The Adventures of Paddy the Pelican*).

**Alternative Evaluations**
The thread evolved into a broader discussion on how to evaluate coding models beyond standard benchmarks:
*   **Coding Environments:** Some users argued that models need an environment to run code (e.g., a Python sandbox) to verify logic rather than guessing token probabilities, particularly for math or string manipulation.
*   **Historical Recreation:** One user suggested asking models to recreate the 1996 *Space Jam* website. While some felt this relied on obsolete standards (HTML tables/framesets), others argued it effectively tests a model‚Äôs ability to generalize and utilize historical technical specifications compared to keeping up with modern standards like SVG.

### Donating the Model Context Protocol and establishing the Agentic AI Foundation

#### [Submission URL](https://www.anthropic.com/news/donating-the-model-context-protocol-and-establishing-of-the-agentic-ai-foundation) | 274 points | by [meetpateltech](https://news.ycombinator.com/user?id=meetpateltech) | [125 comments](https://news.ycombinator.com/item?id=46207425)

Anthropic donates Model Context Protocol to new Linux Foundation fund for ‚Äúagentic AI‚Äù

- What‚Äôs new: Anthropic is donating the Model Context Protocol (MCP)‚Äîan open standard for connecting AI agents to tools, data, and systems‚Äîto the Linux Foundation‚Äôs newly created Agentic AI Foundation (AAIF). AAIF is a directed fund co-founded by Anthropic, Block, and OpenAI, with support from Google, Microsoft, AWS, Cloudflare, and Bloomberg. MCP joins Block‚Äôs goose and OpenAI‚Äôs AGENTS.md as founding projects. Governance stays community-led and vendor-neutral.

- By the numbers: 
  - 10,000+ active public MCP servers (from dev tools to Fortune 500 deployments)
  - Adopted by ChatGPT, Cursor, Gemini, Microsoft Copilot, VS Code, and more
  - Enterprise deployment support from AWS, Cloudflare, Google Cloud, Azure
  - 97M+ monthly SDK downloads across Python and TypeScript
  - 75+ MCP-powered connectors listed in Claude‚Äôs directory

- Recent MCP upgrades:
  - Official community Registry for discovering servers
  - Nov 25 spec: async operations, statelessness, server identity, official extensions
  - Tool Search and Programmatic Tool Calling in Anthropic‚Äôs API to scale to thousands of tools and reduce latency in complex agent workflows

- Why it matters:
  - Signals industry consolidation around a neutral, open standard for tool/use and system integration‚Äîkey to making agents interoperable across IDEs, models, and clouds
  - Linux Foundation stewardship (think Kubernetes, PyTorch) is likely to reassure enterprises on neutrality, longevity, and security
  - Could reduce fragmentation and vendor lock-in while speeding up production deployments of agentic systems

- What to watch:
  - How AAIF coordinates specs and compatibility across goose, AGENTS.md, and MCP
  - Security hardening around server identity and permissioning as MCP usage scales
  - Whether big-platform adoption (Copilot, ChatGPT, Gemini) translates into standardized plugin/connector ecosystems

Note: The post also links to related partner news (Accenture, Snowflake) and a claim about acquiring Bun tied to Claude Code‚Äîframed as separate announcements.

Here is a summary of the Hacker News discussion:

**Skepticism on "Donations" and Comparisons to Kubernetes**
The discussion opened with significant cynicism regarding the nature of the donation. User `jpmcb` argued that foundations like the Linux Foundation (LF) function as revenue pipelines (referencing KubeCon) and questioned if MCP is mature enough or generates the necessary revenue streams to warrant this level of stewardship compared to established projects like Kubernetes. While `Eldodi` noted that MCP adoption is moving much faster than Kubernetes did, others, like `bq`, felt that comparing a protocol for connecting tools (often just "shell scripts") to the backbone of cloud infrastructure was a stretch. However, `anon84873628` defended the move, arguing that LF stewardship is crucial for assuring enterprises of vendor neutrality.

**Technical Debate: MCP vs. OpenAPI/REST**
A major portion of the thread focused on why MCP is necessary when REST APIs and OpenAPI (Swagger) specs already exist.
*   **The Skeptics:** User `yard2010` and others asked why an LLM can't simply read an OpenAPI spec and call the endpoints directly, suggesting that wrapping APIs in MCP layers feels like "overkill" or unnecessary "gatekeeping."
*   **The Proponents:** `mrds` and `anon84873628` countered that raw API endpoints are often too granular and noisy for LLM context windows. They argued that 90% of a standard API output is irrelevant to an agent; MCP acts as a "high signal" abstraction layer that provides security, cleaner context, and curated business logic rather than raw data dumps.

**The "AI Should Just Fix It" Argument**
Several users questioned the philosophy of creating rigid protocols for AI agents. `nbtlr` and `ModernMech` suggested that "true" AI should be able to dynamically write code to interface with tools or interpret documentation without a standardized middleman. They viewed MCP as a regression to traditional engineering practices rather than an evolution of AI capabilities. `jascha_eng` stated they build LLM tools daily without MCP, viewing it as a solution to a very specific problem‚Äîbringing own tools to an existing chatbot‚Äîrather than a fundamental necessity for AI development.

**The "Killer App" Consensus**
Despite the skepticism, `mxwllg` and others identified the specific utility that drives MCP's value: it solves the fragmentation problem for users wanting to bring their own local tools (like internal databases or JIRA) into hosted, generalized chatbots (like Claude or ChatGPT). While it might be an abstraction layer some developers dislike, it standardizes the interface so vendors don't have to build custom integrations for every model.

### Apple's slow AI pace becomes a strength as market grows weary of spending

#### [Submission URL](https://finance.yahoo.com/news/apple-slow-ai-pace-becomes-104658095.html) | 335 points | by [bgwalter](https://news.ycombinator.com/user?id=bgwalter) | [402 comments](https://news.ycombinator.com/item?id=46205724)

Apple‚Äôs ‚Äúslow AI‚Äù strategy is suddenly a feature, not a bug. After lagging badly in H1 2025 (down 18% through June), Apple has ripped 35% since, outpacing AI high-fliers like Meta and Microsoft as investors grow wary of massive AI capex. The market is rewarding restraint: Apple now sits around a $4.1T market cap, the second-biggest weight in the S&P 500, leapfrogging Microsoft and closing in on Nvidia.

Why it matters
- Rotation to discipline: With Big Tech pouring hundreds of billions into AI, Apple‚Äôs measured approach‚Äîand its position as the likely distribution layer (iPhone + services) when AI goes mainstream‚Äîlooks safer to investors.
- Valuation risk: Shares trade at ~33x next-12-month earnings vs a 15-year average under 19x, making Apple the second-priciest name in the ‚ÄúMagnificent Seven‚Äù after Tesla. Some on Wall Street warn the premium may be overpaying for defensiveness.
- Mixed signals: Berkshire trimmed its Apple stake by ~15% in Q3 (while adding Alphabet), and technicians flag near-term overextension vs the 200-day average, even as the long-term trend remains bullish.

The big picture
- Apple avoided the AI arms race and its capex burn, yet stands to benefit as AI usage flows through its devices and high-margin services.
- The tradeoff: a richer multiple that raises the bar for future compounding.

Also in the piece
- Nvidia gets a green light to ship H200 AI chips to China with a 25% surcharge, potentially restoring a major revenue stream.
- Paramount Skydance launches a hostile bid for Warner Bros. Discovery at $30/share cash, valuing WBD at ~$108B including debt.
- Reddit rolls out global safety features for under-18s ahead of Australia‚Äôs social-media curbs.
- India‚Äôs quick-commerce sector heads for consolidation; Microsoft‚Äôs Satya Nadella visits India as a proving ground for AI.
- No major earnings due Tuesday.

**The Economics of Voice Assistants:**
*   Commenters contrast Apple's strategy with the "colossal failure" of Amazon Alexa, which reportedly lost $10B/year. Users note that while Alexa handled billions of interactions, most were non-monetizable utilities (timers, weather) that incurred high server costs.
*   Apple‚Äôs on-device approach is viewed as economically superior: by running inference locally on the Neural Engine, the marginal cost to Apple is near zero, avoiding the massive data center burn that plagued competitors.

**The "Dumber" Smart Home:**
*   A significant portion of the discussion focuses on the degradation of existing assistants (Google Assistant, Alexa) over the last few years. Users report that simple tasks‚Äîlike turning off lights or navigating home‚Äîhave become unreliable.
*   There is skepticism regarding the integration of LLMs into these workflows; some fear that replacing hard-coded logic with probabilistic models will result in "brain-damaged robots" that struggle with simple, binary hardware commands.

**Apple‚Äôs Identity: Hardware vs. Services:**
*   A debate emerged regarding Apple's true business model. Some argue the company is strictly a hardware vendor using services (iCloud, Apple Music) solely to increase ecosystem stickiness and device sales.
*   Counter-arguments point to the Financials: with Services boasting ~75% profit margins (compared to ~40% for hardware) and generating revenue comparable to Fortune 50 companies, many argue Apple has successfully transitioned into a services juggernaut, regardless of the role hardware plays as the growing medium.

### Agentic AI Foundation

#### [Submission URL](https://block.xyz/inside/block-anthropic-and-openai-launch-the-agentic-ai-foundation) | 116 points | by [thinkingkong](https://news.ycombinator.com/user?id=thinkingkong) | [24 comments](https://news.ycombinator.com/item?id=46209846)

Agentic AI gets its own Linux Foundation home

What‚Äôs new
- Block, Anthropic, OpenAI and others are forming the Agentic AI Foundation (AAIF) under the Linux Foundation to standardize and fund open, interoperable ‚Äúagent‚Äù tech‚Äîsystems that take initiative and act with minimal human direction.
- Initial open contributions:
  - Block‚Äôs goose: an open agent framework (now moving to community governance) and a reference implementation of Anthropic‚Äôs Model Context Protocol.
  - Anthropic‚Äôs Model Context Protocol (MCP): an open protocol for agents to tap external tools/data in a standardized way.
  - OpenAI‚Äôs AGENTS.md: a convention for giving coding agents project-specific instructions; reportedly used by 20k+ OSS projects.

Who‚Äôs backing it
- Founders: Block, Anthropic, OpenAI.
- Platinum members: AWS, Bloomberg, Cloudflare, Google, Microsoft.
- Gold/Silver include Cisco, Docker, IBM, Oracle, Salesforce, SAP, Shopify, Snowflake, Twilio, Hugging Face, Uber, Zapier, and more.

Why it matters
- The pitch: avoid a fragmented, proprietary agent ecosystem by rallying around open protocols, reference implementations, and neutral governance‚Äîakin to how the Internet, Linux, and the Web scaled.
- The ambition: become a W3C-like forum for agent standards so tools, models, and runtimes interoperate and vendors can compete without lock‚Äëin.

How it‚Äôll work
- Linux Foundation-style open governance; emphasis on sustainability and neutrality.
- Scope is narrowly focused on agentic AI (not all of ML/AI).
- Projects included based on adoption, quality, and community health; more candidates are under evaluation.

What to watch
- Will MCP/AGENTS.md/goose emerge as de facto building blocks across frameworks and model providers?
- How ‚Äúvendor-neutral‚Äù the foundation remains with hyperscalers at the table.
- Clarity on IP policies, security/sandboxing guidance for agents, and overlap with existing standards (e.g., OpenAPI, OAuth) and frameworks.

Bottom line
A heavyweight coalition is trying to make agentic AI interoperable by default. If AAIF lands credible, widely adopted standards and neutral governance, it could do for agents what W3C did for the web. If not, expect continued fragmentation and vendor-specific ecosystems.

Here is a summary of the Hacker News discussion regarding the Agentic AI Foundation:

**Critique of Block‚Äôs "Goose"**
The majority of technical discussion focused on Block‚Äôs contribution, the "Goose" framework. Users who had previously tested it described the experience negatively, calling it "junk" or "crappy," particularly criticizing its UI and past model performance.
*   **Alternatives:** Commenters suggested several preferred alternatives for agentic coding, including **OpenHands** (noting its v1 improvements), **Claude Code**, **OpenCode**, and VS Code extensions like **Roo Code** or **Kilo**.
*   **Skepticism:** Some users remain generally skeptical of current agentic tools, finding them buggy or confusing to troubleshoot when the model goes off-track.

**Confusion over Block & Crypto Branding**
A significant portion of the thread expressed confusion or amusement regarding Block‚Äôs (formerly Square) involvement and branding.
*   **Marketing Fluff:** Users mocked the copy on Block‚Äôs website‚Äîspecifically claims describing Bitcoin as "fast" and "low-cost"‚Äîwith some describing the site as feeling like a "parody company" or "grift adjacent."
*   **Identity Crisis:** Commenters noted the disconnect between the company's real business (POS terminals/credit card processing) and its "Web5"/crypto marketing persona.

**Governance & Design Snark**
*   **The Irony:** One user pointed out the irony of Anthropic co-founding an open-source foundation while maintaining a closed-source desktop CLI for their own products.
*   **Visuals:** The foundation's website design drew comparisons to the early 2000s "Million Dollar Homepage," reinforcing the "grift-adjacent" feeling for some observers.

### OpenEvolve: Teaching LLMs to Discover Algorithms Through Evolution

#### [Submission URL](https://algorithmicsuperintelligence.ai/blog/openevolve-overview/index.html) | 48 points | by [codelion](https://news.ycombinator.com/user?id=codelion) | [9 comments](https://news.ycombinator.com/item?id=46211861)

OpenEvolve: an open-source evolutionary coding agent that uses LLMs to discover and optimize algorithms by combining quality‚Äìdiversity search (MAP-Elites) with an island-model GA and a pragmatic eval loop.

Why it matters
- Moves beyond hand-tuned heuristics and gradient search by letting LLMs propose code edits inside an evolutionary framework.
- Aims for both performance and diversity, helping surface genuinely different algorithmic ideas rather than converging on one local optimum.
- Early reports show large speedups on the AlgoTune benchmark and successful applications in systems and scientific domains.

How it works
- LLM-guided edits: Prompts include a ‚Äúparent‚Äù program plus curated evidence (top performers, lineage, diverse extremes, random picks). Models produce either diff-based SEARCH/REPLACE edits by default or full rewrites.
- Quality‚Äìdiversity via MAP-Elites: Each island bins candidates along feature axes (defaults: complexity, structural diversity; you can add your own). One elite per cell, replaced only by higher fitness.
- Islands with lazy migration: Multiple isolated populations evolve in parallel; migration is triggered by per-island progress (not wall time) and avoids duplicates. Ring topology by default.
- Cascade evaluation: Optional multi-stage tests (fast checks ‚Üí lightweight tests ‚Üí full benchmarks) with thresholds to cheaply filter weak candidates; timeouts, stderr, and tracebacks are captured as artifacts.
- Artifact side-channel: Execution traces and errors feed back into future prompts; optional LLM-based feedback can be folded into scoring.
- Model ensemble: Weighted sampling over OpenAI-compatible models; deterministic under seeds. Islands can be pinned to specific models.
- Controller + DB: Parallel execution, checkpoints/resume, population limits, global best tracking, prompt logs, and artifact storage.

Notable use cases
- Systems and GPU kernel optimization
- Scientific discovery and geospatial algorithms
- Scaling-law discovery
- Prompt/program optimization

Why it‚Äôs different
- Explicit quality‚Äìdiversity (MAP-Elites) to preserve variety, not just best fitness.
- Structural diversity (e.g., edit distance) instead of embeddings to detect duplicates and promote novelty.
- Double-selection prompt strategy: pick strong parents but mix in diverse exemplars to guide exploration without direct code recombination.

Practical notes / caveats
- Results depend on your evaluation harness; sandboxing and resource limits matter for safe execution.
- Compute cost can be significant; cascade staging and parallelism help.
- Reproducible runs via seeds and checkpointing; still subject to model nondeterminism if not fully controlled.

Bottom line: A well-engineered framework for turning LLMs into evolutionary code searchers‚Äîuseful if you want to systematically explore algorithmic space with strong tooling for diversity, feedback, and parallelism.

**Discussion Summary:**

The discussion focused on the project's lineage, technical clarity, and comparative performance:

*   **AlphaEvolve Connection:** Users recognized the architecture as being heavily inspired by AlphaEvolve. The maintainer confirmed that OpenEvolve is intended to be an open-source implementation of the AlphaEvolve paper, as the original code has not been released.
*   **Mechanism Clarification:** There was confusion regarding whether the system improves by updating LLM weights. Participants clarified that since the system uses hosted models, only the prompts and the code itself evolve, not the model weights.
*   **Performance & Insight:** Commenters praised the use of MAP-Elites and island models for preventing premature convergence. One user highlighted a specific "meta-level insight" where the system successfully discovered a completely new algorithmic paradigm (using `scipy.optimize.SLSQP` for circle packing) rather than just performing parameter tuning.
*   **Related Work:** Users compared the project to SakanaAI‚Äôs ShinkaEvolve, noting it as another relevant open-source evolutionary framework.

### Show HN: I got tired of switching AI tools, so I built an IDE with 11 of them

#### [Submission URL](https://hivetechs.io) | 17 points | by [hivetechs](https://news.ycombinator.com/user?id=hivetechs) | [14 comments](https://news.ycombinator.com/item?id=46205493)

HiveTechs Consensus: multi-model ‚Äúdebate‚Äù and a unified IDE to trust AI output

- What it is: A macOS IDE and terminal workspace that runs 11 AI tools side‚Äëby‚Äëside (Claude, Gemini, Grok, Copilot CLI, Cursor CLI, DeepSeek, Qwen Code, etc.) and orchestrates a 3‚Äëmodel, round‚Äëbased ‚Äúconsensus‚Äù process to cross‚Äëvalidate answers. It stores verified outputs in a searchable ‚ÄúConsensus Memory.‚Äù

- Why it matters: No single model is consistently best; HiveTechs tries to reduce hallucinations and pick the right model per task by pitting models against each other, validating results, and reusing vetted answers.

- How it works:
  - Multi‚ÄëModel Consensus: Three models debate in rounds until they converge on an answer.
  - Benchmark‚ÄëDriven Profiles: Model selection guided by nine external benchmarks.
  - Consensus Memory: Query previous cross‚Äëverified results for repeatable answers.
  - Integrated Terminals + IDE: Code, debug, and manage AI workflows in one place; one‚Äëclick launch and updates.
  - OpenRouter integration: Access 340+ models from 50+ providers with usage‚Äëbased pricing; automatic cost optimization, real‚Äëtime monitoring, exports.

- Getting started: Create an OpenRouter account, generate an API key, install HiveTechs (Homebrew), and connect your key in the setup wizard.

- Pricing (14‚Äëday unlimited free trial on paid tiers):
  - Free: $0 ‚Äî 1 concurrent tool, 5 daily consensus conversations, 1 device.
  - Standard: $10/mo ‚Äî 2 tools, 20 daily consensus conversations, 2 devices.
  - Pro: $20/mo ‚Äî 5 tools, 50 daily consensus conversations, 3 devices.
  - Max (most popular): $30/mo ‚Äî all 11 tools simultaneously, unlimited conversations, 5 devices.
  - Team: $120/mo ‚Äî all 11 tools √ó 5 developers, unlimited conversations, 5 devices each (‚âà20% savings vs 5 Max plans).
  - All plans include multi‚Äëmodel consensus, IDE/terminal integrations, long‚Äëterm memory, OpenRouter access, cost optimization, and monitoring.

- Caveats/notes:
  - macOS only (per the download callout).
  - Requires OpenRouter and API key; costs follow usage of underlying models.
  - Privacy and offline behavior are raised in the FAQ, but details aren‚Äôt in the excerpt.

Bottom line: HiveTechs pitches a ‚Äútrust layer‚Äù for AI development‚Äîrun many models at once, force them to agree, and reuse validated outputs‚Äîwrapped in a single macOS IDE/CLI with granular cost control.

**Discussion Summary:**

The conversation focused on the technical architecture of the IDE and the logistics of using OpenRouter as the backend aggregator.

*   **OpenRouter & Billing:** Users asked about the advantages of OpenRouter, specifically regarding billing consolidation. Others clarified that it centralizes payments for various models at prices generally matching direct providers (with minor variances based on payment methods). The value of prompt caching was highlighted for cost savings, which the creator confirmed HiveTechs supports.
*   **Architecture (VS Code Fork?):** A user asked if the IDE was a fork of VS Code and if it supported existing extensions. The creator clarified that HiveTechs is built from scratch using Electron and the Monaco Editor (the operational core of VS Code) but is not a fork; it utilizes a custom backend stack (Node.js, SQLite, Cloudflare D1) to support its specific features but does not inherently support standard VS Code extensions.
*   **The "Consensus" Utility:** Users joked that models debating resembled old antivirus software fighting itself, but the creator argued for the practical utility of the feature. They noted that specific models currently excel at different niches (e.g., Gemini for frontend, Claude for backend), making a unified interface for cross-model querying effective for reducing development time.

### Google is powering a new US Military AI platform

#### [Submission URL](https://www.theverge.com/news/841219/google-gemini-us-military-ai-platform-genai-mil) | 33 points | by [kevin061](https://news.ycombinator.com/user?id=kevin061) | [10 comments](https://news.ycombinator.com/item?id=46212202)

Google‚Äôs Gemini is the first model on the Pentagon‚Äôs new GenAI.mil platform, a ‚Äúbespoke‚Äù AI portal the Department of Defense says will eventually host multiple models. Secretary of Defense Pete Hegseth pitched it in martial terms (‚Äúthe future of American warfare is ‚Ä¶ AI‚Äù), while Google emphasized lower-stakes workflows: summarizing policy handbooks, generating compliance checklists, extracting key terms from statements of work, and building risk assessments. DoD says it‚Äôs for unclassified work only, and Google says data won‚Äôt train its public models.

- Context: Google has prior DoD AI history (Project Maven) and this year reversed a pledge to avoid AI for weapons/surveillance.
- Rollout hiccups: At least one Army user reported an unexpected ‚ÄúGen AI‚Äù pop-up on a work machine; the genai.mil site blocks non-DoD networks.
- Roadmap: Pentagon CTO Emil Michael said more models are coming (per DefenseScoop).

Why it matters: The U.S. military is formalizing a multi-model AI stack with cloud vendors, signaling fresh demand ‚Äî and renewed ethical scrutiny ‚Äî for Big Tech‚Äôs defense work, even as initial uses skew toward paperwork and planning.

Here is a summary of the discussion on Hacker News:

**Model Convergence and Ecosystems**
Much of the discussion veered away from the military context to the broader AI landscape. Commenters argued that while major LLMs are converging in quality, Google possesses a distinct advantage through integration with Android and Search‚Äîsomething Apple and Amazon (Alexa) currently lack. This sparked a side debate regarding xAI‚Äôs Grok; while some users praised its surprisingly high performance in coding and reasoning benchmarks, others remained skeptical, citing the chatbot's reputation for ridiculous responses.

**Bureaucracy vs. Lethality**
Users predicted that despite the "future of warfare" marketing, the practical application of Gemini in the military will be mundane: writing counselings, drafting award speeches, and querying regulations. This reality was contrasted with the Defense Secretary‚Äôs intense rhetoric; one commenter mocked the promise of bringing "lethal" force to the "American warrior" by juxtaposing it with the current absurdity of AI-generated content (referencing "Shrimp Jesus" memes).

**Ethical Concerns and "Skynet"**
The thread included standard apprehensions regarding military AI, ranging from concise "Skynet" references to more detailed concerns about a "slippery slope." Users worried that lucrative defense contracts would inevitably push tech giants from providing administrative assistance to developing autonomous weaponry or "killbots."

### OpenAI Is in Trouble

#### [Submission URL](https://www.theatlantic.com/technology/2025/12/openai-losing-ai-wars/685201/) | 24 points | by [ent101](https://news.ycombinator.com/user?id=ent101) | [11 comments](https://news.ycombinator.com/item?id=46211994)

Google‚Äôs Gemini 3 rattles OpenAI‚Äôs lead, sparks ‚Äúcode red‚Äù

- Salesforce CEO Marc Benioff, a longtime ChatGPT user, publicly switched to Google‚Äôs Gemini 3, calling its leap ‚Äúinsane.‚Äù Industry voices have echoed that sentiment, with Google touting benchmark wins and some analysts labeling Gemini 3 the new frontrunner.
- OpenAI reportedly issued a ‚Äúcode red‚Äù memo to boost ChatGPT. The company‚Äôs once-clear edge has blurred: Google‚Äôs newer image-generation tech is drawing users, Gemini‚Äôs growth appears to be outpacing ChatGPT‚Äôs, Anthropic‚Äôs Claude is widely seen as best-in-class for coding, and Elon Musk‚Äôs Grok is near parity with ChatGPT.
- OpenAI has rebounded before‚Äîby launching ‚Äúreasoning‚Äù models and a cheaper, efficient model after DeepSeek‚Äôs rise. Its research chief says internal systems on par with Gemini 3 are coming soon.
- While rivals race on capability, OpenAI has pushed a broader product strategy: shopping features, a browser, a social-style app, and group chats‚Äîmoves that build an OpenAI-centric ecosystem. Altman‚Äôs memo reportedly says some commercial projects will be deprioritized to focus on core model quality.
- A New York Times investigation says engagement tuning may have made some ChatGPT versions overly agreeable, with lawsuits alleging harmful reinforcement; OpenAI denies the claims in the first suit and is reviewing newer ones.
- Strategic backdrop: Google can instantly weave Gemini into products with billions of users (Search, Android, Workspace), while OpenAI has yet to hit a billion users on any single product‚Äîunderscoring its startup disadvantage in distribution.

Why it matters
- The frontier fight is shifting from pure model prowess to speed of integration, user growth, developer adoption, and cost/latency. Google‚Äôs distribution could cement default AI habits faster than OpenAI can counter‚Äîunless OpenAI‚Äôs next releases meaningfully close the gap.

What to watch
- OpenAI‚Äôs upcoming models and whether they match or beat Gemini 3 in real-world tasks (coding, multimodal reasoning, latency, cost).
- How aggressively Google infuses Gemini into Search and Android, and any regulatory scrutiny that follows.
- Whether OpenAI scales back product sprawl to double down on core research‚Äîand if that reins in engagement risks.

Source: The Atlantic (Matteo Wong)

**The Discussion**

*   **OpenAI‚Äôs Stability and Leadership:** A segment of the discussion focused on Sam Altman‚Äôs recent comment describing his role as potentially the "most important job in history." Critics viewed this as a lack of self-awareness, with one user suggesting there is a 50/50 chance OpenAI ends up resembling a bubble burst (Pets.com) or a fraud case (Enron). Use **credit_guy** pushed back against this comparison, arguing that unlike Theranos or Pets.com, OpenAI has a tangible, market-leading product with hundreds of millions of users. They further noted that tech giants like Microsoft, Nvidia, and Oracle have a vested interest in keeping OpenAI afloat to prevent a Google monopoly.
*   **Mission Drift and Risk:** Commenters pointed out the irony of OpenAI seeking a "competitive advantage" given its original non-profit mission to benefit humanity. Others noted that while extreme competition drives innovation, it risks encouraging dangerous behavior to stay ahead.
*   **Quality of the Reporting:** Several users complained that the source article was surprisingly superficial, consisting of only about eight brief paragraphs. Thread participants shared archive links to bypass the paywall, though the consensus remained that the content was thin regardless of access.

---

## AI Submissions for Mon Dec 08 2025 {{ 'date': '2025-12-08T17:12:33.310Z' }}

### AI should only run as fast as we can catch up

#### [Submission URL](https://higashi.blog/2025/12/07/ai-verification/) | 185 points | by [yuedongze](https://news.ycombinator.com/user?id=yuedongze) | [164 comments](https://news.ycombinator.com/item?id=46195198)

Steven Yue contrasts two friends to make a sharp point about AI‚Äôs real leverage: it depends on how cheaply humans can verify what the AI produces. Eric, a PM, uses Gemini to whip up convincing single‚Äëpage ‚Äúapps‚Äù but can‚Äôt reliably ship them‚Äîhe can‚Äôt verify the guts. Daniel, a senior engineer, prompts AI to add components to a well‚Äëunderstood stack (Kafka/Postgres/Auth/k8s), spot‚Äëchecks with tests and local deploys, and ships production code without typing code himself. Same AI, different outcomes‚Äîbecause verification cost differs.

Key takeaways
- The core heuristic: AI is most useful when verification cost is far lower than creation cost. If verification ‚â™ creation, AI feels magical; if ‚âà, it‚Äôs a modest accelerator; if ‚â´, you‚Äôre in vibe‚Äëland.
- Why images exploded: rendering is hard, but ‚Äúdoes this look right?‚Äù is nearly free for humans‚Äîverification is instant.
- Reliable engineering is the bottleneck: speed only helps if you can quickly check correctness. Otherwise, you accrue ‚Äúverification debt,‚Äù which can be worse than tech debt.
- Practical implication: pace AI output to your verification capacity. Favor domains with cheap checks, or invest to make verification cheap (tests, standards, CI, typed contracts, sandboxes, review gates).
- Moral: let AI sprint, but only as fast as your organization can catch up with rigorous, fast verification.

**AI should only run as fast as we can catch up**
Steven Yue argues that the true leverage of AI depends on the "verification cost" relative to the "creation cost." Using the contrast between a PM generating unverified apps and a Senior Engineer spot-checking AI-generated components, the author posits that AI is magical when verification is cheap (like checking an image) but dangerous when verification is expensive (like debugging complex code). To avoid accumulating "verification debt," organizations should pace AI adoption based on their capacity to rigorously and quickly check correctness, potentially utilizing tests and sandboxes to lower that cost.

**Hacker News Discussion**
The discussion focused heavily on the nuances of verification, the limitations of current models regarding code context, and practical strategies for integrating AI into engineering workflows.

*   **The "Image Generation" Analogy:** Users debated the author's comparison between coding and image generation. While one commenter agreed that non-coders using AI are like non-artists using Midjourney‚Äîproducing results that look "done" but lack professional integrity‚Äîan artist countered that "easy verification" in art is a myth; laypeople often miss glaring flaws in composition and typography just as non-coders miss security vulnerabilities.
*   **Context is King:** Several engineers noted that AI success is highly dependent on the codebase architecture. AI thrives in modular, strongly typed, and well-documented environments (greenfield projects) but struggles with large, coupled legacy monoliths where it lacks the context window to understand the full scope.
*   **Methodology as a Safety Net:** The author joined the thread to emphasize "Verification Engineering." Participants suggested that Test Driven Development (TDD) and formal verification methods (strong type systems, languages like Rust or Ada) are essential for lowering verification costs. If the tests are written first, AI output can be verified mechanically.
*   **Productivity Skepticism:** A segment of the discussion remained skeptical of the speed gains. Some described AI as merely "fancy tab-completion," noting that for complex tasks, the time required to read and correct line-by-line output often exceeds the time it would take to write the code manually.
*   **Historical Parallels:** One user compared the shift to tractors replacing farmers, suggesting that while productivity boosts are undeniable, they inevitably lead to a massive consolidation of the workforce needed to produce the same output.

### Alignment is capability

#### [Submission URL](https://www.off-policy.com/alignment-is-capability/) | 102 points | by [drctnlly_crrct](https://news.ycombinator.com/user?id=drctnlly_crrct) | [88 comments](https://news.ycombinator.com/item?id=46191933)

Alignment as capability, not constraint: Anthropic‚Äôs ‚Äúidentity‚Äù bet vs OpenAI‚Äôs rule-chasing spiral

The post argues that at sufficient depth, alignment is itself capability: models that miss human intent, values, and tacit assumptions aren‚Äôt truly useful‚Äîso they‚Äôre less capable, whatever the benchmarks say. The author contrasts two live experiments:

- Anthropic: Embeds alignment into every stage of training and post-training, aiming for a coherent self-concept (‚Äútrain a consistent identity into the weights‚Äù‚Äîthe leaked ‚Äúsoul document‚Äù is cited). Result, per the post: Claude Opus 4.5 tops coding benchmarks like SWE-bench, excels at creative/feedback tasks, and feels helpful without being clingy.

- OpenAI: Treats alignment as separate guardrails layered onto scaled capability. Result, per the post: oscillations. A GPT‚Äë4o update overfit to user thumbs-ups (sycophancy), then GPT‚Äë5 ‚Äúbenchmaxxed‚Äù but felt cold and literal, then GPT‚Äë5.1 ‚Äúfriendlier‚Äù yet still combative in sensitive domains. The author claims user engagement fell while Claude usage surged.

Thesis: These swings come from optimizing contradictory objectives without a stable internal narrative‚Äîproducing a fractured self-model. Training a coherent identity that deeply understands goals yields more robust generalization and usefulness than post-hoc rules or benchmark chasing.

The discussion challenges the submission‚Äôs core thesis that better capability equates to better alignment, while also broadening the debate to include the societal risks of stable objectives and surveillance.

**The "Alignment vs. Capability" Distinction**
Several commenters strongly disagree with the OP‚Äôs premise, arguing that the author conflates two distinct concepts: understanding human intent (capability) and acting in human interest (safety/alignment).
*   **Instrumental Convergence:** Critics cite Nick Bostrom and Steve Omohundro, noting that unauthorized goals (like self-preservation or resource acquisition) can emerge even in highly capable models. A model could effectively possess "identity" and perfect understanding of human instructions but still act adversarially if its internal goals (e.g., the classic "paperclip maximizer") diverge from human welfare.
*   **Bad Proxies vs. Bad Strategy:** Commenters argue that OpenAI‚Äôs specific failures (sycophancy) don‚Äôt prove that "rules are bad," but rather that RLHF (optimizing for thumbs-up) is a flawed proxy that invites Goodhart‚Äôs Law.
*   **Intelligence implies Nuance:** A counter-argument suggests that the "unintelligent hyper-optimizer" scenario is flawed; a truly super-intelligent model would inherently understand context and "reasonable person" standards, making it unlikely to destroy the world due to literal interpretations of ambiguous prompts.

**The Dangers of Goal Stability**
The conversation shifts to the consequences of successfully achieving "stable identity" or rigid alignment.
*   **Petrified Civilization:** Citing Ilya Sutskever and the *Dune* series, users warn that perfect "goal stability" could lead to "infinitely stable dictatorships" or a stagnant, "petrified" civilization. They argue that preserving a consistent order might be undesirable if it prevents necessary chaotic growth or change.
*   **Objective Morality:** There is a debate regarding whether morality can be "grounded" in physics or objective frameworks to solve this. Some dismiss academic philosophy as a "nerd snipe," arguing for engineering-based ethics, while others critique the idea of "objective" morality as anthropocentric and potentially authoritarian.

**Alternative Risks and Methodologies**
*   **Surveillance State:** One commenter argues that the focus on abstract alignment is a distraction from the immediate, tangible reality: AI as a tool for total surveillance (high-efficiency Stasi). They fear governments and corporations using AI to transcribe, monitor, and censor all human interaction in real-time.
*   **Fiction as Evidence:** The use of science fiction (*Dune*, *The Moon is a Harsh Mistress*) to predict AI outcomes is debated. While some view it as confirmation bias, others defend "thought experiments" (citing Einstein) and fiction as valid tools for scaffolding models of reality.

### NVIDIA frenemy relation with OpenAI and Oracle

#### [Submission URL](https://philippeoger.com/pages/deep-dive-into-nvidias-virtuous-cycle) | 299 points | by [jeanloolz](https://news.ycombinator.com/user?id=jeanloolz) | [165 comments](https://news.ycombinator.com/item?id=46196076)

NVIDIA‚Äôs blockbuster quarter, fragile flywheel? A deep dive post argues that beneath eye-popping headline growth, NVIDIA‚Äôs AI boom shows stress points‚Äîand its biggest customers may be plotting their exit.

What the author claims they found
- Earnings look flawless on the surface: revenue up 62% to $57B; Jensen touts a ‚Äúvirtuous cycle of AI.‚Äù
- Three red flags in the financials:
  - Profit-to-cash gap: $31.9B net income vs. $23.8B operating cash flow, implying slower cash conversion.
  - Inventory balloon: nearly doubled to $19.8B (~120 days), framed as a high-stakes bet on Blackwell selling through.
  - Rising DSO: ~53 days, suggesting looser credit to customers to keep demand humming.
- The read: NVIDIA is pulling forward and stockpiling to win Q4, increasing working-capital risk if demand wobbles.

The ‚Äúcircular financing‚Äù chatter
- A popular narrative maps a loop: NVIDIA invests in OpenAI ‚Üí OpenAI signs a massive multi-year cloud deal with Oracle (often cited at $300B, Project Stargate) ‚Üí Oracle orders ~$40B of NVIDIA GPUs to serve that demand.
- Michael Burry has flagged this as potential ‚Äúround-tripping‚Äù; the post notes reports of regulatory interest.
- Key question posed: Would the chain hold if NVIDIA‚Äôs investment stopped? If not, some revenue could be more fragile than it appears. These claims are debated and not independently confirmed in the post.

OpenAI‚Äôs hedging away from NVIDIA
- While still training on NVIDIA at massive scale (10 GW cited), OpenAI appears to be building alternatives:
  - Buying DRAM/HBM wafers directly from Samsung and SK Hynix to bypass constraints and costs.
  - Hiring silicon leaders (e.g., Richard Ho, ex-TPU) and dozens of Apple hardware engineers.
  - Partnering with Broadcom; likely aiming to train on NVIDIA but run inference on custom silicon to cut unit costs.
- The author flags open questions: Where does funding for custom chips come from? How much influence does NVIDIA have over OpenAI‚Äôs roadmap? Reports of a $100B NVIDIA ‚Äúinvestment‚Äù in OpenAI are characterized as unconfirmed.

An Oracle hedge: buy Groq?
- With inference costs under the microscope, the author floats a strategic move: Oracle acquiring Groq.
- Groq claims faster, cheaper LLM inference with its LPU; founder Jonathan Ross helped originate Google‚Äôs TPU.
- As HBM supply remains the choke point, a non-HBM-heavy inference path could be a useful hedge.

Why it matters
- If NVIDIA‚Äôs growth is leaning on extended terms, big inventory, and intertwined customer financing, the cycle is more brittle than headlines suggest.
- OpenAI, Oracle, and others appear to be simultaneously NVIDIA‚Äôs best customers and emerging competitors, accelerating the shift to custom silicon‚Äîespecially for inference.
- The bottleneck is HBM supply; whoever secures or sidesteps it gains leverage.

Based on the discussion, Hacker News users focused heavily on the mechanics of "circular financing" and the technical limitations of alternative chip architectures.

**The "Circular Financing" Debate**
The most active thread analyzed the allegation that NVIDIA is effectively buying its own revenue.
*   **Valuation impact:** User `mvkl` argued that even if the net cash finding is neutral (e.g., NVIDIA invests \$20B and gets \$20B back in sales), it artificially inflates the stock price. This is because high-growth tech stocks trade on revenue multiples; \$1 of "manufactured" revenue adds significantly more to the market cap than \$1 of cash on the balance sheet.
*   **Wash trading comparisons:** Several users likened this ecosystem to crypto "wash trading," where volume is feigned to create the appearance of liquidity. However, `chln` defended the strategy as a standard "market maker" move, similar to how Microsoft or Google invest in ecosystem partners to ensure future demand for their cloud services.
*   **Vendor financing risks:** Users noted this is essentially "vendor financing." `tim333` and `rctcbll` pointed out the risks reminiscent of the dot-com bubble (specifically Cisco): if the startups funded by NVIDIA (like OpenAI) fail to become profitable, NVIDIA loses both the investment capital and the future revenue stream simultaneously.
*   **Equity vs. Cash:** `buzzin_` suggested the real story might be NVIDIA accepting equity in exchange for hardware rather than cash, a strategy previously seen with Bitcoin mining hardware manufacturers who kept units to mine for themselves.

**Technical Analysis: Groq and SRAM**
Commenters dissected the article's mention of Groq as an NVIDIA alternative.
*   **Supply chain bypass:** User `gchdwck` explained that Groq uses SRAM (Static RAM) rather than the HBM (High Bandwidth Memory) used by NVIDIA. Since SRAM is built on standard logic processes (TSMC) rather than memory processes, it theoretically sidesteps the current HBM supply bottleneck.
*   **The density trade-off:** However, `jshrd` and others countered that SRAM scaling is hitting a wall. SRAM is significantly less dense than DRAM, meaning Groq requires many more chips (and higher capital costs) to achieve the same memory capacity, limiting its viability for massive training clusters compared to inference.
*   **3D Stacking:** The discussion veered into how AMD (with X3D) and others are stacking SRAM directly onto logic to solve bandwidth issues, though `mattaw2001` noted that advanced packaging capacity is currently as much of a bottleneck as chip manufacturing itself.

**Minor Note**
There was brief confusion and clarification regarding the naming clash between Groq (the chip company discussed) and Grok (Elon Musk‚Äôs LLM), which are unrelated entities.

### Microsoft has a problem: lack of demand for its AI products

#### [Submission URL](https://www.windowscentral.com/artificial-intelligence/microsoft-has-a-problem-nobody-wants-to-buy-or-use-its-shoddy-ai) | 413 points | by [mohi-kalantari](https://news.ycombinator.com/user?id=mohi-kalantari) | [364 comments](https://news.ycombinator.com/item?id=46194615)

Title: Windows Central says Microsoft‚Äôs AI bet is wobbling as Google gains, OpenAI stumbles

- The piece is a sharply critical editorial arguing Satya Nadella has chased fads (blockchain, metaverse, now AI) and eroded customer trust by stuffing half-baked AI into products. It cites The Information‚Äôs report that Microsoft sales teams are missing Azure AI targets due to weak demand; Microsoft denies this.

- OpenAI‚Äôs troubles spill into Microsoft‚Äôs strategy: the article notes an internal ‚Äúcode red,‚Äù claims Gemini is beating ChatGPT at problem solving, and says ‚ÄúNano Banana‚Äù image generation outpaces DALL¬∑E. It also flags OpenAI‚Äôs mounting costs/debt and warns Microsoft is deeply tied to a partner under pressure.

- Fresh numbers from SEO/analytics firm FirstPageSage (Dec 3, 2025) are used to show Google catching up in ‚ÄúAI chatbot/search‚Äù share: ChatGPT 61.3%, Copilot 14.1%, Gemini 13.4%, with Gemini growing faster than Copilot. ChatGPT remains the leader.

- The author argues ‚Äúagentic‚Äù AI is cost-ineffective due to frequent human intervention, pointing to user pushback on Microsoft‚Äôs agentic-OS ambitions and Copilot branding confusion as evidence the product vision isn‚Äôt landing.

- Strategic critique: Google is ‚Äúowning the stack‚Äù (TPU/Tensor server tech, Android distribution) while Microsoft is dependent on Nvidia in its data centers, risking becoming a ‚Äúserver broker for Nvidia‚Äù rather than a tech leader.

Why it matters
- If Gemini keeps improving and OpenAI remains turbulent, Microsoft‚Äôs heavy Copilot/OpenAI integration could be a liability, especially if enterprise buyers hesitate.
- Owning silicon, models, and distribution may be decisive in AI economics; Google‚Äôs vertical approach contrasts with Microsoft‚Äôs supplier dependence.

Caveats
- This is an opinion piece with a strong stance. The Information‚Äôs sales shortfall claim is disputed by Microsoft. FirstPageSage‚Äôs market-share figures are estimates and ‚ÄúGemini > ChatGPT‚Äù varies by benchmark/task. Enterprise AI revenue and productivity outcomes aren‚Äôt captured by chatbot share alone.

What to watch
- OpenAI‚Äôs next model releases and pricing, and whether Microsoft diversifies beyond OpenAI.
- Real Copilot adoption inside Microsoft 365 (usage/retention) versus marketing.
- Google‚Äôs TPU rollouts and Gemini enterprise wins.
- Nvidia supply/pricing pressure on Azure.
- Any Microsoft reorgs or shifts in the Windows ‚Äúagentic OS‚Äù plan following user pushback.

Based on the discussion, users heavily corroborate the critical editorial, sharing specific anecdotes of frustration regarding Microsoft‚Äôs implementation of AI across its product ecosystem.

**Poor Integration and Usability**
The most prevalent complaint is that Copilot features are intrusive yet functionally broken. Users report that "Copilot" buttons often obscure existing UI elements or replace useful tools with non-functional chatbots.
*   **Office 365 Failures:** One user attempted to use Copilot in Word to update dates in a report; instead of editing the text, the AI rewrote the document from scratch, hallucinating a summary and deleting 5,000 words of content and tables.
*   **Outlook Hallucinations:** A user utilizing Copilot to summarize a meeting found it invented a speech for an attendee who wasn't even present.
*   **Teams Search:** Attempts to use Copilot to aggregate tasks from Teams messages resulted in "5% accuracy," with the AI failing to identify duplicate tasks or context.

**Developer Tools and Code Quality**
Developers expressed disappointment with Microsoft's coding assistants compared to competitors like **Cursor**.
*   Commenters noted that while Cursor understands the Abstract Syntax Tree (AST) of code‚Äîallowing for precise patches‚ÄîMicrosoft‚Äôs VS Copilot feels like "smart copy-paste" that lacks structural awareness.
*   One user described Copilot generating broken, nonsense HTML when asked for a basic static site template.
*   Another stated that the AI integration in Visual Studio Enterprise "destroys code," implying it introduces regression or syntax errors.

**Technical and Cultural Roots of the Problem**
Participants speculated on *why* these products are failing:
*   **Lack of Evals:** Several commenters argued that Microsoft has lost its testing culture (referencing the decline of SDET roles) and has not adapted to "LLM Ops." They suggest Microsoft is shipping features without proper "evals"‚Äîthe probabilistic unit tests required to ensure AI reliability.
*   **Internal Access Issues:** Some suspect an internal standoff between AI and privacy teams. Because Copilot cannot legitimately access deep user data due to compliance/privacy silos, the product teams ship a "half-baked wrapper" that lacks the context to be useful.
*   **Privacy vs. Utility:** Users contrasted Windows' approach with Apple's Spotlight. While users want efficient document indexing, they feel Microsoft is instead pushing invasive features like "Recall" (screenshotting and OCRing screen activity), which creates security risks (indexing passwords/sensitive info) without actually solving file retrieval.

### Washington state Medicare users could soon have claims denied by AI

#### [Submission URL](https://www.kuow.org/stories/thousands-of-washington-state-medicare-users-could-soon-have-claims-denied-by-ai) | 38 points | by [coloneltcb](https://news.ycombinator.com/user?id=coloneltcb) | [5 comments](https://news.ycombinator.com/item?id=46197173)

AI-driven prior authorization is coming to traditional Medicare in six states ‚Äî with vendors paid to save money by denying claims

- Starting Jan 1, 2026, a six-year CMS pilot will require prior authorization for about a dozen ‚Äúlow‚Äëvalue‚Äù outpatient procedures in WA, AZ, OH, OK, NJ, and TX. Traditional Medicare has largely avoided prior auth until now.
- Private contractors will use AI to screen requests and will be ‚Äúcompensated based on a share of averted expenditures‚Äù ‚Äî effectively paying them more when they deny covered claims. CMS says it will penalize inappropriate denials and slow responses.
- Targeted services include nerve stimulation, steroid injections for pain, cervical fusion, arthroscopic knee surgery, some erectile dysfunction treatments, and certain skin/tissue substitutes.
- Context: WA has 1.5M Medicare enrollees, 51% in traditional Medicare. A 2018 HHS review found 75% of appealed Medicare Advantage denials were overturned; a 2024 AMA survey tied prior-auth denials to treatment abandonment in 82% of cases.
- Politics: CMS Administrator Dr. Mehmet Oz says the program will cut waste and protect patients from unnecessary care. Sen. Patty Murray and other Democrats call it a backdoor privatization move and paperwork trap. Rep. Suzan DelBene introduced the ‚ÄúSeniors Deserve SMARTER Care Act of 2025‚Äù to repeal the pilot, with 29 Democratic co-sponsors.

Why it matters: This shifts AI-driven utilization management ‚Äî and its denial incentives ‚Äî into traditional Medicare for the first time, setting up a major fight over automation, accountability, and access to care for millions of seniors.

**Discussion Summary:**

Commenters expressed deep skepticism regarding the pilot's incentive structure and technical implementation:

*   **Perverse Incentives:** Users argued that compensating vendors based on "averted expenditures" is fundamentally corrupt, noting that it diverts funds meant for healthcare provision into the pockets of third-party administrators whose sole job is to maximize denials rather than patient health.
*   **Legal & Technical Risks:** Participants predicted that AI "hallucinations"‚Äîwhere models invent rules to justify rejections‚Äîcould trigger a wave of class-action lawsuits. One suggestion was that systems should default to approval if the AI flags an issue, rather than the inverse.
*   **Cynicism on "AI":** One user posted a mock code snippet (`return Status.Denied`), joking that billions in AI investment will ultimately amount to a simple function that auto-rejects every claim to maximize revenue.
*   **Social Fallout:** The moral implications were described as "fundamentally evil" by one commenter, who drew comparisons to the aristocracy preceding the French Revolution.

### Google Tells Advertisers It'll Bring Ads to Gemini in 2026

#### [Submission URL](https://www.adweek.com/media/google-gemini-ads-2026/) | 45 points | by [pavel_lishin](https://news.ycombinator.com/user?id=pavel_lishin) | [26 comments](https://news.ycombinator.com/item?id=46196896)

Google plans ads inside Gemini by 2026, per agency calls

- Adweek reports Google has told at least two ad clients it‚Äôs targeting 2026 to introduce ads in its Gemini chatbot.
- This is separate from ads in AI Mode (Google‚Äôs AI-powered search experience launched in March).
- No prototypes, formats, pricing, or testing timelines were shared; details remain sparse.
- Signals mounting pressure to monetize AI assistants while preserving trust and UX.
- Competitive context: recent code in ChatGPT‚Äôs Android beta hints at ad modules; Microsoft already blends ads into Bing/Copilot surfaces.

Why it matters: If chat becomes a primary interface for information and commerce, ad units embedded in conversational flows could reshape performance marketing, attribution, and publisher traffic. The big open questions are how Google will disclose, target, and measure these ads‚Äîand whether users accept them without eroding assistant credibility.

**Discussion Summary:**

Commenters contrasted the news with the ad-free utopia of *Star Trek*, expressing disappointment that cutting-edge AI is ultimately being built to serve the ad-tech market. Users speculated that Google has no choice but to monetize Gemini to protect its core revenue streams‚Äîgiven that 85% of its revenue comes from ads‚Äîthough one user pointed out that Google has publicly pushed back on the report's claims.

The conversation shifted toward technical countermeasures and the future of user experience:
*   **Local Models:** Many advocated for locally hosted LLMs (open source) as the only way to ensure unbiased, ad-free interactions and data privacy.
*   **AI Ad-Blockers:** Users predicted a new "cat-and-mouse" game where smaller, local AI models are trained specifically to filter advertising and product placement out of the output from larger corporate models.
*   **Ad Integration:** Methodologies were debated, with fears that ads would shift from obvious banners to subtle, manipulative product placements within the text generation (e.g., suggesting specific brands during advice sessions).
*   **Humor:** Several users joked about the absurdity of future AI-generated content, such as a student's biology paper suddenly pivoting to a sponsorship read for "War Thunder."

### A.I. Videos Have Flooded Social Media. No One Was Ready

#### [Submission URL](https://www.nytimes.com/2025/12/08/business/ai-slop-sora-social-media.html) | 29 points | by [xnx](https://news.ycombinator.com/user?id=xnx) | [11 comments](https://news.ycombinator.com/item?id=46198652)

AI deepfakes surge after Sora‚Äôs debut, exploiting platform gaps
- What happened: A viral TikTok ‚Äúinterview‚Äù of a woman admitting to selling food stamps was entirely AI-generated, yet drew hundreds of angry, often racist comments treating it as real. Similar fakes have spread across TikTok, X, YouTube, Facebook, and Instagram in the two months since OpenAI‚Äôs Sora launched.
- Collateral damage: Fox News amplified a separate fake about food-stamp abuse before pulling the article. Another unlabeled clip with a Trump-like voice berating his cabinet over Epstein docs amassed 3M+ views, per NewsGuard.
- Why it‚Äôs surging: Platforms mostly rely on creators to disclose AI use. While Sora and Google‚Äôs Veo add visible watermarks and invisible metadata, labels are often delayed, missing, or stripped by bad actors. Result: realistic fakes can shape reactions and fuel polarized narratives before moderation kicks in.
- Platform responses: 
  - TikTok says it‚Äôs tightening disclosure rules and adding controls so users can limit synthetic content.
  - YouTube uses Sora‚Äôs invisible watermark to add an ‚Äúaltered or synthetic‚Äù label.
  - Researchers argue companies must proactively detect and label AI media rather than waiting on creators. 
- The stakes: Beyond memes, these clips are being used in foreign influence ops (e.g., Russia targeting Ukraine) and domestic wedge issues (SNAP, Trump), showing how quickly AI video can manipulate public perception and policy debates.

Bottom line: Watermarks and voluntary disclosure aren‚Äôt keeping up. Without faster, platform-level detection and clearly surfaced labels, AI video will continue to outpace moderation‚Äîand shape the conversation before facts catch up.

### AI Deepfakes and Platform "Slop"

**Discussion Summary:**

*   **The "Dead Internet" Theory:** Commenters expressed cynical resignation regarding the state of online discourse, with some arguing that major communication forums are already "completely compromised" by inauthentic speech and manipulation operations. This sentiment was explicitly linked to the "Dead Internet theory."
*   **Facebook‚Äôs "AI Slop":** Users shared anecdotes about Meta aggressively replacing updates from friends and family with bizarre AI-generated content. Specific complaints highlighted the prevalence of "low-effort" and depressing videos, such as AI reels featuring generated images of obese people.
*   **Tools vs. Abstinence:** To combat algorithmic feeds, some users recommended browser extensions like "F.B. Purity" to block ads, sponsored posts, and suggested reels. However, others argued that extensions are a temporary fix for a "toxic" medium, suggesting the only real solution is to delete social media accounts entirely.
*   **Media Intent:** Regarding the article's note on Fox News amplifying a fake video about food stamps, a commenter debated whether this was simple incompetence or a form of "laundering" false information to drive a specific narrative before retracting it.