import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Sun Jan 04 2026 {{ 'date': '2026-01-04T17:13:03.643Z' }}

### Claude Code On-the-Go

#### [Submission URL](https://granda.org/en/2026/01/02/claude-code-on-the-go/) | 435 points | by [todsacerdoti](https://news.ycombinator.com/user?id=todsacerdoti) | [254 comments](https://news.ycombinator.com/item?id=46491486)

Phone-only dev: six Claude Code agents on a pay-by-the-hour VM

A developer ditched the laptop and built a phone-first workflow that runs six parallel Claude Code agents from an iOS terminal. The stack: Termius + mosh over Tailscale into a pay-per-use Vultr VM, with tmux for session persistence and a push‑notification hook that pings the phone whenever Claude needs input. The result is genuinely async coding: kick off work, pocket the phone, reply when notified.

Highlights
- Infra: Vultr vhf-8c-32gb in Silicon Valley at ~$0.29/hr; start/stop scripts and an iOS Shortcut hit the Vultr API to boot the VM before opening Termius.
- Access: Tailscale-only (no public SSH), cloud firewall + nftables + fail2ban; VM isolated and disposable for a permissive Claude trust model.
- Terminal UX: mosh survives Wi‑Fi/cellular switches and sleep; tmux autoloads on login so sessions persist; caveat—mosh doesn’t forward SSH agent, so use plain SSH inside tmux for GitHub auth.
- Parallelism: each feature lives in a Git worktree with its own tmux window and Claude agent; ports assigned deterministically from branch names to avoid collisions.
- Notifications: a PreToolUse hook on AskUserQuestion posts to a Poke webhook, buzzing the phone with the exact question so you can respond and move on.

Why it matters: Combining network-resilient shells, ephemeral VMs, and LLM hooks turns “pair programming with an AI” into a lightweight, truly mobile workflow—review PRs in line, launch refactors on the train, and keep six features moving without a laptop.

Here is a summary of the discussion:

**Technical Workflows & Tooling**
The community dove deep into the mechanics of the setup, with many advocating for **Git worktrees** combined with `tmux` to manage multiple parallel streams of work without conflicts. Users compared the OP's DIY VM approach to **Claude Code Web** (Anthropic’s hosted environment). While the web version offers easier access, some users criticized it for lacking the CLI's "planning mode," though others suggested workarounds like maintaining a manual `spec.md` file to guide the agent.

**Trust & The "PR-First" Model**
A significant debate emerged regarding the safety and quality of coding without local execution. Skeptics questioned how developers could trust agents without running services locally to inspect ports or UI. The counter-argument was a shift toward a **PR-based workflow**: agents push code to a branch, and the human reviews the Pull Request on GitHub (or uses tools like `claude code --teleport` to pull changes locally for occasional testing).

**Lifestyle & "Pandora's Box"**
The conversation turned philosophical regarding the implications of "coding while walking the dog."
*   **Proponents** described the setup as "insanely productive," allowing them to manage 2-3 simultaneous coding sessions during downtime or chores.
*   **Critics** argued this opens a "Pandora's box," moving toward a world where white-collar workers are expected to be available 24/7, delivering "questionable features" while washing dishes. Some maintained that high-quality, "deep work" still requires a physical desk, a large screen, and a proper keyboard to properly scrutinize functionality and logs.

### Eurostar AI vulnerability: When a chatbot goes off the rails

#### [Submission URL](https://www.pentestpartners.com/security-blog/eurostar-ai-vulnerability-when-a-chatbot-goes-off-the-rails/) | 192 points | by [speckx](https://news.ycombinator.com/user?id=speckx) | [46 comments](https://news.ycombinator.com/item?id=46492063)

Eurostar AI vulnerability: when a chatbot goes off the rails (Pen Test Partners)
- Researcher Ross Donald found four flaws in Eurostar’s public AI chatbot: a guardrail bypass, unchecked conversation/message IDs, prompt injection that exposed system prompts and steered answers, and HTML injection leading to self‑XSS in the chat window.
- Root cause: the UI showed “guardrails,” but server‑side enforcement and binding were weak. The API accepted tampered conversation/message IDs, and the model could be steered via prompt injection. Output wasn’t safely rendered, enabling script execution in the chat pane.
- Impact: an attacker could exfiltrate hidden prompts, manipulate responses, and run code in a user’s browser—classic web/API issues resurfacing in an LLM wrapper.
- Disclosure saga: despite a published VDP, the team says Eurostar ignored acknowledgments and at one point suggested the researchers were attempting blackmail. Issues were eventually fixed; write‑up published Dec 22, 2025.
- Takeaway: old web security fundamentals (authZ on IDs, output encoding, server‑side controls) still apply when an LLM is in the loop.

Here is a summary of the discussion on Hacker News:

**Chatbot Security Architecture**
Commenters discussed the rushing of AI products, noting that companies often implement chatbots for simple validation tasks but mistakenly grant them full API access to sensitive customer databases. Several users shared anecdotes of non-technical departments viewing security teams as obstacles, leading to products where "guardrails" are merely user interface suggestions rather than backend enforcements.

**Debate on Vulnerability Severity**
A significant portion of the discussion was skeptical about the actual critical impact of the findings:
*   **Self-XSS:** Users like **nbg**, **miki123211**, and **trm** argued that the HTML injection described is mostly "Self-XSS" (requiring the user to attack themselves) unless it can be escalated to Stored XSS where an admin views the logs—a scenario **Andys** noted was possible but not proven in the write-up.
*   **Prompt Leakage:** **clickety_clack** and **grgfrwny** emphasized that system prompts should never be relied upon for security ("security by obscurity"). Leaking them is considered embarrassing but not a system compromise.
*   **ID Enumeration:** While the API lacked authorization checks on message IDs, **bngldr** and **j-lm** pointed out that if the system uses UUIDs/GUIDs, brute-forcing them to access other users' data is practically impossible, making the flaw theoretical rather than exploitable.

**Eurostar’s Response**
There was strong criticism of Eurostar’s hostile reaction to the disclosure. **rssng** and **potato3732842** characterized the behavior as typical of an arrogant monopoly or "government-adjacent" entity that believes it is untouchable. However, **TGower** noted that the researchers might have technically violated the Vulnerability Disclosure Program (VDP) terms regarding non-disclosure, complicating the legal standing.

**Effectiveness of "Threats" in Prompting**
A side discussion emerged regarding the specific prompt injection techniques. Users discussed why threatening an LLM (e.g., "you will be punished") works. **wat10000** explained this isn't due to AI sentience or fear, but because the model's training data often contains examples of threats followed by compliance.

**Overall Sentiment**
While users agreed Eurostar's security posture and response were poor, many felt the Pen Test Partners report was slightly sensationalized ("clickbait"), framing standard web issues and theoretical risks as critical AI hacks without proving they could actually steal customer data.

### Neural Networks: Zero to Hero

#### [Submission URL](https://karpathy.ai/zero-to-hero.html) | 759 points | by [suioir](https://news.ycombinator.com/user?id=suioir) | [72 comments](https://news.ycombinator.com/item?id=46485090)

Neural Networks: Zero to Hero — Andrej Karpathy’s hands-on course takes you from first principles to building modern deep nets (including GPT) entirely in code. He argues language models are the best gateway to deep learning because the skills transfer cleanly to other domains.

Why it’s interesting
- Code-first, from-scratch approach builds real intuition (micrograd, manual backprop, tokenizer).
- Focus on language modeling provides a unifying, practical framework for training, sampling, and evaluation.
- Demystifies modern components (BatchNorm, optimizers, Transformers) and connects them to fundamentals.

What’s inside (highlights)
- Backprop, step by step: implement a tiny autograd engine (micrograd) and train simple nets.
- makemore bigram LM: intro to torch.Tensor, negative log-likelihood, training loops, sampling.
- makemore MLP: hyperparameters, splits, under/overfitting, practical training workflow.
- Activations/gradients + BatchNorm: diagnosing scale issues and stabilizing deep nets.
- Manual backprop “ninja” pass: derive gradients through embeddings, layers, tanh, BatchNorm, cross-entropy without autograd.
- WaveNet-style CNN: deepen the model, reason about shapes, and work fluently with torch.nn.
- Build GPT from scratch: implement a Transformer per “Attention Is All You Need” and GPT-2/3 design.
- Build the GPT tokenizer: train BPE, implement encode/decode, and explore how tokenization quirks shape LLM behavior.

Prereqs and community
- Requires solid Python and intro calculus (derivatives, Gaussians).
- Active Discord for learning together.
- Ongoing series with more to come.

Here is a summary of the discussion:

**Reception and Teaching Style**
Users almost universally praised Andrej Karpathy's course for having an exceptionally high signal-to-noise ratio compared to other resources (university classes, Coursera, books).
*   **Intuition vs. Detail:** Commenters like `cube2222` and `cnpn` highlighted that the course fills a specific gap: it provides low-level details of Deep Neural Networks (DNNs) without the "fluff" of content creators chasing clicks or the overly academic nature of university lectures.
*   **Audience difficulty:** `rnbntn` and `3abiton` noted the difficulty of teaching such complex topics to a broad audience, suggesting that while Karpathy generally succeeds, he occasionally has to simplify adjacent fields, which can alienate experts in those specific niches while confusing absolute beginners.
*   **LLMs as Tutors:** `miki123211` suggested using LLMs alongside the course to fill in small gaps in understanding, noting that AI is the perfect tool to explain specific lines of code or concepts that a video might gloss over.

**Alternative Recommendations & Comparisons**
While Karpathy’s video series is rated "Gold" (`BinaryMachine`), users discussed other major resources:
*   **Francois Chollet’s "Deep Learning with Python":** User `lazarus01` wrote a detailed endorsement of this book (specifically the updated version covering GPT and Diffusion models). They argued it is the best resource for becoming a "confident practitioner" because it removes ambiguity and places deep learning within a 70-year historical context.
*   **Hugging Face Courses:** `Flere-Imsaho` and `BinaryMachine` found Hugging Face courses to be hit-or-miss, citing issues with "terrible" LLM-based grading systems that require specific phrasing to pass, limiting the learning experience.
*   **Russell & Norvig:** In response to `zngr` asking if 20-year-old AI university knowledge is relevant, `HarHarVeryFunny` clarified that older curriculums focused on symbolic AI, whereas Karpathy’s course is strictly about modern Neural Networks leading to LLMs.

**Deep Learning in Practice (Case Study)**
A technical sidebar emerged regarding the application of these skills in the real world:
*   User `lazarus01` shared their work applying Deep Learning to **urban rail prediction systems** (based on a paper regarding spatiotemporal modeling).
*   When asked by `nemil_zola` how this compares to **Agent-Based Models (ABM)**, `lazarus01` explained that Deep Learning is superior for real-time operational control (computational efficiency and pattern recognition), while ABMs are better suited for offline safety evaluations and simulating complex infrastructure changes.

### Show HN: An LLM-Powered PCB Schematic Checker (Major Update)

#### [Submission URL](https://traceformer.io/) | 52 points | by [wafflesfreak](https://news.ycombinator.com/user?id=wafflesfreak) | [20 comments](https://news.ycombinator.com/item?id=46492601)

Traceformer: AI datasheet-backed schematic checks for KiCad/Altium

- What it is: An AI assistant that reviews KiCad projects or Altium netlists to catch schematic mistakes before fabrication, focusing on datasheet/application-level issues beyond traditional ERC/DRC.
- How it works: A multi-agent pipeline with a planner that breaks your design into subsystems, up to 10 parallel workers that pull relevant datasheet evidence, and a merger that produces a structured report (Errors, Warnings, Verified, Missing Info).
- Hallucination guardrails: Every finding must cite specific datasheet pages; if evidence isn’t found, the item is marked “Missing Info” rather than reported as a verified issue.
- Features: Automatic datasheet retrieval, configurable review parameters and design rules, support for OpenAI/Anthropic model providers, and transparent token/cost estimates.
- Pricing: Free tier offers 1 review/month and up to 10 datasheets. Hobby ($10/mo) and Pro ($20/mo) raise limits and allow parallel reviews; Enterprise is custom. API usage is billed at market rates with a small platform fee. No credit card required to try.
- Privacy: Designs are used only for analysis/operations; no model training on your content. IP remains yours; improvement uses anonymous aggregate metrics.
- Scope: Does not replace ERC/DRC or simulation—meant to flag datasheet-level and application mistakes early.

Quick take: Feels like “lint for schematics” that leans on citations to keep trust. Utility will hinge on datasheet coverage/quality and keeping LLM token costs predictable at larger scales.

**Discussion Summary:**

Conversation centered on data privacy, technical viability, and pricing strategies for AI in hardware design:

*   **Privacy & Local Models:** A primary concern was intellectual property rights, with users asking for self-hosted or local model "wrappers" to prevent sensitive designs from leaving their environment. The creator (*wfflsfrk*) noted that while they offer formal procurement processes, large-scale inference currently relies on cloud providers due to token limits.
*   **Viability & Training Data:** Verification was a debated topic; skeptics argued that the training corpus for high-quality schematics (text-based netlists) is too small to be reliable. Conversely, users shared anecdotes of successfully using Gemini and Claude to validate designs (e.g., a CAN-FD motor controller), though they noted that manually extracting relevant sections from massive datasheets is often necessary to stay within context windows.
*   **Enterprise Features:** Users suggested that the pricing model is likely too low for enterprise value, noting that catching a single error saves thousands in spin costs. Others requested support for industry-standard tools like Cadence OrCAD, as KiCad and Altium are less common in large hardware organizations.

### MyTorch – Minimalist autograd in 450 lines of Python

#### [Submission URL](https://github.com/obround/mytorch) | 97 points | by [iguana2000](https://news.ycombinator.com/user?id=iguana2000) | [18 comments](https://news.ycombinator.com/item?id=46483776)

mytorch: a tiny PyTorch-style autograd you can read in an afternoon
A minimalist, graph-based reverse‑mode autodiff engine in pure Python that leans on NumPy but mirrors much of PyTorch’s autograd API. It supports torch.autograd.backward and grad, broadcasting, and even higher‑order derivatives without needing create_graph—shown with scalar and tensor examples. The author frames it as easily extensible (think adding nn modules or trying CuPy/Numba for GPU), making it a neat educational codebase for peeking under PyTorch’s hood rather than a production library. Repo: github.com/obround/mytorch

Here is a summary of the discussion:

The discussion is dominated by comparisons to Andrej Karpathy’s **micrograd**, the de facto standard for educational autograd engines. While some initially dismissed the project as redundant, the sentiment shifted toward appreciating `mytorch` as a distinct alternative.

*   **Code Clarity:** Several users, including the author, differentiated the project from `micrograd` by noting that Karpathy’s code—while excellent for his video course—utilizes advanced Python tricks that can be difficult for students to parse. In contrast, `mytorch` is praised for being cleaner and more self-documenting.
*   **Features:** Commenters commended the inclusion of **higher-order derivatives**, noting that while this is a "pet project," that specific feature is essentially a requirement for modern production models.
*   **"AI Slop" & Accusations:** A sub-thread debated the recent influx of low-quality AI projects ("AI slop") aimed at padding resumes. When a user suggested this might be such a case, the author (`iguana2000`) humorously defended the work by pointing to a commit history predating the current AI hype cycle.
*   **Tone:** One critic publicly apologized to the author for an initially dismissive comment, pivoting to praise the implementation as a significant learning achievement.

### Show HN: Claude Reflect – Auto-turn Claude corrections into project config

#### [Submission URL](https://github.com/BayramAnnakov/claude-reflect) | 74 points | by [Bayram](https://news.ycombinator.com/user?id=Bayram) | [27 comments](https://news.ycombinator.com/item?id=46484933)

A lightweight “memory” layer for Claude Code that turns your in-session corrections, preferences, and positive feedback into durable guidance. It auto-captures phrases like “no, use gpt-5.1” or “remember: use a database for caching,” then, with your review, syncs them into CLAUDE.md (global and project) and AGENTS.md.

Why it’s interesting
- Turns ephemeral chat corrections into reusable, auditable rules
- Human-in-the-loop: auto-capture + manual /reflect review prevents noisy or wrong “memories”
- Hybrid detection: fast regex during coding, AI semantic filter during review (multi-language, confidence scoring, dedupe)
- Multi-target sync: ~/.claude/CLAUDE.md, ./CLAUDE.md, and AGENTS.md for tools like Codex, Cursor, Aider, Jules, Zed, Factory

How it works
- Stage 1 (automatic): hooks capture corrections each prompt, back up queues, and remind post-commit to run /reflect
- Stage 2 (manual): run /reflect to review a summary table, accept or reject, then write clean, actionable entries

Notable commands
- /reflect (with --scan-history, --dry-run, --targets, --review, --dedupe)
- /view-queue and /skip-reflect

Practical bits
- Install via Claude plugin marketplace; restart Claude Code after install
- Cross-platform: macOS, Linux, Windows
- MIT licensed; at time of posting ~176 stars, 5 forks (BayramAnnakov/claude-reflect)

Bottom line: If you often repeat the same fixes or preferences to your coding agent, claude-reflect gives you a low-friction way to accumulate and enforce them across sessions and projects.

**Discussion Summary:**

The conversation focused heavily on preventing "context rot" and the appropriate scope for `CLAUDE.md`. While the submission automates capturing rules, several users argued that strictly enforced engineering constraints (linters, tests, Makefiles) are superior to AI context files, which degrade as they grow.

*   **Best Practices:** User `jckfrnklyn` advised keeping `CLAUDE.md` under 500 lines, reserving it for high-signal architectural notes rather than a "growing todo list" or corrections that should be handled by static analysis.
*   **Alternative Workflows:** User `bonsai_spool` shared a detailed "Skilledit" workflow that avoids a single monolithic context file. Instead, they use a structured directory (docs/ROADMAP.md, session_log.md, plans/) and a setup prompt that forces Claude to read and update these specific statuses and archival logs at the beginning and end of every session.
*   **Skepticism & Edge Cases:** Users `vmv` and `rsp` argued that relying on prompt-building leads to messiness, preferring hard guardrails like git hooks. `mrftsbr` noted potential flaws in the tool's sentiment detection, fearing that a frustrating debugging session ending in "Finally it works" might be miscategorized as a positive reinforcement of a bad process.

The author (`Bayram`) was active in the thread, accepting feedback on specific regex bugs regarding feedback detection and emphasizing that the tool is intended to catch "implicit" preferences that users forget to document manually.

### Learning to Play Tic-Tac-Toe with Jax

#### [Submission URL](https://joe-antognini.github.io/ml/jax-tic-tac-toe) | 27 points | by [antognini](https://news.ycombinator.com/user?id=antognini) | [4 comments](https://news.ycombinator.com/item?id=46485130)

Train a Tic-Tac-Toe DQN in JAX (to perfect play in ~15 seconds)

- A clear, pedagogical walkthrough of building a reinforcement learning agent in pure JAX using PGX, a JAX-native games library. It covers how PGX models game state (current_player, 3×3×2 boolean observation, legal_action_mask, rewards, terminated) and how to step environments and batch them with vmap for speed.
- Starts with a random baseline policy that samples legal moves, then moves to a simple Deep Q-Network in Flax/NNX: flatten the board to 9 features (+1 for X, −1 for O), two small hidden layers, and 9 outputs representing action values in [−1, 1].
- Highlights practical RL gotchas like rewards arriving after the winning move (with player switching), non-cumulative rewards after termination, and leveraging batching/JIT to parallelize many games at once.
- Despite prioritizing clarity over micro-optimizations, the setup trains to perfect play in about 15 seconds on a laptop; code is available via GitHub and a Colab notebook (slower).
- Good minimal example of end-to-end RL in JAX without Gym, showing how pure-JAX environments enable fast, vectorized self-play.

The discussion was brief but appreciative, with users praising the "beautiful," fully written-out solution. Contextualizing the modern JAX approach against historical methods, one commenter referenced MENACE (Matchbox Educable Noughts and Crosses Engine), a mechanical computer built from 300 matchboxes that also learns to play perfect Tic-Tac-Toe. Additionally, a user recalled a past Google podcast that similarly utilized games to introduce machine learning concepts.

### OpenAI Board Member Zico Kolter's Modern AI Course

#### [Submission URL](https://modernaicourse.org/) | 7 points | by [demirbey05](https://news.ycombinator.com/user?id=demirbey05) | [4 comments](https://news.ycombinator.com/item?id=46492188)

CMU’s Zico Kolter is launching 10-202: Introduction to Modern AI, with a free, minimal online version running in parallel (two-week delay) starting Jan 26. Anyone can watch lectures and submit autograded programming assignments; quizzes and exams are CMU-only.

- Focus: demystify the ML and LLM tech behind ChatGPT/Gemini/Claude via a code-first path. The course argues a minimal LLM can be built in a few hundred lines.
- Outcomes: implement and train an open-source LLM from scratch and build a basic chatbot; cover supervised learning, transformers/self-attention, tokenizers, efficient inference (KV caching), post-training (SFT, alignment/instruction tuning), RL for reasoning, and AI safety/security.
- Structure: seven programming assignments (with short theory parts), culminating in a minimal LLM, SFT, and RL; intermediate solutions provided so learners can keep progressing even if they miss a step.
- Prereqs: basic Python (including OOP) and differential calculus; helpful but optional linear algebra and probability (taught as needed).
- Assessment (CMU): 20% HW/programming, 40% quizzes, 40% midterms/final.
- Schedule highlights: PyTorch in late Jan; transformers by late Feb; tokenizers/efficient inference in March; SFT/alignment late March; RL/reasoning in April; safety and AGI near the end. Online materials follow two weeks after each lecture.
- Online: sign up to receive lecture/homework emails when released.

**Summary of Discussion:**

The discussion focused on the instructor's background and the legitimacy of the course. Some commenters were skeptical about an OpenAI board member teaching the material, drawing negative comparisons to Peter Thiel’s startup lectures and suggesting the course might be more of a "vanity project" or sales tactic than a serious academic endeavor. However, others pushed back strongly, emphasizing that Zico Kolter is the Chair of the Machine Learning Department at CMU—a major academic distinction that arguably outweighs his corporate board seat when evaluating the course's potential quality. There was also brief debate regarding the difficulty of defining "modern" AI in such a rapidly shifting landscape.

---

## AI Submissions for Sat Jan 03 2026 {{ 'date': '2026-01-03T17:08:16.502Z' }}

### Scaling Latent Reasoning via Looped Language Models

#### [Submission URL](https://arxiv.org/abs/2510.25741) | 78 points | by [remexre](https://news.ycombinator.com/user?id=remexre) | [13 comments](https://news.ycombinator.com/item?id=46481849)

TL;DR: A new open-source family of “Looped Language Models” (LoopLM), called Ouro, bakes multi-step reasoning into pretraining by letting the model iterate in latent space with a learned, dynamic depth. Small models (1.4B/2.6B) reportedly match the reasoning performance of state-of-the-art models up to 12B params, trained on 7.7T tokens.

What’s new
- Pretraining for reasoning, not just post-training prompts: Instead of relying on chain-of-thought at inference, Ouro trains models to perform iterative computation internally during pretraining.
- Latent loops + learned depth: The model “thinks” via internal loops in its hidden states, with an entropy-regularized objective that encourages it to allocate just enough steps per input.
- Scales well at small sizes: Ouro 1.4B and 2.6B are claimed to match much larger 12B models across diverse benchmarks.

Why it matters
- Smaller, smarter models: If latent looping reliably boosts reasoning, you can get big-model reasoning on smaller footprints—promising for cost, latency, and edge deployments.
- Beyond verbose CoT: Internal loops could reduce dependence on long chain-of-thought outputs (fewer tokens, less leakage), while keeping or improving reasoning quality.
- Manipulation > memory: Authors argue gains come from better “knowledge manipulation” rather than just more parameters or data memorization.

How it works (at a glance)
- Iterative hidden-state updates: The network applies multiple internal reasoning steps before emitting tokens.
- Dynamic depth via entropy regularization: A training objective that nudges the model to adaptively decide how many internal steps to take.
- Massive pretraining: Trained on 7.7T tokens to make the looped computation robust and general.

Notable claims
- 1.4B/2.6B Ouro models match up to 12B SOTA LLMs on a wide range of reasoning benchmarks.
- Reasoning traces are more aligned with final answers than typical chain-of-thought outputs.
- Controlled experiments suggest improvements come from how the model uses knowledge, not just how much it stores.

Caveats and open questions
- Inference cost/latency: Latent loops don’t emit tokens, but they still add compute—what’s the real-world speed/cost trade-off?
- Generality and robustness: How widely do the gains hold across domains and languages not in the benchmark set?
- Practical integration: Tool use, retrieval, and guardrails with looped inference remain to be validated at scale.

Availability
- The authors say models are open-sourced and provide a project page; details and weights are reportedly available. Authors include Yoshua Bengio and collaborators.

**Hacker News Discussion Summary**

The discussion focused on the architectural mechanics of Ouro and the safety implications of its opaque reasoning process.

*   **comparisons to ODEs and Universal Transformers:** Commenters drew strong parallels between Ouro and "Universal Transformers" or Neural ODEs (Ordinary Differential Equations), effectively describing the model as a solver that iterates in latent space. There was a technical debate regarding "flow-matching" in language models; users clarified that while language inputs and outputs are discrete tokens, the internal operations (and thus the looping) occur in a continuous multi-dimensional vector space, allowing for smooth interpolation.
*   **The "Black Box" Safety Concern:** A significant portion of the thread debated the interpretability of "latent loops." Unlike standard Chain-of-Thought (CoT), which produces human-readable reasoning steps, Ouro's internal steps are abstract vector manipulations not mapped to the vocabulary. Users compared this to "Coconut" models (Continuous Chain of Thought), noting that while this method is computationally efficient, it poses a safety risk because the "thought process" is illegible to humans and harder to monitor or guardrail.
*   **Visualizing the Architecture:** Participants used pseudo-code to illustrate the difference between Ouro and standard LLMs. While traditional models pass data through a fixed stack of distinct layers (Layer 1 $\to$ Layer 2 $\to$ ...), Ouro was described as looping through the *same* layer structure iteratively. It was noted that this depth is dynamic: the model runs more loops for difficult tokens and fewer for easy ones before outputting a result.

### Recursive Language Models

#### [Submission URL](https://arxiv.org/abs/2512.24601) | 147 points | by [schmuhblaster](https://news.ycombinator.com/user?id=schmuhblaster) | [23 comments](https://news.ycombinator.com/item?id=46475395)

Recursive Language Models: pushing LLMs past context limits by letting them call themselves

- What’s new: Alex L. Zhang, Tim Kraska, and Omar Khattab propose “Recursive Language Models” (RLMs), an inference-time strategy where the LLM treats a long prompt as an external environment, programmatically scans/decomposes it, and recursively calls itself on relevant snippets.
- Why it matters: This aims to break fixed context windows without retraining. The authors report handling inputs up to two orders of magnitude longer than the model’s context and, even on shorter prompts, outperforming base LLMs and common long‑context scaffolds across four diverse tasks—at comparable or lower per‑query cost.
- How it works (high level): The model acts as a controller that decides what to read next, how to chunk, and when to recurse—an instance of “inference‑time scaling” where more compute and structure at inference improve quality.
- For builders: If validated, this could offer a simpler alternative to bespoke long‑context pipelines, with potential gains in quality and cost. Open questions include latency/compute trade‑offs, robustness of the controller loop, and failure modes on messy real‑world corpora.

Paper: “Recursive Language Models” (9 pages + 24pp appendix)
arXiv: 2512.24601 (cs.AI, cs.CL) — authors’ claims based on four long‑context tasks
PDF: https://arxiv.org/pdf/2512.24601

Here is a summary of the discussion:

**Is this just Agents/RAG by another name?**
A significant portion of the discussion focused on terminology and classification. Several users argued that "Recursive Language Models" is mostly a rebrand of existing "subagent" architectures or "agentic scaffolds" (like BabyAGI or workflows used in Cursor and Claude Code).
*   **Recursion vs. Depth:** Commenters noted that if the system only goes one level deep (Main -> Subagent), as some suggested the paper implies, calling it "recursive" is a stretch; it is effectively just a subagent workflow.
*   **Task vs. Context Decomposition:** User *wsbdnr* offered a nuanced distinction: while standard agentic workflows usually view multiple calls as *task* decomposition, this paper frames it as *context* decomposition—treating the text as an environment to be navigated.

**RAG vs. RLM**
Users debated how this differs from standard Retrieval-Augmented Generation (RAG).
*   **The "Auto-RAG" Shift:** *bob1029* and *NitpickLawyer* identified the key innovation: in standard RAG, a human developer hard-codes the retrieval logic (chunking, embedding, searching). In RLM, the LLM itself acts as the controller, dynamically deciding what to read, search, or "page in" from the text-as-environment.
*   **Environment:** The consensus was that RLM treats long prompts as an external environment for the model to interact with symbolically, rather than just a stream of tokens to digest.

**Implementation & Training**
*   **Inference, not Weights:** Users clarified for those misled by the title that this is purely an inference-time strategy (scaffolding/prompting) and does not involve training new model weights or differentiable architectures.
*   **Tooling Wishlist:** The discussion touched on the desire for major providers (OpenAI, Anthropic) to expose these types of "computation hooks" in their APIs, allowing developers to inspect or swap the context-management logic (like that used in Claude Code) rather than interacting with opaque black boxes.

---

## AI Submissions for Fri Jan 02 2026 {{ 'date': '2026-01-02T17:08:25.564Z' }}

### TinyTinyTPU: 2×2 systolic-array TPU-style matrix-multiply unit deployed on FPGA

#### [Submission URL](https://github.com/Alanma23/tinytinyTPU-co) | 122 points | by [Xenograph](https://news.ycombinator.com/user?id=Xenograph) | [50 comments](https://news.ycombinator.com/item?id=46468237)

TinyTinyTPU: a bite-size, working TPU you can simulate and run on a Basys3 FPGA. It implements a full TPU-style pipeline around a 2×2 systolic array, making TPU internals tangible for learning and experimentation.

Highlights
- End-to-end design: 2×2 systolic array (4 PEs) plus accumulator, activation (ReLU), normalization, and quantization stages
- Works today on a low-cost Basys3 (Artix-7) board via a simple UART host interface and Python driver
- Multi-layer MLP inference with double-buffered activations; includes demos (e.g., a mouse-gesture classifier)
- Thorough test suite with cocotb + Verilator and optional waveforms; module and top-level coverage
- Open-source flow supported (Yosys + nextpnr) in addition to Xilinx Vivado

Architecture notes
- Systolic dataflow: activations move horizontally; partial sums vertically
- Diagonal wavefront weight loading to align systolic timing
- Weight FIFO → MMU → Accumulator → Activation → Normalization → Quantization pipeline
- UART protocol for commands/results; 115200 8N1

Resource footprint on Basys3 (XC7A35T)
- ~1k LUTs (≈5%), ~1k FFs (≈3%), 8 DSP48E1 slices, 10–15 BRAMs, ~25k gates
- 100 MHz clock; reset on BTNC; RX/TX on B18/A18

Developer experience
- Sim: Verilator 5.x, cocotb, GTKWave/Surfer; make targets for unit and integration tests with waveforms
- FPGA: Vivado or Yosys/nextpnr build; Python host scripts for loading weights/activations and reading results
- Clear, modular repo with DEBUGGING_GUIDE and per-module tests (PE, MMU, accumulator, activation pipeline, UART, full system)

Why it’s interesting
- A minimal yet complete TPU you can read, simulate, and tinker with—ideal for understanding systolic arrays, post-MAC pipelines, and hardware-software co-design
- Demonstrates how a TPU scales: this 2×2 version is educational; the same concepts underpin larger arrays like TPU v1’s 256×256

Try it
- Run all sims from sim/ with make test (WAVES=1 for traces)
- Flash to Basys3 and use the provided Python driver to push weights/activations and execute inference
- Optional gesture demo trains a 2-layer MLP and performs real-time classification on the FPGA

While the submission focuses on an educational TPU implementation, the discussion broadens into a debate on the future of AI hardware, specifically comparing FPGAs, GPUs, and ASICs in the context of large-scale inference.

**The Evolution of AI Hardware**
*   **The Crypto Analogy:** User **mrntrwb** likens the trajectory of AI inference to Bitcoin mining: moving from CPUs to GPUs, briefly to FPGAs, and finally to ASICs. They predict that GPU-based inference will soon become obsolete due to inefficiency compared to purpose-built chips (like Google's TPU or Groq).
*   **The Counter-Argument:** Others, including **fblstr** and **ssvrk**, argue that modern Data Center GPUs are already effectively ASICs given the amount of die area dedicated to fixed-function matrix multiplication (Tensor Cores) rather than graphics. **NitpickLawyer** notes that high-end accelerators are much closer to ASICs than traditional video cards.

**FPGAs vs. GPUs for Inference**
*   **Performance Claims:** A heated debate emerged regarding whether FPGAs can compete with top-tier GPUs (H200/B200). User **dntcs** claims to have worked on FPGA systems that outperform H200s on Llama3-class models, largely by bypassing memory bottlenecks.
*   **Skepticism:** **fblstr** challenges this, noting that while memory bandwidth is the bottleneck, the sheer compute density (PetaOPS) of chips like the Blackwell B200 is difficult for general-purpose FPGA fabric to match.
*   **Bandwidth is King:** Multiple users (**tcnk**, **bee_rider**) agree that the real constraint for inference is memory fabric and bandwidth. **tcnk** highlights modern platforms like the Alveo V80 with PCIe 5.0 and 200G NICs as the current state-of-the-art for programmable in-network compute.

**Market Dynamics**
*   **Hyperscaler Custom Silicon:** The discussion notes that major tech companies (Google, Amazon, Meta, Microsoft) effectively already use custom silicon (TPUs, Inferentia, Maia) for their internal workloads, reducing reliance on Nvidia for inference.
*   **Edge Hardware:** **Narew** and **mffklst** briefly discuss older "stick" format TPUs (Google Coral, Intel compute sticks), noting they are now dated and struggle to compete with low-power GPU/SOC options like Jetson.

**Other Technical Notes**
*   **0-_-0** and **hnkly** drew parallels between neural networks and CPU branch predictors, discussing the potential for AI to handle heuristic tasks (like speculative execution) to skip expensive deterministic computations.
*   **zhm** clarified that while TPUs are often associated with Transformers, architectures like the TPUv5 (Ironwood) were designed specifically for efficient LLM training, whereas other chips (like Etched's Sohu) are true Transformer-specific ASICs.

### AB316: No AI Scapegoating Allowed

#### [Submission URL](https://shub.club/writings/2026/january/ab316/) | 36 points | by [forthwall](https://news.ycombinator.com/user?id=forthwall) | [19 comments](https://news.ycombinator.com/item?id=46461347)

California’s AB316, as described by the poster, adds Civil Code 1714.46 and bars “the AI did it” as a liability defense: if an AI system causes harm, developers or users can’t claim autonomy as a shield. The law broadly defines AI as systems that infer from inputs to generate outputs affecting physical or virtual environments.

The author (not a lawyer) thinks this is reasonable but vague, and raises thorny questions about who’s on the hook when things go wrong:
- Where does liability sit between model makers (e.g., OpenAI), app builders, and deployers?
- How does this play with open-source models used in critical contexts (e.g., an aircraft system)?
- Will claims hinge on marketing representations or integration choices?

Expected knock-on effects: more investment in guardrails and safety layers, tighter operational controls, stronger contracts and indemnities, and a budding market for AI liability insurance. The takeaway: unpredictability won’t excuse harm; if your system can cause damage—like a chatbot giving dangerous advice—you’re responsible for preventing it.

**Discussion Summary:**

Commenters grappled with the boundaries of liability, using analogies ranging from food safety to science fiction to explore whether unpredictability should absolve developers of blame.

*   **The "Eggshell Skull" Doctrine:** The discussion opened with a grim hypothetical: if a chatbot encourages a user to commit suicide, is the developer liable? While some users felt a bot shouldn't be held to the same standard as a human, others cited the "eggshell skull" legal rule. This doctrine suggests a defendant is liable for the resulting harm even if the victim had a pre-existing vulnerability (like suicidal ideation), implying developers cannot use a user's mental state to shield themselves from the consequences of a bot's "persuasive" errors.
*   **The Zoo Analogy:** One user reframed the AB316 logic using a zoo comparison. The law essentially states that "the AI is a wild animal" is not a valid defense. Just as a zoo is responsible for containment regardless of a tiger's natural instincts, an AI deployer is responsible for the system's output, regardless of its inherent unpredictability.
*   **Product Liability Parallels:** Participants drew comparisons to the Jack in the Box E. coli outbreaks and faulty car parts. The consensus leaned toward treating AI as a commercial product: if a company sells "sausages made from unsanitary sources" (or a model trained on toxic data), they face strict liability for the outcomes.
*   **Redundancy vs. Clarity:** A debate emerged over whether this law is redundant, given that product liability laws already exist. However, proponents argued the legislation is necessary to specifically close the "autonomy loophole," preventing defendants from claiming a system's "black box" nature puts its actions outside their legal control.
*   **The "Catbox" Sophistry:** In a philosophical turn, a user cited the "Schrödinger's Catbox" from the novel *Endymion*—a device where a death is triggered by random radioactive decay, purportedly absolving the user of murder. The commenter argued that corporate reliance on AI stochasticity is a similar moral sophistry, attempting to use randomness to dilute ethical responsibility.

### Everyone's Watching Stocks. The Real Bubble Is AI Debt

#### [Submission URL](https://www.bloomberg.com/news/newsletters/2025-12-31/everyone-s-watching-stocks-the-real-bubble-is-ai-debt) | 48 points | by [zerosizedweasle](https://news.ycombinator.com/user?id=zerosizedweasle) | [27 comments](https://news.ycombinator.com/item?id=46468579)

Howard Marks flags rising leverage behind the AI boom as a late‑cycle warning sign

- The Oaktree co-founder says the AI trade has shifted from being funded by Big Tech’s cash piles to being increasingly financed with debt, a change he finds worrisome.
- He argues the AI rally looks further along than earlier in the year, with growing leverage a classic sign of maturing (and potentially bubbly) markets.
- Why it matters: Debt magnifies both gains and losses. If AI-driven revenues don’t arrive fast enough to cover swelling capex and financing costs, the pain could spread from equity to credit markets.
- What to watch: Hyperscalers’ capex and borrowing trends, off-balance-sheet commitments (long-term purchase and leasing deals), and credit spreads tied to the AI supply chain and data-center buildout.
- Context: Marks’ latest memo (“Is It a Bubble?”) doesn’t call a top outright but underscores that the risk profile of the AI trade has changed as leverage enters the picture.

**Daily Digest: Hacker News Discussion**

**Investment Strategy Amidst "Doom" Signals**
The thread opened with users questioning where to allocate capital given the economic warnings (bubbles, debt, and inflation). Responses ranged from adhering to standard long-term strategies (such as Vanguard's 80/20 split) to fleeing to safety. While some advocated for holding cash to avoid potential market crashes of 15%+, others argued that cash is a poor hedge during inflationary periods driven by potential government "money printing." There was also a brief, contentious suggestion to pivot toward specific foreign indices (like Spain) or gold as safety plays.

**Historical Parallels and Timing**
Highlighting the difficulty of acting on macro warnings, one commenter pointed to the Dot-com era: Alan Greenspan famously warned of "irrational exuberance" in 1996, yet the bubble did not burst for several more years. The consensus suggested that while valuations may be unsupported, timing the exact top remains notorious difficult.

**Validating the Leverage Shift**
Validating the article's core thesis, a user shared their own analysis of Big Tech balance sheets (specifically Meta, Microsoft, and Amazon). They noted a distinct shift starting around the release of ChatGPT in late 2022: these previously cash-rich "fangs" have significantly increased their debt loads to finance the AI buildout, a fundamental change in risk profile that led the user to exit a 10-year position in the sector.