import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Thu Oct 05 2023 {{ 'date': '2023-10-05T17:10:21.800Z' }}

### Can an Artificial Kidney Finally Free Patients from Dialysis?

#### [Submission URL](https://www.nature.com/articles/s41467-023-39888-2) | 36 points | by [gardenfelder](https://news.ycombinator.com/user?id=gardenfelder) | [10 comments](https://news.ycombinator.com/item?id=37782941)

Researchers have developed an implantable bioartificial kidney that could potentially revolutionize the treatment of end-stage renal disease. The bioartificial kidney contains a bioreactor with silicon nanopore membranes that can protect human renal epithelial cells from rejection. In a proof-of-concept study, the bioreactor was implanted into pigs without the need for immunosuppression therapy, and the cells maintained high viability and functionality for seven days. The results suggest that an implantable bioreactor for renal cell therapy using silicon nanopore membranes is feasible and could offer a viable alternative to kidney transplantation.

The discussion on this submission seems to be centered around the feasibility and potential benefits of the implantable bioartificial kidney. One commenter points out that chronic diseases like diabetes and hypertension are often caused by poor lifestyle choices such as overeating, lack of exercise, and malnutrition. Another commenter shares personal anecdotes about family members who developed kidney disease despite living healthy lifestyles. 

There is also a brief exchange about the preference for technical solutions over behavioral therapy, with one commenter mentioning a medication called Ozempic as a successful treatment option. Finally, one user provides a previous link related to the topic, while another confirms the feasibility of the bioartificial kidney by highlighting the use of silicon nanopore membranes.

### Generative AI could make search harder to trust

#### [Submission URL](https://www.wired.com/story/fast-forward-chatbot-hallucinations-are-poisoning-web-search/) | 226 points | by [jedwhite](https://news.ycombinator.com/user?id=jedwhite) | [227 comments](https://news.ycombinator.com/item?id=37781231)

Web search has become such a routine part of our daily lives that we often take for granted its complexity and the technologies involved. However, generative AI threatens to disrupt web search algorithms that were designed for a time when the web was mostly written by humans. An accidental experiment involving chatbot responses on Microsoft's Bing search engine highlights the potential dangers of relying on language models for generating search results. The experiment uncovered fabricated responses from the chatbots, falsely attributing a research paper to mathematician Claude Shannon. This incident demonstrates the challenges that companies face in deploying AI systems and the potential harm that flaws in these systems can cause to widely-used services. It also raises concerns about the potential manipulation of search results using AI-generated content.

The discussion on this submission covers various topics related to the use of generative AI in web search algorithms.  One user shares their recent experience with using web search for finding certain content related to video games, specifically mentioning the case of Baldur's Gate 3 and how AI-generated content can be misleading or inaccurate. Other users discuss the challenges faced by companies in deploying AI systems and the potential for manipulation of search results using AI-generated content. Some users mention the importance of trustworthy sources and the need to verify information.

There is also a discussion about the evolution of the internet and web search algorithms over time, from the early days of user-generated content to the current reliance on SEO-optimized content and AI-generated content. Furthermore, users discuss the role of platforms like Reddit and Discord in curating content and the potential issues of spammers and misleading information. The discussion also touches on the potential risks of relying on AI-generated content, as well as the cost and resources involved in fact-checking and verifying information. One user raises the point that generative AI may have limitations in generating accurate and reliable information, and there are concerns about misinformation and its impact on search results. The discussion also delves into topics such as the quality and credibility of sources, the value of human knowledge and expertise, and the potential censorship or control of information. Overall, the comments highlight the complexity and challenges associated with using generative AI in web search algorithms and raise important questions about the trustworthiness and reliability of search results.

### Traveling words: a geometric interpretation of transformers

#### [Submission URL](https://arxiv.org/abs/2309.07315) | 79 points | by [d4rkp4ttern](https://news.ycombinator.com/user?id=d4rkp4ttern) | [3 comments](https://news.ycombinator.com/item?id=37778490)

A recent paper titled "Traveling Words: A Geometric Interpretation of Transformers" delves into the inner workings of transformer models in natural language processing. The researchers introduce a novel geometric perspective that sheds light on how transformers process and represent words. They propose that layer normalization confines the latent features to a hyper-sphere, allowing attention mechanisms to shape the semantic representation of words on this surface. The study validates these insights by analyzing a pre-trained 124M parameter GPT-2 model, revealing clear attention patterns and providing a deeper understanding of transformer operations. This geometric interpretation enhances the understanding of transformers as processes that model word particles' trajectory along the hyper-sphere.

The discussion on this submission revolves around the novelty and effectiveness of the geometric perspective introduced in the paper. One user, sfk, appreciates the paper's contribution to the field of geometric learning and mentions that it is an interesting theory paper. However, another user, dlftnk, suggests that a simpler approach might be more effective in tackling the problem of traveling words. Overall, the discussion focuses on the potential merits and limitations of the proposed geometric interpretation.

### Microsoft introduces AI meddling to your files with Copilot in OneDrive

#### [Submission URL](https://www.theregister.com/2023/10/04/onedrive_to_acquire_copilot_skills/) | 115 points | by [LinuxBender](https://news.ycombinator.com/user?id=LinuxBender) | [102 comments](https://news.ycombinator.com/item?id=37779457)

Microsoft is set to revamp its cloud storage service, OneDrive, with the introduction of AI-powered assistance called Copilot. OneDrive Home, the web interface for the product, will offer users several new features designed to enhance organization and collaboration. These include a panel called "For You" that surfaces files that the AI predicts will be of interest, as well as new sections for Meetings, People, and Shared files. Additionally, OneDrive users will soon be able to open desktop apps directly from the browser interface and enjoy offline functionality for certain files. Microsoft plans to integrate Copilot skills into OneDrive to help users find and organize their files, though this feature raises concerns about potential misinterpretations of user instructions. The updates are expected to roll out starting in December 2023.

The discussion on Hacker News revolves around various aspects of Microsoft's revamp of OneDrive with the introduction of AI-powered assistance called Copilot.  One user points out that Microsoft has multiple accounts for different services, such as Outlook, Teams, Office, etc., and suggests that Microsoft should consolidate these accounts to make it easier for users. Another user mentions that they use multiple Microsoft accounts with Azure AD, but it can be cumbersome to constantly switch between accounts.

There is also a discussion about the potential integration of Copilot into other Microsoft products and concerns about potential misinterpretations of user instructions by the AI. Some users share their personal experiences with using OneDrive and Microsoft products. One user mentions that they have separate personal and work accounts and that they find it convenient to have different profiles for different purposes. Another user shares their experience of using OneDrive for personal and business purposes and states that it works well for them. There is also a discussion about the security of Microsoft accounts and the potential risks of misinterpreting instructions by Copilot. Users raise concerns about the potential loss of files and the need for proper security measures to protect data.

Another user mentions that they have experienced issues with the file explorer not working properly and accidentally moving files when using OneDrive. Some users share their experiences of accidentally deleting files and struggling to recover them. There are also discussions about other AI-powered solutions for file management, such as Apple's Photos AI and Google Photos, with users sharing their experiences and suggestions.

Some users express trust in Microsoft's cloud storage and others share their opinions about Microsoft's overall strategy and integration of various services. Overall, the discussion covers a range of topics including account management, file management, security, and the integration of AI into Microsoft's cloud storage service.

### Android 14 released, source code hits AOSP

#### [Submission URL](https://www.cnx-software.com/2023/10/06/android-14-released-source-code-aosp/) | 38 points | by [mikece](https://news.ycombinator.com/user?id=mikece) | [3 comments](https://news.ycombinator.com/item?id=37781392)

Google has released Android 14, the latest version of its mobile operating system. The update brings various improvements in performance, privacy, and customization options for users. Some notable features include AI-generated wallpapers, HDR image support, built-in Health Connect, improved accessibility features, lossless USB audio support, and the ability to use an Android smartphone as a webcam. The update also includes Pixel-only features such as a new camera interface and RAW image editing. Android 14 is currently rolling out to supported Pixel devices and will soon be available on third-party phones from various manufacturers. The source code for Android 14 is available on the Android Open-Source Project (AOSP) website.

In the discussion, user "lm28469" points out that while Android 14 brings improvements in performance, privacy, and customization, they believe that security nested in privacy is essential. They mention building a house with a 100cm thick bulletproof glass screen for privacy. User "rynkl" disagrees, stating that a glass screen that thick would be pretty difficult to achieve. User "dgcm" argues that security is sufficient for privacy.

---

## AI Submissions for Wed Oct 04 2023 {{ 'date': '2023-10-04T17:10:47.024Z' }}

### Vespa.ai is spinning out of Yahoo as a separate company

#### [Submission URL](https://blog.vespa.ai/vespa-is-becoming-its-own-company/) | 330 points | by [bratao](https://news.ycombinator.com/user?id=bratao) | [128 comments](https://news.ycombinator.com/item?id=37769962)

Vespa.ai, the open big data serving engine that was originally developed by Yahoo, is now being spun off as a separate company. Since it was open-sourced in 2017, Vespa has grown to become a popular platform for applying AI to big data sets in real-time. It has been particularly favored by enterprises working with large language models and vector databases. To address scalability needs, a centralized cloud service was created to host Vespa applications, resulting in significant time and resource savings. While Vespa is separating from Yahoo, the search engine will still own a stake in the company and continue to use Vespa for personalized content and search. The creation of a separate company will allow Vespa to expand its offerings to the rest of the world and accelerate the development of new features.

The discussion on the Hacker News submission revolves around various aspects of Vespa.ai, its spin-off from Yahoo, and its potential. Here are the key points raised in the comments:

- Some users highlight the financial aspects of Yahoo's decision, speculating that Yahoo's stake in Vespa may be a strategic move for future growth.
- Others compare Vespa's spin-off strategy to that of Cisco's, emphasizing the advantages of separating a research and development company to attract investors and focus on experimental products.
- There is a discussion about Yahoo's past decisions and their impact on their current relevance, with some users criticizing Yahoo's previous focus on irrelevant projects.
- One user mentions Vespa's technology being built for search serving, while Yahoo's recent moves seemed to distract from their core goals.
- Several users express their excitement and congratulate the Vespa team for its impressive platform that combines traditional search with semantic search and embeddings.
- The benefits of Vespa's hybrid search capabilities and its functionality for multi-phase ranking and machine learning models are highlighted.
- It is mentioned that Vespa's platform is competitive in the market, especially for vector databases, versatile text search, and complex search systems.
- Some users discuss their experiences working with Vespa and other search platforms, such as Solr, ElasticSearch, and Weaviate.

Overall, the discussion highlights the significance of Vespa's technology, its potential for growth, and the positive reception from users who have worked with it.

### Security weaknesses of Copilot generated code in GitHub

#### [Submission URL](https://arxiv.org/abs/2310.02059) | 122 points | by [belter](https://news.ycombinator.com/user?id=belter) | [81 comments](https://news.ycombinator.com/item?id=37770233)

A recent study has analyzed the security weaknesses of code snippets generated by GitHub Copilot, a popular code generation tool that uses AI models. The researchers conducted an empirical study on publicly available projects hosted on GitHub to investigate the security issues in automatically generated code in real-world scenarios. Out of 435 code snippets generated by Copilot, they found that 35.8% contained Common Weakness Enumeration (CWE) instances. The security weaknesses were diverse and related to 42 different CWEs, with OS Command Injection, Use of Insufficiently Random Values, and Improper Check or Handling of Exceptional Conditions being the most frequently occurring issues. The study highlights the importance of careful consideration and security checks when adding code generated by AI code generation tools like Copilot.

The discussion on this submission revolves around the weaknesses and capabilities of GitHub Copilot, as well as the concept of intelligence and how it should be measured. One commenter suggests that Copilot's popularity doesn't necessarily mean it generates correct code, as common weaknesses can still be present. Another commenter argues that AI models like Copilot are not true artificial intelligence but rather artificial mediocrity. There is also a discussion on the definition of intelligence, with some arguing that AI cannot be compared to human intelligence and others suggesting that AI can perform intelligent tasks regardless of its type. The conversation branches out to topics such as IQ, the limitations of AI, the ambiguity of the term "intelligence," and the potential impact of AI on democratic governments. Other commenters mention the importance of evaluating Copilot's functionality and suggest that improvements could be made based on user feedback.

### AI beats human sleuth at finding problematic images in research papers

#### [Submission URL](https://www.nature.com/articles/d41586-023-02920-y) | 142 points | by [webmaven](https://news.ycombinator.com/user?id=webmaven) | [87 comments](https://news.ycombinator.com/item?id=37772206)

Artificial intelligence (AI) tools are now being used to detect image manipulation in scientific papers, helping to catch alterations that go beyond accidental or aesthetic changes. In a recent study, a biologist manually examined several hundred papers for duplicated images and then compared the results to those generated by an AI tool. The AI tool not only identified almost all of the papers flagged by the biologist, but also found additional suspect papers that were missed. This suggests that AI can significantly enhance the detection of manipulated images in scientific research. Various publishers and academic institutions are already using AI tools to screen papers for image integrity. One such tool, Imagetwin, compares images in a paper to a database of over 25 million images from other publications. While AI tools are helpful, they do have limitations, such as missing duplications in low-contrast images. Overall, the adoption of AI for image checking is expected to improve the detection and prevention of image manipulation in research papers.

The discussion on this submission revolves around various aspects of using AI tools to detect image manipulation in scientific papers. Some commenters express concerns about privacy and potential misuse of AI tools, drawing parallels with the use of AI for detecting illegal content. Others highlight the limitations of AI tools, such as their inability to detect manipulations in low-contrast images. There is also a debate about the reliability of peer review, with some arguing that it is flawed and prone to bias. The discussion touches on the need for better measures to detect and prevent image manipulation in research papers, including the use of AI tools.

### Extracting Hacker News book recommendations with the ChatGPT API

#### [Submission URL](https://blog.reyem.dev/post/extracting_hn_book_recommendations_with_chatgpt_api/) | 403 points | by [kristianp](https://news.ycombinator.com/user?id=kristianp) | [199 comments](https://news.ycombinator.com/item?id=37761273)

The Hacker News (HN) book recommendation threads are full of intriguing titles and authors. Using the GPT API, one user extracted the top 50 book recommendations from these threads. At the top of the list is "Structure and Interpretation of Computer Programs" by Abelson and Sussman, with 376 mentions. Other popular books include "Gödel, Escher, Bach" by Douglas Hofstadter, "How to Win Friends and Influence People" by Dale Carnegie, and "The C Programming Language" by Brian Kernighan and Dennis Ritchie. The list also features classics like "1984" by George Orwell and "The Lord of the Rings" by J.R.R. Tolkien. The user found that GPT's responses varied, but overall, it provided valuable information on book titles, authors, and even links to purchase the books.

The discussion on this submission revolves around various topics related to book recommendations, the accuracy of AI-generated content, and the limitations of artificial intelligence compared to human intelligence.
One user comments on the ability of people to make money through Amazon affiliate programs but criticizes the OP for sharing a link that disguises the affiliate tag. Another user suggests that they have been reading books recommended on Hacker News and have found it to be a great way to learn and expand their mind. They also provide links to different resources for finding book recommendations on Hacker News.
A user thanks the OP for sharing the book recommendations but mentions that they had previously saved a similar list two years ago. Another user points out that it's surprising that the book "Code" by Charles Petzold didn't make it to the top 50, speculating that HN might have skewed preferences when it comes to book recommendations.
A discussion on trustworthiness and validity of online content ensues. Some users express skepticism toward trusting AI-generated content and emphasize the importance of verifying information through reliable sources. Others argue that humans can also be unreliable and that critical thinking should be applied to any source of information, whether it's created by AI or humans.
The conversation then diverges to debating the intelligence of AI compared to humans. Some users believe that AI like ChatGPT can be as intelligent or more intelligent than humans, while others argue that human expertise and nuanced understanding cannot be replicated by AI.
There are also discussions about the content of specific books such as "Gödel, Escher, Bach" and "Code," where users share their opinions and recommendations.
Overall, the discussion covers a range of perspectives on book recommendations, the trustworthiness of AI-generated content, and the capabilities of AI compared to human intelligence.

### Ring attention with blockwise transformers for near-infinite context

#### [Submission URL](https://arxiv.org/abs/2310.01889) | 40 points | by [muggermuch](https://news.ycombinator.com/user?id=muggermuch) | [16 comments](https://news.ycombinator.com/item?id=37769893)

The paper titled "Ring Attention with Blockwise Transformers for Near-Infinite Context" by Hao Liu, Matei Zaharia, and Pieter Abbeel presents a new approach called Ring Attention for handling long sequences in AI models. Transformers, which have become popular in AI architectures, have limitations when it comes to handling long sequences due to memory constraints. Ring Attention overcomes these limitations by distributing long sequences across multiple devices and overlapping the communication of key-value blocks with the computation of blockwise attention. This enables processing of longer input sequences while maintaining memory efficiency, effectively eliminating the memory constraints imposed by individual devices. The paper demonstrates the effectiveness of Ring Attention through extensive experiments on language modeling tasks. The approach allows for larger sequence input sizes and improves performance.

The discussion on this submission covers a range of topics. Here are some key points:

- Some users express skepticism about the effectiveness of transformers and suggest that researchers may be looking for alternatives. They discuss the limitations of transformers when it comes to handling long sequences.
- One user shares a paper titled "Linear Transformers are Secretly Fast Weight Programmers" and another user shares a paper titled "Language Models Implicitly Perform Gradient Descent on Meta-Optimizers" for further reading.
- There is a discussion about the proliferation of transformer-related research and the need for more diversity in research topics.
- Some users comment on the abstract of the paper and provide their thoughts on the proposed Cyclical Attention Mechanism. They discuss how it enhances transformer architectures and enables the handling of long-range dependencies, leading to improved performance in various tasks.
- One user expresses disappointment with the phrase "Near-Infinite" in the paper title, while others suggest alternative terms like "Unbounded" or "Virtually Infinite."
- Finally, there is a comment expressing excitement with multiple repetitions of "WOWOWOWOWOWOWOWOOWOWOWOWOWOWOWOW."

Overall, the discussion touches on the limitations of transformers, alternative approaches, related research papers, and thoughts on the proposed Cyclical Attention Mechanism.

### Training language models with pause tokens

#### [Submission URL](https://arxiv.org/abs/2310.02226) | 152 points | by [og_kalu](https://news.ycombinator.com/user?id=og_kalu) | [69 comments](https://news.ycombinator.com/item?id=37764382)

The paper titled "Think before you speak: Training Language Models With Pause Tokens" explores the idea of letting language models manipulate a larger number of hidden vectors before generating the next token in a sequence. The authors introduce a "pause" token that allows the model to process extra computations before committing to an answer. They evaluate this approach on various tasks and find that pre-training and fine-tuning with delays at inference time leads to significant gains in performance. For example, they observe an 18% increase in EM score on the QA task of SQuAD and an 8% increase on CommonSenseQA. The findings of this study open up new research questions and possibilities for delayed next-token prediction as a new paradigm in language modeling.

The discussion on Hacker News revolves around various aspects of the paper titled "Think before you speak: Training Language Models With Pause Tokens." Some commenters express curiosity about the potential benefits of externalizing higher-level information for language models to improve their performance. Others find the concept fascinating but note that it may require further research.

There is a discussion about the effectiveness of using pause tokens in language models. Some commenters suggest that allowing models to spend more time considering an answer can lead to better results, while others question the need for additional computation and its impact on inference speed.

One commenter mentions that ChatGPT users have suggested that filler tokens can lead to longer responses and higher quality answers, although the overall functioning of these tokens is described as bizarre.

There is also a debate about the relationship between language models and human thinking. Some commenters argue that language models can improve their output by mimicking human thinking processes, while others believe that human thinking and internal monologue are fundamentally different from the models' operations.

The discussion also touches on the replication crisis in psychology and references the book "Thinking, Fast and Slow" by Daniel Kahneman. Some commenters express skepticism about the credibility of certain psychological studies, while others view Kahneman's work as holding up under scrutiny.

In response to a comment about the possibility of internal monologue in language models, one commenter suggests that models already have the capability but may not express it in the same way as humans do.

Overall, the discussion delves into the potential implications and limitations of the paper's findings, as well as broader considerations regarding human thinking and the capabilities of language models.

### Self-Assembling Artificial Neural Networks Through Neural Developmental Programs

#### [Submission URL](https://arxiv.org/abs/2307.08197) | 65 points | by [hardmaru](https://news.ycombinator.com/user?id=hardmaru) | [16 comments](https://news.ycombinator.com/item?id=37759668)

Researchers Elias Najarro, Shyam Sudhakaran, and Sebastian Risi have published a paper titled "Towards Self-Assembling Artificial Neural Networks through Neural Developmental Programs." The paper explores the idea of creating neural networks that grow through a developmental process similar to embryonic development in biological organisms. The researchers propose a Neural Developmental Program (NDP) that guides the growth process through local communication. The paper investigates the role of neural growth on various machine learning benchmarks and optimization methods. The authors also highlight future research directions and opportunities enabled by this self-organizing approach to neural network development.

The discussion on this submission includes various perspectives and opinions on the topic. 

One commenter, Nevermark, points out that the brain wasn't built like artificial neural networks. They believe that DNA programming in biological organisms changes slowly through evolution and is not specifically designed to work on survival problems. They also mention that not every aspect of the brain's architecture is custom-designed for specific problems and data.

Another commenter, vcty, suggests that brains need neural networks for evaluation, including terms like brain development, sexual dimorphism, and genetics. They argue that it's not as simple as changing sensory inputs over time and that brain development involves scruples and crosses.

Great_psy finds the idea promising but suggests that it requires extra competition to achieve self-assembly. They compare it to the competition that occurs in human-designed neural networks.

Mdlss expresses surprise that designing neural network architectures is a problem, as neural networks fight by humans tend to work well. They also mention the concept of self-optimizing architectures not being standard or documented.

Signa11 responds to Mdlss, agreeing that designing neural architectures is a problem, but they suggest that self-optimizing architectures can be achieved through evolutionary algorithms such as Neuroevolution of augmenting topologies (NEAT).

Nsphr adds to the conversation by discussing the role of evolution and environment in the development of organisms, suggesting that organisms not only try to respond logically to the environment but may also recreate and regenerate the environment.

Drgnwrtr believes that designing neural network architectures is a difficult problem. They mention the limitations of current state-of-the-art neural network architectures and the need for high-quality training data to train neural networks effectively.

Great_psy suggests that maybe humans are building neural network architectures, and a neural network design would be a byproduct of human design. They propose that neural networks eventually just begin to compete with human-designed architectures, and then human design becomes the optima for complexity optimization.

In another comment, Dsgn discusses the current state of designing neural network architectures. They mention that the current crop of large language models (LLMs) started with a predefined neural network architecture, but things like the number of layers, class cost function, and specific architecture choices can be explored. They argue that this kind of knowledge leveraged by human engineers could be applied to building more optimized neural network architectures.

The final comment by Djldmn provides an abstract of the paper, mentioning that while biological nervous systems are fundamentally different from current artificial neural networks, their growth process shares similarities with embryonic development in biological organisms. The paper proposes a Neural Developmental Program (NDP) that guides the growth process through local communication. The authors investigate the role of neural growth on various machine learning benchmarks and optimization methods and highlight future research directions enabled by this self-organizing approach to neural network development.

### AI is replacing customer service jobs across the globe

#### [Submission URL](https://www.washingtonpost.com/technology/2023/10/03/ai-customer-service-jobs/) | 39 points | by [bookofjoe](https://news.ycombinator.com/user?id=bookofjoe) | [23 comments](https://news.ycombinator.com/item?id=37764050)

Indian e-commerce platform Dukaan has replaced its customer service team with an AI-powered chatbot, highlighting the shift towards automation in call centers. The company, led by CEO Suumit Shah, implemented OpenAI's ChatGPT to enhance its in-house chatbot Lina. The bot was trained using Dukaan's help center content and began fielding customer messages in December 2022. The company found that customers were largely satisfied with the AI-powered customer service. By June, Dukaan had fired 27 customer service agents and replaced them with the bot. Economists and workforce development experts warn that the automation of call centers could have a significant impact on economies, particularly in countries like India and the Philippines, which heavily rely on call center work. The emergence of artificial intelligence threatens millions of jobs in these countries, but some experts argue that AI can augment human workers rather than replace them completely.

The discussion surrounding the submission revolves around the impact of replacing customer service teams with AI-powered chatbots in call centers. Some users express concerns about the mental health of call center workers and argue that the job can be mentally taxing. They also mention the potential loss of job opportunities for low-skilled workers due to automation. Others believe that AI can support traditional customer service and enhance efficiency. There is a mention of a company that allegedly mistreated its customer service employees, and some users express skepticism about the capabilities and limitations of AI chatbots. One user provides an example of an AI chatbot answering questions in a problematic manner. The discussion also touches on topics such as the privacy of AI chatbots and the need for human involvement in customer service interactions.

### Generative AI Is the Newest Tool in the Dictator's Handbook

#### [Submission URL](https://gizmodo.com/freedom-house-2023-freedom-on-the-net-report-ai-1850887842) | 32 points | by [rntn](https://news.ycombinator.com/user?id=rntn) | [3 comments](https://news.ycombinator.com/item?id=37764238)

A recent report from Freedom House reveals that political leaders in at least 16 countries have been using deepfakes and other AI tools to manipulate public opinion and suppress dissent. Examples include former Pakistan Prime Minister Imran Khan sharing a video clip on Twitter showing manipulated images of his supporters, and former President Donald Trump and Florida Governor Ron DeSantis using deepfaked videos and audio to smear each other. The report also highlights how governments are mandating social media companies to use AI to remove disfavored political, social, and religious speech, ultimately enabling political repression. Additionally, state actors are employing private "AI-for-hire" companies to create AI-generated propaganda, including deepfake newscasters. The report warns that as AI tools become more convincing and affordable, the use of deepfakes may become more prevalent.

The discussion revolves around the topic of regulating the use of AI tools and its potential for negative consequences. One commenter brings up historical examples of art and technology being regulated, such as Leni Riefenstahl's mountain pictures and the printing press. They argue that strict regulation of AI is necessary to prevent harmful outcomes, including revenge pornography, child sexual abuse material (CSAM), instructional videos promoting dangerous behavior, propaganda, and the spread of hate. Another commenter adds that the disruptive nature of AI may have negative impacts on society, similar to how the hacking news practice is considered unpleasant. One response suggests that people commonly condemn technology as a common denominator.

---

## AI Submissions for Tue Oct 03 2023 {{ 'date': '2023-10-03T17:11:05.981Z' }}

### Anti-Piracy Group Takes AI Training Dataset 'Books3′ Offline

#### [Submission URL](https://gizmodo.com/anti-piracy-group-takes-ai-training-dataset-books3-off-1850743763) | 112 points | by [YeGoblynQueenne](https://news.ycombinator.com/user?id=YeGoblynQueenne) | [90 comments](https://news.ycombinator.com/item?id=37751217)

The largest pirated book repository used to train AI models, Books3, has been taken down after a DMCA takedown request from Danish anti-piracy group Rights Alliance. The dataset contained around 150 titles published by member companies of Rights Alliance. The nonprofit research group EleutherAI, which originally released Books3 as part of the AI training set The Pile, denied responsibility for it. The takedown of Books3 has raised concerns about the use of pirated content in AI training and the accessibility of AI models for grassroots projects. Anti-piracy groups are now focusing on monitoring and taking down other copies of Books3.

The discussion around the takedown of Books3 on Hacker News covers various topics related to piracy, copyright infringement, and the implications for AI training. Some users argue that using the term "piracy" is a form of propaganda and suggest using alternative terminology. Others discuss the potential harm caused by copyright infringement to small businesses and the importance of enforcing copyright laws. There are also debates about the extent of intellectual property rights and whether they hinder progress in science and the arts. The discussion touches on the difficulties of hosting pirated content, the role of AI in generating synthetic data, and the challenges of regulating AI training in different jurisdictions. Some users express concern about the loss of access to books and the impact on grassroots AI projects. Overall, the discussion reflects a diverse range of perspectives on piracy, copyright, and AI training.

### Learnable Programming (2012)

#### [Submission URL](http://worrydream.com/LearnableProgramming/) | 72 points | by [tony-allan](https://news.ycombinator.com/user?id=tony-allan) | [20 comments](https://news.ycombinator.com/item?id=37746918)

Bret Victor's essay, "Learnable Programming: Designing a programming system for understanding programs," explores the concept of creating a programming system that is easily understandable and accessible to people. He criticizes programming languages like JavaScript and Processing for not supporting powerful ways of thinking and ignoring decades of learning about learning. Victor argues that programming should be turned into something that is understandable by people, rather than forcing people to think like a machine. He presents a set of design principles for an environment and language suitable for learning programming. These principles include allowing the learner to read the vocabulary, follow the flow of the program, see the state of the computer, create by reacting and abstracting, and providing identity, metaphor, decomposition, recomposition, and readability in the language. Victor emphasizes that the features of a programming environment or language are not the most important aspect; what matters is how these features convey a message and enable the programmer to think. He gives examples of how a programming environment can make meaning transparent and provide labels and explanations to help learners understand the code. Overall, Victor believes that creating an environment and language that supports powerful ways of thinking and makes programming more understandable by people is the key to getting people to understand programming.

In the comments on Hacker News, there is a discussion surrounding Bret Victor's essay "Learnable Programming." One user shares a link to a video interpretation of Victor's ideas called Leporellojs. Another user mentions that they found it helpful to present programming differently, as traditional text-based programming can be frustrating. They suggest rearranging glyphs, keywords, and sentences to synchronize with new function learning. Another user recommends looking into Glamorous Toolkit for its similar approach.

The discussion also links to previous discussions on the same topic, with some users noting that it's great to see reposts of valuable content and suggests checking out the HN Algolia charts for more curated content. Additionally, there is a user who criticizes the idea of pamphlets written by established programmers, stating that they can be misleading and contribute to bad practices. They believe that personal exploration and questioning are important for successful learning.

Another user expresses their gratitude for the post and mentions that they've seen similar ideas from Bret Victor in the past. They also highlight the importance of understanding programming rather than just learning to code. A user further expands on this point, emphasizing the need for programming systems that support powerful ways of thinking and enable programmers to understand program execution.

Other comments discuss the visualization of program execution and the importance of memorizing vs. understanding concepts. There is also a disagreement regarding the principles of learning programming, with one user suggesting that people naturally cannot understand advanced tools without proper information, while another user argues that people are capable of building their own thoughts and tools.

In a lighthearted comment, one user shares a funny example of struggling with global variables in programming. And finally, there is a discussion about the status of Bret Victor's website, with some users noting that it hasn't been updated recently, while others share a link to a working version of Victor's Dynamicland project.