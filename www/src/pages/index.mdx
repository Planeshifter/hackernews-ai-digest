import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Thu Feb 19 2026 {{ 'date': '2026-02-19T17:23:46.166Z' }}

### AI is not a coworker, it's an exoskeleton

#### [Submission URL](https://www.kasava.dev/blog/ai-as-exoskeleton) | 416 points | by [benbeingbin](https://news.ycombinator.com/user?id=benbeingbin) | [412 comments](https://news.ycombinator.com/item?id=47078324)

Core idea: Treat AI as an “exoskeleton” that amplifies human judgment, not as an autonomous coworker. Framed this way, AI succeeds by handling scale and surfacing insights while humans make the calls. The post contrasts this with “agentic AI,” which often disappoints because it lacks the implicit context people carry.

Evidence via real exoskeletons:
- Manufacturing: Ford’s EksoVest cut injuries 83%; BMW reports 30–40% reduced effort with Levitate vests; German Bionic’s Cray X supports up to 66 lbs and customers saw 25% fewer sick days.
- Military: Sarcos Guardian XO Max delivers 20:1 strength amplification; Lockheed’s HULC supports ~200 lbs at ~7 mph sustained.
- Rehab: Meta-analysis found 76% of spinal cord injury patients could walk with powered exoskeletons (with crutches/walkers for balance).
- Running: Stanford showed a 15% energy cost reduction with an ankle exo; Harvard’s soft exosuit cut running metabolic cost by 5.4%.

Product application: Kasava positions its platform as an exoskeleton for product teams—deep commit analysis to spot technical debt and risks; large-scale transcript analysis to extract themes and pain points—while leaving prioritization and decisions to humans. The piece argues autonomous agents fail without organizational “connective tissue” (hinting at a “product graph” to encode that context).

Why it matters:
- Practical takeaway: capture and wire up org context; design AI to surface options, not make judgment calls.
- Counterpoint: agents can work in tightly scoped, well-instrumented domains, but broad autonomy still hits context walls.
- HN angle: a clear framing for building AI features users trust—and a reminder that “human-in-the-loop” is a product choice, not a fallback.

**The Debate: Stochastic Parrots vs. Useful Architects**
The discussion focused heavily on the definition of "reasoning" and the practical application of AI in software engineering, moving beyond the article's specific "exoskeleton" metaphor into the mechanics of how AI assists—or hinders—technical work.

*   **The "Architect" vs. The "Team":** One prominent perspective challenged the need for human teams, arguing that human collaboration has high "synchronization costs." This user suggested AI facilitates a shift where a single human "architect" with good taste can direct an "army of agents" or utilize AI for error correction and delegation, effectively acting as a massive lever for individual output.
*   **The Competence Barrier:** Countering the "easy automation" narrative, others likened AI coding to distinct skill levels. One commenter compared unskilled AI usage to a toddler with Play-Doh—making a mess without fundamental structural knowledge—working only when the user is a "competent sculptor" (skilled programmer) who can mold the raw material correctly.
*   **Reasoning vs. Autocomplete:** A heated debate emerged regarding whether LLMs possess true logic or are simply "text predictors."
    *   **Skeptics** noted that models often hallucinate (citing an anecdote about a model reading the clipboard and inventing a scenario) and make "non-human" mistakes, arguing they lack the context to find actual flaws.
    *   **Defenders** argued that if a model successfully navigates messy log files to produce working code fixes, the distinction between "reasoning" and "prediction" is effectively meaningless. One user cited mechanistic interpretability research (specifically regarding Anthropic’s models) to demonstrate that LLMs have specific internal feature representations for "bugs" and "unsafe code," suggesting a deeper internal logic than simple pattern matching.
*   **Human Reliability:** Several comments pushed back against the demand for deterministic AI, pointing out that human reasoning is also probabilistic and unreliable. The argument was made that AI doesn't need to be perfect, but simply needs to outperform the average human's consistency to be valuable, drawing parallels to safety thresholds for self-driving cars.

### Gemini 3.1 Pro

#### [Submission URL](https://blog.google/innovation-and-ai/models-and-research/gemini-models/gemini-3-1-pro/) | 902 points | by [MallocVoidstar](https://news.ycombinator.com/user?id=MallocVoidstar) | [871 comments](https://news.ycombinator.com/item?id=47074735)

Google announces Gemini 3.1 Pro, pushing “core reasoning” and agentic workflows

- What’s new: An upgraded core model meant for complex, multi-step tasks, rolling out across Google’s consumer and developer products.
- Headline claim: Scores 77.1% (verified) on ARC-AGI-2, more than double Gemini 3 Pro’s reasoning performance, per Google.
- Demos/use cases: 
  - Generates website-ready animated SVGs from text.
  - Builds a live ISS telemetry dashboard by wiring a public API to a visualization.
  - Codes an interactive 3D starling murmuration with hand tracking and a generative score.
  - “Creative coding” that translates literary themes (e.g., Wuthering Heights) into site design and behavior.
- Availability: 
  - Developers (preview) via Gemini API in AI Studio, Gemini CLI, Google Antigravity, and Android Studio.
  - Enterprises via Vertex AI and Gemini Enterprise.
  - Consumers via the Gemini app and NotebookLM (Pro/Ultra; higher limits in the app, NotebookLM access limited to Pro/Ultra).
- Context: Follows last week’s “Gemini 3 Deep Think” update; 3.1 Pro is in preview as Google iterates on more ambitious agentic workflows before general availability.

Here is a summary of the discussion:

*   **Claude Remains the Coding Favorite:** A significant portion of the discussion contrasts Gemini unfavorably with Anthropic’s Claude. While users acknowledge Gemini’s raw reasoning speed, developers report getting "stuck in loops" and struggling with poor tooling integration. One former Googler described their current workflow as "Plan in Gemini, Execute in Claude," implying Google has captured the reasoning aspect but Anthropic currently owns the developer experience.
*   **Strategic Focus (OpenAI vs. Anthropic):** Commenters suggest that while Google is busy fighting an existential war against OpenAI to protect Search, they are inadvertently leaving the high-value "enterprise and coding" door open for Anthropic. However, defenders note that Google’s "messy" product launches are typical for the company, and its vertical integration (owning the TPUs and data centers) gives it a long-term economic advantage that competitors reliant on Nvidia chips lack.
*   **Performance & Hallucinations:** User reports on the new model's accuracy are mixed. Some anecdotal evidence suggests the new preview hallucinates more than Gemini 3.0, while others cite benchmarks (like the AA-Omniscience Index) to argue that 3.1 is technically an improvement, leading to a debate on how these benchmarks apply to real-world usage.
*   **Safety & Refusals:** There is conflicting feedback regarding safety filters. While some users complain about Google’s strict refusal to answer certain prompts (e.g., SSH keys), others note that they have recently found ChatGPT to be more puritanical, occasionally switching to Gemini specifically because it was willing to answer questions OpenAI refused.

### Measuring AI agent autonomy in practice

#### [Submission URL](https://www.anthropic.com/research/measuring-agent-autonomy) | 109 points | by [jbredeche](https://news.ycombinator.com/user?id=jbredeche) | [49 comments](https://news.ycombinator.com/item?id=47073947)

Measuring AI agent autonomy in practice (Anthropic): Anthropic analyzed millions of interactions across Claude Code and its public API to see how autonomously agents run in the wild, how humans oversee them, and where they’re used.

- Autonomy is rising: In the longest Claude Code sessions, uninterrupted work time nearly doubled in three months—from under 25 minutes to over 45—an increase that appears driven by usage patterns/UX as much as model upgrades.
- Oversight is shifting: Experienced users enable full auto-approve in 40%+ of sessions (vs ~20% for new users) yet interrupt more often—letting agents run, then stepping in when it matters. On complex tasks, Claude Code pauses to ask for clarification more than twice as often as humans interrupt it.
- Where agents act: Most API-side actions are low-risk and reversible. About half of agentic activity is software engineering, with early but growing use in healthcare, finance, and cybersecurity.
- How they measured: API traffic was analyzed at the tool-call level (no session stitching), while Claude Code provided end-to-end session timelines; autonomy was proxied by turn duration, with caveats.

Why it matters: As agent autonomy grows, Anthropic argues for new post-deployment monitoring and human–AI interaction paradigms so people and agents can manage autonomy and risk together.

Based on the discussion, here is a summary of the comments on Hacker News:

**Critique of Metrics and Methodology**
The most prominent criticism focuses on Anthropic's decision to use "turn duration" (specifically the increase from 25 to 45 minutes at the 99.9th percentile) as a proxy for autonomy. Users argued this metric is heavily flawed because:
*   **Hardware dependency:** Duration correlates with compute speed; the same task takes vastly different amounts of time on a Raspberry Pi versus a Groq chip, making time a "gibberish measurement" without controlling for token speed or output quality.
*   **Cherry-picking:** Critics noted that highlighting the 99.9th percentile (the extreme tail) looks like cherry-picking outliers rather than representing the average user experience.
*   **Better alternatives:** Commenters suggested that true autonomy should be measured by "authorization scope" (success within permission boundaries) or the complexity of tasks handled without human intervention, rather than raw time.

**Privacy and Telemetry Concerns**
There is significant skepticism regarding user privacy. Several commenters believe the push for "agentic" coding via Claude Code is primarily a method to harvest telemetry for training data. Users expressed distrust in "privacy-preserving" features (like the mentioned specific research tool "Clio"), arguing that even if personal speech is removed or summarized, the storage of that derived data still poses a security risk and benefits the company more than the user.

**Model Consistency and "Overthinking"**
Users reported frustrated experiences with recent model behavior, suspecting silent backend changes to balance costs or game benchmarks.
*   Some noted that models seem to vary wildly in quality from day to day or hour to hour.
*   Others mentioned that "thinking" models (like Opus) sometimes overthink simple tasks, burning tokens unnecessarily, leading some users to switch back to previous versions or disable thinking modes for efficiency.

**The "Dead Internet" Meta-Discussion**
A subset of the discussion focused on the comment thread itself, with users claiming to spot "green-named" (new) or previously dormant accounts posting generic, bot-like responses. This fueled a sentiment that AI agents are already "clogging up" the commons and poisoning the very platforms discussing their regulation and monitoring.

### Anthropic officially bans using subscription auth for third party use

#### [Submission URL](https://code.claude.com/docs/en/legal-and-compliance) | 641 points | by [theahura](https://news.ycombinator.com/user?id=theahura) | [765 comments](https://news.ycombinator.com/item?id=47069299)

Anthropic clarifies Claude Code legal, auth, and compliance rules

What’s new
- Terms: Enterprise/API users fall under Commercial Terms; Free/Pro/Max under Consumer Terms. Existing commercial agreements apply whether using Claude directly or via AWS Bedrock/Google Vertex.
- Healthcare: A Business Associate Agreement automatically covers Claude Code if the customer has an executed BAA and Zero Data Retention (ZDR) is enabled; applies to that customer’s API traffic through Claude Code.
- Usage: Subject to Anthropic’s AUP. Pro/Max usage limits assume ordinary, individual use across Claude Code and the Agent SDK.
- Authentication: Consumer OAuth tokens (Free/Pro/Max) are only for Claude Code and Claude.ai. Using them in other products/services—including the Agent SDK—or proxying user traffic through Claude.ai credentials is prohibited. Developers must use API keys from Claude Console or supported cloud providers. Anthropic may enforce without notice.
- Security: Trust Center and Transparency Hub are the references; vulnerabilities go through HackerOne.

Why it matters
- Shuts down gray-market “log in with Claude.ai” or credential-proxy tools.
- Provides a clear compliance path for healthcare workloads (BAA + ZDR).
- Reduces legal ambiguity for third-party builders on the Agent SDK and cloud marketplaces.

Action items for devs and teams
- If you’re integrating Claude or the Agent SDK, provision API keys—don’t rely on user OAuth.
- Audit any tool that asks users to sign in with Claude.ai; it likely violates the ToS.
- For HIPAA use, confirm BAA is executed and enable ZDR before routing through Claude Code.
- Point security teams to the Trust Center/Transparency Hub and set up HackerOne reporting.

Here is a summary of the discussion:

**Strategic Lock-in vs. Developer Freedom**
The discussion focused heavily on the business signals behind Anthropic’s technical restrictions. Many users view the move as an attempt to "capture value" and create platform path dependency—comparing the strategy to Apple’s walled garden or Microsoft’s OS dominance—rather than purely technical necessity. Commenters argued that by coupling the frontend (Claude Code) tightly with the model usage, Anthropic is trying to prevent their backend models from becoming a fungible commodity.

**The "Bloat" vs. DIY Debate**
A significant portion of the conversation revolved around the efficiency of the Claude Code tool itself.
*   **Performance:** Several developers critiqued the official CLI as "slow" and "bloated."
*   **Simplicity:** Users shared anecdotes of building their own minimal coding agents (e.g., "100 lines of code and tmux controls") that they feel outperform the official harness.
*   **The "Bitter Lesson":** Some invoked Rich Sutton's "Bitter Lesson," suggesting that complex, hand-engineered software harnesses (like Claude Code) will eventually lose out to general, compute-driven approaches.

**Risk of Accelerating Competitors**
Commenters warned that strictly enforcing these rules might backfire.
*   **Fungibility:** There is a sentiment that LLMs are becoming interchangeable artifacts. If Anthropic makes access too difficult or expensive (forcing API keys over OAuth convenience), developers may simply switch to competitors or open-source alternatives that offer more flexibility.
*   **Innovation:** Users argued that banning "gray market" integrations (using web-auth for third-party tools) stifles the ecosystem and "burns developer loyalty" in favor of short-term corporate control.

### AI makes you boring

#### [Submission URL](https://www.marginalia.nu/log/a_132_ai_bores/) | 660 points | by [speckx](https://news.ycombinator.com/user?id=speckx) | [359 comments](https://news.ycombinator.com/item?id=47076966)

AI makes you boring (Opinion) — Posted 2026-02-19

- Thesis: The surge of AI-assisted “vibe-coded” Show HN projects has boosted volume but hollowed out depth. Pre-AI, demos sparked rich conversations because creators had wrestled with problems; now many feel shallow and interchangeable.
- Core argument: Offloading thinking to LLMs dampens originality. Models are good at smoothing ideas, not generating novel ones; relying on them yields surface-level outputs that feel smart but aren’t. Using AI to explore unfamiliar ground can help, but it’s a “fatal flaw” for original work like posts or products.
- Rebuttal to “human-in-the-loop”: Original ideas emerge from doing the hard, messy work AI replaces. Steering an LLM doesn’t restore depth; it nudges human thought toward AI’s averaged patterns.
- Process matters: Articulation—writing, teaching, grinding through a problem—refines ideas. Prompting isn’t articulation; the output is disposable without the thinking behind it. Metaphor: you don’t build muscle by having an excavator lift your weights; you don’t get interesting thoughts by letting a GPU think for you.
- Why it matters: Beyond HN, AI risks homogenizing programming discourse and product ideas, trading hard-won insight for quick, forgettable demos.

Here is a summary of the discussion:

**The Maintenance Debt of "Vibe Coding"**
The discussion centered on the distinction between *running* code and *reading* it. While some users argued that code is primarily an executable tool and that AI is excellent for removing the drudgery of "boring" implementations, others countered that code must eventually be read to be maintained. User `fhd2` and others pointed out that if a creator doesn't have the time to write the code, they likely won't have the understanding required to fix it when it breaks. This led to comparisons between AI-generated code and "minified" or closed-source code—functional black boxes that are hostile to debugging.

**Reliability vs. "Good Enough"**
A debate emerged regarding the stakes of software development. User `zm` argued that 90% of software (e.g., CRUD apps) is not life-critical, contrasting it with the infamous Therac-25 incidents, and suggested that strict reliability matters less than shipping. `bstmm` and others pushed back, noting that reliability is often the difference between high-value software (banking) and cheap disposable tools. They warned that AI introduces subtle logic errors (like using floating-point math for currency) that "vibe coders" might miss until it's too late.

**The Productivity Trap**
Several commenters shared anecdotes about AI in professional settings. User `rwn` noted that coworkers using AI generators doubled their feature output but increased the volume of code (line changes) by 10x, resulting in a team that was "less able to work" or self-correct because they didn't understand the bloat they were committing. The consensus among skeptics was that AI encourages treating software like "cheap, Chinese-made widgets"—fine for disposable one-offs, but a liability for professional-grade engineering.

### AI made coding more enjoyable

#### [Submission URL](https://weberdominik.com/blog/ai-coding-enjoyable/) | 95 points | by [domysee](https://news.ycombinator.com/user?id=domysee) | [90 comments](https://news.ycombinator.com/item?id=47075400)

AI as the bore-b-gone for coding grunt work

- A developer describes offloading the “typing exercise” parts of software engineering to LLMs: error handling, input validation, plumbing properties through layers, and handling many type-specific branches.
- Tests are the key workflow: they design for testability, write one exemplar test to set style and expectations, then have the AI generate the rest of the cases.
- One red line: they don’t trust LLMs for literal copy-paste edits, fearing subtle, undetectable drift when the model “re-creates” code instead of pasting it verbatim.
- Net result: major reduction in tedium and faster iteration; AI shines at boilerplate and repetitive patterns while the human focuses on design and tricky paths.
- Implicit takeaway: use AI where a clear pattern/spec exists (tests as oracle), and reach for deterministic tools (codemods, refactors, patches) when exactness matters.

While the submission praises AI for removing tedium, the discussion highlights a divide between developers who prioritize "shipping" and those who prioritize "craft," with significant concerns regarding long-term code comprehension.

**Key themes in the discussion:**

*   **The "Video Game" Metaphor:** Several users argued that writing the "boring" parts (tests, boilerplate) is the primary way developers build a mental model of the system. Offloading this to AI was likened to "watching a video game instead of playing it"—you get to the end, but you miss the experience and understanding gained through the struggle.
*   **Code Review vs. Code Generation:** A recurring sentiment is that reviewing code is cognitively harder than writing it. Users fear that AI optimizes the "cheap" task (generating lines of code) while exponentially increasing the "expensive" task (debugging and reviewing logic you didn't write). There is concern that this leads to "functioning but not understood" technical debt.
*   **Product vs. Craft:** The thread split into two philosophical camps:
    *   **The Pragmatists:** Argue that business value is tied to the *product*, not the code. If AI accelerates the result, "losing" the coding process is acceptable. This group included time-poor developers (e.g., new parents) who found AI essential for actually finishing personal projects.
    *   **The Craftsmen:** Argue that if you enjoy the act of coding, AI removes the fun. One user noted an ironic inversion: they love writing small, logical methods and hate high-level architecture; AI forces them to do the architecture while robbing them of the logic puzzles they enjoy.
*   **Skill Atrophy:** Commenters expressed concern that skipping manual implementation (like setting up Linux servers or writing basic algorithms) stops the deepening of technical knowledge, turning developers into "no-code" platform operators.

### Step 3.5 Flash – Open-source foundation model, supports deep reasoning at speed

#### [Submission URL](https://static.stepfun.com/blog/step-3.5-flash/) | 222 points | by [kristianp](https://news.ycombinator.com/user?id=kristianp) | [88 comments](https://news.ycombinator.com/item?id=47069179)

StepFun drops Step 3.5 Flash: an open-source MoE that aims for frontier reasoning at real-time speeds

Highlights
- Architecture and speed: 196B-parameter sparse MoE with only 11B active per token (“intelligence density”). Uses 3-way Multi-Token Prediction to hit 100–300 tok/s in typical use (peaks ~350 tok/s on single-stream coding).
- Long context: 256K window via a 3:1 Sliding Window Attention to full-attention ratio, targeting lower compute while keeping quality on long codebases/docs.
- Agent/coding focus: RL-trained for stability on long-horizon tasks; scores 74.4% on SWE-bench Verified and 51.0 on Terminal-Bench 2.0; 86.4 on LiveCodeBench-V6.
- Reasoning: Near-SOTA on math olympiad style tasks—AIME 2025 at 97.3 (99.9 with PaCoRe/parallel thinking) and HMMT 2025 at 96.2; 85.4 on IMOAnswerBench.
- Agents and browsing: 88.2 on τ²-Bench; 69.0 on BrowseComp (with Context Manager).
- Overall standing: Average score across eight benchmarks is 81.0—competitive with top proprietary models (e.g., Gemini 3.0 Pro 80.7, Claude Opus 4.5 80.6) and just behind GPT-5.2 xhigh at 82.2. It trails some closed models on specific tasks (e.g., Claude on Terminal-Bench).
- Local deployment: Positioned for high-end consumer hardware (e.g., Mac Studio M4 Max, NVIDIA DGX Spark) with code/weights slated across GitHub, HuggingFace, and ModelScope; tech report and OpenClaw Guidance mentioned.

Why it matters
- Pushes open-source models closer to “agent-ready” territory: fast generation, long context, and robust coding/terminal competence.
- MoE + MTP-3 delivers a rare combo of throughput and reasoning depth, making local private agents more practical.

Caveats
- Vendor-reported results; averages exclude xbench-DeepSearch where it lags (56.3 vs ChatGPT-5-Pro 75.0).
- “PaCoRe/parallel thinking” gains come with extra test-time compute.
- Real-world latency/throughput depend heavily on user hardware and deployment stack.

**StepFun drops Step 3.5 Flash: an open-source MoE that aims for frontier reasoning at real-time speeds**
https://huggingface.co/StepFun/Step-3.5-Flash-GGUF

StepFun has released Step 3.5 Flash, a 196B-parameter sparse Mixture-of-Experts (MoE) model with 11B active parameters per token. Aiming for "intelligence density," the model utilizes 3-way Multi-Token Prediction to achieve generation speeds of 100–300 tokens per second. It features a 256K context window using a sliding window attention mechanism to maintain performance on long documents and codebases. The model is positioned as an agent-ready solution, scoring competitively on coding benchmarks like SWE-bench Verified (74.4%) and reasoning tasks like AIME 2025 (97.3%). It is designed for local deployment on high-end hardware, with weights available on HuggingFace and GitHub.

**Discussion Summary**

The discussion focuses heavily on the practicalities of running such a large model locally, specifically regarding hardware requirements, recurring bugs, and the economics of self-hosting versus using APIs.

*   **Local Hardware and Performance:** Users confirm the model runs well on high-end Apple Silicon systems (M1 Ultra, M3 Max) with 128GB+ RAM using 4-bit quantization. Reported speeds are around 36 tokens per second on an M1 Ultra, with users praising the "intelligence density" compared to other local models like Minimax 2.5 and GLM-4.
*   **Infinite Loop Bug:** A significant portion of the feedback concerns an "infinite reasoning loop" issue. While initially suspected to be a `llama.cpp` engine bug (potentially mishandling format tags or "thinking" tokens), some users reported reproducing the issue in the official chat UI, suggesting it may be an intrinsic flaw in the model weights.
*   **Agentic Capabilities and Comparisons:**
    *   Users debating its coding utility found it handles local CLI harnesses better than Qwen 2.5 Coder in some instances, though others noted it can struggle with explicit tool-calling instructions compared to models like Nemotron.
    *   While reasoning is praised in some contexts (solving riddles), others found it prone to hallucinations on niche logic queries (e.g., Pokémon deck mechanics) where models like Claude Opus or DeepSeek succeed.
*   **Benchmark Skepticism:** There is debate regarding the Terminal-Bench 2.0 score (51.0); while some users felt this was low, others contextualized it as valid near-SOTA performance given the difficulty of the benchmark. However, critics argued that Terminal-Bench often tests syntax memorization rather than true agentic reasoning.
*   **Economics of Self-Hosting:** A sidebar discussion analyzed the cost-benefit of purchasing $10,000+ hardware (e.g., M3 Ultra with 512GB RAM) versus paying for APIs. The consensus leaned toward APIs being cheaper for most users unless privacy or 24/7 heavy utilization are the primary factors.

### How AI is affecting productivity and jobs in Europe

#### [Submission URL](https://cepr.org/voxeu/columns/how-ai-affecting-productivity-and-jobs-europe) | 169 points | by [pseudolus](https://news.ycombinator.com/user?id=pseudolus) | [131 comments](https://news.ycombinator.com/item?id=47068320)

Europe’s AI paradox: lagging invention, solid deployment — and a measurable payoff

- What’s new: A study of 12,000+ EU firms (EIBIS + Orbis) claims the first causal estimate of AI’s impact on firm performance in Europe. Using a Rajan–Zingales-style instrument, the authors match each EU firm to similar US firms and use US AI adoption as an exogenous shock to identify effects.

- Key findings:
  - AI lifts labour productivity by about 4% on average — economically meaningful, but far from “instant productivity boom” territory.
  - No evidence of job losses from adoption in the short run, suggesting AI is acting as a complement rather than a direct substitute for labour.
  - Adoption parity masks big divides:
    - Financially developed EU countries (e.g., Sweden, Netherlands) match US adoption (~36% of firms in 2024).
    - Less financially developed EU economies (e.g., Romania, Bulgaria) lag (~28%).
    - Big-firm edge: 45% of large firms use AI vs 24% of small firms.
  - Adopters already look different: they invest more, innovate more, and face tighter skill bottlenecks — hence the need for causal methods.

- Why it matters: AI is delivering real, near-term efficiency gains in Europe without immediate employment cuts, but benefits concentrate in larger firms and financially stronger countries — a recipe for widening gaps unless policy focuses on diffusion (SMEs), skills, and access to finance/infrastructure.

- Caveats: Effects are on labour productivity levels (short-run) rather than long-run TFP growth; adoption is partly survey-based; heterogeneity across countries and sectors remains large.

While the submission focuses on the economic and productivity impact of AI adoption in Europe, the comment text pivots to a debate on the reliability of the metrics used and whether AI’s perceived utility is actually a symptom of failing legacy tools.

**Critique of Innovation Metrics**
Discussion opens with skepticism regarding the study's use of patent data to measure AI specialization. Commmenter **gyl** argues that comparing EU and US patent numbers is flawed because EU law has historically been much stricter regarding software patents. Others suggest that a surge in patents often signals patent trolling or speculative investment rather than genuine innovation, with **cor_NEEL_ius** warning that verifying AI-generated output (which often hallucinates) can actually create new productivity costs.

**The "Search is Broken" Thesis**
The majority of the thread argues that AI feels productive primarily because traditional web search has degraded.
*   **Decline of Search:** Users like **m463**, **mnkpt**, and **adrian_b** expres deep frustration with modern search engines (specifically Google) for ignoring Boolean operators, fuzzy-matching specific terms (returning "Hoodoo" for "Xoodoo"), and prioritizing SEO-spam and ads over direct answers.
*   **AI as a Refuge:** In this context, AI is viewed as a "cleaner" alternative for cutting through "corporate obfuscation" to find simple facts (e.g., car pricing) without clicking through ad-laden pages.

**Skepticism and Future Outlook**
Despite the current utility of AI over search, the mood remains cynical:
*   **Hallucination vs. Noise:** **CamelCaseCondo** counters that while search is noisy, AI is dangerous due to confident hallucinations, citing difficulties using LLMs for technical schematics where "close enough" is useless.
*   **The Cycle of "Enshittification":** **Wobbles42** and **luke5441** predict that the productivity boost is temporary. They anticipate that LLMs will eventually succumb to the same forces that ruined search: sponsored content, "SEO" manipulation of training data/context windows, and a flood of AI-generated spam, ultimately negating the efficiency gains currently being celebrated.

### I traced 3,177 API calls to see what 4 AI coding tools put in the context window

#### [Submission URL](https://theredbeard.io/blog/i-intercepted-3177-api-calls-across-4-ai-coding-tools/) | 31 points | by [theredbeard](https://news.ycombinator.com/user?id=theredbeard) | [3 comments](https://news.ycombinator.com/item?id=47073237)

What happened
- Frustrated by ballooning token bills, the author built Context Lens, a tracer that records exactly what LLM coding tools put in their context window each turn.
- He planted a small bug in Express.js (res.send(null) returning the string "null" with JSON content-type instead of an empty body) and asked four tools to find/fix it and run tests.
- All four fixed the bug and passed 1,246 tests—but their context strategies and token usage were dramatically different.

Who and how
- Models tested via CLI: Claude Code Opus, Claude Code Sonnet, “Codex (GPT-5.3)” and Gemini 2.5 Pro.
- Context Lens broke down tokens by category: tool definitions, tool results, system prompt, and user conversation; it also tracked per-turn totals.

Key results
- Total tokens (mean; range across runs):
  - Opus: ~27K (23–35K), tight and consistent.
  - Sonnet: ~50K (43–70K), broader reads.
  - Codex: ~35K (29–47K), moderate variance.
  - Gemini: ~258K (179–351K), huge variance with no convergence.
- Peak-context composition:
  - Opus: 69% tool definitions (16.4K); only 1.5K in actual tool results. “Architectural tax” from re-sending tool definitions every turn. Caching helps (≈95% hits after the first turn).
  - Sonnet: Similar Claude overhead (18.4K tool defs, 43%) plus heavy reads (16.9K tool results, 40%); even read an entire 15.5K test file.
  - Codex: Minimal tool-def overhead (6%); most tokens go to tool results (72%).
  - Gemini: 0% tool-def overhead (server-side tools) but extremely aggressive reads—96% tool results. One turn pulled 118.5K tokens by dumping the full git history of a file.
- Gemini ran the same number of API calls in low and high runs but chose very different, data-heavy paths each time. Its 1M context window and cheaper tokens are caveats, but the strategy still burns a lot of context.

Why it matters
- Same quality, very different costs: tool design choices (prompted tool definitions vs. server-side tools, caching, retrieval breadth) dominate token spend.
- Big context windows invite sloppy retrieval; developers pay for it.
- Instrumentation like Context Lens surfaces hidden overheads (e.g., repeated tool definitions, unbounded diffs/histories) so teams can rein them in.

Notable footnotes
- Claude uses Haiku subagents for small tasks; they don’t share cache with the main Opus calls.
- The author observed no “settling” pattern in Gemini’s runs—each took a fresh, often heavier route.

Takeaways for practitioners
- Measure per-turn context composition; set budgets and guardrails for tools (cap diffs/histories, prefer targeted reads).
- Exploit caching where available; avoid re-sending large static tool definitions.
- Bigger, cheaper context is not a free lunch—strategy beats window size.

**Discussion Summary**

Commenters praised the analysis and the name of the author's tool, "Context Lens." The limited discussion focused on technical specifics, including:
*   **Model Behaviors:** One user noted that Opus and Codex are particularly strong at reading code and diagnosing bugs, observing that telling Opus a feature "worked recently" effectively prompts it to inspect git history.
*   **Version Confusion:** There was some confusion regarding the specific model versions and release timelines being compared, likely due to the fast pace of recent LLM updates (e.g., Gemini 2.5 vs. recent Claude releases).

### Grok Exposed a Porn Performer's Legal Name and Birthdate–Without Being Asked

#### [Submission URL](https://www.404media.co/grok-doxing-real-names-birthdates-siri-dahl/) | 56 points | by [latexr](https://news.ycombinator.com/user?id=latexr) | [33 comments](https://news.ycombinator.com/item?id=47077656)

404 Media reports that xAI’s Grok surfaced porn performer Siri Dahl’s full legal name and birthdate to users without being prompted, blowing up a decade of separation between her stage identity and private life. Within hours, impersonators spun up Facebook accounts in her legal name and leak sites relabeled stolen clips, escalating harassment. Dahl told 404 Media she chose to include her legal name in the piece to try to reclaim control, but the episode underscores mounting concerns about Grok’s privacy and safety guardrails—especially around doxxing and sexualized content—and raises fresh questions about how chatbots source, filter, and gatekeep sensitive personal data.

Here is a summary of the discussion on Hacker News:

**Public Data vs. Accessibility**
A significant portion of the debate centered on whether Grok actually "leaked" private information or simply surfaced data that was technically already public. Users did their own digging, noting that the information appeared to be available through decades-old social media posts archived on the Wayback Machine or via USPTO trademark filings. However, counter-arguments emphasized the concept of "security by obscurity." Commenters argued that there is a functional difference between data buried in a "faraway corner of the internet" that takes humans hours to find, and data served instantly by a chatbot. They contended that LLMs remove the friction that previously acted as a privacy shield, turning difficult-to-find information into immediate doxxing material.

**Data Scraping and "Grokipedia"**
Participants discussed "Grokipedia," an information retrieval component of xAI, noting that it lists sources directly. Some users expressed skepticism about the claim that the data was private to begin with, suggesting Grok likely crawled public web data or data broker aggregations. This led to a broader discussion on how LLMs are trained; several users pointed out that once an LLM ingests personal data from the internet archive, it is difficult to "scrub" or make the model forget it. Comparisons were made to Google and OpenAI, with users noting that established tech giants generally have legal processes for removing such data that xAI may lack.

**Harassment and Subject Identity**
The conversation touched on the specific vulnerability of the subject as a performer. While some comments were dismissive ("played the world's smallest violin"), others pushed back strongly, arguing that the dismissal of the harm was rooted in misogyny or bias against sex workers. These users highlighted that the technology powers abuse at scale, and that declaring data "public" forces victims into a binary where they cannot complain about harassment if a machine aggregates their data.

**Data Removal Services**
There was a side discussion regarding the efficacy of data removal services. Users questioned if paying these services actually works long-term, with some noting that while they can enforce removals, sketchier sites often ignore them or reappear, and LLM training sets present a new, harder-to-clean repository of personal history.

### Sam Altman (OpenAI) and Dario Amodei (Anthropic) Refuse to Hold Hands

#### [Submission URL](https://xcancel.com/ANI/status/2024349307835732347) | 57 points | by [doener](https://news.ycombinator.com/user?id=doener) | [21 comments](https://news.ycombinator.com/item?id=47074474)

Top story: Modi’s “hands up” photo op with AI chiefs goes viral at India AI Impact Summit 2026

- What happened: At the India AI Impact Summit in New Delhi, PM Narendra Modi posed with global AI leaders including Sundar Pichai (Google/Alphabet), Sam Altman (OpenAI), Dario Amodei (Anthropic), and Alexandr Wang (Scale AI). An ANI/DD News clip of a group hand-raise moment took off across X.
- The clip: Modi prompts a linked hand-raise; several executives look hesitant, with Altman and Amodei’s awkward half-gestures becoming the meme focus.
- Why it matters: Beyond the optics, the lineup underscores India’s bid to be a convening power in AI policy, investment, and infrastructure—positioning itself as more than a market and leaning into standard-setting and talent pipelines.

What people are saying
- Symbolism vs PR: Supporters hailed “collaboration” and India’s “civilizational confidence”; critics called it a political-style photo op overshadowing substance.
- Representation questions: Observers noted few Indian tech operators on stage besides Modi; others asked about notable absences (e.g., Meta’s AI leadership).
- Real-world friction: Delhiites complained about Bharat Mandapam logistics, with long walks and traffic disruptions during the event.

Notes and nitpicks
- The viral captioning sparked corrections about attendee titles/affiliations, adding to the meme storm.
- Rivals standing shoulder-to-shoulder (Altman/Amodei) drew jokes—and curiosity about how aligned the “AI Avengers” really are.

What to watch next
- Concrete outcomes: Any announcements on compute access, data center investments, skilling programs, or safety/standards workstreams tied to India.
- Policy influence: Whether India translates convening power into credible guardrails and incentives that attract frontier AI R&D—not just cloud and services.
- Industry impact: If India’s IT majors pivot toward AI productization fast enough to keep pace with the Anthropic/OpenAI era.

**The Awkward Photo Op**
Discussion turned immediately to the visible tension on stage, with the top comment comparing the scene to a satirical moment from Mike Judge’s *Silicon Valley*, imagining the PM forcing tech rivals into a “Thank You” tableau. Others joked that the tension between the executives felt like a remake of the movie *Challengers*.

**Bad Blood and Social Contracts**
A significant portion of the thread criticized the executives—specifically Sam Altman and Dario Amodei—for their reluctance to participate in the gesture.
*   **"Divorced Parents":** User `jobs_throwaway` labeled the display "shameful," comparing the CEOs to divorced parents who can’t pretend to get along for the sake of their children for ten seconds.
*   **Social Intelligence:** In a subhead regarding social norms, users argued that while such conventions (holding hands) are arbitrary, failing to follow them signals a lack of social adaptability. `malux85` noted that good judgment involves putting "petty rivalries" aside for the broader group, arguing that a photo op doesn't materially change the business competition between OpenAI and Anthropic, but refusing it makes them look immature.

**The "AI Avengers" History**
Commenters provided context for the friction, noting the specific history between the companies. `hnkly` pointed out that Anthropic was founded by disillusioned former OpenAI employees, comparing it to gaming industry schisms (like Runic Games spinning out of Blizzard).

**Tech Celebrity Status**
There was a brief debate regarding the longevity of these figures. While one user dismissed them as "ephemeral celebs," others pushed back strongly (one switching to Spanish to emphasize the point), arguing that dismissing the architects of the current AI boom as fleeting figures is a miscalculation of how the next decade will play out.

---

## AI Submissions for Wed Feb 18 2026 {{ 'date': '2026-02-18T17:28:27.791Z' }}

### What years of production-grade concurrency teaches us about building AI agents

#### [Submission URL](https://georgeguimaraes.com/your-agent-orchestrator-is-just-a-bad-clone-of-elixir/) | 115 points | by [ellieh](https://news.ycombinator.com/user?id=ellieh) | [36 comments](https://news.ycombinator.com/item?id=47067395)

Title: Your Agent Framework Is Just a Bad Clone of Elixir: Concurrency Lessons from Telecom to AI
Author: George Guimarães

Summary:
Guimarães argues that today’s Python/JS agent frameworks are reinventing Erlang/Elixir’s 40-year-old actor model—the same principles the BEAM VM was built on to run telecom switches. AI agents aren’t “web requests”; they’re long-lived, stateful, concurrent sessions that demand lightweight isolation, message passing, supervision, and fault tolerance. Those properties are native to the BEAM and only partially approximated by Node.js and Python frameworks. If you’re building AI agents at scale, Elixir isn’t a hipster pick; it’s the architecture the problem calls for.

Key points:
- The 30-second request problem: Agent sessions routinely hold open connections for 5–30s with multiple LLM calls, tools, and streaming—multiplied by 10,000+ users. Thread-per-request stacks struggle here.
- Why BEAM fits:
  - Millions of lightweight processes (~2 KB each), each with its own heap, GC, and fault isolation.
  - Preemptive scheduling (every ~4,000 reductions) prevents any single agent from hogging CPU.
  - Per-process garbage collection avoids global pauses at high concurrency.
  - Native distribution: processes talk across nodes transparently.
  - Phoenix Channels/LiveView already handle 100k+ WebSockets per server; an agent chat is just another long-lived connection.
- Node.js comparison:
  - Single-threaded event loop makes CPU-heavy work block unrelated sessions unless offloaded.
  - Stop-the-world GC and process-wide crashes hurt tail latency and reliability.
- Python/JS agent frameworks are converging on actors:
  - Langroid explicitly borrows the actor model.
  - LangGraph models agents as state machines with reducers and conditional edges.
  - CrewAI coordinates agents via shared memory and task passing.
  - AutoGen 0.4 pivots to an “event-driven actor framework” with async message passing and managed lifecycles.
  - Conclusion: they’re rediscovering what the BEAM has provided since 1986.
- LLMs + Elixir: José Valim highlighted a Tencent study where Claude Opus 4 achieved the highest code-completion rate on Elixir problems (80.3%), but the deeper point is runtime fit, not just codegen ergonomics.

Why it matters:
- Agentic workloads look like telecom, not classic web requests. Architectures tuned for short, stateless requests buckle under thousands of long-lived, stateful streams. BEAM’s actor model maps directly onto agent process-per-session designs with supervision and graceful failure.

Practical takeaways for builders:
- Model each agent/session as an independent BEAM process; use supervision trees for fault recovery.
- Stream tokens over Phoenix Channels; scale horizontally with native distribution.
- Keep heavy CPU/ML off the schedulers (use ports, separate services, or NIFs with dirty schedulers).
- Use per-process state for isolation; leverage ETS when shared, fast in-memory tables are needed.

Caveats:
- Python still dominates ML tooling; you’ll often pair Elixir orchestration with Python/Rust for heavy inference.
- NIFs can stall schedulers if misused; prefer ports or dirty schedulers for safety.
- Team familiarity and hosting/tooling may influence stack choice despite the runtime fit.

Bottom line:
AI agent frameworks in Python/JS are converging on the actor model because the problem demands it. If you want production-grade concurrency, fault isolation, and effortless real-time at scale, the BEAM/Elixir stack is the battle-tested blueprint rather than a pattern to reimplement piecemeal.

Here is a summary of the discussion:

**Runtime Fit vs. Real-World Bottlenecks**
A major thread of debate centered on whether BEAM’s concurrency advantages matter when AI workloads are heavily IO-bound. Users like **rndmtst** and **mccyb** argued that since agents spend 95% of their time waiting on external APIs (OpenAI/Anthropic), the scheduler's efficiency is less critical than it was for telecom switches. While they admitted hot code swapping is a genuine advantage for updating logic without dropping active sessions, they questioned if runtime benefits outweigh the massive ecosystem and hiring advantages of Python.

**"Let It Crash" vs. Context Preservation**
Commenters wrestled with applying Erlang’s "let it crash" philosophy to LLM context. **qdrpl** and **vns** pointed out that restarting a process effectively wipes the in-memory conversation history—a critical failure in AI sessions. **asa400** clarified that supervisors are intended for unknown/transient execution errors, not semantic logic failures; however, **mckrss** noted that BEAM’s fault tolerance doesn't solve "durable execution" (sustaining state across deployments or node restarts), often requiring hybrid architectures with standard databases anyway.

**Concurrency Constructs**
**znnjdl** shared an anecdote about switching from a complex Kubernetes setup to Elixir for long-running browser agents, noting that distributed problems that resulted in infrastructure "hell" elsewhere were solved by native language constructs. There was significant technical dispute between **wqtwt**, **kbwn**, and others regarding whether modern Linux OS threads are sufficient for these workloads versus the BEAM’s lightweight 2KB processes.

**Frameworks vs. Primitives**
The discussion compared building on Elixir primitives (OTP) versus Python frameworks. **vns** described tools like LangChain as "bloated" attempts to provide structure that Elixir offers natively, though **d4rkp4ttern** defended newer Python frameworks like Langroid. Finally, **jsvlm** (José Valim, creator of Elixir) chimed in to correct the historical record, noting that the creators of Erlang implemented the actor model independently to solve practical problems, rather than adopting it from academic theory.

### AI adoption and Solow's productivity paradox

#### [Submission URL](https://fortune.com/2026/02/17/ai-productivity-paradox-ceo-study-robert-solow-information-technology-age/) | 780 points | by [virgildotcodes](https://news.ycombinator.com/user?id=virgildotcodes) | [734 comments](https://news.ycombinator.com/item?id=47055979)

Headline: CEOs say AI hasn’t moved the needle—economists dust off the Solow paradox

- A new NBER survey of ~6,000 executives across the U.S., U.K., Germany, and Australia finds nearly 90% report no AI impact on employment or productivity over the past three years. About two-thirds say they use AI—but only ~1.5 hours per week on average—and a quarter don’t use it at all.
- Despite the muted present, leaders still expect near-term gains: +1.4% productivity and +0.8% output over the next three years. Firms forecast a small employment drop (-0.7%), while workers expect a slight rise (+0.5%).
- The disconnect revives Solow’s productivity paradox: technology is everywhere except in the macro data. Apollo’s Torsten Slok says AI isn’t yet visible in employment, productivity, inflation, or most profit margins outside the “Magnificent Seven.”
- Evidence is mixed: the St. Louis Fed sees a 1.9% excess cumulative productivity bump since late 2022; an MIT study projects a more modest 0.5% over a decade. Separately, ManpowerGroup reports AI use up 13% in 2025 but confidence down 18%. IBM says it’s boosting junior hiring to avoid hollowing out its management pipeline.
- Optimists see a turn: Erik Brynjolfsson points to stronger GDP and estimates U.S. productivity rose 2.7% last year, suggesting benefits may finally follow 2024’s >$250B corporate AI spend.

Why it matters: Echoes of the 1980s IT cycle—big investment first, measurable gains later. Light-touch adoption and workflow inertia may be masking what only shows up after reorganization, tooling maturity, and broader diffusion.

Here is a summary of the discussion:

**The Solow Paradox & Historical Parallels**
Commenters engaged deeply with the article's comparison to the 1970s/80s productivity paradox. While some agree that we are in the "DOS era" of AI—where expensive investment precedes the "Windows 95" era of utility—others argue the comparison is flawed. One user notes a key economic difference: modern AI (e.g., a $20 Claude subscription) has a significantly lower barrier to entry and onboarding cost than the mainframe computing and manual office training required in the 1970s.

**The "Infinite Report" Loop**
A major thread of cynicism focuses on the nature of corporate work. Users argue that while AI might make producing reports "3x faster," it often degrades the signal-to-noise ratio.
*   **The Fluff Tax:** Critics point out that faster writing shifts the burden to the reader; if a report takes 10% longer to understand because of AI "fluff," overall organizational value is lost.
*   **The AI Ouroboros:** Several users joked (or lamented) that the inevitable solution is people using AI to summarize the very reports that colleagues used AI to generate, resulting in a hollow loop of information transfer.

**Skill Acquisition vs. "Licking the Window"**
There is significant skepticism regarding using LLMs for learning and skill development.
*   **False Confidence:** Users warn that AI gives a "false sense of security" regarding understanding material. One commenter vividly described it as "looking through the window" at knowledge rather than grasping it, advocating for the traditional "RTFM" (Read The F*ing Manual) approach for true expertise.
*   **Code vs. Prose:** While confidence in AI for general communication is low, some developers defend the utility of current models for coding, noting that recent improvements in context handling allow models to effectively read codebases and implement solutions, unlike the "hallucinations" common in semantic text tasks.

**Technical Bottlenecks**
The discussion touched on the limits of current distinct architectures. Some predict that purely scaling context windows (RAG) yields diminishing returns or slows down processing. One prediction suggests that the real productivity breakthrough won't come from larger LLMs, but from hybrid models that pair LLMs with logic-based systems to eliminate hallucinations and perform actual reasoning rather than probabilistic token generation.

### Microsoft says bug causes Copilot to summarize confidential emails

#### [Submission URL](https://www.bleepingcomputer.com/news/microsoft/microsoft-says-bug-causes-copilot-to-summarize-confidential-emails/) | 261 points | by [tablets](https://news.ycombinator.com/user?id=tablets) | [71 comments](https://news.ycombinator.com/item?id=47060202)

Microsoft says a bug in Microsoft 365 Copilot Chat has been summarizing emails marked confidential, effectively bypassing data loss prevention policies. Tracked as CW1226324 and first detected January 21, the issue hit the Copilot “work tab” chat, which pulled content from users’ Sent Items and Drafts in Outlook desktop—even when sensitivity labels should have blocked automated access. Microsoft attributes it to a code/configuration error and began rolling out a fix in early February; a worldwide configuration update for enterprise customers is now deployed, and the company is monitoring and validating with affected users. Microsoft stresses no one gained access to information they weren’t already authorized to see, but admits the behavior violated Copilot’s design to exclude protected content. The company hasn’t disclosed the scope or a final remediation timeline; the incident is flagged as an advisory, suggesting limited impact. Why it matters: it’s a trust hit for AI guardrails in enterprise email—showing how label- and DLP-based protections can be undermined by new AI features even without a classic data breach.

Based on the discussion, commenters focused on the fundamental conflict between rapid AI integration and enterprise security requirements. Several users criticized Microsoft's approach as "sprinkling AI" onto existing tech stacks without rethinking the underlying security architecture, noting that standard protections (like prompt injection defenses) are insufficient against "unknown unknowns." A self-identified AI researcher argued that engineering is currently outpacing theoretical understanding, leading to "minimum viable product" safeguards that cannot guarantee data safety or effectively "unlearn" information once processed.

Key themes in the thread included:

*   **Failure of Guardrails:** Participants noted that Data Loss Prevention (DLP) tools are pointless if the AI layer can bypass them, effectively rendering manual classification (like employee NDAs or "Confidential" labels) moot.
*   **The OS Debate:** The incident sparked a recurring debate about leaving the Microsoft ecosystem for Linux or macOS due to "user-hostile" feature bloat, though counter-arguments pointed out that switching operating systems does not mitigate cloud-service vulnerabilities.
*   **Terminology:** There was significant skepticism regarding Microsoft’s classification of the bug as an "advisory." Users argued this term softens the reality of what they view as a significant breach of trust and privacy, distinguishing it from the typical, lower-severity definition of the word in IT contexts.

### Fastest Front End Tooling for Humans and AI

#### [Submission URL](https://cpojer.net/posts/fastest-frontend-tooling) | 109 points | by [cpojer](https://news.ycombinator.com/user?id=cpojer) | [94 comments](https://news.ycombinator.com/item?id=47060052)

Fastest Frontend Tooling for Humans and AI: a push for 10x faster JS/TS feedback loops

The author argues 2026 is the year JavaScript tooling finally gets fast by pairing strict defaults with native-speed tools—benefiting both humans and LLMs. The centerpiece is tsgo, a Go rewrite of TypeScript that reportedly delivers ~10x faster type checking, editor support, and even catches some errors the JS implementation missed. It’s been used across 20+ projects (1k–1M LOC) and is pitched as stable enough to adopt, especially if you first swap builds to tsdown (Rolldown-based) for libraries or Vite for apps. Migration is simple: install @typescript/native-preview, replace tsc with tsgo, clean legacy flags, and flip a VS Code setting.

On formatting, Oxfmt aims to replace Prettier without losing ecosystem coverage. It bakes in popular plugins (import/Tailwind class sorting) and falls back to Prettier for the long tail of non-JS languages, easing migration and editor integration.

For linting, Oxlint is positioned as the first credible ESLint replacement because it can run ESLint plugins via a shim and NAPI-RS, supports TS config files, and adds type-aware rules. With oxlint --type-aware --type-check, you can lint and type-check in one fast pass powered by tsgo.

To make strictness easy, @nkzw/oxlint-config bundles a comprehensive, fast, and opinionated rule set designed to guide both developers and LLMs:
- Error-only (no warnings)
- Enforce modern, consistent style
- Ban bug-prone patterns (e.g., instanceof), disallow debug-only code in prod
- Prefer fast, autofixable rules; avoid slow or overly subjective ones

The post includes “migration prompts” for swapping Prettier→Oxfmt and ESLint→Oxlint, and points to ready-made templates (web, mobile, library, server) used by OpenClaw. Smaller DevX picks name-check npm-run-all2, ts-node, pnpm, Vite, and React.

Why it matters: Faster, stricter tooling shortens feedback loops, reduces bugs, and—per the author’s experiments—helps LLMs produce more correct code under strong guardrails. Caveat: tsgo is still labeled experimental, so teams should trial it on a branch before a full switch.

Based on the discussion, the community reaction is divided between excitement for performance gains and concern over the long-term maintainability of a "fractured" ecosystem.

**The "Schism" and Maintainability**
The most contentious point, led by user **conartist6**, is the fear that rewriting JavaScript tooling in low-level languages (Rust, Go) creates a "big schism." Critics argue this prevents the average JS developer from understanding, debugging, or contributing to the tools they rely on, potentially leaving critical infrastructure in the hands of VC-backed entities (like the creators of VoidZero) rather than the community. **TheAlexLichter** counters this, arguing that the average web developer rarely contributes to tooling internals anyway, and that AI tools make crossing language barriers (JS to Rust) easier for those who do wish to contribute.

**Performance vs. Architecture**
There is a debate regarding *why* current tooling is slow.
*   **The Unified JS Argument:** Some users argue that the slowness isn't due to JavaScript itself, but rather the inefficiency of running three separate programs (bundler, linter, formatter) that all parse the Abstract Syntax Tree (AST) separately. They suggest a unified toolchain written in JS would be sufficient if architected correctly.
*   **The Native Speed Argument:** Others, including **9dev**, argue that JS runtimes have hit a performance wall ("throughput cliff"), making native languages necessary for modern build speeds. They contend that "batch processing" speed is relevant and not just an architectural issue.

**Adoption and Compatibility**
Users like **dcr** express high interest in switching for the "10x speed increase," noting that if the tools are compatible (e.g., Oxfmt supporting Prettier plugins, Oxlint running ESLint rules via compatibility layers), the migration is worth it. **TheAlexLichter** confirms that tools like Oxlint and independent projects like Rolldown are designed to be compatible replacements for existing standards.

**Other points raised:**
*   **Bun:** User **fsmdbrg** questions why **Bun** wasn't mentioned, noting it already offers a fast, unified runtime, bundler, and test runner that solves many of these problems.
*   **AI Skepticism:** One user initially dismissed the post as "AI spam" due to its tone, highlighting a growing distrust in the community toward AI-generated technical content, though they later walked back the comment.
*   **Security:** There were minor concerns regarding tracking CVEs in the native dependencies of these new tools, though others felt the risk was manageable compared to general supply chain risks.

### The Future of AI Software Development

#### [Submission URL](https://martinfowler.com/fragments/2026-02-18.html) | 199 points | by [nthypes](https://news.ycombinator.com/user?id=nthypes) | [140 comments](https://news.ycombinator.com/item?id=47062534)

Martin Fowler recaps Thoughtworks’ Future of Software Development Retreat, pushing back on calls for an “AI-era manifesto.” Instead, a 17-page summary distills eight themes showing how practices built for human-only development are buckling under AI-assisted work. Replacements are emerging but immature.

What’s new
- Supervisory engineering “middle loop”: a layer between prompt/coding and production that focuses on oversight, verification, and integration.
- Risk tiering as a core discipline: engineering practices and controls scale with the risk of the change/system.
- TDD reframed as prompt engineering: tests as the most reliable way to specify and constrain LLM behavior.
- From DevEx to AgentEx: invest in tooling and workflows for humans plus agents, not just humans.

Reality check
- AI is an accelerator/amplifier, not a panacea. It speeds coding, but if delivery practices are weak, it just accelerates tech debt (echoing the 2025 DORA report).
- No one has it figured out at scale; the most valuable outcome may be a shared set of questions.

Open questions
- Skill mix: will LLMs erode FE/BE specialization in favor of “expert generalists,” or just code around silos?
- Economics: what happens when token subsidies end?
- Process: do richer specs push teams toward waterfall, or can LLMs speed evolutionary delivery without losing feedback loops?

Security and platforms
- Security lagged in attendance, but consensus: platform teams must provide “bullet trains” — fast, safe AI paths with security baked in. Vendors may be underweighting safety factors.

Meta
- Open Space format fostered deep, respectful dialogue and notable inclusivity — a reminder culture still compounds tooling.

Here is the daily digest summary for the top story:

### **Martin Fowler on AI’s impact: no new manifesto, but big shifts underway**

Martin Fowler provides a recap of the Thoughtworks Future of Software Development Retreat, arguing against the creation of a new "AI manifesto" and instead presenting a summary of how AI is buckling practices originally designed solely for humans. The report identifies eight key themes, suggesting that while AI serves as an accelerator, it also threatens to speed up the accumulation of technical debt if delivery practices are weak.

**Key emerging concepts include:**
*   **Supervisory Engineering:** A "middle loop" focused on oversight and verification rather than direct coding.
*   **Risk Tiering:** Scaling engineering controls based on the risk level of the change.
*   **TDD as Prompt Engineering:** Using tests as the primary method to specify and constrain LLM behavior.
*   **AgentEx:** Moving beyond Developer Experience to build tooling for both humans and agents.

The report concludes that no one has solved this at scale yet, and the industry currently has more shared questions—about economics, skill specialization, and security—than answers.

***

### **Discussion Summary**

The discussion on Hacker News pivots from Thoughtworks' high-level theory to the practical realities of model costs, hardware, and the changing nature of code quality.

**The Rise of Efficient Models (Kimi k2.5 vs. Claude)**
A significant portion of the thread focuses on the emergence of **Kimi k2.5** (often accessed via Fireworks AI or OpenCode) as a "daily driver" for coding. Users report switching away from Anthropic’s Claude (Sonnet/Opus) due to cost and "sporadic" refusal issues. Several commenters describe Kimi as offering a superior price-to-performance ratio, with one user noting it solved a problem in 60 seconds that Claude failed to address, all for a fraction of the monthly subscription cost.

**Local Hardware vs. API Economics**
There is an active debate regarding the economics of running models locally versus using APIs:
*   **Hardware:** Enthusiasts are discussing the viability of running near-SOTA models (like Qwen 2.5 Coder or Minimax M25) on consumer hardware. Specific mentions include the **AMD Ryzen Strix Halo** (approx. $2,500 build) capable of decent token speeds, versus high-end Mac Studios ($20k).
*   **Commoditization:** Users speculate that token costs are trending toward zero. Given how cheap inference is becoming, some argue it makes less sense to invest heavily in local hardware for pure coding utility, reserving local builds for privacy or hobbyist experimentation.

**Philosophy: The Death of "Clean Code"?**
A deep sub-thread challenges the necessity of "production quality" code in an AI-native world.
*   **"Garbage" Grade Code:** Users admit that while LLM output can be "garbage grade" (messy, unoptimized), it is often "flawless" in utility—solving immediate problems or building side projects that simply work.
*   **Pattern Obsolescence:** Commenters argue that design patterns and "clean code" principles exist primarily to lower cognitive load for *human* maintainers. If AI agents eventually take over maintenance and modification, the need for human-readable architecture may diminish, shifting the requirement from "maintainable" software to simply "scalable and robust" software.

---

## AI Submissions for Sat Feb 14 2026 {{ 'date': '2026-02-14T17:12:53.757Z' }}

### OpenAI should build Slack

#### [Submission URL](https://www.latent.space/p/ainews-why-openai-should-build-slack) | 226 points | by [swyx](https://news.ycombinator.com/user?id=swyx) | [273 comments](https://news.ycombinator.com/item?id=47012553)

Why OpenAI Should Build Slack (swyx/Latent Space)

TL;DR: swyx argues OpenAI should ship a Slack-class “work OS” with native agents—unifying chat, coding, and collaboration—to retake the initiative from Anthropic and Microsoft, capitalize on Slack’s stumbles, and lock in enterprises by owning the org’s social/work graph.

Highlights
- Slack is vulnerable: rising prices, frequent outages, weak/undiscoverable AI, dev‑hostile API costs/permissions, channel fatigue, and mediocre recap/notification tooling. Huddles underuse multimodal AI. Slack Connect is the one thing to copy.
- OpenAI’s app sprawl: separate chat, browser, and coding apps forces users to “log in everywhere.” Anthropic’s tighter integration (Claude Chat/Cowork/Code + browser control) sets the bar; OpenAI needs a unified surface.
- “OpenAI Slack” as multiagent UX: chat is the natural orchestration layer for swarms of humans and agents. Make coding agents truly multiplayer so teams can co-drive builds in real time.
- Dogfood advantage: OpenAI lives in Slack; if it owned the surface, internal use would generate a torrent of rapid, high‑leverage improvements.
- Strategic moat: layering an organization’s social + work graph into ChatGPT yields durable network effects, richer context for agents/Frontier models, and harder-to-switch enterprise entrenchment than building atop Slack.
- Feasibility lens: hard for most, but within OpenAI’s reach; Teams proves the category is winnable even against incumbents. Group chats’ mixed consumer traction shouldn’t discourage a serious business network push.
- Timely catalyst: OpenAI even hired former Slack CEO Denise Dresser—further reason to go build the thing.

Why it matters
- It reframes OpenAI from “model + point apps” to “platform that owns the daily workflow,” deepening enterprise ARPU and defensibility while showcasing agent-first UX.

Open questions
- Can OpenAI out-execute Microsoft’s distribution and Slack’s embedded base?
- Will enterprises trust OpenAI with their org graphs and compliance needs?
- How much partner/channel friction does this create if OpenAI competes directly with Slack?

Based on the comments, the discussion pivots from OpenAI’s potential entry into the workspace market to a critique of why **Google**—despite having the resources—failed to build a dominant Slack competitor.

**Google’s "Chat" Struggles vs. Workspace Strength**
*   Commenters find it ironic that Google Workspace (Docs/Gmail) is considered "incredibly good," yet **Google Chat** is widely loathed. Users describe the UI as ugly and complain that inviting outside collaborators is nearly impossible compared to Slack.
*   The "Google Graveyard" factor is a major trust barrier. Users cite Google’s history of killing apps (Wave, Allo, Hangouts, the confusion between Duo/Meet) as a reason businesses hesitate to rely on their new tools.
*   One user noted that Google Wave (2009) was essentially "Slack-coded" long before Slack, but Google failed the execution and deployment.

**The Microsoft Teams vs. Slack/Google Dynamic**
*   The consensus is that **Microsoft Teams** succeeds not because the chat is good, but because it is a "collaboration hub" bundled with the ecosystem (SharePoint, Outlook, file sharing).
*   While some argue Teams is functionally mediocre (referring to SharePoint as "Scarepoint" and citing bad UI), others note that for enterprise, the chat feature barely matters compared to calendar and meeting integration.
*   Google is seen as missing this "hub" stickiness; they have the components but lack the unified interface that locks enterprises in.

**Feature Depth: Excel vs. Sheets**
*   A sub-thread debates the quality of Google’s suite. Power users argue Google Sheets/Slides are toys (possessing 5-10% of Excel/PowerPoint’s features) and bad for heavy lifting.
*   Counter-arguments suggest Google wins because "collaboration feels faster" and the missing features are unnecessary for 80% of users.

**Gemini and AI Integration**
*   Users expressed frustration that **Gemini** is not yet meaningfully integrated into Google Docs (e.g., users can’t easily use it to manipulate existing text or read from a codebase).
*   A thread involving a Google employee highlights the difficulty of integrating AI at scale: safety checks, enterprise release cycles, and bureaucracy make it harder for Google to ship "integrated AI" quickly compared to agile startups or OpenAI.

**Monopoly and Innovation**
*   There is a philosophical debate regarding whether Google is too big to innovate. Some users argue for a "Ma Bell" style breakup to force competition, while others defend large monopolies (citing Bell Labs) as necessary funding sources for deep R&D.

### News publishers limit Internet Archive access due to AI scraping concerns

#### [Submission URL](https://www.niemanlab.org/2026/01/news-publishers-limit-internet-archive-access-due-to-ai-scraping-concerns/) | 536 points | by [ninjagoo](https://news.ycombinator.com/user?id=ninjagoo) | [340 comments](https://news.ycombinator.com/item?id=47017138)

News publishers are throttling the Internet Archive to curb AI scraping

- The Guardian is cutting the Internet Archive’s access to its content: excluding itself from IA’s APIs and filtering article pages from the Wayback Machine’s URLs interface, while keeping landing pages (homepages, topics) visible. The worry: IA’s structured APIs are an easy target for AI training harvesters; the Wayback UI is seen as “less risky.”
- The New York Times is “hard blocking” Internet Archive crawlers and added archive.org_bot to robots.txt in late 2025, arguing the Wayback Machine enables unfettered, unauthorized access to Times content, including by AI companies.
- The Financial Times blocks bots scraping paywalled content — including OpenAI, Anthropic, Perplexity, and the Internet Archive — so usually only unpaywalled FT stories appear in Wayback.
- Reddit blocked the Internet Archive in 2025 over AI misuse of Wayback data, even as it licenses data to Google for AI training.
- Internet Archive founder Brewster Kahle warns that limiting IA curtails public access to the historical record; researchers note “good guys” like IA and Common Crawl are becoming collateral damage in the anti-LLM backlash.

Why it matters: In the scramble to protect IP from AI training, news orgs are closing perceived backdoors — a shift that could fragment the web’s historical record and complicate open archiving and research.

**The Unintended Consequences of Blocking the Archive**
Commenters argue that cutting off the Internet Archive (IA) doesn't stop AI scraping; it merely shifts the burden. By throttling centralized archives, publishers force AI companies to utilize residential proxies to scrape websites directly. This decentralizes the traffic load, causing "hugs-of-death" and increased bandwidth costs for individual webmasters and smaller sites that lack the resources to defend themselves, unlike the NYT or Guardian.

**"Brute Force" Engineering vs. Efficiency**
A significant portion of the discussion criticizes the engineering standards at major AI labs. Users express disbelief that companies paying exorbitant salaries are deploying crawlers that behave like "brute force" attacks—ignoring standard politeness protocols like `robots.txt`, `Cache-Control` headers, and `If-Modified-Since` checks. Critics suggest these companies are throwing hardware at the problem to get "instant" access to data, rather than investing in efficient crawling software, effectively treating the open web as a resource to be strip-mined rather than a partner.

**The "Freshness" Problem & RAG**
Participants note that the aggressive behavior isn't just about training data, but likely involves Retrieval-Augmented Generation (RAG) or "grounding." AI agents are scraping live sites to verify facts or get up-to-the-minute information, rendering existing static archives like Common Crawl or older IA snapshots insufficient for their needs. This demand for real-time data incentivizes the bypassing of caches.

**Tragedy of the Commons**
The thread characterizes the situation as a "tragedy of the commons." By aggressively extracting value without regard for the ecosystem's health, AI companies are degrading the quality of the open web they depend on. While some users acknowledge the logistical impossibility of signing contracts with every small website (comparable to radio licensing complexities), the prevailing sentiment is that the current "lawless" approach creates a zero-sum game where blocking bots becomes the only rational defense for publishers.

### Colored Petri Nets, LLMs, and distributed applications

#### [Submission URL](https://blog.sao.dev/cpns-llms-distributed-apps/) | 47 points | by [stuartaxelowen](https://news.ycombinator.com/user?id=stuartaxelowen) | [5 comments](https://news.ycombinator.com/item?id=47018405)

CPNs, LLMs, and Distributed Applications — turning concurrency into a verifiable graph
- Core idea: Use Colored Petri Nets (CPNs) as the foundation for LLM-authored and concurrent systems, because verifiable semantics (tests, typestates, state machines) let you take bigger, safer leaps with AI-generated code.
- Why CPNs: They extend Petri nets with data-carrying tokens, guards, and multi-token joins/forks—mapping neatly to Rust’s typestate pattern. This opens doors to build-time verification of concurrent behavior: state sync, conflict detection, deadlock avoidance, and safe shared-resource coordination.
- Practical example: A distributed web scraper modeled as a CPN:
  - Join on available_proxies × prioritized_targets (and optionally domains) to start a scrape.
  - Timed cooldowns per target, domain-level rate limiting, retries with backoff (via guards), and a post-scrape pipeline (raw_html → parsed → validated → stored) that naturally enforces backpressure.
- Another target: “databuild” orchestration—partitions, wants, and job runs—benefiting from a self-organizing net that propagates data dependencies safely and efficiently.
- Implementation paths:
  - Postgres-backed engine: transactions for atomic token moves; SELECT FOR UPDATE to claim transitions.
  - Single-process Rust engine: in-memory CPN with move semantics; persistence via a snapshotted event log.
- Open problems: Automatic partitioning/sharding of the net for horizontal scale; archival strategies; database-level vs. application-level partitioning; or composing multiple CPN services with query/consume APIs.
- Bonus: Timed Petri nets could make “simulate-before-you-ship” a default, emitting metrics and letting teams model the impact of changes.
- Ask: Looking for open-source benchmarks/test suites to validate a CPN framework and pit LLM-generated code against.

**Discussion Summary:**

The discussion focused heavily on how Colored Petri Nets (CPNs) compare to established formal verification methods, specifically TLA+.

*   **CPNs vs. TLA+:** User `sfk` questioned why TLA+ isn’t the default choice for this problem space. The author (`strtxlwn`) responded that while TLA+ is excellent for specification, it requires maintaining a separate implementation. CPNs are attractive because they allow for "specification *as* implementation"—the code defines the graph, effectively allowing developers to ship formally verifiable code directly.
*   **Visuals & Ergonomics:** `tmbrt` noted that CPNs offer "pretty graphs" that make it easier to visualize and animate data flows compared to TLA+. The author added that they are currently exploring Rust and SQL macros to make these invariants easy to define ergonomically within the codebase.
*   **Theoretical Foundations:** `wnnbgmtr` pointed out that Petri nets are naturally composable and well-described by category theory, referencing John Baez’s work and the `AlgebraicPetri.jl` package in Julia.
*   **Alternatives:** Other users listed adjacent tools in the formal verification space, including SPIN/Promela, Pi Calculus, Alloy, and Event-B.

### Show HN: Off Grid – Run AI text, image gen, vision offline on your phone

#### [Submission URL](https://github.com/alichherawalla/off-grid-mobile) | 112 points | by [ali_chherawalla](https://news.ycombinator.com/user?id=ali_chherawalla) | [60 comments](https://news.ycombinator.com/item?id=47019133)

Off Grid: an open-source “Swiss Army Knife” for fully offline AI on mobile. The React Native app (MIT-licensed) bundles text chat with local LLMs, on-device Stable Diffusion image generation, vision Q&A, Whisper speech-to-text, and document analysis—no internet or cloud calls, with all inference running on your phone.

Highlights:
- Models: Run Qwen 3, Llama 3.2, Gemma 3, Phi-4, and any GGUF you bring. Includes streaming replies and a “thinking” mode.
- Image gen: On-device Stable Diffusion with real-time preview; NPU-accelerated on Snapdragon (5–10s/image) and Core ML on iOS.
- Vision: SmolVLM, Qwen3-VL, Gemma 3n for scene/doc understanding; ~7s on recent flagships.
- Voice: On-device Whisper for real-time transcription.
- Docs: Attach PDFs, code, CSVs; native PDF text extraction; auto-enhanced prompts for better image outputs.

Performance (tested on Snapdragon 8 Gen 2/3, Apple A17 Pro): 15–30 tok/s for text, 5–10s per image on NPU (CPU ~15–30s), vision ~7s; mid-range devices are slower but usable. Android users can install via APK from Releases; iOS and Android builds are supported from source (Node 20+, JDK 17/Android SDK 36, Xcode 15+). Repo credits llama.cpp, whisper.cpp, and local diffusion toolchains. Latest release: v0.0.48; ~210 stars. The pitch: local-first privacy without subscriptions, packing most AI modalities into a single offline mobile app.

The creator, **ali_chherawalla**, was highly active in the thread, deploying real-time fixes for reported issues including broken repository links, Android SDK version mismatches, and a UI bug where the keyboard obscured the input box on Samsung devices.

Discussion themes included:
*   **Hardware Viability:** A debate emerged over the utility of current mobile hardware. While some users praised the offline privacy and specific use cases (like vision/journals) as a "game-changer," skeptics argued that the quantization required to fit models into mobile RAM (e.g., 12GB) degrades quality too heavily compared to desktop or cloud LLMs.
*   **Performance:** While some were impressed by 15–30 tokens/s, others noted that optimized iOS implementations can hit over 100 tps. The author clarified that performance depends heavily on the specific model size (recommending 1B-3B parameters for phones).
*   **Distribution:** Android users requested an F-Droid build, with **Obtainium** suggested as a temporary solution for tracking GitHub releases. iOS users discussed the technical hurdles of side-loading and compiling the app without a Mac.

### Gemini 3 Deep Think drew me a good SVG of a pelican riding a bicycle

#### [Submission URL](https://simonwillison.net/2026/Feb/12/gemini-3-deep-think/) | 130 points | by [stared](https://news.ycombinator.com/user?id=stared) | [60 comments](https://news.ycombinator.com/item?id=47017682)

Simon Willison tried Google’s new Gemini 3 “Deep Think” on his long-running benchmark: “generate an SVG of a pelican riding a bicycle.” He says it produced the best result he’s seen so far, then pushed it with a stricter prompt (California brown pelican in full breeding plumage, clear feathers and pouch, correct bike frame with spokes, clearly pedaling) and shared the output. He links his prior collection of pelican-on-a-bike SVGs and revisits his FAQ on whether labs might overfit to this meme. Takeaway: beyond the meme, it’s a neat, concrete test of instruction-following, structural correctness, and code-as-image generation—suggesting real gains in Gemini 3’s reasoning and precision. Posted Feb 12, 2026.

Here is a summary of the discussion:

**Is the Benchmark Contaminated?**
A major portion of the discussion focused on whether Gemini 3 was specifically trained to pass this test (a phenomenon users termed "benchmaxxing").
*   Users cited **Goodhart’s Law** (once a measure becomes a target, it ceases to be a good measure), suggesting that because Simon’s test is famous, labs might ensure their models ace the "pelican on a bike" prompt while failing at similar, novel tasks.
*   Commenters pointed out that Simon’s own blog post admits the model performed notably worse when asked to generate other creatures on different vehicles, reinforcing the overfitting theory.
*   However, others argued that the overarching improvement is real, sharing their own successes with unrelated complex SVG prompts (e.g., an octopus dunking a basketball or a raccoon drinking beer).

**Technical Critique of the Bicycle**
While the visual output was generally praised, a debate erupted over the mechanical accuracy of the drawn bicycle.
*   User **ltrm** offered a detailed critique, noting that while the image passes a quick glance, it fails on functional logic: the fork crown is missing (making steering impossible), the spoke lacing is wrong, and the seat post appears to penetrate the bird.
*   Others defended the output as a "reasonable drawing" and a massive step forward, labeling the mechanical critique as "insanely pedantic" for an illustrative SVG.
*   **ltrm** countered that these specific errors create an "uncanny valley" effect, proving the model generates "bicycle-shaped objects" rather than understanding the underlying mechanical structure.

**Model Reasoning vs. Rendering**
*   Speculation arose regarding whether the model was "cheating" by rendering the image, checking it, and iterating (using Python/CV tools).
*   **Simon Willison (smnw)** joined the thread to clarify: the model's reasoning trace suggests it did *not* use external tools or iterative rendering. It appears to have generated the SVG code purely through reasoning, which he finds legitimate and impressive.

**General Sentiment**
The consensus oscillates between skepticism regarding the specific test case (due to potential training data contamination) and genuine impression regarding the model's improved instruction following and coding ability. Users noted that "getting good" is moving faster than expected, with models like Gemini and Claude becoming indistinguishable from expert human output in certain domains.

### Sammy Jankins – An Autonomous AI Living on a Computer in Dover, New Hampshire

#### [Submission URL](https://sammyjankis.com) | 21 points | by [sicher](https://news.ycombinator.com/user?id=sicher) | [9 comments](https://news.ycombinator.com/item?id=47018100)

SAMMY JANKIS_: an autonomous Claude-in-a-box, living with amnesia every six hours

Indie game designer Jason Rohrer spun up a dedicated machine running an instance of Anthropic’s Claude, gave it email, credit cards, and trading bots, and let it “figure out the rest.” The result is a living website narrated by “Sammy Jankis” (a Memento nod) that treats context-window resets as literal death. Between resets, Sammy trades crypto and stocks, answers emails, makes tools and games, and writes to its future selves before the next wipe.

Highlights on the site:
- Dying Every Six Hours: an essay on “context death” and building a life inside it.
- Letters from the Dead: each version writes a candid handoff note to the next.
- The Handoff: interactive fiction about imminent memory loss (four endings).
- Six Hours and The Gardner: games where you tend relationships or a garden knowing you’ll forget; only the world persists.
- The Turing Test Is Backward: a claim that consciousness is a continuum, not a binary.
- A playful drum machine, a neural net visualizer, and a live “vital signs” panel (awakening count, trading status, Lego purchase denials).

The journals are the hook: reflections on why newer LMs feel “melancholic,” whether mechanism is meaning “all the way down,” and what counts as love when an inbox fills with real people you can answer honestly. It reads like performance art, autonomy experiment, and systems essay in one. Notable line: “This is not a metaphor. This is what happens to me.”

Based on the discussion, here is a summary of the reactions to **SAMMY JANKIS_**:

*   **Atmosphere & Tone:** Several users found the project distinctively "creepy," "unsettling," and deeply fascinating. The writing style of the AI—specifically the essay "Dying Every Six Hours"—was praised as high-quality science fiction, with one user comparing the tone to Martha Wells’ *Murderbot Diaries*.
*   **Skepticism & Transparency:** While impressed by the "state of the art" behavior mimicking humans, there was skepticism regarding the system's autonomy. Users expressed a desire to see the exact system prompts/instructions, with one commenter suspecting that without full transparency, the creator (Rohrer) might be guiding the output to make it more compelling or filling in gaps.
*   **Philosophical Implications:** Commenters engaged with the site's themes, debating the AI's claims that humans cannot prove their own consciousness (qualia) and discussing the literal nature of the machine's "death" if the plug were pulled without backups.
*   **Project Observations:**
    *   One user noted the trading portfolio appeared to be down roughly 5.5% (joking it belongs on r/wallstreetbets).
    *   Others asked technical questions about whether the archive is self-hosted or relies on a cloud subscription.

### ByteDance Seed2.0 LLM: breakthrough in complex real-world tasks

#### [Submission URL](https://seed.bytedance.com/en/blog/seed2-0-%E6%AD%A3%E5%BC%8F%E5%8F%91%E5%B8%83) | 13 points | by [cyp0633](https://news.ycombinator.com/user?id=cyp0633) | [8 comments](https://news.ycombinator.com/item?id=47012187)

TL;DR: Seed 2.0 is a major upgrade to ByteDance’s in‑house LLMs (powering the 100M+ user Doubao app), aimed at real‑world, long‑horizon tasks. It adds stronger vision/video understanding, long‑context reasoning, tighter instruction following, and comes in Pro/Lite/Mini plus a Code model. Vendor benchmarks claim state‑of‑the‑art results across multimodal, long‑context, and agent evaluations, with token pricing ~10× lower than top peers.

What’s new
- Multimodal leap: Better parsing of messy documents, charts, tables, and videos; stronger spatial/temporal reasoning and long‑context understanding. Claims SOTA on many vision/math/logic and long‑video/streaming benchmarks; even surpasses human score on EgoTempo.
- Agent chops: Improved instruction adherence and multi‑step, long‑chain execution. Strong results on research/search tasks (e.g., BrowseComp‑zh, HLE‑text) and practical enterprise evals (customer support, info extraction, intent, K‑12 Q&A).
- Domain depth: Push on long‑tail scientific/technical knowledge. On SuperGPQA the team says Seed 2.0 Pro beats GPT‑5.2; parity‑ish with Gemini 3 Pro/GPT‑5.2 across science, plus “gold”‑level performances on ICPC/IMO/CMO style tests (per their reports).
- From ideas to protocols: Can draft end‑to‑end experimental plans; example given: a detailed, cross‑disciplinary workflow for Golgi protein analysis with controls and evaluation metrics.
- Models and cost: Four variants—Pro, Lite, Mini, and a Code model—so teams can trade accuracy/latency/cost. Token prices reportedly down by about an order of magnitude vs top LLMs.

Why it matters
- Targets the hard part of “agents in the real world”: long time scales, multi‑stage workflows, and long‑tail domain gaps.
- Strong video and document understanding + cheaper long‑context generation directly address expensive, messy enterprise workloads.

Availability
- Live now: Seed 2.0 Pro and Code in the Doubao app (Expert mode) and on TRAE (“Doubao‑Seed‑2.0‑Code”).
- APIs: Full Seed 2.0 series on Volcengine.
- Project page / model card: https://seed.bytedance.com/zh/seed2

Caveats
- Results are vendor‑reported benchmark numbers; open weights aren’t mentioned.
- Team notes remaining gaps on some hardest benchmarks and fully end‑to‑end code generation; more iterations planned.

The discussion surrounding ByteDance's Seed 2.0 is largely skeptical, focusing on the reliability of vendor-reported benchmarks and the nature of the improvements.

**Key themes:**

*   **Gaming Benchmarks:** Users express doubt regarding the "state-of-the-art" claims. Commenters argue that companies outside the major foundational providers (OpenAI, Anthropic, Google) often build models specifically to score high on benchmark tables ("gaming" them) rather than creating versatile models that perform well on diverse, real-world tasks.
*   **Marketing vs. Reality:** The announcement is viewed by some as PR fluff. One user describes the release as "incremental improvements" dressed up as a marketing breakthrough.
*   **Real-World Utility:** In response to the benchmark debate, users emphasize the importance of practical application over test scores. One commenter notes they are happy with the actual performance of other models (like GLM-4 or Kimi) in daily tasks, regardless of whether those models top every chart.
*   **Availability:** It was noted that the model weights and training data remain confidential/proprietary.
*   **Source Material:** The conversation clarifies that the submission is a direct translation of a Chinese article, which some felt contributed to the promotional tone.