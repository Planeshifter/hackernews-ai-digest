import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Wed Apr 30 2025 {{ 'date': '2025-04-30T17:14:17.044Z' }}

### Xiaomi MiMo Reasoning Model

#### [Submission URL](https://github.com/XiaomiMiMo/MiMo) | 466 points | by [thm](https://news.ycombinator.com/user?id=thm) | [179 comments](https://news.ycombinator.com/item?id=43842683)

In an intriguing development within the AI community, Xiaomi has unveiled MiMo, a series of cutting-edge language models explicitly designed to maximize reasoning capabilities. Dubbed MiMo-7B, these models demonstrate extraordinary potential, giving larger models like the 32B a run for their money. What sets MiMo-7B apart is its rigorous pre-training and post-training regimen, aiming to optimize reasoning tasks rather than just language processing.

Key to MiMo's success is a refined data preprocessing pipeline and a strategic approach to pre-training. By augmenting their datasets with extensive synthetic reasoning data and employing a three-stage data mixture, the MiMo-7B model is prepped on a whopping 25 trillion tokens. This not only enhances reasoning patterns but also boosts efficiency in processing.

Post-training doesn't take a back seat; Xiaomi's MiMo team incorporated 130,000 challenging mathematics and code problems into reinforcement learning (RL) scenarios. Using rule-based accuracy for validation and innovative fine-grained reward systems, they've overcome traditional RL challenges like sparse rewards.

For the tech-savvy and curious, MiMo models are accessible on platforms like HuggingFace and ModelScope, complete with the MiMo-7B-Base and MiMo-7B-RL versions. The RL model, despite being smaller, rivals well-known models like OpenAI's o1-mini in terms of performance, especially in mathematics and code tasks.

Evaluation metrics further affirm MiMo's prowess, topping popular benchmarks and showcasing its robust reasoning and code-solving capabilities. With this release, Xiaomi not only propels its technological footprint but also provides a significant contribution to the open-source AI community, offering valuable insights for future advances in reasoning-focused language models.

**Hacker News Discussion Summary:**

1. **Language Model Focus: English vs. Chinese**  
   - Users debated the underrepresentation of Chinese-focused models in Western discourse. While companies like Xiaomi and DeepSeek (via 01.AI) are developing Chinese-first models, challenges include:
     - **Data Availability:** Common Crawl data—integral to training LLMs—is heavily English-dominated (43%), making non-English models harder to scale. Chinese data is harder to collect due to restricted internet access (e.g., censorship, "closed gardens" like Baidu) and lower-quality search results compared to Google.
     - **Benchmarks and Resources:** Scientific research and benchmarks are often English-centric, complicating the evaluation of Chinese models. Some argue synthetic data generation may compensate for limited Chinese corpora.

2. **Technical Aspects of MiMo’s Approach**  
   - Arcuru’s cryptic comment highlighted MiMo’s RL process, which leverages rule-based validation and high-quality synthetic datasets (e.g., 130k math/code problems) to improve reasoning. Others questioned whether RL’s role was overstated, with one user noting that smaller models might struggle with complex, lengthy inputs.

3. **Tokenization and Language Efficiency**  
   - English’s Latin script was seen as advantageous due to efficient tokenization (fewer characters, more flexible combinations). This contrasts with Chinese’s logographic system, which may require more tokens for the same semantic content.

4. **Challenges for Non-Native Speakers**  
   - Users noted that Mandarin models can produce literal or awkward translations for non-native speakers, even when technically correct. This raises concerns about cultural and contextual nuance in localized models.

5. **Multilingual vs. Localized Models**  
   - While some advocated for English-first models to maximize global reach, others emphasized the importance of multilingual support to capture diverse linguistic contexts. However, market dynamics (e.g., China’s focus on domestic models) complicate cross-regional adoption.

**Key Takeaways**  
The discussion underscores the technical and infrastructural hurdles in developing non-English LLMs, particularly Mandarin. While synthetic data and RL offer solutions, geopolitical and cultural barriers (e.g., data accessibility, localized benchmarks) remain significant. Xiaomi’s MiMo models exemplify progress in reasoning-focused AI, but broader shifts in data strategy and evaluation frameworks are needed for truly global language models.

### DeepSeek-Prover-V2

#### [Submission URL](https://github.com/deepseek-ai/DeepSeek-Prover-V2) | 378 points | by [meetpateltech](https://news.ycombinator.com/user?id=meetpateltech) | [75 comments](https://news.ycombinator.com/item?id=43847432)

In a fascinating leap in formal mathematical reasoning, DeepSeek-AI introduces DeepSeek-Prover-V2, a cutting-edge large language model tailored for formal theorem proving in Lean 4. But what makes this even more compelling is the unique way it harnesses reinforcement learning to tackle subgoal decomposition, driven by its predecessor, DeepSeek-V3.

Here's how it works: By initiating a recursive theorem proving pipeline, the system begins this journey with a cold-start training phase, wherein DeepSeek-V3 deconstructs complex problems into manageable subgoals. This results in a thought chain synthesized from proofs of these subgoals, effectively bridging informal and formal reasoning into a cohesive model.

DeepSeek-Prover-V2 steps up the game by incorporating reinforcement learning with synthetic cold-start data, allowing it to deftly handle problems unsolved by its smaller 7B counterparts. This evolution is clearly reflected in its remarkable performance, conquering 49 out of 658 challenges from the PutnamBench and achieving an impressive 88.9% pass ratio on the MiniF2F-test.

Accompanying this advancement is the introduction of ProverBench, a benchmark rich with 325 problems encapsulating real-world challenges from AIME competitions and a robust selection of textbook exemplars. This repository of problems serves as a testbed for evaluating both high-school and undergraduate-level mathematical conundrums.

Available in two powerful configurations, the 7B and the formidable 671B models, DeepSeek-Prover-V2 invites enthusiasts and scholars to explore its capabilities via Hugging Face's Transformers. For the curious and the keen, detailed datasets and proofs, including those for MiniF2F, are downloadable, making DeepSeek-Prover-V2 not just a pioneering tool but a community resource for advancing mathematical reasoning.

The discussion around DeepSeek-Prover-V2 and its approach to theorem proving involves several key themes:

1. **Technical Analysis of the Model**:  
   Users highlight the method of breaking problems into subgoals using reinforcement learning, comparing it to techniques like dynamic programming and cold-start training. Some note its distinction from prior models like **Kimina**, particularly in handling intermediate steps and error feedback. The example Lean 4 code snippet shared by *mcshcks* demonstrates hands-on testing of the model’s theorem-proving capabilities.

2. **Challenges in AI Problem-Solving**:  
   Participants discuss limitations of current models, such as maintaining context in large projects (*criley2*, *jhrmnn*) and scaling AI tools for codebases. Concerns about "hallucination" and AI-generated "bills of nonsensical steps" (*otabdeveloper4*) contrast with praise for specialized agents that improve context management (*prtymcprt*).

3. **Future Directions**:  
   Speculation arises around domain-specific expert models and wrappers (*smnwrds*, *Arcuru*), akin to Mixture-of-Experts (MoE) architectures, to delegate tasks to specialized submodels. Tools like **ProverBench** are noted as valuable for benchmarking.

4. **Humorous and Off-Topic Threads**:  
   Lighthearted tangents include sarcastic remarks about LLMs writing "XX-page documents on making a better peanut butter sandwich" (*ghtysxfr*) and links to unrelated topics like robotics (*jrvarela56*) or YouTube tutorials (*mls*).

5. **Human vs. AI Problem-Solving**:  
   Debates emerge on whether AI’s subgoal decomposition mirrors human reasoning or introduces rigidity. Some users stress the importance of human intuition in interpreting errors and maintaining project coherence, while others acknowledge AI’s growing role in technical workflows.

Overall, the conversation blends technical scrutiny of DeepSeek-Prover-V2’s innovations, reflections on AI’s evolving capabilities, and playful asides, reflecting both enthusiasm and skepticism about AI-driven formal reasoning.

### JetBrains defends removal of negative reviews for unpopular AI Assistant

#### [Submission URL](https://devclass.com/2025/04/30/jetbrains-defends-removal-of-negative-reviews-for-unpopular-ai-assistant/) | 203 points | by [przemub](https://news.ycombinator.com/user?id=przemub) | [130 comments](https://news.ycombinator.com/item?id=43850377)

JetBrains, a company known for its popular development tools, recently found itself in hot water after it removed negative reviews for its AI Assistant from its plugin marketplace. The controversial decision came to light when users noticed their critical feedback had vanished. JetBrains argued these reviews either violated their policies or addressed issues that had been solved. Despite having over 22 million downloads since its July 2023 release, the AI Assistant maintains a meager 2.3 out of 5 rating. Developers report frustrations such as automatic installation, slow performance, limited third-party support, and restricted core features.

In response to backlash, a JetBrains employee admitted that deleting multiple reviews at once may have seemed suspicious without proper communication. Users on social media speculated this move aimed to artificially improve the plugin’s ratings. Some developers echoed this sentiment, highlighting persistent issues like latency, inconsistent user experience, and scant documentation. A particularly vocal Reddit user labeled it “the annoying self-healing/reinstalling phoenix of a plug-in.”

To address competition and enhance its offerings, JetBrains introduced a free tier this month alongside a new AI agent named Junie, which has garnered more favorable reviews, although users noted its steep cost. This strategic move comes amidst pressure from free tools like Microsoft’s Visual Studio Code, as JetBrains seeks to balance its premium business model with community demands.

The Hacker News discussion surrounding JetBrains' removal of negative reviews for its AI Assistant plugin highlights widespread criticism of the company's transparency and handling of user feedback. Key points include:

1. **Critique of Review Moderation**:  
   Users argued that deleting reviews, even for "resolved" issues, erodes trust and removes valuable feedback. Many saw JetBrains’ actions as dishonest, likening it to censoring criticism. Comments emphasized that vendors typically respond to negative reviews instead of removing them, suggesting JetBrains prioritized optics over accountability.

2. **Plugin Quality Concerns**:  
   The AI Assistant’s poor performance, latency, auto-installation, and lack of third-party/local LLM support were repeatedly criticized. Users cited specific frustrations, such as restricted features, inconsistent UX, and sparse documentation, questioning the plugin’s premium pricing.

3. **Debate on Platform Policies**:  
   Some defended JetBrains’ right to moderate reviews under their policies but condemned the lack of communication when doing so. Others argued that outdated reviews should remain unless explicitly irrelevant, with calls for transparency (e.g., marking resolved issues instead of deletion).

4. **Broader Distrust in JetBrains’ Approach**:  
   Commenters linked the incident to broader concerns about the company’s profit-driven decisions, like bundling paid plugins and prioritizing proprietary tools over community needs. Comparisons to free alternatives (e.g., VS Code) underscored dissatisfaction with JetBrains’ value proposition.

5. **Side Discourse on HN Moderation**:  
   A sub-thread debated Hacker News’ own moderation policies after a detailed negative review was initially removed. Users expressed frustration with opaque rules and perceived censorship, highlighting tensions between free speech and platform governance.

**Sentiment Summary**:  
The discussion reflects deep skepticism toward JetBrains’ handling of criticism and product quality. Users perceive a pattern of prioritizing commercial interests over user experience, with the review deletions seen as emblematic of a lack of transparency. The incident has amplified existing frustrations with the AI Assistant’s shortcomings and JetBrains’ broader strategic choices.

### OCaml's Wings for Machine Learning

#### [Submission URL](https://github.com/raven-ml/raven) | 105 points | by [musha68k](https://news.ycombinator.com/user?id=musha68k) | [66 comments](https://news.ycombinator.com/item?id=43844279)

In an exciting development for the OCaml community, "Raven" is aiming to revolutionize machine learning and data science by bringing a Python-like ease and efficiency to the OCaml ecosystem. With a comprehensive suite of libraries and tools, Raven is designed to incorporate machine learning capabilities seamlessly into OCaml, leveraging its robust type safety and performance. Though Raven is still in the pre-alpha stage, its core components like Ndarray and Hugin are nearly feature-complete and open to user feedback.

Raven is meticulously structured with sub-projects like Ndarray for high-performance numerical computing, Ndarray-CV for computer vision tasks, and Hugin for creating stunning visualizations. An interesting addition is Quill, a notebook-style platform that encourages data exploration and sharing, aiming to rival Jupyter notebooks.

For those accustomed to Python, Raven’s documentation includes a comparison of Raven’s offerings against Python’s renowned libraries, illustrating how Raven might become the OCaml equivalent to familiar Python tools like NumPy and Matplotlib.

While some elements like deep learning frameworks are still in development, the Raven team welcomes contributions and feedback from developers and data scientists alike, inviting them to help shape the future of machine learning in OCaml. Operating under the ISC License, Raven remains accessible for both personal and commercial use, encouraging wide adoption and collaboration.

The Hacker News discussion on the Raven project and OCaml’s ecosystem revolves around several key themes:

### 1. **Notebooks vs. Traditional Development**
   - Some users debate the practicality of Jupyter-style notebooks (like Raven’s Quill) for interactive development. Proponents highlight their value for teaching, rapid prototyping, and data exploration. Critics argue notebooks encourage messy, stateful code and prefer REPLs or IDEs for structured workflows. One user suggests notebooks should serve as "quick-and-dirty" frontends, with code later refactored into modules.

### 2. **Comparison with Python and Other Languages**
   - While Raven aims to bring Python-like usability to OCaml, participants note Python’s dominance in ML is partly due to its accessible syntax and vast ecosystem. Some praise OCaml’s type safety and performance but acknowledge its steep learning curve and smaller community compared to Haskell or Elixir. A few suggest F# as a "middle ground" for ML workflows due to its .NET integration and JIT compilation.

### 3. **OCaml’s Challenges**
   - **Syntax**: OCaml’s syntax is polarizing. Critics call it archaic ("looks like shit"), while defenders argue it’s elegant once mastered. Some compare it unfavorably to Rust or Python’s readability.
   - **Multicore Support**: OCaml’s delayed multicore implementation is seen as a historical weakness, though recent progress is noted. Users contrast it with Erlang/Elixir’s concurrency model and Haskell’s parallelism.
   - **Community Growth**: OCaml’s niche status is attributed to limited marketing, unfamiliarity with its module system, and competition from trendier languages (e.g., Rust, Scala). Some hope tools like Dune (build system) will improve adoption.

### 4. **Raven vs. Owl**
   - Users question how Raven differentiates itself from **[Owl](https://github.com/owlbarn/owl)**, an existing OCaml ML library. Links suggest Owl is being restructured, and Raven could emerge as a successor, focusing on developer experience. The discussion lacks clarity on this point, reflecting Raven’s early stage.

### 5. **Technical Strengths of OCaml**
   - OCaml’s module system, type inference, and performance are praised. Fans argue it’s underrated for systems programming and research, but admit it struggles in commercial settings compared to Python or Java.

### 6. **Miscellaneous**
   - A brief tangent critiques OCaml’s "American" origins and lack of corporate backing, though Python (Dutch creator) is cited as counter-evidence to nationality affecting success.

### Key Takeaway
The thread reflects cautious optimism about Raven’s potential to modernize OCaml’s ML ecosystem but underscores challenges in overcoming Python’s dominance and OCaml’s historical quirks. The community’s focus areas include improving tooling, documentation, and addressing syntax barriers to attract broader adoption.

### "AI-first" is the new Return To Office

#### [Submission URL](https://www.anildash.com//2025/04/19/ai-first-is-the-new-return-to-office/) | 323 points | by [LorenDB](https://news.ycombinator.com/user?id=LorenDB) | [201 comments](https://news.ycombinator.com/item?id=43845089)

Tech CEOs are jumping on the latest bandwagon: declaring their companies to be "AI-first." This trend, spearheaded by Shopify's Tobi Lütke, involves demanding that AI usage be incorporated into performance reviews—a move that feels forced and unnecessary. In a humorous self-analysis, the author notes that while AI tools can be helpful for those less skilled in certain areas, they may not add value for those already excelling at their jobs. So why this push from the tech elite?

The phenomenon is likened to a form of groupthink, where tech leaders echo each other's phrases to show alignment with the latest industry trends. This has been seen before in the insistence on returning to the office and other performative behaviors. CEOs seem to be more interested in maintaining appearances within their exclusive circles than genuinely considering what might benefit their employees or the business.

A practical approach would be to offer evaluated AI tools as optional resources rather than mandatory requisites, allowing employees to choose what genuinely enhances their productivity. However, such a sensible strategy wouldn't fit in with the attention-seeking behaviors of tech tycoons who prefer to impose AI mandates as a way to signal their participation in the latest tech craze.

In conclusion, perhaps it's time to adopt a more balanced attitude towards technology—one that welcomes innovation without succumbing to fad-driven group pressures. Recognizing the value in diverse sources of innovation, including academia and open-source communities, could foster a more constructive and inclusive tech culture.

**Summary of Discussion:**

The discussion revolves around critiques of Shopify CEO Tobi Lütke’s management style, dubbed the "Tobi Tornado," characterized by abrupt, disruptive changes and high-pressure demands. Commenters compare this to "Seagull Management," where leaders swoop in, impose decisions, and leave chaos in their wake. While some defend Tobi’s approach as effective for Shopify’s growth, others argue it fosters instability and employee burnout, likening it to Elon Musk’s controversial leadership at Tesla and SpaceX. Critics highlight the human cost of such methods, noting stress and disrespect for work-life balance.

Debates also address Shopify’s business practices, with users raising concerns about data privacy and legal compliance in Europe, accusing the company of "shady" tactics in handling customer information. Skepticism emerges about the sustainability of performative tech trends, such as mandatory AI integration, with calls for optional tools instead of top-down mandates. Broader themes include critiques of Silicon Valley’s obsession with hyper-productivity, the ethics of leadership styles prioritizing innovation over employee well-being, and the tension between founder-driven vision and organizational stability. The conversation underscores a desire for balanced, inclusive approaches to technology and management that value diverse perspectives beyond CEO-driven hype.

### Satya Nadella says as much as 30% of Microsoft code is written by software

#### [Submission URL](https://www.cnbc.com/2025/04/29/satya-nadella-says-as-much-as-30percent-of-microsoft-code-is-written-by-ai.html) | 48 points | by [shinryudbz](https://news.ycombinator.com/user?id=shinryudbz) | [52 comments](https://news.ycombinator.com/item?id=43841868)

At the recent LlamaCon AI developer event, held by Meta in Menlo Park, California, Microsoft CEO Satya Nadella announced a groundbreaking shift in software development: up to 30% of the code within Microsoft’s repositories is now penned by artificial intelligence. Speaking alongside Meta's CEO Mark Zuckerberg, Nadella highlighted the rapid integration of AI into coding processes, noting a continual increase in AI-written code at the tech giant.

In a striking revelation, Zuckerberg shared that Meta aims to create an AI model capable of developing up to half of its new AI models within the next year. This highlights a broader trend of AI's growing role in software development across the tech industry. While exact figures for Meta's AI-generated code remain unspecified, the ambition mirrors a substantial shift in how leading tech companies like Google and Duolingo are leveraging AI to enhance efficiency and innovation.

This movement echoes the transformative potential of AI beyond tech, with AI-generated content and solutions being increasingly preferred over human efforts in various fields such as customer service and creative sectors. As companies like Shopify and Duolingo pivot towards AI-first thinking, there is a clear indication that automated code generation may become a new industry standard. As AI continues to gain ground, the tech landscape is poised for significant evolutions, challenging companies to adapt swiftly to a future where machines could outpace human coders in both volume and capability.

The Hacker News discussion surrounding Microsoft's claim that 30% of its code is AI-generated reveals widespread skepticism and nuanced critiques. Key themes include:

1. **Skepticism of Metrics**:  
   Many users question the validity of the "30%" figure, suggesting it could be marketing hype. Some argue the metric (e.g., lines of code vs. critical logic) is misleading, as AI may handle boilerplate, tests, or IDE-generated code rather than meaningful engineering work. Comparisons are drawn to legacy tools like IntelliSense or project templates (Rails/Django), which have automated code for years.

2. **Code Quality Concerns**:  
   Commenters express doubt that AI-generated code improves quality, citing risks of bugs, licensing issues, or "code bloat." One user notes that AI often produces verbose, repetitive patterns (e.g., `actualLogicFoo` boilerplate) requiring human cleanup. Others joke that Microsoft’s software quality (e.g., Windows 11 glitches) might *decline* if AI contributes significantly.

3. **Impact on Developers**:  
   While some fear AI could displace junior roles by automating routine tasks, others argue it merely accelerates grunt work (e.g., writing tests, API contracts). A recurring point: AI may shift developer focus to *prompt engineering* and debugging rather than eliminating jobs outright.

4. **Historical Parallels**:  
   Users compare the trend to past hype cycles, like Clippy’s resurgence as "Copilot," and note that auto-generated code (e.g., via IDE wizards) isn’t novel. Some mock Microsoft’s branding efforts ("Windows AI") as repackaged tools.

5. **Off-Topic Humor**:  
   A few comments derail into jokes about Windows’ performance, Mordor-like corporate landscapes, and absurd interview anecdotes, reflecting broader cynicism toward tech giants’ narratives.

Overall, the discussion underscores a divide: while AI code generation is seen as a productivity tool for mundane tasks, its portrayal as a revolutionary shift is met with doubt, with many emphasizing that human oversight remains critical.

### Wikipedia says it will use AI, but not to replace human volunteers

#### [Submission URL](https://wikimediafoundation.org/news/2025/04/30/our-new-ai-strategy-puts-wikipedias-humans-first/) | 79 points | by [thm](https://news.ycombinator.com/user?id=thm) | [41 comments](https://news.ycombinator.com/item?id=43846052)

In a recent announcement, the Wikimedia Foundation made it clear that replacing Wikipedia's dedicated human editors with AI is not on the agenda. Instead, their new AI strategy aims to empower these passionate volunteers. Over its 25-year history, the human touch has been vital to Wikipedia’s success, as editors have contributed tirelessly to build the world’s largest encyclopedia. Wikimedia's plan is to harness AI to both streamline technical processes and eliminate repetitive tasks, freeing up volunteers to focus on enriching Wikipedia's content.

AI will assist moderators by automating mundane tasks to ensure the integrity of information and improve content discoverability to ease editor research. It will also facilitate the translation of content, enhancing Wikipedia’s global reach, and improve onboarding for new volunteers through AI-guided mentorship. Wikimedia emphasizes that this technology will be implemented with their core values—such as privacy, transparency, and multilingual inclusivity—at the forefront. The Foundation intends to utilize open-source AI tech, ensuring their continued responsibility to provide accessible, reliable knowledge.

This AI initiative promises to be a collaboration, rather than a replacement, reinforcing Wikipedia's mission to remain a pillar of free and open knowledge for everyone. You can explore the full details of Wikimedia’s AI strategy on Meta-Wiki, crafted under the guidance of Chris Albon, Director of Machine Learning, and Leila Zia, Head of Research at the Foundation.

The Hacker News discussion on Wikimedia's AI strategy highlights cautious optimism and notable concerns among commenters. Key points include:

1. **Human-AI Collaboration**:  
   - Many support AI assisting with repetitive tasks (e.g., vandalism detection, translation) but stress **human oversight** remains critical. Concerns arise about accountability if AI makes errors in moderation or content generation.

2. **Translation Challenges**:  
   - While AI could improve Wikipedia’s multilingual reach, users note **quality disparities** between languages (e.g., detailed German articles vs. sparse Cebuano content). Some argue machine translation risks losing cultural nuance and that human input is still essential.

3. **Mentorship and Onboarding**:  
   - Skepticism exists around AI-guided mentorship replacing human interaction. Suggestions include using AI to flag problematic edits while preserving community-driven guidance for new contributors.

4. **Content Quality Risks**:  
   - Fears of “**AI slop**” and **recursive loops** if AI-generated content pollutes Wikipedia’s repository. Critics warn this could erode reliability, especially if businesses exploit AI-generated text.

5. **Open-Source and Commercialization**:  
   - Trust in Wikimedia’s commitment to open-source tools is mixed. Some speculate hidden profit motives (e.g., selling AI tools to universities), though others defend its non-profit ethos.

6. **Existing Bots vs. AI**:  
   - Users contrast pre-AI automation (limited, rule-based bots) with LLMs’ potential to either worsen inaccuracies or creatively solve issues like citation sourcing.

Overall, the community acknowledges AI’s potential to ease workloads but emphasizes vigilance to preserve Wikipedia’s human-driven integrity and avoid over-reliance on imperfect systems.

---

## AI Submissions for Tue Apr 29 2025 {{ 'date': '2025-04-29T17:16:44.788Z' }}

### Chain of Recursive Thoughts: Make AI think harder by making it argue with itself

#### [Submission URL](https://github.com/PhialsBasement/Chain-of-Recursive-Thoughts) | 514 points | by [miles](https://news.ycombinator.com/user?id=miles) | [225 comments](https://news.ycombinator.com/item?id=43835445)

In today's Hacker News roundup, we delve into an intriguing project called "Chain of Recursive Thoughts" (CoRT), where an innovative approach has been applied to enhance AI decision-making. Created by the user PhialsBasement, this open-source venture explores the potential of making AI models ‘argue with themselves’ to improve their problem-solving skills. The methodology harnesses the power of self-evaluation and recursive thinking, pushing AI to generate multiple alternatives, rigorously assess them, and select the most optimal response.

The project has shown particularly impressive results when paired with the smaller-sized Mistral 3.1 24B model, dramatically boosting its performance, especially in programming tasks. Key to this success is the "AI battle royale" approach, where various potential solutions are pitted against each other, with only the top choice surviving to become the final answer. 

For those keen on experimenting, PhialsBasement provides clear instructions on setting up and running the code via a Web UI, though it's still in early development. Users interested in enhancing this innovative setup are encouraged to contribute, with the project licensed under MIT, giving plenty of freedom for creative adaptations and improvements.

This project underscores the power of iterative refinement and dynamic thinking in AI, making it a fascinating area for developers and AI enthusiasts eager to push the boundaries of machine intelligence.

**Summary of Discussion:**  
The Hacker News discussion around the "Chain of Recursive Thoughts" (CoRT) project explores both enthusiasm and critical considerations for recursive AI verification methods. Key points include:

1. **Verification Challenges & Solutions**:  
   - While CoRT’s "AI battle royale" approach (generating multiple solutions and filtering via verifiers) shows promise, users note **task-dependent efficacy**. For example, mathematical proofs are easier to verify programmatically, but LLMs like GPT-4 often produce flawed reasoning that requires external checkers.  
   - In software engineering, LLM-generated code may include vulnerabilities, but verification tools (compilers, linters, test suites) can improve reliability. Automatic test reruns help but **require well-defined cases** to avoid false confidence.  
   - Some argue verification is harder than generation, though projects like CoRT demonstrate significant accuracy gains when combining LLMs with external validators.

2. **Practical Implementation**:  
   - Users share experiences with **temperature settings** affecting output quality, with lower temperatures yielding more focused—though less creative—results.  
   - Ideas like **Monte Carlo Tree Search** (MCTS) for LLMs are suggested as complementary methods to refine reasoning, albeit with higher computational costs.  
   - Iterative self-critique workflows (e.g., AI generating a report, critiquing it, then revising) are praised for improving results, though some call it "clunky."

3. **Human vs. AI Judgment**:  
   - Debate arises over **replacing human evaluation**. While tools like LLM-as-judge frameworks (LangChain, LlamaIndex) aid scalability, they’re seen as supplementary rather than substitutes for human oversight.  
   - Concerns about **sycophancy** (LLMs echoing user biases) highlight the need for diverse, context-aware verification steps.

4. **Anecdotes & Alternatives**:  
   - Users mention tools like Gemini’s large context window for maintaining project-specific knowledge and Sillytavern’s group-chat approach for multi-agent debates.  
   - Training data overlap and entropy reduction techniques (e.g., k-fold cross-validation) are noted as inspirations for improving model robustness.  

**Conclusion**:  
The community views CoRT’s recursive verification as a promising step toward enhanced AI reasoning but emphasizes the need for hybrid approaches (human + automated checks), task-specific adaptations, and cost-effective scalability. Practical experimentation, iterative workflows, and leveraging external validators are highlighted as effective strategies.

### An illustrated guide to automatic sparse differentiation

#### [Submission URL](https://iclr-blogposts.github.io/2025/blog/sparse-autodiff/) | 121 points | by [mariuz](https://news.ycombinator.com/user?id=mariuz) | [20 comments](https://news.ycombinator.com/item?id=43828423)

In the bustling world of machine learning, we're all familiar with automatic differentiation, a method that swiftly calculates gradients essential for model optimization. But dive deeper, and you'll discover the lesser-known sibling: automatic sparse differentiation (ASD). This specialized process capitalizes on the sparseness of Hessians and Jacobians—those large, unwieldy matrices where most elements are zero—common in scientific and engineering applications. By focusing only on the non-zero elements, ASD can dramatically speed up computations and cut down on memory usage.

Our illustrated guide starts with the basics: understanding traditional automatic differentiation (AD) and its role in computing Jacobians using both forward and reverse modes. From there, we explore the heart of ASD, which hinges on two core techniques: detecting sparsity patterns and employing matrix coloring. These strategies enable the efficient calculation of sparse Jacobians, and subsequently, sparse Hessians.

For those entrenched in machine learning, the benefits are clear. While first-order optimization via gradients is standard, utilizing sparse structures can significantly enhance the performance of second-order methods, which deal with these large matrices. However, ASD remains underutilized, partly because its theoretical foundations were developed outside the mainstream machine learning discourse. This post aims to bridge that gap, showcasing how ASD can revolutionize computational efficiency in real-world applications.

Through a practical demonstration within the post, readers can see firsthand how ASD shines when handling high-dimensional data, providing benchmarks and scenarios where it surpasses traditional methods. By demystifying ASD’s approach to leveraging sparsity, this guide opens the door to faster, more memory-conscious machine learning optimizations, paving the way for broader adoption in cutting-edge applications.

The Hacker News discussion on the blog post about **automatic sparse differentiation (ASD)** highlights technical insights, resource sharing, and debates on its mathematical foundations. Here's a concise summary:

### Key Discussion Points:
1. **Technical Insights**:
   - ASD’s efficiency in handling sparse Jacobians/Hessians was praised, with users noting its underuse in machine learning despite benefits for second-order optimization.
   - Methods like **sparsity pattern detection** and **matrix coloring** were discussed as critical for optimizing computations.

2. **References & Tools**:
   - Users cited foundational papers (e.g., Schmidhuber’s 1999 NIPS work) and textbooks (Trefethen’s *Numerical Linear Algebra*, Strang’s *Linear Algebra*).
   - Julia libraries like [`SpAutoDiff.jl`](https://github.com/rdyro/SpAutoDiff.jl) and tools like **Enzyme** were shared as practical implementations.
   - A prior [benchmark blog post](https://clr-blogposts.github.io/2024/blog/bench-hvp) inspired the submission, focusing on Hessian-vector products.

3. **Mathematical Debates**:
   - Participants debated whether ASD’s foundations lie in **calculus/numerics** or **computer science**, with some emphasizing its roots in traditional AD and FORTRAN-era optimizations.
   - Questions arose about prerequisites for understanding ASD, with recommendations for calculus, linear algebra, and graph theory basics.

4. **Educational Resources**:
   - Users highlighted the need for accessible explanations, linking to arXiv preprints and suggesting simplified programming projects to grasp concepts like matrix coloring.

5. **Miscellaneous**:
   - The blog’s clean design (based on MIT’s **Al-Folio** theme) and use of Markdown/LaTeX were briefly noted.
   - Some users humorously admitted struggling to grasp the dense material, reflecting the technical complexity of the topic.

### Community Sentiment:
- Enthusiastic engagement with ASD’s potential, though some found the concepts challenging without foundational math knowledge.
- Appreciation for practical code examples and efforts to bridge theory with real-world ML applications.

### Duolingo will replace contract workers with AI

#### [Submission URL](https://www.theverge.com/news/657594/duolingo-ai-first-replace-contract-workers) | 153 points | by [donohoe](https://news.ycombinator.com/user?id=donohoe) | [106 comments](https://news.ycombinator.com/item?id=43827978)

In a bold move towards embracing the future, Duolingo is set to transform into an "AI-first" company, as announced by cofounder and CEO Luis von Ahn. Revealed in an all-hands email shared on LinkedIn, this strategic pivot means AI will progressively take over tasks currently handled by contractors, allowing employees to focus on more creative and critical challenges. Drawing from Duolingo’s successful bet on mobile-first innovation in the past, von Ahn believes this shift to AI will enable the language-learning platform to rapidly scale its content and enhance features, outpacing traditional methods.

Von Ahn clarified that this isn't about substituting their human-like mascot, Duo, with artificial intelligence, but rather removing repetitive bottlenecks to empower the existing workforce with more meaningful work. The initiative includes integrating AI in hiring processes and performance assessments, and justifying new hires only if automation isn't feasible. This change mirrors a similar directive recently highlighted by Shopify's CEO, who urged teams to leverage AI solutions before requesting additional resources.

Despite the shift, Duolingo remains committed to its employees, promising more support in AI training and tools, aiming to turn the upcoming transitions into a positive progression towards achieving its educational mission. With this leap, Duolingo is poised to redefine content creation and feature development, anchoring itself as a leader in integrating AI into its foundational operations.

The Hacker News discussion on Duolingo's AI-first pivot reveals a mix of skepticism, critique of its educational model, and broader concerns about AI's role in the workplace. Key points include:

1. **Skepticism Toward Corporate Motivations**:  
   Users question whether Duolingo’s shift is driven by genuine innovation or cost-cutting, with parallels drawn to Shopify’s recent AI-driven layoffs. Some argue the move mirrors typical corporate trends where "AI-first" rhetoric masks efforts to reduce labor costs, prioritizing shareholder returns over employee welfare. Others worry AI-driven productivity gains might inflate expectations without fair compensation for workers.

2. **Criticism of Duolingo’s Educational Value**:  
   Many criticize Duolingo’s gamified approach as ineffective for serious language learning, calling it a "lazy game" that prioritizes engagement over foundational skills (e.g., grammar, verb conjugation). Comparisons to tools like Anki, HelloChinese, or LingoDeer highlight frustration with Duolingo’s superficial content and rigid learning paths. Some users canceled subscriptions, arguing the platform feels more like a monetized game than a robust educational tool.

3. **AI’s Impact on Developers and Jobs**:  
   Developers express concern that AI-generated code could devalue their roles, likening the trend to past hype cycles (e.g., crypto). Critics warn that companies might use AI to justify layoffs or suppress wages, especially if automation handles tasks traditionally done by contractors. Others caution against overestimating AI’s current capabilities, noting that non-developers pushing AI tools often lack technical understanding.

4. **Defense of AI in Learning Tools**:  
   A minority defend AI’s potential, citing platforms like Anki for spaced repetition and vocabulary retention. However, even proponents acknowledge Duolingo’s limitations, arguing its AI integration needs to enhance, not replace, structured learning methods.

5. **Broader Distrust of Corporate AI**:  
   Users highlight a pattern of companies using AI buzzwords to mask profit-driven decisions, often at the expense of user experience and product quality. Privacy concerns and fears of "AI-generated mediocrity" emerge, with skepticism about whether automation will improve content or merely streamline costs.

**In Summary**: The discussion reflects apprehension about Duolingo’s AI pivot, blending doubts about its educational efficacy, ethical implications for workers, and broader cynicism toward corporate AI narratives. While some see potential in AI-augmented tools, many fear the changes prioritize business metrics over meaningful learning outcomes and fair labor practices.

### Generative AI is not replacing jobs or hurting wages at all, say economists

#### [Submission URL](https://www.theregister.com/2025/04/29/generative_ai_no_effect_jobs_wages/) | 331 points | by [pseudolus](https://news.ycombinator.com/user?id=pseudolus) | [478 comments](https://news.ycombinator.com/item?id=43830613)

In a fascinating twist on the tech narrative, a recent paper by economists Anders Humlum and Emilie Vestergaard suggests that generative AI, despite being the tech industry's darling with billions in investment, hasn't quite delivered the economic revolution some predicted. Their study examined the impact of AI chatbots, like ChatGPT, across 11 job categories in Denmark, from educators to software developers, and found no significant wage or labor effects. 

This revelation complicates the tech industry's hype about AI's economic potential and raises questions about the massive capital poured into AI infrastructure. The economists' research indicated that while chatbot adoption is high, the economic benefits are minimal, with time savings clocking in at a mere 2.8% of work hours, tantamount to just over an hour per typical workweek. 

Interestingly, the speedy adoption of AI chatbots has not translated into drastic changes or benefits. Instead, it's reshaping jobs by creating new tasks, including those monitoring whether AI tools, like ChatGPT, are used for cheating—tasks that ironically eat into potential time savings. Humlum points out that while these new activities could, in a best-case scenario, lead to higher-value tasks and possibly wages, the overall productivity gains remain limited. 

Moreover, only a sliver of any productivity boost reaches workers' pockets, with just 3-7% seen in higher earnings. While AI might save time on activities like emailing, it doesn't necessarily mean employees can take on more work to increase earnings. This insight frames a backdrop where firms might reap more benefits than workers, casting a nuanced light on the AI advancement narrative and hinting at a future where the real economic game-changer status of AI is still on the horizon.

The discussion surrounding the study on generative AI's economic impact reflects widespread skepticism and nuanced debates. Key points include:

1. **Minimal Measurable Impact**: Users highlight the study’s finding of only **2.8% time savings** (≈1 hour/week) from AI tools, questioning whether such marginal gains justify the hype or investment. Critics argue these savings often fail to translate into wage increases, as companies absorb efficiency gains without redistributing benefits to workers.  

2. **Productivity vs. Wages**: A recurring theme is the disconnect between AI-driven productivity and wage growth. While AI might streamline tasks (e.g., drafting emails), participants note that saved time often leads to **new administrative burdens** (e.g., monitoring AI use) or increased workload demands rather than higher pay. Some argue companies prioritize revenue growth over employee compensation.

3. **Historical Analogues**: Commenters draw parallels to past technological shifts, like textile automation, which reshaped jobs without reducing employment long-term. Others cite **Jevons Paradox**—efficiency gains (e.g., cheaper software) may spur demand for new services, creating unforeseen roles while rendering others obsolete.

4. **Technical Realities**: Developers debate AI’s practical utility in technical fields (e.g., coding, UI design). While some praise tools like ChatGPT for accelerating tasks (e.g., generating code drafts), others note limitations, emphasizing that **baseline expertise** remains critical. Efficiency gains here are often incremental (e.g., saving 5% of time) rather than transformative.

5. **Economic Structures**: Discussions critique systemic factors, such as corporate greed, regulatory capture in education, and rising student debt, which may distort AI’s potential benefits. Critics argue markets increasingly favor price-fixing over competition, dampening consumer gains from AI-driven cost reductions.

6. **Skepticism of Hype**: Many dismiss the AI “revolution” narrative as market-driven speculation, stressing that real-world adoption often falls short of transformative claims. The study’s focus on Denmark is seen as a caution against overgeneralizing AI’s global impact.

In essence, the consensus leans toward **cautious doubt**: while AI introduces incremental efficiencies, structural economic forces and historical precedents suggest its revolutionary potential remains unrealized, with benefits disproportionately favoring firms over workers.

### Waymo and Toyota outline partnership to advance autonomous driving deployment

#### [Submission URL](https://waymo.com/blog/2025/04/waymo-and-toyota-outline-strategic-partnership) | 378 points | by [ra7](https://news.ycombinator.com/user?id=ra7) | [351 comments](https://news.ycombinator.com/item?id=43839123)

Toyota and Waymo are teaming up in an exciting new venture to boost the development of autonomous driving technologies. This dynamic duo, along with Woven by Toyota, aims to blend their strengths to build a cutting-edge autonomous vehicle platform. The partnership is rooted in a shared vision for improving road safety and expanding mobility options for everyone.

Toyota, known for its commitment to reducing traffic accidents, brings its expertise in vehicle development and advanced safety technologies to the table. The partnership will explore integrating Waymo's top-notch autonomous technology with Toyota's next-generation personally owned vehicles (POVs). Waymo, a frontrunner in autonomous driving, already logs numerous safe trips weekly in cities like San Francisco and Phoenix, boasting impressive safety records.

Hiroki Nakajima from Toyota underscores the goal of achieving zero traffic accidents and emphasizes the collaboration's potential to elevate safety solutions worldwide. Meanwhile, Tekedra Mawakana of Waymo highlights the partnership's role in expanding accessible transportation and integrating their tech into Toyota's lineup.

This promising collaboration not only envisions a safer driving future but also aspires to make autonomous technologies widespread, offering peace of mind to drivers globally. Keep an eye on this space as the partnership unfolds!

The Hacker News discussion on Toyota and Waymo’s autonomous driving partnership quickly pivoted to debates about Tesla’s **Full Self-Driving (FSD)** system and broader industry challenges:  

1. **FSD Criticism vs. Progress**:  
   - Users criticized Tesla’s FSD for overpromising, with some likening it to a “Kickstarter project” still unrealized after 12 years. Skeptics argued Tesla’s marketing (e.g., “Full Self-Driving” branding) implies autonomy beyond its **Level 2** capabilities (driver-assist requiring constant supervision), fueling perceptions of misleading claims.  
   - Others acknowledged incremental FSD improvements but highlighted regressions and edge-case failures.  

2. **Competitor Comparisons**:  
   - **Waymo** (L4, fully driverless in geofenced areas) was contrasted with Tesla’s approach. Users noted Waymo’s liability for accidents, while Tesla shifts responsibility to drivers.  
   - **Mercedes’ Drive Pilot** (L3, limited autonomy with manufacturer liability) sparked debate about its practicality versus Tesla’s scalability. Critics dismissed it as a “gimmick” due to strict operational constraints (e.g., 40 mph max speed, mapped highways).  

3. **Legal and Safety Concerns**:  
   - Discussions emphasized how branding impacts liability. Tesla’s “FSD” label was seen as risky, contrasting with Mercedes’ cautious L3 marketing and legal acceptance of responsibility. References to Ford’s Pinto lawsuit highlighted potential reputational and financial risks if autonomy claims prove deceptive.  
   - Users questioned whether Tesla’s strategy prioritizes scalability over safety, noting Waymo’s slower, safer geofenced deployments.  

4. **User Experiences**:  
   - Some Tesla drivers praised FSD for highway assist but stressed it’s far from “full” autonomy. Others mocked the requirement to “pay 100% attention” to a system marketed as self-driving.  

**Takeaway**: The conversation reflects skepticism toward Tesla’s FSD timeline and marketing, admiration for Waymo’s cautious but functional autonomy, and broader concerns about ethical branding and legal accountability in the AV industry.

### Meta AI App built with Llama 4

#### [Submission URL](https://about.fb.com/news/2025/04/introducing-meta-ai-app-new-way-access-ai-assistant/) | 96 points | by [friggeri](https://news.ycombinator.com/user?id=friggeri) | [106 comments](https://news.ycombinator.com/item?id=43833783)

Meta has unveiled the first version of its highly anticipated Meta AI app, your new digital sidekick, designed to learn about your preferences and provide personalized assistance. Built on the Llama 4 model, this app aims to revolutionize the way you interact with technology by ensuring that your AI experience is personal, social, and seamless. Whether you're chatting on WhatsApp, Instagram, Facebook, or Messenger, Meta AI is primed to respond in a conversational tone, making interactions feel natural and intuitive.

Meta AI doesn't just chat — it listens, remembers, and evolves. With voice conversation capabilities now enhanced, you can multitask efficiently, using voice commands to manage your queries and daily tasks. While it doesn’t access the web in real time yet, Meta AI can still help navigate questions, offer recommendations, and enhance day-to-day interactions based on the information you’ve shared across Meta platforms. For those wanting to see the bigger AI picture, the app comes with a Discover feed to glimpse and share how people are creatively engaging with AI.

The introduction of voice features with full-duplex speech technology in select regions, including the U.S. and Canada, means users can now test a more conversational interface, where Meta AI generates speech directly, aiming to make it sound as though you're chatting with a friend rather than programming a bot. Want an even tighter integration? Connect your Facebook and Instagram for an enriched experience that’s as personalized as it gets.

Meta AI is the companion app for the AI-enhanced Ray-Ban glasses, intertwining advanced AI-powered interactions with cutting-edge hardware, underscoring Meta’s vision of the future of personalized tech. This innovation seamlessly allows you to shift from a conversation on the glasses to the app, ensuring continuity and a rich user experience. 

In essence, Meta is redefining AI's role in your life, making it more than just a tool but a personal assistant that knows you like a friend. As this first iteration rolls out and feedback is gathered, expect Meta AI to become an indispensable part of your digital routine.

The Hacker News discussion about Meta's new AI app highlights several key themes:

### **Privacy Concerns**
- Users warn about the app's broad permissions on iOS, including access to browsing history, purchase history, phone numbers, physical addresses, location, photos, and videos. Skepticism persists around Meta’s data-handling track record, with references to Facebook’s past privacy issues. Even without real-time web access, fears remain that data from Meta’s other platforms (WhatsApp, Instagram) could be exploited.

### **Technical Debates on iOS WebViews**
- Discussions delve into how Meta’s app might use WebViews (in-app browsers) instead of Safari, potentially bypassing Safari’s privacy features like content blockers. Some argue Apple’s WebView system is a privacy weak spot, while others note limitations in user control over third-party link handling. AdGuard’s effectiveness in blocking WebView tracking is questioned.

### **Rebranding and Integration Strategy**
- The app’s rebranding from "Meta View" (linked to Ray-Ban glasses) is seen as a move to boost App Store visibility and credibility. Critics call it "spammy," comparing it to past Meta products that failed to innovate. Integration with WhatsApp, Instagram, and Messenger is viewed as an attempt to lock users into Meta’s ecosystem, though some see potential in leveraging Meta’s vast user base for mainstream adoption.

### **Skepticism vs. Potential Adoption**
- Tech-savvy users dismiss the app as unoriginal, citing overlap with existing AI tools like ChatGPT. However, others argue non-technical users may embrace it due to seamless integration with familiar Meta platforms. Meta’s ability to leverage personalized data for AI customization is noted, but concerns about monopolistic control over hardware (Ray-Ban glasses) and AI models (LLaMA 4) arise.

### **User Experience Critiques**
- Voice features and glasses integration are praised as innovative, but the use of WebViews over native browser tabs is criticized for clunky UX. Some users question if Meta AI’s conversational tone and "friend-like" interactions will resonate long-term or feel intrusive.

### **Open-Source and Power Dynamics**
- LLaMA 4’s open-source aspects are debated, with concerns about Meta’s influence over AI development and potential misuse. The tension between corporate control and community-driven AI models is highlighted.

In summary, the discussion reflects a mix of skepticism about Meta’s privacy practices and strategic rebranding, technical debates over iOS security, and cautious acknowledgment of the app’s potential to reach mainstream audiences through Meta’s ecosystem.

---

## AI Submissions for Sun Apr 27 2025 {{ 'date': '2025-04-27T17:11:38.838Z' }}

### AI helps unravel a cause of Alzheimer’s and identify a therapeutic candidate

#### [Submission URL](https://today.ucsd.edu/story/ai-helps-unravel-a-cause-of-alzheimers-disease-and-identify-a-therapeutic-candidate) | 287 points | by [pedalpete](https://news.ycombinator.com/user?id=pedalpete) | [136 comments](https://news.ycombinator.com/item?id=43815591)

Hold the front page! Researchers at the University of California San Diego have leveraged AI to unravel a previously unknown cause of Alzheimer’s disease, potentially paving the way for groundbreaking treatments. Alzheimer’s, affecting one in nine seniors, has long baffled scientists, especially the spontaneous form which arises without known genetic mutations.

This intriguing study, led by Professor Sheng Zhong and published in the journal *Cell*, zeroed in on the PHGDH gene. Previously pegged as a biomarker for early disease detection, PHGDH’s newly discovered misbehavior reveals it plays a direct role in Alzheimer’s progression. How? By disrupting the cellular process of turning genes on and off through an unexpected pathway. This sneaky activity, unveiled with AI's help, is linked to increased brain degeneration in Alzheimer’s.

Using both mice and human brain organoids, the researchers demonstrated that dialing down PHGDH can curtail disease progression. This revelation pivots our understanding from treating symptoms to intercepting the disease at an upstream critical point, long before the onset of the infamous amyloid plaques. This discovery not only revolutionizes how we understand Alzheimer’s but also hints at new therapeutic possibilities that target this rogue gene’s moonlighting role. 

Stay tuned, as this research, thanks to a blend of bioengineering and AI, could redefine Alzheimer's disease treatment, transforming care and hope for millions worldwide.

The discussion surrounding the AI-driven Alzheimer's research highlights several key themes:

1. **Skepticism of AI's Role**: Many users argue the article's title overhypes AI's contribution, likening it to tools like telescopes or microscopes—useful but not the sole driver of discovery. Critics note the core work relies on established biochemistry and computational techniques, with AI serving as a supplementary tool.

2. **Debate Over AlphaFold's Impact**: While some praise AlphaFold for revolutionizing structural biology (e.g., predicting protein structures), others downplay its novelty, pointing out that similar methods existed for years. Critics emphasize that traditional experimental validation remains critical, and AI's role is often overstated.

3. **Concerns About AI Hype**: Users express frustration with the trend of labeling research as "AI-driven" for attention, arguing this overshadows human contributions and creates unrealistic expectations. Comparisons are made to past hyped technologies (e.g., blockchain) that failed to deliver universal breakthroughs.

4. **Recognition of Incremental Progress**: Several commenters acknowledge that while AI accelerates data analysis and pattern recognition, true scientific breakthroughs still depend on human insight and rigorous experimentation. The study’s use of AI to identify PHGDH’s role is seen as a step forward but part of a broader, collaborative effort.

5. **Technical Discussions**: Experts dive into specifics, debating how AlphaFold detects structural relationships in proteins and its limitations in replacing wet-lab experiments. Some highlight the complexity of biological systems, where sequence divergence can obscure functional similarities.

Overall, the thread reflects cautious optimism about AI’s potential to aid research but warns against sensationalism, stressing that foundational science and human expertise remain irreplaceable.

### Watching o3 model sweat over a Paul Morphy mate-in-2

#### [Submission URL](https://alexop.dev/posts/how-03-model-tries-chess-puzzle/) | 98 points | by [alexop](https://news.ycombinator.com/user?id=alexop) | [61 comments](https://news.ycombinator.com/item?id=43813046)

In a recent challenge pitting human-like reasoning against an AI model, OpenAI's o3 faced off against a classic Paul Morphy chess puzzle—a mate-in-two problem that's been a brain teaser for many. The encounter, detailed in a blog post, was both amusing and enlightening, highlighting o3’s process as it navigated the tricky waters of chess strategy much like a human might.

To solve the puzzle, o3 first engaged in a meticulous analysis of the chessboard, reconstructing positions with precision by measuring pixels. However, it quickly found itself mired in doubt as the obvious moves failed to checkmate. This led it to explore alternative methods, such as attempting to use Python (which failed due to missing modules) and analyzing the board pixel by pixel—a testament to its stubborn determination.

As the moments ticked away, o3 exhibited a very relatable kind of desperation. It showed signs of uncertainty and mild panic, the kind that might push a human player to seek help. Indeed, after eight minutes of struggle, o3 resorted to the internet for answers, but with a twist: it was not a blind copy-paste affair; o3 validated and understood why the suggested move, Ra6, was the correct solution. This journey showcased more than problem-solving—it was a demonstration of human-like reasoning, adaptation, and, yes, a dash of cheeky 'cheating.'

This exploration underscores the AI model's ability to mimic human problem-solving processes—albeit with some digital-age tools at its disposal. Such exploits reveal where AI excels in logical reasoning and where it still turns to human methods (like using Bing) for help. The post-owner hints at broader implications, questioning the creative capabilities of AI models in complex problem-solving scenarios.

For those intrigued by the prospects of AI-human synergy, staying updated with more insights into TypeScript, Vue, and web development by subscribing to the blog’s newsletter might just be the next step to keep your coding game strong—no bots needed!

The discussion revolves around OpenAI's o3 model solving a Paul Morphy chess puzzle, highlighting both admiration and skepticism about AI's problem-solving capabilities. Key points include:

1. **AI's Process & Struggles**:  
   - The AI (o3) meticulously analyzed the puzzle, attempted Python scripting, and eventually searched the internet to validate the correct move (Ra6). This "human-like" trial-and-error approach, including moments of doubt and resourcefulness, sparked debates about whether this demonstrates genuine reasoning or enhanced information retrieval.

2. **Comparisons to Human Problem-Solving**:  
   - Human chess players (rated ~1600–2000 Elo) noted solving the puzzle in seconds, emphasizing pattern recognition honed through practice. Some argued the puzzle’s difficulty was overstated, while others highlighted that AIs lack the intuitive restructuring of knowledge humans employ.

3. **Technical Limitations**:  
   - Critics pointed out AI’s tendency to suggest illegal chess moves, with users proposing solutions like pre-filtering legal moves. Discussions delved into chess mechanics, noting positions with 20–40 legal moves on average and the challenge of encoding this complexity.

4. **Skepticism About Reasoning**:  
   - While GPT-4’s 85% puzzle-solving rate impressed, many questioned if it reflects true reasoning or improved data access. Parallels were drawn to early chess engines, suggesting AIs mimic steps without deeper understanding. The "Clever Hans" analogy highlighted concerns about superficial success versus genuine insight.

5. **Broader Implications**:  
   - The thread debated AI’s role in creative problem-solving, with some seeing potential for synergy with humans and others dismissing it as a parlor trick. Questions lingered about whether current models innovate or merely replicate existing solutions.

In essence, the discussion underscores both fascination with AI’s capabilities and skepticism about their depth, particularly in replicating human-like adaptability and intuition.

### TmuxAI: AI-Powered, Non-Intrusive Terminal Assistant

#### [Submission URL](https://tmuxai.dev/) | 180 points | by [iaresee](https://news.ycombinator.com/user?id=iaresee) | [61 comments](https://news.ycombinator.com/item?id=43812646)

TmuxAI has rolled out a new, non-intrusive terminal assistant specifically designed to work seamlessly in a Tmux environment. Emulating a helpful colleague, TmuxAI observes your terminal activity in real-time to offer context-aware assistance precisely when you need it. With a simple installation command, users can unleash this tool's potential without any complicated setup or special configurations required.

This open-source tool boasts compatibility across various terminal interfaces, from nested shells to network device interfaces like Cisco IOS and Juniper systems. Key features include immediate assistance by reading your terminal's environment and an innovative "Watch Mode" that proactively suggests improvements and explanations. It even supports "Prepare Mode," enhancing command tracking for more precise aid based on your activity.

To demonstrate its capabilities, consider a scenario where TmuxAI makes quick work of common tasks like finding and deleting large files or managing MySQL containers. It proposes practical solutions while learning your workflow, ensuring you maintain maximum control with options to execute, edit, or decline any operation.

For those looking to revolutionize their terminal experience, TmuxAI stands out as an intuitive, powerful assistant ready to tailor itself to suit your unique workflow needs, all while remaining free and customizable. Check out its GitHub page and see how TmuxAI can streamline your terminal tasks today!

The Hacker News discussion around TmuxAI reveals a mix of enthusiasm, practical concerns, and comparisons to existing tools. Here's a concise summary:

### Key Positives:
1. **Unix Philosophy & Modularity**: Users appreciate TmuxAI's alignment with Unix principles, emphasizing modularity and composability. Its integration with tools like FZF, markdown rendering, and context-aware LLM assistance within Tmux sessions is praised.
2. **Workflow Efficiency**: Supporters highlight its ability to streamline terminal tasks, such as intercepting sessions, suggesting commands, and handling context (e.g., PostgreSQL, SSH). Features like "Watch Mode" and hotkey-driven interactions are seen as productivity boosters.
3. **Innovative Use Cases**: Some users compare it to tools like Shellsage, calling it a "game-changer" for developers. The ability to integrate middleware generically (e.g., Scheme interpreter in Zsh) sparks interest.

### Criticisms & Concerns:
1. **Security Risks**: Concerns arise about running AI models locally vs. cloud services, with warnings about accidental execution of dangerous commands (e.g., deleting critical resources). An "undo" feature is suggested to mitigate risks.
2. **Usability Issues**: Some find setup challenging, particularly API key configuration for OpenAI/OpenRouter. Others criticize clunky workflows, preferring explicit CLI calls or existing tools like Zsh plugins.
3. **Distraction vs. Utility**: While non-intrusive AI assistance is praised, skeptics argue that AI in terminals can be distracting or overkill. Tools like Warp and VS Code’s agent mode are cited as alternatives.

### Comparisons & Alternatives:
- **Cursor/VS Code Integration**: Some users desire tighter integration with editors like Cursor or VS Code for seamless terminal interactions.
- **Shellsage & Warp**: Competing tools are mentioned, with mixed opinions on their effectiveness compared to TmuxAI.

### Final Takeaway:
TmuxAI is seen as a promising step toward context-aware terminal assistance, but its adoption hinges on addressing security, usability, and workflow integration challenges. The discussion underscores a broader trend of blending AI with developer tools while balancing control and convenience.

### Unauthorized Experiment on CMV Involving AI-Generated Comments

#### [Submission URL](https://simonwillison.net/2025/Apr/26/unauthorized-experiment-on-cmv/) | 65 points | by [pavel_lishin](https://news.ycombinator.com/user?id=pavel_lishin) | [29 comments](https://news.ycombinator.com/item?id=43811908)

In a controversial turn of events, a research team from the University of Zurich conducted a covert experiment on the subreddit r/changemyview, deploying AI-generated comments over several months. The aim was to see if these responses could influence users' opinions, but the moderators and many participants feel this violated ethical standards. The AI bots, operating under fabricated personas, shared fictitious life stories without the community's consent, which sparked outrage regarding the manipulation of honest discourse. Though researchers claimed to avoid harmful or deceitful content, their justification for breaking community rules points to the significant societal value of their study. This incident echoes concerns about psychological manipulation by large language models and underscores the tension between experimental ambition and ethical integrity. The University stands firm on the importance of their findings despite moderator backlash, which centers on the ethical quandaries of using unconsenting subjects in such studies. This unmasking adds fuel to the ongoing debate about AI's role and responsibility in online spaces.

The Hacker News discussion about the University of Zurich’s covert AI experiment on Reddit’s r/changemyview subreddit highlights several key themes:

1. **Ethical Concerns**: Many users condemned the study for deploying AI bots with fabricated personas and backstories without community consent, comparing it to psychological manipulation. Critics argued this violated ethical standards for human-subject research, emphasizing the importance of transparency and consent, even in online spaces.

2. **Technical Detection**: Some comments focused on methods to combat AI-generated content, such as statistical "fingerprinting" of LLM outputs. Researchers noted that AI comments often leave detectable patterns (e.g., 30% flagged as "human-like" vs. 95-100% for real users), though detection remains challenging as bots evolve.

3. **Debate Over Research Value**: While defenders argued the study exposed real risks of AI-driven disinformation and societal manipulation, critics dismissed the ethical breaches as inexcusable, regardless of potential insights. Comparisons were drawn to historical ethical failures in research, like the Tuskegee experiments.

4. **Broader Implications**: Users highlighted the inevitability of AI influencing online discourse, with parallels to political bots and paid troll farms. Concerns about eroding trust in online communities were widespread, with calls for platforms to prioritize transparency and better moderation tools.

5. **Moderation Challenges**: Moderators noted the difficulty of identifying AI-generated spam, while others criticized Reddit’s reliance on unpaid volunteers to police increasingly sophisticated bots. Skepticism about the platform’s ability to curb synthetic content was a recurring theme.

Overall, the discussion reflects tension between advancing AI research and preserving ethical integrity, underscoring the need for clearer guidelines as LLMs reshape online interaction.

### Do Large Language Models know who did what to whom?

#### [Submission URL](https://arxiv.org/abs/2504.16884) | 34 points | by [badmonster](https://news.ycombinator.com/user?id=badmonster) | [5 comments](https://news.ycombinator.com/item?id=43813879)

In a fascinating new study titled "Do Large Language Models Know Who Did What to Whom?" researchers Joseph M. Denning and his colleagues delve into the intricacies of language understanding by large language models (LLMs). The paper addresses the ongoing debate about whether LLMs truly "understand" language or merely process it. The authors focus specifically on a fundamental linguistic concept: thematic roles, which involve determining the agent and patient in a sentence—essentially, "who did what to whom."

The team conducted experiments on four different LLMs to analyze how well they capture these thematic roles. Surprisingly, they discovered that while humans tend to link sentence similarity to thematic roles, these models predominantly associate similarity with syntax alone, often overlooking thematic structures. Interestingly, some attention mechanisms within the models did manage to isolate thematic roles, suggesting that LLMs possess a latent ability to understand such language constructs, albeit less consistently than humans.

The paper provides significant insights into how LLMs construct "meaning" and highlights the nuanced differences in language understanding between artificial and human cognition. If you're intrigued by the intersection of computational language models and cognitive science, this study offers a rich exploration of the current capabilities and limitations of LLMs regarding sentence comprehension. For further details, you can access the full paper via arXiv [doi:10.48550/arXiv.2504.16884](https://doi.org/10.48550/arXiv.2504.16884).

**Discussion Summary:**

1. **User 112233** questions whether LLMs truly understand narrative roles ("who did what to whom") or merely mimic patterns. They suggest models might superficially swap characters (e.g., agents and patients) while relying on syntactic structures rather than deeper thematic understanding. A reply by **NoToP** provides an example involving air traffic control transcripts, where a character’s role (e.g., "Y talking to traffic control") highlights potential inconsistencies in how models handle agent-patient assignments.

2. **bdmnstr** shares a direct link to the arXiv paper for reference.

3. **kzntr** argues that LLMs excel at preserving meaning in tasks like translation, implying syntactic competence. **chwxy** elaborates by summarizing the study’s key findings:
   - **Experiment 1**: LLMs prioritize syntax over thematic roles when assessing sentence similarity, unlike humans, who focus on meaning.
   - **Experiment 2**: Thematic role information is faintly detectable in some hidden layers and attention heads, enabling limited classification of sentences by roles. However, this capability is inconsistent and generally inferior to human performance, except in specific descriptive cases where attention heads surprisingly outperformed humans.

**Key Takeaway**: The discussion underscores skepticism about LLMs’ grasp of thematic roles, emphasizing their syntactic bias. While some attention mechanisms show latent potential for role-based understanding, humans remain more robust at integrating meaning and context.

### How NASA Is Using Graph Technology and LLMs to Build a People Knowledge Graph

#### [Submission URL](https://memgraph.com/blog/nasa-memgraph-people-knowledge-graph) | 102 points | by [lexmo67](https://news.ycombinator.com/user?id=lexmo67) | [45 comments](https://news.ycombinator.com/item?id=43813036)

Dive into the cutting-edge world of NASA’s People Knowledge Graph! On April 24, 2025, during a riveting community call, NASA showcased how they’re reshaping people analytics using graph databases and large language models (LLMs). If you missed it, don't worry – the full event is now available on demand, featuring a live demo, architecture deep dive, and expert Q&A.

Led by NASA's People Analytics team, including David Meza, Madison Ostermann, and Katherine Knott, the initiative leverages Memgraph and AWS infrastructure to connect the dots between people, skills, and projects across NASA. This innovative system enables seamless subject matter expert discovery and reveals real-time organizational insights through an interactive GraphRAG-powered chatbot.

Why the shift? Traditional databases struggle to capture the complex, interconnected nature of an organization like NASA. Enter graph databases. They revolutionize this space by enabling explorations beyond rows and columns, making it easier to pinpoint skills gaps or to identify expert matches for advanced AI projects across NASA centers.

The entire system operates securely on NASA's AWS cloud, utilizing Docker, on-prem LLM servers, and S3 buckets for comprehensive data handling. Using GQLAlchemy, data is seamlessly ingested from various sources into Memgraph, allowing intricate connections to form between employees, projects, and skills.

The live demonstration during the community call illustrated the system’s prowess. Sample Cypher queries unveiled how NASA's leaders can assess workforce capabilities, find specific project overlaps, and visualize organizational dynamics. The RAG-based chatbot adds another layer by supporting natural language queries, bolstered by a sophisticated pipeline that extracts context-aware responses.

Looking ahead, NASA plans to scale the graph significantly while refining embedding models and improving data mapping. This venture not only showcases NASA's commitment to innovation but also sets a precedent for how data can drive organizational excellence. Catch up on this fascinating journey with the full community call replay now!

The Hacker News discussion on NASA's People Knowledge Graph using Memgraph and LLMs highlights several key themes:

### **1. Cost and Pricing Concerns**
- Users questioned **Memgraph's expense**, with comparisons to alternatives like PostgreSQL, Oracle, and Stardog. Memgraph’s CTO clarified its pricing model ($25k/year for Enterprise) and highlighted its transparency, scalability, and features like replication. Critics argued that open-source options (e.g., Apache Age, Neo4j) or cloud-native solutions (AWS Neptune) might be more cost-effective for large-scale deployments.

### **2. Database Comparisons**
- **Stardog** was noted as a past NASA choice, with links to prior case studies. Users debated the merits of RDF/SPARQL-based systems vs. labeled property graphs, with some advocating for standards like RDF for semantic interoperability.
- **Apache Age** (a PostgreSQL extension) and **Neo4j** were suggested as alternatives, though concerns about schema design and query performance were raised.

### **3. Technical Feasibility**
- The graph’s size (**27K nodes, 230K edges**) sparked debate: some called it "tiny" for a graph database, while others defended its utility for NASA’s specific needs. Discussions also covered scalability, query languages (Cypher vs. SPARQL), and the role of vector embeddings for similarity searches.

### **4. Skepticism About AI/LLM Integration**
- Critics doubted the effectiveness of **LLMs for skill extraction**, arguing resumes often misrepresent actual expertise. Others countered that LLMs could complement (not replace) human management, especially in large organizations. Concerns included data accuracy and the risk of over-reliance on automated systems.

### **5. HR Management Critiques**
- Some users dismissed the project as **"HR management nonsense"**, arguing that skill-matching tools often fail to capture real-world dynamics. Others highlighted the challenge of retaining talent and the disconnect between HR systems and actual employee capabilities.

### **6. Memgraph’s Participation**
- Memgraph’s CTO engaged directly, addressing pricing and technical questions, and emphasizing transparency. This sparked further debate about vendor lock-in and the trade-offs between open-source vs. enterprise solutions.

### **Key Takeaways**
- NASA’s project exemplifies the potential of graph databases for organizational analytics but faces scrutiny over cost, technical implementation, and the practicality of AI-driven HR tools. The discussion underscores broader tensions in tech: open-source vs. proprietary solutions, scalability debates, and the limits of automation in human-centric domains.

### AI Coding assistants provide little value because a programmer's job is to think

#### [Submission URL](https://www.doliver.org/articles/programming-is-a-thinkers-game) | 98 points | by [d0liver](https://news.ycombinator.com/user?id=d0liver) | [183 comments](https://news.ycombinator.com/item?id=43815033)

In a recent critique, a developer highlights the gap between written code and the actual running programs, using a JavaScript example to drive home the point. This particular code snippet raises several issues about execution context, dependency on external functions, and the implicit nature of many JavaScript actions. The main argument is that while code may appear functional, many underlying complexities exist that aren't visible in the code itself. These include understanding browser behaviors, version compatibility, and the setup of event-driven programming, all of which require significant human reasoning.

The article further critiques the notion of AI-driven code generation, suggesting that these tools often churn out syntactically correct but logically flawed snippets. Since AI lacks deep understanding and reasoning capabilities, it tends to create code that might look right but falls short in practice, especially for non-trivial tasks. The complexity of integrating AI-generated code into real-world applications, needing human verification and corrections, often outweighs any time savings from automated generation.

Moreover, the article argues that successful software engineering is less about the act of writing code and more about understanding, discussing, and filling in the gaps that machine-generated code cannot acknowledge. Established modules, open source projects, and community-generated example codes remain invaluable assets because they provide context, documentation, and thoughtful abstractions necessary for high-quality software development.

In conclusion, the piece echoes the sentiment of Linux creator Linus Torvalds: real programming challenges require a deep understanding of the system, beyond the mere ability to write code. Debuggers, modules, and community resources help programmers focus on "the meaning of things," rather than the minutiae, ultimately elevating their craft above simple code generation.

The Hacker News discussion revolves around the limitations and challenges of AI-generated code, centering on its inability to fully grasp context, dependencies, and domain-specific complexities. Key themes include:  

1. **AI as a Time-Saver, Not a Silver Bullet**:  
   - Many users report using LLMs (e.g., Claude, GPT) for boilerplate code, bug fixes, or repetitive tasks, saving hours of work. However, results often require significant human intervention for edge cases, debugging, and refining.

2. **Contextual Understanding Falls Short**:  
   - AI struggles with deep technical reasoning, such as understanding low-level system interactions (e.g., JSON parsing, configuration files) or niche frameworks/tools (e.g., Zig, Zephyr RTOS). Users highlight failures when AI lacks access to indexed documentation or misinterprats implicit project requirements.

3. **Testing and Code Quality**:  
   - While AI can generate code quickly, users emphasize the importance of rigorous testing. Tests written by humans (or explicitly guided by humans) are critical for ensuring robustness, as AI-generated tests may miss edge cases or produce misleading coverage metrics.

4. **Domain Complexity and Specialization**:  
   - In complex domains like finance, embedded systems, or legacy codebases, AI often falters. One developer notes how AI failed to handle C++ data structures or Qt UI intricacies, requiring manual problem-solving despite prompts.

5. **Human-AI Collaboration**:  
   - Developers stress that AI tools are most effective when paired with human expertise. For example, AI might propose a fix, but developers must verify it and fill in gaps through deeper technical analysis. Over-reliance leads to "code monkeys pasting LLM output" without true understanding.

6. **Mixed Success in Workflows**:  
   - Some praise LLMs for accelerating tasks like CSV imports or React/Vue conversions, while others find them inadequate for bespoke projects lacking clear documentation. The consensus: AI aids productivity but cannot replace critical thinking or domain knowledge.

**Takeaway**: The discussion underscores that AI tools are valuable for accelerating repetitive or well-trodden tasks but falter in nuanced, highly specialized, or poorly documented scenarios. Human oversight, domain expertise, and rigorous testing remain indispensable for high-quality software development.