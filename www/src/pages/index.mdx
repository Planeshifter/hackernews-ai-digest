import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Tue Sep 12 2023 {{ 'date': '2023-09-12T17:09:59.533Z' }}

### Gmail and Instagram are training AI, and thereâ€™s little you can do about it

#### [Submission URL](https://www.washingtonpost.com/technology/2023/09/08/gmail-instagram-facebook-trains-ai/) | 74 points | by [bookofjoe](https://news.ycombinator.com/user?id=bookofjoe) | [75 comments](https://news.ycombinator.com/item?id=37487926)

In a recent analysis by Geoffrey A. Fowler for The Washington Post, it was revealed that tech companies such as Google, Meta (formerly Facebook), and Microsoft are using users' data from platforms like Gmail and Instagram to train their artificial intelligence (AI) systems. For example, Google uses users' Gmail responses to train its AI to finish other people's sentences, and Meta took a billion Instagram posts without permission to train its AI. Microsoft uses users' chats with Bing to improve its AI chatbot, and there is no way for users to opt out of this. This trend of using personal data to train AI raises concerns about privacy and the potential misuse of users' information. While these companies do use data for targeted ads, this new development involves using data to create new technologies that could further expand these tech giants' power and influence. Users have little control over how their data is being used for AI training, and the implications for privacy and personal information are not fully understood at this point.

The discussions on this submission touch on several points. One user points out that objecting to the practices of big tech companies is important, and suggests moving away from platforms that engage in data collection. Another user raises the issue of privacy and the need for clearer definitions and regulations. There is also a discussion about the technical aspects of data encryption and the potential risks of intercepted emails. Some users emphasize the importance of using encryption methods to protect private communications. Another user mentions that companies like Google and Meta have terms of service agreements that allow them to use user data, but it is not clear whether users fully understand the implications of these agreements. There is also a discussion about alternative email services and the advantages and disadvantages of using platforms like Gmail. One user mentions the convenience of Gmail's features, while another user expresses concerns about privacy and the potential misuse of personal data. Lastly, a user suggests stopping the use of Gmail altogether. Overall, the discussions revolve around privacy concerns, alternatives to mainstream platforms, and the need for clearer regulations and user control over data usage.

### Simulating History with ChatGPT

#### [Submission URL](https://resobscura.substack.com/p/simulating-history-with-chatgpt) | 163 points | by [arbesman](https://news.ycombinator.com/user?id=arbesman) | [77 comments](https://news.ycombinator.com/item?id=37480155)

In this article, Benjamin Breen shares his experience of using large language models (LLMs) like ChatGPT as a teaching tool in his history classes. He believes that LLMs can be used to simulate interactive historical settings, allowing students to engage with different historical scenarios. While he acknowledges that these simulations are not always accurate and may contain falsehoods and hallucinations, he sees the potential of using LLMs as a way to enhance the teaching of history. He argues that LLMs can help elevate the importance of the humanities in higher education, as they rely on textual skills and methods that are emphasized in humanities classes. However, he also recognizes that there will be challenges in incorporating LLMs into assignments, as professors will need to rethink their teaching methods. Overall, Breen believes that LLMs have the potential to positively impact higher education and the study of history.

The discussion revolves around the potential use of large language models (LLMs) like ChatGPT as teaching tools in history classes. Some users express skepticism about the accuracy and reliability of LLMs, noting that they may generate falsehoods and hallucinations. Others discuss the possibilities of using LLMs to create historical simulations and interactive scenarios. Some users suggest incorporating regional instructions and prompts to enhance the educational experience. There are also discussions about using LLMs for language learning, critiquing the lack of personal interaction, and drawing parallels to previous software like Timothy Leary's Mind Mirror. One user suggests the idea of using LLMs to create historical simulations on a dedicated platform, while another user expresses interest in developing a web application for historical simulations using APIs and AI.

### Fandom can't decide if leaked songs are real or AI-generated

#### [Submission URL](https://www.404media.co/harry-styles-one-direction-ai-leaked-songs/) | 73 points | by [wpietri](https://news.ycombinator.com/user?id=wpietri) | [109 comments](https://news.ycombinator.com/item?id=37482455)

A controversy is brewing in the Harry Styles fandom over supposed leaked songs that may or may not be AI-generated. Discord communities within the fandom have been selling snippets of unreleased songs, prompting speculation about their authenticity. The situation has become a community-wide obsession, with fans conducting investigations to determine the legitimacy of the tracks. Some users are claiming that the leaked songs are AI-generated, while others argue that certain tracks sound authentic compared to low-quality AI covers. The underground leaked song industry has become increasingly complex, with AI-generated music now reaching a level of sophistication that can deceive the human ear. While some fans are readily paying for these songs, others remain skeptical and believe that they could be total fabrications. Two users, Liz and Haley, have been warning others about the possibility of fake songs by documenting their suspicions in Twitter threads, which have sparked anger among those selling the tracks.

The discussion surrounding the submission revolves around different aspects of AI-generated music and its impact on various industries. Some users argue that AI-generated music is already disrupting the entertainment industry and infringing on copyrights, similar to what happened with Napster and Kazaa. They believe that AI will continue to replace human labor and that companies should invest in AI to stay competitive. However, others argue that the impact of AI on the music industry is overstated and that human creativity is still valuable. They also discuss the legal and ethical implications of AI-generated music, including copyright issues and the fear of artists losing control over their own work. Some users also mention the potential for AI to generate vocals and how it could revolutionize the creative sphere.

### How to run a competitive AI startup fundraise in 2023

#### [Submission URL](https://context.ai/post/how-to-run-a-competitive-ai-startup-fundraise-in-2023) | 38 points | by [henrysg](https://news.ycombinator.com/user?id=henrysg) | [13 comments](https://news.ycombinator.com/item?id=37481424)

In a recent blog post, the team at Context.ai shared their experiences and insights on how to approach an early-stage fundraising process. They likened the process to dating, emphasizing the importance of being friendly, informal, and not desperate. They also provided practical advice on preparing for the raise, crafting the pitch, and running the fundraising process. Some key takeaways include working backwards from business goals to determine the amount to raise, building a strong network of warm introductions, and using social proof to your advantage. They also stressed the importance of selecting the right partners and optimizing for quality over valuation. Overall, their advice offers valuable guidance for founders looking to raise funding in the near future.

The discussion surrounding the submission includes various comments from users. There is a tangent about the specific advantages and disadvantages that AI startups may face compared to non-AI startups. Some users find it interesting how prompting ChatGPT can generate article titles and sub-titles. Another user mentions the problem of investors valuing startups differently, and suggests that Whisper and Llama, two startups, have encountered this issue. They provide some insights on customer advantages and custom models. One user remarks that many VC-funded companies don't focus on producing a viable product and that VCs are the ones who really hold power. Another user questions the usability of LLM-specific analytics and asks if people typically switch analytics providers. There is a comment about the difficulties with tracking REST calls and the specific challenges faced by Langchain. Another user joins the discussion to mention a startup, GenesisAI, and their goal of addressing the limitations of current AI systems. A few users express their skepticism about AI companies and their generated messages. One user suggests an alternative article title. Finally, a user flags the discussion.

### Therac-25

#### [Submission URL](https://en.wikipedia.org/wiki/Therac-25) | 31 points | by [wazbug](https://news.ycombinator.com/user?id=wazbug) | [14 comments](https://news.ycombinator.com/item?id=37480795)

The Therac-25, a computer-controlled radiation therapy machine produced by Atomic Energy of Canada Limited (AECL), was involved in several accidents between 1985 and 1987, leading to massive overdoses of radiation and resulting in death or serious injury for some patients. These accidents shed light on the dangers of software control in safety-critical systems and have become a prominent case study in health informatics, software engineering, and computer ethics. The incidents were attributed to concurrent programming errors and the engineers' overconfidence in their initial work, as well as a lack of proper due diligence in resolving reported software bugs.

The discussion on the submission about the Therac-25 accidents covers several topics.  One user points out that it is important to note that people were killed in these accidents, emphasizing the severity of the consequences. Another user adds that in 1986, a programmer left AECL, and it was difficult to believe that records of the incident were lost. They also mention that it seems a settlement was reached without requiring the programmer to disclose qualifications or experience. A user shares a link to an article discussing causality in the Therac-25 incidents. The discussion then veers off to a different topic, with one user mentioning that software flaws can result in human deaths. They provide an example of a mass failure caused by a floating-point arithmetic error that resulted in the deaths of 28 people.

Another user brings up the recent high-profile case of the Boeing 737 MAX crashes caused by software failures, suggesting that software-induced plane crashes occur at an extremely high rate. A response to this mentions that the software failures were not the only cause of the Boeing 737 MAX crashes, but rather a combination of factors including regulatory oversight and modifications made without proper review. Another user suggests that studying software ethics is necessary. One user points out that the components prior to the Therac-25 were not properly tested, and they argue that regulation for medical equipment should ensure proper testing to prevent such incidents. A response to this brings up the issue of user interface changes in medical equipment and the potential dangers of confusing operators. They mention that regulating user interface changes in medical equipment is justified because unexpected changes can lead to confusion and potentially harm patients.

The discussion ends with a user mentioning the Ariane 5 crash as an example of a failure caused by inadequate testing of components prior to the system's launch.

---

## AI Submissions for Mon Sep 11 2023 {{ 'date': '2023-09-11T17:10:14.769Z' }}

### The meeting of the minds that launched AI

#### [Submission URL](https://spectrum.ieee.org/dartmouth-ai-workshop) | 166 points | by [fremden](https://news.ycombinator.com/user?id=fremden) | [54 comments](https://news.ycombinator.com/item?id=37469849)

In a guest article for IEEE Spectrum, Grace Solomonoff explores the story behind a group photo taken at the 1956 Dartmouth AI workshop, a milestone event that marked the beginning of AI as a research discipline. The photo captured seven of the workshop's main participants, but one person remained unidentified for years. After thorough research and the discovery of a letter among Ray Solomonoff's papers, it was revealed that the unknown person was Peter Milner, who attended the workshop alongside other influential figures like John McCarthy, Marvin Minsky, Claude Shannon, and Nathaniel Rochester. The revelation helps complete the historical narrative of that significant event.

The discussion among Hacker News commenters covers a wide range of topics related to the submission. Some commenters provide personal anecdotes about their experiences with AI, while others discuss the limitations and capabilities of current AI systems. One commenter, "trd hardI," shares their experience studying AI in the 1980s and criticizes the problem-solving approach used at Stanford University during that time. They argue that focusing on formalizing problem spaces hindered progress in machine learning and that the field only started to make significant advancements in the past 15 years. "ggm" adds to the discussion by sharing their memories of the 1956 Dartmouth AI workshop and their interactions with influential figures like Marvin Minsky and John McCarthy. They also mention McCarthy's involvement in Usenet discussions and his controversial posts. "jhndh" mentions that their father and friends were computer scientists working in AI back in the 1960s and that the path to popular AI systems today has not been straightforward. Other commenters, like "fnrdpglt," discuss the current state of AI, specifically the capabilities of language models like GPT-3. They argue that while these models display impressive abilities in certain areas, they have limitations in understanding complex semantic spaces and often rely on flawed reasoning. The discussion also touches on topics like deductive and inductive reasoning, the definition of a computer, and the distinction between abductive and inductive reasoning. Overall, the conversation ranges from personal experiences to technical aspects of AI and its limitations.

### Atari doubles down on retro, buys beloved Atari homebrew maker

#### [Submission URL](https://arstechnica.com/gaming/2023/09/zap-atari-acquires-beloved-retro-homebrew-vendor-atariage/) | 42 points | by [thunderbong](https://news.ycombinator.com/user?id=thunderbong) | [4 comments](https://news.ycombinator.com/item?id=37464250)

In a surprising move, Atari has announced its acquisition of AtariAge, an online community for Atari enthusiasts that has been around for over two decades. AtariAge is known for selling high-quality cartridge versions of Atari 2600, 5200, and 7800 console homebrews, as well as games for Atari computers and other retro systems. The founder of AtariAge, Albert Yarusso, will join Atari in a full-time role and continue running AtariAge as usual. However, he will focus on updating the site's games database. Despite being a different entity from the original Atari, the new Atari has been leaning heavily on its retro heritage, publishing new Atari 2600 games on cartridges and releasing a new console called the Atari 2600+. The acquisition of AtariAge makes sense as it aligns with Atari's strategy of leveraging its retro IP. Atari CEO Wade Rosen expressed the importance of AtariAge's community and plans to support its mission for years to come. While some members of the AtariAge community have expressed concerns about corporate interference, Yarusso reassured them that the acquisition will not result in changes to the forums or censorship of critical posts. On the positive side, the acquisition opens opportunities for AtariAge's homebrew game creators, who may now have the chance to have their games published on Atari's platforms. Overall, the acquisition of AtariAge shows that Atari is serious about its retro-related IP and aims to create new hardware and software based on it.

### FBI, Fed Judge: Fighting Botnets Means Allowing FBI Remote Installs on Computers

#### [Submission URL](https://www.techdirt.com/2023/09/11/fbi-federal-judge-agree-fighting-botnets-means-allowing-the-fbi-to-remotely-install-software-on-peoples-computers/) | 44 points | by [rntn](https://news.ycombinator.com/user?id=rntn) | [17 comments](https://news.ycombinator.com/item?id=37472841)

In a recent operation to take down a major component of the cybercrime ecosystem, the FBI remotely installed software on over 700,000 computers around the world without notifying the victims. The operation was authorized by a court warrant and aimed to disrupt a botnet known as Qakbot. While disrupting botnets is generally seen as a public good, concerns arise about the potential invasion of privacy and abuse of power. The warrant allowed the FBI to search and seize information from any computer it sent its software to. It is unclear how the FBI determined which computers were infected, as the warrant seems to authorize an intrusion into all accessible computers. The only limitation mentioned in the warrant is that it does not authorize the seizure of any tangible property or alteration of functionality. This operation raises questions about the extent to which the government can coerce content moderation decisions and the means by which it can do so.

The discussion on Hacker News revolves around the recent operation conducted by the FBI to take down the Qakbot botnet. Users express concerns about the potential invasion of privacy and abuse of power that could result from the FBI remotely installing software on over 700,000 computers without notifying the victims. Some users argue that the FBI should have disclosed the existence of any 0-day vulnerabilities used in the operation, while others highlight the need for effective law enforcement. There is also a debate about the legality and oversight of such operations, with one user questioning if foreign intelligence agencies are acting illegally domestically. Another user raises concerns about the compromised computers becoming part of a shady hacker collective. Some users discuss the FBI's preference for Linux systems over Windows, and there is a general sentiment of fear and distrust towards the power and secrecy of federal agencies.

### Microsoft has not stopped forcing Edge on Windows 11 users

#### [Submission URL](https://www.ctrl.blog/entry/windows-system-components-default-edge.html) | 733 points | by [extr0pian](https://news.ycombinator.com/user?id=extr0pian) | [436 comments](https://news.ycombinator.com/item?id=37461449)

The recent blog post from Microsoft announcing that Windows system components would start respecting the default web browser setting has caused confusion among users. While many in the tech media celebrated this as a win for users, Daniel Aleksandersen, who developed the open-source EdgeDeflector program, has conducted extensive testing and found no changes in the new Windows Insider version. Despite the highlight in the changelog suggesting that Microsoft had caved to pressure from the European Union, Aleksandersen's findings indicate that Windows 11 still strongly discourages changing the default browser away from Microsoft Edge. Web links continue to force-open in Microsoft Edge rather than the user's default browser. Microsoft has not publicly clarified the changes or responded to requests for comment, leading to speculation that the assumed changes may be a gradual rollout. Despite the lack of confirmation, the story has received significant positive press attention.

The discussion on Hacker News revolves around several different topics. 

One user highlights the surprise and confusion regarding the recent changes to Windows system components respecting the default web browser setting. They mention Paul Thurrott's article about the issue and speculate that the changes may not have been fully implemented in the new Windows Insider version.
Another user brings up the past instances of Microsoft's behavior regarding default browser settings, mentioning cases from Windows 7 and Windows 98. They argue that Microsoft's actions have previously been driven by non-competitive behavior.
There is a discussion about the reluctance of governments to regulate non-competitive behavior in the tech industry, with one user specifically mentioning the EU's regulations and another user referring to the military-industrial complex in the US.
Some users express frustration with Microsoft and their integration of certain products, mentioning forced installations and the lack of options to disable or switch to alternatives.
The conversation then shifts to a broader discussion about the influence of big business and the manipulation of public discourse. There are also mentions of the catch-22 situation of criticism against billionaires and the difficulty of confronting those with power and influence.
There is a brief discussion about the EU's willingness to regulate non-competitive behavior and the need for laws that promote competition and benefit consumers.
One user expresses dissatisfaction with Microsoft's practices, stating that they find it inconvenient to be forced to use certain products, while another user argues that the integration of certain software can have practical benefits.
The discussion then touches on the topic of tightly integrated software and its pros and cons. Examples are given of various platforms and their level of integration, with some users arguing that tightly integrated software can be beneficial, while others argue that it can be messy and limiting.
The conversation ends with a user discussing the need for controls on integrations and the potential issues that can arise from undocumented APIs and non-competitive practices.

### Show HN: Mavex.ai â€“ Your Personal AI Executive Assistant

#### [Submission URL](https://mavex.ai) | 15 points | by [yednap868](https://news.ycombinator.com/user?id=yednap868) | [8 comments](https://news.ycombinator.com/item?id=37465321)

Introducing Mavy, your personal AI Executive Assistant! Need help with scheduling, email drafting, and more? Mavy is here to streamline your tasks and optimize your productivity. With Mavy, you can easily draft emails, access all your tools through a unified interface, connect your favorite apps, and summon Mavy from anywhere using keyboard shortcuts. It can also assist with calendar management, content generation, and day-to-day planning. Don't take our word for it â€“ users are raving about how Mavy has transformed their work-life, making operations smoother and more efficient. So why wait? Try Mavy now and experience the power of an AI-powered executive assistant for yourself!

The discussion surrounding the submission primarily revolves around concerns and skepticism regarding the use of AI in personal assistant applications.

One user mentions that they dislike AI apps that require access to personal information, questioning the privacy and security aspects. They express reluctance to sign up for Mavy due to its workflow requiring sign-in and completion of certain steps.
Another user criticizes AI platforms in general, mentioning their dislike for AI apps that gather personal information. They specifically mention their preference for using traditional communication channels like SMS instead of platforms like WhatsApp.
In response to the claims made about Mavy's capabilities, a user expresses skepticism based on their personal experience with AI tools. They state that most AI tools promise more than they can deliver and suggest that Mavy may not be as capable as portrayed in the screenshots and videos on the landing page.
Another user suggests that Mavy is a machine-learning-based writer, comparing it to ChatGPT but with a focus on formal business emails. They mention having tried similar tools but ultimately faced difficulties with awkward wording.
Lastly, there is a discussion about the potential use of GPT-4 in analyzing emails and the need for clearer descriptions or prompts to guide its behavior.

It's worth noting that the discussion contains some unrelated comments about India's WhatsApp statistics and a question about whether AI wrote the post itself.

### The IKEA-powered homelab on a wall

#### [Submission URL](https://ounapuu.ee/posts/2023/09/07/ikea-powered-homelab/) | 95 points | by [PaulHoule](https://news.ycombinator.com/user?id=PaulHoule) | [52 comments](https://news.ycombinator.com/item?id=37472984)

Are you tired of cluttered floors and limited space for your home computer setup? Well, one clever hacker found a solution using IKEA's SKÃ…DIS series pegboard. They mounted their entire homelab on the wall, creating a sleek and space-saving setup. By using zip ties to secure everything in place, they were able to neatly organize all their computing equipment. Although there are some improvements they plan to make, such as hiding cables for a cleaner look, the overall setup is already quite promising. The author also shared their experience with maintenance and the convenience of their setup. If you're interested in seeing more details and joining the discussion about this innovative home setup, head over to Hacker News.

---

## AI Submissions for Sun Sep 10 2023 {{ 'date': '2023-09-10T17:09:50.120Z' }}

### CityDreamer: Compositional Generative Model of Unbounded 3D Cities

#### [Submission URL](https://arxiv.org/abs/2309.00610) | 79 points | by [PaulHoule](https://news.ycombinator.com/user?id=PaulHoule) | [13 comments](https://news.ycombinator.com/item?id=37457426)

The paper titled "CityDreamer: Compositional Generative Model of Unbounded 3D Cities" introduces a novel approach to generating realistic 3D cities. While 3D natural scene generation has been extensively studied, generating 3D cities presents greater challenges due to the complex structural distortions and varied appearances of buildings. To address these challenges, the authors propose CityDreamer, a compositional generative model that separates the generation of buildings from other background objects like roads and green lands. The model leverages two datasets, OSM and GoogleEarth, to enhance the realism of the generated cities. The experiments demonstrate that CityDreamer outperforms state-of-the-art methods in generating lifelike 3D cities. This research opens up possibilities for generating virtual cities with high levels of detail and realism.

The discussion on this submission covers various topics related to generative city modeling. One user points out that the OSM dataset could lead to black spots in the generated cities, while another user suggests removing special identifiable landmarks like the Eiffel Tower from the background. One comment provides a link to the CityDreamer gallery and a YouTube video for further exploration. Another user compares CityDreamer to SceneDreamer, highlighting their different approaches to generating cities. One user appreciates the research but also mentions the importance of well-tended interactions in producing realistic and complex city compositions. Another user discusses the concept of generative city modeling and its applications for urban planning. There is a reference to CityEngine, a procedural city generation tool presented at SIGGRAPH 2001, and its use of L-systems and context-dependent rules to generate detailed cities. The comment also provides links for further reading. Shape grammars are mentioned as a related concept with a similar syntax. There is a mention of the game Townscaper and a link to a tweet thread discussing its development. Another user shares a video link to Night Call, which showcases the generation of Paris using an nth-fly technique. The conversation shifts to discussing the use of digital twins and how city planning departments often rely on outdated and incomplete data. One user finds it fascinating how City Skylines, a game, doesn't have an API to model its city generator. The discussion concludes with a few short comments.

### Training and aligning LLMs with RLHF and RLHF alternatives

#### [Submission URL](https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives) | 95 points | by [rasbt](https://news.ycombinator.com/user?id=rasbt) | [14 comments](https://news.ycombinator.com/item?id=37455859)

Today's article on Sebastian Raschka's blog explores the process of Reinforcement Learning with Human Feedback (RLHF) in the context of training Language Models (LLMs). RLHF is an important component of modern LLM training as it allows for the incorporation of human preferences into the optimization process, leading to improved model performance and safety.

The article provides a step-by-step breakdown of RLHF, starting with a discussion of the canonical LLM training pipeline. This pipeline consists of three main steps: pretraining, supervised finetuning, and alignment. In the pretraining phase, models learn from vast unlabeled text datasets using a next-word prediction task. This allows for the leveraging of large, unlabeled datasets without the need for manual labeling.

The next step is supervised finetuning, where models are trained on instruction-output pairs. This involves predicting the next tokens in the output given specific instructions. Supervised finetuning requires smaller datasets compared to pretraining, as it involves human or high-quality LLM-generated instructions and desired outputs.

Finally, the article delves into the RLHF-based alignment step, which aims to align the LLM with human preferences. RLHF involves fine-tuning the model using reinforcement learning, where human feedback is used to guide and improve the model's responses. The article also provides a comparison between ChatGPT's and Llama 2's implementations of RLHF.

For those interested in alternatives to RLHF, the article includes a section that highlights the most recent alternatives and intends to keep it regularly updated.

Overall, the article offers a comprehensive overview of RLHF and its role in LLM training, providing insights into its importance and practical implementations.

The discussion on the submission revolves around various aspects of RLHF and its implications for language models. Here are some key points raised by the commenters:

- Discussions on the alignment process of LLMs highlight the challenges related to the quality and quantity of training data. The commenter "og_kalu" points out that producing good supervised fine-tuning (SFT) datasets can be quite challenging, while "phllpcrtr" emphasizes the high degree of reliability and reproducibility needed in LLMs, which may not be achieved easily.
- "wstrnr" suggests the addition of additional layers in LLMs to determine the relevance and logical soundness of presented conclusions, supporting the conclusions presented in the article.
- The issue of hallucination in language models is addressed by multiple commenters. "bgglbtl" states that the high degree of reliability and reproducibility in LLMs is crucial, and "Salgat" discusses the limitations of simple linear regression models in dealing with hallucinations and the need for more complex algorithms.
- "ftxbr" argues that large language models lack creativity and are limited in their cognitive capabilities, while "Salgat" counters that hallucination is a part of creativity and that wrong predictions are an inherent flaw of creative language models.
- "ShamelessC" criticizes the article for subtly misrepresenting OpenAI's intention regarding RLHF's role in hallucination reduction, stating that it was partially flawed in its design and purpose. The commenter also mentions Yann LeCun's opinion on training lesser models and retraining researchers.

Overall, the discussion explores the challenges and implications of RLHF in language models, including issues related to training data quality, hallucination, reliability, reproducibility, and creativity.

### What You Say to Google Assistant and Alexa (Not Siri) Gets Used for Ad Targeting

#### [Submission URL](https://www.consumerreports.org/electronics/digital-assistants/voice-assistants-and-ad-targeting-a1098726954/) | 64 points | by [CharlesW](https://news.ycombinator.com/user?id=CharlesW) | [13 comments](https://news.ycombinator.com/item?id=37460403)

Have you ever wondered if your interactions with voice assistants like Google Assistant and Alexa are being used for ad targeting? Well, a new study by Northeastern University in Boston has found evidence that Google and Amazon do indeed use your voice interactions to draw conclusions about you. Based on what you say to these devices, Google can infer details like your marital status and homeowner status, while Amazon takes note of products you may be interested in. These companies use this data to supplement the information they already gather from your phones and laptops, and in turn, marketers use this valuable information to target ads to the people they believe are most likely to buy their products. So the requests you make to Alexa or Google Assistant can actually influence the ads you see online. The study also revealed that Google Assistant's responses to you can be influenced by your past interactions. On the other hand, Apple's Siri does not appear to connect any tags directly to users or use voice interactions to build a marketing profile. This study highlights the need for better transparency in the data privacy practices of voice assistants. While you may be used to being tracked online, it's important to consider how your interactions with voice assistants may also be contributing to targeted advertising.

The discussion on this submission revolves around the implications of the study's findings and the differences between voice assistants. Some users are surprised by the extent to which Google and Amazon use voice interactions for targeted advertising, while others expect this level of personalization. Commenters point out that Google profits from targeted ads based on user searches, while Amazon builds marketing profiles from Alexa purchases. There is also a mention of Apple's Siri, which does not seem to use voice interactions for ad targeting, leading to a discussion on Siri's privacy practices. One commenter shares an anecdote about their family's experience with targeted ads on their iPhones, raising questions about IP-based targeting and privacy. Another commenter mentions that Siri uses location services. One user dismisses the idea of any conspiracy theories surrounding the study findings and emphasizes the predictability and relevance of targeted ads. However, it is noted that people may not fully understand the extent to which their interactions are being used. A final comment raises the question of whether Siri has a private API or if Apple has access to Siri's data.

### A.I. tools fueled a 34% spike in Microsoftâ€™s water consumption

#### [Submission URL](https://fortune.com/2023/09/09/ai-chatgpt-usage-fuels-spike-in-microsoft-water-consumption/) | 26 points | by [alexzeitler](https://news.ycombinator.com/user?id=alexzeitler) | [8 comments](https://news.ycombinator.com/item?id=37460532)

Microsoft, OpenAI, and Google are facing increased scrutiny over the environmental impact of their artificial intelligence (AI) projects. Specifically, these companies' AI tools, such as Microsoft's ChatGPT, require significant amounts of water to cool their powerful supercomputers. In Microsoft's latest environmental report, it revealed that its global water consumption rose by 34% from 2021 to 2022, largely due to its AI research. Similarly, Google reported a 20% growth in water use during the same period, which can also be attributed to its AI work. Researchers estimate that ChatGPT uses around 500 milliliters of water each time it is prompted with a series of questions. The companies' water usage includes both direct and indirect consumption, such as the water required to cool power plants that supply electricity to the data centers. Microsoft and OpenAI have stated that they are investing in research to measure the energy and carbon footprint of AI and are working on making large systems more efficient. They are also committed to meeting sustainability goals, including being carbon negative, water positive, and zero waste by 2030.

The discussion revolves around the environmental impact of the water consumption by Microsoft, OpenAI, and Google for their AI projects. Here are some key points:

1. Veserv comments on the massive amount of water used, stating that 17 billion gallons of water were consumed, which is equivalent to 7 million cubic meters or 5,200 Olympic swimming pools. The cost of this water consumption amounts to $50 per cubic meter or $35 million in water gas charges.

2. PrivateButts questions why the water cannot be discharged after being used for cooling. They suggest that perhaps the worst pumped holding pond could release the water back into the supply.

3. 1letterunixname argues that unless the data centers are partially designed to reuse municipal potable water or adopt innovative methods for cooling, such as using vaporative cooling or thermal gradients, the water consumption is wasteful. They give the example of data centers using chilled water from a cogeneration plant to save non-potable freshwater. They also highlight the importance of preventing Legionnaires' disease by using non-potable water with disinfectants.

4. Phil21 adds that evaporative cooling is a standard method in the industry and has been dependent on local geography for the past 15 years. They are surprised that it is still a widely-used cooling method.

5. Rch shares a link to an archived article discussing the issue.

6. __loam comments that it is important for people to realize the wastefulness of certain technologies and the energy requirements of data centers.

7. Bone_frequency suggests that it is a key issue and emphasizes the importance of tackling it.

8. PeterStuer compares the energy footprints of traveling and data centers.

Overall, the discussion highlights concerns about the water consumption of AI projects and encourages the adoption of more sustainable and efficient practices in data centers to reduce environmental impact.

### Moral Crumple Zones: Cautionary Tales in Human-Robot Interaction (2019)

#### [Submission URL](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2757236) | 23 points | by [viburnum](https://news.ycombinator.com/user?id=viburnum) | [19 comments](https://news.ycombinator.com/item?id=37452801)

In a thought-provoking paper, Madeleine Clare Elish explores the concept of "moral crumple zones" in human-robot interaction. Drawing on examples of high-profile accidents involving complex automated systems, the author argues that the responsibility for an action may be unfairly attributed to a human operator who has limited control over the behavior of an autonomous or automated system. 

Much like a crumple zone in a car absorbs the force of impact in a crash, the author describes the human operator in a complex and automated system as a "component" that bears the brunt of moral and legal responsibilities when the system malfunctions. This concept challenges the design and regulation of human-robot systems, highlighting the potential for consumer and worker harm in new complex technologies.

By examining cases such as accidents involving driverless cars and other autonomous technologies, the paper raises important questions about responsibility, ethics, and the social perceptions of technology. Understanding and addressing these moral crumple zones is crucial as the use of artificial intelligence and automation continues to expand in society.

Overall, Elishâ€™s paper provides a valuable perspective on the complex relationship between humans and technology, emphasizing the need for responsible design and regulation in the development of autonomous systems.

The discussion on this submission revolves around the concept of assigning responsibility in cases of accidents involving autonomous or automated systems. One user points out that legal systems have already adapted to handle situations where crimes are committed by proxy through human agents, but questions whether it makes sense to attribute responsibility to AI systems. Another user raises concerns about the consequences of neglecting human responsibility and shifting liability to non-human entities. They argue that failure to hold companies accountable for the actions of AI systems could lead to negative outcomes. 

The discussion also touches upon the role of truck drivers and whether they can be blamed for accidents caused by AI systems. Some argue that truck drivers are being unfairly blamed for accidents caused by system malfunctions. Others point out that human drivers may react differently in dangerous situations and that AI malfunctions may not be fully preventable.

There is also a discussion about the power dynamics and politics involved in assigning responsibility for accidents caused by AI systems. One user suggests that the legal system allows courts to determine appropriate punishments based on circumstances, while another argues that the legal system is flawed and that personal background and limited experience are factors in decision-making. 

The conversation delves into the broader issues of employer demands and the consequences of violating laws. Some argue that employees are forced to comply with unsafe conditions and that companies should be held responsible. Others mention the power imbalance between employers and employees and the importance of protections for employees. 

There is also a brief discussion on the role of courts and the ability of defendants to control their actions, as well as a comment about the inability to assign blame to non-human entities.

Overall, the discussion highlights the complexities of assigning responsibility and the need to address ethical and legal considerations in human-robot interaction.

### Improving NeRF quality by progressive camera placement

#### [Submission URL](https://arxiv.org/abs/2309.00014) | 33 points | by [PaulHoule](https://news.ycombinator.com/user?id=PaulHoule) | [12 comments](https://news.ycombinator.com/item?id=37460465)

A recent paper titled "Improving NeRF Quality by Progressive Camera Placement for Unrestricted Navigation in Complex Environments" explores how to enhance the quality of novel view synthesis using Neural Radiance Fields (NeRFs) in complex environments. NeRFs excel at object-centric reconstructions, but their performance in environments such as rooms and houses is often underwhelming. The researchers argue that high-quality data is crucial for optimizing a NeRF effectively, and propose an algorithm that suggests new camera placements to improve the visual quality of the reconstruction. Their solution outperforms existing methods and can be used with any NeRF model. This research has the potential to advance the field of computer vision and pattern recognition.

In the comments, user "brdknwls" suggests that capturing 3D video instead of stills and using optimal camera placement can capture more information. User "fudged71" wonders if Gaussian Splattering techniques and AR headsets could also be effective. "Legend2440" mentions that NeRFs work with Gaussian splatting and improving 3D representation. "PaulHoule" finds the research exciting and suggests that the field could benefit from incorporating light field cameras for creating 3D models. "blvscff" mentions a separate research paper on Gaussian splattering. "lucb1e" explains the role of Neural Radiance Fields (NeRFs) in rendering 3D reconstructions and user "PaulHoule" agrees. "CharlesW" brings up a similar technique called Multi-View Stereo (MVS), and "blvscff" comments on the differentiable process of NeRFs.


ffstrf finds this interesting and speculates that it might be possible to leverage hacking services or replicate existing solutions that support machine learning.

The conversation ends without further comments.

---

## AI Submissions for Sat Sep 09 2023 {{ 'date': '2023-09-09T17:09:45.510Z' }}

### SmartKnob â€“ Haptic input knob with software-defined endstops and virtual detents

#### [Submission URL](https://github.com/scottbez1/smartknob) | 264 points | by [e3a8](https://news.ycombinator.com/user?id=e3a8) | [42 comments](https://news.ycombinator.com/item?id=37448659)

Introducing SmartKnob: an open-source input device with software-configurable endstops and virtual detents. This innovative gadget combines a brushless gimbal motor with a magnetic encoder to provide closed-loop torque feedback control, allowing users to dynamically create and adjust the feel of detents and endstops. The SmartKnob View, which includes an integrated display, is currently under active development. Motors are now available for purchase, thanks to the community's efforts in identifying the original manufacturer and collaborating with SparkFun Electronics. While SmartKnob is a DIY project, it requires advanced soldering skills and troubleshooting abilities due to its small-pitch surface-mount soldering and delicate assembly process.

The discussion on Hacker News about the submission "Introducing SmartKnob: an open-source input device with software-configurable endstops and virtual detents" covers various topics related to the project. Here are some highlights:

- Users express their admiration for the well-documented source hardware project and discuss the challenges of software projects, 3D design printing, hardware sourcing, component selection, and support from the community.
- There is a discussion about the feasibility of low-cost PCB manufacturing, availability of affordable tooling, such as CO2 laser cutters, and the increasing popularity of hobbyist hardware projects.
- One user asks a technical question about voltage protection for the motor and the TMC6300 IC schematic connection. Another user, who has electrical engineering knowledge, provides a response and mentions high resistance, low-kV motors, and the importance of handling high voltage correctly.
- The conversation also touches on the difficulties of handling high RPM motors and the potential issues with ESCs (Electronic Speed Controllers) and motor braking.
- Users express their appreciation for the UX (user experience) aspect of the project and discuss the detection of flexing in the PCB and the need for magic touch buttons.
- Several users share their interest in similar projects, such as building digital jukeboxes with physical controls and knob controls for shuffling tracks.
- There is a mention of a YouTube video demonstrating the behavior of a rotary encoder and the availability of software-based detents.
- One user compares the project with existing commercial products, such as the 3Dconnexion SpaceNavigator and SpaceMouse Pro, pointing out the similarities in behavior.
- The discussion veers into the field of music synthesis, with users highlighting the importance of control panels, hardware synthesizers, and the ability to quickly modify parameters for improved performance.
- The topic expands to include haptics and electronic instruments, with users mentioning specific devices and their design, such as the Roland/Boss GT-100 guitar processor and the TP-7 with haptic feedback.
- There are mentions of DIY smart thermostats, power button rotary knobs, flight simming, and even a humorous comment about Tesla's shifters.

Overall, the discussion shows a mix of technical questions, positive feedback about the project, and various related topics sparked by the submission.

### Show HN: WhatsApp-Llama: A clone of yourself from your WhatsApp conversations

#### [Submission URL](https://github.com/Ads-cmu/WhatsApp-Llama) | 115 points | by [advaith08](https://news.ycombinator.com/user?id=advaith08) | [49 comments](https://news.ycombinator.com/item?id=37448005)

Introducing WhatsApp-Llama: Fine-tune Llama 7b to Mimic Your WhatsApp Style

Have you ever wanted an AI that speaks just like you? Well, now you can! A team from Carnegie Mellon University has forked the popular Llama-recipes repository and created WhatsApp-Llama, a tool that allows you to train a language model to replicate your personal texting style on WhatsApp.

By inputting your WhatsApp conversations, you can fine-tune the Llama-2 7b chat model to respond just like you do. The team used parameter efficient finetuning (QLoRA) and int4 quantization on a single GPU to achieve impressive results.

In their experiments, the fine-tuned Llama-2 model quickly picked up on the nuances of the user's texting style. The average number of words generated by the finetuned Llama-2 was 300% more than the vanilla Llama-2 model. The model accurately replicated common phrases and emoji usage.

To test the model's performance, the team conducted a Turing Test with friends. They asked their friends to ask three questions on WhatsApp and then responded with two candidate responses, one from themselves and one from the Llama-2 model. The friends had to guess which response was from the user and which one was from the AI. The model fooled 10% of the friends, with some of its responses being eerily similar to the user's own.

To get started with WhatsApp-Llama, you need to export your WhatsApp chats and preprocess the dataset. The detailed steps can be found in the README file of the repository. The team recommends exporting 10 WhatsApp chats from friends you frequently speak to, excluding media.

With WhatsApp-Llama, you can have an AI that speaks just like you on WhatsApp. This opens up possibilities for personalization and automation in text-based communication. So go ahead, unleash your AI clone and start chatting!

The discussion around the submission "Introducing WhatsApp-Llama: Fine-tune Llama 7b to Mimic Your WhatsApp Style" on Hacker News covered various topics. Here is a summary of the key points:

- One user mentioned that they haven't watched Silicon Valley, prompting another user to share a link to a funny clip from the show.
- There was a reference to the Black Mirror episode "Be Right Back," which reminded another user of the episode.
- Discussions shifted to the technical aspects of using Llama models. One user mentioned that using a 13B model instead of a 7B model significantly improved results. However, others pointed out that the cost of using larger models, such as the 3090 GPU, could be high.
- Users expressed their experiences with the Llama models and the level of accuracy in mimicking their texting style.
- Some users shared their interest in replicating other famous fictional characters, such as Harry Potter portraits.
- The conversation then moved to discussions about other topics, including the Black Mirror series, text messaging durations, and the potential of personalizing AI dialogue systems.
- Users expressed interest in the development of grief counseling AI systems and techniques for healthier AI-human interactions.
- There were discussions regarding the potential profitable business opportunities based on training Llama models and charging for access to personalized AI models.
- Users shared their experiences and plans for exporting chat histories from WhatsApp or other platforms and the necessary pre-processing steps.
- Some users discussed the potential privacy implications of training AI models on personal chat histories.
- There were inquiries about the hardware requirements for running Llama models, and users shared their experiences with different GPUs and services like Google Colab.
- Users mentioned different versions of Llama models, such as Llama 2, and asked about the differences between them.
- The conversation concluded with users discussing other messaging platforms like Telegram and Discord.

Overall, the discussion covered a range of topics, from technical details and experience with Llama models to potential privacy concerns and broader implications of AI conversation systems.

### Show HN: TaleBot â€“ AI-Generated Personalized Bedtime Stories for Kids

#### [Submission URL](https://talebotai.com/) | 16 points | by [cagrisarigoz](https://news.ycombinator.com/user?id=cagrisarigoz) | [16 comments](https://news.ycombinator.com/item?id=37443790)

Introducing TaleBot, the ultimate companion AI storyteller for your children! Tired of telling the same old fairytales? With TaleBot, you can customize characters, develop new storylines, and create personalized bedtime stories in minutes. Say goodbye to monotony and hello to endless giggles and unforgettable memories.

Parenting is a tough job, especially at the end of a long day. Just when you think you can finally relax, your child asks for a story. Instead of resorting to tired old tales, why not let their imagination soar? With TaleBot, they'll become the heroes of their dreams, embark on wild adventures, and bring along their favorite friends, real or imaginary. It's the perfect way to teach them values, nurture curiosity, and ignite their creativity.

Using TaleBot is a breeze. Simply name your main character, choose an adventure, add a moral, and let our AI bot work its magic. Don't worry, we respect your privacy and only ask for non-identifiable information to personalize the story. In just a few minutes, you'll have a unique and engaging story ready to be enjoyed.

But don't just take our word for it. Here's what some of our happy parents have to say: "My 4-year-old loved the story we created together!" Praises like these make us smile and motivate us to provide the best possible experience.

When you create a story with TaleBot, you'll receive a PDF copy as well as an AI-narrated audio recording right in your inbox. The story is yours to keep, share, and cherish as a precious memory. And with your permission, we can even publish your story on our website or podcast, spreading the joy to others.

But that's not all! We're constantly working on new features to enhance your storytelling experience. Keep an eye out for upcoming updates like AI voice selection and video storytelling. We want to make personalized storytelling even more magical and immersive for you and your children.

So, why wait? Start your storytelling journey with TaleBot today and experience the wonder of creating unique and captivating stories. Let your child's imagination flourish, one story at a time.

---

### GitHubGuessr

#### [Submission URL](https://github-guessr.vercel.app/) | 115 points | by [tan-z-tan](https://news.ycombinator.com/user?id=tan-z-tan) | [49 comments](https://news.ycombinator.com/item?id=37432067)

Introducing GitHub-Guessr, a fun and exciting game that will put your coding knowledge to the test! Can you guess the correct GitHub repository just by looking at the code? Get ready to showcase your skills and prove that you're a true code sleuth.

GitHub-Guessr presents you with snippets of code taken from various repositories. Your mission is to analyze the code and guess which repository it belongs to. Each correct guess earns you points and brings you closer to becoming the ultimate GitHub-Guessr champion.

Whether you're a seasoned developer or a coding enthusiast, this game offers a thrilling challenge that will keep you on your toes. It's a great way to explore different coding styles, learn from other developers, and discover interesting projects hosted on GitHub.

So, get your detective hat on, flex your coding muscles, and dive into the world of GitHub-Guessr. Can you guess the GitHub repository from the code? Start playing now and let the coding adventure begin!

In the discussion on this submission, there are several threads of conversation. 

One commenter, gshgg-blk, expressed appreciation for the game but pointed out that it would be helpful to have more information or references when trying to guess the repositories. Another commenter, stvbmrk, apologized for not reading the dropdown options correctly and offered suggestions for improving the game by implementing autocomplete and sharing buttons.

dysc shared three suggestions: making it easier to select options, gradually increasing the difficulty by introducing more complex puzzles, and adding a share button to challenge friends.

tn-z-tn responded to dysc's suggestions, stating that they had implemented some improvements but found it challenging to change the selection style. FireInsight commented on the unbalanced character distribution in a Swift code snippet.

arh68 simply found the game interesting, while trln and zX41ZdbW provided feedback on minor issues they encountered while playing.

smrphc- suggested improving the dropdown guess box by sorting the options alphabetically, and tn-z-tn agreed.

sxstrngthry praised the circular perspective view concept and suggested using smaller options to make scrolling through the list more convenient.

tn-z-tn created the GitHub-Guessr game and shared their thoughts on its development. Other commenters, such as wbdvvr and jtwlsn, offered suggestions for improving the game's functionality and expressed interest in using similar tools in their work.

lnkr suggested including reverse engineering puzzles that target web development. And cglng found the game challenging but admitted to being unfamiliar with most of the listed projects.

bllq clarified a confusion regarding scrollbar visibility in the MongoDB repository, and skttr pointed out that sometimes the highlighted code moment moves unexpectedly.

nvdmf mentioned a sudden scroll jump issue, and 38 reported a compatibility error with Firefox.

Overall, the discussion included suggestions for improvements, feedback on code snippets, and appreciation for the concept of the game.

### TSMC warns AI chip crunch will last another 18 months

#### [Submission URL](https://www.theregister.com/2023/09/08/tsmc_ai_chip_crunch/) | 156 points | by [rntn](https://news.ycombinator.com/user?id=rntn) | [94 comments](https://news.ycombinator.com/item?id=37432948)

TSMC has warned that the chip shortage for high-spec GPUs, such as Nvidia's A100 and H100, will continue until at least the end of 2024. The issue lies with the lack of advanced packaging capacity, which is used to assemble the silicon chips. TSMC can currently only meet around 80% of the demand for its chip on wafer on substrate (CoWoS) packaging technology, particularly used in chips for AI purposes. TSMC expects additional CoWoS capacity to be available within the next 18 months. In the meantime, the shortage will impact the availability of Nvidia's H100 and A100, as well as AMD's upcoming Instinct MI300-series accelerators.

The discussion on Hacker News revolves around the chip shortage and its impact on the availability of high-spec GPUs. Some users speculate about the reasons for the shortage, with one user noting that TSMC's packaging technology and capacity are the main bottlenecks. Others discuss the involvement of Chinese companies and the use of stolen intellectual property. There is also a discussion on the availability and pricing of Raspberry Pi boards, with some users pointing out the challenges faced by hobbyists due to increased prices. The topic of global chip manufacturing capacity and demand is also debated, with some users suggesting that increased supply would lead to smaller profit margins. There are also discussions about the German region of Saxony and its perceived xenophobia, as well as the issues related to staffing and assembly in the chip manufacturing industry. Overall, the discussion covers various aspects of the chip shortage, its causes, and its impact on different segments of the market.

### Show HN: Rivet â€“ open-source AI Agent dev env with real-world applications

#### [Submission URL](https://rivet.ironcladapp.com/) | 160 points | by [gogwilt](https://news.ycombinator.com/user?id=gogwilt) | [30 comments](https://news.ycombinator.com/item?id=37433218)

Ironclad, the leading digital contracting platform, has released Rivet, a visual programming environment for building AI agents with large language models (LLMs). Rivet allows teams to effectively design, debug, and collaborate on complex LLM prompt graphs, and deploy them in their own environment. It provides a visual interface to visualize and build complex chains for AI applications, a remote debugger to observe prompt chain execution in real-time, and the ability to version and review prompt graphs using YAML files. The community has praised Rivet's game-changing nature, its ability to address limitations and facilitate collaboration, and its power in rapidly prototyping and understanding complex AI workflows. Rivet has already been used by Ironclad to develop their virtual contract assistant, powered by AI agents. To get started with Rivet, the Getting Started guide, integration with Node or TypeScript applications, and an example application are available.

The discussion on Hacker News surrounding the submission about Rivet, an open-source visual AI programming environment, covered various topics.

One user mentioned their interest in a native Rust and TypeScript solution for Rivet and asked if there were plans to add plugins to dynamically change and manage prompt graphs. Another user replied, stating that Rivet does not have built-in chat GPT plugins and that prompt graphs are explicitly built by publishing prompt chains as a publicly available plugin.

Another user shared that they have been following two similar services that solve the problem of building workflows with large language models and recommended checking out FlowiseAI and lgspc-lngflw on GitHub.

Another user expressed excitement about trying Rivet and mentioned considering supporting open-source language models, inference servers, and LocalAI.

Someone else mentioned that they recently open-sourced a platform for building workflows with language models visually and locally and provided a link for it.

Another user stated that they haven't seen this kind of support in a while and highlighted the importance of requirements for production applications.

Someone else praised the work done and mentioned the usefulness of the dev tools for structuring functions and integrating powerful tools for careful spending in a controlled manner.

Another commenter mentioned that they have implemented AI features recently and found Rivet to be a perfect match for building large language model applications.

A user expressed their support for Rivet in building large language model applications and mentioned collaborating with the Ironclad team on integrating user experience paradigms.

Another user mentioned their interest in real-world projects and suggested open-sourcing the implementation and possibly using multiple APIs for specific purposes.

Someone else mentioned their excitement about trying AI efforts and praised Rivet's applicability in the field.

Another commenter asked if Rivet supports TypeScript and plans to have a desktop version, to which the Rivet team clarified that while it runs locally on a web app, there are plans to add a desktop version in the future.

Several comments congratulated the Rivet team on the launch and expressed support for the project.

Lastly, there was a brief discussion about the target audience for Rivet, with one user asking if it targets enterprise startups. The response clarified that Rivet is designed for growth-stage startups and partners who collaborate with Ironclad.

Overall, the discussion highlighted enthusiasm for Rivet's capabilities and potential applications, as well as requests for additional features and integrations with different technologies.

### Show HN: Find jobs at top AI startups

#### [Submission URL](https://workinai.xyz/) | 27 points | by [himanshujaju](https://news.ycombinator.com/user?id=himanshujaju) | [5 comments](https://news.ycombinator.com/item?id=37435689)

Looking to land a job at one of the hottest AI companies? Well, you're in luck! WorkinAI.xyz has compiled a comprehensive list of over 450 job openings at more than 20 AI companies. This is your chance to get in on the cutting edge of technology and work alongside some of the brightest minds in the industry. Just make sure to view the page on a desktop, as it allows you to easily sort, filter, and search through the available positions. And if you have any suggestions, feedback, or inquiries, don't hesitate to reach out to hello@workinai.xyz. Get ready to take the next big step in your career!

The comments on the submission are brief but positive. One user suggests starting with a small company to gain experience, another user expresses happiness and shares the link on Twitter, one user thanks the OP for the helpful job application, and another user commends the work and hopes that it expands.

### We built an AI-powered Magic the Gathering card generator

#### [Submission URL](https://txt.cohere.com/urzas-ai/) | 126 points | by [MWil](https://news.ycombinator.com/user?id=MWil) | [85 comments](https://news.ycombinator.com/item?id=37427854)

Magic the Gathering (Magic) is a beloved collectible card game that has captivated players for years with its unique asymmetrical gameplay. However, one complaint is that there aren't enough cards to satisfy the players' thirst for new content. That's where Urza's AI comes in. Urza's AI is a website created by three individuals who wanted to generate more Magic cards using the power of artificial intelligence (AI). The team utilized a combination of language AI and text-to-image AI to generate playable Magic cards based on prompts. First, they used a large language model (LLM) to generate text information about the card, such as its cost, type, subtype, and description. They finetuned the model with a dataset of existing Magic card descriptions, leading to more realistic and playable card outputs.

But a Magic card isn't complete without an accompanying image. To generate card images, the team employed the Wombo Art API, which uses text input and image output. They provided the API with the card name, types, and subtypes, resulting in impressive and thematic card illustrations. Not stopping there, the team also used AI to generate the card back and Mana icons. By creating prompts and leveraging the Wombo API, they successfully produced card backs and Mana icons that complemented the generated cards.

With all the components ready, the team built the Urza's AI website where users can enter a card name and have a complete card rendered. The outcome has been astounding, with thousands of visitors trying out the site within the first four days of its launch. Magic enthusiasts and players now have access to an ever-expanding pool of custom Magic cards, giving them the opportunity to explore new strategies and deck combinations. Urza's AI has tapped into the power of AI to bring excitement and novel experiences to the Magic community.

The discussion surrounding the Urza's AI submission on Hacker News covers various aspects of the topic:

- Some commenters express their appreciation for the quality and capabilities of the AI-generated Magic cards. They highlight the impressive text-to-image generation and note that the cards are suitable for standard, historic, and explorer formats.

- Others are amazed by the use of AI in generating card backs and Mana icons for the Magic cards. They commend the team's work and express gratitude for the exciting possibilities this brings to the Magic community.

- Commenters discuss the potential legal issues surrounding AI-generated cards and the involvement of Wizards of the Coast (WOTC), the company behind Magic the Gathering. One user mentions a previous AI-generated Magic card project called RoboRosewater.

- The topic of AI playing Magic the Gathering is also touched upon. Some users mention instances where AI players have defeated professional human players, highlighting the capabilities of AI in the game.

- The balance and power level of AI-generated cards are also a point of discussion. Commenters debate the ability of AI to generate cards with comparable power levels and the potential impact on the game's balance.
- The topic diverges to discuss AI and machine learning in creating game content in general. Users share insights on AI-driven card generation, win conditions in games, and the challenges of achieving proper balance.
- A few commenters draw parallels between AI-generated Magic cards and existing Magic cards, highlighting similarities and discussing potential strategies and interactions.
- The conversation also touches on the consideration of AI-generated Magic cards as collectibles and potential market value.
- One user jokingly mentions the association of Magic the Gathering with MtGox, a defunct cryptocurrency exchange platform, due to the similar abbreviation.
- The discussion concludes with references to related articles and posts from previous years that explored similar AI-generated Magic cards.

Overall, the discussion on Hacker News explores the excitement and implications of AI-generated Magic cards, delves into gameplay and balance considerations, and provides references to additional resources for those interested in the topic.

### NVIDIA introduces TensorRT-LLM for accelerating LLM inference on H100/A100 GPUs

#### [Submission URL](https://developer.nvidia.com/blog/nvidia-tensorrt-llm-supercharges-large-language-model-inference-on-nvidia-h100-gpus/) | 67 points | by [mkaushik](https://news.ycombinator.com/user?id=mkaushik) | [21 comments](https://news.ycombinator.com/item?id=37439280)

NVIDIA has announced the upcoming release of TensorRT-LLM, an open-source software for accelerating large language model (LLM) inference on NVIDIA GPUs. The software, which integrates the TensorRT deep learning compiler, includes optimized kernels, pre- and post-processing steps, and multi-GPU/multi-node communication primitives for improved performance. TensorRT-LLM aims to make LLMs more accessible and customizable for developers by providing a modular Python API for defining, optimizing, and executing new architectures. It also supports in-flight batching, allowing for efficient execution of dynamic workloads with varying output sizes. The software has already been integrated by several leading companies and has demonstrated significant performance improvements on NVIDIA GPUs.

The discussion around NVIDIA's announcement of TensorRT-LLM on Hacker News includes various comments from users discussing different aspects of the software.

One user mentions the availability of optimized versions of well-known LLMs like OpenAI's GPT-2 and GPT-3 in TensorRT-LLM. Another user asks about the availability of special weights, to which another user responds that the weights for GPT-3 have not been released openly.

Another user comments that TensorRT-LLM is available through the NVIDIA NeMo framework, which is part of the NVIDIA AI Enterprise software platform. They also mention that developers and researchers can access TensorRT-LLM through the NVIDIA Developer Program.

The efficiency and performance of TensorRT-LLM are discussed by users. One user mentions that acceleration work on H100/A100 GPUs can improve performance by 30-40 times compared to regular GPUs. Another user questions why there is only a 15GB/s communication rating on PCIe for TensorRT-LLM, to which another user responds that it is due to the difference in GPU-to-GPU communication capabilities between NVLink and PCIe.

There are also comments about compatibility and hardware requirements. One user notes that future versions of TensorRT may require higher-end GPUs. Another user shares a link to a GitHub issue where someone mentioned that TensorRT-LLM did not work well with the 30-series GPUs.

The discussion also touches on the flexibility and extensibility of TensorRT-LLM. One user points out that it provides a modular Python API for defining and optimizing new architectures, making it customizable with deep knowledge of C++ and NVIDIA CUDA.

The comparison between NVIDIA and AMD is brought up by a user commenting that while AMD may have caught up with NVIDIA in some areas, NVIDIA still has a more mature and fully optimized software stack.

Another user brings up the similarity between TensorRT-LLM and vLLM (very Large Language Model), mentioning that vLLM uses intermediate-level tensor parallelism and that both technologies have their own advantages and applications.

Lastly, there is a brief discussion about the cost of running H100 and A100 GPUs on the cloud, with one user mentioning that H100 costs around $4 per hour and A100 costs around $2 per hour on the public market.

### Scientific sleuths spot dishonest ChatGPT use in papers

#### [Submission URL](https://www.nature.com/articles/d41586-023-02477-w) | 93 points | by [pseudolus](https://news.ycombinator.com/user?id=pseudolus) | [81 comments](https://news.ycombinator.com/item?id=37431946)

Researchers have been using OpenAI's ChatGPT to write scientific papers without disclosing its use, leading to concerns about the integrity of peer-reviewed publications. ChatGPT is an AI chatbot that generates fluent text in response to user prompts. However, some researchers have been found to have used ChatGPT to help draft their manuscripts without declaring it. In one case, a paper was retracted by the journal Physica Scripta because the authors did not disclose their use of the tool. Since April, more than a dozen journal articles have been flagged for containing telltale ChatGPT phrases without proper disclosure. This raises concerns about the widespread use of AI language models in scientific publishing.

The discussion on this submission covers a range of topics related to the use of AI in scientific publishing. Some commenters express frustration with the current state of academic journals, noting that they are slow, expensive, and often prioritize profit over the dissemination of research. Others argue that the current peer review system is flawed and prone to biases and conflicts of interest. Some commenters defend the anonymity of peer reviewers, while others suggest alternative systems for collecting review data.

There is also a discussion about the use of AI in academia, with some commenters arguing that it can be a helpful tool for researchers, while others express concerns about its impact on the integrity of scientific publications. Some commenters posit that the problem lies not with AI itself, but with the lack of transparency and honesty among researchers in disclosing their use of AI tools.

Other points raised include the need for better document hosting services and functionality, the importance of replication in scientific publishing, and the systemic issues within the university system that may contribute to the current state of academic publishing.

Overall, the discussion reflects a variety of perspectives on the use of AI in scientific publishing and raises important questions about transparency, integrity, and the future of the peer review system.

### Large Language Models as Optimizers. +50% on Big Bench Hard

#### [Submission URL](https://arxiv.org/abs/2309.03409) | 92 points | by [og_kalu](https://news.ycombinator.com/user?id=og_kalu) | [33 comments](https://news.ycombinator.com/item?id=37434069)

Researchers from Google AI have proposed a new approach called Optimization by PROmpting (OPRO) that leverages large language models (LLMs) as optimizers for solving complex optimization problems. Traditional derivative-based optimization algorithms face challenges when dealing with tasks that lack gradients. OPRO uses LLMs to generate new solutions based on a prompt that contains previously generated solutions. These new solutions are then evaluated and added to the prompt for the next optimization step. The researchers demonstrate the effectiveness of OPRO on linear regression and traveling salesman problems. They also show that prompts optimized by OPRO outperform human-designed prompts by up to 8% on GSM8K and up to 50% on Big-Bench Hard tasks. The paper, "Large Language Models as Optimizers," is available for download.

The discussion surrounding the submission about Google AI's OPRO approach includes various viewpoints and comments. One commenter argues that traditional derivative-based optimization algorithms cannot handle tasks without gradients, but another points out that these distinctions are inventions and not necessarily applicable in the realm of technology. Another user brings up Microsoft's Tay as an example of the unpredictability of training models. 

There is also a discussion about the importance of understanding the underlying system and the role of advanced technology in distinguishing it from magic. Some users emphasize the potential capabilities of language models (LLMs), while others express the difficulty in comprehending LLMs and their workings due to the lack of mathematical details. 

Another user mentions Charlie Stross's similar submission from a month ago, which leads to a brief conversation about whether one can credibly learn a service in a short period of time. Another thread veers off-topic into discussing Skyrim, the internet in general, and breaking large problems into smaller ones. 

The next set of comments focuses on the paper itself. One commenter highlights that OPRO outperforms human-designed prompts, while another comments on the difficulty in understanding the paper beyond its "academic crust." There are some exchanges regarding the process of optimization and the potential of LLMs to handle optimization problems. One user suggests that LLMs might be able to offer insights into proving optimization problems, while another brings up the distinction between text-based models like PaLM and 2L-Thought. 

There is further discussion about the challenges of providing good prompts and the potential for LLMs to help explain their thinking process. One user mentions their related work on black-box optimization using LLMs. Finally, there is a brief conversation about solving Sudoku puzzles and the potential applications of LLMs in solving complex problems.