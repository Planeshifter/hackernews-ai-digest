import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Thu Aug 03 2023 {{ 'date': '2023-08-03T17:11:21.356Z' }}

### Launch HN: Sweep (YC S23) – A bot to create simple PRs in your codebase

#### [Submission URL](https://github.com/sweepai/sweep) | 176 points | by [williamzeng0](https://news.ycombinator.com/user?id=williamzeng0) | [103 comments](https://news.ycombinator.com/item?id=36987454)

Sweep is an AI junior developer that aims to streamline the process of addressing bug reports and implementing new features. Unlike other AI solutions like GitHub Copilot or ChatGPT, Sweep handles the entire development flow end-to-end, from reading the codebase to planning the changes and writing a pull request with code. The unique aspect of Sweep is that it can directly transform bug reports and feature requests into pull requests without the need for an IDE. Developers can describe bugs, small features, and refactors to Sweep just as they would to a junior developer, and it takes care of the rest.

Sweep leverages embedding-based code search and supports all languages that GPT-4 supports, including Python, JavaScript/TypeScript, Rust, Go, Java, C#, and C++. It also addresses developer replies and comments on its pull requests and can handle multiple tickets in parallel. However, there are some limitations to be aware of. Sweep may struggle with large-scale refactors involving more than three files or more than 150 lines of code changes. It may also have trouble with using the latest APIs that have changed after 2022. Additionally, non-text assets like images cannot be edited using Sweep, and it cannot access external APIs or fetch API tokens.

Sweep is powered by GPT-4 32k 0613 and uses ActiveLoop DeepLake for Vector DB with MiniLM L12 as the embeddings model. The infra and deployment are handled by Modal Labs. Sweep offers unlimited GPT3.5 tickets to every user and provides five GPT4 credits, which are used when a pull request is created. For professionals who require more tickets and priority support/feature requests, there is a Sweep Pro option available.

The discussion on this submission mainly revolves around the capabilities and limitations of Sweep, as well as the practicality and potential impact of an AI junior developer.

One user points out that Sweep appears to be a backend system powered by GPT-4. Another user clarifies that Sweep is self-hosting and runs entirely on GitHub, so there is no need for manual setup. Others discuss the potential challenges of testing and verifying the correctness of Sweep's code changes, as well as its ability to handle large-scale refactorings or changes to APIs. There is some skepticism about the effectiveness of an AI junior developer, with one user mentioning that it may be difficult to accurately test and measure its success in implementing changes. However, another user expresses appreciation for Sweep's small and successful ticket migration functions. The discussion also touches on the importance of writing clean and maintainable code, with one user noting that junior developers often learn by writing code and building small features. Some users express concern about the potential impact of AI replacing junior developers, while others suggest that it could be a helpful tool in assisting and offloading some tasks. The conversation ends with users discussing the potential benefits and drawbacks of Sweep and expressing interest in its development.

### Hackers manage to unlock Tesla software-locked features

#### [Submission URL](https://electrek.co/2023/08/03/hackers-manage-unlock-tesla-software-locked-features/) | 791 points | by [1970-01-01](https://news.ycombinator.com/user?id=1970-01-01) | [717 comments](https://news.ycombinator.com/item?id=36988262)

A group of hackers has discovered an exploit that allows them to unlock Tesla's software-locked features, which are worth up to $15,000. This includes features like heated seats and Tesla's Full Self-Driving package. The hackers from TU Berlin plan to present their findings in a talk titled "Jailbreaking an Electric Vehicle in 2023 or What It Means to Hotwire Tesla's x86-Based Seat Heater" next week. The hack requires physical access to the car and involves a "voltage fault injection attack" on the onboard computer. The hackers claim that their "Tesla Jailbreak" is "unpatchable" and allows them to run arbitrary software on the infotainment system. However, they believe unlocking Full Self-Driving would require more reverse-engineering. Despite the exploit, the hackers believe Tesla's security is better than other automakers.

The discussion on this submission covers a range of topics related to privacy, security, and hacking. Some users express concerns about the potential misuse of location data and the implications of license plate recognition technology. Others discuss various methods of facial recognition evasion, including wearing masks or using distorted fonts. The conversation also touches on the legality of dash cams and CCTV surveillance in different countries, with some pointing out the potential privacy violations and others highlighting the need for security measures in certain situations. Overall, the discussion reflects a mix of opinions and perspectives on the topics raised in the submission.

### Commercial quantum computer identifies molecular candidate for better solar cell

#### [Submission URL](https://www.ornl.gov/news/researchers-use-commercial-quantum-computer-identify-molecular-candidate-development-more) | 131 points | by [gmays](https://news.ycombinator.com/user?id=gmays) | [50 comments](https://news.ycombinator.com/item?id=36990162)

Researchers from the Department of Energy's Oak Ridge National Laboratory (ORNL) have used a commercial quantum computer to identify a molecular candidate for the development of more efficient solar cells. By modeling singlet fission, a process in which absorption of a single photon of light by a molecule produces two excited states, the team confirmed that the energetic levels of the linear H4 molecule match the requirements for singlet fission. Singlet fission has the potential to increase the efficiency of solar cells beyond the theoretical limit of 33%. The ORNL team used a quantum solver called PDS, which offers higher accuracy and fewer computational demands than classical strategies, to perform the calculations. They applied three independent strategies to decrease the computational workload, reducing their time to solution from months to a few weeks. The project was funded by the DOE's Office of Basic Energy Sciences, and access to the quantum computer was provided by the Quantum Computing User Program at the Oak Ridge Leadership Computing Facility.

The discussion on this submission revolves around the validity and accuracy of the research conducted by the researchers from Oak Ridge National Laboratory (ORNL). Some commenters criticize the research, questioning the practicality and relevance of using a quantum computer to model a simple molecule like H4. They argue that the research is misleading and suggest that the funding and resources allocated to quantum computing should be utilized more effectively. Others defend the research, pointing out that quantum computing is a growing field with significant potential and that the calculations performed by the ORNL team are important for benchmarking. There is also a discussion about the stability and existence of H4 under normal conditions, with some commenters providing scientific explanations and others raising doubts about the accuracy of the research claims. The conversation also touches on the limitations of classical computers compared to quantum computers and their respective advantages in certain calculations.
Overall, the discussion highlights a divide between skeptics who question the practicality and validity of the research and proponents who argue for the potential of quantum computing in scientific calculations.

### Malicious Android Apps Slip into Disguise

#### [Submission URL](https://krebsonsecurity.com/2023/08/how-malicious-android-apps-slip-into-disguise/) | 76 points | by [todsacerdoti](https://news.ycombinator.com/user?id=todsacerdoti) | [51 comments](https://news.ycombinator.com/item?id=36984302)

Mobile malware purveyors have been exploiting a bug in the Google Android platform to sneak malicious code into mobile apps and evade security scanning tools, according to researchers from security firm ThreatFabric. The bug involves corrupting components of an app so that the malicious code is treated as invalid by security scanning tools, but is accepted as valid by the Android OS. ThreatFabric says it has seen an increase in the use of this obfuscation method in mobile banking trojans, which it attributes to a semi-automated malware-as-a-service offering in the cybercrime underground. Google has updated its app malware detection mechanisms in response to the research.

The discussion on Hacker News revolves around various aspects related to the issue of mobile malware and the bug in the Google Android platform. Here are some key points from the discussion:

1. A user mentioned caution when downloading certain apps like Microsoft Teams, as there have been cases where apps claim to be developed by recognized entities but turn out to be malicious.

2. Another user suggested using F-Droid, an alternative app store that focuses on open-source apps. F-Droid complies with the source code of the apps it hosts, making it difficult for malware to go unnoticed.

3. There was a discussion about the difference in security between Android and iOS platforms. Some users expressed that iOS faces fewer security issues compared to Android, although others mentioned that both platforms have their own vulnerabilities.

4. One user shared their experience with a Chinese phone brand and mentioned that they encountered malware when installing unofficial apps. They emphasized the importance of sticking to trusted and official app sources.

5. The conversation also covered topics such as QR code reader apps, the need for better security measures in app updates, and the supply chain attacks in the tech industry.

Overall, the discussion highlights the importance of being cautious when downloading apps, using trusted sources, and staying updated on security issues in mobile platforms.

### IBM and NASA Open Source Largest Geospatial AI Foundation Model on Hugging Face

#### [Submission URL](https://newsroom.ibm.com/2023-08-03-IBM-and-NASA-Open-Source-Largest-Geospatial-AI-Foundation-Model-on-Hugging-Face) | 295 points | by [drkommy](https://news.ycombinator.com/user?id=drkommy) | [76 comments](https://news.ycombinator.com/item?id=36985197)

IBM and NASA have partnered to release the largest geospatial AI foundation model on the open-source AI platform Hugging Face. The model, called watsonx.ai, was built using NASA's satellite data and aims to democratize access and application of AI for climate and Earth science research. By making the geospatial foundation model openly available, IBM and NASA hope to accelerate climate-related discoveries and improve our understanding of the planet. The model has already shown a 15% improvement in performance compared to state-of-the-art techniques using less labeled data. IBM plans to release a commercial version of the model later this year through the IBM Environmental Intelligence Suite.

### Extras worry they'll be replaced by AI. Hollywood is already doing body scans

#### [Submission URL](https://text.npr.org/1190605685) | 71 points | by [rntn](https://news.ycombinator.com/user?id=rntn) | [112 comments](https://news.ycombinator.com/item?id=36987273)

Background actors in Hollywood are concerned that they could be replaced by artificial intelligence (AI) technology. Many actors have recently been required to undergo body scans, where their faces and bodies are digitally replicated without their explicit consent. This has become a central issue in the ongoing labor dispute between studios and the SAG-AFTRA union. While studios argue that the digital replicas would only be used in projects the performers were hired for, actors fear that AI will eventually render them obsolete. The use of AI in Hollywood has already advanced significantly, with technology allowing for the creation of synthetic crowds and the manipulation of actors' performances, appearance, and dialogue. Actors and writers see the ongoing strike as an opportunity to establish rules for the ethical use of AI in the industry.

The discussion on this submission revolves around the use of AI technology in Hollywood and its potential impact on actors. Some users argue that AI technology has advanced to a point where it can realistically replicate actors, leading to concerns about job security. Others point out that CGI has been used for decades and hasn't replaced actors entirely. There is a debate about the ethical implications of using AI to replicate actors without their explicit consent, with some suggesting that actors should have more control over the use of their digital likeness. Additionally, the discussion touches on the broader issues of labor rights and regulations in the entertainment industry.

### Kenyan moderators decry toll of training of AI

#### [Submission URL](https://www.theguardian.com/technology/2023/aug/02/ai-chatbot-training-human-toll-content-moderator-meta-openai) | 55 points | by [heavyset_go](https://news.ycombinator.com/user?id=heavyset_go) | [80 comments](https://news.ycombinator.com/item?id=36986847)

A group of Kenyan content moderators who worked on OpenAI's ChatGPT AI model have filed a petition to the Kenyan government, alleging exploitative working conditions. The moderators claim to have suffered psychological trauma, low pay, and abrupt dismissals while working for Sama, the data annotation services company hired by OpenAI. The moderators state that they were exposed to graphic and violent content, including scenes of sexual violence, and were not adequately warned or provided with sufficient psychological support. They were paid between $1.46 and $3.74 per hour. OpenAI declined to comment on the allegations.

The discussion on this submission primarily revolves around the low wages and working conditions of the Kenyan content moderators who worked on OpenAI's ChatGPT AI model. Some commenters argue that the starting salary of $300 per month is staggering considering the average monthly household income in Kenya is $145. Others point out that Kenya has significant instability and suggest that the moderators are fortunate to have the opportunity to work for Sama. The issue of exploitative working conditions in developing countries is also raised, with some arguing that it is a result of capitalism and the interests of wealthy global companies. The psychological toll of moderating traumatic content is acknowledged, with one commenter comparing it to the stress experienced by emergency responders. Overall, there is a recognition of the need for better wages, working conditions, and support for content moderators.

---

## AI Submissions for Wed Aug 02 2023 {{ 'date': '2023-08-02T17:11:16.296Z' }}

### Tidal Cycles – Live coding music with Algorithmic patterns

#### [Submission URL](https://tidalcycles.org/) | 95 points | by [pabs3](https://news.ycombinator.com/user?id=pabs3) | [12 comments](https://news.ycombinator.com/item?id=36967413)

If you're into music and coding, Tidal Cycles is worth checking out. Tidal Cycles, also known as Tidal, is a free/open-source live coding environment for creating algorithmic patterns. Developed in Haskell, this powerful tool allows users to generate flexible and dynamic sequences of sounds, notes, parameters, and much more.

Tidal Cycles takes advantage of another open-source software called SuperCollider for synthesis and I/O. This combination opens up a world of possibilities for musicians and composers who want to experiment with algorithmic music.

One of the notable features of Tidal Cycles is its pattern-based approach to music creation. With Tidal, you can write code to create patterns, enabling you to explore polyphonic, polyrhythmic, and generative sequences of sounds. It's a flexible and expressive way to compose, improvise, and delve into the depths of algorithmic music.

But Tidal is not just a tool; it's also a thriving community of musicians who utilize the software for their compositions, improvisations, and explorations. The Tidal Blog offers insights from fellow community members, and you can even submit your own blog post to share your experiences and knowledge. If you're looking to connect and learn from other Tidal enthusiasts, this community is the place to be.

Whether you're a seasoned musician or a curious coder, Tidal Cycles offers an exciting platform to express your creativity through algorithmic music. Give it a try, and who knows, you might just discover a whole new world of sonic possibilities.

There are a few comments in the discussion about Tidal Cycles. One user suggests trying an alternative called Strudel, another mentions that they have been making music with Tidal for 10 years and shares some links to their work. Another user asks for a comparison between Tidal and other similar packages like Sonic Pi, Ruby FoxDot, and Python TidalHaskell in terms of workflow and style. A user named "jrmtg" responds, saying they are more interested in writing SuperCollider code and find visual programming languages less interesting. They mention that TidalCycles can depend on SuperCollider for MIDI and sample playback. Another user mentions that Tidal Cycles connects with Ableton MIDI, making composition a fun experience with declarative sequencing. Overall, the discussion includes some alternative suggestions, personal experiences, and comparisons with other music packages.

### Open-sourcing AudioCraft: Generative AI for audio

#### [Submission URL](https://ai.meta.com/blog/audiocraft-musicgen-audiogen-encodec-generative-ai-audio/) | 868 points | by [iyaja](https://news.ycombinator.com/user?id=iyaja) | [301 comments](https://news.ycombinator.com/item?id=36972347)

Meta, the parent company of Facebook, has open-sourced AudioCraft, a framework that generates high-quality audio and music from text-based user inputs. This technology allows professional musicians to explore new compositions without needing to play any instruments, indie game developers to add realistic sound effects on a budget, and small business owners to easily add soundtracks to their social media posts. AudioCraft consists of three models: MusicGen, AudioGen, and EnCodec. The pre-trained models and code are now available for research purposes, enabling researchers and practitioners to train their own models and advance the state of the art in generative audio.

The discussion on Hacker News revolves around the licensing issues related to the open-sourced AudioCraft framework. Users point out that the CC-BY-NC license used for the MusicGen models restricts commercial use, which could limit its practicality. Some argue that the definition of "noncommercial" in copyright law is subjective and varies, while others provide examples and legal references to support their interpretations. The conversation also touches on the potential challenges and benefits of generating commercial music using AudioCraft, as well as the nuances of noncommercial licensing.

### ChromeOS is splitting the browser from the OS, getting more Linux-y

#### [Submission URL](https://arstechnica.com/gadgets/2023/08/google-is-finally-separating-chrome-from-chromeos-for-easier-updates/) | 106 points | by [mfiguiere](https://news.ycombinator.com/user?id=mfiguiere) | [75 comments](https://news.ycombinator.com/item?id=36977107)

Google is preparing to split up ChromeOS and its Chrome browser in an upcoming release. Codenamed "Lacros," this project will separate ChromeOS's Linux OS from the Chrome browser, allowing for independent updates. ChromeOS will move from the homemade Freon graphics stack to Wayland, the normal desktop Linux graphics stack. On the browser side, ChromeOS will switch to the Chrome browser for Linux. The split is expected to make it easier to update ChromeOS and could extend the lifespan of older devices. Google has not officially confirmed the project, but the code suggests it is heading in that direction.

The discussion on this submission covers a range of topics and opinions. Here are some key points:

- One user suggests that Microsoft may release their own Chromebook-like devices running EdgeOS and Edge browser.
- Another user argues that Microsoft is targeting the education market with locked-down operating systems and software like Teams, but it may be difficult for them to compete with Chromebooks in that space.
- The topic of data privacy and advertising is brought up, with one user mentioning concerns about Google harvesting advertising data from students' Chromebooks.
- There is a discussion about the benefits of using Chromebooks in schools, such as centralized management and affordability, as well as the possibility of using Linux laptops running Firefox and LibreOffice.
- Some users question the necessity of laptops for kids in schools, suggesting that desktop computers or tablets may be more suitable.
- The reliability and cost-effectiveness of Chrome OS compared to Windows and macOS is debated.
- A disagreement arises regarding the importance of traditional subjects like clear speech, critical thinking, mathematics, geography, and history in the curriculum, with one user arguing that Chromebooks can't replace the value of these subjects.
- The availability and popularity of Chromebooks in Scandinavia are questioned, with some users suggesting that they are not widely used in schools there.
- One user finds it interesting that Microsoft is selling a Linux-based consumer device.
- The completion of Google's Project LaCros, which separates ChromeOS and Chrome browser, is discussed.
- There is a conversation about running different Linux distributions on Chromebooks and the limitations of virtual machines.
Overall, the discussion covers a wide range of perspectives on Chromebooks, their use in education, and the future of ChromeOS and Chrome browser.

### Cookbook: Finetuning Llama 2 in your own cloud environment, privately

#### [Submission URL](https://blog.skypilot.co/finetuning-llama2-operational-guide/) | 116 points | by [covi](https://news.ycombinator.com/user?id=covi) | [12 comments](https://news.ycombinator.com/item?id=36975245)

Yesterday, Meta released Llama 2, a pre-trained language model that can be fine-tuned on user data and used commercially. In this article, the authors provide a step-by-step recipe for finetuning Llama 2 on your own data using open-source tools. They emphasize the advantages of this approach, including full control over compute, data, and models, support for multiple cloud providers, high GPU availability, and reduced costs through the use of spot instances. The recipe includes instructions for obtaining access to the Llama-2 model, installing SkyPilot (the tool used for training), and configuring the training data and model identity. It also provides a command to start training on any cloud, with options for selecting cloud provider, GPU availability, and cost optimization. Overall, this guide offers a comprehensive and open-source approach to fine-tuning Llama 2 and using it in commercial settings.

The discussion about the submission mainly revolves around the cost and efficiency of using Llama 2 for fine-tuning and production inference. One user points out that the cost depends on the GPU type and the serving system's traffic patterns, recommending the use of higher-cost optimized GPUs. They also highlight the benefits of cost optimization and mention the difference in cost between Llama-2 and GPT models.

Another user raises a question about the running cost of Llama 2 on a 70B GPU, assuming maximum utilization. There is also a mention of the latest release of Vicuna-15.

The topic of fine-tuning is also discussed, with one user suggesting replacing the retrieval step with a knowledge organization step. However, another user points out the challenges of fine-tuning based on organizational data, as the underlying data can change significantly, leading to high maintenance costs.

The possibility of customizing the knowledge identity and the challenges of fine-tuning due to the chit-chat problem are discussed. A user suggests that fine-tuning cannot address the chit-chat problem effectively, and contextual solutions that provide relevant answers should be considered.

The advantage of combining methods for better performance is also mentioned, such as the combination of fine-tuning and retrieval steps.

A related thread about running Llama 2 locally and the potential use of Llama 2 for specific purposes like Apple Silicon is also mentioned.

Overall, the discussion revolves around cost optimization, challenges in fine-tuning, and the customization and limitations of Llama 2 for various use cases.

### Nvidia AI Image Generator Fits on a Floppy Disk and Takes 4 Minutes to Train

#### [Submission URL](https://decrypt.co/150861/nvidia-ai-image-generator-floppy-disk-4-minutes) | 20 points | by [magoghm](https://news.ycombinator.com/user?id=magoghm) | [3 comments](https://news.ycombinator.com/item?id=36974890)

Nvidia researchers have introduced a new text-to-image personalization method called Perfusion, which allows for significant creative flexibility in AI-generated art while maintaining the identity of specific concepts. Perfusion outperforms other AI art generators in terms of efficiency and offers the feature of combining multiple personalized concepts in a single image with natural interactions. The key idea behind Perfusion is "Key-Locking," which connects new concepts to more general categories during image generation, preventing overfitting and enabling the portrayal of personalized concepts while retaining their core identity. Perfusion's small size of just 100KB makes it more efficient and customizable compared to bulkier AI image generators.

The discussion on Hacker News focused on the technical aspects and potential implications of Nvidia's Perfusion text-to-image personalization method. One user expressed skepticism, stating that the key-locking approach of connecting new concepts to general categories seemed like a dishonest form of clickbait. They argued that Perfusion should not be called an art generator but recognized that it outperforms other AI image generators in terms of efficiency. Another commenter compared Perfusion to other existing models in the AI art generation landscape, such as Stable Diffusion and MidJourney, but did not fully understand the personalization method used in Perfusion. They acknowledged the small size of Perfusion and its potential for better performance and customization compared to larger models. Another user appreciated the idea of embedding the model in just 100KB.
