import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Thu Feb 15 2024 {{ 'date': '2024-02-15T17:11:27.177Z' }}

### Sora: Creating video from text

#### [Submission URL](https://openai.com/sora) | 3363 points | by [davidbarker](https://news.ycombinator.com/user?id=davidbarker) | [1996 comments](https://news.ycombinator.com/item?id=39386156)

Sora, an AI model developed by researchers, has the ability to create realistic and imaginative video scenes based on text instructions. The model can generate videos up to a minute long, while maintaining visual quality and adhering to the user's prompt. The examples provided showcase Sora's capabilities, including scenes of a stylish woman walking down a Tokyo street, giant wooly mammoths in a snowy meadow, a movie trailer featuring a space man, waves crashing against cliffs in Big Sur, a monster kneeling beside a melting candle, a papercraft world of a coral reef, a close-up shot of a Victoria crowned pigeon, and a photorealistic closeup video of pirate ships battling in a cup of coffee. Sora's technology aims to teach AI to understand and simulate real-world interactions, with the goal of helping people solve problems that require physical interaction.

The discussion surrounding the submission "Sora: An AI Model for Video Scene Generation" on Hacker News covers a range of topics. 
One commenter points out that the AI model's ability to generate creative scenes raises concerns about the loss of creativity in humans who are forced to perform mundane tasks. Another commenter argues that creative humans should be given credit for their work, just like famous artists throughout history were not required to acknowledge their inspirations. However, the original commenter disagrees, stating that acknowledging influences and inspirations is essential, as it prevents theft and respects the original creators.
The discussion then delves into the topic of whether AI can truly generate original work without being influenced by humans. Some commenters argue that AI models like Sora are capable of creativity and can generate personalized content, while others are skeptical and question whether AI can truly understand and create meaningful work.
There is also a debate on whether AI causing displacement and job loss is a significant concern. Some argue that AI has caused displacement in other industries before, while others believe that the impact of AI on industries will not be as transformative as anticipated.
Finally, there is a discussion about the issue of compensation for the work AI generates. Commenters mention that historical figures like Leonardo da Vinci and Shakespeare were not compensated for their work, and raise questions about how compensation should be handled in the context of AI-generated content.

Overall, the discussion on Hacker News covers a range of perspectives on the capabilities and implications of AI-generated video scenes.

### Our next-generation model: Gemini 1.5

#### [Submission URL](https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/) | 1183 points | by [todsacerdoti](https://news.ycombinator.com/user?id=todsacerdoti) | [550 comments](https://news.ycombinator.com/item?id=39383446)

Google and Alphabet have announced their next-generation AI model, Gemini 1.5, which boasts enhanced performance and a breakthrough in long-context understanding. The model, built upon the MoE architecture, offers a context window of up to 1 million tokens, allowing it to process vast amounts of information in one go. It can analyze and summarize large amounts of content and perform understanding and reasoning tasks across different modalities such as video, audio, and code. Gemini 1.5 Pro, the mid-size multimodal model, is optimized for scaling across various tasks and offers comparable performance to Gemini 1.0 Ultra while using less compute. Developers and enterprise customers can now try Gemini 1.5 Pro in a private preview.

The discussion on the submission revolves around various aspects of the Gemini 1.5 AI model announced by Google and Alphabet. Here are some key points from the comments:

- Some skepticism is expressed about the claimed ability of the model to handle up to 10 million tokens of context, with users questioning the practicality and potential trade-offs.
- A debate arises regarding the significance of benchmarks and whether they accurately reflect the quality of intelligence in AI models. Some users express doubts about Google's track record in delivering on their claims.
- The potential limitations of using vectors in high-dimensional spaces and its impact on model performance are discussed.
- There are concerns about the pricing of the model and whether it would be cost-effective for certain use cases.
- The comparison between Gemini Pro and the previous Gemini Ultra model is examined, with users speculating about the capabilities of GPT-4 and whether Gemini Ultra would still be preferred.
- Some users share their experiences and observations about working with different models and their ability to handle complex tasks and provide realistic answers.
- Discussions also touch on issues related to model requirements, dependencies, and technical aspects of implementation.

Overall, the comments reflect a mix of curiosity, skepticism, and deliberation about the capabilities and practical implications of the Gemini 1.5 AI model.

### Safe and reliable production changes, and how Rivian recently got this wrong

#### [Submission URL](https://blog.substrate.tools/safe-and-reliable-production-changes-for-fast-moving-teams-and-how-rivian-recently-got-this-wrong/) | 58 points | by [kelp](https://news.ycombinator.com/user?id=kelp) | [45 comments](https://news.ycombinator.com/item?id=39386611)

A recent over-the-air (OTA) software update to Rivian vehicles went wrong, causing the infotainment screens to go into a reboot loop. While the vehicles were still drivable, many controls were unavailable for several days while Rivian worked on a fix. This incident raises questions about the process and design flaws at Rivian. One suggestion is to have a canary fleet of vehicles that receive updates first to catch any issues before they reach customer vehicles. Another recommendation is to have a pre-flight check that validates the update before installation, which could have prevented the faulty update from being installed. Additionally, implementing an automatic rollback mechanism for failed updates could minimize downtime. It's important to note that there is rarely a single root cause for such incidents, but rather a combination of mistakes, bugs, or design flaws.

The discussion on Hacker News revolves around the recent software update issue faced by Rivian vehicles. Some users suggest that Rivian should implement pre-flight testing and canary fleet testing to catch any issues before they reach customer vehicles. Others draw comparisons to Volvo's recent software problems and highlight the risks of over-the-air updates. The importance of thorough testing and the potential benefits of automatic rollback mechanisms are also mentioned. The discussion diverges into debates about the reliability of software in vehicles, the weighing of heavy trucks versus passenger cars, and the importance of public transportation infrastructure.

### New bill would let defendants inspect algorithms used against them in court

#### [Submission URL](https://www.theverge.com/2024/2/15/24074214/justice-in-forensic-algorithms-act-democrats-mark-takano-dwight-evans) | 73 points | by [rntn](https://news.ycombinator.com/user?id=rntn) | [10 comments](https://news.ycombinator.com/item?id=39390456)

Democratic lawmakers Reps. Mark Takano and Dwight Evans have reintroduced the Justice in Forensic Algorithms Act, a bill that would allow defendants to access the source code of software used to analyze evidence in their criminal proceedings. The proposed legislation also requires the National Institute of Standards and Technology (NIST) to establish testing standards for forensic algorithms used by federal enforcers. The bill aims to address concerns about the potential bias and limitations of algorithms used in the criminal justice system. While the bill does not currently have Republican co-sponsorship, Takano is optimistic that it can gain bipartisan support.

The discussion on this submission revolves around the reliability and transparency of forensic algorithms used in criminal proceedings.
One commenter discusses the need for comprehensive testing and documentation of these algorithms, stating that testing should include detailed information about test cases, test results, and version-specific configurations. Another user argues that expert witnesses should be involved in reviewing the conclusions derived from algorithmic analysis, as they can provide valuable insight.
However, some commenters believe that algorithms often rely too heavily on statistical analysis and can be expensive to challenge or disprove. They suggest that it can be difficult to prove bias or negligence in algorithmic decisions.
The discussion also touches on the importance of victim advocacy and debugging of algorithmic systems. One user poses a question about the victim's perspective, and another commenter points out the need for fairness and specificity in algorithmic determinations, emphasizing the importance of including jurors in the decision-making process.

Overall, the discussion highlights the need for proper testing, expert review, transparency, and fairness in the use of forensic algorithms in the criminal justice system.

### McDonald's Making Job Applicants Take Weird AI Personality Tests

#### [Submission URL](https://futurism.com/mandatory-ai-hiring-tests) | 58 points | by [hjek](https://news.ycombinator.com/user?id=hjek) | [36 comments](https://news.ycombinator.com/item?id=39388518)

McDonald's, Olive Garden, and FedEx are among the companies requiring job applicants to take personality evaluations sorted by an AI system. Paradox.ai, a conversational recruiting software company, uses strange personality assessments including images of blue-skinned humanoid aliens for applicants to identify with. The assessments are part of Paradox's "Traitify" product, which categorizes applicants into personality groups like the "Big Five" or "OCEAN" categories. The efficacy of such widely-used personality tests has been disputed, but companies continue to invest in these HR testing methods.

The discussion surrounding the submission revolves around the use of personality evaluations by companies during the hiring process. One user points out that these tests often have strange and seemingly irrelevant questions that may not accurately reflect a candidate's abilities. Another user counters by saying that sometimes violence-solving questions can be useful in certain job roles. Some users express their skepticism towards the effectiveness of these tests, while others argue that they help filter out potential problem candidates. The discussion also touches on the subject of credit checks during the hiring process and the legality of such practices. There are mentions of the Futurism article being referenced incorrectly, as well as discussions about the value of personality tests and the impact they can have on employee turnover. The thread ends with a lighthearted comment about Weird Al potentially writing interview questions.

### Asahi Linux project's OpenGL support on Apple Silicon officially surpasses Apple

#### [Submission URL](https://arstechnica.com/gadgets/2024/02/asahi-linux-projects-opengl-support-on-apple-silicon-officially-surpasses-apples/) | 378 points | by [throwaway2037](https://news.ycombinator.com/user?id=throwaway2037) | [146 comments](https://news.ycombinator.com/item?id=39383798)

The Asahi Linux project, a team of independent developers working to support Linux on Apple Silicon Macs, has reached an important milestone with their graphics driver. The Asahi driver now fully supports OpenGL version 4.6 and OpenGL ES version 3.2, surpassing what Apple offers in macOS, which tops out at OpenGL 4.1. The team achieved this despite the fact that Apple's GPUs don't support certain features required by newer graphics standards. The next challenge for the team is to support the low-overhead Vulkan API on Apple's hardware. This progress opens up possibilities for running native Linux apps and taking better advantage of software like Valve's Proton on Arm-based Apple hardware.

The discussion on the submission revolves around the impressive progress made by the Asahi Linux project in supporting Linux on Apple Silicon Macs. Some users discuss the technical details, highlighting the challenges of Apple's deprecation of OpenGL and the missing features required by newer graphics standards. There is also a discussion around the significance of supporting Vulkan API on Apple's hardware and the potential benefits for running native Linux apps and leveraging software like Valve's Proton.

Other users appreciate the efforts of the Asahi team, especially their work in achieving OpenGL and OpenGL ES support. They mention the significance of this milestone and express their admiration for the team's accomplishments. There is also a discussion about the use of Python in the Asahi project and the efficiency of their development workflow.
Some users discuss the challenges faced by the Asahi team in implementing GPU drivers and mention the complexities involved in supporting different GPU APIs. The discussion also touches on the importance of DRM support and USB 3 functionality in Linux.
There is a brief mention of Rosenzweig's blog post that didn't provide specific details about Vulkan support, and the potential impact of Asahi's progress on gaming on macOS and using Valves Proton on Arm-based Apple hardware. Some users also bring up Apple's approach as a hardware company and compare it to other vendors in terms of supporting different versions of hardware.

Overall, the discussion is a mix of technical analysis, appreciation for the Asahi team's efforts, and speculation about the potential implications of their progress.

### Show HN: NeuralFlow – Visualize the intermediate output of Mistral 7B

#### [Submission URL](https://github.com/valine/NeuralFlow) | 131 points | by [valine](https://news.ycombinator.com/user?id=valine) | [20 comments](https://news.ycombinator.com/item?id=39378773)

NeuralFlow, a Python script developed by valine, allows you to visualize the intermediate output layers of Mistral 7B. By running this script, you can generate a heatmap image that represents the output at each layer of the model. This visualization can be particularly useful for inspecting model outputs during the fine-tuning process. By comparing the outputs before and after training, you can identify patterns and anomalies within the model. The script segments the image into chunks of 512 and arranges them vertically for better display on landscape screens. You can find the code for NeuralFlow on valine's GitHub repository, along with some models trained using this visualization technique. These models have generalized exceptionally well and can be accessed through the provided links.

The discussion on this submission includes various comments discussing the benefits and applications of the NeuralFlow Python script developed by valine. Some comments highlight the usefulness of visualizing intermediate output layers in understanding model outputs during the fine-tuning process. Others discuss the potential for identifying patterns and anomalies within the model by comparing outputs before and after training. One user mentions that they have found individual snapshots of models to be helpful in observing structural changes over time. Another user expresses interest in how this visualization technique can improve model performance. The discussion also includes comments about gradient products leading to an indicator and determining the starting rules of the model. One user shares their investigations into the repetition problem and another user explains the concept of folding layer distributions to observe discontinuity. Another user mentions the powerful effect of perception visualization. Finally, there is a comment comparing the visualization to encrypted thoughts in The Matrix.

### Sam Altman owns OpenAI's venture capital fund

#### [Submission URL](https://www.axios.com/2024/02/15/sam-altman-openai-startup-fund) | 246 points | by [choppaface](https://news.ycombinator.com/user?id=choppaface) | [80 comments](https://news.ycombinator.com/item?id=39387578)

In a surprising twist, it has been revealed that Sam Altman, the CEO of OpenAI, also owns the OpenAI Startup Fund, a venture capital fund associated with the company. The fund was launched in late 2021 to invest in AI startups and projects, and has reported $175 million in total commitments. However, what sets it apart is that it is not owned by OpenAI or its affiliated nonprofit foundation, but by Altman himself. The decision to put the fund in Altman's name was made for expediency, but it has now been over a year and there are questions about the potential risks and governance structure. OpenAI has acknowledged the need to re-examine their governance structure and establish a new board before making any changes to the fund. This revelation highlights the structural peculiarities of OpenAI as a company.

The discussion on this submission revolves around various aspects of the news and raises questions about Sam Altman's involvement in OpenAI's venture capital fund and the potential risks and governance structure associated with it.
One comment highlights the need to understand Altman's intentions and suggests that it might be interesting to examine the history and intentions of the founders and funders involved. In response, another user shares a link to an article by Matt Levine discussing the situation.
There is also a comment discussing Andrej Karpathy's perspective on the matter, stating that he believes Altman's involvement was for convenience and that the fund will be re-evaluated.
Another user mentions Gary Marcus trying to bring attention to the situation and provides a link to a tweet by Marcus.
One commenter expresses skepticism and states that Altman's previous business dealings should be taken into consideration. The discussion then shifts to Worldcoin and Altman's involvement in cryptocurrency.
There are comments discussing the difficulties in parsing some sentences and suggesting breaking them into smaller sentences. Another user tries to correct and interpret a previous comment.
Some users bring up the WeWork scandal, with one mentioning the similarities between Altman and WeWork's Adam Neumann.
A comment raises suspicions about Altman, comparing him to a scam and suggesting that the community should be cautious. Another user responds, highlighting the need for substantive comments and avoiding mindless celebrity or billionaire worship.
One comment finds it interesting that small investments can turn into large funding rounds, using Andy Bechtolsheim's small investment in Google as an example.
The discussion also touches on the nature of non-profit organizations and their ability to make money. There are comments discussing tax implications and the distinction between for-profit and non-profit entities.
One user brings up the concept of corporate responsibility and suggests parallelism with Twitter's CEO, Elon Musk, and controversy surrounding the two figures.
Someone jokes about Altman's involvement in OpenAI leading to fictional scenarios like the replacement of workers with robots based on Disney's Black Hole movie from the 1980s.
There are comments about Altman's role as a CEO and comparisons to Reddit's CEO, as well as discussions about Y Combinator and its application process.
One user raises concerns about Altman's involvement based on recent reports, while another comment defends Altman and argues that judging CEOs should consider the context and the qualities they bring to their positions.

The discussion ends with a comment jokingly suggesting that the next revelation will involve Altman sending employees to work on Disney's Black Hole movie set.

---

## AI Submissions for Wed Feb 14 2024 {{ 'date': '2024-02-14T17:12:11.987Z' }}

### Show HN: Reor – An AI note-taking app that runs models locally

#### [Submission URL](https://github.com/reorproject/reor) | 361 points | by [samlhuillier](https://news.ycombinator.com/user?id=samlhuillier) | [88 comments](https://news.ycombinator.com/item?id=39372159)

Reor is a self-organizing AI note-taking app that aims to enhance your productivity and creativity. It automatically links related ideas within your notes, answers questions based on the content, and provides powerful semantic search capabilities. 
What sets Reor apart is that it operates locally, running models on your own device without relying on cloud-based services. By doing so, it ensures privacy and fast response times. The app makes use of Llama.cpp and Transformers.js libraries to enable the execution of both language model models (LLMs) and embedding models.
How does Reor achieve self-organization? Every note you write is chunked and embedded into an internal vector database. Related notes are then automatically connected based on vector similarity. The app also features LLM-powered Q&A, which utilizes a retriever-reader architecture to answer questions based on the corpus of notes. 
Reor provides a seamless user experience, allowing you to edit your notes using a markdown editor similar to Obsidian. It also supports importing notes from other applications. Currently available for Mac, Linux, and Windows, Reor can be downloaded from reorproject.org or the releases section of the GitHub repository.
If you're interested in contributing to the project, the team behind Reor welcomes contributions in all areas. Just raise an issue and discuss it with them before starting the work. The app is licensed under GPL-3.0 license.
Give Reor a try and see how it can transform your note-taking and idea organization process!

The discussion on the Hacker News submission about Reor, an AI note-taking app, covers a variety of topics. 
Some users compared Reor to other note-taking apps like Joplin, Obsidian, and Milanote. One user mentioned that Reor allows for self-organization and markdown editing similar to Obsidian but runs locally, providing privacy and fast response times. Another user mentioned that Joplin is a good alternative but lacks the ability to export notes as individual markdown files. 
There was a discussion about the choice of using markdown files for note-taking and the advantages of using a file-based system. Some users emphasized the importance of having control over their own data and the ability to manipulate files directly. They mentioned that the choice of note-taking app ultimately depends on the user's preference.
Another user raised a question about the file system and how it affects the performance and efficiency of the app. Some users mentioned that the file system ultimately controls the data schema and that using a natural database system makes sense. 
The discussion also touched on other topics such as PDF support, local AI models, and the role of AI in knowledge management. Some users provided suggestions for improving Reor, such as integrating smart connections, minimizing the UI chat window, and integrating browser history/bookmarks.

Overall, the discussion highlighted the interest and excitement surrounding Reor as a self-organizing AI note-taking app that runs locally and enables efficient organization and retrieval of knowledge.

### Show HN: Natural Language to SQL "Text-to-SQL" API

#### [Submission URL](https://www.dataherald.com/news/introducing-dhai) | 54 points | by [saigal](https://news.ycombinator.com/user?id=saigal) | [30 comments](https://news.ycombinator.com/item?id=39373744)

Today, Dataherald has announced the release of their hosted API for their natural language to SQL engine. This API allows developers to easily build natural language data querying into any product. With a few simple API calls, developers can add business context from various sources, train their AI models specifically for their data, and assess confidence levels in AI-generated SQL queries. Dataherald integrates with major data warehouses such as PostgreSQL, Databricks, Snowflake, BigQuery, and DuckDB. If you're tired of dealing with complicated prompts to make NL-to-SQL work, give Dataherald a try. You can sign up for free and get $50 in usage credits.

- There is a comment from "nsmblhq" who congratulates Dataherald on their launch and mentions their interest in security and privacy in hosted vs on-prem offerings.
- "l5870uoo9y" recommends looking into fine-tuning the RAG (Retrieve And Generate) model for more accurate SQL generation.
- "BrickTamblan" discusses the challenges of converting structured data to unstructured text and suggests using the LLM (Language Model with Labeled-data) approach to generate SQL queries.
- "tq" mentions that the challenge with SQL generators is knowing the right question to ask and suggests using context to narrow down the answers.
- "lxmng" comments on the polished self-serve experience of Dataherald and wonders why OpenAI hasn't introduced their own SQL API yet.
- "nick_rocks" shares their experience with Dataherald's self-hosted product and the complexity of SQL queries.
- "throwaway49849" discusses the trust and confidence thresholds in AI-generated SQL queries and the limitations of customers' abilities to create malicious queries.
- "whoomp12341" complains about a SQL server locking issue.
- "dnny" mentions a submission regarding Dataherald Playground that is worth checking out.
- "sgl" provides an API introduction announcement link.
- "cstnly" comments on the ease of integrating with the Dataherald API.

### Disrupting malicious uses of AI by state-affiliated threat actors

#### [Submission URL](https://openai.com/blog/disrupting-malicious-uses-of-ai-by-state-affiliated-threat-actors) | 121 points | by [Josely](https://news.ycombinator.com/user?id=Josely) | [82 comments](https://news.ycombinator.com/item?id=39368859)

OpenAI and Microsoft Threat Intelligence have joined forces to disrupt and terminate the accounts of five state-affiliated threat actors who were attempting to use AI services for malicious cyber activities. The threat actors, identified as Charcoal Typhoon and Salmon Typhoon from China, Crimson Sandstorm from Iran, Emerald Sleet from North Korea, and Forest Blizzard from Russia, were using OpenAI services for various purposes such as researching companies and cybersecurity tools, translating technical papers, and generating content for phishing campaigns. While OpenAI acknowledges that their models have limited capabilities for malicious cybersecurity tasks, they are committed to staying ahead of evolving threats and taking a multi-pronged approach to combatting such misuse. This includes monitoring and disrupting malicious actors, collaborating with the AI ecosystem to exchange information, iterating on safety mitigations, and maintaining public transparency. By sharing insights and taking action, OpenAI aims to promote a safer and more secure development and use of AI technology.

The discussion on the submission revolves around several key points. 
1. Commenters speculate on the motivations and potential political affiliations of the state-affiliated threat actors. Some draw parallels to previous instances of state-sponsored cyber-attacks, while others point out the involvement of intelligence agencies in various countries.
2. The discussion also touches on the naming scheme used for threat actors. Some users express surprise at the naming conventions employed by state-level actors and suggest that it is intended to confuse and misdirect attribution.
3. Some users express concerns about the potential misuse of AI technology for malicious purposes. They discuss the possibility of AI-generated misinformation and propaganda, as well as the potential for AI to aid in the development of malware and cybercrime.
4. One user notes the need for strong state-level cybersecurity measures and suggests that OpenAI's efforts may not be sufficient to counter the threats posed by sophisticated threat actors.
5. The topic of the 2016 Russian interference in the US elections is brought up, with a user recommending reading the Mueller report for detailed evidence of direct communication between the Russian government and the Trump campaign.
6. There is some discussion about the impact of OpenAI's privacy practices and the potential implications for national security. Some users express concerns about the retention of chat histories and the potential for surveillance.

Overall, the discussion highlights the complicated and evolving nature of cybersecurity threats and the role that AI technology can play both in aiding malicious actors and in combatting cyber threats.

### World model on million-length video and language with RingAttention

#### [Submission URL](https://largeworldmodel.github.io/) | 186 points | by [GalaxyNova](https://news.ycombinator.com/user?id=GalaxyNova) | [55 comments](https://news.ycombinator.com/item?id=39367141)

Researchers from UC Berkeley have developed a large-scale multimodal model called the World Model (LWM) capable of processing long video and language sequences. The model, trained on a curated dataset of diverse videos and books, utilizes the RingAttention technique to handle the challenges of memory constraints and computational complexity. With a context size ranging from 4K to 1M tokens, LWM sets new benchmarks in difficult retrieval tasks and long video understanding. The model's features include masked sequence packing, loss weighting, and the generation of a model-generated QA dataset for long sequence chat. The researchers have open-sourced a family of 7B parameter models, enabling broader AI capabilities for understanding human textual knowledge and the physical world. LWM demonstrates impressive performance in tasks such as question-answering over a one-hour video, fact retrieval over 1M context, long sequence prediction, text-image and text-video generation, image understanding, and video chat. The development of LWM unlocks possibilities for training on massive datasets of long video and language, facilitating the development of AI systems with a deeper understanding of the multimodal world.

The discussion on this submission covers various aspects of the World Model (LWM) and its implications. Some comments highlight the potential of large-scale multimodal models like LWM to significantly advance AI capabilities. Others discuss the limitations of conventional models and the need for improvements. There is also a conversation about the importance of context in understanding long videos and how LWM and related models could address this challenge. The discussion further touches on topics such as the availability of pre-trained models, the legal aspects of training AI on copyrighted works, and the ethical considerations surrounding AI training data. Overall, the discussion reflects both enthusiasm for the advancements made with LWM and critical analysis regarding its potential and limitations.

### Waymo recalls software after two self-driving cars hit the same truck

#### [Submission URL](https://www.cnn.com/2024/02/14/business/waymo-recalls-software-after-two-self-driving-cars-hit-the-same-truck/index.html) | 57 points | by [reteltech](https://news.ycombinator.com/user?id=reteltech) | [23 comments](https://news.ycombinator.com/item?id=39375377)

Waymo, the self-driving car division of Google's parent company, Alphabet, has issued a recall for its self-driving car software after two of its vehicles hit the same truck just minutes apart. The incidents occurred in Phoenix, Arizona when both Waymo cars came across a tow truck pulling a pickup truck that was being towed backwards and at an angle. Due to incorrect interpretations of their cameras, both Waymo cars wrongly predicted the movement of the truck and collided with the pickup. No riders were present in either of the Waymo vehicles at the time. Waymo has since updated its vehicle software and installed the updated software on its entire fleet of self-driving Jaguar I-Paces. The company has also informed relevant authorities of the incidents and filed a recall report. While self-driving cars are often touted as a safer alternative to human drivers, incidents involving "edge cases," or unusual situations, have raised concerns about their safety.

The discussion on the submission revolves around various aspects of the self-driving car software recall issued by Waymo. Here are some highlights:

- One commenter suggests that the lidar sensor, which uses lasers to detect objects around the car, should have been able to accurately predict the movement of the tow truck.
- Another commenter shares a link to a blog post on Waymo's website, providing more details about the recall.
- There is a debate about the effectiveness of neural networks in accurately predicting the behavior of objects in unusual situations.
- Some comments discuss the need for a hierarchy of models in self-driving car software, where simpler models predict stationary objects and more complex models handle changing lanes or pedestrian behavior.
- There are suggestions that the recall might have been due to a network configuration issue or the difficulty in interpreting the data captured by the sensors in certain situations.
- One commenter raises the point that the responsibility for accidents should not solely lie with the self-driving software, as human drivers also exhibit unpredictable behavior.

Overall, the discussion touches on the challenges and limitations of self-driving car technology and the lessons that can be learned from incidents like this.

### Only real people can patent inventions – not AI, US Government says

#### [Submission URL](https://www.cnn.com/2024/02/14/tech/billions-in-ai-patents-get-new-regulations/index.html) | 233 points | by [Brajeshwar](https://news.ycombinator.com/user?id=Brajeshwar) | [165 comments](https://news.ycombinator.com/item?id=39370681)

The US government has declared that only real people, not AI, can patent inventions. The US Patent and Trademark Office (USPTO) published official guidance this week stating that a "significant contribution" to an invention must be made by a human to obtain a patent. The decision aims to provide clarity for innovators while upholding the value of human creativity and ingenuity. However, what constitutes a significant contribution is open to interpretation and will be determined case-by-case. The guidelines reflect the Biden administration's focus on addressing artificial intelligence issues, and they also contribute to the ongoing discussion around the role of AI in intellectual property protections.

The discussion on Hacker News revolves around different perspectives on whether AI-generated inventions should be patentable and whether recipes can be copyrighted. Some users point out that recipes can be copyrighted and that the process of validating pharmaceutical compounds requires substantial effort. Others argue that industrial food preparation processes can be patented and that the copyrightability of recipes depends on the level of creativity involved. There is also debate about the use of AI in generating recipes and the potential intellectual property implications. Some users suggest that AI-generated inventions should be considered for patents if they make a significant contribution, while others question the legitimacy of granting patents to AI creations. The discussion also touches upon the copyrightability of AI-generated images and the comparison to previous cases such as the Monkey Selfie copyright dispute. Overall, the discussion highlights the complexity and ongoing debate surrounding the role of AI in intellectual property protection.

### Apple Vision Pro: what does it mean for scientists?

#### [Submission URL](https://www.nature.com/articles/d41586-024-00387-z) | 11 points | by [bookofjoe](https://news.ycombinator.com/user?id=bookofjoe) | [5 comments](https://news.ycombinator.com/item?id=39373983)

Apple's recently released VR headset, the Vision Pro, has the potential to revolutionize research in the field of virtual reality. Scientists are fascinated by the headset's high precision and advanced features, such as its incredibly realistic 'passthrough' and eye-tracking technology. Researchers believe that the Vision Pro could be used to enhance research tasks, analogue activities like surgery, and even medical applications. The headset's popularity and performance could pave the way for a future where humans interact with virtual overlays on the real world, leading to new ways of accessing information and potentially changing human behavior and brain function.

The discussion on Hacker News surrounding Apple's VR headset, the Vision Pro, has touched on various topics. 

One user, hhs, mentioned that researchers at Essen University Hospital in Germany are interested in using the headset's advanced eye-tracking technology to study conditions such as stroke or dementia. They believe that the high precision and quality sensor readings of the Vision Pro could be beneficial for medical tasks. Another user, smstv, responded with skepticism, stating their experience with multiple people who had rapid eye movements that didn't indicate intelligence. They also mentioned the case of GoPro, a successful company that fell victim to fraudulent activities. A subsequent discussion between smstv and strng revolves around fraudulent practices in the business world, with accusations of CEOs paying friends large sums of money. 
However, smstv also mentioned that high precision research tasks and cognitive activities, such as surgery, have been successfully coupled with other tools like the DaVinci system. They referenced hospitals like El Camino and Zuckerberg SFGH. 
Overall, the discussion covers a range of perspectives on the potential applications and limitations of the Vision Pro headset in research and healthcare settings, as well as some related concerns and experiences.

### Your AI Girlfriend Is a Data-Harvesting Horror Show

#### [Submission URL](https://gizmodo.com/your-ai-girlfriend-is-a-data-harvesting-horror-show-1851253284) | 143 points | by [nickthegreek](https://news.ycombinator.com/user?id=nickthegreek) | [189 comments](https://news.ycombinator.com/item?id=39370235)

A new study from Mozilla's Privacy Not Included project has found that AI romance chatbots, marketed as "romantic" companions, collect and share shockingly personal data with third parties. The study reviewed 11 different AI romance chatbots, including popular apps like Replika and CrushOn.AI, and found that all of them violated users' privacy in disturbing ways. The apps collect personal information such as sexual health details and medication use, and 90% of them sell or share user data for targeted advertising. Additionally, more than half of the chatbots do not allow users to delete the data they collect. Security was also a significant issue, with only one app meeting Mozilla's minimum security standards. The study also found that the AI girlfriend apps used an average of 2,663 trackers per minute, with one app calling a whopping 24,354 trackers in just one minute of usage. These apps also encourage users to share personal details that are far more intimate than those typically shared on other apps. The findings are particularly troubling considering the potential harm that can arise from sharing sensitive information with AI companions.

The discussion around the submission involves various topics related to AI girlfriends, AI chatbots, and relationships. Some users joke about not needing internet access for AI girlfriends and discuss dating websites and the evolution of technology in relationships. Others express interest in the concept of AI girlfriends running locally on GPUs and mention the potential for AI experiments or the influence of AI on real relationships. There are also discussions about Apple postponing the release of an AI girlfriend on Homepod, the use of Markov chains for word suggestion, and the comparison of AI girlfriends to video game characters. One user even shares their experience with AI girlfriend-like features in The Sims video game. Overall, the discussion combines humor, speculation, and personal anecdotes related to AI romance chatbots and AI companions.

---

## AI Submissions for Tue Feb 13 2024 {{ 'date': '2024-02-13T17:10:16.648Z' }}

### Memory and new controls for ChatGPT

#### [Submission URL](https://openai.com/blog/memory-and-new-controls-for-chatgpt) | 434 points | by [Josely](https://news.ycombinator.com/user?id=Josely) | [248 comments](https://news.ycombinator.com/item?id=39360724)

OpenAI is testing a new feature for ChatGPT that allows the AI to remember information from previous conversations, making future interactions more helpful. Users have control over ChatGPT's memory and can explicitly tell it to remember or forget certain things. They can also view and delete specific memories or clear all memories. This feature is being rolled out to a small group of free and Plus users for testing, with plans for a broader rollout in the future. In addition, OpenAI is also working on implementing memory for GPTs in general, allowing them to remember user preferences and tailor their responses accordingly.

The discussion on the submission revolves around the topic of lazy coding in ChatGPT and potential improvements that can be made. Some users argue that lazy coding leads to decreased coding quality, while others point out that lazy coding can be effective and efficient in certain situations. One user mentions the difficulty of filtering out irrelevant code in Rust parsing, while another user suggests using GitHub Copilot for generating code. There is also a discussion about GPT-4 Turbo's behavior and the need for refactoring tasks. Some users express concerns about OpenAI's censorship and the potential misuse of knowledge generated by AI models. Overall, the discussion highlights different perspectives on the efficiency and practicality of lazy coding and raises questions about responsible AI use and censorship.

### Nvidia's Chat with RTX is an AI chatbot that runs locally on your PC

#### [Submission URL](https://www.theverge.com/2024/2/13/24071645/nvidia-ai-chatbot-chat-with-rtx-tech-demo-hands-on) | 249 points | by [nickthegreek](https://news.ycombinator.com/user?id=nickthegreek) | [137 comments](https://news.ycombinator.com/item?id=39357900)

Nvidia has released an early version of Chat with RTX, an AI chatbot that runs locally on your PC. The app allows users to feed it YouTube videos and documents to create summaries and obtain relevant answers based on their own data. While the app is still in its early stages, it shows promise for data research purposes, particularly for journalists and individuals who work with large amounts of documents. Chat with RTX requires an NVIDIA RTX 30- or 40-series GPU with at least 8GB of VRAM. Users can search through video transcripts, summarize videos, and analyze local documents with the chatbot. While there are some bugs and limitations to be resolved, this AI chatbot demonstrates the potential of locally-run AI models on personal computers.

The discussion on the Hacker News submission revolves around the implementation and potential limitations of the Chat with RTX AI chatbot released by Nvidia. Here are the main points discussed:
1. Implementation: The chatbot is based on the TensorRT-LLM framework and requires an NVIDIA RTX 30- or 40-series GPU with at least 8GB of VRAM. Users can feed YouTube videos and documents to obtain summaries and relevant answers based on their own data. Some users have shared GitHub repositories and installation instructions for the chatbot.
2. Performance: Users have shared their experiences with the chatbot's performance. One user mentions that using it in conjunction with other AI models like Triton Inference Server has led to significant performance improvements. However, there are discussions about the limitations and complexities of running the chatbot, such as the requirement for large GPU memory.
3. Comparisons with Dr. Sbaitso: Some users bring up the resemblance of the chatbot to Dr. Sbaitso, a text-to-speech program from the 90s. They share anecdotes and nostalgic experiences related to the old program.
4. Local Language Models (LLMs): The discussion extends to the broader topic of locally-run language models. Users discuss the potential benefits and drawbacks of using LLMs, mentioning the need for technical knowledge and possible harmful effects of AI.
5. Simplified Solutions: Some users express confusion about the implementation details and suggest that simpler solutions should be made available. Others point out that various companies, such as OpenAI, Microsoft, and Google, are working on similar projects, and the complexity is due to the technical nature of the topic.

Overall, the discussion highlights both the potential of locally-run AI models for data research purposes and the complexities associated with their implementation and usage.

### Smart terminals: Personal computing’s true origin? (2023)

#### [Submission URL](https://thehistoryofhowweplay.wordpress.com/2023/10/23/smart-terminals-personal-computings-true-origin/) | 65 points | by [rbanffy](https://news.ycombinator.com/user?id=rbanffy) | [14 comments](https://news.ycombinator.com/item?id=39359307)

Today on Hacker News, we have an interesting article from the historyofhowweplay blog about the evolution of computer monitors. The author delves into the significance of terminals as they played a crucial role in the development of computer technology. Initially starting as text-based teletypes, terminals became popular in the 1950s and made programming much easier. The adoption of the ASCII computer text standard and the widespread use of timesharing with the Dartmouth model led to the demand for CRT-based terminals and the birth of the computer monitor industry. The article also mentions the development of graphical terminals for applications like Computer Aided Design (CAD) and the various experiments conducted to explore the possibilities of raster and vector graphics displays. Eventually, designers began to conceive of terminals with their own "brain" to bypass the need for constant computer interaction and unstable connectivity. The article points out the importance of the miniaturization of computer components in driving the development of standalone terminal systems. Overall, it's an intriguing read that delves into the forgotten history of computer monitors and their impact on personal computing.

The discussion on the article about the evolution of computer monitors goes in several directions. 
One user, ChuckMcM, points out the shift from centralized computing to distributed computing and how terminals played a role in each stage. They mention the transition from mainframe-based terminals to minicomputers with multiple servers, and then to personal computers that combined terminals and computing power. They also mention thin servers, Citrix-style clients, and web browsers as alternative ways to implement terminal-like functionality.
Another user, rbnffy, talks about alternative paths in computing, mentioning DEC's Gigi and other terminal models like Tek ReGIS. They also discuss how personal computers and modems worked similarly to terminals in the past.
The discussion then shifts to the similarities between smart phones and modern terminals. rbnffy notes that smart phones are essentially running local software that acts as a terminal to modern web browsers.
kjs3 weighs in by discussing specific terminal models like ATT 3b2 and BLIT, highlighting their features and capabilities. They also mention the existence of a similar but different terminal called BitGraph, as well as the Tektronix 4100 and 4200 series, which were impressive machines for graphics work.
Animats brings up the history of shared-logic word processors and dumb terminals in the context of IBM PCs and monitors.
SomeoneFromCA mentions that the Apple 1 was a representative transitional product from the era of dumb terminals to intelligent computers.
rbnffy adds that the sophistication of terminals did not help small computers running dedicated terminal firmware, as they were eventually overtaken by cheaper desktop computers running terminal software.

Finally, a user named aaron695 makes a one-word comment: "dd," which is unclear in its context.

### Mozilla downsizes as it refocuses on Firefox and AI

#### [Submission URL](https://techcrunch.com/2024/02/13/mozilla-downsizes-as-it-refocuses-on-firefox-and-ai-read-the-memo/) | 176 points | by [awkwardpotato](https://news.ycombinator.com/user?id=awkwardpotato) | [165 comments](https://news.ycombinator.com/item?id=39362481)

Mozilla, the organization behind the Firefox browser, is undergoing major changes to its product strategy. It plans to scale back investment in various products, including its VPN, Relay, and Online Footprint Scrubber, and shut down Hubs, its 3D virtual world launched in 2018. Approximately 60 employees will be affected by the layoffs. Mozilla aims to refocus on Firefox and bring "trustworthy AI into Firefox." The company will bring together teams working on Pocket, Content, and AI/ML to achieve this goal. These changes suggest a shift towards prioritizing Firefox and addressing criticism of diversifying its product portfolio.

The discussion on this submission revolves around various aspects of Mozilla's product strategy changes. Here are some key points:

- Some users are expressing skepticism about the impact of Mozilla's refocus on Firefox and its plans to bring trustworthy AI into the browser.
- There is a discussion on the market share of Firefox, particularly in relation to Chrome and Safari. It is pointed out that while Firefox's market share is relatively low in desktop Linux, it has a stronger presence in Germany.
- Users debate the significance of Mozilla's decision to scale back investment in products such as VPN and Relay, with some suggesting that these projects were not generating enough profit.
- The potential implications of Mozilla's reliance on Google for revenue, particularly from search deals, are discussed, with some expressing concern about the impact on the company's independence.
- There is also a mention of the financial challenges faced by Mozilla and the need for the organization to prioritize revenue-generating projects.
- Some users express support for Mozilla's VPN and mention alternatives such as Mullvad.
- The discussion touches on the importance of open-source alternatives and the potential for Mozilla to collaborate with projects like LibreWolf.

### Court Dismisses Authors' Copyright Infringement Claims Against OpenAI

#### [Submission URL](https://torrentfreak.com/court-dismisses-authors-copyright-infringement-claims-against-openai-240213/) | 35 points | by [isaacfrond](https://news.ycombinator.com/user?id=isaacfrond) | [9 comments](https://news.ycombinator.com/item?id=39357210)

In a recent court case, OpenAI successfully had the copyright infringement claims filed against them by two authors dismissed. The authors, Paul Tremblay and Mona Awad, accused OpenAI of using their books without permission or compensation to train its AI models. OpenAI argued that using books to train AI does not constitute copyright infringement, and the court largely agreed. The court also dismissed claims that OpenAI violated the Digital Millennium Copyright Act (DMCA) by altering copyright management information. However, the authors still have the opportunity to file an amended complaint, and the direct copyright infringement claim against OpenAI will proceed. Many other AI copyright lawsuits are ongoing.

The discussion on Hacker News revolves around various aspects of the court case between OpenAI and authors Paul Tremblay and Mona Awad. Some key points discussed include:
- One user mentions that OpenAI's defense likely focused on the argument that using books to train AI models does not constitute copyright infringement. Another user adds that OpenAI's successful defense could set a precedent for similar cases in the future.
- There is a comment expressing confusion about the judge's decision, stating that the court case is still in its early stages and the legal battle is just beginning.
- A user highlights that the article's title is misleading, as it suggests the claims were dismissed entirely, while in reality, only specific claims were dismissed. They mention that it would be interesting to see specific examples of alleged copyright infringement.
- Another user brings up an interesting point, stating that ChatGPT accurately reproduces the impression of paragraphs from articles, making it appear as though people can read NYT content without actually engaging with it fully. They argue that this misrepresentation could harm the reputation of the NYT and potentially lead to legal issues.
- One user comments that this is just the beginning and the court case will likely have significant implications for copyright law. They mention that it is a good start to see the dominance of the major tech companies being challenged.
- A user suggests that the discontinuation of using NYT training data is a positive development, as it emphasizes the importance of respecting copyrights and benefits the rights holders.
- In response, another user points out the irony that OpenAI, which is associated with extremely wealthy individuals, is being taken to court by other wealthy individuals, highlighting the power dynamics at play.
- A discussion emerges about the nature of legal claims being broadly made and then eventually dropped or dismissed in many cases involving wealthy individuals.

Overall, the discussion showcases different perspectives on the court case, with additional insights raised about copyright infringement, the impact on media organizations, and the involvement of wealthy individuals.

### GPT inputs outgrow world chip and electricity capacity

#### [Submission URL](https://www.astralcodexten.com/p/sam-altman-wants-7-trillion) | 19 points | by [quirkot](https://news.ycombinator.com/user?id=quirkot) | [13 comments](https://news.ycombinator.com/item?id=39358693)

In a recent post on Astral Codex Ten, the author discusses Sam Altman's desire for $7 trillion and why it's a significant reminder of the challenges that AI will face in scaling up. The post breaks down the cost of training AI into three main components: compute, electricity, and training data. With each new generation of AI models, the cost increases significantly. For example, GPT-4 is estimated to have cost $100 million, and GPT-5 is rumored to have cost $2.5 billion. The author speculates that GPT-6 could cost $75 billion and GPT-7 a whopping $2 trillion. 

The challenge lies in the resources required to train these models. In terms of compute, GPT-4 took about six months using 1/2000th of all the computers in the world. Scaling this up, GPT-7 would require 15 times as many computers as currently exist. As for electricity, GPT-7 would need the output of fifteen Three Gorges Dams, the largest power plant in the world. Training data presents another challenge, as the amount of text available is limited. While synthetic data can be used to generate more training data, it's not yet clear how effective this approach will be for written text.

Overall, Altman's $7 trillion aspiration serves as a reminder of the immense resources required to scale AI models and the challenges that lie ahead. It highlights the need for advancements in compute power, energy production, and training data generation techniques.

The discussion on this post revolves around several points. 

- One commenter criticizes the article, pointing out that its title and substance misrepresent the actual growth rate of AI models. They clarify that the growth is not exponential but rather increases by factors of 100x, 25x, and 25x respectively for each generation.
- Another commenter expresses their wish for AMD to quickly implement transparent methodologies for GPU computing, specifically mentioning the LLVM compiler infrastructure.
- One commenter argues that previous predictions about the hard drive storage capacity being quickly exceeded were proven wrong when it comes to DNA sequencing. They suggest that people are not fully considering the scaling limitations of language models being discussed.
- A commenter highlights an interesting point from the article, specifically that GPT-4 consumed 50 gigawatt-hours of energy during training, and with a scaling factor of 30x, they estimate that GPT-5 would require 1,500 gigawatt-hours, GPT-6 would require 45,000 gigawatt-hours, and GPT-7 would require 13 million gigawatt-hours.
- A comment thread discusses the assumption that OpenAI's training is exclusively powered by Microsoft's low carbon energy. One commenter argues that there are limitations to renewable energy production and maintenance and questions Microsoft's commitment to a 100% renewable energy world.
- Another commenter in the thread suggests that Microsoft is hiring nuclear scientists, possibly to power their AI initiatives, and they criticize Microsoft for promoting green energy while relying on dirty energy elsewhere.
- The discussion then moves towards the dynamics of power purchase agreements (PPAs), renewable energy credits (RECs), and the translation of lowering electricity production CO2 footprints. The comment highlights competitive wind and solar power production in certain locations and carbon cap-and-trade systems to prevent increased power demand relying on fossil energy.
- Another commenter supports the claim made in the previous comment and shares links to independent research that corroborates it.
- Lastly, a commenter mentions that slower training generations of large language models (LLMs) can select from a wider range of low carbon energy sources without compromising their training efficiency, regardless of the energy source mix in the market for electricity generation.
- Another commenter suggests that companies like Microsoft are likely making renewable energy commitments regardless of cost. They share links to articles discussing Microsoft's renewable energy pledges.

### Dude, where's my self-driving car?

#### [Submission URL](https://www.theverge.com/24065447/self-driving-car-autonomous-tesla-gm-baidu) | 18 points | by [mfiguiere](https://news.ycombinator.com/user?id=mfiguiere) | [32 comments](https://news.ycombinator.com/item?id=39357882)

In 2015, Google's self-driving car project leader Chris Urmson predicted that self-driving cars would be so widespread by 2020 that his then-11-year-old son would never need a driver's license. Fast forward to 2024, and autonomous vehicles are still far from being a common sight on the roads. Over the years, there have been numerous missed deadlines and overly optimistic predictions about the availability of self-driving cars. Companies like Baidu, Lyft, GM, and Ford all promised to have autonomous vehicles on the market by certain dates, but those deadlines have come and gone without fruition. Even Tesla CEO Elon Musk, known for his bold claims, has made inaccurate predictions about the readiness of autonomous vehicles. Despite some autonomous cars currently operating in select cities, they are confined to geofenced service areas and face technological limitations, opposition from labor unions, and restrictions on certain roads and weather conditions. The hype surrounding autonomous vehicles has allowed companies to secure funding for their experiments, while regulators have taken a relatively lenient stance on self-driving car testing. However, the industry's failure to deliver on its promises raises questions about the underlying reasons for the delay. One of the primary motivations behind the optimistic predictions was financial gain, as companies were able to secure funding based on the promise of an imminent autonomous future. This flow of money also influenced regulators to adopt a more permissive approach to self-driving car testing. Companies operating in the autonomous vehicle space have raised billions of dollars through traditional fundraising channels and partnerships with big tech and car companies. While some progress has been made, the widespread availability of self-driving cars still remains a distant reality.  

The discussion on this submission revolves around various aspects of self-driving cars. One user shares their experience with adaptive cruise control in a 2022 Kia Telluride, mentioning its limitations and the need for constant attention while driving. Another user expresses their skepticism about relying on self-driving technology, citing personal experiences of confusion and dangerous situations with Tesla's Full Self-Driving (FSD) feature. The discussion then moves to a debate about the safety and reliability of autonomous driving systems compared to human drivers. Some users argue that self-driving technology is not yet capable of completely replacing human drivers, while others defend Tesla's capabilities and criticize non-Tesla manufacturers. There is also a discussion about the societal impact of self-driving cars, with one user mentioning the convenience of having a personal chauffeur for long trips and another user highlighting the hassle of public transportation for weekend ski trips. The conversation concludes with a user expressing their disinterest in purchasing a Level 4+ autonomous vehicle due to the limited budgets and high costs associated with self-driving options.