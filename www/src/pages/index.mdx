import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Mon Dec 04 2023 {{ 'date': '2023-12-04T17:10:07.519Z' }}

### Towards accurate differential diagnosis with large language models

#### [Submission URL](https://arxiv.org/abs/2312.00164) | 96 points | by [gwintrob](https://news.ycombinator.com/user?id=gwintrob) | [75 comments](https://news.ycombinator.com/item?id=38524595)

Researchers at arXiv have published a paper titled "Towards Accurate Differential Diagnosis with Large Language Models" by Daniel McDuff and his team. The paper introduces an optimized Large Language Model (LLM) for diagnostic reasoning and evaluates its effectiveness in generating a differential diagnosis (DDx) for medical cases. The LLM outperformed unassisted clinicians in terms of accuracy and produced higher-quality DDx compared to clinicians assisted by search engines and standard medical resources. The study suggests that LLMs have the potential to improve clinicians' diagnostic reasoning and accuracy in challenging cases, thus empowering physicians and increasing patients' access to specialist-level expertise. Further real-world evaluations are needed to confirm these findings.

The discussion on this submission revolves around various experiences and perspectives related to the use of AI in medical diagnosis, as well as criticisms towards the medical system.
One commenter expresses frustration with doctors not considering certain tests and relying on their own judgments instead. They share personal anecdotes of doctors dismissing their concerns and not investigating further. Another commenter argues that doctors have limited time and resources and implies that AI could potentially help improve their decision-making process.
Another discussion revolves around the challenges in creating medical guidelines and the importance of clinical experience. One commenter emphasizes the need for doctors to have practical experience and argues that AI should not replace them entirely. They also point out the difficulty of creating guidelines that fit every scenario.
There is also a debate about the role of doctors and the healthcare system. One commenter expresses dissatisfaction with the current system, stating that doctors prioritize monetary gain over patient care. They hope that AI can help improve the situation. Another commenter argues that insurance and the cost of medical tools are major problems and that the current system needs to change.

Overall, the discussion highlights a range of opinions on the role of AI in medical diagnosis, the challenges faced by doctors, and criticisms of the medical system.

### Joint initiative for trustworthy AI

#### [Submission URL](https://actu.epfl.ch/news/joint-initiative-for-trustworthy-ai/) | 63 points | by [huhtenberg](https://news.ycombinator.com/user?id=huhtenberg) | [20 comments](https://news.ycombinator.com/item?id=38523736)

ETH Zurich and EPFL have announced the launch of the "Swiss AI Initiative", a joint effort to position Switzerland as a global leader in transparent and reliable artificial intelligence (AI). The initiative will leverage the power of the new Alps supercomputer, equipped with 10,000 graphics processing units (GPUs), to provide Swiss scientists with access to cutting-edge computing power for AI applications. The aim of the project is to develop and train large language models (LLMs) that are transparent and open source, ensuring comprehensible and ethical results. The initiative will also foster collaboration among science, industry, and politics to drive the development and use of AI in Switzerland, with a focus on industry-specific applications in sectors such as robotics, medicine, climate sciences, and diagnostics.

In the discussion section of this submission about the Swiss AI Initiative, there are several different conversations taking place. 
One commenter points out that the government computing project with 10,000 GPUs is impressive hardware but may not provide a competitive advantage against offerings from Google and AWS, who are providing double computing power in a single data center. Another commenter responds that it's still a significant development and mentions that it's being led by EPFL and ETH Zurich.
Another conversation revolves around the trustworthiness of AI and the need for legal and ethical considerations. One commenter mentions the importance of reliability and validity in evaluating AI models and brings up the intersection of math, philosophy, and law. This leads to a discussion about the complexity of AI and how some people oversimplify it or misunderstand it from a technical perspective.
There is also a conversation about the philosophical implications of AI, with one commenter mentioning metaphysics, ethics, and the intersection with formal logic and German-Austrian mathematics. Another commenter argues that AI is ultimately grounded in mathematics and that things like trust and ethics can be applied to AI using mathematical models.
One commenter brings up the topic of safety in AI and how it should be handled in the real world. They discuss legal and monetary incentives for ensuring safety and mention that AI systems can make mistakes and should be held accountable.
Another commenter points out that discussions about large language models (LLMs) often generate a lot of words but lack substance, emphasizing the importance of non-linguistic aspects of AI.
In a separate conversation, one commenter argues that humans are the creators of AI, and discussions about the topic should adhere to certain philosophical demands. They mention that professionals design the systems based on legal, ethical, and scientific criteria.
There is also some discussion about the implications of AI in various industries. One commenter suggests that AI models and responses need to consider legal implications, while another mentions the importance of security and execution in business processes.

Lastly, there are comments flagged that discuss unrelated topics such as nationalistic filmmaking, banking secrecy in Switzerland, and democratic traditions in the country.

### AI and Trust

#### [Submission URL](https://www.schneier.com/blog/archives/2023/12/ai-and-trust.html) | 196 points | by [CTOSian](https://news.ycombinator.com/user?id=CTOSian) | [87 comments](https://news.ycombinator.com/item?id=38516965)

The author of this blog post explores the concept of trust in society and how it relates to artificial intelligence (AI). They argue that there are two types of trust: interpersonal trust (based on personal connections and intentions) and social trust (reliability and predictability). They believe that AI confuses these two types of trust, leading people to see AI as friends rather than just services. They also highlight concerns that corporations controlling AI may take advantage of this confusion. The author argues that it is the role of the government to create trust in society, including regulating the organizations that control and use AI. They conclude by emphasizing the importance of social trust in scaling society and the need to consider biases embedded in trust systems.

The discussion on this submission covers a range of topics related to trust in AI and the regulation of AI systems.

One user points out that AI controlled by corporations can lead to profit-maximizing behavior, potentially compromising trust. They highlight concerns about surveillance and manipulation business models, emphasizing the need for greater regulation. Another user agrees and suggests that smaller, open-source models can provide alternatives to large corporate-controlled AI systems.
The discussion also touches on the challenges of implementing private, locally installed models. Some users argue that it can be complicated to set up, requiring support from web browsers and operating systems. Others suggest that most people install apps without much difficulty.
One user highlights the importance of implementing technical and protocol approaches to ensure trustworthy AI. They argue for the use of local models and note the need for clear instructions and constraints in training data to avoid unintended biases.
The topic of regulating AI is also brought up. Some users argue that attempts at regulation should not focus solely on AI but should also address surveillance and manipulation, as these are existing issues in society. Another user brings up Promise Theory, which explores the notions of promises, obligations, and trust in systems.
The discussion veers into the realm of corporate control and the potential harm that can result from AI proxying human agents. The analogy of the music industry and contracts is used to illustrate the problems that can arise from proxying human decision-making to AI systems.
One user draws parallels between the arguments made in this discussion and the software movement's focus on trustworthiness and responsibility. They argue that trustworthiness should apply to non-AI software as well, highlighting the need for accountability and transparency in corporate-controlled software systems.
Overall, the discussion emphasizes the importance of trust, transparency, and responsible use of AI systems, and the need for regulatory measures to ensure these principles are upheld.

### IBM releases first-ever 1k-qubit quantum chip

#### [Submission URL](https://www.nature.com/articles/d41586-023-03854-1) | 38 points | by [birriel](https://news.ycombinator.com/user?id=birriel) | [4 comments](https://news.ycombinator.com/item?id=38520757)

IBM has announced the development of the first quantum computer with over 1,000 qubits – the quantum equivalent of digital bits in a classical computer. However, instead of continuing to increase the number of qubits in its machines, IBM will now focus on improving error resistance. Quantum states are notoriously prone to errors, and IBM plans to address this issue by using error-correction techniques that would require fewer physical qubits. The company aims to build chips designed to hold a few error-corrected qubits in about 400 physical qubits and network them together. This approach could reduce the number of physical qubits needed for useful computations by a factor of 10 or more. IBM's road map for its quantum research envisions achieving useful computations within the next decade.

The discussion on this submission primarily revolves around the topic of logical qubits and error-correction techniques in quantum computing. One user, vlovich123, points out that the reported numbers of qubits in quantum computers may refer to logical qubits rather than physical qubits. They mention that research generally indicates that error-correction techniques require around 1000 physical qubits for each logical qubit, and express excitement about an alternative error-correction scheme called quantum low-density parity-check (qLDPC) that could reduce this ratio by a factor of 10.

Another user, d-bcn, argues that 10 logical qubits defined on just one physical qubit is a reasonable definition of a logical qubit and highlights Google's work on quantum error correction. They discuss experiments that demonstrate fault-tolerant quantum error correction using 13 physical qubits and note that other groups have also achieved small-scale logical qubits with improved performance.

In response to vlovich123's earlier comment, d-bcn mentions that they have likely managed to create a logical qubit, pointing to experimental evidence from Egan and Google. They provide links to two experiments that showed small improvements in logical performance, although the improvements were relatively small in scale.

In conclusion, the discussion involves a technical debate about the definition and implementation of logical qubits, as well as the advancements and challenges in error-correction techniques for quantum computing.

### Show HN: HelpMoji – Build Co-pilots for your favorite Games

#### [Submission URL](https://helpmoji.com/) | 8 points | by [hienyimba](https://news.ycombinator.com/user?id=hienyimba) | [3 comments](https://news.ycombinator.com/item?id=38517221)

Introducing Helpmoji, the ultimate companion for gamers! With Helpmoji, you can access a wide range of friendly NPC helpers designed to assist and guide you through your favorite games. Whether you need personalized guidance, real-time assistance, or helpful walkthroughs, Helpmoji has got you covered. Plus, it's completely free!

So, how does Helpmoji work? It's simple! Just follow these three steps to create your very own game helper:

1. Choose your game: Input the name of the game you're playing, and Helpmoji will scour the web to find the best knowledge base and wiki sources for your co-pilot.
2. Select training data: Browse through the cool resources Helpmoji has found, and feel free to add your own if you have any.
3. Train your co-pilot: Helpmoji will ingest the chosen resources and train your co-pilot in the background. In just three minutes, your co-pilot will be ready to assist you to victory!

And that's not all! As a member of Helpmoji's community, you'll gain access to over 1000 pre-made helpers created by other members. From Genshin Impact to Among Us, Fortnite to Grand Theft Auto V, you'll find helpers for a wide range of popular games. It's like having a personal gaming assistant at your fingertips!

But what sets Helpmoji apart is its versatility. This tool is multilingual, meaning you can train your co-pilot in one language and chat in 30 different languages. Connect with gamers from around the world and level up your gameplay together!

Ready to boost your gaming experience? Create a game helper for your favorite game and gain free access to helpers crafted by the Helpmoji community. Get started now and let Helpmoji be your co-pilot to victory!

The discussion surrounding the submission revolves around the terminology used in the description of the product. One user comments that the term "NPC" (non-playable character) is commonly used in gaming and finds it humorous that it is being used to describe the game helpers in Helpmoji. Another user suggests that the inclusion of terms like "personalized walkthrough helper" would make it easier for people to understand the purpose of the product.

### Show HN: AIConsole, an Open-Source Desktop AI Editor to Customize Your Workflow

#### [Submission URL](https://aiconsole.ai) | 52 points | by [mcielecki](https://news.ycombinator.com/user?id=mcielecki) | [24 comments](https://news.ycombinator.com/item?id=38517060)

Introducing AIConsole, an open-source desktop AI editor designed to enhance your workflow. With its powerful features, this tool allows you to run code locally and perform all the tasks that you can on your machine. But that's just the beginning.

One of the standout features of AIConsole is its ability to learn from your input. Simply describe how to perform a task in plain text once, and AIConsole will remember it forever. This means that the more you use it, the better it gets at understanding and automating your tasks.

You can also leverage your own notes to teach AIConsole. By using your existing knowledge, you can train the AI to complete and automate even more complex tasks. It's like having a personal assistant that learns from your own expertise.

AIConsole also excels in expert prompt engineering. For every step in your tasks, you can expect a precise, efficient, and automatic multi-agent RAG system. The level of prompt engineering provided by AIConsole is on par with expert standards, ensuring that you have the most effective tools at your disposal.

One of the greatest perks of AIConsole is its commitment to being fully open-sourced. You can rest assured that no data is being sent to anyone other than the LLM APIs, and you can verify this for yourself. Additionally, AIConsole encourages you to build and share your domain-specific AI tools with the community. Whether it's on platforms like GitHub or Discord, you have the opportunity to contribute to the growth of this powerful tool.

If you're eager to give AIConsole a try, you can download it right away. But if you require a customized, hosted, or enterprise version, the AIConsole team is ready to develop it according to your needs.

Embrace the future of productivity with AIConsole and revolutionize your workflow.

The discussion surrounding the submission "Introducing AIConsole, an open-source desktop AI editor designed to enhance your workflow" on Hacker News revolves around various aspects of the tool.

One user, android521, encountered an issue with downloading the MacIntel MaxM2 Edit and mentioned that they received an "Invalid Open AI API key" error while trying to access the playground chat. Another user, mchlkljsz, suggested that the problem might be due to the requirement of the GPT-4 API for proper functioning.
The conversation then shifted towards the capabilities of AIConsole, with mchlkljsz noting that it could be useful for quickly generating specific tailored prompt tasks. Another user, smcld, mentioned that AIConsole has the potential for integration with Ollama, an RNA model server.
Odene expressed curiosity about using AIConsole to create prompts for Google Analytics 4 and generate dashboards. Other users, mritchie712 and mclck, discussed the limitations of various APIs such as Stripe, Shopify, and Hubspot in handling the collection of data and recommended AI-powered data assistants as a potential solution.
Krlrkssn chimed in to express interest in sharing tools created using AIConsole, to which mclck responded with the word "Effects."
Some users, including ylwht and glbrtk, praised the capabilities of AIConsole and its cross-platform nature. The conversation briefly referenced CodeLama and the release schedule, with psychctv mentioning "Lama 70b" and mchlkljsz noting the potential long-term benefits of LLMs (Large Language Models) in a local environment.
Xchn congratulated the AIConsole team on the launch and expressed amazement at its cross-platform functionality. Other users, such as lbns and lvnd, expressed their positive impressions and potential use cases for AIConsole. The discussion concluded with users KodakKojak and dwc10 simply stating that it looks impressive.

### OpenAI COO thinks AI for business is overhyped

#### [Submission URL](https://www.theverge.com/2023/12/4/23988019/openai-enterprise-hype-chatgpt-lightcap) | 23 points | by [mfiguiere](https://news.ycombinator.com/user?id=mfiguiere) | [9 comments](https://news.ycombinator.com/item?id=38522314)

OpenAI's chief operating officer, Brad Lightcap, has cautioned against the overhyping of AI for business transformation. In an interview with CNBC, Lightcap acknowledged that companies often expect AI to deliver significant changes instantly, but he emphasized that AI is still in its infancy and there is no one-size-fits-all solution. While AI has the potential to improve businesses, Lightcap explained that it is currently in the experimental phase and has yet to become an integral part of critical tools and applications. OpenAI has recently launched an enterprise version of its ChatGPT platform, which offers better data protection and more customization options. However, some early adopters of AI have faced challenges, including Morgan Stanley, whose chatbot built with OpenAI was reportedly underutilized by users. The article also highlights instances where AI-generated content has received backlash for being insensitive or inaccurate.

The discussion on this submission touches on a few different points:

- One user suggests that human loop verification is necessary to ensure the accuracy of AI-generated content. They argue that AI should not be relied upon to generate insights without human oversight.
- Another user shares their experience of using AI in their former company, where AI was used to search and classify documents. They emphasize the usefulness of AI in synthesizing information and identifying conflicting data.
- The discussion also touches upon the potential impact of AI on jobs. One user argues that AI-led automation may lead to job losses, while another user counters that innovation and technological advancements tend to create new job opportunities.
- One user points out the limitations of chatbot applications and suggests that the industry has not yet fully transformed or seen significant advancements in underlying technology.
- Another user highlights the importance of responsible management and executive involvement in understanding the capabilities and limitations of AI. They argue that proper research and testing are necessary to set appropriate expectations for AI projects and align them with business goals.

Overall, the discussion reflects varying perspectives on the current state and future potential of AI, as well as the challenges and considerations that come with its adoption in business settings.

---

## AI Submissions for Sun Dec 03 2023 {{ 'date': '2023-12-03T17:10:56.866Z' }}

### Stuxnet Source Code

#### [Submission URL](https://github.com/research-virus/stuxnet) | 158 points | by [Jimmc414](https://news.ycombinator.com/user?id=Jimmc414) | [106 comments](https://news.ycombinator.com/item?id=38511563)

The top story on Hacker News today is the public release of the Stuxnet malware's open-source code. Stuxnet, also known as MyRTUs, is a notorious piece of malware that was discovered in 2010 and is believed to have been developed as a cyberweapon to target Iran's nuclear program. The code provided in this repository was extracted from Stuxnet binaries and is now being made available for analysis and research purposes. The repository contains not only the Stuxnet code but also a rootkit source code. The authors of the code, Christian Roggia and Amr Thabet, have copyrighted their work, but they have made it available for free and have only asked for recognition and credit for their hard work. This release of Stuxnet's code is expected to provide valuable insights into the workings of the malware and help researchers better understand its capabilities and implications.

The discussion on the release of Stuxnet's open-source code touches on various topics. One user points out that the code contains interesting things related to the development of the malware. Another user discusses the involvement of Israel in the Stuxnet attack on Iran's nuclear program, while another user mentions a joint effort between the US and Israel. The conversation then veers towards the potential risks of mobile devices and the combination of GPS, audio, and video surveillance. There is a mention of a strategy involving tailored cyber warfare and the possibility of self-delivering non-nuclear weapons. The conversation also touches on genetic modifications and testing protocols, French nuclear weapons programs, and the relevance of a Darknet Diaries episode on the topic. The discussion dives into technical aspects of Stuxnet, such as how it did not break the working regime of the control systems and the difficulty of recovering source code. Users discuss obfuscating code, the possibility of decrypting binaries, and the availability of similar techniques since the 1980s. Overall, the discussion covers a wide range of topics related to Stuxnet and cybersecurity.

### Watsonx: IBM's code assistant for turning COBOL into Java

#### [Submission URL](https://www.pcmag.com/articles/ibms-plan-to-update-cobol-with-watson) | 116 points | by [Anon84](https://news.ycombinator.com/user?id=Anon84) | [174 comments](https://news.ycombinator.com/item?id=38508250)

IBM is developing an AI-powered code assistant called WatsonX to modernize COBOL, a programming language widely used in industries such as banking, insurance, and healthcare. COBOL, which handles $3 trillion worth of transactions each day, is becoming increasingly difficult to maintain as programmers with expertise in the language retire. WatsonX aims to save coders time by converting COBOL code into a more modern language. The process involves breaking down the application into smaller pieces and selectively choosing which parts to modernize. However, skeptics have raised concerns about IBM's previous AI project, Watson Health, which failed to deliver on its promises. While WatsonX is still in its early stages, IBM remains optimistic about its potential.

The discussion on the submission covers various perspectives on IBM's AI-powered code assistant, WatsonX, and the challenges of modernizing COBOL.

- Some users express skepticism about IBM's track record with previous AI projects like Watson Health, raising concerns about the success of WatsonX.
- Others argue that the management problem in AI projects can hinder their effectiveness, as AI may not be able to fix management issues.
- One user mentions that the demand for COBOL programmers is high, leading to higher salaries for contractors in the insurance industry.
- There is a discussion about the difficulty of migrating COBOL systems to modern languages, with some users suggesting that rewriting the code from scratch is not feasible due to the complexity and compatibility issues.
- The performance implications and technical challenges of converting COBOL to Java are also discussed.
- Some users point out that COBOL has specific features, such as global variables, that make it challenging to convert to Java.
- Additionally, there is a debate about the motivations and limitations of rewriting COBOL code, with some users suggesting that it is more practical to maintain and modernize legacy systems rather than rewriting them entirely.
- The discussion also touches on the topic of technical debt and the risks and benefits of adopting new technologies versus maintaining existing systems.

Overall, the conversation covers various perspectives on the challenges of modernizing COBOL and the potential efficacy of IBM's WatsonX in addressing these challenges.

### 1960s chatbot ELIZA beat OpenAI's GPT-3.5 in a recent Turing test study

#### [Submission URL](https://arstechnica.com/information-technology/2023/12/real-humans-appeared-human-63-of-the-time-in-recent-turing-test-ai-study/) | 132 points | by [layer8](https://news.ycombinator.com/user?id=layer8) | [44 comments](https://news.ycombinator.com/item?id=38506175)

A preprint research paper titled "Does GPT-4 Pass the Turing Test?" conducted by researchers from UC San Diego compares OpenAI's GPT-4 AI language model to human participants, GPT-3.5, and the 1960s computer program called ELIZA. The study found that human participants were only able to correctly identify other humans in 63% of interactions, and that ELIZA outperformed the GPT-3.5 AI model. However, GPT-4 achieved a success rate of 41%, second only to actual humans. The study raises questions about using the Turing test to evaluate AI model performance and the limitations of current AI models.

In the discussion, there are different viewpoints regarding the research paper and the Turing test. Some participants argue that the Turing test is flawed and that there are better ways to evaluate AI models. They point out that the study shows the limitations of current AI models and raises questions about their performance compared to humans. Others discuss the historical significance of ELIZA and its comparison to modern AI models. Some participants also discuss the practical applications of AI and the importance of human oversight in customer support. The discussion touches on topics such as the nature of consciousness and the definition of intelligence. Overall, there is a mix of opinions and perspectives in the conversation.

### GPT-4 Can Almost Perfectly Handle Unnatural Scrambled Text

#### [Submission URL](https://arxiv.org/abs/2311.18805) | 199 points | by [saliagato](https://news.ycombinator.com/user?id=saliagato) | [137 comments](https://news.ycombinator.com/item?id=38506140)

In a recent study, researchers have found that GPT-4, a large language model, can almost perfectly handle and correct unnatural scrambled text. The researchers designed a suite called the Scrambled Bench to measure the capacity of language models to handle scrambled input. The experimental results showed that GPT-4 was able to reconstruct the original sentences from scrambled ones with an impressive 95% reduction in edit distance, even when all letters within each word were scrambled. This resilience displayed by GPT-4 is counter-intuitive, considering the severe disruption to input tokenization caused by scrambled text. The findings provide novel insights into the inner workings of large language models and their ability to handle unconventional textual input.

The discussion on this submission revolves around the ability of GPT-4, a large language model, to handle scrambled text and its implications. Some users express their surprise at GPT-4's capability to reconstruct original sentences from scrambled ones with a 95% reduction in edit distance. Others discuss the challenges in word segmentation and the use of backtracking. The conversation also touches on the limitations and imperfections of GPT-4 and the role of tokenization in language models. Some users experiment with feeding scrambled text into search engines and observe different results. The discussion concludes with users discussing alternative models and their success in similar tasks.

### Mozilla Lets Folks Turn AI LLMs into Single-File Executables

#### [Submission URL](https://hackaday.com/2023/12/02/mozilla-lets-folks-turn-ai-llms-into-single-file-executables/) | 69 points | by [anonymousiam](https://news.ycombinator.com/user?id=anonymousiam) | [3 comments](https://news.ycombinator.com/item?id=38503588)

Mozilla's innovation group has released an open-source method called "llamafile" to turn a set of weights into a single binary file, making it easier to distribute and run Large Language Models (LLMs). Llamafile supports six different operating systems and ensures that a particular version of an LLM remains consistent and reproducible. It uses the "Cosmopolitan" build-once-run-anywhere framework created by Justine Tunney, along with the llama.cpp tool. Sample binaries using different LLMs are available, with the LLaVA 1.5 LLM being the only one that can run on Windows due to the 4 GB limit on executable files.

The discussion about the submission revolves around the technical aspects of llamafile and how it can be beneficial in distributing and running Large Language Models (LLMs). Some users mention that distributing LLMs as multiple files can be challenging to distribute, as changes and tweaks to the software can lead to different results with different versions. One user mentions that llamafile supports multiple operating systems including macOS, Windows, Linux, FreeBSD, OpenBSD, and NetBSD. They also point out that using llamafile dramatically simplifies the distribution process of LLMs, ensuring that a specific version of an LLM remains consistent and reproducible indefinitely. Another user notes that llamafile relies on the Cosmopolitan build-once-run-anywhere framework developed by Justine Tunney, specifically llama.cpp. This framework is commended for its efficiency in running self-hosted LLMs. In the comments, a link to a related discussion is shared. It is not clear what the specific topic of that discussion is.

Finally, one user simply mentions "ppl" (presumably referring to people) and "llmcpp" without further clarification, leaving their comment's meaning open to interpretation.

### Run 70B LLM Inference on a Single 4GB GPU with This New Technique

#### [Submission URL](https://ai.gopubby.com/unbelievable-run-70b-llm-inference-on-a-single-4gb-gpu-with-this-new-technique-93e2057c7eeb?gi=cbe7920f4cd2) | 108 points | by [gardenfelder](https://news.ycombinator.com/user?id=gardenfelder) | [56 comments](https://news.ycombinator.com/item?id=38508571)

Have you ever wondered if it's possible to run inference on a single GPU with a large language model? Well, Gavin Li has come up with a new technique that allows you to do just that. In a recent article, he explains the key techniques for extreme memory optimization of large models. The most critical technique is layer-wise inference. By leveraging the divide and conquer approach, Li shows that you don't actually need to keep all layers in GPU memory. Instead, you can load whichever layer is needed from disk when executing that layer, do all the calculations, and then free up the memory. This significantly reduces the GPU memory required per layer.

Li also introduces flash attention, which is a critical optimization for large language models. Flash attention deeply optimizes CUDA memory access to achieve multi-fold speedups for inference and training. This optimization reduces the memory complexity from O(n²) to O(n), making it much more efficient. To further optimize the memory usage, Li discusses model file sharding. The original model file is typically sharded into multiple chunks, but loading the entire file for each layer execution wastes a lot of memory and disk reading time. By pre-processing the model file and sharding it by layers, the memory usage is minimized. To implement these techniques, Li utilizes the meta device feature provided by HuggingFace Accelerate. The meta device is a virtual device designed for running ultra large models. It allows you to dynamically transfer parts of the model from the meta device to a real device like the CPU or GPU during execution, reducing memory usage. If you're interested in trying out these optimizations for yourself, Li has open-sourced the code in a library called AirLLM. The library, which can be found on the Anima GitHub, allows you to achieve extreme memory optimization with just a few lines of code.

So there you have it, with these techniques and the AirLLM library, you can run inference on a single 4GB GPU with a 70B large language model. It's truly unbelievable!

The discussion on this submission revolves around various aspects of running inference on a single GPU with a large language model. Here are some key points from the comments:

- One commenter mentions that they tried running the techniques on a GTX 1060 6GB GPU but found it to be slow, taking 13 hours to generate a sentence. They speculate that the GPU memory usage might be the bottleneck.
- Another commenter discusses the technique of model file sharding and suggests looking into PyTorch's gradient checkpointing for efficient memory usage.
- A discussion arises regarding the difference in performance between CPU and GPU inference. Some commenters mention that CPU inference is typically limited by memory bandwidth, whereas GPU inference benefits from loading weights onto the GPU. SSD loading of weights is also mentioned as a way to reduce GPU memory usage.
- There is some skepticism raised regarding the capabilities of non-batched inference for large language models. Commenters mention that certain context and historical understanding provided by complete sentences might be necessary for relevant embeddings and projections in custom matrix operations.
- The potential drawbacks of extreme memory optimization are also discussed, with one commenter pointing out that the process seems to swap VRAM to RAM and disk, which can significantly impact performance.
- Various optimization techniques and resources are shared, including libraries such as AirLLM and llmcpp, as well as discussions on quantizing models and using DirectStorage for improved IO.
- Further discussions revolve around distributed computing and the benefits of using multiple GPUs to process different layers simultaneously, though latency is a concern in such setups.

Overall, the discussion explores different techniques, optimizations, and limitations related to running inference on a single GPU with a large language model.

---

## AI Submissions for Sat Dec 02 2023 {{ 'date': '2023-12-02T17:09:38.438Z' }}

### Unsupervised speech-to-speech translation from monolingual data

#### [Submission URL](https://blog.research.google/2023/12/unsupervised-speech-to-speech.html) | 20 points | by [atg_abhishek](https://news.ycombinator.com/user?id=atg_abhishek) | [4 comments](https://news.ycombinator.com/item?id=38497549)

Google Research has introduced Translatotron 3, an unsupervised speech-to-speech translation architecture that can learn the translation task from monolingual data alone. Traditional speech-to-speech translation models rely on parallel speech data, which is scarce, leading to the need for synthesized data. However, Translatotron 3 eliminates the requirement for bilingual speech datasets by incorporating techniques such as back-translation, pre-training with SpecAugment, and unsupervised embedding mapping based on Multilingual Unsupervised Embeddings (MUSE). Experimental results between Spanish and English show that Translatotron 3 outperforms a baseline cascade system. By preserving paralinguistic characteristics, such as tone and emotion, Translatotron 3 aims to improve the quality and authenticity of translated speech.

The discussion revolves around the Translatotron 3 submission on Hacker News. One user, "xnx," comments on the challenges of translating languages and mentions that it often stretches the mind. Another user, "grsv," responds, stating that it is humans' sentience and comprehension that allows them to learn languages, and the proposed method relies on training models with monolingual speech-text datasets. "grsv" further explains that the proposed method utilizes a shared embedding space for languages, forcing the model to learn semantics independently of the language. Additionally, they mention using back-translation for training and conducting performance checks using a Spanish-English-Spanish translation loop. They express enthusiasm for the promising and interesting results, wondering how similar the languages need to be at a lexical level for the model's performance to excel. Another user, "IanCal," shares their curiosity about the level of complexity in English and internal linguistic representation, and suggests that if there is a low complexity in mapping internal representations, then a sensible sentence in English should result in a sensible sentence in the translated language.

Overall, the discussion focuses on the methodology and potential implications of Translatotron 3, with users expressing interest in the results and exploring the nuances of language translation.

### Galactic algorithm

#### [Submission URL](https://en.wikipedia.org/wiki/Galactic_algorithm) | 115 points | by [sockaddr](https://news.ycombinator.com/user?id=sockaddr) | [19 comments](https://news.ycombinator.com/item?id=38500782)

In computer science, there is a concept called a galactic algorithm. These are algorithms that have incredible theoretical performance but are never actually used in practice. There are a few reasons for this. Sometimes the performance gains only apply to problems that are so large they never occur in real-world scenarios. Other times, the complexity of the algorithm outweighs the relatively small gain in performance. These algorithms are named "galactic" because they will never be used on any data sets here on Earth.

Even though galactic algorithms are not used in practice, they can still contribute to computer science in a few ways. Firstly, they may introduce new techniques that can eventually be used to create practical algorithms. Secondly, as computational power advances, previously impractical algorithms may become feasible to use. Thirdly, even if an algorithm is impractical, it can still demonstrate that certain bounds can be achieved or prove that proposed bounds are incorrect, thereby advancing the theory of algorithms.

For example, there are galactic algorithms for problems like integer multiplication, matrix multiplication, communication channel capacity, sub-graph testing, cryptographic breaks, and the traveling salesman problem. These algorithms have impressive theoretical performance, but their large constants make them impractical for real-world use. However, they still serve important purposes. For instance, they can inspire further research and refinement to make them more practical or they can settle important open problems in computer science, like the P versus NP problem.

So, while galactic algorithms may never be used in practice, they still have value in advancing the field of computer science and pushing the boundaries of what is possible.

The discussion on this submission covers a range of topics related to galactic algorithms. One commenter shares a link to a paper that discusses a simulated annealing algorithm for solving global optimization problems. Another commenter expresses doubt about the practicality of these algorithms, suggesting that simulated annealing and random restarts may not be effective. Another user finds the topic interesting and mentions that numbers can be fascinating. 

One commenter asks if the concept of galactic algorithms can be applied to other fields. Another user shares a link to Optimal Universal Search, which is related to the topic of optimal algorithms. 

There is also a discussion about the classification of General Relativity as a galactic algorithm for solving Newtonian Equations. A user argues that General Relativity is not a true galactic algorithm because it provides similar answers when calculating the motion of spacecraft around a black hole. However, another user argues that the comparison is not accurate and that the complexity and practicality of General Relativity differ from galactic algorithms.

The discussion ends with a user mentioning that General Relativity is used in GPS to correct clock discrepancies based on its small correction below a galactic scale.

### Show HN: ChatCBT – AI-powered cognitive behavioral therapist for Obsidian

#### [Submission URL](https://github.com/clairefro/obsidian-chat-cbt-plugin) | 55 points | by [marjipan200](https://news.ycombinator.com/user?id=marjipan200) | [18 comments](https://news.ycombinator.com/item?id=38499722)

Introducing ChatCBT: an AI-powered journaling plugin for your Obsidian notes. Inspired by cognitive behavioral therapy (CBT), this plugin acts as a journaling assistant that helps you reframe negative thoughts and rewire your reactions to distressful situations. 

With ChatCBT, you can start chatting in a note and receive kind and objective responses to help uncover negative thinking patterns. Conversations are stored privately on your computer, and you can automatically summarize your reframed thoughts in a table to inspire affirmations. 

The plugin offers two options for handling your data: you can choose to use a cloud-based AI service (OpenAI) or a 100% local and private service (Ollama). OpenAI provides excellent conversation quality and speed, but it is a paid service. On the other hand, Ollama is free and offers good conversation quality.

To install ChatCBT, follow the instructions provided in the repository. The plugin is currently under review to become an official Obsidian Community Plugin, but you can still install it in developer mode. Once installed, you can configure an AI platform connection from the plugin settings menu.

Overall, ChatCBT is a powerful tool for journaling and self-reflection, leveraging the capabilities of AI to assist you in improving your mental well-being.

The discussion around the submission of ChatCBT on Hacker News covers various aspects of the plugin and its potential benefits.

One user raises concerns about the effectiveness of AI-powered therapy compared to traditional cognitive behavioral therapy (CBT). They argue that while AI may have potential, it is important to diagnose problems correctly, and certain issues require the guidance of a qualified human therapist.

Another user points out that the plugin installation may not work properly and reports encountering errors related to server problems. Another user suggests investigating the issue further.

A user mentions that relying on an AI plugin may discourage people from seeking help from licensed therapists. They argue that AI lacks the understanding and learning methods that are a significant part of therapy, and interacting with a human therapist provides a more effective way of improving one's life. They suggest exploring cheaper alternatives with qualified professionals instead of relying solely on AI.
In response to this, another user clarifies that ChatCBT is not intended to replace professional therapy but rather supplement it. They explain that the plugin is designed to provide interactive journaling similar to CBT worksheets that therapists provide to patients. They emphasize that it is not meant to replace human interaction but rather be used as a tool for self-reflection.
There is a discussion about the affordability of therapy and the challenges many people face in accessing mental health care. Some users express frustration with the stigmatization of mental health issues and the limited coverage provided by insurance providers, making therapy inaccessible to many.
One user points out that therapy is important and should not be undervalued, highlighting that licensed therapists can help people understand and work through their problems in a realistic and systematic way.
Another user suggests that AI could be beneficial in developing personal self-assistants, such as an interactive AI like GPT-4, which could have therapeutic effects and help individuals make decisions, manage regrets, and provide guidance similar to that of a powerful therapist.
Overall, the discussion reflects varying perspectives on the role of AI in mental health care, emphasizing the importance of professional therapy while acknowledging the potential benefits of AI-powered tools as supplements for self-reflection and journaling. There is also recognition of the challenges in accessing affordable and comprehensive mental health care.

### Optical effect advances quantum computing with atomic qubits to a new dimension

#### [Submission URL](https://www.tu-darmstadt.de/universitaet/aktuelles_meldungen/einzelansicht_410816.en.jsp) | 51 points | by [FinnKuhn](https://news.ycombinator.com/user?id=FinnKuhn) | [14 comments](https://news.ycombinator.com/item?id=38494466)

Scientists at the Technische Universität Darmstadt in Germany have developed a technique using the optical Talbot effect to increase the number of qubits in a quantum computer without requiring additional resources. Qubits are the basic units of information in quantum computing and can process both "0" and "1" simultaneously, allowing for parallel calculations. Currently, quantum computers are limited to a few hundred qubits, but for practical applications, such as optimizing traffic flows, thousands or millions of qubits are needed. The Darmstadt team's approach uses laser beams and a glass element with microlenses arranged like a chessboard to create a 3D lattice of focal points that can hold individual atoms as qubits. By exploiting the Talbot effect, multiple layers of qubits can be added without needing additional laser output. The researchers were able to create 16 layers of qubits, potentially allowing for over 10,000 qubits. The team plans to further develop the technology for applications in quantum technologies and high-precision optical atomic clocks.

The discussion on this submission revolves around various aspects of the technology and its implications.

- One user raises a concern about the potential difficulty of scaling this technology due to the largest single-qubit coherence time limitation.
- Another user counters this argument, stating that quantum error correction schemes help mitigate coherence limitations and make it easier to achieve thousands of qubits.
- A user suggests that the discovery of transistors in the past took 50 years to reach mass production, and it is expected that the development of quantum computers will follow a similar incremental timeline.
- The difficulty of developing transistors for quantum computing is discussed, with one user suggesting that it is significantly harder to create transistors for quantum computing compared to classical computing.
- Another user raises the point that the statement about the difficulty of developing transistors for quantum computing is redundant and does not make sense in the context of the discussion.
- The challenges of funding quantum technology development are mentioned, with private funding playing a significant role compared to government investment.
- A user believes that decoherence times for qubits are becoming longer, making natural times higher and enabling better performance.
- Discussion moves towards the technical aspects of qubit coupling and the configuration of individual qubits.
- One user shares a link to a paper that may provide more information on the topic.
- Another user shares a different paper that describes a scalable multi-layer architecture for single-time qubit arrays using a three-dimensional Talbot interferometer lattice.

Overall, the discussion covers topics such as scalability, coherence limitations, development timelines, funding challenges, and technical aspects of qubit coupling and configuration.

### Scalable extraction of training data from (production) language models

#### [Submission URL](https://arxiv.org/abs/2311.17035) | 99 points | by [wazokazi](https://news.ycombinator.com/user?id=wazokazi) | [14 comments](https://news.ycombinator.com/item?id=38496715)

In a paper titled "Scalable Extraction of Training Data from (Production) Language Models," researchers explore the concept of extractable memorization, referring to training data that can be extracted from machine learning models through queries without prior knowledge of the dataset. The study reveals that adversaries can efficiently extract gigabytes of training data from various language models, including Pythia, GPT-Neo, LLaMA, Falcon, and ChatGPT. The researchers demonstrate that existing techniques can attack unaligned models, and they develop a new divergence attack specifically targeting ChatGPT. This attack causes the model to deviate from its chatbot-style responses, resulting in the emission of training data at a rate 150 times higher than normal. The findings suggest that current alignment techniques do not eliminate memorization and expose the potential for practical attacks to recover more data than previously thought.

The discussion around the submission revolves around various aspects of the research paper on scalable extraction of training data from language models.

- One commenter discusses the use of memorization techniques to reduce hallucinations and improve the relevance of passages retrieved by language models.
- Another user suggests that the issue of memorization is not surprising, considering that language models like ChatGPT have access to vast amounts of internet data. They mention that DeepMind recently extracted personally identifiable information (PII) from ChatGPT prompts.
- There is a question raised about whether language models are trained on secret data. The response suggests that they likely contain copyrighted data, such as lyrics from songs or track crashes.
- A user raises a question about the capacity of models to memorize an infinite amount of information and whether the extraction of total memorized data is possible. The response highlights that language models are restricted to a limited number of keywords and that the size of the training dataset is a determining factor.
- A discussion ensues about the capabilities of models like GPT-NeoX in generating grammatically novel sentences based on different settings and training inputs.
- A user clarifies that the 50-grams mentioned in the research paper refer to substrings of 50 words from the dataset, and generating complete 50-grams is challenging even for a modern GPU.
- There is further clarification about creating the 50-gram dataset and its relevance to the research paper's findings.

Overall, the discussion delves into the implications and limitations of language models' memorization capabilities and the concerns surrounding the extraction of training data.

### Meta will enforce ban on AI-powered political ads in every nation, no exceptions

#### [Submission URL](https://www.zdnet.com/article/meta-will-enforce-ban-on-ai-powered-political-ads-in-every-nation-no-exceptions/) | 111 points | by [jnord](https://news.ycombinator.com/user?id=jnord) | [42 comments](https://news.ycombinator.com/item?id=38495875)

Meta, the parent company of social media platforms like Facebook and Instagram, has announced that it will enforce a ban on AI-powered political ads in all nations, without exceptions. This comes as several countries are set to hold elections next year. The ban extends to ads targeting specific services and issues related to politics, elections, housing, employment, credit, social issues, health, pharmaceuticals, and financial services. Meta's generative AI advertising tools, which include features like text variation and image cropping, will not be accessible for these types of campaigns. Meta has emphasized the importance of AI and plans to add generative AI capabilities across all its platforms. The company's Ads Manager tool serves as a launchpad for running ads, providing advertisers with an all-in-one tool for ad creation, management, and tracking. It also recently introduced an AI chatbot called Meta AI and an AI image generator tool called Emu. Meta's AI products have been adopted by more than half of advertisers, with the Advantage+ tools helping advertisers achieve a $10 billion run rate from shopping campaigns.

The discussion on Hacker News revolves around Meta's announcement to ban AI-powered political ads on its platforms. Some users express skepticism about the effectiveness of Meta's detection algorithms, highlighting challenges in accurately detecting and preventing AI-generated content. Others argue that the ban is necessary to prevent exploitation and misleading advertising. There is also a debate about Meta's intentions and trustworthiness as an organization. Some users question whether Meta's ban is selective and if it will be effectively enforced. There is a suggestion that AI detectors may be developed to identify AI-generated political ads. Additionally, there are discussions about the limitations of AI-generated text and images and the impact of the ban on political campaigns. Some users express support for Meta's decision, while others express concerns about potential censorship and exceptions to the ban.

### Good old-fashioned AI remains viable in spite of the rise of LLMs

#### [Submission URL](https://techcrunch.com/2023/12/01/good-old-fashioned-ai-remains-viable-in-spite-of-the-rise-of-llms/) | 76 points | by [webmaven](https://news.ycombinator.com/user?id=webmaven) | [41 comments](https://news.ycombinator.com/item?id=38499723)

In a recent article on TechCrunch, it is highlighted that task-based models in artificial intelligence (AI) are not going away despite the rise of generalized large language models (LLMs) like ChatGPT. Task-based models have been the cornerstone of AI in the enterprise for a long time, and they still play a crucial role in solving real-world problems. While LLMs offer flexibility and the ability to handle varied tasks, task models are smaller, faster, cheaper, and more performant for specific tasks. The industry is still debating the capabilities and limitations of LLMs in comparison to task models. Amazon CTO Werner Vogels and Atul Deo, general manager of Amazon Bedrock, both believe that task models are valuable AI tools and are not likely to disappear. They argue that an all-purpose model is appealing on an aggregate level, but task models offer the advantage of reusability and specialized design. However, the upgrades made to Amazon's machine learning operations platform, SageMaker, indicate that the company recognizes the importance of managing large language models. While LLMs have gained significant attention, enterprises are unlikely to abandon their investments in task models. Data scientists still play a vital role in understanding data and AI within companies, regardless of the model being used. The article concludes that both task models and LLMs will continue to coexist as they have their own strengths and applications in the AI landscape.

The discussion around the submission revolves around the comparison between task-based models and large language models (LLMs) in artificial intelligence (AI). Some users argue that knowing the appropriate tools for modern AI work is crucial. LLMs may struggle with gradient-based training algorithms and require significant amounts of data, which can lead to subpar results. Task models, on the other hand, are smaller, faster, cheaper, and more effective for specific tasks.  There is a debate about the capabilities and limitations of LLMs compared to task models. Some users point out that LLMs like BERT and RoBERTa can outperform smaller models in certain tasks, while others argue that LLMs fall short and smaller models focused on specific approaches can dominate in the field. There is also a discussion about the challenges and strengths of different AI models. Some users mention that traditional symbolic AI, also known as GOFAI (Good Old-Fashioned AI), has limitations, while others argue that LLMs have their own disadvantages. There is a mention of using fasttext and word2vec for production work and the complexities involved in training models from scratch. The discussion touches on topics like GOFAI, symbolic AI, probabilistic AI, deep learning, and the use of LLMs for tasks such as classification and generation. Some users express skepticism about the viability and long-term sustainability of certain AI approaches. Various users also discuss the importance of quality data, the limitations of LLMs, and the impact of AI on industries like customer service and marketing.

Overall, the discussion highlights the coexistence of task models and LLMs in the AI landscape, with users sharing their perspectives on the strengths and weaknesses of both approaches.

### AI can tell what you're typing by listening to the sound of your keyboard

#### [Submission URL](https://www.theregister.com/2023/08/07/audio_keystroke_security/) | 22 points | by [thisAintReal](https://news.ycombinator.com/user?id=thisAintReal) | [21 comments](https://news.ycombinator.com/item?id=38496967)

Researchers in the UK claim to have developed a method to turn typing sounds into text with 95% accuracy. Using deep learning and self-attention transformer layers, the team was able to capture the sounds of typing and translate them into data for exfiltration. The method achieved high accuracy rates even over remote methods like Zoom and Skype calls. The researchers suggest that changing one's typing style or using randomized passwords with multiple cases can mitigate the risk of this type of attack. They also recommend using a second authentication factor and playing fake keystroke sounds to mask the real ones to further protect sensitive information. Further research is being conducted to explore new sources for recordings and improve the effectiveness of acoustic snooping.

- One commenter mentions that the concept of using typing sounds for cybersecurity purposes has been around since the 1960s, and provides a link to an article on acoustic cryptanalysis.
- Another commenter points out that the accuracy of recordings dropped significantly on Zoom calls (93%) compared to Skype calls (917%). They find it interesting that Skype messenger is well-known for its good audio quality.
- A discussion ensues about voice codecs and the potential for variance in accuracy based on sampling rates and other factors.
- A user asks about a previous article they read about a laptop keyboard typing detection application and how reliable it is.
- The implications of the article regarding passwords are discussed. One commenter mentions that they have typed passwords in their comments, unaware that it was for their benefit.
- The topic of typing rhythm and variations in typing patterns is raised, indicating that it may have an impact on the effectiveness of acoustic snooping.
- A commenter discusses their use of the Colemak keyboard layout and how it can potentially render acoustic attacks ineffective.
- Some users discuss operating system-level filtering and mention that Zoom has built-in noise filtering.
- A humorous comment suggests creating powerless keyboards to counteract the threat.
- There is a conversation about the convenience of typing on various keyboards and the different sounds they produce.
- The possibility of creating keyboards specifically designed to produce distinct tones for each keypress is mentioned.
- A user comments on the processing requirements and limitations of using acoustic data for analysis.
- One commenter mentions that they type at a high speed of over 100 words per minute.
- The discussion wraps up with a mention of Facebook and Google.

### Javelin Missile guidance computer – Part 1: teardown [video]

#### [Submission URL](https://www.youtube.com/watch?v=11_5TB0-lNw) | 50 points | by [dun44](https://news.ycombinator.com/user?id=dun44) | [4 comments](https://news.ycombinator.com/item?id=38494777)

It seems like you've provided some information about YouTube and NFL Sunday Ticket. Is there something specific you would like to know or discuss about these topics?

The discussion on this submission revolves around the surprise that FPGAs (Field-Programmable Gate Arrays) are being used in the mass production of missiles. One user comments that they are surprised because FPGA designs are often considered less reliable than ASICs (Application-Specific Integrated Circuits). Another user justifies this use by mentioning that debugging custom silicon for thousands of missiles would have been expensive, and FPGAs offer the advantage of reconfigurability. 

In response to this, another user points out that military procurement budgets often have constraints, and it is likely that the Ukrainian military, which is mentioned in the initial submission, may not have had the resources to spend on expensive custom hardware. 

Finally, a user comments that the post has been duplicated and provides a link to the original discussion.

### Pentagon Scientists Discuss Cybernetic Super Soldiers in Dystopian Presentation

#### [Submission URL](https://www.vice.com/en/article/n7eky8/pentagon-scientists-discuss-cybernetic-super-soldiers-that-feel-nothing-while-killing-in-dystopian-presentation) | 32 points | by [rntn](https://news.ycombinator.com/user?id=rntn) | [22 comments](https://news.ycombinator.com/item?id=38498884)

In a dystopian presentation at the Interservice/Industry Training, Simulation and Education Conference (I/ITSEC), officials from the Pentagon discussed the concept of creating cybernetic "super soldiers" inspired by characters like Captain America and Iron Man. The panel of military and military-adjacent scientists delved into topics like breeding programs, Marvel movies, The Matrix, and the various technologies being researched to achieve this vision. Some ideas discussed included cybernetic implants, pain-numbing stimulants, synthetic blood, and the ability to regrow limbs. The conversation also touched on the ethical concerns surrounding bodily autonomy and the potential for extending the service of veteran soldiers or enlisting older individuals by leveraging the technology. The panelists acknowledged the applicability of these ideas and the potential benefits they could bring. Additionally, they discussed the use of non-invasive brain stimulation techniques to interface with the brain directly, similar to the concept portrayed in The Matrix. The conversation delved into the ethical and legal boundaries associated with creating super soldiers and questioned societal norms and ethics. Overall, the talk highlighted the ongoing efforts to enhance military capabilities using cutting-edge technology but also raised important questions about the ethical implications of such advancements.

The discussion surrounding the submission on creating cybernetic "super soldiers" had a range of responses. One commenter pointed out that military and industrial complexes are constantly pursuing profit-centered contracts, inventing terrible weapons, and taking back control from corporatocracy. Another commenter mentioned that technology exists in diverse hands, implying that the potential for cybernetic enhancements is not limited to the military. There was also a discussion about the ethnicity of the super soldiers, with one commenter asking about the ethnicity of Hispanic/Latino super soldiers. The question was further explored, with another commenter considering the significance of ethnicity in this context. Another point raised in the discussion was the nature of the objectives of the defensive vs offensive groups. One commenter argued that the focus should be on defensive objectives, while another pointed out that detecting and disabling the enemy is inherently offensive. There were also references to fictional works, such as a comparison to the concept of super soldiers in the game Deus Ex and a mention of Peter Watts' book about zombies and combat effectiveness. Some commenters expressed concern about the consequences of this technology, while others expressed skepticism or resigned acceptance.

### The Evolution of Intelligence Itself

#### [Submission URL](https://metastable.org/evolution.html) | 9 points | by [pbw](https://news.ycombinator.com/user?id=pbw) | [6 comments](https://news.ycombinator.com/item?id=38495385)

In a recent article, Philip Winston reflects on the evolution of intelligence and its relationship with AI. He highlights the tremendous progress made in computing power over the past decades, demonstrating how AI is now able to surpass human capabilities in various domains. Winston notes that the accessibility of AI systems like AI Art and ChatGPT has drastically changed our perception of AI, as they produce text and images at a level that is indistinguishable from human creations. He predicts that generative AI will soon expand to create all types of media, including movies, music, and video games. However, while some embrace the potential of AI to improve various aspects of human life, others express concerns about its impact on employment, meaning, and even humanity's place in the world. Winston believes that these concerns are valid, but he also emphasizes the immense benefits that AI could bring to areas such as energy, manufacturing, healthcare, and education. He argues that with thoughtful and careful navigation, we can mitigate the risks associated with AI. Winston likens AI to organized groups of people, highlighting the parallel between human collaboration and the coordination that occurs within AI systems. He suggests visualizing AI accomplishments as the work of a team of people, emphasizing the effort and hard work involved rather than treating it as something mysterious or threatening. Winston also discusses the significant role of training in the development of AI systems, noting that they are built and trained on the collective work of humans. He envisions a future where AI technology is accessible to all, providing individuals with the power of humanity's collective knowledge and talent. Ultimately, Winston finds reassurance in the inevitability of AI, seeing it as a natural extension of human evolution and emphasizing the need to manage and embrace its potential.

The discussion on this submission revolves around various aspects of AI and its potential impact on humans. Below are some notable comments:

1. One user argues that the current hype around AGI (Artificial General Intelligence) is misguided and suggests that AI advancements are simply sophisticated text compressors and word prediction models. They express concern about the large amount of money and energy spent on training AI systems while poverty still remains a major issue.
2. Another user counters this argument, stating that AI has the potential to significantly improve various aspects of human life. They believe that AI requires intelligence, and if intelligence is limited, then AI can no longer progress. They highlight the complexity and potential benefits AI can bring.
3. In response to the second user, someone else raises the issue of resource allocation, pointing out that while billions of dollars are spent on AI development, there are still pressing global problems like cancer, COVID-19, and Alzheimer's that require significant funding.
4. Another user points out the policy problem of healthcare spending, stating that AI has the potential to improve things, but the issue lies in the allocation of resources and the implementation of AI solutions.
5. One user imagines a future where AI aids in decision-making for small, hard problems, such as text-to-speech messages, video calls, and more.

6. A comment suggests that AI can consolidate the power of humanity and its decision-making processes.

Overall, the discussion touches on the potential benefits of AI, concerns about resource allocation, and the role AI could play in decision-making and problem-solving.