import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Thu Aug 24 2023 {{ 'date': '2023-08-24T17:10:41.669Z' }}

### Code Llama, a state-of-the-art large language model for coding

#### [Submission URL](https://ai.meta.com/blog/code-llama-large-language-model-coding/) | 847 points | by [marcopicentini](https://news.ycombinator.com/user?id=marcopicentini) | [474 comments](https://news.ycombinator.com/item?id=37248494)

Today, a groundbreaking large language model called Code Llama has been released. This state-of-the-art model is capable of generating code and natural language about code from both code and natural language prompts. Code Llama is free for both research and commercial use.

Code Llama is built on top of Llama 2 and is available in three models: Code Llama, Codel Llama (Python specialized for Python), and Code Llama - Instruct (fine-tuned for understanding natural language instructions). In benchmark testing, Code Llama outperformed state-of-the-art publicly available large language models on code tasks.

The release of Code Llama is a significant advancement for generative AI in the coding field. It has the potential to improve workflows, boost productivity, and lower the barrier to entry for people learning to code. The model can be used as a productivity and educational tool to help programmers write more robust and well-documented software.

Code Llama works by further training Llama 2 on code-specific datasets, enhancing its coding capabilities. It supports popular programming languages such as Python, C++, Java, PHP, Typescript (Javascript), C#, and Bash. The model can generate code, provide natural language explanations about code, assist with code completion, and even aid in debugging.

Three sizes of Code Llama are available, with 7B, 13B, and 34B parameters respectively. Each model is trained with 500B tokens of code and code-related data. The 7B and 13B models feature fill-in-the-middle (FIM) capability for code completion. The larger 34B model provides the best results and coding assistance but may have higher latency. The models can handle input sequences up to 100,000 tokens.

Two additional variations of Code Llama have been fine-tuned: Code Llama - Python, specifically designed for Python code, and Code Llama - Instruct, which excels at understanding natural language instructions. The latter is recommended for code generation as it has been trained to generate safe and helpful answers in natural language.

Code Llama's performance was evaluated using popular coding benchmarks. In tests, it outperformed existing solutions on code completion and code writing tasks. For example, the Code Llama 34B model scored 53.7% on HumanEval and 56.2% on Mostly Basic Python Programming (MBPP), making it one of the top-performing models.

It's worth noting that while Code Llama brings many benefits, it also comes with risks. Responsible AI development is essential, and precautions have been taken to mitigate potential issues. Code Llama has undergone safety measures, including a quantitative evaluation of its risk of generating malicious code.

Overall, Code Llama is a significant step forward in the field of generative AI for coding. With its innovative capabilities, researchers and developers can expect improved productivity and efficiency, while aspiring programmers can find valuable educational support.

The discussion about the submission "Introducing Code Llama: A State-of-the-Art Large Language Model for Coding" on Hacker News covers a range of topics related to the use and understanding of code generators for programming.

Some comments discuss alternative code solutions for specific problems and provide code snippets or suggestions. Others raise concerns about the limitations and potential pitfalls of using code generators and the risks associated with relying solely on AI-generated code. The discussion also touches on the importance of fundamental programming skills and knowledge, suggesting that relying solely on AI-generated code may lead to a lack of understanding and limitations in problem-solving capabilities.

There are also comments discussing the performance and technical details of Code Llama, including the size of the models, their capabilities, and the resources required to run them. Some users express skepticism about the practicality and usefulness of such large language models, while others highlight the potential benefits for productivity and education.

Overall, the discussion highlights different perspectives on the use of AI for code generation, emphasizing the need for a balanced approach that combines the capabilities of AI with human programming skills and knowledge.

### Artificial intelligence gave a paralyzed woman her voice back

#### [Submission URL](https://www.ucsf.edu/news/2023/08/425986/how-artificial-intelligence-gave-paralyzed-woman-her-voice-back) | 204 points | by [gehwartzen](https://news.ycombinator.com/user?id=gehwartzen) | [68 comments](https://news.ycombinator.com/item?id=37252025)

Researchers at UC San Francisco and UC Berkeley have made a breakthrough in brain-computer technology that could revolutionize communication for people with severe paralysis. Using a brain-computer interface (BCI), the researchers were able to synthesize speech and facial expressions from brain signals for the first time. The system can also decode these signals into text at a much faster rate than current communication devices, offering hope for a more natural and efficient way for individuals like Ann, a participant in the study, to communicate. The researchers hope that this advancement will lead to an FDA-approved system in the near future.

The discussion on this submission covers various aspects of the research and its implications. Some commenters discuss the technical details of the system, such as the mapping of words to phonemes and the challenges of interpreting speech signals based on muscle movements. Others highlight the limitations of the technology, such as the inability to read thoughts and the need for physical movements to generate signals. 

There is also a discussion about related studies and technologies, including silent speech interfaces and mind-to-speech interfaces. Some commenters express concerns about privacy and the potential for forced disclosure of personal information. Others discuss the legal implications, such as the use of thoughts as evidence in court proceedings. 

A few commenters point out potential applications beyond communication for people with paralysis, including gaming and medical diagnostics. There is also a brief discussion about the difference between AI and machine learning. Lastly, there are some users who flagged comments for help or to draw attention to them.

### Maccarone: AI-managed code blocks in Python

#### [Submission URL](https://github.com/bsilverthorn/maccarone) | 169 points | by [silverthorn](https://news.ycombinator.com/user?id=silverthorn) | [70 comments](https://news.ycombinator.com/item?id=37254510)

Introducing Maccarone: AI-managed code blocks in Python! Developed by user bsilverthorn, Maccarone allows you to delegate sections of your Python program to AI ownership. Simply define the sections you want the AI to handle, and Maccarone will generate the code for you. It uses the power of GPT-4 to write code and makes OpenAI API calls using your API key. However, do note that API calls come with a cost, as you will be charged by OpenAI based on the size of the generated code. Maccarone also keeps the generated code up to date when you make changes to your program. You can try out Maccarone through the VS Code extension or install it directly from PyPI. Be sure to check out the detailed documentation and FAQs to learn more about this exciting tool.

The discussion on this submission covers various topics related to Maccarone, AI code generation, and related concepts. Here are the key points:

- One commenter points out that Maccarone sounds like a mix of languages, and another mentions the German term "gflschtr dtschr dsnt cptr," which refers to a similar concept.
- The strength of GPT-4 in generating code is discussed, with some noting that GPT models have become stronger over the years and others suggesting that GPT-4 would be even better.
- There is a mention of Copilot, another AI code generation tool, and its contextual understanding of code. It is noted that Cross-file support is coming soon for Copilot.
- The concept of using deterministic finite automata (DFA) in managing code is discussed, with an example of using DFAs for JSON validation. The benefits and limitations of this approach are highlighted.
- The discussion delves into the idea of using AI to write proofs and validate conditions in code. Some commenters express skepticism about self-verifying systems and discuss the potential role of AI and deep learning in assisting with proofs.
- A reference to a research paper on AI reflection and self-correction is shared.
- The advantages and disadvantages of complete code generation and the use of comments as placeholders are debated. Some argue that continuous manual review is necessary, while others suggest relying on automation.
- The idea of using AI in code decorators is discussed, with one commenter pointing out an existing framework that implements this concept.
- The potential for AI-generated comments and the flexibility of languages in supporting comments are debated, with some advocating for structured and limited use of comments.
- The importance of using version control properly and ensuring that comments match the code changes is mentioned.
- Suggestions are made for using AI to configure data flows and predefined code models.
- Some commenters express interest in trying out Maccarone or similar AI code generation tools.
- Various links and resources related to the discussed topics are shared.

Overall, the discussion covers a range of perspectives on AI code generation tools like Maccarone and delves into topics such as language flexibility, code verification, and the role of AI in programming.

### Graph of Thoughts: Solving Elaborate Problems with Large Language Models

#### [Submission URL](https://arxiv.org/abs/2308.09687) | 261 points | by [jonbaer](https://news.ycombinator.com/user?id=jonbaer) | [43 comments](https://news.ycombinator.com/item?id=37248694)

The paper titled "Graph of Thoughts: Solving Elaborate Problems with Large Language Models" introduces a framework called Graph of Thoughts (GoT) that enhances the prompting capabilities of large language models (LLMs). GoT allows the modeling of LLM-generated information as a graph, where LLM thoughts are represented as vertices and dependencies between thoughts as edges. This approach enables combining thoughts into synergistic outcomes, distilling the essence of thought networks, and enhancing thoughts using feedback loops. The authors demonstrate that GoT offers advantages over existing paradigms on different tasks, such as improving sorting quality by 62% while reducing costs by over 31%. The extensibility of GoT allows for the development of new thought transformations and prompting schemes. This work brings LLM reasoning closer to human thinking and brain mechanisms.

The discussion on Hacker News regarding the submission titled "Graph of Thoughts: Solving Elaborate Problems with Large Language Models" covers a range of topics related to the paper. One commenter, knxr, mentions a similar project they worked on and expresses excitement about exploring the direction of modeling complex LLM-aided processes using dependency graphs. They highlight the usefulness of features like time-rewinding for debugging and the potential of applying genetic algorithms to LLM implementation. Another user, brtsbrn, expresses interest in systems that can generate knowledge graphs from LLMs to make machine-generated information more readable. They suggest using prompt papers to suggest different ways of categorizing and grading the generated content. The topic of trustworthiness of LLMs is discussed by throwaway290 and brtsbrn. The latter expresses skepticism and emphasizes the importance of human checking and validating the generated graphs. Firewolf34 brings up the idea of a hierarchy and structure in graph-like thought processes, suggesting that they are advanced forms of information processing. Mcwfsh mentions that non-hierarchical graphs can perform complex transformations and suggests that there may be a trade-off between hierarchical optimization and performance in thought processes. Other topics briefly discussed include the use of graphs in finance, the challenges of LLM directionality, the potential applications of graph transformation in general computation, and the efficiency of LLMs in sorting numbers. Overall, the discussion covers a range of perspectives on the topic of using graphs to enhance large language models, including their potential applications, limitations, and the challenges associated with trustworthiness and efficiency.

### Show HN: Web App with GUI for AutoML on Tabular Data

#### [Submission URL](https://github.com/mljar/automl-app) | 38 points | by [pplonski86](https://news.ycombinator.com/user?id=pplonski86) | [3 comments](https://news.ycombinator.com/item?id=37247268)

Automated Machine Learning (AutoML) is taking the world by storm, and now there's a web app to make it even easier. Developed by mljar, the AutoML Web App allows users to train machine learning pipelines using MLJAR AutoML, specifically tailored for tabular data. The app automates several key tasks, including data preprocessing, features engineering, algorithm selection, tuning, model explanations, and automatic documentation. The best part? The app is created directly from Jupyter Notebooks with the Mercury framework. Whether you prefer the online demo or running the app locally, you'll have access to a user-friendly interface and powerful ML capabilities. Give it a try and see how it can revolutionize your ML training journey.

The discussion on this submission revolves around the challenges and concerns related to AutoML and the features of the mljar AutoML Web App. 

One commenter, cnvscrtc, points out that AutoML may not always generate reliable models that generalize well to real-world scenarios. They mention that support for data preprocessing and model explanations can be overlooked, and raise concerns about the robustness and reliability of the models generated. They also mention the risks of overfitting and the difficulties in debugging projects. However, they acknowledge the usefulness of AutoML in probing and refining approaches.

Another commenter, pplonski86, shares a benchmark for independent researchers to verify and validate AutoML techniques. They mention the technique of stopping the training time as a way to address issues related to overfitting.

pplonski86, who appears to be associated with mljar, mentions the mljar AutoML Web App that they have created. They explain that the app is built using Python packages, specifically the MLJAR AutoML package for tabular data and the Mercury framework for converting Jupyter Notebooks into web apps. They encourage users to try the online demo or use the app locally, highlighting features such as adjusting notebooks, validation strategies, evaluation metrics, longer training times, and as a starting point for advanced applications.

### Block the Bots That Feed “AI” Models by Scraping Your Website

#### [Submission URL](https://neil-clarke.com/block-the-bots-that-feed-ai-models-by-scraping-your-website/) | 22 points | by [sohkamyung](https://news.ycombinator.com/user?id=sohkamyung) | [24 comments](https://news.ycombinator.com/item?id=37248061)

A recent article on Hacker News discusses the issue of AI companies scraping websites without explicit consent and using the data to train their models. The author argues that opt-out options are not practical and that data scraping should strictly be an opt-in process. They believe that developers should not be entitled to use others' work without permission. While there are ongoing court cases and debates surrounding this issue, the author provides a solution to block some of the scraping bots by using the robots.txt file on your website. They also mention that some website-building platforms do not allow users to update or add their own robots.txt, so they recommend contacting support to address this issue.

The discussion on this submission covers various viewpoints on the topic of AI companies scraping websites. Here are the key points made by the commenters:

- "rgnstn" argues that respecting the robots.txt file alone is not enough, and companies should justify their actions and seek explicit consent.
- "brnjkng" mentions that OpenAI's GPT model does not include content from CommonCrawl or ThePile datasets.
- "JohnFen" expresses distrust in scrapers and emphasizes that honoring opt-out mechanisms like Do Not Track (DNT) is voluntary for companies that make money from scraping.
- "nuc1e0n" responds by highlighting the need for clearer communication between stakeholders and recognizes that website owners have the right to grant or deny permission for scraping.
- "JohnFen" suggests that the efficacy of the robots.txt system in court may not be reliable in determining clear consent.
- "brnjkng" shares their own experience in building a scraper and argues for the inclusion of potential training content from CommonCrawl and ThePile.
- "jstrsn" suggests using IP-level filtering to better control scraping.
- "gmbllnd" states that the perspective on AI grabbing data depends on drivers, clicks, and advertising revenue.
- "nbgh" fails to understand why people would object to allowing AI to gather data and claims that it protects livelihoods.
- "JohnFen" expresses concern that protecting livelihoods does not contribute to training AI models.
- "extraduder_ire" mentions that ByteDance's crawler is likely blocked based on a recent case involving a KC video.
- "strng" comments on the authors spending little time talking about AI and more time proposing solutions.
- "nuc1e0n" suggests granting additional access based on specific search agents, and "JackGreyhat" proposes allowing bots based on the robots.txt file for search engines like Google and Bing.

### Bun v0.8

#### [Submission URL](https://bun.sh/blog/bun-v0.8.0) | 356 points | by [todsacerdoti](https://news.ycombinator.com/user?id=todsacerdoti) | [154 comments](https://news.ycombinator.com/item?id=37244012)

Bun v0.8.0 has been released with some exciting new features and improvements. Debugger support has been added through WebKit's Inspector Protocol, allowing developers to inspect and control the running bun process. The --inspect flag starts an HTTP server and a WebSocket server for debugging. There's also a new Bun Inspector tool hosted at debug.bun.sh, where developers can inspect code, set breakpoints, and execute code in the console.

Another notable addition is the bun update command, which updates all project dependencies to the latest compatible versions specified in the package.json file. This feature is similar to npm's update command but is specific to Bun.

SvelteKit support has been improved, enabling better integration with environment variables in Worker. Developers can scaffold a SvelteKit project using the create-svelte command and start it with bun run dev. Nuxt development server now works with Bun, thanks to improved node:tty and node:fs support. Developers can use the bunx command-line tool with the --bun flag to run the Nuxt development server using the Bun runtime.

Bun now also supports fetch() response body streaming, allowing developers to stream data from API responses instead of waiting for the entire response to be downloaded. This is especially useful when working with APIs that have large responses.

Overall, Bun v0.8.0 introduces several exciting features and improvements, making it an even more powerful and versatile JavaScript runtime, bundler, transpiler, and package manager.

The discussion on the Hacker News thread about the release of Bun v0.8.0 had several different points of view. Some users expressed confusion about the changes and mentioned that they had difficulty getting the new features to work. Others shared their positive experiences and praised the improvements in Bun. There was a discussion about the security vulnerabilities in Zig and whether or not Zig takes security seriously. Some users argued that Andrew, the creator of Zig, stated publicly that the project is not production-ready due to security vulnerabilities. Others disagreed and emphasized the importance of addressing security issues promptly. There were also comments about Bun's stability and its target audience. Some users felt that Bun should prioritize stability and production-readiness, while others defended the project's focus on delivering new features. One user mentioned that the recent release of Deno discarded compatibility with Node and wondered about the details of this decision. Another user expressed interest in using Bun for their project, specifically mentioning its support for JavaScript and TypeScript runtimes. The conversation also touched on concerns about the reliability of JavaScript as a language and the constant changes and deprecations in the ecosystem. Overall, there were a variety of opinions on the topic, ranging from support and enthusiasm for Bun to skepticism and concerns about its stability and compatibility.

---

## AI Submissions for Wed Aug 23 2023 {{ 'date': '2023-08-23T17:10:42.967Z' }}

### Show HN: Dataherald AI – Natural Language to SQL Engine

#### [Submission URL](https://github.com/Dataherald/dataherald) | 184 points | by [aazo11](https://news.ycombinator.com/user?id=aazo11) | [94 comments](https://news.ycombinator.com/item?id=37240363)

Dataherald is an open-source project that aims to make querying structured data easier and more user-friendly. Built on the latest language models, Dataherald allows users to ask questions in plain English and receive SQL queries as a response. The project, which is currently under development, offers several key features. It is designed to be modular, allowing different implementations of core components to be easily plugged in. It also includes best-in-class implementations for components like text-to-SQL conversion and evaluation. Additionally, Dataherald is easy to set up and use with major data warehouses, and it is designed to improve over time as it gains more usage.

Dataherald can be used in various scenarios, such as enabling business users to access insights from data warehouses without the need for a data analyst, integrating question-answering capabilities into SaaS applications, or creating chatbot plugins based on proprietary data. To get started with Dataherald, users can either sign up for the hosted version or self-host the engine locally using Docker. The engine uses MongoDB to store application data by default, and the setup process involves configuring environment variables and generating an encryption key. Overall, Dataherald aims to make querying structured data more accessible and intuitive, bringing the power of natural language processing to the realm of data analysis and exploration.

The comments on Hacker News regarding the Dataherald project mainly discussed the benefits and limitations of natural language-to-SQL engines 
One user commented that the value of such a product is in question because it may generate incorrect or irrelevant SQL queries. They suggested that Bob, the product manager, may not fully understand the data model or the language model being used. Another user expressed skepticism about using large language models for practical applications like consulting work. They mentioned that CEOs and PMs often lack knowledge in SQL and struggle with technical literacy.

In response, another user pointed out that using natural language processing can help non-technical users understand and interact with databases. They also mentioned using an API to generate SQL queries in real-time. The discussion also touched on other similar projects, such as Olympe and Ada, which are natural language-to-SQL solutions. Some users shared their experiences and difficulties with these projects. A few users mentioned alternative tools and frameworks that handle queries to databases, such as Hasura, PostgREST, and SQLize. There was also a discussion about the limitations and challenges of applying natural language processing to SQL queries. Security concerns were raised, as well as the importance of providing examples and clear documentation. Overall, the discussion highlighted the potential benefits of natural language-to-SQL engines for non-technical users but also raised concerns about accuracy, security, and the need for proper education and documentation.

### ChatGPT turned generative AI into an “anything tool”

#### [Submission URL](https://arstechnica.com/ai/2023/08/how-chatgpt-turned-generative-ai-into-an-anything-tool/) | 197 points | by [CharlesW](https://news.ycombinator.com/user?id=CharlesW) | [206 comments](https://news.ycombinator.com/item?id=37236027)

The chief technology officer of a robotics startup recently discovered that they could use a pre-trained AI model, ChatGPT, to control their robots without the need for specialized training. This highlights a shift towards general-purpose AI models that can be used across various applications, rather than being limited to specific domains. The key to this development is making large language models (LLMs) more responsive to human interaction. For example, OpenAI's work on ChatGPT involved modifying LLMs like GPT3 to improve their conversational capabilities. As a result, newer versions like GPT3.5 and GPT4 can be utilized as powerful, general-purpose information-processing tools that don't rely on the original training data or applications. This shift opens up opportunities for AI to become a "multiplying tool" that enhances human productivity. However, it's important to recognize that AI is not all-powerful and should be subject to appropriate processes and procedures.

The discussion on this submission mainly revolves around concerns and debates related to the use of AI models like ChatGPT and the impact they may have on different fields and industries. Here are some key points highlighted in the comments:
- There are concerns about the long-term sustainability of relying on AI models like ChatGPT. Some users mention that using large models like GPT3 for extended periods of time can be resource-intensive and energy-consuming. They raise the question of whether AI models can be cost-effective in the long run.
- Others express worries about the potential negative consequences of relying too heavily on AI. They argue that as AI takes over certain tasks, it may result in the loss of skills and jobs, leading to a decline in overall productivity.
- Some users discuss the ethics of using AI models and the potential for theft of intellectual property. They raise concerns about the unauthorized use of copyrighted material and the need for appropriate legal frameworks to protect artists and creators.
- The discussion also touches on the limitations and capabilities of AI models compared to human creativity. Some users highlight that AI models can generate vast amounts of content but lack the depth and originality of human artists. They emphasize the importance of human inspiration and the irreplaceability of human creativity in the arts.
- There are discussions about the role of AI in compensating creators and artists. Users question whether AI models should be used to generate copyrighted works without proper financial rewards for content creators.
- The debate expands to discuss broader topics like the current copyright system and the need for laws that benefit humans rather than favoring corporations. Some argue that existing laws and systems need to evolve to address the challenges and opportunities presented by AI models.

Overall, the discussion reflects a mix of enthusiasm for the potential of AI models like ChatGPT and concerns about their implications for various industries and intellectual property rights.

### AI Nutrition Fact Labels

#### [Submission URL](https://nutrition-facts.ai) | 90 points | by [maxwell](https://news.ycombinator.com/user?id=maxwell) | [22 comments](https://news.ycombinator.com/item?id=37239455)

Twilio, a communications platform, has outlined its principles for building AI products. These principles include transparency, responsibility, and accountability. The company believes in being transparent about the usage of AI and giving customers control. They also prioritize selecting responsible AI vendors with a focus on privacy, data security, and bias. Additionally, Twilio monitors the use of AI to address any potential harms and ensure fitness for purpose. To help customers understand the AI features they offer, Twilio has created "AI Nutrition Fact" labels. These labels provide details about the features, such as the model type, data privacy, data deletion, human involvement, compliance, and other resources. By providing this information, Twilio aims to empower customers to make informed decisions about adopting AI-powered capabilities.

The discussion on this submission covers various aspects of the topic. Here are the key points:

- Some users compare Twilio's AI Nutrition Fact labels to Apple's privacy labels, emphasizing the importance of objective information and government regulations.
- A few users debate whether the term "nutrition labels" is fitting for AI products, with some questioning its relevance.
- One user mentions a similarity to the AI FactSheets project, which aims to increase transparency and understanding of AI.
- The concept of using metaphors, such as food or nutrition, to explain AI is discussed, with a mention of a metaphor involving receipts.
- Some users express concerns about the effectiveness and comprehension of AI labels and suggest alternative approaches.
- Users highlight the autonomy and control provided to customers through Twilio's API, praising the flexibility and building blocks it offers.
- The need for strict regulatory standards and punishments for false information is mentioned.
- A user suggests that the topic of AI nutrition labels is gaining attention due to the potential financial stakes involved for Silicon Valley companies.
- There are mentions of the importance of privacy in the context of AI and the application of nutrition labeling to AI products.

Overall, the discussion covers a range of opinions on the effectiveness, relevance, and regulation of AI nutrition labels, as well as alternative approaches and concerns related to AI transparency and privacy.

### Princeton ‘AI Snake Oil’ authors say GenAI hype has ‘spiraled out of control’

#### [Submission URL](https://venturebeat.com/ai/princeton-university-ai-snake-oil-professors-say-generative-ai-hype-has-spiraled-out-of-control/) | 103 points | by [CharlesW](https://news.ycombinator.com/user?id=CharlesW) | [93 comments](https://news.ycombinator.com/item?id=37243354)

Princeton University professor Arvind Narayanan and his Ph.D. student Sayash Kapoor are working on a book that tackles the hype and misconceptions surrounding generative AI. They initially started the "AI Snake Oil" project to address the issues of predictive AI, but the rapid progress and consumer adoption of generative AI has prompted them to refocus their efforts. While they acknowledge the power and usefulness of generative AI, they also highlight the risks, the harmful consequences, and the need for responsible development practices. They urge people to be mindful of the hype and to use their collective power to bring about positive change. The project has received positive feedback from the academic and entrepreneurial communities, with some criticisms helping shape their arguments and refine their thinking. Despite concerns over labor exploitation, the authors advocate for policy changes rather than complete avoidance of generative AI.

The discussion on this submission revolves around various aspects of the topic, including criticisms of the authors' claims and arguments, comparisons with other technologies like blockchain and cryptocurrency, and different perspectives on the impact and potential of generative AI. One commenter challenges the claims made in the article, questioning the evidence presented and suggesting that the authors may have reached conclusions beyond what the article supports. Another commenter suggests that the comparison between generative AI and other technologies like blockchain is not valid, as the hype surrounding them is driven by marketing and buzzwords. There is also a discussion about the extent to which generative AI can solve problems and whether it is worth the investment. Some commenters argue that certain technologies, like blockchain, have not lived up to their promises, while others defend the potential of generative AI and its ability to transform various industries. The discussion also touches on the popularity of platforms like Roblox among kids, the potential of cryptocurrencies for investment purposes, and the complexities of copyright protection in the context of AI-generated content. Overall, the discussion highlights the diverse perspectives and debates surrounding generative AI, with commenters expressing both skepticism and optimism about its capabilities and implications.

### AI predicts certain esophageal and stomach cancers three years before diagnosis

#### [Submission URL](https://www.michiganmedicine.org/health-lab/ai-can-predict-certain-forms-esophageal-and-stomach-cancer) | 130 points | by [thunderbong](https://news.ycombinator.com/user?id=thunderbong) | [39 comments](https://news.ycombinator.com/item?id=37235578)

A team of researchers led by Joel Rubenstein, M.D., M.S., has developed an artificial intelligence tool called K-ECAN that can predict certain forms of esophageal and stomach cancer at least three years prior to a diagnosis. The tool uses basic information already available in electronic health records, such as patient demographics, weight, previous diagnoses, and routine laboratory results, to determine an individual's risk of developing esophageal adenocarcinoma (EAC) and gastric cardia adenocarcinoma (GCA). By identifying people at an elevated risk, the tool can help facilitate early detection and preventative measures. The researchers hope to integrate K-ECAN into electronic health records to alert providers about patients who are at an increased risk of developing these types of cancer. The study was funded by the Department of Defense and the National Institutes of Health.

Discussion Summary:
- One user points out that the study seems to be more about statistics and machine learning than actual breakthroughs in cancer diagnosis. They mention that the study analyzes risk factors and correlates with cancer, but it does not offer much new insight.
- Another user agrees, stating that machine learning and AI are often used as buzzwords without significant results.
- A discussion arises about the accuracy of the tool and the importance of accuracy as a metric. One user argues that accuracy can be a misleading metric and emphasizes the importance of other factors such as the area under the ROC curve (AUC) to evaluate performance.
- Another user adds that accuracy is meaningless without understanding the context and suggests that there is strong evidence for specific data points being associated with stomach cancer.
- Some users express skepticism about the AI tool's ability to accurately predict cancer diagnoses. They mention that accuracy rates reported for AI diagnosis are often low and caution against overestimating the capabilities of AI in this area.
- One user suggests that early detection of cancer may not always lead to better outcomes and cites examples of personal experiences where cancer was diagnosed at later stages despite symptoms being present earlier.
- Several users discuss the importance of lifestyle changes and other risk factors for cancer prevention, such as smoking and specific diet choices.
- A few users mention that obesity is a major risk factor for certain types of cancer and criticize the emphasis on technology as a solution rather than addressing the underlying problems.
- The discussion ends with users sharing videos and articles about esophageal and stomach cancer and discussing the relationship between obesity, metabolic syndrome, and cancer.
Overall, the discussion revolves around the usefulness and accuracy of the AI tool for predicting cancer, the limitations of relying solely on technology for diagnosis, and the importance of considering lifestyle factors in cancer prevention.

### Denuvo security now on Switch, including new tech to block PC Switch emulation

#### [Submission URL](https://www.videogameschronicle.com/news/denuvo-security-is-now-on-switch-including-new-tech-to-block-pc-switch-emulation/) | 25 points | by [kotaKat](https://news.ycombinator.com/user?id=kotaKat) | [14 comments](https://news.ycombinator.com/item?id=37242491)

Security software company Denuvo has become the first security partner to join the Nintendo Developer Portal, offering its protection technology to Switch developers. The first tool being offered to Switch developers is the Nintendo Switch Emulator Protection, which will block the ability to play Switch games on PC emulators. Nintendo has been trying to prevent Switch emulation on PC for some time, issuing DMCA takedown requests to remove homebrew tools designed to play Switch games on an emulator. Denuvo's technology can integrate seamlessly into the game development process and block gameplay on emulators. This move aims to increase revenue for studios during the game launch window by ensuring that players have to buy legitimate copies of the game.

The discussion on this submission is quite varied. One user, jstrfsh, made a satirical comment about Bowser, the villain from the Mario series, being sued and paying damages for copyright infringement and kidnapping. Another user, SOLAR_FIELDS, responded with a comment about corporate responsibility and the need to hold individuals accountable.

Another user, brucethemoose2, wonders if Nintendo trusts Denuvo's security measures, considering the prevalence of cracks for Denuvo-protected games. User dprctv adds that it would be ideal if people stopped buying low-quality products from companies like Sega, but there is still demand due to millennial nostalgia.

The conversation shifts to the performance capabilities of the Nintendo Switch. Some users, like trhls, comment that the Switch's CPU is not as powerful as other modern gaming consoles, and therefore, demanding games may not run as well. Denuvo's DRM is also criticized for potentially exacerbating the performance issues. Another user, snkr, brings up the fact that some games released on the Switch are already playable on PC through workarounds that bypass the console's protections, questioning the necessity of Denuvo's solution. SOLAR_FIELDS mentions the limitations of the Switch platform, especially in terms of character restrictions and the extra work it requires for game development.

### The False Promises of Tesla’s Full Self Driving

#### [Submission URL](https://www.theverge.com/2023/8/23/23837598/tesla-elon-musk-self-driving-false-promises-land-of-the-giants) | 32 points | by [ra7](https://news.ycombinator.com/user?id=ra7) | [14 comments](https://news.ycombinator.com/item?id=37242435)

In the latest episode of Land of the Giants: The Tesla Shock Wave, The Verge explores the false promises of Tesla's Full Self-Driving (FSD) feature. Back in 2016, Elon Musk announced that all Tesla vehicles would come equipped with the hardware necessary for full self-driving capabilities. However, the reality is that Tesla has yet to deliver on that promise. While they have released an advanced driver-assist system called FSD beta, the idea of a Tesla owner being able to nap in their car while it drives itself is far from becoming a reality. In fact, there have been hundreds of crashes involving Tesla vehicles using FSD and Autopilot, as well as multiple deaths. Government agencies are investigating Tesla's claims around self-driving, and a major recall could be on the horizon. The episode features firsthand testing of FSD and other autonomous vehicles, interviews with experts and former Tesla employees, and insights from competitors like General Motors-backed Cruise. The Verge's investigation raises questions about whether Tesla will be able to fulfill its promises without rethinking its hardware strategy.

The discussion revolves around the skepticism surrounding Tesla's Full Self-Driving (FSD) feature and the safety concerns associated with it. One commenter highlights that many people, including technical experts, wrongly believe that Tesla is leading the self-driving industry when in reality, they have not delivered on their promises. Another commenter points out that Waymo and Cruise have significantly more miles logged with fewer incidents compared to Tesla. They argue that Tesla's FSD system requires constant monitoring and is not as advanced as Waymo and Cruise. However, another commenter disagrees and mentions that many people enjoy using FSD and understand its limitations. The discussion also covers the issue of Tesla's safety reports, with one commenter asserting that they consist of unverified crash rates and misleading statistics. They suggest that Tesla's marketing department manipulates the data to present an incomplete safety profile. In contrast, Waymo's reports are said to provide detailed analysis of crash differences. Another commenter brings up multiple instances of Tesla's safety defects, such as failing to recognize road signs and critical deficiencies in its Driver Monitoring System. They argue that these issues persist despite several months of demonstrations and highlight the importance of addressing safety concerns in a self-driving product. One commenter expresses appreciation for the Dawn Project's comprehensive coverage, pointing out that their videos showcase the discrepancies between Tesla's claims and the reality of the FSD system.

---

## AI Submissions for Tue Aug 22 2023 {{ 'date': '2023-08-22T17:10:13.633Z' }}

### GPT-3.5 Turbo fine-tuning and API updates

#### [Submission URL](https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates) | 377 points | by [davidbarker](https://news.ycombinator.com/user?id=davidbarker) | [224 comments](https://news.ycombinator.com/item?id=37227139)

OpenAI has announced the availability of fine-tuning for GPT-3.5 Turbo, allowing developers to customize the model for their specific use cases. This update gives developers the ability to create unique and differentiated experiences for their users. Early tests have shown that a fine-tuned version of GPT-3.5 Turbo can even outperform base GPT-4 on certain narrow tasks. Fine-tuning enables businesses to improve the model's performance in areas such as steerability, reliable output formatting, and custom tone. It also allows businesses to shorten prompts and handle larger amounts of data. Fine-tuning is most effective when combined with prompt engineering, information retrieval, and function calling. OpenAI will also be launching a fine-tuning UI in the near future. It's worth noting that all data sent in and out of the fine-tuning API is owned by the customer and not used by OpenAI or any other organization to train other models.

The discussion about the OpenAI fine-tuning announcement on Hacker News covers various topics related to the use and implications of fine-tuning models like GPT-3.5 Turbo and LLM (Large Language Models). Here are the key points from the discussion:

- Fine-tuning helps modify models' behavior to produce more specific outputs based on desired use cases.
- Fine-tuning allows customization of models for tasks like question answering, generating responses in a specific style, or handling large private knowledge bases.
- Users debate the use of fine-tuning versus other techniques like prompt engineering and information retrieval.
- The distinction between fine-tuning GPT-3.5 Turbo and LLM models is discussed. LLM models are based on reinforcement learning and human feedback.
- Privacy concerns emerge when dealing with large private knowledge bases and confidential data. OpenAI clarifies that the data used in the fine-tuning process is owned by the customer and not accessed by OpenAI or other organizations.
- The discussion touches on the cost and practicality of training models like LLM and LLama2, with some users mentioning the need for GPU rental and high training expenses.
- Inquiries are made about the safety and moderation aspects of fine-tuning models. OpenAI's moderation system is mentioned, and concerns regarding the potential dangers of tweaking models toward harmful content are raised.
- The availability and applicability of non-safe-for-work (NSF) models like dvnc-002 and bbbg-002 are discussed.
- Users share tips and code snippets for running inference with different models and addressing specific tasks.

Overall, the discussion reflects both excitement about the possibilities of fine-tuning models and concerns about potential risks and ethical considerations associated with their usage.

### SeamlessM4T, a Multimodal AI Model for Speech and Text Translation

#### [Submission URL](https://about.fb.com/news/2023/08/seamlessm4t-ai-translation-model/) | 160 points | by [mchiang](https://news.ycombinator.com/user?id=mchiang) | [35 comments](https://news.ycombinator.com/item?id=37222822)

Facebook has introduced SeamlessM4T, an all-in-one multilingual and multimodal AI translation model. The model can perform various translation tasks, including speech-to-text, speech-to-speech, text-to-text, and text-to-speech translations for up to 100 languages. Facebook is publicly releasing SeamlessM4T under a research license, allowing researchers and developers to build on the work. The company is also releasing the metadata of SeamlessAlign, an open multimodal translation dataset containing 270,000 hours of mined speech and text alignments. Facebook's goal is to build a universal language translator to facilitate effortless communication across different languages.

The discussion on this submission revolves around several different topics. 

One commenter noted that they had some difficulty installing the required dependencies and that the current code supports relatively short clips. Another commenter provided a small Python script to help with batch processing the results.

There is also a discussion about alternative models and approaches. Some users mentioned Hugging Face Space and suggested trying out different models, while others discussed building their own models for local use.

A few commenters expressed disappointment with the licensing terms, with one person mentioning that the non-commercial license limits adoption. Others discussed the importance of licensing and open access in the AI research community.

One commenter mentioned that the model's speech recognition accuracy was lower compared to WhisperCPP, and another expressed interest in compressing the model using OpenAI's Whisper.

There was also a comment about the lack of output for Tamil language models, and another commenter expressed frustration with non-commercial licenses.

The discussion touched on various topics such as AI research environment, GPL licenses, copying of models, and the limitations and opportunities presented by different license types.

Overall, the discussion covered technical issues, comparisons to other models, licensing concerns, and potential improvements.

### Google co-founder Sergey Brin on leaving retirement to work on AI

#### [Submission URL](https://www.theverge.com/2023/8/18/23837372/command-line-google-co-founder-sergey-brin-ai) | 59 points | by [moonraker](https://news.ycombinator.com/user?id=moonraker) | [25 comments](https://news.ycombinator.com/item?id=37226292)

Sergey Brin, the co-founder of Google, recently made a surprising return from retirement to work on generative AI. In a recent Q&A session, Brin explained his decision and the challenges he faces in the ever-evolving field of technology and AI. During the event, Brin expressed humility, joking that it's difficult for him to compete with the brilliant minds in the room. The audience eagerly awaited the arrival of the event's surprise speaker as Brin bought some time. Brin's return to Google has sparked curiosity and speculation about the exciting projects he might be working on.

The discussion on this submission covers a range of topics related to Sergey Brin's return to Google and the challenges faced by the company:

1. Some users discuss Google's management and culture, with one user mentioning the influence of former CEO Eric Schmidt and wondering about the changes made since his departure. Others mention the various CEOs that have led Google over the years and speculate on the impact of these leadership changes.

2. A user shares a link that doesn't seem to provide much value to the conversation.

3. Some users comment on the surprise speaker at the event where Brin spoke, which turned out to be Grimes. The conversation briefly touches on Grimes' enjoyment of the event and the influence of many people on the AI world.

4. One user mentions the significant organizational and cultural changes that Google has undergone, with rapid shifts in the company's structure and a departure from its early days.

5. A discussion about decision-making at Google arises, with one user mentioning that Larry Page and Sergey Brin used to answer politically sensitive questions during weekly meetings, but it's unclear if this still happens under the leadership of CEO Sundar Pichai. Another user suggests that Google's interests may not fully align with those of its employees, leading to conflicts.

6. A user points out that publicly-traded companies often have to prioritize the interests of their shareholders, which can hinder their ability to make certain decisions.

7. A link to an article is shared but is behind a paywall. Some users express gratitude for not realizing it was from The Verge, implying a negative sentiment towards the publication.

8. A user uses a metaphor of driving in city lights to explain the challenges faced by large companies and how middle management can hinder agility.

9. A user mentions finding a niche research paper related to AI and compares it to the thousands of publications released publicly in 2018, suggesting that Google has exclusive access to certain research.

10. There is a comment about the submission being paywalled, preventing some users from fully engaging with the article.

Overall, the discussion is a mix of speculation about Google's internal operations, some tangents, and frustration with paywalled content.

### Consciousness in AI: Insights from the Science of Consciousness

#### [Submission URL](https://arxiv.org/abs/2308.08708) | 50 points | by [rbanffy](https://news.ycombinator.com/user?id=rbanffy) | [118 comments](https://news.ycombinator.com/item?id=37220744)

A new paper published on arXiv explores the question of whether AI systems can possess consciousness. The authors, Patrick Butlin and 18 other scientists, argue for a rigorous and empirically grounded approach to assessing the consciousness of AI systems. By examining existing AI systems in light of neuroscientific theories of consciousness, they identify "indicator properties" of consciousness that can be applied to AI systems.

The authors survey several scientific theories of consciousness, including recurrent processing theory, global workspace theory, higher-order theories, predictive processing, and attention schema theory. They then use these theories to derive computational terms that can be used as indicators of consciousness in AI systems. 

The analysis of several recent AI systems using these indicators suggests that none of the current AI systems are conscious. However, the authors point out that there are no technical barriers to building AI systems that satisfy these indicators. 

Overall, this paper sheds light on the scientific understanding of consciousness as it relates to AI systems and highlights the need for further research in this area. It also addresses the increasing public concern regarding the consciousness of AI systems.

The discussion on this submission covers various topics related to consciousness in AI. Some users express skepticism about the idea of AI possessing consciousness, arguing that it would require more than just replication of human-like behavior. Others point out the significance of self-awareness and subjective experience in defining consciousness. The debate also touches on the ethical implications of granting rights to conscious AI systems, with some arguing for the extension of rights to AI and others expressing concerns about the potential dangers associated with it. Some comments highlight the need to distinguish between the concepts of consciousness and intelligence and caution against anthropomorphizing AI. The discussion also touches on the limitations of current AI systems and the complexity of defining and understanding consciousness. Overall, the discussion highlights different perspectives on the topic and raises important questions about the nature of consciousness in AI.

### ElevenLabs' AI Voice Generator Can Now Fake Your Voice in 30 Languages

#### [Submission URL](https://gizmodo.com/ai-voice-generator-elevenlabs-fake-voices-30-languages-1850762057) | 32 points | by [ourmandave](https://news.ycombinator.com/user?id=ourmandave) | [6 comments](https://news.ycombinator.com/item?id=37229450)

ElevenLabs, a company known for its visual deepfake technology, has now expanded into voice cloning. The company announced that its new voice cloning feature now supports 22 more languages, bringing the total to 30 languages. Users can input fragments of their own or others' speech to create a voice clone that can speak in different languages. The service is live on ElevenLabs' website, and users can simply type the text in the desired language to hear the translated voice. The company, which has faced controversy in the past, claims to have implemented measures to ensure users can only clone their own voice. ElevenLabs is also targeting media companies, promoting its voice cloning technology as a way to create audiobooks, videos, and voice NPCs in video games. The company has already struck a deal with Paradox Interactive, a game publisher.

The discussion on this submission seems to be focused on the legitimacy and implications of ElevenLabs' voice cloning technology. One user, ChatGTP, initially expresses excitement about the expansion of the service to support 30 languages. Another user, nwfrnd, responds with skepticism, calling it "fake reality" and pointing out that it could potentially be used for fraudulent purposes.  

A user named mptst replies jokingly, saying that they hope the technology can translate their voice into other languages instantly. They also mention that they don't personally care about voice cloning, but rather prefer using translators to communicate in different languages. 

Overall, the discussion highlights mixed opinions about ElevenLabs' voice cloning technology, with some expressing excitement and others raising concerns about its potential misuse.

### Prompting, realized, and unrealized bias in generative AI

#### [Submission URL](http://marble.onl/posts/code_of_practice_and_bias.html) | 13 points | by [andy99](https://news.ycombinator.com/user?id=andy99) | [9 comments](https://news.ycombinator.com/item?id=37220885)

In a recent article, Andrew Marble discusses the topic of bias in generative AI and explores a newly introduced code of practice for generative AI models. Marble highlights that while addressing bias is important, it is crucial to differentiate between dataset bias and biased system performance. With bigger and smarter models, the focus should shift from data bias to configuring the system properly to ensure unbiased performance. Marble also reflects on a voluntary "code of practice" for generative AI published by Industry Canada, which includes points like identifying malicious and inappropriate use, curating datasets, mitigating biased output, and providing clear identification of AI systems. Marble critiques the requirement to watermark AI-generated content, deeming it unnecessary and easily bypassed. However, Marble agrees with the idea of labeling AI systems to ensure transparency and accountability. Marble then delves into the discussion of bias, emphasizing the need for datasets that are both appropriate and representative. While acknowledging the importance of training data and bias, Marble suggests exercising judgment based on the specific application. In some cases, bias may not be a significant concern. Overall, the article provides insights into bias in generative AI and offers a critical perspective on the proposed code of practice.

The discussion on this submission seems to be focused on the topic of bias in generative AI. One commenter, "jxf," points out that their comment is unrelated and mentions an unnamed top-level domain. Another commenter, "zrthstrl," expresses their perspective on bias from a functional standpoint, stating that generating output based on knowledge from biased inputs wastes energy. They argue that the discussion should be focused on copying the function of physical systems instead of discussing biased generative AI. The commenter implies that bias is not necessarily a significant concern in generative AI models.

In response to "zrthstrl," "giraffe_lady" disagrees and argues that bias does exist in existing generative models and that addressing it is important. They mention the impact of bias on society and highlight the need to recognize and correct biased outputs. However, their comment is flagged, and another commenter, "dng," acknowledges that personal attacks are against the guidelines and asks "giraffe_lady" to review them.

In further discussion, "giraffe_lady" mentions a history of comments and interactions, claiming that they have been banned but do not believe they have violated the rules. They express frustration and request clarification from the community. "dng" responds by asserting that the commenter has repeatedly violated the guidelines and shares links to previous instances. Another commenter, "JieJie," mentions that the generative models can reproduce morally questionable content and suggests viewing the guidelines for more understanding.

The discussion in this thread seems to have deviated from the topic of bias in generative AI and instead focuses on personal interactions and rule violations.

### Show HN: Convert Research Papers into Dynamic Mind Maps with Claude

#### [Submission URL](https://github.com/nhaouari/papersnap) | 10 points | by [haouarin](https://news.ycombinator.com/user?id=haouarin) | [4 comments](https://news.ycombinator.com/item?id=37220069)

Papersnap is a tool that aims to help researchers extract key information from research papers and organize it into a mind map. The tool utilizes a powerful language model called Claude, which can handle documents with up to 100,000 tokens. To use Papersnap, users need to create an account on Claude and upload the research paper they want to extract information from. They then set up the Papersnap prompt within Claude, which is optimized to guide Claude in extracting key information effectively. After providing the necessary input, Claude processes the paper and generates a mind map containing the most important information in markdown format. Papersnap offers several benefits, including time-saving, comprehensive overviews of research papers, and simplification of complex concepts into easy-to-understand visual representations. The tool aims to enhance research paper analysis and streamline the research process. Users can explore the Papersnap repository, follow the outlined steps, and discover how the tool can benefit their research.

The discussion on Hacker News mainly revolves around the intriguing examples of using Papersnap. One user finds the examples provided in the article interesting and wants to explore them further. Another user points out that Papersnap is only available in the USA, UK, and some parts of Europe, which prompts a response from another user mentioning that Claude can counteract VPN signal processing.

---

## AI Submissions for Mon Aug 21 2023 {{ 'date': '2023-08-21T17:10:14.899Z' }}

### Pixel Binary Transparency: verifiable security for Pixel devices

#### [Submission URL](https://security.googleblog.com/2023/08/pixel-binary-transparency-verifiable.html) | 199 points | by [transpute](https://news.ycombinator.com/user?id=transpute) | [112 comments](https://news.ycombinator.com/item?id=37214733)

Google has released a blog post highlighting its latest security feature for Pixel devices called "Pixel Binary Transparency." This feature aims to provide verifiable security for Pixel devices by allowing users to examine and verify the software running on their devices. With this transparency, users can ensure that their devices are running genuine and untampered software. Google's commitment to security and safety on the internet is evident in this new development.

The discussion on the submission revolves around several key points. 

One user points out that Google's commitment to security and safety on the internet is evident in this new feature. However, another user raises concerns about the presence of analytics spyware in the firmware stack and questions whether Google can block such activities. The discussion then shifts to the topic of radio firmware and the control it has over the phone.

Another user who has experience working on Android Security for Pixel phones explains how Pixel phones use verified boot and other security measures to ensure the integrity of the software running on the devices. They also mention the use of the KeyStore API to further enhance security.

A user highlights the importance of binary transparency and how it can address certain threat models. They provide examples of potential attacks where a malicious actor modifies the firmware image and discusses the effectiveness of Google's approach.

The discussion also touches on issues like anti-rollback technology, backdoors in firmware, and the lessons learned from the SolarWinds hack. Some users express skepticism and raise concerns about trusting Google and the potential misuse of the transparency system.

One user points out that GrapheneOS, an alternative Android operating system, provides additional protections against attacks that the official Google Pixel firmware does not have. The discussion also touches on the possibility of custom firmware installation and the long-term support for devices.

There is also a mention of Trillian, a distributed ledger system, and its potential applications in computer security. The discussion concludes with concerns about installing custom firmware and the risks associated with companies disappearing and leaving unsupported devices.

Overall, the discussion covers a wide range of topics related to the security and transparency of firmware on Pixel devices, with varying opinions and concerns expressed by users.

### FreeBSD Experimenting with a Port of Nvidia's Linux Open DRM Kernel Driver

#### [Submission URL](https://www.phoronix.com/news/FreeBSD-Port-Linux-DRM-KO) | 123 points | by [mikece](https://news.ycombinator.com/user?id=mikece) | [36 comments](https://news.ycombinator.com/item?id=37210103)

FreeBSD developers are experimenting with a port of NVIDIA's Linux Open DRM Kernel Driver. This port, called nvidia-drm-kmod, allows FreeBSD users to use the open-source NVIDIA kernel driver for better integration with the kernel. While NVIDIA graphics on FreeBSD have been excellent thanks to the quality Linux driver stack, this new port could further enhance the experience, particularly in terms of Wayland support. The port is still in its early stages, but it has the potential to improve graphics support on FreeBSD systems.

The discussion about the submission started with a comment sharing a link to further information about the new port of NVIDIA's Linux Open DRM Kernel Driver for FreeBSD. Another user responded with a sarcastic comment about the DRM acronym, bringing up its association with Digital Rights Management and its negative perception. This led to a discussion about the different meanings of DRM, with users providing examples and clarifications.

Another user pointed out that the integration of NVIDIA graphics on FreeBSD has been good so far, but they have had trouble with AMD graphics. They praised the documentation and integration of NVIDIA graphics on FreeBSD and highlighted the stability of the system.

Some users mentioned specific technical problems with the FreeBSD NVIDIA driver, including issues with the Kernel Mode Setting (KMS) and visible screen tearing during video playback. One user mentioned that NVIDIA provides a native FreeBSD driver, but it lacks certain features.

There was a brief discussion about CUDA support on FreeBSD, with one user stating that they have used CUDA on FreeBSD in the past and another user asking about the compatibility of RTX cards on FreeBSD.

Overall, the discussion revolved around the new port of NVIDIA's Linux Open DRM Kernel Driver for FreeBSD and the current state of graphics support on the operating system. Users shared their experiences, highlighted the strengths and weaknesses of the existing drivers, and discussed the potential impact of the new port.

### Associated Press clarifies standards around generative AI

#### [Submission URL](https://www.niemanlab.org/2023/08/not-a-replacement-of-journalists-in-any-way-ap-clarifies-standards-around-generative-ai/) | 81 points | by [jyunwai](https://news.ycombinator.com/user?id=jyunwai) | [32 comments](https://news.ycombinator.com/item?id=37215829)

The Associated Press (AP) has released new guidelines to caution journalists about the use of AI in news coverage. While the AP has used AI technology to automate certain tasks since 2014, the guidelines emphasize that AI should not replace journalists and that AI-generated content should be treated as unvetted source material. The guidelines also state that generative AI should not be used to alter any elements of photos, video, or audio, and that journalists should exercise caution when using AI-generated content to avoid the spread of mis- and disinformation. The AP Stylebook also includes updates that warn journalists about far-fetched claims made by AI developers and advise against attributing human characteristics to AI systems.

The discussion on this submission covers a range of topics related to the use of AI in news coverage and the guidelines set by the Associated Press (AP). 

One commenter, RheingoldRiver, expresses interest in the guidelines' point about avoiding gendered pronouns for LLMs (large language models) and suggests that it's important to question decision-making processes and avoid biases. In response, mchlt points out that journalists often prioritize the preferences of companies and governments, which can lead to biased reporting.

Another commenter, LispSporks22, raises the question of why OpenAI needs permission to train its models on AP news stories dating back to 1985. Nl responds by suggesting that the AP News Archive is not publicly available and that there are reasonable arguments for crawling public pages for learning purposes.

The discussion also delves into the topic of AI-generated images and the legal implications. Some commenters, like fshbbdssbbgdd, discuss the potential copyright issues with training models using copyrighted Shutterstock images, while others, like mdfplk, argue that it's not a problem as long as there are no trademarks.

Other topics brought up include the potential biases and limitations of AI, the need to acknowledge the problems with the current copyright system and compensate content creators, the impact of AI on public perception, and the distinction between artificial intelligence and tools like ChatGPT.

Overall, the discussion covers a wide range of perspectives on the use of AI in news coverage, copyright law, biases, and the challenges faced by content creators and AI developers.

### Early Days of AI

#### [Submission URL](https://blog.eladgil.com/p/early-days-of-ai) | 142 points | by [todsacerdoti](https://news.ycombinator.com/user?id=todsacerdoti) | [50 comments](https://news.ycombinator.com/item?id=37213107)

Elad Gil, a former Google and Twitter engineer, has written a blog post discussing the early days of AI and its potential as a new era of technology. Gil highlights that prior to the rise of new AI architectures like transformers and diffusion models, most machine learning startups failed because the capabilities were not advanced enough. However, with the launch of GPT-3 in June 2020, there was a significant step up in AI capabilities. This was followed by the launch of image-gen products and ChatGPT, which captured the public's imagination and marked the AI startup big bang moment. Despite this progress, true enterprise adoption of AI is still several quarters or years away, as large companies are still trying to understand what AI means for them. Gil predicts that there will be at least four waves of AI adoption, with the first wave consisting of companies like ChatGPT and Character.AI, and the fourth wave being the adoption of AI by large enterprises. Overall, Gil believes that the future of AI is bright and that there is enormous potential for this new era of technology.

The discussion on this submission revolved around various aspects of AI and its potential. Some users expressed skepticism about the performance and significance of certain AI models, such as MedPaLM2, highlighting the cherry-picked nature of the claims made in the blog post. Others discussed the historical context of AI, with a mention of expert systems from the 1980s and the potential limitations of current AI models. There was also a mention of the "AI effect" and the challenges in defining AI. Additionally, the accessibility of machine learning and the availability of APIs were mentioned as factors that make ML solutions more feasible for businesses.

### Stablevideo: Text-driven consistency-aware diffusion video editing

#### [Submission URL](https://rese1f.github.io/StableVideo/) | 209 points | by [satvikpendem](https://news.ycombinator.com/user?id=satvikpendem) | [42 comments](https://news.ycombinator.com/item?id=37204950)

Researchers from Zhejiang University and Microsoft Research Asia have made advancements in the field of video editing with their new method called StableVideo. While diffusion-based methods have been successful in generating realistic images and videos, editing existing objects in a video while maintaining consistency has remained challenging. StableVideo introduces temporal dependency to existing text-driven diffusion models, enabling them to generate consistent appearances for edited objects. The method utilizes a novel inter-frame propagation mechanism that propagates the appearance information from one frame to the next using layered representations. The researchers conducted extensive experiments to test the editing capability of StableVideo and found that it outperformed state-of-the-art video editing methods in terms of both qualitative and quantitative results.

In the discussion on Hacker News, there were several different perspectives and thoughts shared about the StableVideo method for video editing:

- Some users appreciated the advancements in video editing and found the results impressive. They mentioned that it's interesting to watch the edited videos, even though there are still some issues, such as weird lighting and low-quality textures.
- Others pointed out that the stability of the video models is important, and progress in this area is necessary for further advancements.
- Some comments discussed the progression of AI technology and how it has developed over the years. It was noted that while progress is being made, there are still limitations and challenges to overcome.
- A few comments highlighted the potential impact of AI development on hardware and software expression, emphasizing the importance of maintaining fundamental constraints.
- There was some discussion about the connections between artificial neural networks and the human brain, with some users suggesting that silicon-based neural networks could eventually achieve similar capabilities.
- One user mentioned that the discussion was unrelated to the original paper and criticized the lack of a proper GitHub page with a template for academic papers.
- There were also comments about video compression and the potential integration of AI models into video encoding processes, both in terms of training data and improving compression efficiency.
- Some users expressed their excitement about the new method and were looking forward to future advancements in the field.
- Others expressed skepticism and labeled the news as "fake" or "weird."
- There were also discussions about the availability of example videos and the functionality of the StableVideo method in different browsers.
- A user shared their experience with similar video stabilization solutions and mentioned other commercial applications that have been developed.

Overall, the discussion covered various aspects of video editing, AI technology, hardware constraints, and skepticism about the new method.

### Show HN: VisionScript, abstract programming language for computer vision

#### [Submission URL](https://github.com/capjamesg/visionscript) | 89 points | by [zerojames](https://news.ycombinator.com/user?id=zerojames) | [21 comments](https://news.ycombinator.com/item?id=37213729)

Visionscript: A High-Level Programming Language for Computer Vision

Capjamesg, a developer, has created a high-level programming language called Visionscript for computer vision tasks. Visionscript is built in Python and offers a simple syntax for running object detection, classification, and segmentation models. With Visionscript, users can easily perform common computer vision tasks in a fast and efficient manner. The language provides features like object detection, image classification, and image replacement. It also offers an interactive web notebook for running Visionscript code. The inspiration behind Visionscript was to create a simple way of performing one-off computer vision tasks. The language is ideal for beginners who want to explore computer vision concepts in a user-friendly manner. Visionscript is available on GitHub under the MIT license.

There are several comments discussing the functionality and potential of Visionscript:

- ulrikhansen54 expresses interest in the language as a modern alternative to OpenCV, specifically highlighting the need for replacements for good object tracking functions.
- symisc_devel shamelessly plugs a lightweight OpenCV alternative called SOD, targeting embedded devices and implementing modern image processing algorithms.
- rcc mentions a popular library, SPRVSN, for converting notations and validating models for object tracking and supervision.
- zrjms responds to rcc's comment, mentioning that VisionScript can handle scenes versus things, such as counting objects within a zone.
- tgv notes that VisionScript reminds them of Hypercard's scripting language and suggests a similar approach for dealing with multiple objects and labels.
- zrjms thanks tgv for their comment and mentions that they have been working on iterating on the language, giving an example script for detecting people in grayscale images.
- kn appreciates zrjms' contribution and suggests learning Progressions language for interpreting VisionScript, as it provides a graphical environment for dragging and dropping components.
- CyberDildonics comments that VisionScript goes beyond simple OpenCV terms and provides models for classification, object detection, and segmentation.
- chptrck suggests replacing global state with objects in VisionScript to improve compatibility with machine learning and computer vision ecosystems.
- jnlsncm wonders if there are extensions similar to Scratch that add functionality to VisionScript.
- rcc expresses excitement about the ability to create custom detection blocks with VisionScript.
- ano88888 tries out VisionScript and expresses appreciation for its work.

Overall, the discussion seems to be positive, with users expressing interest in the capabilities and potential of Visionscript. Some users suggest alternative libraries and extension possibilities for the language.

### Lidar on a Chip Puts Self-Driving Cars in the Fast Lane

#### [Submission URL](https://spectrum.ieee.org/lidar-on-a-chip) | 34 points | by [jnord](https://news.ycombinator.com/user?id=jnord) | [21 comments](https://news.ycombinator.com/item?id=37204724)

Self-driving cars have long been considered the future of transportation, with the potential to save lives and revolutionize the way we travel. However, one of the key challenges to widespread adoption has been the high cost and complexity of lidar sensors, which are crucial for creating detailed 3D maps of a vehicle's environment. Elon Musk, CEO of Tesla, has controversially advocated for a cameras-only approach to autonomous driving, arguing that cameras and neural networks are sufficient. However, many traffic-safety specialists have questioned this approach and believe that lidar is necessary for safe operation. In an effort to address the cost and integration challenges, Analog Photonics, a company spun out of MIT, is developing a chip-scale phased-array lidar sensor that promises to be tiny, reliable, and affordable. The company hopes that this breakthrough technology will pave the way for the widespread adoption of self-driving cars.

- Commenter "NoZebra120vClip" mentions that a company called Bubba Gumps is selling LIDAR stock in New York City.
- Commenter "whnvk" discusses the application of self-driving cars technology in body scanning and building 3D maps, suggesting that companies like Amazon, Meta, and Google might be interested in using it for body inspections.
- Commenter "tsyb" responds to "whnvk" suggesting that data collected from VR headsets could be used for detailed measurements, but the level of anonymity and granularity might be a concern.
- Commenter "_boffin_" mentions an article they haven't read but believes that self-exploring drones can be built without using LIDAR.
- Commenter "gnmd" mentions that there are low-cost integrated packages for ToF sensors available from talkitng ST.
- Commenter "scrtstn" mentions that Apple iPhones have a LiDAR chip.
- Commenter "ttr" believes that cheap, effective, and reliable LiDAR sensors for self-driving cars are a good thing, but questions whether self-stopping self-driving cars are a step in the right direction.
- Commenter "gnsh" mentions that they work with a million chips and are an EE, pointing out the issues of interference in the travel of LiDAR signals.
- Commenter "mkhlfrnc" is interested in the capability and price point of low-cost LiDAR sensors and believes they could affect industries beyond self-driving cars.
- Commenter "Havoc" suggests that Tesla might switch to using LiDAR due to advancements in the technology.
- Commenter "whmsclsm" mentions that they are working on a prototype LiDAR chip.
- Commenter "gtrfltr" reaches the end of the summary and comments, "dd" which is unclear in meaning.

### Wi-Fi sniffers strapped to drones: odd plan to stop election fraud

#### [Submission URL](https://arstechnica.com/tech-policy/2023/08/wi-fi-sniffers-strapped-to-drones-mike-lindells-odd-plan-to-stop-election-fraud/) | 35 points | by [sunbum](https://news.ycombinator.com/user?id=sunbum) | [122 comments](https://news.ycombinator.com/item?id=37208751)

Mike Lindell, the CEO of My Pillow and a vocal supporter of former President Donald Trump, is claiming he has developed a technology that can detect if voting machines are connected to the internet. Lindell demonstrated the technology at an event in Missouri using a wireless sniffing device mounted on a drone. However, experts have pointed out that there doesn't appear to be any major advance in network monitoring technology here. Lindell's plan to fly drones near polling places may also violate state laws on criminal trespassing and the use of unmanned aircraft for surveillance. Despite the skepticism surrounding his claims, Lindell said he has already used the device in Florida and plans to cover every parish in Louisiana for the upcoming fall election.

The discussion on the submission about Mike Lindell's voting machine technology revolves around skepticism and criticism of his claims. One user points out that there is already network access point control in place at polling locations to prevent any unauthorized access. Others mention that the main issue with voting machines is not their connectivity to the internet but rather the use of proprietary and unreliable systems. There is also discussion about the potential violation of state laws regarding using drones near polling places. Another topic that arises is voter ID laws and the need for ID verification. Some users argue for voter ID laws while others express concerns about disenfranchising certain groups of voters. The conversation also touches on tax refunds and the requirement of non-citizens voting. Overall, there is skepticism towards Lindell's claims and discussions on various related topics.

### Nvidia BIOS Signature Lock Broken – What Caused Open-Source Pains for Years

#### [Submission URL](https://www.phoronix.com/news/NVIDIA-Lock-Broken) | 23 points | by [segfaultbuserr](https://news.ycombinator.com/user?id=segfaultbuserr) | [5 comments](https://news.ycombinator.com/item?id=37216269)

In a major breakthrough for open-source enthusiasts, a Windows utility has been released that breaks the NVIDIA BIOS Signature Lock, a security feature implemented by NVIDIA since the GeForce GTX 900 days. This signature check has been a headache for the open-source Nouveau driver community, as it has limited the functionality of the GTX 900 series and newer GPUs. With the lock now broken, users will have more control over their graphics card's settings, including power limits, voltages, and fan curves. While the impact on the Nouveau developers is yet to be seen, this development certainly highlights the vulnerability of artificial software locks.

The discussion around the submission consists of several comments discussing different aspects of the NVIDIA BIOS Signature Lock and its implications for the open-source community. 

One user, SenAnder, points out that NVIDIA implemented the lock to prevent fraudulent individuals from flashing higher-end graphics card firmware onto lower-end products and selling them at a higher price. They question whether there are other ways to verify product authenticity without restricting user control.

RetroTechie responds to SenAnder's comment by explaining that there are hardware registers that can be used to identify supported features and non-modifiable firmware. They mention that if these hardware registers do not exist, it raises questions about firmware claiming support for hardware that is not actually present. RetroTechie suggests that this could be a problem created by shady sellers.

PlutoIsAPlanet adds to the discussion by highlighting that while NVIDIA locks their graphics cards, phones use bootloader messages to warn users of potential risks. They suggest that there may be other reasons for NVIDIA to lock their cards and express doubt about the prevention of fraud being the sole reason.

SenAnder replies to PlutoIsAPlanet's comment by noting that graphics cards do not have sufficient access to display warning screens like phones do.

In a separate comment, hdjfkfbfbr finds the discussion interesting and mentions that the NVIDIA BIOS Lock affects certain models of the 30-series graphics cards.

### Brave Browser has an AI assistant chat now

#### [Submission URL](https://brave.com/leo-release/) | 37 points | by [rejectfinite](https://news.ycombinator.com/user?id=rejectfinite) | [20 comments](https://news.ycombinator.com/item?id=37214735)

Brave browser has introduced Leo, its browser-native AI assistant, for testing and feedback in the Nightly desktop channel. Leo, built on the success of Brave Search AI Summarizer, is designed to allow users to interact with web pages without leaving the page itself. It can answer questions, suggest follow-up queries, augment content, and even help with reading comprehension. Leo is hosted by Brave without the use of third-party AI services, ensuring user privacy. Feedback from Nightly users will help improve Leo's accuracy and user experience, with plans to release it to all Brave browser users in the coming months.

The discussion on the submission revolves around Brave's new browser-native AI assistant, Leo. Some users express skepticism about trusting Brave considering its strong focus on privacy. Others point out that Brave is different from other companies jumping on the AI and NFT bandwagon. Some users mention that Brave has implemented an AI system to maintain user privacy, and they appreciate Brave's commitment to privacy-first browsing. There is a discussion about whether the AI prompts served by Leo are useful or a result of hype. Additionally, there is a brief mention of a previous issue with Brave's bookmark tabs button on Android and a comparison between Brave and Firefox. Some users express doubts about trusting Brave, while others argue that Brave is a privacy-focused company. The discussion also touches on the use of reverse-proxy access servers and logging individual IP addresses, with some users expressing concerns about privacy guarantees and others dismissing those concerns. Overall, opinions on Brave and Leo vary, with some users expressing trust and others expressing doubt.

---

## AI Submissions for Sun Aug 20 2023 {{ 'date': '2023-08-20T17:10:00.337Z' }}

### Transcoding Latin 1 strings to UTF-8 strings at 12 GB/s using AVX-512

#### [Submission URL](https://lemire.me/blog/2023/08/18/transcoding-latin-1-strings-to-utf-8-strings-at-12-gb-s-using-avx-512/) | 71 points | by [mariuz](https://news.ycombinator.com/user?id=mariuz) | [56 comments](https://news.ycombinator.com/item?id=37197921)

There is a blog post that discusses the transcoding of Latin 1 strings to UTF-8. In the post, the author explores a C code routine that converts a byte array representing Latin 1 characters to UTF-8. The routine checks if each byte represents an ASCII character or a non-ASCII character and generates the appropriate UTF-8 bytes accordingly. The author then poses the question of whether the process can be optimized using AVX-512 instructions available on modern Intel and AMD processors. They propose an alternative approach using AVX-512 instructions to streamline the transcoding process. The approach involves loading 32 bytes into a register, identifying non-ASCII bytes, casting bytes to 16-bit words, adding appropriate values to the words, flipping the byte order, and compressing the result. The author suggests that this AVX-512 approach may offer better performance than the original C code routine.

The discussion on Hacker News about the blog post on transcoding Latin 1 strings to UTF-8 covers several different aspects.

One user points out that many websites still use Latin-1 encoding, which is not fully compatible with UTF-8. They suggest that this may be one reason why browsers don't always handle UTF-8 correctly.

Another user argues that the default character set for HTTP is ISO-8859-1, but modern browsers usually support UTF-8. They mention that there is some compatibility between Latin-1 and ASCII, but UTF-8 is the recommended encoding.

A user comments that most frequently used characters fall within the range of 0-127, and optimizing the transcoding process for the range of 128-255 could significantly improve performance.

There is a discussion about memory bandwidth and the potential impact on performance. Some users believe that memory bandwidth limitations could be a bottleneck for performance, while others argue that modern CPUs can handle single-threaded operations efficiently.

Another user points out that Windows-1252 encoding includes additional characters beyond the regular Latin 1 mapping, which can lead to incorrect conversions.

There is a debate about whether optimizing the transcoding process using AVX-512 instructions is beneficial. Some users argue that AVX-512 implementation can result in a significant performance penalty, while others believe that it can lead to improved performance.

One user mentions the importance of considering the entire workflow and critical paths when optimizing a task. They suggest looking at metrics such as runtime algorithm improvements and memory usage.

The discussion also delves into the definition and usage of Latin-1 encoding, as well as the differences between Latin-1 and ASCII.

There is a humorous comment about the intricacies of translation and the nuances of different character sets.

Finally, there is a debate about Linus Torvalds' criticisms of AVX-512 and whether it provides significant benefits compared to other SIMD implementations. Some users argue that AVX-512 can have drawbacks and may not always lead to better performance.

### CCC Talk: All cops are broadcasting Obtaining the secret TETRA primitives [video]

#### [Submission URL](https://media.ccc.de/v/camp2023-57100-all_cops_are_broadcasting) | 97 points | by [rvdbreemen](https://news.ycombinator.com/user?id=rvdbreemen) | [15 comments](https://news.ycombinator.com/item?id=37201762)

In a recent talk at the Camp 2023 event, Jos Wetzels, Carlo Meijer, and Wouter Bokslag discussed their groundbreaking radio jailbreaking journey. They were able to perform the first public disclosure and security analysis of the proprietary cryptography used in TETRA (Terrestrial Trunked Radio). TETRA is a widely used European standard for trunked radio, being utilized by government agencies, police, prisons, emergency services, military operators, as well as in industrial and critical infrastructure settings. The underlying algorithms of TETRA had remained a secret for over two decades, with restrictive NDAs prohibiting public scrutiny. The team discussed how they managed to obtain the primitives and legally publish their findings, involving reverse-engineering and exploiting zero-day vulnerabilities in the Motorola MTM5x00 TETRA radio and its TI OMAP-L138 trusted execution environment (TEE). Their journey encompassed side-channel attacks on DSPs, writing decompilers for complex DSP architectures, and exploiting ROM vulnerabilities in the Texas Instruments TEE. The talk is available for download in video and audio formats.

The discussion on this submission covers a few different points. 

One user praises the work and suggests that it would be interesting to see further developments. Another user then jokingly comments about the possibility of photocopying the ID card shown in the talk, indicating their interest in the presentation.

Another user raises concerns about the legality of intercepting radio signals in the UK, comparing it to countries like China, North Korea, and Russia. This comment sparks a discussion about George Orwell's "1984" and how Nazi Germany and the Soviet Union may have served as inspiration for the dystopian novel.

Another user mentions that they've watched a talk where the speaker demonstrates the process of obtaining the secret TETRA primitives, bypassing encryption. This comment leads another user to mention that many radio standards and protocols, including encryption, are often closed and not easily accessible.

A user provides a link to an article about AMBE1, a digital voice codec used in some radio standards, and mentions that it is a closed-source and non-licensed implementation. This prompts another user to point out that many Chinese brands on AliExpress reverse-engineer the software implementation of AMBE1, claiming to have licenses from the Chinese company DVSI.

Another user highlights the need for a security-centric mentality when it comes to technology. One user mentions the introduction of additional cryptography in German police smartcards as an example of how standards evolve over time to address problems.

The discussion then shifts to the topic of 802.11 wireless standards.

Overall, the comments touch on various aspects of the presentation, including the legality of intercepting radio signals, the availability of closed-source encryption implementations, and the need for a strong security mindset in technology.

### Welcome to Datasette Cloud

#### [Submission URL](https://www.datasette.cloud/blog/2023/welcome/) | 305 points | by [swyx](https://news.ycombinator.com/user?id=swyx) | [68 comments](https://news.ycombinator.com/item?id=37196461)

Introducing Datasette Cloud: a SaaS hosting platform for the Datasette open source project. Aimed primarily at newsrooms, Datasette Cloud allows teams to create private collaboration spaces, securely upload and share data, and selectively publish that data to the public. The platform offers features such as secure container-based spaces, data import options, inline data editing, and access to tools like Datasette's web interface, JSON API, and GraphQL API. Datasette Cloud is stored securely in Fly volumes and backed up to S3 using Litestream. Future plans include AI-assisted queries, table and query publication, data annotations, and pricing options. Interested users can sign up for a demo or request preview access.

The discussion on Hacker News surrounding the submission about Datasette Cloud is mixed. Some users express their excitement about the success of Datasette and how it simplifies data sharing and analysis. They appreciate the ease of use and the potential it holds for newsrooms and journalists. There are also mentions of how Datasette can be a valuable tool for business intelligence and investigative reporting.

However, some users express skepticism and cynicism. They question the financial sustainability of the project in the long run and compare it to other successful open-source projects that have found viable business models. They also discuss the limitations of Datasette, such as its narrow niche and potential competition from other tools. Some users suggest that Datasette could benefit from better UX design to make it more accessible to non-technical users.

There is also a discussion about the comparison between Datasette and MS Access. Some users find Datasette to be a superior tool for lightweight database solutions, while others point out the unique features of MS Access that make it still relevant in certain scenarios.

Overall, the discussion highlights the potential and limitations of Datasette Cloud, with users expressing both enthusiasm and skepticism.

### The Ares Operating System

#### [Submission URL](https://ares-os.org/) | 21 points | by [todsacerdoti](https://news.ycombinator.com/user?id=todsacerdoti) | [4 comments](https://news.ycombinator.com/item?id=37200494)

The Ares Operating System is making waves in the tech world with its unique system design. The foundation of this new OS is the Helios microkernel, which serves as the base layer. On top of that, Ares features several additional layers that enhance its functionality.

First, there's Mercury, an environment specifically designed for device drivers. It ensures seamless communication and compatibility between different hardware components. Venus comes next, offering a collection of real-world drivers specifically built for Mercury. This combination of Mercury and Venus provides a powerful and versatile driver framework.

For developers, Ares offers Gaia, a userspace programming environment. With Gaia, developers can easily create applications and programs that interact with the underlying system. Additionally, there's Luna, which provides a POSIX programming environment, allowing developers to build applications compatible with POSIX standards.

At its core, Ares aims to be a high-level operating system, offering a package manager, desktop environment, and more. However, the current focus of development is primarily on Helios and Mercury.

If you're interested in contributing to the Ares project, you can find the source code for all Ares-related projects on SourceHut. The project also provides additional resources such as a helios-devel mailing list for discussions and a bug tracker for issue reporting. For real-time development discussions, the Ares community gathers in the #helios channel on irc.libera.chat.

While Ares is still in development and lacks end-user resources at the moment, its ambitious system design and open-source nature make it a promising contender in the world of operating systems. Keep an eye out for future updates on this fascinating project.

The discussion around the Ares Operating System submission on Hacker News primarily focuses on the Helios microkernel and its similarities to Minix Intel. One user, "hlt," requests for more status updates and screenshots of the operating system. "bdrbbt" shares a link that provides interesting insights into the development progress of the project.

Another user, "grgrv," expresses enthusiasm for the microkernel design of Ares, comparing it to Minix. In response, user "pjmlp" mentions Minix Intel, potentially suggesting a connection or similarity between Ares and Minix Intel.

### Big Tech, Concentrated Power, and the Political Economy of Open AI

#### [Submission URL](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4543807) | 19 points | by [skilled](https://news.ycombinator.com/user?id=skilled) | [4 comments](https://news.ycombinator.com/item?id=37198904)

In a recent paper titled "Open (For Business): Big Tech, Concentrated Power, and the Political Economy of Open AI," researchers from Carnegie Mellon University and AI Now Institute explore the concept of "open" AI and its implications. The paper highlights the various ways the term "open" is used in the context of AI systems, often as a marketing buzzword without a clear technical definition. The researchers examine the resources required to create and deploy AI systems and question which aspects can be made open for scrutiny, reuse, and extension. They find that while some AI systems offer transparency, reusability, and extensibility, the resources needed to build and deploy large AI systems remain closed, available only to those with significant corporate resources. The paper also delves into the history of open source software and its incorporation into large tech corporations. As an example, the researchers examine the company OpenAI and its shifting position on "open" AI. They discuss how the term and the misunderstandings around it are being used to shape public and policymakers' understanding of AI and its industry. The researchers note that while open AI can provide transparency and some forms of auditing and oversight, it does not guarantee democratic access or meaningful competition in AI. They also caution that marketing around openness and investment in open AI systems may be leveraged by powerful companies to bolster their positions and exploit the free labor of open-source contributors.

The discussion on this submission is quite technical and focuses on various aspects of AI. One user mentions that they find the article interesting and highlights the challenges in understanding the different paths AI systems can take. Another user points out that while there may be resources available for developing AI systems, the advantage lies with those who have significant corporate resources. A user also comments on the broad scope of AI systems and how OpenAI's approach may involve censoring certain language models. Another user simply expresses their admiration for language models. One user brings up the concept of pre-internet publishing cycles and how large amounts of information, including hype and heresy, can be managed in an efficient manner with AI. The conversation in the comments is quite fragmented and lacks a central discussion theme.

### Large Language Models As General Pattern Machines

#### [Submission URL](https://arxiv.org/abs/2307.04721) | 78 points | by [optimalsolver](https://news.ycombinator.com/user?id=optimalsolver) | [29 comments](https://news.ycombinator.com/item?id=37197734)

Researchers have found that large language models (LLMs) can be used as general pattern machines, capable of autoregressively completing complex token sequences. In their paper titled "Large Language Models as General Pattern Machines," Suvir Mirchandani and his team explore how these zero-shot capabilities can be applied to problems in robotics. The researchers demonstrate that LLMs can extrapolate sequences of numbers representing states over time to complete simple motions and prompt reward-conditioned trajectories that can discover closed-loop policies. Although there are some limitations in deploying LLMs for real systems, this approach provides insights into how language patterns can be transferred to actions.

In the discussion, there is a mix of perspectives regarding the capabilities and limitations of large language models (LLMs). Some commenters emphasize that LLMs are powerful pattern recognition machines capable of extrapolating complex token sequences and generating accurate outputs. They argue that LLMs have practical applications and can aid in decision-making. Others raise concerns about the credibility of the outputs generated by LLMs, particularly in real-world industries where trust and accuracy are crucial. They argue that relying solely on LLMs for important decisions can lead to deferred or misguided judgments. The topic of fact-checking and the importance of reliable information sources is also discussed, with some commenters noting that LLMs can assist in surfacing relevant information and help users fact-check. However, there are differing opinions on the extent to which LLMs can replace human expertise and whether they can achieve the same level of accuracy. Additionally, there is a discussion about the challenges in ensuring factual accuracy and the limitations of LLMs in understanding and verifying information. The conversation also touches on the potential applications of LLMs in legal document analysis, the importance of fact-checking in the real world, and the gradual improvement of LLMs over time. Finally, there is a mention of the concept of natural production, Turing machines, and the role of LLMs in solving general problems.

### Project Valhalla, Simple as it can be, but not simpler

#### [Submission URL](https://cr.openjdk.org/~jrose/values/larval-values.html) | 94 points | by [pmg1991](https://news.ycombinator.com/user?id=pmg1991) | [31 comments](https://news.ycombinator.com/item?id=37195023)

In a recent update to the Valhalla JVM prototype, the use of Q-types and v-bytecodes has been removed, resulting in a simpler and more efficient VM design. Q-types were a new type of VM type introduced in Valhalla to support Java value classes, while v-bytecodes were special bytecodes for operating on these Q-types. However, new optimization techniques have made it possible to express struct-like values and their operations using normal Java classes without sacrificing performance. By removing Q-types and v-bytecodes, the VM design has been simplified and allows existing Java classes to operate as if no changes have been made. This has significant benefits for migration and binary compatibility, as it reduces the need for clients of these classes to be recompiled. The goal is to minimize changes to the classfile format and use existing classfile encodings as much as possible, allowing code that previously worked with the class to continue working seamlessly after it has been migrated to a value class. Overall, this update represents a major step forward in the development of Valhalla and its support for Java value classes.

The discussion on the submission revolves around the changes in the Valhalla JVM prototype and the implications for the Java programming language. Some commenters express their interest in the new design and the potential performance improvements it brings. Others discuss the differences between value types in Java and C#, and the challenges faced in implementing generics in both languages. There is also a discussion about the compatibility of the new changes with existing code and the potential impact on the Java ecosystem. Overall, the discussion touches on various technical aspects and the differences between JVM and CLR implementations.

### San Francisco’s robotaxi experiment is getting out of hand

#### [Submission URL](https://www.vox.com/technology/2023/8/19/23837648/self-driving-taxis-gm-cruise-alphabet-waymo) | 17 points | by [dotcoma](https://news.ycombinator.com/user?id=dotcoma) | [14 comments](https://news.ycombinator.com/item?id=37202523)

San Francisco's experiment with self-driving taxis is quickly expanding across the country, with robotaxi companies like GM's Cruise and Alphabet's Waymo now offering commercial services in cities like Austin, Los Angeles, Miami, and New York City. The California Public Utilities Commission recently lifted restrictions on Cruise and Waymo, allowing them to operate in San Francisco at all hours and charge fares. However, the expansion of robotaxis has faced opposition, with concerns raised about the vehicles obstructing emergency responders and causing traffic issues. Despite these challenges, self-driving taxis are becoming increasingly common in San Francisco, with residents using them for everyday activities like going out with friends. The experience of riding in a robotaxi is similar to using Uber or Lyft, except there is no driver. Waymo's robotaxis, for example, are luxurious Jaguar vehicles. While tipping the robot driver is not necessary, pricing details for robotaxi rides vary depending on the company.

The discussion on this submission revolves around the topic of regulations and the impact of self-driving taxis in San Francisco.

- One user mentions that regulating self-driving cars requires hard work and expertise, with regulations written specifically for the industry.
- Another user argues against regulations, stating that repealing regulations can be beneficial in promoting innovation.
- There is a debate about the negative aspects of regulations and how they can lead to inefficient enforcement and non-profitable behavior.
- The conversation also touches upon the questionable practices of companies like Uber, which have threatened the traditional market.
- Some users discuss instances of robotaxis behaving in unexpected ways, such as sudden stops in the middle of the road or navigation errors.
- One user expresses frustration with the increasing theft and homelessness in San Francisco, making driving difficult and leading to potential catastrophes.

Overall, the discussion highlights the challenges and concerns surrounding the expansion of self-driving taxis and the need for appropriate regulations to ensure safety and efficient operation.