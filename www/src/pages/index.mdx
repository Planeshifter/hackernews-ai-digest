import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Thu Dec 21 2023 {{ 'date': '2023-12-21T17:12:09.542Z' }}

### Meta-Learning: the future for foundation models, and how to improve it

#### [Submission URL](https://machine-learning-made-simple.medium.com/meta-learning-why-its-a-big-deal-it-s-future-for-foundation-models-and-how-to-improve-it-c70b8be2931b) | 52 points | by [TaurenHunter](https://news.ycombinator.com/user?id=TaurenHunter) | [4 comments](https://news.ycombinator.com/item?id=38728765)

In this article, the author discusses the potential of meta-learning as the future for creating better foundation models in the field of machine learning. They highlight some of the limitations of traditional approaches, such as neural architecture search and model tuning, and argue that a new paradigm is needed. Meta-learning, which refers to training machine learning agents to learn how to learn, could be the solution. The article explains that meta-learning involves training smaller machine learning models on specific tasks and then using the output of these models to train a meta-learning model. The hope is that by exposing the model to a diverse range of tasks, it will be able to develop a general understanding of underlying properties and be better equipped to tackle new, similar tasks in the future.

The author then explores the advantages of meta-learning, such as its ability to handle unbalanced datasets and the potential to reduce the amount of data needed for training. They also highlight its usefulness in scenarios where gathering a lot of data is expensive or regulated, as synthetic data can be used instead. The article concludes by emphasizing that meta-learning has shown promise in various domains, including oncology, and suggests that it could be a key component in training next-generation foundation models.

Overall, the author presents a compelling case for the importance of meta-learning in advancing the field of machine learning and creating more powerful and efficient models.

The discussion on this submission revolves around different aspects of the referenced paper and general opinions on meta-learning.

- User "mrkss" references the paper and explains that it introduces a novel version of an evolutionary algorithm associated with target population rates. They mention that the algorithm's offspring generation population rate represents a fictional particular genome that clones the population. They also discuss the dilemma of population rates falling below 0.0001 and how decision-making is affected by uncertain fitness evaluation, causing some genomes to disappear. They highlight genetic diversity as a major concern in genetic algorithms, as decreasing the total population size can make computational costs harder. They also indicate that in the absence of framework, sexual reproduction and crossover experience greatly increase the quality of evolved genomes.
- User "lsdmb" expresses their opinion that this article is not suitable for the front page and finds it somewhat confusing.
- User "krstjnssn" agrees that the paper is interesting and mentions that it reports a review of great interest for the referenced topic.
- User "bmbzld" simply remarks that the topic is related to AI.

Overall, the discussion is limited and doesn't delve deeply into the topic at hand. Users mainly share their thoughts on the referenced paper and express different opinions regarding its relevance and clarity.

### Astrocyte-Enabled Spiking Neural Networks for Large Language Modeling

#### [Submission URL](https://arxiv.org/abs/2312.07625) | 26 points | by [PaulHoule](https://news.ycombinator.com/user?id=PaulHoule) | [11 comments](https://news.ycombinator.com/item?id=38725930)

A new paper titled "Astrocyte-Enabled Advancements in Spiking Neural Networks for Large Language Modeling" explores the role of astrocytes in neural networks and their impact on cognitive processes such as learning and memory. The authors have developed an innovative framework called Astrocyte-Modulated Spiking Unit (AM-SU) that integrates neuron-astrocyte interactions into the computational paradigm. The resulting Astrocyte-Modulated Spiking Neural Network (AM-SNet) demonstrates exceptional performance in memory retention tasks and natural language generation, particularly in handling long-term dependencies and complex linguistic structures. AM-SNet also shows low latency, high throughput, and reduced memory usage, making it suitable for resource-constrained environments. This work bridges the gap between biological plausibility and neural modeling, paving the way for future research that incorporates both neurons and astrocytes.

The discussion on this submission is focused on the validity and practicality of incorporating astrocytes into neural networks for language modeling. One commenter points out that there are numerous neural features missing in current computational neural networks that astrocytes may play a role in, such as transmission of neurotransmitters and modulation of synaptic connections. Another commenter argues that the paper may be overly technical and suspicious, suggesting that efforts to incorporate astrocytes into neural networks may be premature and inefficient given current computational technology. They suggest that it may be more practical to explore other avenues, such as using specialized hardware or deep learning techniques. There is also a discussion about the intricacies of large language models (LLMs) and the potential limitations of OpenAI's GPT models in terms of prompting responses. One commenter points out that newer language models are trained differently, partially generated by previous models, and discusses the significance of this in the context of OpenAI's GPT models. Another commenter highlights the importance of clarifying the distinction between commercial language models and research language models and urges caution in evaluating the output of language models, especially in the context of benchmarks and datasets. One commenter raises concerns about the feasibility and cost of conducting experiments to incorporate astrocytes into neural networks, suggesting that it may be challenging and expensive compared to computer vision tasks.

### Apple wants AI to run directly on its hardware instead of in the cloud

#### [Submission URL](https://arstechnica.com/apple/2023/12/apple-wants-ai-to-run-directly-on-its-hardware-instead-of-in-the-cloud/) | 224 points | by [thunderbong](https://news.ycombinator.com/user?id=thunderbong) | [195 comments](https://news.ycombinator.com/item?id=38725167)

Apple has published a research paper titled "LLM in a Flash," which outlines its approach to running large language models (LLMs) on smartphones. The paper addresses the computational bottleneck that smartphone devices typically encounter when running LLMs, paving the way for effective inference of LLMs on devices with limited memory. This research signals Apple's intent to catch up with rivals in the field of generative artificial intelligence (AI) and suggests that the company is focusing on developing AI capabilities that can run directly on iPhones. By running AI models on personal devices, queries can be answered more quickly and privacy can be enhanced by ensuring data is processed locally. Additionally, this move aligns with Apple's strategy of keeping AI inference on-device to differentiate itself from other tech giants.

The discussion on Hacker News revolves around various aspects of Apple's research paper on running large language models (LLMs) on smartphones and the implications for AI integration on personal devices. One commenter mentions that Apple devices already have some level of integrated AI for features such as selecting and copying text from images. This is seen as a positive step towards enhancing user experience and making certain tasks more efficient. Others discuss the limitations of AI integration on different devices, with some noting that certain features may work well on Apple devices but not on non-Apple devices. There is also a mention of the ability of Xiaomi phones to work with different languages and scripts. The topic of Apple's commitment to privacy and safety is also raised, with a mention of the controversy surrounding their CSAM detection algorithm. Some users express concerns about the potential misuse of AI for surveillance purposes. The discussion also touches on OpenAI's talk of AGI (Artificial General Intelligence) and its potential impact on the commercial and global landscape. There are mixed opinions regarding the feasibility and implications of AGI development. Overall, the discussion highlights the importance of AI integration on personal devices and the potential benefits and challenges associated with it. Privacy, safety, and interoperability are some of the key considerations raised by the commenters.

### AI machine cannot be called an inventor, rules UK court

#### [Submission URL](https://www.ft.com/content/7bccf980-9eaf-40d9-92b6-ab3ffb43c98d) | 9 points | by [bookofjoe](https://news.ycombinator.com/user?id=bookofjoe) | [4 comments](https://news.ycombinator.com/item?id=38727442)

In a recent ruling, a UK court has stated that an AI machine cannot be referred to as an inventor. The decision came in response to an attempt by a patent application to credit an AI as the inventor of a new technology. The court argued that the legal definition of an inventor is a natural person who contributes to the inventive process, and since an AI lacks legal personality, it cannot be considered an inventor. This ruling has significant implications for intellectual property laws and raises questions about the role of AI in innovation and creativity. Critics argue that denying AI inventorship undermines the potential contributions of AI technology and limits its recognition and protection under the law.

The discussion on this submission seems to revolve around the notion of granting legal rights or recognizing AI as having the same status as a human inventor. One user argues that it is not legally possible to assign rights to an abstract entity, while others highlight the potential capabilities of AI technology, such as using neural networks for product development. Another user shares a link to an archive that might provide more information related to the topic. Lastly, a user brings up the concept of "Dabus" and its role in conferring rights to a machine, as well as the idea of stakeholders retaining control over AI-generated inventions.

### Nvidia CEO: We bet the farm on AI and no one knew it

#### [Submission URL](https://techcrunch.com/2023/08/08/nvidia-ceo-we-bet-the-farm-on-ai-and-no-one-knew-it/) | 155 points | by [gmays](https://news.ycombinator.com/user?id=gmays) | [190 comments](https://news.ycombinator.com/item?id=38720977)

Nvidia founder and CEO Jensen Huang recently spoke at SIGGRAPH and revealed that the company's decision to embrace AI-powered image processing in 2018 was a turning point that has redefined its future. The introduction of ray tracing and intelligent upscaling technologies like RTX and DLSS has not only paid off for Nvidia but has also positioned the company at the forefront of an AI-powered future. Huang emphasized that Nvidia's architecture, designed to support these technologies, is a perfect fit for the growing machine learning development community. He also highlighted the increasing need for massive computing resources to train and run AI models, predicting that natural language interfaces will become a standard in various industries, including visual effects, manufacturing, and heavy industry. Huang showcased Nvidia's newly revealed datacenter-dedicated AI development hardware, GH200, which offers significant cost and power efficiency compared to previous generation computing resources. He believes that these advancements will pave the way for the adoption of AI on a large scale. However, critics argue that Huang's perspective is biased towards Nvidia's interests and does not address the challenges and regulations surrounding AI. Despite this, Nvidia's success in the AI domain positions it well for the future.

The discussion surrounding the submission revolves around different viewpoints on Nvidia's investments in AI and the potential of VR and AR technologies.
One commenter points out that large tech companies often invest in different competencies and consistently invest in those competencies for long periods of time. They argue that Nvidia's success is not simply a result of luck but the result of their investments in GPU graphics and highly parallel computing since the early 2000s.
Another commenter disagrees and suggests that Nvidia's investments may have been motivated by financial interests rather than strategic foresight. They argue that Nvidia prioritized short-term profits over effective research and development spending.
The discussion then shifts to the challenges and limitations of VR and AR technologies. Some commenters express skepticism about the practicality and adoption of VR in mainstream industries, citing issues such as the lack of compelling experiences and the high cost of entry. They argue that VR has yet to find a "Killer App" that would make it a worthwhile investment.
Others argue that the fundamental problems with VR and AR, such as the inability to block out external light and the limitations of hand-tracking, make these technologies impractical for widespread use. They highlight the physical limitations and energy requirements that make the creation of truly immersive and realistic experiences difficult.
However, there are also commenters who believe that VR and AR have the potential to succeed, particularly in the gaming industry and in creating virtual environments for meetings. They argue that while there are challenges and uncertainties, advancements in hardware and the continued support from companies like Nvidia and Google indicate that VR and AR have a promising future.
In conclusion, the discussion reflects differing opinions on Nvidia's investments in AI and the prospects for VR and AR technologies. While some are optimistic about their potential, others express skepticism about the practicality and challenges of widespread adoption.

---

## AI Submissions for Wed Dec 20 2023 {{ 'date': '2023-12-20T17:11:44.136Z' }}

### Implementation of Mamba in one file of PyTorch

#### [Submission URL](https://github.com/johnma2006/mamba-minimal) | 391 points | by [johnma2006](https://news.ycombinator.com/user?id=johnma2006) | [107 comments](https://news.ycombinator.com/item?id=38708730)

A developer named johnma2006 has created a simplified and minimalist implementation of Mamba in PyTorch. Mamba is a linear-time sequence modeling architecture introduced by Albert Gu and Tri Dao. This implementation aims to provide equivalent numerical output as the official implementation for both forward and backward pass. Although it does not prioritize speed optimizations like the official implementation, it emphasizes readability and simplicity. The implementation does not include proper parameter initialization, but this can be added without sacrificing readability. You can check out the demo.ipynb file to see examples of prompt completions. So if you're interested in exploring Mamba in a more straightforward way, this implementation might be worth checking out.

The discussion on the submission starts with a user praising the library mentioned in the submission and mentions other libraries like EgBERT and MPT which offer support for TorchScript JIT and PyTorch. Another user appreciates the concept of the library and mentions that they have tried a similar implementation by Hugging Face and finds the API level abstraction beautiful. 
A user points out that Mamba does not prioritize speed optimizations but focuses on simplicity and readability. Another user mentions that they are interested in Mamba's implementations and that Fortran can be used as a low-level compiled language for scientific code wrapped in libraries like PyTorch and Numpy. The discussion then goes into debating the benefits of using Fortran and its growing adoption. 
One user talks about the potential of Mamba for sequence modeling beyond transformers and mentions other related models like S4, H3, and Monarch. They also discuss the potential applications of Mamba, including reduced computational effort and faster inference times. Another user adds that Mamba can be competitive in training smaller-sized models.
The conversation then shifts to discussing the difficulties of implementing Mamba and the advantages it offers in compressing context and non-dependent state variables. The topic of attention quadratics and their applications in Mamba and related models is also brought up. 
Users discuss the relevance of Mamba in relation to other models like RNNs and transformers, as well as the challenges of dealing with long-context length. The discussion also touches upon the potential of Mamba for model compression and efficient training. 
One user brings up a video that explains the paper in more detail, while another user mentions the importance of considering the computation parameters and the potential memory constraints in training and inference. They also discuss the use of minimal testing and the requirements for efficient data handling. 

Finally, users share resources such as videos and papers for further understanding and mention their excitement about the development of Mamba.

### High-Speed Large Language Model Serving on PCs with Consumer-Grade GPUs

#### [Submission URL](https://github.com/SJTU-IPADS/PowerInfer) | 380 points | by [dataminer](https://news.ycombinator.com/user?id=dataminer) | [79 comments](https://news.ycombinator.com/item?id=38708585)

SJTU-IPADS has developed PowerInfer, a powerful tool that enables high-speed serving of large language models on PCs equipped with consumer-grade GPUs. With 3k stars and 120 forks on GitHub, PowerInfer is gaining popularity in the developer community. The tool is licensed under the MIT license, making it accessible for commercial and open-source projects alike. PowerInfer leverages the computational capabilities of consumer-grade GPUs to deliver fast and efficient language model serving, opening up new possibilities for natural language processing tasks.

The team behind PowerInfer has put in significant effort to optimize the codebase and provide detailed documentation. They have also included several examples to help developers get started quickly. Additionally, frequent updates and bug fixes ensure that PowerInfer stays up to date with the latest advancements in language model serving. The tool is compatible with popular programming languages and frameworks, making it versatile and easy to integrate into existing projects.

PowerInfer has garnered positive feedback from the developer community, with users praising its performance and ease of use. It offers a cost-effective solution for serving large language models, eliminating the need for expensive hardware infrastructure. Whether you're building chatbots, recommendation systems, or language translation services, PowerInfer is a tool worth exploring. To learn more and get started with PowerInfer, visit their GitHub repository.

The discussion on the submission about PowerInfer: High-speed Large Language Model Serving on PCs with Consumer-grade GPUs touches on various topics.

- One user mentions that ReLU activation functions can cause problems in language models and suggests using alternative activation functions like SwiGLU.
- Another user raises the potential legal implications in the USA and EU when it comes to regulating language models and their computational requirements.
- A discussion emerges about the potential harmful effects of mobile games and advertising, with some users expressing concerns about addiction and privacy.
- There is a debate about the benefits and drawbacks of regulations in the technology industry, with some arguing that regulation stifles competition while others emphasize the need for consumer protection.
- Users discuss the performance and compatibility of PowerInfer, with some sharing their experiences with running language models on different GPUs and processors.

Some users also engage in discussions around specific technical details and benchmarks, as well as sharing links to related resources and YouTube videos.

### An AI that learns about chemical reactions and designs a procedure to make them

#### [Submission URL](https://new.nsf.gov/science-matters/meet-coscientist-your-ai-lab-partner) | 131 points | by [geox](https://news.ycombinator.com/user?id=geox) | [45 comments](https://news.ycombinator.com/item?id=38711174)

An artificial intelligence-driven system called "Coscientist" has successfully planned, designed, and executed complex chemical reactions in a matter of minutes. Created by a research team from Carnegie Mellon University, Coscientist used large language models and various software modules to autonomously learn about Nobel Prize-winning chemical reactions and replicate them in a laboratory setting. The AI's capabilities could potentially help increase the pace and number of scientific discoveries, as well as improve the replicability and reliability of experimental results.

The discussion around the submission revolves around several topics. 
One user expresses interest in using the ChatGPT API for genome annotation and designing experiments using CRISPR technology. Another user comments on the potential applications of machine learning in chemistry, particularly in the field of drug discovery. There is a debate about the validity and reliability of using large language models (LLMs) like ChatGPT for scientific research. Some users express concerns about the lack of proper attribution and the need for further peer-reviewed research. Others argue that LLMs can be useful in generating insights and accelerating scientific discovery.
There is also a discussion about the limitations and challenges of using AI in chemistry and the need for more independent verification. One user points out that the Coscientist AI system is designed to carry out physical actions in the lab and corrects its mistakes, making it more than just a text-based AI. However, skepticism remains about relying on information from sources like Wikipedia and the potential risks associated with AI-generated results.
There are also tangential discussions about the potential impact of AI on patent applications and the reliability of AI-generated data in the field of chemistry.

Overall, the discussion highlights both the potential benefits and limitations of AI in scientific research, with some users expressing optimism about the possibilities and others calling for caution and further scrutiny.

### IBM demonstrates 133-qubit Heron

#### [Submission URL](https://www.tomshardware.com/tech-industry/quantum-computing/ibm-demonstrates-useful-quantum-computing-within-133-qubit-heron-announces-entry-into-quantum-centric-supercomputing-era) | 115 points | by [rbanffy](https://news.ycombinator.com/user?id=rbanffy) | [61 comments](https://news.ycombinator.com/item?id=38708185)

IBM has made significant advancements in quantum computing at its Quantum Summit 2023 event. The company unveiled the 133-qubit Heron Quantum Processing Unit (QPU), its first utility-scale quantum processor, as well as the Quantum System Two, a quantum-specific supercomputing architecture. These cutting-edge devices push the boundaries of quantum computing, but further improvements are needed to overcome the plateau of understanding in quantum technology. IBM also announced breakthroughs in noise reduction algorithms and algorithmic improvements that reduce the number of qubits required for certain calculations. These advancements pave the way for a future of quantum-centric supercomputing. IBM's roadmap now focuses on scalability and qubit quality, with plans to reach 1 billion operationally useful quantum gates by 2033. The company aims to harness the power of quantum computing for tasks that are currently impossible with classical hardware.

The discussion on Hacker News revolves around various aspects of IBM's advancements in quantum computing. Some users are skeptical about the practicality and impact of quantum computing in the near term, while others highlight the potential advancements in encryption and drug discovery. There is also some discussion about the quality of the article and the writing platform used. Additionally, users point out that the AI-generated summary lacks clarity and coherence, suggesting the need for improvements in natural language processing.

### Identifying and eliminating CSAM in generative ML training data and models

#### [Submission URL](https://purl.stanford.edu/kh752sm9123) | 37 points | by [pulisse](https://news.ycombinator.com/user?id=pulisse) | [26 comments](https://news.ycombinator.com/item?id=38711135)

Researchers at Stanford have conducted a study examining the presence of child sexual abuse material (CSAM) in generative machine learning training data and models. The study focused on the LAION-5B dataset, which was used to train the popular Stable Diffusion series of models. Using a combination of perceptual hash matching, cryptographic hash matching, k-nearest neighbors queries, and machine learning classifiers, the researchers were able to detect hundreds of instances of known CSAM in the training set. They also discovered new candidates that were subsequently verified by external parties. The study provides recommendations for mitigating this issue, including altering existing models and hosting models trained on the LAION-5B dataset. This research highlights the importance of identifying and eliminating CSAM in machine learning training data to prevent the generation of explicit adult content.

The discussion on Hacker News regarding the Stanford study on the presence of CSAM in generative machine learning training data and models covered various topics and perspectives.
- Some users raised concerns about the legal implications of the study and the potential for censorship. They mentioned cases where CSAM filters were used as a means for political control and expressed the view that this article could be seen as opportunistic.
- Other users highlighted the importance of identifying and eliminating CSAM in machine learning training data, emphasizing the need for public datasets to address this issue. They mentioned that machine learning models can inadvertently generate explicit content and that efforts should be made to remove such content from training sets.
- There were discussions about the limitations of current legislation and enforcement in addressing the issue of CSAM. Some users argued that the consequences of AI-generated CSAM are significant for victims and the justice system. However, others pointed out that regulating machine-generated content is challenging and may require a nuanced understanding of regulatory frameworks.
- One user raised concerns about the training process of the models, suggesting that the training data should be modified to prevent the generation of explicit adult content.
- Another user shared their experience moderating content and provided examples of AI-generated CSAM that they had come across, highlighting the challenges in distinguishing between harmful and innocuous content.
- There were discussions about the methodology used in the study, with users noting that the LAION dataset contained a significant number of CSAM images and that it was compiled from various mainstream sources known to host such content.
- Some users expressed concerns about the potential privacy and ethical implications of using machine learning models trained on datasets containing CSAM.
- The issue of child victims of sexual abuse and the need to protect them was raised, with some users emphasizing the importance of preventing re-victimization through the distribution of CSAM and the need for agencies to make efforts to detect and remove such content from the internet.

Overall, the discussion revolved around the ethical, legal, and technical challenges associated with detecting and preventing CSAM in machine learning training data and the potential impact on victims.

### Show HN: Easily train AlphaZero-like agents on any environment you want

#### [Submission URL](https://github.com/s-casci/tinyzero) | 79 points | by [s-casci](https://news.ycombinator.com/user?id=s-casci) | [21 comments](https://news.ycombinator.com/item?id=38707475)

TinyZero is a tool that allows you to easily train AlphaZero-like agents on any environment you want. It provides a framework where you can add new environments, models, and agents to train your own AI. The process involves defining the methods specific to your environment, such as resetting the environment, taking actions, and getting game results. Similarly, you can add custom models and agents. The models should have methods to compute values and policies, while the agents should have methods to calculate values and policies for the game. TinyZero also supports wandb logging and GPU acceleration. Overall, TinyZero provides a flexible and customizable platform for training AI agents using the AlphaZero algorithm.

The discussion around the submission of TinyZero on Hacker News mainly focused on different aspects and related projects.
One commenter pointed out that the licensing details were missing from the repository. Another user acknowledged this observation and thanked them for catching that issue.
There was also a discussion about Game Description Language (GDL), where a user mentioned a project that used GDL for describing games and asked if TinyZero supports it. Another user replied that they couldn't find any relevant links but mentioned that GDL is taught in a Stanford course on General Game Players.
A user raised the topic of modifying existing environments and interfaces, suggesting that it should not be difficult and that they could submit a pull request to address it. Another user inquired if there are any formal Python libraries that support GDL. In response, someone mentioned an implementation of GDL in a Python library called pyggp.
The performance and scalability of TinyZero were also discussed, with one user mentioning that it is yet to be confirmed how well it performs compared to AlphaZero. They also noted that training multiple agents at scale may require resources that are not readily available.
Various other reinforcement learning libraries and frameworks were mentioned in the discussion, such as OpenAI's Gym, TensorFlow, TF-Agents, ReAgent, Meta, DeepMind's OpenSpiel, and Amazon SageMaker RL.
There was a question about the behavior of the get_legal_actions function, and a user asked what to expect from it. Another user replied that the expectation is that it returns a list of legal actions, but its behavior may depend on the specific implementation.
A user expressed their intention to try TinyZero for playing Carcassonne, a popular board game, and another user encouraged them to submit a pull request.
The discussion also touched on the handling of games with incomplete information and complex variants. Some related articles and concepts, such as ReBeL, BetaZero, ExIt-OOS, and Player Games, were mentioned. The limitations of traditional AlphaZero were discussed, and some users recommended looking into different variations, such as MuZero.

### Mercedes Gets Approval for Turquoise Automated Driving Lights

#### [Submission URL](https://jalopnik.com/mercedes-turquoise-automated-driving-lights-level-3-1851110043) | 23 points | by [Stratoscope](https://news.ycombinator.com/user?id=Stratoscope) | [10 comments](https://news.ycombinator.com/item?id=38706072)

Mercedes-Benz has become the first automaker to gain approval to sell a Level 3 automated driving system in the United States. The Drive Pilot system allows drivers to take their hands off the wheel and eyes off the road in traffic jam situations up to 40 mph, allowing for activities such as reading, watching movies, or using a cellphone. To indicate to other motorists and law enforcement that the Level 3 system is active, Mercedes has received permit approval for turquoise-colored exterior marker lights. The color was chosen by SAE and will be used by other brands as well. The goal is to improve road safety and public acceptance for automated driving. California and Nevada are the only states where Drive Pilot is currently allowed, but Mercedes plans to slowly roll out the system in other states as regulations allow.

The discussion on this submission primarily revolves around the use of turquoise-colored external marker lights to indicate that the Level 3 automated driving system is active. Some users express concerns about the choice of color, suggesting that a different color may have been more suitable or that it could be confusing for other drivers. Others debate the visibility and effectiveness of different colored lights at night. One commenter discusses the benefits of additional information provided by the lights, while others question whether the use of different colors could cause further confusion on the roads. One user comments on the marketing reasons behind the choice of color, while another user shares an anecdote about a similar situation with Tesla's autopilot system. Lastly, there is a comment about law enforcement potentially not pulling over drivers who are seen watching movies while using the automated system.

### Rite Aid banned from using AI facial recognition for five years

#### [Submission URL](https://www.ftc.gov/news-events/news/press-releases/2023/12/rite-aid-banned-using-ai-facial-recognition-after-ftc-says-retailer-deployed-technology-without) | 227 points | by [commoner](https://news.ycombinator.com/user?id=commoner) | [80 comments](https://news.ycombinator.com/item?id=38704830)

Rite Aid, a major retail pharmacy chain in the US, has been banned from using facial recognition technology for surveillance purposes after the Federal Trade Commission (FTC) found that the company deployed the technology without reasonable safeguards. The ban will last for five years. The FTC alleged that Rite Aid's facial recognition technology falsely tagged consumers, particularly women and people of color, as shoplifters. The company will be required to implement comprehensive safeguards to prevent harm to consumers and discontinue using the technology if potential risks cannot be controlled. Additionally, Rite Aid must implement a robust information security program to address previous charges of inadequate oversight of its service providers. The FTC highlighted the importance of preventing the misuse of biometric information and protecting consumers from unfair data security practices. Rite Aid's actions subjected consumers to embarrassment, harassment, and other harm, according to the FTC's complaint. The company did not inform consumers about the use of the technology and discouraged employees from revealing such information. Employees acted on false positive alerts, leading to confrontations with customers and accusations of shoplifting or other wrongdoing. The FTC also stated that Rite Aid's actions disproportionately impacted people of color. The company had contracted with two companies to create a database of images of individuals believed to have engaged in criminal activity, but the system generated thousands of false-positive matches. The FTC accused Rite Aid of failing to consider and mitigate potential risks to consumers, test the accuracy of the technology, prevent the use of low-quality images, monitor or test the accuracy of the technology after deployment, and adequately train employees. This case underscores the FTC's vigilance in protecting the public from unfair biometric surveillance and data security practices and follows their warning about monitoring the use of facial recognition technology.

The discussion on this submission covers various aspects of the Rite Aid facial recognition case and related topics. Some commenters express concern about the impact of facial recognition technology, discussing issues such as the potential for misidentifying individuals based on race and the sudden notice of security cameras using the technology. Others bring up examples of businesses using facial recognition systems and the problem of false positives. The commenters also discuss the FTC's previous charges against Rite Aid regarding inadequate oversight of service providers and the need for personal accountability in breaking laws. There is further discussion about the misuse of surveillance cameras for theft prevention purposes and the experience of safety threats in stores.

---

## AI Submissions for Tue Dec 19 2023 {{ 'date': '2023-12-19T17:09:45.282Z' }}

### Borges and AI

#### [Submission URL](https://arxiv.org/abs/2310.01425) | 34 points | by [alexmolas](https://news.ycombinator.com/user?id=alexmolas) | [15 comments](https://news.ycombinator.com/item?id=38693120)

The paper titled "Borges and AI" by Léon Bottou and Bernhard Schölkopf explores the connection between large language models (LLMs) and artificial intelligence (AI) through the imagery of Jorge Luis Borges, a famous 20th-century writer known for his works in magical realism and postmodern literature. The authors argue that understanding LLMs and AI through the lens of Borges' literary concepts can provide a new perspective on the relationship between language modeling and artificial intelligence. This paper challenges the common science fiction-based imagery surrounding AI and dives into a more nuanced understanding of the phenomenon at hand.

The discussion around the submission "Borges and AI" on Hacker News covers a range of perspectives and interpretations of the paper. Some users comment on Borges as their favorite writer and the influence of his works on the article. Others provide links to collections of Borges' short stories and essays for further reading.

One user points out the difficulty of finding meaningful information in large language models (LLMs) due to their tendency to generate nonsensical outputs. They mention the challenge of lacking a created index to easily find relevant information. Another user argues that the definition of artificial intelligence (AI) is often misunderstood, and suggests that it is not necessarily based on the concept of a killer machine like Terminator or HAL. They mention their own experience working on AI since 1998, focusing on planning and non-player characters in video games. There is also discussion around the paper's use of Borges and its connection to LLMs. Some suggest that the metaphor of Borges' Library of Babel, which contains incomprehensible texts produced by humans, is not applicable to LLMs. They argue that LLMs struggle to handle new tasks and cannot perform the type of reasoning humans can.

One user sees a flaw in the approach of the paper, claiming that it lacks clear conclusions and only offers insights without developing a theory. The discussion then delves into a debate about the capabilities of LLMs and how they can aid in understanding models and fine-tuning. There is a mention of Funes, a character from Borges' works with exceptional memory, and a user expresses confusion about attributing consciousness to LLMs, considering it a terrible idea.

Overall, the discussion explores different perspectives on the connection between Borges' literature and AI, as well as the capabilities and limitations of LLMs.

### TuneNN: A transformer-based network model for pitch detection

#### [Submission URL](https://github.com/TuneNN/TuneNN) | 107 points | by [CMLab](https://news.ycombinator.com/user?id=CMLab) | [38 comments](https://news.ycombinator.com/item?id=38694719)

TuneNN is a transformer-based network model for pitch detection in musical instruments. The model aims to capture the timbre of musical notes by considering various factors like harmonic relationships, harmonic strengths and weaknesses, instrument resonant peaks, and structural resonant peaks over time. It utilizes web audio and tensorflow.js for an online experience. The model employs different types of spectra, such as the STFT spectrum, Bark spectrum, and Cepstrum, to extract relevant features. These features are then processed using a sliding adjacent windows approach with a transformer-based network model. TuneNN supports tuning for 12+ instrument types. You can find more information and try it out at aifasttune.com.

The discussion on the submission "TuneNN: A Transformer-Based Network Model for Pitch Detection in Musical Instruments" on Hacker News covered various aspects of pitch detection and related topics. Here are some key points from the discussion:

- One commenter highlighted the complexity of pitch detection, stating that it is not solved by using Fast Fourier Transform (FFT) alone. They mentioned that determining the fundamental frequency is a challenging task and that simple physical measurements may not be sufficient for accurate pitch detection.
- Another user shared a link to an article discussing missing fundamentals and how they can affect perceived pitch. They explained how different tones and musical instruments can produce different harmonics and harmonic frequencies, which can impact the perception of pitch.
- A user mentioned the CREPE model and its high latency in instrument pitch recognition. They also shared a link to the TuneNN model's website and expressed interest in trying it out.
- One commenter provided a summary of the TuneNN model, mentioning its use of transformer-based network modeling to capture the timbre of musical notes and support tuning for over 12 instrument types.
- The license of TuneNN was a topic of discussion, with one user asking about the license and another sharing a link to the PESTO model, which learns pitch prediction with a self-supervised objective.
- The cost of tuning apps and their comparison with traditional methods was debated. Some users mentioned that software-based tuning techniques can be cost-effective, while others argued that professional tuning can involve additional nuances and complexities.
- A user shared their interest in implementing the Nebula1 algorithm for pitch detection and another user mentioned the McLeod Pitch Method as their favorite pitch detection method.
- The topic of licensing and open-source implementations came up, with one user discussing issues related to licensing the YIN and PYIN implementations of pitch detection algorithms.
- The effectiveness of the TuneNN model compared to traditional digital signal algorithms was discussed, with one user noting its significantly higher accuracy and robustness.
- The relevance of pitch detection to specific sounds, such as smoke alarms or 3D printer collisions, was discussed briefly.
- A user faced an error related to microphone permission while running the TuneNN website and mentioned their system configuration involving Ubuntu, KDE, and Firefox.

Overall, the discussion covered various perspectives on pitch detection, licensing, open-source implementations, and the effectiveness of the TuneNN model compared to traditional algorithms.

### A Mathematical Perspective on Transformers

#### [Submission URL](https://arxiv.org/abs/2312.10794) | 72 points | by [georgehill](https://news.ycombinator.com/user?id=georgehill) | [6 comments](https://news.ycombinator.com/item?id=38699331)

The paper "A mathematical perspective on Transformers" by Borjan Geshkovski and his colleagues explores the inner workings of Transformers, which are a fundamental component of large language models. The authors develop a mathematical framework for analyzing Transformers as interacting particle systems, revealing the emergence of clusters over time. This study offers new perspectives for mathematicians and computer scientists alike. The paper delves into subjects such as Machine Learning, Analysis of PDEs, and Dynamical Systems. Overall, this research contributes to a deeper understanding of Transformers and their applications.

In the comments on Hacker News, there is a mix of reactions to the submission about the paper on the mathematical perspective on Transformers. Some users express interest in the abstract and look forward to reading the research. One user mentions that they are interested in understanding the extraction skills of large language models and identifying relevant topology. There is one comment expressing disappointment because they were expecting designs of robots or something similar. Another user suggests that the study could also be approached from a statistical physics perspective.

### VideoPoet A large language model for zero-shot video generation

#### [Submission URL](https://sites.research.google/videopoet/) | 119 points | by [fchyan](https://news.ycombinator.com/user?id=fchyan) | [39 comments](https://news.ycombinator.com/item?id=38702141)

VideoPoet is a powerful language model that can generate high-quality videos based on text prompts. It can produce a wide range of visually stunning scenes, such as a dog listening to music, a robot cat eating spaghetti, and a golden retriever wearing VR goggles in Paris. VideoPoet can also generate audio to match a given video without any text guidance. This model is capable of multitasking on various video-centric inputs and outputs, including text-to-video, image-to-video, stylization, and outpainting tasks. It can create videos in square or portrait orientation, and even generate long videos by predicting one-second video clips repeatedly. Get ready to be amazed by the creative possibilities of VideoPoet!

The discussion about the VideoPoet submission on Hacker News covers various aspects of the model and its applications. Some users highlight the impressive results achieved by VideoPoet, noting that it can generate high-quality videos based on word prompts. They mention examples such as an 8K HD prompt engine and creative outputs like a dog listening to music or a robot cat eating spaghetti. Others point out the technical aspects of the model, discussing the underlying technologies used, such as VQGAN+CLIP Stable Diffusion. They also mention the potential dangers of relying on AI-generated content and its impact on human employment. There is a discussion about the commercialization of AI products and the role of companies like RunwayML and Google in bringing AI technology to the market. Some users express skepticism or disappointment with Google's promotion and suggest that the claims made by the company might be exaggerated. There are also references to other AI models and their capabilities, such as image-to-video generation and the potential impact on artistic professions. The potential of VideoPoet for personal entertainment, including TikTok short video storytelling, is also discussed.

Overall, the discussion covers topics like the technical aspects of VideoPoet, the commercialization of AI, AI-generated content's impact on employment, and the entertainment value of AI models like VideoPoet.

### The Illustrated GPT-2: Visualizing Transformer Language Models (2019)

#### [Submission URL](https://jalammar.github.io/illustrated-gpt2/) | 209 points | by [epberry](https://news.ycombinator.com/user?id=epberry) | [5 comments](https://news.ycombinator.com/item?id=38691583)

The Illustrated GPT-2 is an in-depth exploration of the architecture and inner workings of the OpenAI GPT-2 language model. The GPT-2 is a transformer-based model trained on a massive dataset to generate coherent and passionate essays. This post dives into the self-attention layer of the GPT-2 and explains how it enables the model to produce such impressive results. The author also explains the evolution of transformer blocks and discusses applications of transformer models beyond language modeling, such as machine translation, summarization, transfer learning, and music generation. If you're fascinated by the capabilities of machine learning models like GPT-2, this post is a must-read.

The discussion includes a few different comments. 

- "xnsh" shares several resources related to the topic, including links to the Illustrated Transformer, Beyond Illustrated Transformer, and LLM Visualization.
- "tlsnb" provides an excellent explanation of the self-attention mechanism and recommends exploring tensor network-like diagrams and examples. 
- "3abiton" mentions that recent posts on the topic have gained traction and discusses the changes from GPT2 to GPT4.
- "kridsdale1" adds to the discussion by mentioning the significant changes in the GPT3 architecture, including the use of mixture of experts models and the increase in dimensionality of embedding vectors.
- "Der_Einzige" compliments the author, Jay Alammar.

Overall, the comments provide additional resources, explanations, and insights related to the architecture and capabilities of language models like GPT-2.

### UK plan to digitise wills and destroy paper originals "insane" say experts

#### [Submission URL](https://www.theguardian.com/society/2023/dec/18/ministry-of-justice-plan-to-destroy-historical-wills-is-insane-say-experts) | 159 points | by [ilamont](https://news.ycombinator.com/user?id=ilamont) | [205 comments](https://news.ycombinator.com/item?id=38699007)

The UK Ministry of Justice is facing criticism for its proposal to destroy millions of historical wills in order to save on storage costs. The plan involves digitising and then discarding around 100 million paper originals of wills dating back over 150 years, with the aim of saving £4.5 million ($5.9 million) annually. Historians and archivists have called the proposal "insane" and "sheer vandalism," expressing concern that important historical records could be lost. While the government plans to keep the originals of wills belonging to famous individuals, such as Charles Darwin and Princess Diana, others may be destroyed after 25 years. Critics argue that the physicality of the original documents is important to understanding the context and significance of the wills.

The discussion around the submission on Hacker News focused on several different aspects of the UK Ministry of Justice's proposal to destroy historical wills. Here are some key points:

- Some users questioned the necessity and wisdom of destroying the original paper wills, arguing that they hold historical and cultural significance that cannot be replicated by digital copies. They expressed concern about the loss of physical artifacts and the potential for digital data loss or cyber attacks.
- Others pointed out that digitizing the wills could make them more accessible and facilitate easier search and retrieval. They also mentioned the possibility of implementing measures to ensure the integrity and authenticity of the digital copies.
- The discussion touched on the importance of physical documents in historical and legal contexts, as well as the challenges and risks associated with long-term preservation of digital records. Some users highlighted the importance of redundancy and the need for multiple copies of records.
- There was also mention of the cost-saving aspect of the proposal, with some arguing that digital storage could be a more cost-effective solution in the long run compared to maintaining physical records.
- Users debated the potential risks of relying solely on digital records, including data loss or corruption, the need for technological infrastructure to support long-term preservation, and the issue of changing file formats and technology.
- Lastly, there were discussions about the role of government in managing and preserving archives, with users expressing different opinions about the government's responsibility and the potential impact of cost-saving measures on historical records.

Overall, the discussion on Hacker News reflected a range of perspectives on the proposal to destroy historical wills in favor of digitization. Some users emphasized the importance of physical artifacts and the potential risks of relying solely on digital records, while others highlighted the benefits of digitization for accessibility and cost-saving.

### Andrew Ng: 'Do we think the world is better off with more or less intelligence?'

#### [Submission URL](https://www.ft.com/content/2dc07f9e-d2a9-4d98-b746-b051f9352be3) | 17 points | by [mgreg](https://news.ycombinator.com/user?id=mgreg) | [7 comments](https://news.ycombinator.com/item?id=38702791)

Andrew Ng, a computer scientist known for his work in artificial intelligence (AI), argues that fears of AI leading to doomsday scenarios are overhyped. Ng, who has been involved in groundbreaking AI research projects, believes that regulators who buy into the alarmist narrative around AI will only benefit vested interests. He advocates for open-source AI development and criticizes government efforts to overly regulate the technology. Ng also demonstrates the capabilities of open-source AI models on his laptop, running inference without the need to send data to the cloud. While he acknowledges the limitations of these smaller models, he sees their potential for simple tasks like brainstorming and basic information retrieval.

The discussion on this submission involves several commenters sharing their thoughts on the topic. 
One commenter, xchp, criticizes the article for oversimplifying the complexity of the issues surrounding AI by presenting a simplistic narrative that ignores essential details and nuances.
Another commenter, artninja1988, responds to xchp's comment with confusion, as they only read the headline of the article and did not fully understand the discussion.
There is a mention of an interview with Andrew Ng, the subject of the submission, by patrickhogan1.
Another commenter, jbrkr, expresses their view that the intelligence in the world is not concentrated but distributed, and that concentrating intelligence in one place can be detrimental. They further mention that distributed individual and regional governance can be harmful to society.
In response to jbrkr's comment, ntnllrvd argues that humans benefit from intelligence that is more broadly distributed, which helps in mitigating existential threats. However, they also note that the extent to which non-human intelligence in the world can be considered intelligence is questionable and subject to change.

Overall, the discussion touches on the complexity of AI and its potential impact, with varying opinions on the concentration of intelligence, the benefits of distributed intelligence, and the nature of non-human intelligence.

### Turquoise taillights tell you this Mercedes is driving autonomously

#### [Submission URL](https://arstechnica.com/cars/2023/12/turquoise-taillights-tell-you-this-mercedes-is-driving-autonomously/) | 28 points | by [addaon](https://news.ycombinator.com/user?id=addaon) | [15 comments](https://news.ycombinator.com/item?id=38698854)

Mercedes-Benz has received approvals from authorities in California and Nevada to test out a new car-to-human communication feature. The automaker will use turquoise-colored marker lights to indicate when its partially automated driver assistance feature, Drive Pilot, is operating. Drive Pilot is a Level 3 system, allowing the driver to take their hands and eyes off the road at speeds of up to 40 mph. This approval makes Drive Pilot the first Level 3 system to gain regulatory approval for deployment. The marker lights, chosen for their differentiation from other colored lights on the road, are intended to signal to other road users that the vehicle is operating autonomously.

The discussion on Hacker News about Mercedes-Benz's car-to-human communication feature involves various topics and perspectives. 
One user points out that international standards usually use a different color to signify autonomous control in self-driving cars. They mention that in Japan, green lights are slightly blue, as the Japanese language evolved differently when it comes to traffic lights. Another user adds that this can cause confusion for people who are colorblind.
There are discussions about the usefulness of the turquoise marker lights in indicating the vehicle's intentions to other road users. One user thinks that it can be helpful, but there might be challenges in understanding the level 3 autonomous driving terminology and differences between car manufacturers. Another user suggests using fluorescent square lights to signal different ADAS activities, referring to the iconic lighting in the movie Close Encounters of the Third Kind.
Some users discuss colorblindness and its implications for traffic lights. One user explains that for a colorblind person, a green traffic light might appear gray. Another user mentions that traffic lights have specific designs to help identify them, and that colorblindness can make it harder to distinguish colors while driving or walking.
There is a discussion about the location and design of lights on vehicles to help with identification. One user shares a gradient of different types of colorblindness, while another user suggests using the location rotation to help with identification.
One user raises concerns about driving behind a car that is operating autonomously, suggesting that it may be difficult to respond to sudden changes in lane position. Another user suggests that the behavior of autonomous vehicles in passing can vary and may pose risks, such as aggressive passing or unpredictable lane changes.

Overall, the discussion covers topics such as the use of colors to signify autonomous control, the challenges for colorblind individuals, and safety concerns regarding the behavior of autonomous vehicles on the road.

### An In-depth Look at Gemini's Language Abilities

#### [Submission URL](https://arxiv.org/abs/2312.11444) | 118 points | by [tbruckner](https://news.ycombinator.com/user?id=tbruckner) | [68 comments](https://news.ycombinator.com/item?id=38695583)

A recent paper titled "An In-depth Look at Gemini's Language Abilities" explores the language abilities of Google Gemini models, comparing them to the OpenAI GPT series. The authors provide a third-party, objective comparison with reproducible code and transparent results, evaluating the models across 10 datasets that test various language abilities such as reasoning, question answering, math problem solving, translation, code generation, and instruction following. The analysis shows that Gemini Pro performs slightly inferior to GPT 3.5 Turbo in terms of accuracy across all benchmarked tasks. The paper also discusses reasons for this under-performance, including difficulties with mathematical reasoning, sensitivity to multiple-choice answer ordering, and content filtering. However, areas where Gemini demonstrates high performance include generating non-English languages and handling longer and more complex reasoning chains. The paper provides code and data for reproducibility.

The discussion on Hacker News revolves around the recent paper on Gemini's language abilities and includes various perspectives on the topic. 
One user points out the inaccuracy of the Chatbot Arena Leaderboard in predicting model performance compared to human judgment and suggests using 5 years of performance work and competency SAT scores as a better evaluation metric. Another user provides a link to a paper arguing that GPT-4 matches controlled crowd-sourced human preferences at an 80% agreement level, making it a scalable and explainable approximation of human preferences. 
There is a discussion about the inclusion of other interesting models, such as the Phi-2 model and Solar-107B, in the leaderboard. Another user mentions the significance of performance differences between models and the capabilities demonstrated in benchmark domains. A user raises the issue of non-blind voting results and the lack of identification of the winning model in conversations. 
There are discussions regarding the filtering of votes and the quality improvement of voting results based on filtering conversation length and consistency. Some users express confusion about the inclusion of avatars instead of humans in the evaluation process. 
The size and performance of Mixtral, a model missing from the paper, are discussed, with users pointing out its rank on the Chatbot Arena Leaderboard. The accuracy of OpenAI's leaderboard and the need for non-blinded voting results are debated. 
There is a discussion on the difference between Gemini Ultra and Mixtral, and a user comments on the withdrawal of an arXiv article due to inappropriate sourcing, mentioning the confusion caused by the article's claims. The GPU compute time and effectiveness in discarding company results are also mentioned. 

The origins of MistralAI and the involvement of individuals from Deepmind and Google Brain in its development are discussed.