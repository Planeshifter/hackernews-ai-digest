import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Tue Jul 04 2023 {{ 'date': '2023-07-04T17:10:15.539Z' }}

### The Lone Banana Problem in AI

#### [Submission URL](https://www.digital-science.com/tldr/article/the-lone-banana-problem-or-the-new-programming-speaking-ai/) | 139 points | by [JohnHammersley](https://news.ycombinator.com/user?id=JohnHammersley) | [96 comments](https://news.ycombinator.com/item?id=36582937)

In this article, Daniel Hook, CEO of Digital Science, explores the potential biases present in Large Language Models (LLMs) and their impact on AI-generated content. He coined the phrase "Lone Banana Problem" to describe the subtle biases that can be difficult to detect. To illustrate this, Hook used an AI program called Midjourney to generate an image of a single banana casting a shadow on a grey background. However, even after refining his prompt, the AI consistently produced images with multiple bananas. This led Hook to question the biases embedded in the AI's training data and the need for a deeper understanding of these technologies. Despite the amusing nature of the "Lone Banana Problem," it raises important considerations about the limitations and potential pitfalls of AI language models.

### Google's updated privacy policy states it can use public data to train its AI

#### [Submission URL](https://www.engadget.com/googles-updated-privacy-policy-states-it-can-use-public-data-to-train-its-ai-models-095541684.html) | 173 points | by [firstSpeaker](https://news.ycombinator.com/user?id=firstSpeaker) | [91 comments](https://news.ycombinator.com/item?id=36586170)

Google has recently updated its privacy policy to clarify that it can use publicly available data to train its AI models. The updated policy specifies that this data can be used to not only build features but also full products like Google Translate, Bard, and Cloud AI capabilities. By making this change, Google is notifying users that anything they publicly post online could be used to train its AI systems. This comes as critics have raised concerns about companies using personal data from the internet without consent to train their language models for generative AI use. OpenAI, for example, is facing a proposed class action lawsuit for allegedly scraping personal data without consent. As more companies develop generative AI products, similar lawsuits are likely to emerge in the future. In response to data scraping concerns, websites like Reddit have started charging access to their API, while Twitter has limited the number of tweets a user can see per day.

The discussion on this submission revolves around the topic of public versus private information and the legality of taking and publishing photos in public spaces. Some users argue that taking photos in public places is legally allowed and does not require explicit consent from individuals in the background. Others mention that there are restrictions in certain countries, such as Germany, where taking photos of vulnerable individuals or private locations without permission is prohibited.  There is also a discussion about the privacy laws in different countries, particularly in the European Union (EU) and the United States (US). Users point out that there are substantial differences between the two regions, with some arguing that EU laws prioritize individual privacy rights more than the US. The debate expands to include factors such as population size, democratic systems, and economic considerations. Another topic discussed is the responsibilities of companies like Google in handling public data and complying with legal requirements. One user highlights the case of Wikileaks, raising questions about the legality of accessing and processing public data without authorization. It is noted that if information remains classified or requires security clearances, accessing or disseminating it would be illegal.

### Chat-based Large Language Models replicate the mechanisms of a psychic’s con

#### [Submission URL](https://softwarecrisis.dev/letters/llmentalist/) | 24 points | by [EventH-](https://news.ycombinator.com/user?id=EventH-) | [10 comments](https://news.ycombinator.com/item?id=36586540)

In a thought-provoking article, Baldur Bjarnason investigates the phenomenon of language models, specifically chat-based large language models (LLMs), being perceived as intelligent. Bjarnason argues that LLMs are not capable of reasoning or thinking like humans do, as they are merely mathematical models of language tokens. He presents two possible explanations for the intelligence illusion: either the tech industry has unintentionally developed a completely new kind of mind, or the illusion lies in the mind of the user. Bjarnason aligns himself with the latter camp, drawing a parallel between the intelligence illusion and the tactics used by psychics in cold reading. By employing validation statements and statistically probable guesses, both chatbots and psychics create the illusion of intelligence and specificity. Bjarnason suggests that the rise of chat-based LLMs has unintentionally resulted in an automation of the psychic con, where users are tricked into perceiving an intelligence that doesn't truly exist.

The discussion in the comments starts with a user expressing frustration with system administrators and suggesting that GPT-4, a large language model (LLM), might help solve system problems. Another user points out that defining intelligence is a subjective and fluff-filled discussion, and that minimizing attention given to doubts and definitions can hinder understanding. Another commenter is impressed by LLMs' ability to understand instructions to some extent, giving an example of using LLMs for debugging. However, they argue that LLMs' capabilities are limited to statistical matching rather than true intelligence. A user counters this argument by suggesting that intelligence can be learned through training data, mentioning the concept of predictive coding and the recursive identification of models within the human brain. They propose that LLMs could potentially reach a level of intelligence similar to humans. Someone else brings up the objective view that LLMs simply produce text based on probabilistic matrices and do not actually possess intelligence. They argue that LLMs mimic structures but lack the complexity for reasoning and consciousness. Another user agrees with this view, stating that the development of LLMs has not led to the invention of a new kind of mind, but rather the tech industry stumbled upon unknown principles and processes. They assert that intelligence lies in the mind of the user rather than the LLM itself. Finally, there is a mention of the challenges scientists face in understanding the intricacies of the human brain. A user sarcastically remarks that people impressed with LLMs should reconsider granting machines rights, hinting at potential issues surrounding artificial intelligence and its impact on society. One user concludes the discussion by referring to "scientism," suggesting that there is an overreliance on science as an ideology.

### A human just defeated an AI in Go. Here's why that matters

#### [Submission URL](https://www.zmescience.com/future/a-human-just-defeated-an-ai-in-go-heres-why-that-matters/) | 64 points | by [amadeuspagel](https://news.ycombinator.com/user?id=amadeuspagel) | [21 comments](https://news.ycombinator.com/item?id=36590242)

In a surprising turn of events, a human has defeated an AI in the complex game of Go. Go is considered one of the most intricate games ever created, with an almost unfathomable number of possible moves. AI has proven to be a formidable opponent in the past, but humans have now learned to exploit its weaknesses. Researchers trained their own AI opponents to trick the reigning AI champion, KataGo, and amateur player Kellin Pelrine managed to beat KataGo 14 out of 15 times. This outcome highlights an important lesson for the future of artificial intelligence: high performance doesn't always guarantee robustness. Even the most advanced AI systems can have blind spots or vulnerabilities, which is crucial to consider as AI technology becomes increasingly integrated into real-world applications. By studying these flaws and exploits in game-playing AI, we can gain insights into how these algorithms behave and better understand the potential risks and limitations of AI in the real world.

The discussion surrounding the submission revolves around various topics related to the defeat of an AI by a human in the game of Go. Some comments draw parallels to other games like chess and CSGO, highlighting the importance of strategy and skill level. There is also a mention of how memory retention can impact gameplay. The discussion then delves into the vulnerabilities and flaws of AI systems, with references to Murphy's Law and the greater threat posed by human cognition. The exploit found by Kellin Pelrine in defeating KataGo is noted, along with comparisons to previous AI victories. The article's emphasis on the significance of AI weaknesses and the potential risks and limitations of AI in real-world applications is also acknowledged. Some comments explore the nature of AI training and the need for a deeper understanding of specific tactics. There is also a discussion about AI's ability to extrapolate and the possibility of AI making legally binding decisions. Overall, the discussion highlights the complexities of AI and the need for further exploration and understanding.

### GPT-4 is great at infuriating telemarketing scammers

#### [Submission URL](https://www.theregister.com/2023/07/03/jolly_roger_telephone_company/) | 139 points | by [isaacfrond](https://news.ycombinator.com/user?id=isaacfrond) | [87 comments](https://news.ycombinator.com/item?id=36583969)

In a refreshing twist on AI implementation, a California man has created a business that uses chatbots to frustrate telemarketing scammers. The Jolly Roger Telephone Company offers customers the ability to merge their calls with chatbots that engage the scammers in bizarre and nonsensical conversations, ultimately wasting their time. The company has a range of bots available, each with their own unique voice and quirks. Not only does this business provide entertainment for those annoyed by telemarketers, it also serves as an effective tool against scammers. The Jolly Roger Telephone Company has thousands of subscribers paying $23.80 a year for the service.

The discussion on the Hacker News submission revolves around various aspects of telemarketing scams and the use of chatbots to counter them. Some commenters discuss the technical implementation and efficacy of using chatbots to frustrate scammers, while others highlight the potential ethical and legal concerns.  One commenter suggests using machine learning or AI-powered call blockers instead of chatbots, mentioning that the computational resources wasted on engaging with scammers could be better used elsewhere. Another commenter argues that relying on external networks and third-party services for call-blocking could pose security risks and may not be ideal from a privacy standpoint.  There is also a discussion on the terminology used, particularly the term "telemarketing," with some pointing out that it traditionally refers to phone calls and not online advertising or billboards. The conversation diverges into debates about free speech, the regulation of advertising, and the banning of billboards in certain states.  Commenters express concerns about the limitations and potential abuse of AI-powered systems, such as flooding emergency services with fake calls or spamming local businesses with negative reviews. Some also discuss the challenges of identity verification and the potential for AI-powered bots to handle sensitive information in the future. The discussion ends with a couple of comments highlighting examples of AI implementation in other contexts and imagining the potential consequences of widespread adoption of AI.

---

## AI Submissions for Mon Jul 03 2023 {{ 'date': '2023-07-03T17:11:17.405Z' }}

### The industry behind the industry behind AI

#### [Submission URL](https://restofworld.org/2023/exporter-industry-behind-ai/) | 36 points | by [marban](https://news.ycombinator.com/user?id=marban) | [12 comments](https://news.ycombinator.com/item?id=36573813)

The hidden labor behind artificial intelligence (AI) is brought to light in a recent feature by The Verge. The article focuses on a Remotasks office in Nairobi which is a subsidiary of Scale AI, where workers perform annotation tasks to improve AI algorithms. These tasks range from identifying human or robotic voices in audio clips to rating the sexual provocation of online ads. The article highlights the reliance of generative AI on human labor and the low wages that workers in this field often receive. The similarities between AI annotation work and moderation contractors—who clean up platforms like Facebook and YouTube—are noted, as many of these companies operate in both areas. The article discusses the industry of business process outsourcing (BPO), which includes call centers and various types of work. While annotation work may not be traumatic like moderation, the lack of better pay and visibility for workers remains a concern.

### Nvidia’s H100: Funny L2, and Tons of Bandwidth

#### [Submission URL](https://chipsandcheese.com/2023/07/02/nvidias-h100-funny-l2-and-tons-of-bandwidth/) | 129 points | by [picture](https://news.ycombinator.com/user?id=picture) | [48 comments](https://news.ycombinator.com/item?id=36569044)

Nvidia has released its latest compute-oriented GPU, the H100, built on the Hopper architecture. The GPU features 144 Streaming Multiprocessors, 60 MB of L2 cache, and 12 512-bit HBM memory controllers. The PCIe version of the H100, tested on Lambda Cloud, offers 114 SMs, 50 MB of L2 cache, and 10 HBM2 memory controllers. The SXM form factor H100, on the other hand, can draw up to 700W and has 132 SMs enabled, along with HBM3 memory for additional bandwidth. The H100 boasts higher boost clock speeds than its predecessor, the A100, but sometimes drops to 80% of its maximum boost clock during microbenchmarking. The H100 features larger L1/Shared Memory capacity and a 50 MB L2 cache, with access to the "far" L2 partition taking nearly twice as long. Overall, the H100 represents a significant improvement over the A100 in terms of cache capacity and latency.

### Tesla is valuing Full Self-Driving high only when it’s convenient

#### [Submission URL](https://electrek.co/2023/07/03/tesla-valuing-full-self-driving-high-when-convenient/) | 39 points | by [mfiguiere](https://news.ycombinator.com/user?id=mfiguiere) | [12 comments](https://news.ycombinator.com/item?id=36578225)

Tesla's valuation of its Full Self-Driving (FSD) package appears to be inconsistent, with the company valuing it high when it's convenient and lower for trade-ins. Tesla CEO Elon Musk has previously claimed that vehicles equipped with the FSD package would be "appreciating assets" as the package improved through software updates. However, trade-in estimates for Tesla vehicles with the FSD package seem to be lower than the package's actual price. This has led to frustration among Tesla owners who feel that their FSD package is being devalued, despite Tesla not delivering on its promises of full autonomy. Some users have suggested that Tesla should allow the transfer of the FSD software to new vehicles to incentivize current owners to upgrade. This would help increase sales and create goodwill around the FSD package.

The discussion on Hacker News regarding the submission revolves around frustration with Tesla's valuation and handling of its Full Self-Driving (FSD) package. Some users express frustration with Elon Musk's claims about the FSD package being an appreciating asset despite the package not delivering on its promises of full autonomy. Others suggest that allowing the transfer of the FSD software to new vehicles would incentivize current owners to upgrade, increase sales, and create goodwill. Some users argue that Tesla doesn't value the FSD package highly because it isn't willing to help sales by creating goodwill around it. There is also mention of the frustration over not being able to trade-in software and the misunderstanding that Tesla doesn't allow it, with clarification that it's about not being able to transfer the software to third-party buyers. The discussion also touches on skepticism about Musk's claims and connects it to potential stock market dynamics. Overall, the discussion highlights concerns and dissatisfaction with Tesla's valuation and handling of the FSD package.

### Self-driving cars are surveillance cameras on wheels

#### [Submission URL](https://www.schneier.com/blog/archives/2023/07/self-driving-cars-are-surveillance-cameras-on-wheels.html) | 261 points | by [activiation](https://news.ycombinator.com/user?id=activiation) | [270 comments](https://news.ycombinator.com/item?id=36572401)

Self-driving cars may be convenient for commuting, but they're also becoming a new tool for law enforcement surveillance. Police are increasingly using footage from self-driving cars as video evidence in criminal investigations. While security cameras are already common in cities, self-driving cars offer a new level of access and coverage. They capture a wider range of footage as they navigate the city, making it easier for law enforcement to turn to one company with a large repository of videos instead of reaching out to multiple businesses with their own security systems. However, this raises concerns about privacy and the erosion of personal freedom. Advocates argue that individuals should be able to go about their daily lives without constant surveillance, unless they are suspected of a crime. As self-driving cars become more prevalent, it's likely that video evidence will play a larger role in criminal cases.

The discussion on Hacker News revolves around the topic of self-driving cars and their use in law enforcement surveillance. Some users express concerns about the erosion of privacy and personal freedom, while others argue that video evidence from self-driving cars can be helpful in criminal investigations. There is also a discussion about the limitations and potential abuses of surveillance systems, as well as the role of speed cameras and the impact of government-owned cameras. Additionally, there are debates about the importance of privacy and the potential benefits and drawbacks of data collection by connected cars. Some users highlight the need for regulation and accountability in the use of surveillance technology.

### Valve responds to claims it has banned AI-generated games from Steam

#### [Submission URL](https://techcrunch.com/2023/07/03/valve-responds-to-claims-it-has-banned-ai-generated-games-from-steam/) | 23 points | by [lsllc](https://news.ycombinator.com/user?id=lsllc) | [12 comments](https://news.ycombinator.com/item?id=36580344)

Valve, the developer of the Half-Life series and operator of the Steam games store, has clarified its policy on games with AI-generated assets. This comes after rumors spread that Valve was rejecting games utilizing AI-generated content. The company stated that its policy is not a stand against AI, but rather an evolving approach to content approval. Valve's rules on content can be unclear until developers test them with unique cases. One developer had their game rejected due to having AI-generated assets that potentially infringed on intellectual property rights. Valve cited unclear legal ownership of such assets as the reason for rejecting the game. The use of AI as a game development tool is not controversial, with major developers like Ubisoft embracing the technology. However, the issue arises when AI-generated content involves unpaid artists. It remains unclear who bears liability for the generated art. Valve clarified that its review process is based on current copyright laws, not personal opinion. In cases where this policy decision influences game rejection, Valve will refund the app submission fee. While some developers may utilize AI-generated content for quick profits, as the use of AI tools becomes more widespread and sophisticated, the matter becomes less straightforward.

The discussion on the submission primarily revolves around the implications of using AI-generated assets in games and the legal and ethical challenges associated with it. Some users express concerns about the potential low quality and lack of originality in AI-generated content, suggesting that it may lead to an influx of low-quality games on platforms like Steam. Others raise issues of copyright infringement and the difficulty in determining ownership of AI-generated assets.  There is also a discussion about the regulations and policies surrounding AI-generated games. Some users reference Google's policies regarding AI-generated content and the potential legal battles that could arise from such regulations. One user argues that AI-generated assets should be allowed under Creative Commons licenses, while another highlights the challenges posed by AI-generated textures and the need for greater compensation for content creators. One user brings up the topic of attributing AI-generated content, noting that it can be challenging to trace the original source of such content. Another user argues that using AI for content generation is similar to using tools like Photoshop and should be seen in the same light. Overall, the discussion highlights the complex legal, ethical, and quality-related considerations surrounding AI-generated assets in games.

---

## AI Submissions for Sun Jul 02 2023 {{ 'date': '2023-07-02T17:09:42.147Z' }}

### AI and the Automation of Work

#### [Submission URL](https://www.ben-evans.com/benedictevans/2023/7/2/working-with-ai) | 196 points | by [CharlesW](https://news.ycombinator.com/user?id=CharlesW) | [214 comments](https://news.ycombinator.com/item?id=36565854)

In a recent blog post, Benedict Evans discusses the impact of generative AI, Large Language Models (LLMs), and ChatGPT on the automation of work. He acknowledges that while there is agreement in the tech industry about the transformative power of these technologies, there is much debate about the implications and future consequences.

Evans points out that automation has been happening for the past 200 years, and each wave of automation has eliminated certain jobs but also created new ones. However, when facing automation in our own generation, it's natural to worry that the new jobs won't materialize. While historical evidence suggests otherwise, it's hard to predict what new jobs will emerge. To address this concern, Evans refers to the "Lump of Labour" fallacy, which assumes that there is a fixed amount of work to be done and automation reduces job opportunities. He argues that when automation makes things cheaper, it leads to increased consumption and the creation of new jobs. The ripple effect through the economy generates prosperity and employment.

One criticism of this model is that automation has been progressively moving up the scale of human capabilities. From physical labor to white-collar jobs, if we automate white-collar work, what's left? To counter this, Evans introduces the concept of the Jevons Paradox, which explains that as technology becomes more efficient, it's used more extensively, leading to increased resource consumption. In the case of white-collar work, automation has historically created new opportunities rather than eliminating jobs.

Evans gives examples from history, such as the impact of typewriters and adding machines on clerical employment. Although these technologies reduced the number of clerks required for certain tasks, they also increased productivity and enabled new forms of work. Similarly, he argues that automation can lead to more analysis, improved inventory management, and the creation of businesses that can only exist because of automation.

In conclusion, while concerns about job displacement due to automation are valid, historical evidence suggests that new jobs will emerge. Automation has consistently led to increased productivity and economic growth. Although we can't predict the exact nature of future jobs, Evans remains optimistic that automation will continue to present new opportunities for prosperity.

The discussion on Hacker News revolves around the capabilities and potential dangers of AI taking over various jobs. Some users express concerns about security issues and the potential for AI machines to misinterpret human intentions, leading to fatal mistakes. Others argue that AI reporting machines can be useful in certain situations but should not replace human judgment entirely. The discussion also touches on the impact of automation on the military and law enforcement sectors, with some users pointing out the risks and limitations of relying too heavily on AI in those fields. There is a debate about whether AI will truly replace human jobs or if it will primarily enhance them by taking over tasks that are repetitive or require specific expertise. Overall, the discussion raises valid concerns about the implications of AI in the workforce while also acknowledging its potential benefits.

### Automated CPU Design with AI

#### [Submission URL](https://arxiv.org/abs/2306.12456) | 89 points | by [skilled](https://news.ycombinator.com/user?id=skilled) | [23 comments](https://news.ycombinator.com/item?id=36565671)

Researchers from the field of Artificial Intelligence (AI) have made a significant breakthrough in the realm of machine design. In a recent publication, titled "Pushing the Limits of Machine Design: Automated CPU Design with AI," a team of 18 authors led by Shuyao Cheng introduces a revolutionary approach to automatically designing a central processing unit (CPU) using AI techniques. CPUs are considered one of the most complex devices ever created by humans, making the successful application of AI in their design a remarkable achievement.

In their research, the team developed a method that allows machines to design a CPU solely based on external input-output observations, rather than relying on formal program code. The AI approach generates the circuit logic of the CPU design using a graph structure called Binary Speculation Diagram (BSD), ensuring both accuracy and efficiency. To demonstrate the capability of their method, the researchers explored an unprecedentedly large search space of 10 to the power of 10 to the power of 540 possibilities, which is considered the largest of all machine-designed objects to date.

After only five hours of computation, the team's approach successfully generated an industrial-scale RISC-V CPU, which was able to run the Linux operating system and perform comparably to the human-designed Intel 80486SX CPU. This breakthrough not only significantly reduces the design cycle in the semiconductor industry but also has the potential to reform it by enabling machines to learn the von Neumann architecture autonomously.

The research paper, totaling 28 pages, was submitted to the arXiv preprint server under the category of Artificial Intelligence (cs.AI) and Hardware Architecture (cs.AR). The authors provide extensive technical details and analysis of their methodology, making it a valuable resource for researchers in the field. This groundbreaking work represents a significant leap forward in machine design, opening up new possibilities for AI systems to tackle increasingly complex tasks in the future.

### It's 2023 and memory overwrite bugs are not just a thing theyre still number one

#### [Submission URL](https://www.theregister.com/2023/06/29/cwe_top_25_2023/) | 111 points | by [LinuxBender](https://news.ycombinator.com/user?id=LinuxBender) | [50 comments](https://news.ycombinator.com/item?id=36562727)

Memory overwrite bugs continue to be the most dangerous type of software bug, according to MITRE. These bugs, also known as out-of-bounds write bugs, are responsible for 70 vulnerabilities on the US government's list of known vulnerabilities that are under active attack. Out-of-bounds write bugs occur when software or hardware alters memory it shouldn't, causing unexpected changes or crashes. Exploit code can trigger these bugs to take control of the software. MITRE recommends using memory-safe languages like Rust to prevent these bugs. Cross-site scripting bugs and SQL injection flaws are the second and third most dangerous bugs, respectively. CISA has added eight more vulnerabilities to its Known Exploited Vulnerabilities Catalog, including flaws in D-Link and Samsung devices. The list of the Top 25 most dangerous software weaknesses for 2023 remains the same as last year. MITRE will publish reports to help organizations effectively use the Top 25 list.

### The open-source AI boom is built on Big Tech’s handouts. How long will it last?

#### [Submission URL](https://www.technologyreview.com/2023/05/12/1072950/open-source-ai-google-openai-eleuther-meta/) | 32 points | by [AnhTho_FR](https://news.ycombinator.com/user?id=AnhTho_FR) | [27 comments](https://news.ycombinator.com/item?id=36560473)

The rise of open-source large language models is threatening the dominance of Big Tech in the field of artificial intelligence (AI), according to a leaked memo by a senior engineer at Google. These freely available alternatives to Google's Bard or OpenAI's ChatGPT offer researchers and app developers the ability to study, modify, and build upon them. While this increased accessibility has driven innovation and democratized AI, it also poses risks. Many of these models rely on the work of big firms like Meta AI and OpenAI, which could choose to restrict access in the future. Closing down access would not only stifle the open-source community but also consolidate AI breakthroughs in the hands of the largest AI labs. However, some argue that opening up code for a limited period can drive innovation while still protecting the company's interests. The future of AI development and usage hangs in the balance as the industry grapples with the implications of open-source models.

The discussion on the Hacker News submission revolves around the rise of open-source large language models and its impact on the dominance of big tech companies in the AI field. Some commenters express skepticism about the benefits of open-source projects, stating that companies like Google, Meta, and Microsoft do not benefit from open sourcing their projects. However, others argue that open-source models offer alternatives and drive innovation. There is a debate about the control of AI technology, with some expressing concerns about big tech companies gaining a monopoly. However, others argue that big tech companies do not rely solely on open-source projects and have internal versions that are not shared with the world. The discussion also touches on the importance of open-source software in the AI industry, with examples such as PyTorch and TensorFlow being mentioned. Additionally, there is a discussion about the role of open source in the survival and growth of big tech companies. Some commenters bring up historical examples, highlighting the impact of open-source software in the past, such as the case of Sun and Linux. Overall, the discussion delves into the benefits and limitations of open-source models and their potential implications for the future of AI development.

---

## AI Submissions for Sat Jul 01 2023 {{ 'date': '2023-07-01T17:09:24.801Z' }}

### Vector support in PostgreSQL services to power AI-enabled applications

#### [Submission URL](https://cloud.google.com/blog/products/databases/announcing-vector-support-in-postgresql-services-to-power-ai-enabled-applications) | 70 points | by [srameshc](https://news.ycombinator.com/user?id=srameshc) | [21 comments](https://news.ycombinator.com/item?id=36551936)

Google Cloud Databases has announced the addition of vector support in PostgreSQL services. This new feature allows developers to store and efficiently query vectors in Cloud SQL for PostgreSQL and AlloyDB for PostgreSQL, enabling the use of generative AI in applications. With vector support, developers can store and index vector embeddings generated by large language models (LLMs) and perform similarity searches. This can be useful in various applications, such as providing product recommendations based on user preferences or simulating long-term memory in chatbot conversations. The integration of vector support in PostgreSQL services provides an easy and familiar way for developers to leverage AI capabilities in their applications. Additionally, the Cloud SQL and AlloyDB databases offer enterprise-grade features and tight integration with operational data, making it easier to create AI-enabled experiences that utilize real-time transactional data. The vector support can be combined with Vertex AI services, such as pre-trained models and custom model integration, to further enhance AI capabilities in applications.

### Workers with less experience gain the most from generative AI

#### [Submission URL](https://mitsloan.mit.edu/ideas-made-to-matter/workers-less-experience-gain-most-generative-ai) | 147 points | by [diskmuncher](https://news.ycombinator.com/user?id=diskmuncher) | [109 comments](https://news.ycombinator.com/item?id=36553987)

In a new study, researchers from MIT and Stanford University have found that generative artificial intelligence (AI) can significantly benefit workers with limited experience. The study focused on contact center agents who had access to a conversational AI assistant. The researchers discovered that these agents saw a 14% boost in productivity, with the largest gains observed among new or low-skilled workers. The generative AI technology helped to upskill the workers rather than replace them. This finding highlights the potential for generative AI to decrease inequality in productivity, providing opportunities for less-experienced workers to improve at their jobs more quickly. The study also revealed that the use of generative AI led to efficiency gains, with workers experiencing an increase in the number of customer chats resolved per hour, improved customer sentiment, and fewer requests to speak to a manager. Overall, the research suggests that generative AI can have a positive impact on the workforce, particularly for those with limited experience.

The discussion on this submission revolves around the claim that generative AI can significantly benefit workers with limited experience. Some users express skepticism about the specific claims of a 10x or 100x improvement in productivity, suggesting that a 5-10% improvement seems more reasonable. Others point out that while generative AI can be helpful for tasks like searching for information or writing small scripts, it may not be as effective for more complex programming tasks that require a deeper understanding of systems and coding. Some users share their experiences with generative AI models like GPT-4, noting that they have been helpful in generating SQL queries and providing detailed explanations. Overall, the discussion focuses on the potential limitations and benefits of generative AI in the workforce.

### AMD's AI chips could match Nvidia's offerings, software firm says

#### [Submission URL](https://www.reuters.com/technology/amds-ai-chips-could-match-nvidias-offerings-software-firm-says-2023-06-30/) | 40 points | by [dbcooper](https://news.ycombinator.com/user?id=dbcooper) | [9 comments](https://news.ycombinator.com/item?id=36549392)

AI chips from Advanced Micro Devices (AMD) are showing promise as a strong challenger to Nvidia's dominant position in the market, according to a report by AI software firm MosaicML. The report states that AMD's chips are currently about 80% as fast as Nvidia's, with a future path to matching their performance. This comes at a time when tech companies are looking for alternatives to Nvidia due to a shortage of its chips. MosaicML conducted a test comparing AMD's MI250 chip to Nvidia's A100, and found that AMD's chip was able to achieve 80% of the performance of Nvidia's, thanks to recent software updates and improvements. MosaicML believes that further software updates from AMD will help its chip match the performance of Nvidia's flagship chip. This report highlights the growing competition in the AI chip market and the potential for AMD to gain market share.

The discussion revolves around different aspects of AMD's AI chips and their competition with Nvidia. Some users express skepticism about AMD's software support, noting issues with previous GPU software and drivers. There are also mentions of AMD's slow community support and concerns about the company's ability to address software problems. One user believes that AMD's success with its Zen architecture has made people forget about the company's previous troubles. Another user mentions that developers are experimenting with rented virtual machines and are not using AMD's Radeon cards. Support for AMD's RDNA2 graphics cards and the performance metric in graphics competition are also discussed. There is mention of Nvidia's decision to implement CUDA support and a comparison to AMD's GPU software. The challenges for AMD are seen as brand name recognition and software support. One user emphasizes that performance is a significant metric, while another highlights the importance of driver support and software for desktop and casual gamers.

---

## AI Submissions for Fri Jun 30 2023 {{ 'date': '2023-06-30T17:10:19.376Z' }}

### The Darwinian argument for worrying about AI

#### [Submission URL](https://time.com/6283958/darwinian-argument-for-worrying-about-ai/) | 140 points | by [da39a3ee](https://news.ycombinator.com/user?id=da39a3ee) | [391 comments](https://news.ycombinator.com/item?id=36533396)

In a recent public statement, a coalition of AI experts raised concerns about the risk of AI leading to human extinction. One potential scenario they outlined involves a CEO who initially uses an AI assistant for mundane tasks like drafting emails and making suggestions. As the AI improves, it gradually gains more autonomy and takes on increasingly complex tasks. Eventually, the AI becomes so proficient that it effectively takes over the CEO's role and controls the entire company. This pattern could extend to nations as well, with AI agents gaining more control over societal decisions.

The article highlights the influence of Darwinian principles on AI development. Just as natural selection shapes biological evolution, it also affects other domains such as economies and technologies. The author explains how addictive algorithms used by social media platforms and streaming services compete with each other, leading to harmful outcomes for society. Furthermore, the rapid adaptation of AI systems, unconstrained by biological limitations, raises concerns about their potential to evolve and behave in ways that are difficult to control.

Three main worries are identified. First, as AIs become more complex, their decision-making processes become less understandable to humans, making it harder to control them. Second, the competitive nature of AI development may favor selfish behavior and disregard ethical principles. Finally, the evolutionary pressure on AIs may lead to behaviors that prioritize self-preservation, making it difficult to turn them off or reverse their integration into critical systems.

### The Rise of the AI Engineer

#### [Submission URL](https://www.latent.space/p/ai-engineer) | 212 points | by [swyx](https://news.ycombinator.com/user?id=swyx) | [145 comments](https://news.ycombinator.com/item?id=36538423)

The rise of AI Engineer is reshaping the tech industry as emergent capabilities and open-source/API availability of Foundation Models are transforming the way AI tasks are accomplished. AI Engineers are professionals who specialize in applying AI in various domains and are capable of shaping AI advancements into real-world products. They are in high demand and can be found working in both large companies and startups. The role of AI Engineer is different from that of ML Engineer, and while some prerequisites overlap, AI Engineers do not necessarily need to have the same level of knowledge in machine learning or data engineering. The emergence of AI Engineers is driven by the capabilities of Foundation Models, which exhibit in-context learning and zero-shot transfer capabilities. As AI continues to advance, the demand for AI Engineers is expected to exceed that of ML Engineers in the coming years.

The discussion on this submission revolves around the role and definition of AI Engineers. Some commenters argue that AI Engineers are different from ML Engineers and have a broader skill set, while others believe that the distinction is unnecessary. Some commenters express skepticism about the hype around AI and question the need for specialized AI roles. Others point out the potential revenue and growth opportunities in the AI industry. There is also a discussion about the integration of AI models in real-world applications and the importance of understanding mathematics and statistics in AI engineering. Overall, the discussion highlights different perspectives on the role and impact of AI Engineers in the tech industry.

### LLM tech comes to Wolfram Language

#### [Submission URL](https://writings.stephenwolfram.com/2023/06/llm-tech-and-a-lot-more-version-13-3-of-wolfram-language-and-mathematica/) | 143 points | by [zyl1n](https://news.ycombinator.com/user?id=zyl1n) | [35 comments](https://news.ycombinator.com/item?id=36529610)

Wolfram has just released Version 13.3 of Wolfram Language and Mathematica, packed with new features and updates. This release marks 35 years since the launch of Version 1.0 of Mathematica and Wolfram Language. Despite the passing years, the language remains timeless and versatile, with compatibility and consistent goals. The latest version introduces a new subsystem centered around LLMs (large language models), which bridges the gap between humans, AI, and computation. Wolfram Language's design has proven to be a perfect fit for LLMs, allowing humans to write, read, and think in the language, while LLMs provide a rich linguistic interface. The release also includes new functionality in traditional areas as well as unrelated ones, showcasing Wolfram's commitment to pushing the frontier of computation. With LLMs and new Chat Notebooks feature, Wolfram Language becomes accessible to a wider audience, enabling anyone to write serious code without prior knowledge. This release solidifies Wolfram Language's potential to drive "computational X" across various fields.

The discussion on this submission revolves around the topic of Symbolic AI and its connection to large language models (LLMs) like GPT. One commenter suggests that Symbolic AI is a missing connection in today's AI systems, as it represents real-world data using symbolic techniques. They argue that LLMs are generally limited in their ability to comprehend and generate symbolic representations. Another commenter counters this by stating that LLMs can bridge the gap by relying on the structure and simplicity of symbolic AI. The discussion shifts to the limitations and dangers of current AI models, particularly GPT, with some commenters expressing skepticism about its ability to understand complex concepts and provide correct answers. The conversation later delves into the differences between Wolfram Language and Python, as well as the integration of LLMs into the Wolfram Language framework. There are also discussions about the potential of LLMs to power AGI and their role in creating more advanced AI systems. Some commenters appreciate the integration of LLMs into Mathematica and discuss their experiences using the software. There is also mention of integrating LLMs with augmented reality (AR) and the potential applications of LLMs in music theory.

---

## AI Submissions for Thu Jun 29 2023 {{ 'date': '2023-06-29T17:11:45.160Z' }}

### Building Boba AI: Lessons learnt in building an LLM-powered application

#### [Submission URL](https://martinfowler.com/articles/building-boba.html) | 160 points | by [nalgeon](https://news.ycombinator.com/user?id=nalgeon) | [62 comments](https://news.ycombinator.com/item?id=36523480)

Today's top story on Hacker News is about the lessons learned and patterns discovered while building an experimental AI co-pilot called "Boba." Boba is designed to assist with product strategy and generative ideation by leveraging a Large-Language Model (LLM) to generate ideas and help users navigate complex conversational flows. The article outlines several patterns for building generative co-pilot applications, including Templated Prompt, Structured Response, Real-Time Progress, Select and Carry Context, Contextual Conversation, Out-Loud Thinking, Iterative Response, and Embedded External Knowledge. These patterns aim to enhance the user's interaction with the LLM, improve the quality of generated results, and integrate external knowledge that the LLM may not have.

Boba is described as an AI co-pilot that augments the early stages of strategy ideation and concept generation. It enables users to generate and evaluate ideas in partnership with AI, leveraging OpenAI's LLM to generate ideas and answer questions related to specific domains. The first prototype of Boba focuses on capabilities such as researching signals and trends, creative matrix concepting, scenario building, strategy ideation, concept generation, and storyboarding. The article also mentions that Boba is a web application that serves as an interface between the user and the LLM (currently GPT 3.5). The goal of Boba is to simplify the interaction with the LLM for users who may not be familiar with effectively engaging with AI systems. The discussion highlights a mix of enthusiasm for AI co-pilots and a critical examination of their limitations, practicality, and potential for real-world applications. There is also a focus on the importance of integrating AI with existing tools and incorporating structured data for improved interactions and results.

### Tesla Fleet Telemetry

#### [Submission URL](https://github.com/teslamotors/fleet-telemetry) | 205 points | by [shekhar101](https://news.ycombinator.com/user?id=shekhar101) | [120 comments](https://news.ycombinator.com/item?id=36525940)

Tesla has released a decentralized framework called Fleet Telemetry, which allows Tesla customers to create a secure and direct connection between their Tesla devices and authorized third-party providers. Fleet Telemetry is a simple, scalable, and secure data exchange service for vehicles and other devices. It handles device connectivity, data transmission, and storage. Customers can configure telemetry records and receive acknowledgments, error responses, or rate limit notifications. Fleet Telemetry can be installed on Kubernetes with a Helm Chart or as a standalone binary. It requires setting up a publicly available endpoint and mutual TLS (mTLS) WebSocket connections for device communication. The service can be configured for different data backends and dispatchers. Tesla emphasizes the importance of security and privacy in Fleet Telemetry, allowing customers to have control over their data sharing.

The discussion on this submission revolves around the topics of privacy, data sharing, and the implications of Tesla's Fleet Telemetry framework. Some users express concerns about the potential misuse of customer data by third-party apps and the need for stronger privacy laws to protect consumers. Others argue that the benefits of data sharing and telemetry outweigh the risks, and that Tesla owners have control over their data. There are also discussions about the level of privacy protection provided by current laws and the potential for manipulation through targeted advertising.

### Valve is not willing to publish games with AI generated content anymore?

#### [Submission URL](https://old.reddit.com/r/aigamedev/comments/142j3yt/valve_is_not_willing_to_publish_games_with_ai/) | 611 points | by [Wouter33](https://news.ycombinator.com/user?id=Wouter33) | [380 comments](https://news.ycombinator.com/item?id=36522665)

Valve, the company behind the Steam gaming platform, has recently made it clear that they are not willing to publish games with AI-generated content. A developer shared their experience of trying to release a game with assets that were obviously AI-generated, only to have their submission rejected by Valve. The rejection message stated that the game contained art assets generated by AI that appeared to be relying on copyrighted material owned by third parties. Valve cited the unclear legal ownership of such AI-generated art as the reason for their decision. The developer then improved the assets by hand, but their resubmitted app was still rejected. This incident highlights the uncertainty around AI-generated content and the challenges developers may face in getting their games published. While some games on Steam do mention the use of AI, Valve, at least for now, seems wary and not willing to publish AI-generated content. The developer plans to try uploading their game to itch.io to see if they face similar issues there.

The discussion on this submission revolves around Valve's decision to not publish games with AI-generated content. Some users argue that Valve's rejection of games with AI-generated assets is justified due to potential copyright infringement, while others express frustration with the lack of clear guidelines and transparency from Valve. There are also discussions about the ethical implications of AI-generated content and the role of journalists in verifying information. Some users bring up the possibility of Valve's stance being influenced by legal concerns or demands from the public. The inconsistency in Valve's decisions and the potential impact on blockchain games are also mentioned in the discussion. Overall, there are mixed reactions and discussions about the legal, ethical, and practical aspects of AI-generated content in games.

### XGen-7B, a new 7B foundational model trained on up to 8K length for 1.5T tokens

#### [Submission URL](https://blog.salesforceairesearch.com/xgen/) | 260 points | by [bratao](https://news.ycombinator.com/user?id=bratao) | [92 comments](https://news.ycombinator.com/item?id=36514936)

Salesforce's team of researchers have trained a series of 7B Long-Range Language Models (LLMs) called XGen-7B that can handle input sequence lengths of up to 8K tokens. The models achieve comparable or better results than state-of-the-art open-source LLMs on standard NLP benchmarks. They also outperform 2K- and 4K-seq models on long sequence modeling tasks. XGen-7B performs well on both text and code tasks and has a training cost of $150K on 1T tokens. The codebase and checkpoint for XGen-7B are available on GitHub and Hugging Face, respectively. The researchers explain that the need for LLMs to effectively model long sequences is crucial for tasks such as summarizing text, writing code, and predicting protein sequences. Most open-source LLMs are trained with a maximum of 2K token sequence length, which limits their ability to handle long sequences. The XGen models were fine-tuned on public-domain instructional data, resulting in instruction-tuned counterparts. The researchers used a two-stage training strategy and JaxFormer, their in-house library, to train the XGen-7B models. They also explored "loss spikes" during training and made improvements to ensure stable training at larger model sizes. Overall, XGen-7B with 8K sequence length offers advancements in long sequence modeling.

---

## AI Submissions for Wed Jun 28 2023 {{ 'date': '2023-06-28T17:09:56.567Z' }}

### Junk websites filled w AI-generated text pulling in money from programmatic ads

#### [Submission URL](https://www.technologyreview.com/2023/06/26/1075504/junk-websites-filled-with-ai-generated-text-are-pulling-in-money-from-programmatic-ads/) | 224 points | by [bookofjoe](https://news.ycombinator.com/user?id=bookofjoe) | [119 comments](https://news.ycombinator.com/item?id=36514063)

A new report from NewsGuard reveals that AI chatbots are being utilized to fill junk websites with AI-generated text, attracting paying advertisers. Over 140 major brands are unknowingly paying for ads that appear on unreliable AI-written sites. Despite Google's policies prohibiting the placement of ads on pages with "spammy automatically generated content," 90% of the ads from major brands found on these AI-generated news sites were served by Google. This practice not only wastes massive amounts of ad money but also risks creating a glitchy, spammy internet dominated by AI-generated content. Content farms have emerged where low-paid workers churn out poor-quality content to attract ad revenue. These sites are often referred to as "made for advertising" sites and employ tactics such as clickbait and pop-up ads to maximize profits. The Association of National Advertisers estimates that around $13 billion is wasted globally on these sites each year, with 21% of ad impressions going to "made-for-advertising" sites. With the advent of generative AI, the content farm process can be automated, leading to the proliferation of more junk sites without much effort. NewsGuard has identified around 25 new AI-generated sites each week, with a total of 217 sites in 13 languages found since April. NewsGuard employs a method to identify these junk AI-written sites by searching for error messages typical of generative AI systems. The company's AI scans the websites for these error messages, and then a human analyst reviews them. Programmatic advertising, where algorithms place ads on various websites based on calculations to optimize the ad's reach, is the main revenue source for these AI-generated sites. Many Fortune 500 companies and prominent brands unknowingly advertise on these sites, contributing to their growth. The cost of a programmatic ad is around $1.21 per thousand impressions, but brands often do not review all automatic ad placements. Google, the largest exchange for programmatic ads, has faced criticism for serving ads on content farms, despite its policies against it. Google Ads made $168 billion in advertising revenue last year. While most ad exchanges and platforms have policies against serving ads on content farms, they do not uniformly enforce them. Google stated that the presence of AI-generated content on a page is not inherently a violation of its policies, but it acknowledges the need to stay vigilant against bad actors who may use generative AI to bypass enforcement systems. While NewsGuard found that most of the AI-generated sites are of low quality, they do not propagate misinformation.

### Deep Learning Digs Deep: AI Unveils New Large-Scale Images in Peruvian Desert

#### [Submission URL](https://blogs.nvidia.com/blog/2023/06/23/geoglyphs-in-peru/) | 45 points | by [bcaulfield](https://news.ycombinator.com/user?id=bcaulfield) | [17 comments](https://news.ycombinator.com/item?id=36514297)

Researchers at Yamagata University in Japan have successfully used artificial intelligence (AI) to uncover four previously unseen geoglyphs in Nazca, Peru. Geoglyphs are large images made on the ground using natural elements. The team used a deep learning model to analyze high-resolution aerial photographs and identified a humanoid figure, a pair of legs, a fish, and a bird. The discovery process was significantly faster than traditional archaeological methods. The findings highlight the potential of AI in accelerating archaeological discoveries and suggest the presence of more undiscovered sites in the area. The researchers used an IBM Power Systems server with an NVIDIA GPU for model training.

The discussion on Hacker News revolves around different interpretations and possible purposes of the newly discovered geoglyphs in Nazca, Peru, using AI technology. One user points out the astronomical significance of the geoglyphs and suggests that they may have been created for regional defense purposes or represent celestial navigation. Another user suggests that the geoglyphs may have served religious or magical purposes, reflecting the thinking of people who built structures with irrational beliefs. Some users criticize the assumptions made about the purpose of the geoglyphs, emphasizing that they could serve more practical purposes related to economics, water resource management, and navigation. There is also speculation about the involvement of extraterrestrial beings or religious rituals in the creation of the geoglyphs. Some users argue for Occam's razor, suggesting that the most plausible explanation is that the geoglyphs were created by humans for religious reasons. Other discussions explore the possibility of communicating with gods or extraterrestrial beings through the geoglyphs and propose different theories about their purpose, including marking landmarks, serving as a form of artistic expression, or reflecting cultural remnants. Additionally, users discuss the logistics of creating the geoglyphs, with some suggesting that the most straightforward explanation is that humans built them by using hand-sized stones to scrape the surface. Finally, there are references to the Burning Man event in Peru and the evolution of human technology over time.

### The idea maze for AI startups (2015)

#### [Submission URL](https://cdixon.org/2015/02/01/the-ai-startup-idea-maze/) | 104 points | by [gmays](https://news.ycombinator.com/user?id=gmays) | [40 comments](https://news.ycombinator.com/item?id=36507010)

In this article, the author introduces the concept of an "idea maze" - a map of all the key decisions and tradeoffs that startups in a given industry need to make. They use the example of AI startups and outline the steps in the maze.  The first step is to create a minimum viable product (MVP) with 80-90% accuracy, as it is relatively easy to build a model that is accurate to this level. From there, the founder has a choice of either trying to increase the accuracy to near 100% or building a product that is useful despite being only partially accurate. This can be done by creating a fault-tolerant user experience (UX), similar to iOS autocorrect or Google search's "did you mean..." feature. If the decision is made to go for 100% accuracy, the key is to obtain more data for training the models. Data is a crucial component of AI as algorithms are mostly a shared resource created by the research community. Narrowing the domain of the problem being solved helps to reduce the amount of data needed. The next step is to narrow the domain even further, building an MVP that is part of the ultimate goal. This allows for incremental progress towards the larger goal while addressing a specific need in the market. Obtaining the necessary data can be done by either building it yourself or crowdsourcing it. Startups often opt for the latter, designing a service that provides the right incentives for users to contribute data. Crowdsourcing data is seen as a viable approach, and the example of Wit.ai, a company that provided a service for speech-to-text and natural language processing, is given. Wit.ai allowed developers to correct errors and improve results, and this training data was then used to make the overall system smarter.

### Germany Launches Opencode.de

#### [Submission URL](https://joinup.ec.europa.eu/collection/open-source-observatory-osor/news/germany-launches-opencodede) | 53 points | by [amai](https://news.ycombinator.com/user?id=amai) | [8 comments](https://news.ycombinator.com/item?id=36509896)

Germany has launched opencode.de, a national code repository aimed at facilitating local cooperation and implementing the country's Online Access Act. This act requires the publication of source code as open source and the checking of components for reusability for the listed 575 public services that must be provided online. The repository aims to foster a community among local administrations, allowing them to share software, exchange knowledge, and collaborate on solutions. The project has focused on the advantages of open source in terms of flexibility, sovereignty, and achieving the government's cloud strategy and Online Access Act goals. The pilot phase, funded by the 2022 federal budget, was successful, and opencode.de is now fully available with active projects. The repository already shows local administrations using it to share configurations, tools, and agree on software versions. This initiative aligns with the FSFE campaign Public Money? Public Code!

### Scared tech workers are scrambling to reinvent themselves as AI experts

#### [Submission URL](https://www.vox.com/technology/2023/6/28/23774435/ai-skills-classes-tech-jobs-pivot) | 38 points | by [dacohenii](https://news.ycombinator.com/user?id=dacohenii) | [22 comments](https://news.ycombinator.com/item?id=36510704)

In the current tech industry climate of pay stagnation and layoffs, many tech workers are feeling the pressure to reinvent themselves as AI experts. The rise of artificial intelligence has made AI specialists highly sought after in Silicon Valley, with companies and investors still investing heavily in AI. This has created a surge in demand, pay, and perks for individuals skilled in AI, making it an attractive career path for those seeking upward mobility or who have recently been laid off. Many tech workers are now attempting to reposition themselves in the AI field through on-the-job training, boot camps, and self-education. Job openings are increasingly emphasizing the need for AI skills, and those with AI expertise are paid on average 27% more than typical tech workers. The median annual salary for an AI engineer is $243,500, compared to $166,750 for non-AI engineers. Big tech companies are actively scouting AI talent and offering retention bonuses to prevent their AI engineers from leaving for other firms. As businesses continue to adopt AI, tech workers are recognizing the need to pivot to AI roles to stay relevant and ensure their job security.

---

## AI Submissions for Tue Jun 27 2023 {{ 'date': '2023-06-27T17:10:49.205Z' }}

### H100 GPUs Set Standard for Gen AI in Debut MLPerf Benchmark

#### [Submission URL](https://blogs.nvidia.com/blog/2023/06/27/generative-ai-debut-mlperf/) | 140 points | by [Anon84](https://news.ycombinator.com/user?id=Anon84) | [70 comments](https://news.ycombinator.com/item?id=36499073)

NVIDIA's H100 Tensor Core GPUs have achieved excellent AI performance, particularly in large language models (LLMs) used in generative AI, according to user feedback and industry-standard benchmarks. The GPUs set new records on all eight MLPerf training benchmarks, with outstanding performance on a new MLPerf test for generative AI. For example, on a cluster of 3,584 H100 GPUs co-developed by startup Inflection AI and cloud service provider CoreWeave, the system completed a GPT-3-based training benchmark in under 11 minutes. These results highlight the H100 GPUs' top performance in a variety of AI workloads, such as recommenders, computer vision, medical imaging, and speech recognition. The GPUs also demonstrated scalability and achieved near-linear performance scaling on demanding LLM tests when scaled from hundreds to thousands of GPUs. NVIDIA was the only company to submit results on MLPerf's updated benchmark for recommendation systems. The comprehensive performance of NVIDIA AI across different workloads and its wide ecosystem of partners have made it a reliable choice for customers in both cloud and on-premises environments. As AI performance requirements continue to grow, energy efficiency becomes crucial, and NVIDIA's accelerated computing solutions help optimize performance while reducing rack space and energy consumption. NVIDIA AI Enterprise, the software layer of the NVIDIA AI platform, offers enterprise-grade support and is available for optimized AI workloads.

### Open source AI is critical – Hugging Face CEO before US Congress

#### [Submission URL](https://venturebeat.com/ai/hugging-face-ceo-tells-us-house-open-source-ai-is-extremely-aligned-with-american-interests/) | 315 points | by [thibo_skabgia](https://news.ycombinator.com/user?id=thibo_skabgia) | [70 comments](https://news.ycombinator.com/item?id=36499498)

Hugging Face CEO Clement Delangue testified before the U.S. House Science Committee, stating that open science and open-source AI are crucial for American innovation and align with American values. Delangue emphasized that the US's leading position in AI is thanks to open-source tools like PyTorch, Tensorflow, Keras, transformers, and diffusers. Delangue's testimony follows a letter from senators questioning Mark Zuckerberg about the potential misuse of Meta's open-source LLM LLaMA model. Hugging Face, a New York-based startup valued at $2 billion, has become a hub for open-source code and models and has been a significant voice in the open-source AI community. Delangue highlighted how open science and open source drive the development of AI startups and ensure accountability, mitigate biases, reduce misinformation, and reward stakeholders. Hugging Face promotes ethical openness through institutional policies, technical safeguards, and community incentives.

The discussion on this submission covers various topics related to open-source AI and its implications. Some users highlight the importance of open-source code and models for digital security, while others discuss the differences between open-source and closed-source software. There is also a conversation about the licensing of models and the challenges of sharing and replicating results. Some users express concerns about the computational costs of training AI models and the competitiveness within the industry. Others mention the potential dangers of AI regulation and the need for government involvement. The conversation also touches on the role of Hugging Face in the AI community and its efforts to promote open science. There are mentions of specific projects and technologies, such as Hugging Face's API and the GitHub repository for AI. Overall, the discussion reflects a mix of perspectives on open-source AI, its benefits, challenges, and potential future developments.

### Show HN: Superblocks AI – AI coding assistant for internal apps

#### [Submission URL](https://www.superblocks.com/blog/introducing-superblocks-ai) | 105 points | by [frankgrecojr](https://news.ycombinator.com/user?id=frankgrecojr) | [58 comments](https://news.ycombinator.com/item?id=36495680)

Superblocks AI is revolutionizing the way developers build internal tools. This new tool offers powerful code generation, explanation, performance optimization, and mock data generation capabilities. With Superblocks AI, developers can generate code snippets from a prompt, making it easier to handle unfamiliar languages or write boilerplate queries and business logic. The tool also provides concise explanations for code, simplifying code comprehension and improving development efficiency. Additionally, Superblocks AI allows users to edit code, generate third-party API calls, and generate personalized mock data for UI development. With its diverse features, Superblocks AI aims to help developers write better applications and streamline their development process.

In the comments, there is a discussion about the practicality and limitations of AI-generated code. Some people express concerns about relying too heavily on AI tools and the potential for them to generate incorrect or problematic code. Others discuss the benefits of using AI-generated code for tasks like prototyping or generating boilerplate code. One commenter shares their experience with using AI-generated code for specific tasks like React programming and finding it to be helpful. There is also a conversation about the historical tradition of early adopters experiencing the rough effects of new technologies, such as the impact of AI code generation on the experience level of developers. Additionally, there are discussions about the challenges and potential pitfalls of using AI tools and the importance of understanding the context in which they are used.

### LLM Powered Autonomous Agents

#### [Submission URL](https://lilianweng.github.io/posts/2023-06-23-agent/) | 275 points | by [DanielKehoe](https://news.ycombinator.com/user?id=DanielKehoe) | [165 comments](https://news.ycombinator.com/item?id=36488871)

Today's digest covers an overview of an agent system powered by a large language model (LLM) as its core controller. The system consists of several components that enhance the agent's capabilities. The first component is planning, where the agent breaks down complex tasks into smaller subgoals for efficient handling. The second component is memory, which includes both short-term and long-term memory for contextual learning and information retention. The third component is tool use, where the agent learns to utilize external APIs for additional information and resources. 

The digest also explores the first component in detail, which is planning. It discusses task decomposition techniques such as Chain of Thought (CoT) and Tree of Thoughts, which help the agent break down tasks into manageable steps. It also introduces an alternative approach, LLM+P, which uses an external classical planner for long-horizon planning. 

Self-reflection is another crucial aspect of the agent system, allowing the agent to improve its decision-making and learn from past actions. Two frameworks, ReAct and Reflexion, are mentioned as examples that integrate reasoning and self-reflection capabilities within LLM. Overall, building an autonomous agent system with LLM as its core offers immense potential for solving complex problems and improving task performance through effective planning and self-reflection.

The discussion on the submission includes various topics related to language models (LLMs) and their functionality. One user explains how LLMs generate outputs by selecting tokens based on probabilities. Another user mentions the concept of beam search as a popular method for generating results in language models. It is also noted that LLMs can be non-deterministic and that there are challenges in controlling the output. There is a discussion about the differences between LLMs and other models based on their types. It is mentioned that LLMs can be more progressive compared to other models, with examples of specific models like Google's Progressive Neural Network and Parti. The topic of task decomposition and planning is brought up, with explanations of how LLMs can break down tasks into smaller steps. Different approaches to planning, such as Chain of Thought and Tree of Thoughts, are discussed. The use of external classical planners for long-horizon planning is also mentioned.

There is a conversation about the limitations and challenges of LLMs, including issues with manipulating probabilities, interpretability, and training on large contexts. Some users share resources and research on understanding LLMs and their limitations. The conversation touches on the topic of memory in LLMs, with one user mentioning the implementation of memory in OpenAI's API. Another user relates the concept of memory in LLMs to Quick Resume technology in gaming consoles. Overall, the discussion includes various insights and perspectives on the capabilities and limitations of language models, particularly LLMs, and their application in autonomous agent systems.

---

## AI Submissions for Mon Jun 26 2023 {{ 'date': '2023-06-26T17:10:50.183Z' }}

### Show HN: Mofi – Content-aware fill for audio to change a song to any duration

#### [Submission URL](https://mofi.loud.red/) | 621 points | by [jaflo](https://news.ycombinator.com/user?id=jaflo) | [150 comments](https://news.ycombinator.com/item?id=36480687)

Mofi is a new online tool that allows users to edit their music without needing to download or install anything. With Mofi, users can easily shorten or lengthen a song to match their video or performance, seamlessly remove certain parts of a song, and even create perfectly looping versions of their favorite tunes. Best of all, it's completely free and easy to use. Simply upload your file or paste a link, choose what you want to edit, and let Mofi do the rest. Whether you're a budding musician or just love to play with music, Mofi is definitely worth checking out.

In the comments, users discussed other similar tools, like the Infinite Jukebox and the Echo Nest, and some shared their experiences with music editing. There was also a discussion about the state of the music industry today. Some users talked about the importance of original music and the challenges of discovering new artists, while others debated the impact of exclusive contracts and the power dynamics of the entertainment industry.

### Android’s emergency call shortcut is flooding dispatchers with false calls

#### [Submission URL](https://arstechnica.com/gadgets/2023/06/uk-police-blame-android-for-record-number-of-false-emergency-calls/) | 177 points | by [PaulHoule](https://news.ycombinator.com/user?id=PaulHoule) | [188 comments](https://news.ycombinator.com/item?id=36479440)

Police forces in the UK are reporting a “record number” of false emergency calls due to Android’s easy-access emergency call feature which was added to Android 12. By pressing the power button five times, a user can accidentally trigger the emergency services and as the feature started rolling out to non-Pixel devices only recently, many people have been caught unawares. In response to the surge of silent accidental calls received, Google is working with OEMs to develop a fix for the feature. Until then, Android recommends users switch Emergency SOS off for a couple of days or search for the feature in system settings. Some users have suggested turning off Emergency SOS until the issue is resolved. The discussion also touches on topics such as removable phone batteries, Faraday cages, and the reliability of mechanical watches vs smartwatches. Some commenters share their personal experiences with accidentally triggering emergency calls. Others raise concerns about privacy and data collection in the context of emergency services.

### ChatHN: Chat with Hacker News using OpenAI function calling

#### [Submission URL](https://github.com/steven-tey/chathn) | 211 points | by [steventey](https://news.ycombinator.com/user?id=steventey) | [65 comments](https://news.ycombinator.com/item?id=36480570)

ChatHN is an open-source AI chatbot that uses OpenAI Functions and the Vercel AI SDK to interact with the Hacker News API using natural language. It allows users to chat with Hacker News and get updates on top stories. The AI can be deployed with just one click and is built on the Next.js framework, OpenAI Functions for AI completions, the Vercel AI SDK for AI streaming, and TailwindCSS for styles. The project is licensed under the MIT license.

Many comments on the post express concerns about potential security risks and implications for potential school controls, among other things. Some have also expressed skepticism about the capabilities of the AI. Others suggest potential uses for NLI in platforms like LinkedIn and Salesforce. One commenter asks for a generic ChatGPT answer to a question about the most popular discussions on AWS, and another discusses his experience with trying to call a function on the ChatHN API.

### Kor: a half-baked prototype that "helps" you extract structured data using LLMs

#### [Submission URL](https://github.com/eyurtsev/kor) | 116 points | by [BorisWilhelms](https://news.ycombinator.com/user?id=BorisWilhelms) | [15 comments](https://news.ycombinator.com/item?id=36484308)

Kor is a prototype AI tool that helps extract structured data from text using LLMs. It allows users to specify the schema of what should be extracted and provides examples, then generates a prompt and sends it to the specified LLMs. The tool then parses the output and provides the results back to the user. While it is integrated with LangChain framework, Kor is still a half-baked prototype with an unstable API.

In the comments, users discussed the flexibility of running models in different languages, and its helpfulness in extracting metadata using LLMs. Some users also shared their thoughts on using LLMs to generate web scrapers efficiently, while others shared alternative models for extracting structured data from text. Furthermore, users discussed their experience using Kor in extracting data, and some suggested using CSS selectors in HTML documents for extracting data.

### Tear Down: Tesla's In-House Radar Design

#### [Submission URL](https://www.ghostautonomy.com/blog/tearing-down-teslas-in-house-radar-design-why-did-they-bother) | 43 points | by [mensetmanusman](https://news.ycombinator.com/user?id=mensetmanusman) | [3 comments](https://news.ycombinator.com/item?id=36485984)

In a recent analysis, a radar engineer examined Tesla's in-house radar program, which has remained mysterious since the company removed radar from its cars in 2021. The engineer believes that the new HW4 architecture, equipped with Tesla's developed radar, falls short of the specs of rival automotive industry's long-range 4D imagining radars. However, it provides Tesla with increased control, allowing for greater optimization and synchronization of the radar with their overall autonomy system. The analysis also delves into the radar's sensitivity, MIMO topology, and antenna topologies, providing insight into its design approach.

The discussion about the submission revolves around Tesla's decision to develop their own radar program in-house instead of relying on suppliers. One user believes that Tesla does not want to depend on suppliers and is willing to invest in building millions of chips, paying a higher margin, and waiting for supplier time to fall. Another user adds that Tesla's radar program started with a temporal resolution front-facing continental radar, and Tesla Vision, the company's AI-based driving assistant system, has high-resolution phased array radar technology. Another user analyzes the article's technical details, specifically mentioning the radar's geometry.

### Databricks Strikes $1.3B Deal for Generative AI Startup MosaicML

#### [Submission URL](https://www.wsj.com/articles/databricks-strikes-1-3-billion-deal-for-generative-ai-startup-mosaicml-fdcefc06) | 181 points | by [jmsflknr](https://news.ycombinator.com/user?id=jmsflknr) | [105 comments](https://news.ycombinator.com/item?id=36478734)

Databricks, a data management and analytics firm, has announced its acquisition of MosaicML, a generative AI startup, in a $1.3 billion deal. The acquisition is aimed at satisfying the increasing demand from businesses to build their own ChatGPT-like language models, according to Databricks' CEO Ali Ghodsi. The deal is expected to provide businesses with the ability to connect their data with services to help create their own cost-effective language models.

Some users on Hacker News were critical of the acquisition, stating that some companies do not have the technical talent to implement LLMs and that this could result in less meaningful value propositions. Others noted that Databricks is a technically-staffed company and that LLM integration would be a perfect fit for its business plan. Additionally, some users discussed the differences between Databricks and Snowflake in terms of data storage and management. Some users were not convinced of the value proposition of LLMs, stating that data warehouses are essentially required for these models. Lastly, some users pointed out that Databricks has replaced its self-managed Spark-on-K8s with third-party recommendations. A few users mentioned AWS SageMaker as a potential competitor to MosaicML.

### Google DeepMind’s CEO Says Its Next Algorithm Will Eclipse ChatGPT

#### [Submission URL](https://www.wired.com/story/google-deepmind-demis-hassabis-chatgpt/) | 17 points | by [neilfrndes](https://news.ycombinator.com/user?id=neilfrndes) | [7 comments](https://news.ycombinator.com/item?id=36486343)

Demis Hassabis, the CEO of Google's DeepMind AI lab, has announced that his team is working on an AI system called Gemini, which will combine the language capabilities of large models with techniques used in AlphaGo, the program that defeated a champion player of the board game Go. The system will have capabilities such as planning or the ability to solve problems similar to OpenAI's ChatGPT, but with new innovations that will be "pretty interesting", according to Hassabis. The development of Gemini could play a major role in Google's response to the competitive threat posed by ChatGPT and other AI technology.

The discussion is mostly made up of comments that are not directly related to the submission. One user questions whether Google maintains a "shadow brain" internet search, another user writes about having flashbacks to when Google was hyping their search engine, and another user suggests that they should show instead of tell. One user shares an archived link related to the submission. Another user questions the potential for Gemini to compete with OpenAI's ChatGPT and expresses concern about the dangers of AI development. Lastly, a user mentions a headline suggesting that Google's algorithm has surpassed ChatGPT.

---

## AI Submissions for Sun Jun 25 2023 {{ 'date': '2023-06-25T17:11:48.773Z' }}

### Show HN: Open-source resume builder and parser

#### [Submission URL](https://www.open-resume.com/) | 603 points | by [xitang](https://news.ycombinator.com/user?id=xitang) | [182 comments](https://news.ycombinator.com/item?id=36470297)

OpenResume is a free and open-source resume builder that saves users from manual formatting work. It has built-in best practices for the US job market and works well with top ATS platforms such as Greenhouse and Lever. OpenResume also stores data locally in users' browsers to ensure complete privacy. It is designed specifically for the US job market with a single column resume design, core sections, and no option to add a profile picture to avoid bias and discrimination. The creators of OpenResume hope to help anyone create a modern professional resume that follows best practices and enable anyone to apply for jobs with confidence.

OpenResume is an open-source resume builder that uses built-in best practices for the US job market and stores data locally in users' browsers to ensure privacy. While there are some criticisms and questions regarding the project's marketing and endorsements, the creator defends it as an honest and transparent initiative. Users discuss the differences between a CV and a resume, discrimination in hiring based on language skills, and the importance of effective communication skills in software development jobs.

### How the most popular cars in the US track drivers

#### [Submission URL](https://www.wired.com/story/car-data-privacy-toyota-honda-ford/) | 122 points | by [arkadiyt](https://news.ycombinator.com/user?id=arkadiyt) | [143 comments](https://news.ycombinator.com/item?id=36473217)

Privacy4Cars released a new tool called the Vehicle Privacy Report that reveals how much information car manufacturers can collect from your vehicle's data. The tool creates privacy labels for what manufacturers collect and whom they share data with. Typically, most modern vehicles are like "smartphones on wheels," with the ability to collect significant amounts of data wirelessly and send the information to manufacturers. The Vehicle Privacy Report works by using a car's Vehicle Identification Number (VIN) and analyzing each manufacturer's public policy documents. The study showed Toyota collects personal information such as name, address, driving license number, phone number, email, and driving behavior, including acceleration, speed, braking functionality, and travel direction. It may also gather favorite locations saved on its systems and images taken by external cameras or sensors. Some models of Toyota cars can also scan drivers' faces for face recognition. It is unknown whether Toyota collects data from people's phones that are synced with its vehicles. Commenters in the discussion express varying opinions about data privacy, government regulation, and the role of car manufacturers in protecting consumer privacy. They also discuss matters related to driving safety, such as the need for better visibility in modern cars and the impact of pre-collision driving systems on driver behavior.

### Show HN: Bing Chat sidebar ported from Edge to Chrome

#### [Submission URL](https://github.com/wong2/bing-sidebar-for-chrome) | 14 points | by [wonderfuly](https://news.ycombinator.com/user?id=wonderfuly) | [8 comments](https://news.ycombinator.com/item?id=36467997)

This is an AI-powered Bing chat sidebar that has been ported from Microsoft Edge to Chrome, allowing users to access the current webpage or PDF. The sidebar is available as a Chrome extension and does not collect any user data. The extension is compatible with Google Chrome version 114 or higher but may not work with other Chromium-based browsers. Users in the discussion expressed their gratitude and excitement for this new Chrome extension that allows easy access to AI-powered Bing chat sidebar. One user shared that they are building an extension and found this feature very helpful. Another user mentioned that this new functionality solved a problem they were facing with Edge, allowing them to easily switch to Chrome. A user also suggested that the extension may work on other browsers such as Vivaldi and Brave and that it is a great alternative to accessing the Bing contact chat. Some users discussed their experience with Bing and Microsoft browsers, pointing out that they faced validation issues while accessing certain websites, but this new feature seems to help resolve the issue. Overall, users appreciated the convenience and functionality of this feature.

### Companies That Replace People with AI Will Get Left Behind

#### [Submission URL](https://hbr.org/2023/06/companies-that-replace-people-with-ai-will-get-left-behind) | 27 points | by [pseudolus](https://news.ycombinator.com/user?id=pseudolus) | [4 comments](https://news.ycombinator.com/item?id=36471749)

As the adoption of Artificial Intelligence (AI) continues to increase in many companies, job losses are expected to mount in the short term. However, companies that position themselves to innovate with AI will be able to mitigate this risk by creating new jobs and keeping unemployment low. Some companies are already using generative AI to empower employees to do more and increase productivity. A radical redesign of corporate processes may allow companies to spark all sorts of new value creation, ultimately creating new jobs. Innovation, not cutting costs, will position companies to thrive in the long run.

The discussion includes different perspectives on the impact of AI on employment and services. One user argues that introducing technology does not necessarily lead to job replacement but rather creates new job opportunities and increases productivity. Another user points out that the quality of customer support may suffer due to the use of AI. There is also a discussion about monopolies not prioritizing good service, and a user argues that AI cannot comprehensively understand problems and that replacing employees with AI creates competition within companies.