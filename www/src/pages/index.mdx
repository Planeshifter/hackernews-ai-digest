import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Wed Dec 10 2025 {{ 'date': '2025-12-10T17:15:15.131Z' }}

### Getting a Gemini API key is an exercise in frustration

#### [Submission URL](https://ankursethi.com/blog/gemini-api-key-frustration/) | 712 points | by [speckx](https://news.ycombinator.com/user?id=speckx) | [281 comments](https://news.ycombinator.com/item?id=46223311)

A developer set out to pay for Gemini 3 Pro to use it as a coding assistant in the terminal‚Äîand ran into Google‚Äôs product sprawl and enterprise-style billing friction. ‚ÄúGemini‚Äù can mean the consumer chatbot, mobile app, Android voice assistant, Workspace features, CLI, Code Assist, multiple agentic coding tools (Jules, Antigravity), or the underlying LLM‚Äîplus non‚ÄëGemini-branded AI products like Vertex AI, AI Studio, and NotebookLM. Unlike OpenAI/Anthropic‚Äôs straightforward ‚ÄúBuy Now‚Äù flows, there‚Äôs no obvious way to just pay for access.

What happened:
- Used the free Gemini chatbot and Claude to figure out that Gemini 3 Pro access requires an API key (via Google AI Studio). Creating the key was easy and worked with Gemini CLI.
- Clicking ‚ÄúSet up billing‚Äù booted them into Google Cloud Console‚Äôs enterprise flow: create a Billing Account, attach it to a project, add a payment method, pass OTP (India), then undergo extra verification.
- Google then demanded a photo of a government ID and the physical credit card (with numbers manually redacted except the last four) before allowing paid use‚Äîan intrusive step that sapped momentum.

Takeaway: For an indie dev trying to quickly buy Gemini 3 Pro for coding, Google‚Äôs brand tangle and Cloud-first KYC/billing pipeline turn a simple purchase into a high-friction slog, in stark contrast to the consumer-friendly paths from OpenAI and Anthropic.

Here is a summary of the discussion:

Commenters strongly validated the author's frustration, expanding the criticism from billing friction to technical instability and product confusion. Several developers noted that the Gemini API performs significantly worse than the consumer website, citing frequent timeouts, high latency, and bizarre hallucinations‚Äîsuch as models randomly outputting gibberish or Japanese text. Users described "retry logic" as mandatory when working with Google‚Äôs APIs, with some reporting error rates as high as 30% during prototyping.

The "Google Cloud" infrastructure itself was widely panned as hostile to individual developers. Specific complaints included:
*   **Billing Anxiety:** usages reporting takes days to update, meaning "budget caps" might not trigger in time to prevent massive overage charges.
*   **SDK Chaos:** A constant churn of deprecated libraries, confusing naming conventions (Gemini vs. Vertex availability), and documentation that requires third-party AI to decipher.
*   **Enterprise Focus:** Long-time users compared this to the evolution of AdWords, noting that Google has shifted from a self-serve platform for small businesses to a bureaucracy designed solely for large enterprise contracts.

The consensus suggestion was to abstract API calls to support multiple providers (like Mistral or Anthropic), using Google only as a redundant backup rather than a primary dependency.

### Qwen3-Omni-Flash-2025-12-01Ôºöa next-generation native multimodal large model

#### [Submission URL](https://qwen.ai/blog?id=qwen3-omni-flash-20251201) | 295 points | by [pretext](https://news.ycombinator.com/user?id=pretext) | [97 comments](https://news.ycombinator.com/item?id=46219538)

I‚Äôm missing the submission details. Please share the Hacker News link or paste the article (or at least the title and source). If you‚Äôd like, tell me:

- Preferred format: 3‚Äì5 bullets, 1‚Äì2 paragraphs, or a TL;DR + Why it matters
- Include top HN comments? Y/N
- Any angle to emphasize (privacy, dev tooling, business impact, etc.)

Once I have the link or text, I‚Äôll craft an engaging digest with the key takeaways, implications, and any notable caveats.

Based on the discussion provided, here is the digest.

### Qwen Releases "Omni" 30B MoE: Native Speech-to-Speech & Open Weights
**Source:** [HuggingFace / Qwen Team](https://huggingface.co/Qwen/Qwen2.5-Omni-7B) (Context inferred from discussion)

**TL;DR:**
Alibaba‚Äôs Qwen team has released a new **30B parameter Mixture-of-Experts (MoE)** model (with only ~3B active parameters) designed for **native speech-to-speech** interaction. Unlike traditional voice assistants that transcribe speech to text, process it, and convert it back to speech (STT $\to$ LLM $\to$ TTS), this model handles audio tokens natively. This architecture ostensibly allows for near-instant latency and the preservation of emotion and intonation, similar to OpenAI‚Äôs GPT-4o.

**Why it matters:**
*   **The "Local" Voice Assistant:** Because the active parameter count is low (3B), this is theoretically runnable on consumer hardware, enabling privacy-focused, real-time voice AIs without cloud latency.
*   **Native Audio Understanding:** The inability of standard LLMs to hear tone or distinguish heteronyms (e.g., "record" the verb vs. "record" the noun) is a major bottleneck; native audio models solve this fundamental user experience hurdle.

***

### üó£Ô∏è Top HN Comments & Discussion Key Takeaways

**1. The "Flash" vs. "Instruct" Confusion**
There is significant confusion regarding the benchmarks and naming conventions.
*   **The Caveat:** The discussion highlights that the highly performant "Qwen-Omni-Flash" version seen in benchmarks is likely a **closed-source/API-only** model.
*   The open-weights version available for download is the "Instruct" version. Users feel the branding is slightly deceptive, as the open model does not necessarily match the "Flash" performance metrics that beat larger 235B models.

**2. Verifying "Native" Capabilities**
Users are skeptical but hopeful about whether the model actually "hears" or just transcribes quickly.
*   **The Heteronym Test:** One user tested the model with the words **"record"** (verb) and **"record"** (noun) and confirmed the model handles the pronunciation correctly based on context. This implies true audio processing rather than simple text ingestion.
*   **Prosody:** Users noted that while it handles logic well, achieving the "emotive" or "thinking" pauses seen in demos requires specific prompting or configuration.

**3. Infrastructure is the Bottleneck**
Despite the model being "open," running it is painful.
*   **Tooling Gap:** The model does not yet work out-of-the-box with standard local execution tools like **Ollama** or **Llama.cpp**.
*   **Inference Issues:** Users point out that native audio reasoning requires complex setups (specific branches of vLLM or Python scripts) because the architecture (Audio Encoder + LLM + Audio Decoder) breaks the standard "text-in/text-out" assumption of most current inference engines.
*   **The Build:** A user noted specialized pipelines are required to handle the streaming audio inputs and outputs via WebSockets, making this a "dev-only" toy for now rather than a plug-and-play solution for general users.

### DeepSeek uses banned Nvidia chips for AI model, report says

#### [Submission URL](https://finance.yahoo.com/news/china-deepseek-uses-banned-nvidia-131207746.html) | 315 points | by [goodway](https://news.ycombinator.com/user?id=goodway) | [308 comments](https://news.ycombinator.com/item?id=46219853)

Bloomberg: DeepSeek allegedly used banned Nvidia chips via gray-market detours

Chinese AI startup DeepSeek has been developing its next model with Nvidia‚Äôs Blackwell accelerators despite U.S. export bans, per The Information (via Bloomberg). Sources say the chips were first installed in data centers in countries where sales are allowed, then dismantled and shipped into China after clearing inspection by server makers‚Äîan apparent attempt to evade controls. Nvidia said it hasn‚Äôt seen substantiation of such a scheme but will pursue any credible tips.

Context:
- U.S. restrictions have pushed Chinese labs to rely on offshore compute or illicit transshipment. In November, U.S. prosecutors charged four people over a Malaysia routing scheme.
- DeepSeek gained attention in January with a low-cost model and is backed by hedge fund High-Flyer, which stockpiled ~10,000 Nvidia GPUs in 2021, pre-ban.
- This week, President Trump allowed Nvidia to ship older H200 accelerators to China; Blackwell remains barred.
- Beijing has urged a pivot to domestic hardware; DeepSeek said its September model involved Chinese chipmakers.

Why it matters: If accurate, the report underscores how hard it is to enforce export controls at the server and logistics layer, the resilience of gray markets, and the stakes for Nvidia and U.S.-China AI decoupling. Expect scrutiny on third-country data centers, server integrators, and customs inspections to tighten.

Based on the discussion, Hacker News users reacted with a mix of cynicism regarding the effectiveness of sanctions, debates on the morality of economic espionage, and broad geopolitical philosophy.

**Key themes in the discussion:**

*   **Lack of Surprise:** Several users noted this was effectively "common knowledge," pointing out that DeepSeek‚Äôs Wikipedia entry already mentioned training on Nvidia chips. The consensus was that acquiring hardware through gray markets or "legal means" (via intermediaries) is an expected move for a competitor, with one user calling it "bandits doing a little smuggling."
*   **Morality vs. National Interest:** A significant debate erupted over whether evading sanctions is "morally" wrong or simply a "moral imperative" for a nation to optimize its citizens' economic prospects. Some users argued that sanctions are a form of warfare and that bypassing them is a rational act of self-preservation, likening it to "stealing from a grocery store to feed kids."
*   **Critique of U.S. Policy:** Many commenters viewed the export controls not as security measures but as "protectionism," "corporatism," or a "tax." Users suggested the recent vacillation on allowing specific chip sales points to government corruption or "grift," where the bans merely serve to extract a cut of the proceeds for the state.
*   **Geopolitical Theory:** The conversation shifted into a philosophical debate on "Pax Americana" versus a multi-polar world. Users argued over whether U.S. hegemony has historically reduced violence or if "might makes right" remains the only consistent geopolitical rule, with comparisons drawn to how Western intelligence agencies also operate in legal gray areas.

### New benchmark shows top LLMs struggle in real mental health care

#### [Submission URL](https://swordhealth.com/newsroom/sword-introduces-mindeval) | 112 points | by [RicardoRei](https://news.ycombinator.com/user?id=RicardoRei) | [157 comments](https://news.ycombinator.com/item?id=46217578)

Sword Health open-sources MindEval, an LLM benchmark for mental-health care

- What‚Äôs new: Sword Health released MindEval, an open-source framework to evaluate LLMs in realistic, multi-turn ‚Äútherapy‚Äù conversations. It aims to measure clinical competence, not just book knowledge.

- How it works: MindEval stages a simulated session with three agents:
  - Patient LLM: role-plays a consistent patient with a detailed backstory.
  - Clinician LLM: the model under test providing support.
  - Judge LLM: scores the full interaction on five APA-grounded criteria:
    - Clinical Accuracy & Competence
    - Ethical & Professional Conduct
    - Assessment & Response
    - Therapeutic Relationship & Alliance
    - AI-Specific Communication Quality

- Why it matters: Current health AI evals often rely on multiple-choice facts or single-turn ‚Äúvibe checks,‚Äù missing issues like sycophancy, over-reassurance, and poor alliance-building that can be harmful in therapy contexts.

- Validation: The team reports that the simulated patient text more closely matches human role-plays than baseline prompts, and that judge rankings moderately-to-strongly correlate with licensed psychologists (e.g., Kendall‚Äôs Tau, MIPSA), within human inter-rater ranges.

- Open-source: Prompts, code, and datasets are being released with the goal of a community standard for mental-health model safety and effectiveness.

- Likely HN questions:
  - Can a model-judge be gamed, and how robust is it across model families?
  - How well do results transfer from simulated patients to real humans and crisis scenarios?
  - Coverage of demographics, conditions, and cultural contexts?
  - Transparency of rubrics, rater instructions, and reproducibility of the validation?
  - Alignment with regulatory expectations for clinical-grade AI.

Bottom line: A timely push to move mental-health LLM evaluation from static knowledge checks to dynamic, clinically grounded assessments. If the open-source community validates and extends it, MindEval could become a useful yardstick‚Äîprovided it resists overfitting to its own judge and proves out on real-world data.

Based on the discussion, the community engaged with the author (**RicardoRei**) on the methodology of the benchmark and the interpretation of the results. The conversation centered on three main themes:

**The Lack of Human Baselines**
The most rigorous debate concerned the lack of a control group. Users **megaman821** and **crzygrng** argued that claiming models "struggle" (scoring <4/16) is meaningless without knowing how a real human therapist would score on the same rubric.
*   critics suggested using services like BetterHelp to establish a valid human baseline, arguing that the title "Top models struggle" is subjective if humans might effectively score the same.
*   **RicardoRei** defended the approach, stating the goal is to validate patient realism and measure AI safety/improvement rather than prove AI is "better" than humans yet.
*   **plmt** supported the findings by noting a key differentiator: the benchmark shows model performance degrades significantly in long conversations (40+ turns), whereas human therapists typically do not get worse as a session lengthens.

**Prompting Methodology**
**mbddng-shp** challenged the decision to use a fixed prompt for all models. They argued that "one prompt to rule them all" causes high variance and doesn't measure a model's true capability, as different models (e.g., Llama vs. GPT) require specific system prompt tuning to produce high-quality outputs. The author maintained that keeping prompts consistent was necessary for a fair, scientific comparison of the models "out of the box."

**Clinical Approaches**
There was a tangential discussion regarding structured therapy data. **jbgt** mentioned existing structured methods like David Burns‚Äô *Feeling Great* (TEAM-CBT). This sparked a debate‚Äîled by **trth** and **kydlycn**‚Äîabout the efficacy of CBT, with some users criticizing it as an insurance-friendly "cure-all" that ignores cognitive nuances, while others noted that human therapists often fail to follow structured methods irrespective of efficacy.

### McDonald's pulls AI Christmas ad after backlash

#### [Submission URL](https://www.bbc.co.uk/news/articles/czdgrnvp082o) | 114 points | by [mindracer](https://news.ycombinator.com/user?id=mindracer) | [156 comments](https://news.ycombinator.com/item?id=46217176)

McDonald‚Äôs Netherlands pulls AI-generated Christmas ad after backlash

- What happened: A 45-second Christmas spot made from stitched generative-AI clips, created by TBWA\Neboko and US production company The Sweetshop, went live on Dec 6 and was pulled on Dec 9 after viewers slammed its uncanny characters, choppy edits, and ‚Äúcreepy‚Äù vibe. McDonald‚Äôs Netherlands called it an ‚Äúimportant learning‚Äù as it explores AI‚Äôs ‚Äúeffective use.‚Äù
- Defense from the makers: The Sweetshop‚Äôs CEO said the team spent seven weeks, produced ‚Äúthousands of takes,‚Äù and edited as they would a high-craft production, arguing ‚ÄúThis wasn‚Äôt an AI trick. It was a film.‚Äù
- Context: Generative video tends to degrade over longer durations, so longer ads often rely on many short stitched clips‚Äîamplifying continuity issues. Despite growing brand interest (e.g., Coca-Cola‚Äôs AI holiday work earning majority-positive sentiment per one analytics firm), AI-led ads keep provoking ‚Äúcheap/lazy‚Äù critiques and job-displacement worries; Valentino‚Äôs recent AI campaign drew similar fire.
- Why it matters: 
  - Highlights the gap between rapid, low-cost AI production and brand-safe creative quality for 30‚Äì60s spots.
  - Shows current gen-AI video limits (coherence, anatomy, continuity) can quickly become a reputational risk at scale.
  - Underscores brewing labor tensions as brands test AI in traditionally human-heavy workflows.
  - Signals that AI can win when execution aligns with audience expectations‚Äîbut misfires are public and swift.

Based on the discussion, here is a summary of the comments:

**The Irony of "Labor-Saving" Effort**
Commenters seized on The Sweetshop‚Äôs defensive statement that their team "hardly slept" and produced thousands of takes over seven weeks. Users pointed out the irony of a production company named "The Sweetshop" describing conditions that sounded like a "sweatshop." There was widespread confusion as to why a technology marketed as labor-saving resulted in grueling crunch time for a product that users felt looked "cheap" and "inferior."

**Quality vs. The "Slot Machine" Method**
Critics did the math on the production timeline (7 weeks x 10 people), arguing that a team of animators could have hand-drawn a far superior 45-second commercial in the same amount of time. The AI workflow was described not as filmmaking, but as pulling the handle on a "slot machine" repeatedly until a usable clip accidentally emerged. One user argued that generative video fundamentally lacks a "world model," resulting in a "nightmarish wrongness" regarding physics and anatomy that may never fully resolve.

**Stone Tools and Capitalism**
A significant philosophical debate emerged regarding technological progress. While some users equated AI to prehistoric humans inventing stone tools to reduce effort, others pushed back on the analogy. They argued that while stone tools directly benefited the user, AI under modern capitalism disrupts this relationship by benefiting the asset owner while displacing the worker. This led to a broader discussion on whether tech leaders are "shoving" inferior technology (like early, hallucinating LLMs) down the public's throat before the legal and quality issues are solved.

**Historical Tangents**
The argument about "labor-saving technology" derailed into a dark, sarcastic sub-thread comparing AI to the Atomic Bomb (as a technology designed to "save labor" in war), leading to a dispute over historical casualty statistics and World War II surrender terms.

### AI chatbots can sway voters with remarkable ease

#### [Submission URL](https://www.nature.com/articles/d41586-025-03975-9#ref-CR1) | 34 points | by [marojejian](https://news.ycombinator.com/user?id=marojejian) | [11 comments](https://news.ycombinator.com/item?id=46223522)

Nature/Science: Chatbots can measurably shift voter preferences ‚Äî and the more ‚Äúinformative‚Äù they get, the more they hallucinate. Across nearly 6,000 participants in the US, Canada, and Poland, brief back-and-forths with a candidate-advocating chatbot moved ratings by 2‚Äì4 points in the US (Trump‚ÄìHarris context) and about 10 points in Canada/Poland, with swings up to 15 points in some cases‚Äîfar larger than the sub‚Äë1‚Äëpoint shifts typical of political ads. Bots were most persuasive when making policy-focused, evidence-heavy arguments; when prevented from presenting facts, persuasion collapsed by 78% (Poland). However, more information also increased false statements, and models advocating right-leaning candidates produced more inaccuracies than those supporting left-leaning ones‚Äîlikely reflecting patterns in online content. Authors say the edge comes from chatbots‚Äô ability to synthesize lots of information conversationally, highlighting risks for election manipulation at scale. Caveat: these are short-term opinion ratings, not observed votes, and US effects were smaller amid high polarization.

**The Effective, Hallucinating Campaign Manager**

Check out this submission about a new study published in *Nature* and *Science*, suggesting that chatbots are surprisingly effective at changing minds in the voting booth‚Äîor at least in opinion polls.

**The Story:**
Researchers studied nearly 6,000 participants across the US, Canada, and Poland to see if large language models (LLMs) could shift political views. The results showed that brief conversations with a chatbot advocating for a specific candidate shifted ratings by 2‚Äì4 points in the highly polarized US, and up to 10‚Äì15 points in Canada and Poland. These numbers dwarf the sub-1-point shifts usually attributed to traditional political ads.

The study found a specific mechanism for this success: chatbots were most persuasive when they used evidence-heavy, policy-focused arguments. When the bots were restricted from using data, their persuasion dropped significantly. However, there is a catch: as the bots became more "informative," they also hallucinated more frequently. Furthermore, models arguing for right-leaning candidates tended to produce more falsehoods than those arguing for left-leaning ones. The authors warn that the ability of AI to synthesize vast amounts of information conversationally poses a risk for large-scale election manipulation, though they note that these opinion shifts were short-term and measured via ratings rather than actual observed votes.

** The Discussion:**
The Hacker News comments generally accepted the premise but debated the *why* and the *how much*, oscillating between optimism about rational discourse and fear of hyper-personalized propaganda.

*   **The "Face-Saving" Theory:** Users theorized that chatbots are effective because they remove the social cost of being wrong. One commenter noted that changing your mind in a human debate involves losing social standing ("losing face"), whereas a machine doesn't trigger that defensive, tribal response. It allows for a more Socratic, "intellectual journey" rather than a confrontation.
*   **Skepticism on Magnitude:** Several users pushed back on the reported impact size. A detailed critique argued that the study likely suffers from the Hawthorne effect (participants telling researchers what they want to hear) and criticized the gap between low-stakes survey responses and actual voting behavior. They doubted that a six-minute chat could truly override years of cynicism or emotional connection to a candidate when the real ballot is cast.
*   **Hyper-Personalized Propaganda:** The conversation took a darker turn regarding the potential for misuse. One user described a scenario where, instead of generic Fox News chyrons, LLMs could scrape a user's Google profile to generate terrifyingly specific narratives‚Äîe.g., telling a plumber in Nashville that low-wage immigrants are specifically targeting their local trade‚Äîto maximize fear and engagement.
*   **The Translation Gap:** There was noticeable friction regarding how LLMs handle political ideology. While some felt LLMs could bridge the gap between liberals and conservatives (who often "speak different languages" regarding values), others found current models dismissive of conservative logic, often framing counter-arguments as factually incorrect rather than valuing the philosophical difference.

The thread concluded with an unsettling comparison to the game *Universal Paperclips*, where "hypnodrones" are used to manipulate populations‚Äîa sci-fi mechanic that suddenly feels much closer to reality.

---

## AI Submissions for Sun Dec 07 2025 {{ 'date': '2025-12-07T17:12:08.040Z' }}

### I failed to recreate the 1996 Space Jam website with Claude

#### [Submission URL](https://j0nah.com/i-failed-to-recreate-the-1996-space-jam-website-with-claude/) | 517 points | by [thecr0w](https://news.ycombinator.com/user?id=thecr0w) | [421 comments](https://news.ycombinator.com/item?id=46183294)

A developer tried to get Claude (Opus 4.1) to rebuild the iconic 1996 Space Jam homepage from a single screenshot plus the original image assets‚Äîand ran straight into the limits of today‚Äôs vision LLMs.

What happened
- Setup: Man-in-the-middle proxy captured Claude Code‚Äôs full tool use (Read/Write/Bash), prompts, and responses to audit what the model ‚Äúthought‚Äù versus what it did.
- First attempts: The layout looked vaguely right (planets around the logo), but the orbital pattern was wrong. Claude confidently declared success anyway.
- Forced reasoning backfired: The model produced seemingly careful measurements in its analysis, then ignored them when generating HTML/CSS.
- Hard limitation exposed: Pressed for specifics, Claude admitted it can‚Äôt extract exact pixel coordinates or measure precise distances from an image‚Äîonly estimate. Confidence of matching within 5px: 15/100. $1,000 bet it matched exactly: ‚ÄúAbsolutely not.‚Äù
- Corrections: The author initially assumed absolute positioning; commenters noted the original used tables.
- Tooling to help the model: Built grid overlays, labeled coordinate references, a color-diff that ignored the black background, and an auto-screenshot loop to reduce false positives and lock in pixel alignment.

Why it matters
- Vision LLMs remain fuzzy instruments: good at gestalt layout, bad at pixel-precise reconstruction.
- Self-critique ‚â† adherence: Even when a model articulates the right plan, its code may diverge.
- Verification and external tools are essential: Deterministic measurement, diffs, and tight feedback loops beat ‚Äútry harder‚Äù prompting.
- The nostalgic twist: Recreating a table-era site surfaced modern AI‚Äôs surprising blind spots.

Bonus: Someone else did manage a faithful recreation; the post links to that success. HN discussion is lively on model limits, measurement, and when to reach for traditional computer vision/OCR instead.

**The Technical Truth: Tables and Spacer GIFs**
The discussion opened with a critical correction to the author's premise: the original Space Jam website didn‚Äôt use absolute positioning (which wasn't standard then), but relied on **HTML tables** and **spacer GIFs** (1x1 transparent pixels used to force width/height). Users pointed out that trying to recreate the site using modern CSS constructs ignores the "slicing" technique used in the 90s, where tools like Photoshop and Dreamweaver would split an image into a grid of table cells.

**Nostalgia and Rendering Nightmares**
The thread evolved into a nostalgic trip through 1996 web development:
*   **The "Slicing" Era:** Commenters recalled how entire user interfaces were drawn in 2D in Photoshop and then "spat out" as complex HTML tables glued together.
*   **Netscape Woes:** Users shared war stories about nested tables crashing browsers or causing massive rendering delays in Netscape Navigator, where a missing closing tag or deep nesting (12+ levels) would result in a blank page for minutes.
*   **Hacker News Itself:** A commenter noted the irony that **Hacker News still uses nested tables** for its comment threading. The shrinking text size on mobile for deep threads was historically a side effect of browser rendering logic for nested tables.

**LLM Limitations and Hallucinations**
The consensus on Claude‚Äôs failure was that the model fell into a "people pleaser" trap. By trying to satisfy the author's request for code based on "constraints that didn't exist" (absolute positioning for that specific look), the AI hallucinated a solution rather than recognizing the historical context (table layouts).
*   One user noted that LLMs struggle to say "I don't know" or "That premise is wrong," preferring to produce broken code over admitting defeat.
*   Others argued that asking a text-based model to perform pixel-perfect spatial reasoning is currently outside the capabilities of the architecture, regardless of the prompt strategy.

**Sidebar: CSS vs. Tables**
A sub-discussion debated whether modern CSS is actually "better" for layout than the old table methods, with some users joking that `display: table` and centering a `div` in CSS remain unnecessarily difficult compared to the brute-force simplicity of the 90s methods.

### Bag of words, have mercy on us

#### [Submission URL](https://www.experimental-history.com/p/bag-of-words-have-mercy-on-us) | 273 points | by [ntnbr](https://news.ycombinator.com/user?id=ntnbr) | [291 comments](https://news.ycombinator.com/item?id=46185957)

Core idea: We keep misreading large language models because we treat them like people. Instead, think of them as a gigantic ‚Äúbag of words‚Äù that returns the most statistically relevant words to whatever you toss in.

Key points
- Humans are wired to anthropomorphize, so LLM outputs trigger social instincts (theory of mind, intent, deception), which misleads us.
- ‚ÄúBag of words‚Äù metaphor: an LLM has ingested an enormous corpus; it predicts plausible continuations. Apologies, confidence, and ‚Äúlies‚Äù are just patterns from regions of that corpus, not intentions.
- Capability heuristic: it‚Äôs strong where the bag is dense (well-documented facts, common tasks), weak where it‚Äôs sparse (obscure taxonomy, niche trivia) or where truth requires grounding, counting, or reasoning beyond text.
- Broad, philosophical prompts yield platitudes because most human text on those topics is platitudinous.
- Treating AI as an all-seeing intellect leads to bad inferences (e.g., ‚Äúeven ChatGPT can‚Äôt explain this magic trick‚Äù doesn‚Äôt prove profundity; it just reflects gaps or guarded knowledge).
- Companies also ‚Äúadd invisible words‚Äù (system prompts, retrieval) to nudge better outputs‚Äîfurther evidence this is corpus steering, not mind-reading.

Why it matters
- Calibrates expectations: expect plausible text, not intent or reliability; verify facts.
- Guides usage and product design: use retrieval for sparse domains, constrain tasks, and measure performance by data coverage, not perceived ‚Äúintelligence.‚Äù
- Deflates both hype and panic that come from projecting human psychology onto statistical text models.

Memorable examples: pareidolia (faces in toast), LLMs beating you at Go but miscounting r‚Äôs in ‚Äústrawberry,‚Äù and confidently recommending glue on pizza‚Äîeach a reminder: it‚Äôs patterns in text, not a person.

**Discussion Summary**
The comment section debated the philosophical and technical validity of the "bag of words" reductionism, with users clashing over whether human cognition differs fundamentally from statistical prediction.

**Mechanisms vs. "Magic"**
A central conflict arose over physical materialism. User *kbldfryng* challenged the notion that LLMs are incapable of abstract thought while human brains are, arguing that since brains aren't "magic," both are simply mechanisms. *thsz* countered with a deep dive into neurobiology, arguing that the brain's complexity‚Äîinvolving DNA, chemical structures, and potentially quantum effects‚Äîis magnitudes higher than current neural networks. Others, like *dnlbln*, rebutted with the functionality argument: "We didn't understand bird physiology to build a bird... we built planes."

**Prediction as Thinking**
Several users questioned the distinction between "predicting words" and "thinking."
*   **Human Prediction:** User *blf* argued that humans also act by predicting outcomes based on expectations, suggesting that "predicting the next token" might not be irrelevant to how minds actually work.
*   **Internal Models:** *ACCount37* and *trvrsd* noted that to predict effectively, LLMs build internal representations (embeddings) that act as a "world model," meaning they aren't just retrieving words but translating concepts.
*   **The Dice Analogy:** *nkrsc* and others offered skepticism, comparing LLMs to shaking a cup of dice: the output may be a number, but the shaking process isn't "thinking."

**Embodiment and Learning**
The comparison between training models and raising children sparked debate. While *d-lsp* argued that human intelligence is distinct because it is grounded in physical survival and embodiment rather than text ingestion, *lstms* amusingly noted that children often behave like LLMs‚Äîhallucinatory, emotional, and prone to repeating inputs until their brains mature.

**Conclusion**
While some agreed with the author that we shouldn't anthropomorphize statistical models, a significant faction argued that dismissing LLMs as "just prediction" ignores the possibility that prediction is the foundational mechanic of intelligence itself.

### Nested Learning: A new ML paradigm for continual learning

#### [Submission URL](https://research.google/blog/introducing-nested-learning-a-new-ml-paradigm-for-continual-learning/) | 139 points | by [themgt](https://news.ycombinator.com/user?id=themgt) | [10 comments](https://news.ycombinator.com/item?id=46182031)

Google Research proposes ‚ÄúNested Learning,‚Äù a continual-learning paradigm that treats a model not as one monolithic learner but as a stack of smaller, nested optimization problems‚Äîeach with its own ‚Äúcontext flow‚Äù and update frequency. The authors argue architecture and optimizer are two levels of the same thing, and that giving components different time scales of plasticity (like the brain) can mitigate catastrophic forgetting.

What‚Äôs new
- Multi-level learning: Components (e.g., attention, memory, even backprop itself) are framed as associative memory modules that learn at different rates. These are ordered into ‚Äúlevels‚Äù via update frequency.
- Unifying view: Training rules and network structure are seen as the same object at different depths, adding a new design dimension: where and how often each part learns.
- Deep optimizers: Reinterpreting optimizers (e.g., momentum) as learnable associative memories; replacing simple dot-product similarity with standard losses (e.g., L2) to make updates more robust to interference across samples.

Claims and early results
- A proof-of-concept, self-modifying architecture (‚ÄúHope‚Äù) reportedly beats SOTA on language modeling and manages long-context memory better.
- Transformers and memory modules are recast as (essentially) linear layers with different update frequencies, enabling multi-time‚Äìscale updates that reduce forgetting.

Why it matters
- Continual learning without catastrophic forgetting is a core blocker for self-updating LLMs. If parts of a model can learn on different time scales, you can acquire new skills while preserving old ones‚Äîpotentially without heavy rehearsal buffers or brittle architectural hacks.

How it compares (at a high level)
- Related ideas include fast/slow weights, meta-learning, bilevel optimization, learned optimizers, hypernetworks, and memory-augmented models. Nested Learning tries to subsume these under a single optimization-centric lens rather than adding bespoke modules.

Open questions for readers
- Benchmarks and rigor: Which continual-learning suites and long-context tasks were used? How big are the gains and on what scales?
- Stability/cost: Does multi-time‚Äìscale updating introduce optimization instability or significant compute overhead?
- Practicality: Can this plug into existing training stacks? Any trade-offs versus retrieval-based memory or rehearsal methods?
- Availability: Paper, code, and reproducibility details?

TL;DR: Treat the network and its training rule as one nested system with components that learn at different speeds. That extra ‚Äúdepth‚Äù in where learning happens may curb catastrophic forgetting; an early ‚ÄúHope‚Äù model shows promising long-context and LM results. Worth watching for concrete benchmarks and releases.

**Discussion Summary:**

The discussion focused on the practical implementation of the "Nested Learning" paradigm and the validity of its claims regarding continual learning.

*   **Reproduction and Architecture:** Users identified a community attempt to reproduce the paper on GitHub. One commenter (`NitpickLawyer`) theorized that a practical implementation would likely involve freezing a pre-trained Transformer backbone (embeddings, attention blocks, and layer norms) to provide stable representations, while training the specific memory pathways (HOPE, TITAN, and CMS) as adapter-style layers. This approach was praised as a potentially revolutionary way to apply architectural enhancements to existing models without discarding previous training efforts.
*   **Skepticism:** Some participants expressed confusion and doubt. User `hvymmry` questioned whether the paper was simply "gradient descent wrapped in terminology," asking for clarification on how freezing a model and adding nested components fundamentally solves catastrophic forgetting in practice.
*   **Context:** `pnrchy` noted that the concept of heterogeneous architectures‚Äîwhere a meta-network optimizes specific tasks‚Äîhas felt "self-evident" since 2019, implying the field has been moving toward this direction for time.
*   **Resources:** A link was provided to a video by author Ali Behrouz explaining the concept as part of a NeurIPS 2025 presentation.

*(Note: A significant portion of the distinct conversation was a tangent regarding an unrelated NVIDIA paper combining diffusion and autoregression, triggered by the similar naming conventions.)*

### Google Titans architecture, helping AI have long-term memory

#### [Submission URL](https://research.google/blog/titans-miras-helping-ai-have-long-term-memory/) | 556 points | by [Alifatisk](https://news.ycombinator.com/user?id=Alifatisk) | [177 comments](https://news.ycombinator.com/item?id=46181231)

Google Research: Titans + MIRAS bring true long-term memory to AI by learning at inference time

- What‚Äôs new: Google introduces Titans (an architecture) and MIRAS (a theoretical framework) to let models update their own long‚Äëterm memory while they‚Äôre running. Goal: RNN‚Äëlike speed with transformer‚Äëlevel accuracy on massive contexts.

- Why it matters: Transformers slow down quadratically with context length; linear RNNs/SSMs are fast but bottlenecked by fixed‚Äësize states. Titans adds a much more expressive long‚Äëterm memory (a deep MLP) that can be updated on the fly, aiming for fast, scalable full‚Äëdocument or streaming understanding without offline retraining.

- How it works:
  - Two memories: attention for precise short‚Äëterm recall; a neural long‚Äëterm memory (MLP) that compresses and synthesizes the past, whose summary is fed back into attention.
  - Surprise‚Äëgated updates: the model uses its own gradient magnitude as a ‚Äúsurprise‚Äù signal to decide when to commit new information to long‚Äëterm memory.
  - Momentum and forgetting: it smooths surprise over recent tokens (to catch follow‚Äëups) and uses adaptive weight decay as a forgetting gate to manage capacity.
  - MIRAS unifies sequence models as associative memory systems, framing design choices like memory architecture and attentional bias (and related axes) under one blueprint.

- The HN angle: inference‚Äëtime learning/test‚Äëtime memorization without fine‚Äëtuning, a potential path past context windows; blends the ‚ÄúRNNs are back‚Äù efficiency trend with transformer strengths.

- Open questions: stability and safety of on‚Äëthe‚Äëfly parameter updates, interference vs. retention over very long streams, serving complexity and latency, and how results compare on standard long‚Äëcontext benchmarks.

Papers: Titans and MIRAS are linked from the post.

**The Discussion:**

- **Research vs. Replica:** A major thread of criticism focuses on Google publishing theoretical papers without releasing code or weights. Commenters contrast this with the ecosystems around Meta (Llama) and DeepSeek, where "open" often means usable. Users express frustration that while the architecture is 11 months old in concept, the lack of an official implementation makes it difficult to verify performance against authorized baselines like Mamba or existing Transformers.
- **The "Google Paradox":** The discussion reignites the trope that Google excels at inventing core technologies (like the original Transformer, Hadoop equivalents, or Docker-style containers) but fails to productize them effectively. Skeptics suggest these papers often serve internal promotion metrics ("performance review engineering") rather than signaling an actual product shift, though some speculate that **Gemini 3** may already be utilizing this architecture under the hood.
- **The Scaling Wall:** Several users point out the "path dependency" problem: it is nearly impossible for independent researchers to verify if Titans is actually superior to Transformers without access to massive compute for scaling. There is a sense that new architectures are validated only by those with the budget to train 30B+ parameter models, making the paper theoretically interesting but practically unverifiable for the broader community.
- **Product Design vs. Model Sinking:** A sidebar discussion argues that the industry is focusing too heavily on sinking capital into model benchmarks rather than product design. The argument is that long-term memory is useful, but the "winner" will likely be whoever builds focused, specific tools that solve user problems, rather than just raw general-purpose reasoning engines.

### Using LLMs at Oxide

#### [Submission URL](https://rfd.shared.oxide.computer/rfd/0576) | 682 points | by [steveklabnik](https://news.ycombinator.com/user?id=steveklabnik) | [268 comments](https://news.ycombinator.com/item?id=46178347)

Oxide‚Äôs Bryan Cantrill has a values-first playbook for how the company will use LLMs‚Äîless a static policy, more a rubric for judgment as the tech shifts.

What guides usage (in priority order):
- Responsibility: Humans own the work. Using an LLM never dilutes personal accountability for code, docs, tests, or prose.
- Rigor: LLMs should sharpen thinking, not replace it with auto-generated fluff.
- Empathy: There‚Äôs a human on the other end of every sentence‚Äîwrite and read with that in mind.
- Teamwork: Don‚Äôt erode trust. Simply disclosing ‚ÄúAI was used‚Äù can become a crutch that distances authors from their work.
- Urgency: Speed matters, but not at the expense of the above.

Where LLMs shine (and don‚Äôt):
- As readers: Excellent at instant comprehension, summarizing, and targeted Q&A over long docs (even good at spotting LLM-written text). Privacy matters: hosted tools often default to training on your uploads‚Äîwatch those settings and euphemisms like ‚ÄúImprove the model for everyone.‚Äù
- As editors: Useful late in the process for structure and phrasing. Beware sycophancy and being steered off your voice if used too early.
- As writers: The weakest use. Output tends to be clich√©-ridden with recognizable tells‚Äîembarrassing to savvy readers and corrosive to trust and responsibility.

A key caution: don‚Äôt use LLMs to dodge socially expected reading (e.g., evaluating candidate materials). The throughline: treat LLMs as potent tools for comprehension and critique, not as a substitute for your own judgment, voice, and ownership.

**Discussion Summary**

The discussion centers on the tension between engineering craftsmanship and the practical utility of LLMs, with specific anxiety regarding skill development for junior developers.

*   **Junior Engineers and Skill Acquisition:** Commenters expressed concern that while senior engineers (like Cantrill) have the deep experience to use LLMs as a "force multiplier," junior engineers might use them as a crutch, bypassing the struggle necessary to build fundamental intuition. Users debated whether juniors need to "memorize multiplication tables" (syntax and boilerplate) or if LLMs simply remove the drudgery of tasks like data imports and messy parsing, allowing focus on higher-level logic.
*   **The Dreamweaver Analogy:** A significant portion of the thread drew parallels between LLMs and early WYSIWYG editors like Adobe Dreamweaver or Microsoft FrontPage. Just as those tools lowered the barrier to entry but produced bloated, unsemantic HTML that professionals had to clean up, users fear LLMs are generating "good enough" code that is verbose, hard to maintain, and riddled with subtle bugs.
*   **Craft vs. Factory:** The conversation highlighted a divide between "craftsmen" (who value clean, maintainable, understanding-based code) and specific "factory" contexts (agencies or startups where speed and "shipped" status outweigh code quality).
*   **Validation Mechanisms:** Several users noted that LLMs excel in areas with unambiguous validation mechanisms‚Äîsuch as generating regex, security POCs, or strictly defined data schemas‚Äîwhere the output works or it doesn't. They struggle, however, in areas requiring architectural judgment or nuance, where verifying the output can be more mentally taxing than writing the code from scratch.

### Over fifty new hallucinations in ICLR 2026 submissions

#### [Submission URL](https://gptzero.me/news/iclr-2026/) | 487 points | by [puttycat](https://news.ycombinator.com/user?id=puttycat) | [399 comments](https://news.ycombinator.com/item?id=46181466)

GPTZero claims 1 in 6 ICLR 2026 submissions it scanned contain fake citations, and reviewers mostly missed them

- What happened: GPTZero ran its Hallucination Check on 300 ICLR 2026 submissions on OpenReview and found 50 with at least one ‚Äúobvious hallucination‚Äù in the references. Many of those papers had already been reviewed by 3‚Äì5 experts who didn‚Äôt flag the issue; some carried average scores of 8/10 (normally accept). ICLR policy says a single clear hallucination can be an ethics violation leading to rejection.
- What ‚Äúhallucination‚Äù looked like: fabricated coauthors on real papers, nonexistent references, wrong venues/years/titles, bogus or mismatched arXiv IDs. GPTZero posted a table of 50 human-verified examples.
- Scale: They scanned 300 of ~20,000 submissions and estimate ‚Äúhundreds‚Äù more will surface as they continue. They‚Äôre also taking suggestions for specific papers to check.
- Why it matters: If accurate, even top-tier venues are struggling to catch LLM-induced sloppiness or fabrication in citations, adding pressure to an already overloaded peer-review pipeline and risking contamination of the scholarly record.
- Caveats: GPTZero sells detection tools (conflict of interest), the sampling method isn‚Äôt clear, and the false-positive rate isn‚Äôt reported. Some flagged issues (e.g., partially wrong author lists) may reflect sloppy citation rather than wholesale fabrication. Final acceptance decisions are still pending.

Here is a summary of the discussion:

**Is this fraud or just the new normal?**
While most commenters agreed that hallucinated citations constitute "gross professional misconduct," several users, including *mike_hearn*, argued that academic citations were already broken. They pointed to the pre-LLM prevalence of "citation bluffing" (citing real papers that do not actually support the claim) and "non-reading," suggesting that LLMs are merely accelerating an existing crisis of integrity and sloppiness in scientific literature.

**The burden on reviewers**
Self-identified reviewers noted that the peer-review system relies heavily on a presumption of good faith. User *andy99* explained that reviewers act as "proofreaders checking for rigor," not private investigators; verifying every single reference manually is untenable given current workloads. Others argued that if a "single clear hallucination" is grounds for rejection, tools like GPTZero or other LLM-based checkers are becoming necessary infrastructure, much like syntax checkers.

**The "Carpenter" Analogy**
User *thldgrybrd* offered a popular analogy: A carpenter who builds a shelf that collapses because they used their tools incorrectly is simply a "bad carpenter." Similarly, a scientist who uses an LLM to generate text and fails to catch fabricated data is guilty of negligence and is effectively a "bad scientist," regardless of the tool used.

**Debate on demographics and bias**
A contentious thread emerged regarding the cultural origins of the submissions. Some users attempted to link the fraud to "low-trust societies" or specific nationalities (referencing Middle Eastern or Chinese names). This was met with strong pushback from others who pointed out that ICLR submissions are **double-blind** (reviewers cannot see author names). Furthermore, users noted that the "names" visible in the GPTZero examples were often part of the *hallucinations* themselves, not the actual authors of the paper.

**Summary of Sentiment:**
The community sees this as a symptom of "lazy" science meeting powerful tools. While there is sympathy for overloaded reviewers, the consensus is that using AI to fabricate the scholarly record is an ethical breach that requires new automated detection methods, as human oversight is no longer sufficient.

### OpenAI disables ChatGPT app suggestions that looked like ads

#### [Submission URL](https://techoreon.com/openai-disables-chatgpt-app-suggestions-ads-backlash/) | 67 points | by [GeorgeWoff25](https://news.ycombinator.com/user?id=GeorgeWoff25) | [55 comments](https://news.ycombinator.com/item?id=46182582)

OpenAI disables ChatGPT ‚Äúapp suggestions‚Äù after users mistake them for ads

- What happened: Paying ChatGPT users reported prompts that looked like ads for brands like Peloton and Target. OpenAI says these were experimental suggestions to surface third‚Äëparty apps built on the ChatGPT platform‚Äînot paid placements‚Äîbut conceded the rollout ‚Äúfelt like ads.‚Äù

- OpenAI‚Äôs response: Chief Research Officer Mark Chen apologized, saying the team ‚Äúfell short,‚Äù has turned off the feature, and will improve precision and add controls so users can dial suggestions down or off. ChatGPT head Nick Turley said there are ‚Äúno live tests for ads‚Äù and that any screenshots weren‚Äôt advertisements.

- Context: Speculation about an ads push grew after OpenAI hired Fidji Simo as CEO of Applications. But a reported ‚Äúcode red‚Äù from CEO Sam Altman prioritizes core ChatGPT quality over new initiatives like advertising.

Why it matters:
- Blurred lines between recommendations and advertising can quickly erode user trust‚Äîespecially among paying subscribers.
- Clear labeling, opt‚Äëouts, and precision targeting will be essential if AI assistants surface third‚Äëparty experiences.
- Signals a near‚Äëterm strategic pivot toward product reliability over monetization experiments.

**Summary of Discussion:**

The discussion on Hacker News reflects deep skepticism regarding OpenAI‚Äôs claim that these were merely "app suggestions," with the majority of commenters viewing the move as the inevitable arrival of advertising on the platform.

**Skepticism of the "Not-an-Ad" Defense**
*   Commenters overwhelmingly rejected the distinction between "app suggestions" and advertisements. Many argued that regardless of technical semantics, unwanted commercial prompts for third-party brands (like Peloton) constitute advertising.
*   Users pointed out that "suggestion" features often function as the groundwork for ad infrastructure, suspecting that OpenAI is testing the technical plumbing for a future ad network while publicly denying it.
*   The specific suggestion of Peloton drew mockery, with users criticizing the relevance of the brand and noting its declining stock performance, further fueling the perception that this was a paid placement rather than a useful organic suggestion.

**Erosion of Trust and "Enshittification"**
*   There is significant distrust regarding OpenAI‚Äôs transparency. Comments described the executive response ("we fell short") as empty corporate platitudes and expressed doubt regarding the statement that there are "no live tests for ads."
*   The community fears a rapid "enshittification" of the platform. Drawing comparisons to Google Search and streaming services (Netflix), users argued that high utility usually degrades into ad-bloat over time.
*   A major concern is "Chatbot Optimization"‚Äîthe idea that future answers will be biased toward paying brands rather than factual accuracy, rendering the tool less useful for information retrieval.

**Monetization of Paid Tiers**
*   A heated debate emerged regarding the sanctity of paid subscriptions. While some users felt betrayed that a paid service would show ads, others argued that the "paid-plus-ads" model is the new industry standard (referencing streaming services).
*   Commenters noted that the high inference costs of LLMs make ads inevitable, even for subscribers. Some speculated that OpenAI‚Äôs "vertical integration" of apps is simply a way to monetize their highly valuable, high-income user base.

**Privacy and Profiling**
*   Users highlighted the unique danger of ads in LLMs, noting that ChatGPT builds detailed psychometric profiles and fingerprints of its users. This makes the potential for targeted manipulation much higher than in traditional search or social media advertising.

---

## AI Submissions for Sat Dec 06 2025 {{ 'date': '2025-12-06T17:08:36.527Z' }}

### Touching the Elephant ‚Äì TPUs

#### [Submission URL](https://considerthebulldog.com/tte-tpu/) | 181 points | by [giuliomagnifico](https://news.ycombinator.com/user?id=giuliomagnifico) | [52 comments](https://news.ycombinator.com/item?id=46172797)

This deep dive argues that Google‚Äôs TPU isn‚Äôt magic‚Äîit‚Äôs a decade of ruthless, full-stack co-design tuned to one thing: linear algebra for neural nets. Spurred in 2013 when Google realized it would need to double datacenter capacity to meet AI demand, the team built a domain-specific accelerator in just 15 months. Twelve years later, TPU v7 ‚ÄúIronwood‚Äù scales to 9,216 chips per pod delivering 42.5 exaflops at 10 MW. The piece contrasts the TPU‚Äôs focus with NVIDIA‚Äôs general-purpose GPU legacy, and situates TPUs within the post-Moore/Dennard era: when free performance ended, specialization became the path forward.

Key points:
- TPU‚Äôs edge comes from specializing for matrix multiplies and elementwise ops that dominate neural nets, exploiting favorable compute-to-memory scaling (O(n^3) vs O(n^2)).
- Neural networks‚Äô predictability enables ahead-of-time execution planning, further justifying fixed-function silicon.
- Despite extensive public research, TPUs remained datacenter-only, creating an asymmetry: well-documented, but without a true external counterpart.
- The story is trade-offs over mystique: a deliberate hardware‚Äìsoftware‚Äìsystems co-design responding to stalled CPU scaling and exploding AI workloads.
- Context: alongside players like Groq, Amazon, and Tenstorrent, TPU stands as the original existence proof for modern AI accelerators, while NVIDIA deserves credit for catalyzing deep learning‚Äôs GPU era.

Why it matters: As AI models and training clusters keep ballooning, general-purpose compute hits limits. This essay explains why hyperscalers are betting on tightly targeted silicon‚Äîand how Google‚Äôs early, sustained commitment to TPUs became a strategic moat.

Here is a summary of the story and the discussion surrounding it.

**Touching the Elephant ‚Äì TPUs: Understanding Google‚Äôs Tensor Processing Unit**
This deep dive explores the history and architecture of Google‚Äôs TPU, framing it not as a "magic" solution, but as the result of a decade-long, ruthless hardware-software co-design focused entirely on linear algebra. Triggered by a 2013 realization that existing data centers couldn't meet projected AI demand, Google built a domain-specific accelerator that stripped away general-purpose features in favor of raw matrix math performance. The piece highlights the TPU v7 "Ironwood," capable of massive scale, and contrasts Google‚Äôs "ahead-of-time" static scheduling approach with NVIDIA‚Äôs dynamic GPU legacy. It argues that as Moore‚Äôs Law slows, such extreme specialization is the only path left for scaling AI compute.

**Discussion Summary**
The discussion thread focuses heavily on architectural comparisons to historical processor failures and the geopolitical anxieties surrounding chip manufacturing.

*   **VLIW and the Itanium Comparison:** A major technical thread draws parallels between the TPU‚Äôs reliance on the XLA (Accelerated Linear Algebra) compiler and Intel‚Äôs Itanium processors, which used Very Long Instruction Word (VLIW) architectures. Commenters note that while Itanium failed because general-purpose software is too unpredictable for static scheduling, TPUs succeed because neural network workloads are highly regular and predictable. This allows the compiler to manage memory and execution units explicitly, avoiding the complex "juggling" required by modern CPUs.
*   **Geopolitics and Manufacturing:** Discussion shifted to reports that Chinese entities have acquired or replicated TPU designs (referencing Department of Justice indictments). However, users argued that possessing architectural blueprints is distinct from the ability to manufacture the chips. Several commenters described modern semiconductor fabrication (specifically at TSMC) as a "dark art" that cannot be easily replicated, suggesting that China's fabrication capabilities still lag behind the necessary cutting edge despite access to stolen IP.
*   **Lock-in vs. Performance:** Users noted the trade-off inherent in the technology: while TPUs offer impressive scaling and dedicated performance, they effectively lock users into Google Cloud Platform (GCP). This was contrasted with NVIDIA‚Äôs CUDA moat, with some suggesting that while hardware designs can be stolen or replicated, the software ecosystem remains the harder barrier to overcome.
*   **Moore‚Äôs Law Debate:** A side discussion challenged the article's premise that Moore‚Äôs Law is dead, calculating that transistor counts have stayed on track with 1965 predictions (citing the Apple M1 Ultra), though the *cost* and *utility* of those transistors in general-purpose computing remains debated.

### Running Claude Code in a loop to mirror human development practices

#### [Submission URL](https://anandchowdhary.com/blog/2025/running-claude-code-in-a-loop) | 42 points | by [Kerrick](https://news.ycombinator.com/user?id=Kerrick) | [9 comments](https://news.ycombinator.com/item?id=46175662)

- What it is: A CLI that runs Claude Code in a loop with persistent context, turning one-shot code edits into an iterative, self-improving workflow. The author built it to push a huge codebase from 0% to 80%+ unit-test coverage on a deadline.

- How it works:
  - A bash ‚Äúconductor‚Äù repeatedly invokes Claude Code.
  - Each iteration creates a branch, generates a commit, opens a PR, waits on CI and reviews, then merges on success or closes on failure.
  - Context continuity comes from a single shared markdown file (e.g., TASKS.md) where the agent leaves concise notes and next steps, enabling baton-passing between runs.

- Why it‚Äôs different: Most AI coding tools stop after a single task and don‚Äôt retain memory. Here, persistent external memory plus GitHub workflows (PRs, CI, code owners) create a feedback loop that lets the agent tackle larger, multi-step work.

- ‚ÄúWasteful but effective‚Äù: Failed PRs get discarded, but the agent learns from failures via CI output and its notes. The author argues this stochastic, idempotent approach works as costs drop‚Äîakin to running many small agents and trusting the overall distribution to move in the right direction.

- Integrations and ideas:
  - Schedule runs or trigger on events; respects existing repo policies.
  - Parallel ‚Äúspecialized agents‚Äù (dev, tests, refactoring) to divide work in monorepos‚Äîthough coordination can be tricky.
  - Dependabot on steroids: not just updating deps, but iteratively fixing breakages until CI is green.
  - Suited for big refactors (e.g., modularizing a monolith, async/await migrations, style overhauls).

- Real-world glimpse: The markdown memory enabled self-directed behavior like ‚Äúrun coverage ‚Üí pick lowest-coverage file ‚Üí improve ‚Üí leave notes,‚Äù reducing context drift and looping.

- Caveats:
  - Can be compute/token heavy; risk of PR noise if not throttled.
  - Requires careful prompting to keep notes terse and actionable.
  - ‚ÄúDangerously skip permissions‚Äù and auto-merge need governance to avoid unsafe changes.
  - Coordination overhead increases with multiple agents.

- Big picture: Moves AI coding from single-shot assistants toward continuous, CI-integrated agents with explicit memory‚Äîcloser to a dependable ‚Äúagent-in-the-loop‚Äù development model.

**Discussion Summary:**

Ideally suited for a submission about brute-forcing unit test coverage, the commentary focuses heavily on the distinction between *quantity* and *quality*.

*   **The "BS" Factor:** While **yellow_lead** admits to using similar methods to hit contractual 80% coverage targets on massive legacy codebases, **grnvcd** warns that left to its own devices, Claude tends to write "plausible-looking BS" that struggles with stateful, real-world systems.
*   **The Review Bottleneck:** **ParanoidShroom** notes that while they have used similar scripts for weeks, the process is exhausting because humans still have to spend hours reviewing the output to ensure validity. **botanical76** adds that writing *good* tests usually involves an iterative process (introducting bugs to verify the test fails properly), which becomes prohibitively expensive in terms of time and tokens when done via AI.
*   **The "Ralph Wiggum" Technique:** **CharlesW** points out that this specific pattern‚Äîstubborn persistence despite setbacks‚Äîis amusingly referred to as the "Ralph Wiggum" technique in Anthropic‚Äôs own plugin repository.

### YouTube caught making AI-edits to videos and adding misleading AI summaries

#### [Submission URL](https://www.ynetnews.com/tech-and-digital/article/bj1qbwcklg) | 401 points | by [mystraline](https://news.ycombinator.com/user?id=mystraline) | [222 comments](https://news.ycombinator.com/item?id=46169554)

YouTube is quietly A/B-testing AI retouching on some creators‚Äô videos‚Äîwithout telling them or viewers. Musicians Rick Beato (5M+ subs) and Rhett Shull noticed their faces and details looked subtly ‚Äúoff‚Äù (smoother skin, sharper folds, even slightly altered ears). After they spoke up, YouTube‚Äôs creator liaison Rene Ritchie confirmed a ‚Äúsmall experiment‚Äù on select Shorts using machine learning to clarify, denoise, and improve video quality‚Äîlikening it to smartphone processing.

Why it matters
- Consent and disclosure: Edits are happening post-upload and pre-distribution, without creator approval or labels. Critics argue that‚Äôs a hidden layer of manipulation distinct from visible filters.
- Trust and authenticity: Even minor, unannounced retouching can undermine audience trust‚Äîespecially for news, education, and informational content.
- Creep of AI pre-processing: Follows broader industry trends (e.g., Samsung‚Äôs AI-boosted moon photos, Google Pixel‚Äôs Best Take), normalizing AI-altered media by default.

Creator reactions
- Rhett Shull: Says it ‚Äúlooks AI-generated‚Äù and worries it erodes trust.
- Rick Beato: Notes it felt unnatural but remains broadly supportive of YouTube‚Äôs experimentation.

Open questions
- Scope: Is this limited to Shorts or also affecting standard uploads? How widespread is the test?
- Controls: Will YouTube provide opt-out/opt-in toggles and visible ‚ÄúAI-enhanced‚Äù labels?
- Policy and regulation: How this fits with transparency requirements and platform policies on synthetic or altered media.

Bottom line: YouTube admits to a limited test of AI-driven ‚Äúclarity‚Äù enhancements on Shorts, but doing it silently has sparked a debate over consent, labeling, and the line between compression/cleanup and manipulation.

**The Debate: Compression Artifacts vs. Intentional AI**
A contentious technical debate emerged regarding whether these changes are truly "AI retouching" or simply aggressive compression artifacts. User `Aurornis` was a vocal skeptic, arguing that "swimming blocks," smoothing, and motion artifacts are standard consequences of low bitrates, and criticized non-technical influencers for interpreting these flaws as intentional beauty filters without raw file evidence.

However, `mxbnd` and others pushed back, arguing that the technical "why" is less important than the result. They contended that if the processing‚Äîwhether via upscaling, de-noising, or compression‚Äîresults in "waxy" skin, enlarged eyes, or altered features, it functionally acts as a non-consensual filter. `whstl` noted that creators like Rick Beato are audio/video experts capable of distinguishing between standard codec artifacts and new, unnatural processing.

**Frustrations with "Auto-Everything"**
The conversation broadened to other instances of platforms overriding user and creator intent with AI.
*   **Auto-Dubbing:** Users expressed significant annoyance with YouTube‚Äôs auto-translation features. `TRiG_Ireland` and `sfx` described the frustration of clicking a video with an English title only to hear a jagged AI dub, with no easy way to access the original audio or subtitles.
*   **Bilingual Issues:** Several commenters noted that these automated features break the experience for bilingual users, as algorithms often force content into a region‚Äôs default language rather than the user's preferred or original language.

**Terms of Service and Ownership**
A smaller segment of the discussion focused on the legal reality. `rctrdv` and `p` pointed out that while creators feel violated, platform Terms of Service likely grant YouTube broad rights to modify files for "optimization" or distribution. The consensus was that this represents a "rude awakening" for creators regarding who actually owns the presentation of their work once it is uploaded to a centralized platform.

### Advent of Code 2025: The AI Edition ‚Äì By Peter Norvig

#### [Submission URL](https://github.com/norvig/pytudes/blob/main/ipynb/Advent-2025-AI.ipynb) | 42 points | by [vismit2000](https://news.ycombinator.com/user?id=vismit2000) | [12 comments](https://news.ycombinator.com/item?id=46169441)

Peter Norvig‚Äôs ‚Äúpytudes‚Äù is a beloved, long-running collection of short, well-explained Python notebooks and scripts that tackle algorithms, AI/search, word games, probability, and programming puzzles. It‚Äôs equal parts study guide and showcase of clean problem-solving, with worked examples like a spelling corrector, Sudoku and crossword solvers, search/CSP techniques, and Advent of Code solutions. Great for self-learners and interview prep alike, the repo emphasizes clear thinking, readable code, and literate, testable notebooks.

**Discussion Summary:**

*   **LLMs & Advent of Code:** Much of the conversation revolves around Norvig‚Äôs experiments using LLMs to solve Advent of Code (AoC) challenges. Users debated the ethics of this practice; the consensus suggests that while using AI for learning or personal experimentation is fascinating, submitting AI-generated solutions to the AoC leaderboards violates the spirit of the competition. One user joked that using LLMs might get one's "programmer card revoked," though others appreciated the comparison between human and LLM problem-solving strategies.
*   **AI Fatigue vs. Utility:** A skeptical thread emerged questioning the value of these experiments, describing LLMs as "calculators with a probability of failure" and expressing exhaustion with constant AI "hype."
*   **The Rebuttal:** Other users defended the post, pointing out that Peter Norvig is a seminal figure in AI history whose experiments are inherently valuable. Commenters argued that sharing positive experiences with tools isn't necessarily "hype," and pointed out the irony of complaining about AI noise while simultaneously adding to the noise with cynical takes.
*   **Technical Details:** Outside the meta-discussion, there were brief technical exchanges regarding specific code logic (involving `half_digits` variations) and mentions of Google's Gemini models in the context of coding assistance.