import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Sat Sep 16 2023 {{ 'date': '2023-09-16T17:10:10.057Z' }}

### Generative Image Dynamics

#### [Submission URL](https://generative-dynamics.github.io/) | 310 points | by [hughes](https://news.ycombinator.com/user?id=hughes) | [26 comments](https://news.ycombinator.com/item?id=37536016)

Google Research has released a paper and demo showcasing their latest project called Generative Image Dynamics. This approach models an image-space prior on scene dynamics, allowing for the transformation of a single image into a seamless looping video or an interactive dynamic scene. The model learns from motion trajectories in real video sequences, such as trees swaying in the wind or clothes billowing. Using a frequency-coordinated diffusion sampling process, the model predicts per-pixel long-term motion representations in the Fourier domain, which are called neural stochastic motion textures. These textures can then be converted into dense motion trajectories that span an entire video. The project also includes an image-based rendering module, which can be used to turn still images into dynamic videos, or to allow users to interact with objects in photos. A demo is available where users can click and drag a point on an image to see how the scene moves. The project enables the simulation of object dynamics in response to user excitation and the generation of slow-motion videos by interpolating predicted motion textures.

The discussion on this submission revolves around various aspects of the project. Some users are discussing the potential of using generative images and cinemagraphs for marketing purposes, noting that they can have a bigger impact on viewers than regular still photos. Others are sharing examples of subtle movement in cinemagraphs and suggesting ways to qualify and describe these types of images. There is also a discussion about the feasibility of implementing the technology in video games, with some users noting that realistic physics and dynamic movements are already being handled in games like Red Dead Redemption 2. One user shares examples of games that have impressive grass and physics simulations. Another user mentions the limitations of the gaming industry in adopting deep learning and AI models due to performance and complexity issues. They also discuss the potential negative effects of randomly breaking immersion in games by generating non-continuous movements for characters.

The discussion also touches on related topics, such as the stability of video game physics and interacting with floors in games, the potential of combining photogrammetry and physics for realistic effects, and the use of low-vector movement requirements in EbSynth. One user appreciates Google's research efforts and shares excitement about the possibilities of combining machine learning and gaming. Another user expresses anticipation for stable diffusion GPT models in video games and notes the challenges in implementing them without extensive computational resources. Overall, the discussion showcases different perspectives on the applications, limitations, and potential of generative image dynamics in various domains, including marketing and gaming.

### Adobe will charge “credits” for generative AI

#### [Submission URL](https://helpx.adobe.com/firefly/using/generative-credits-faq.html) | 130 points | by [tambourine_man](https://news.ycombinator.com/user?id=tambourine_man) | [145 comments](https://news.ycombinator.com/item?id=37538878)

Generative credits are a new feature introduced by Adobe that provide priority processing of generative AI content. These credits are used when performing actions such as generating text effects, loading more images in Text to Image, using generative recolor in Adobe Illustrator, using text effects in Adobe Express, and using generative fill in Adobe Photoshop. The consumption of generative credits depends on the computational cost of the generated output and the value of the generative AI feature used. However, certain actions, such as using generative AI features defined as "0" in the rate table or trying a prompt in the Firefly gallery without refreshing, do not consume generative credits. 

The number of generative credits you have depends on your plan, and they reset each month. Different plans offer different numbers of generative credits, with higher-tier plans including more credits. For example, the Creative Cloud All Apps plan includes 1,000 generative credits per month, while lower-tier plans may include 250 or 100 generative credits. Adobe Stock paid subscriptions also include 500 generative credits per month. 

It's important to note that until November 1, 2023, subscribers of Creative Cloud, Adobe Firefly, Adobe Express, and Adobe Stock won't be subject to generative credit limits. However, starting from November 1, 2023, credit limits will apply. Adobe plans to expand generative AI features to include higher-resolution images, animation, video, and 3D in the future, and the number of generative credits consumed for those features may be greater. Overall, generative credits aim to enhance creative possibilities and empower users to create extraordinary content using AI technology.

The discussion on this submission covers various topics related to the use of generative AI, Adobe's plans, hardware requirements, and the implications for users. Some users express concerns about charging for AI-powered features and the need for compliance with regulations. Others discuss the possibility of Adobe moving towards local deployment of AI models and the trustworthiness of Adobe products. There are also discussions about the capabilities of consumer GPUs, the cost of hardware, and the potential energy consumption of running AI models. Users analyze the performance of different GPUs and compare them to Adobe's offerings. The discussion also touches on the future of AI models, the limitations of hardware, and the impact of energy constraints. Some users mention the availability of open-source alternatives and the flexibility of locally-run models. There are also discussions about licensing and the commercial use of AI-generated content.

### Unity's Self-Combustion Engine

#### [Submission URL](https://www.gamesindustry.biz/unitys-self-combustion-engine-this-week-in-business) | 148 points | by [erickhill](https://news.ycombinator.com/user?id=erickhill) | [144 comments](https://news.ycombinator.com/item?id=37535910)

Unity, the popular game development platform, faced backlash after introducing a new Runtime Fee. The fee applies to Unity developers of a certain size and requires them to pay a fee every time their game is installed on a new device after January 1, 2024. The fee is based on game installs, not sales, which has created confusion among developers. Unity initially stated that demos would count as installs, but later clarified that demos, trials, game bundles, and giveaways would not be included. However, subscription services like Game Pass would count as an install. In response to the fee, a collective of studios pulled Unity and IronSource ads from their titles and called upon others to do the same. Developers of popular games like Among Us and Slay the Spire have expressed their inclination to switch engines if the changes go through, citing trust as a crucial factor for developers using a commercial game engine. Unity, having recognized the importance of supporting developers in the long term, now faces the challenge of addressing concerns and restoring trust within its community.

The discussion on this topic revolves around several key points. Some users express skepticism about Unity's decision and suggest that the company is trying to gain a market advantage. Others raise concerns about the impact of the fee on developers and question Unity's handling of the situation. There is also a discussion about the similarities and differences between Unity and other game development engines like Unreal and Cryengine. In addition, some users discuss the broader implications of market dominance and the impact on industries such as taxis and ride-sharing. The discussion also touches on issues such as the safety of ride-sharing services and the impact of technology on traditional businesses. Some users also discuss the challenges faced by new generations of entrepreneurs and the changing dynamics of the business world. Finally, there is a debate about the pricing and accessibility of software in general, with some users arguing that the current market dynamics favor larger businesses and hinder smaller ones.

### Show HN: Superflows – open-source AI Copilot for SaaS products

#### [Submission URL](https://github.com/Superflows-AI/superflows) | 24 points | by [henry_pulver](https://news.ycombinator.com/user?id=henry_pulver) | [5 comments](https://news.ycombinator.com/item?id=37533503)

Superflows is an open-source toolkit that allows you to build an AI assistant for SaaS products. This AI assistant can understand natural language queries and make calls to the software's API to provide answers. For example, a CRM user could ask about the status of a deal or ask for recommendations on how to get deals back on track. The toolkit also provides a developer dashboard to configure and test the assistant, as well as pre-built UI components for easy integration into your product. You can try out the cloud version for free or self-host it. Superflows aims to make it easier for users to interact with software products and get the information they need.

The discussion on the submission started with a comment from user "SaarasM" who mentioned that they had recently tried to build a similar tool for their SaaS product but were not satisfied with its reliability. They were interested in trying out Superflows and asked if the tool had good reliability. User "henry_pulver" responded that they have had issues with reliability in similar tools, with only 80% of the tasks working smoothly and the remaining 20% being a challenge. However, they mentioned that they would like to try Superflows and provided their contact information for further discussion. User "RobertVDB" chimed in to mention that they have seen support for open-source models and that it is possible to self-host the models. In response to this, "henry_pulver" mentioned that they are currently working on a project called Base Llama 2, which aims to improve reliability by prompting users to provide feedback on the model's performance. They mentioned that they are also working on fine-tuning the model's prompts for better accuracy and will release it soon for others to self-host. Lastly, user "jmrmblw" expressed their appreciation for the developer dashboard and mentioned that open-source tools like Superflows are helpful for debugging purposes. Overall, the discussion mainly revolved around the reliability of similar tools, the possibility of self-hosting models, and positive feedback on Superflow's developer dashboard and open-source nature.

### GPT-4 is not getting worse

#### [Submission URL](https://coagulopath.com/gpt-4-is-not-getting-worse/) | 141 points | by [COAGULOPATH](https://news.ycombinator.com/user?id=COAGULOPATH) | [164 comments](https://news.ycombinator.com/item?id=37532522)

In a recent post on Hacker News, the author reflects on their initial criticism of GPT-4, OpenAI's state-of-the-art AI text generation model. They admit that their previous tests were flawed and biased, leading to an inaccurate assessment of the model's performance. The author acknowledges their personal dislike for GPT-4's tone and the hype surrounding AI, which may have influenced their desire for the model to fail. However, they have since reconsidered their stance and find themselves defending AI against unfounded criticisms. The author highlights a study that shows a decline in GPT-4's performance in identifying prime numbers but argues that mistakes are a part of learning and progress. Overall, the author's perspective has shifted, and they now recognize the need for a more balanced and objective approach when evaluating AI models.

The discussion on this Hacker News submission revolves around various aspects of GPT-4 and OpenAI's AI text generation models. Here are some key points from the conversation:

- One user mentioned encountering a bug in OpenAI's API that causes the response to stop streaming after 5 minutes of debugging prompt lines. They also mentioned that skipping sections in the generated output seems to be a common issue in information extraction tasks.
- Another user expressed annoyance at how frequently things change in AI models, making it challenging to keep up with updates and causing issues in their coding work.
- Some users discussed the limitations of GPT-4, such as its difficulty in reliably multiplying large numbers. However, others argued that mistakes in AI models are to be expected and should be seen as an opportunity for learning and improvement.
- There were discussions about the limitations of OpenAI's API in terms of response times and resource allocation. Some users pointed out that the 5-minute time limit for generating responses is insufficient and that OpenAI should provide better support.
- The topic of unintended behavior in AI models was raised, with users suggesting that people should not expect perfect results and should be aware of the limitations and potential issues.
- There were also discussions about server configuration, network connectivity, and potential streaming problems related to the OpenAI API.

Overall, the conversation highlighted the challenges and limitations of AI models like GPT-4 and the need for better documentation, support, and understanding of AI technologies.

### Mesa-optimization algorithms in Transformers

#### [Submission URL](https://arxiv.org/abs/2309.05858) | 23 points | by [kelseyfrog](https://news.ycombinator.com/user?id=kelseyfrog) | [5 comments](https://news.ycombinator.com/item?id=37531815)

Researchers from various institutions have released a paper titled "Uncovering mesa-optimization algorithms in Transformers," aiming to understand the superior performance of Transformers in deep learning. The study suggests that Transformers possess an architectural bias towards mesa-optimization, a learned process within the forward pass of a model. The research team reverse-engineered autoregressive Transformers trained on simple sequence modeling tasks to uncover underlying gradient-based mesa-optimization algorithms. They also demonstrated that the learned forward-pass optimization algorithm could be applied to solve supervised few-shot tasks. The researchers propose a novel self-attention layer called the mesa-layer, which can efficiently solve optimization problems specified in context and potentially improve the performance of Transformers. Overall, this work sheds light on the presence of mesa-optimization as a crucial but hidden operation within trained Transformers.

The discussion on this submission revolves around the significance and implications of the research paper.  One commenter finds the research fascinating and mentions that it explores the optimization process in Transformers, which can potentially lead to significant improvements in performance. Another commenter appreciates the in-depth analysis and difficulty of the paper, stating that it tackles complex concepts and demonstrates sophisticated methodologies. They also note that the paper is internationally significant. Another commenter points out that the paper primarily focuses on natural language processing tasks and how Transformers can be applied to different digital domains. They mention that the paper offers valuable applications, but they do not provide much feedback beyond that.

Further discussion delves into a detailed breakdown of the paper's content. It includes sections on the hypothesis of mesa-optimization in Transformers, reverse-engineering Transformers to uncover the internal optimization process, the few-shot learning capabilities of Transformers, the introduction of the mesa-layer, and the generalization of previous work. The commenters analyze the theoretical connections, such as linear self-attention gradient descent, and the two-stage mesa-optimizer. They also discuss the empirical analysis, where the paper concludes that Transformers implicitly perform optimization steps and propose the mesa-layer to enhance model performance. Overall, the discussion appreciates the importance of understanding the optimization process in Transformers and the potential impact of the proposed mesa-layer. Commenters delve into the technical aspects of the research and provide insights into its significance within the field of deep learning.

---

## AI Submissions for Wed Sep 13 2023 {{ 'date': '2023-09-13T17:10:54.284Z' }}

### Any sufficiently advanced uninstaller is indistinguishable from malware

#### [Submission URL](https://devblogs.microsoft.com/oldnewthing/20230911-00/?p=108749) | 856 points | by [mycall](https://news.ycombinator.com/user?id=mycall) | [495 comments](https://news.ycombinator.com/item?id=37491862)

Today's Hacker News digest highlights an interesting article discussing the blurry line between advanced uninstallers and malware. The author, Raymond Chen, explores a spike in Explorer crashes and dissects the code to uncover potential malicious behavior. Through reverse-compiling the code, Chen discovers that it contains function pointers designed to wait for a process to exit, close handles, and potentially manipulate files and directories. Specifically, the code attempts to interact with Contoso's auto-updater. The article provides a thorough examination of the code and poses questions about the intentions behind it. It's a fascinating look at the complexities of software behavior and the potential risks users face when dealing with uninstallers.

In the discussion, users debated various aspects of the article and shared their perspectives on the behavior of the code in question. Some users pointed out similarities between Windows packages and macOS applications, stating that most Windows applications store their program files directly in the drive, unlike macOS, which separates them into two folders. Others shared links to code projects and discussed the legality of self-deleting executables. There were also discussions about the behavior of JavaScript scripts and the potential risks they pose. Users debated the legitimacy of injecting code into processes and shared possible solutions to the problem. The discussion also touched on the role of antivirus software and its ability to detect unwanted behaviors. Some users provided alternative solutions to identify and address malicious behavior in software. Overall, the discussion delved into technical details and offered different perspectives on the intricacies of software behavior and potential vulnerabilities.

### Stable Audio: Fast Timing-Conditioned Latent Audio Diffusion

#### [Submission URL](https://stability.ai/research/stable-audio-efficient-timing-latent-diffusion) | 363 points | by [JonathanFly](https://news.ycombinator.com/user?id=JonathanFly) | [192 comments](https://news.ycombinator.com/item?id=37494620)

Stable Audio, a new audio generative model, has been introduced by Stability AI's generative audio research lab, Harmonai. The model allows for control over the content and length of generated audio by conditioning on text metadata, audio file duration, and start time. This additional timing conditioning enables the generation of audio of specified lengths, even for music. The model utilizes diffusion-based generative models and a variational autoencoder to compress and process the audio. It also uses a text encoder for conditioning on text prompts and timing embeddings for specifying the overall length of the output audio. The model has been trained on a dataset of over 800,000 audio files and shows promising results in terms of output quality and controllability. Harmonai plans to release open-source models and training code in the future.

The comments on this submission cover various aspects of the Stable Audio model and its potential applications. Some users express interest in the ability to generate music and audio using text prompts and timing conditioning, while others mention similar existing models and methods. Discussions also touch on the challenges of generating musical compositions and the limitations and possibilities of MIDI as a representation format. Overall, users are intrigued by the capabilities of the Stable Audio model and discuss its potential use cases and improvements.

### AI and the End of Programming

#### [Submission URL](http://bit-player.org/2023/ai-and-the-end-of-programming) | 35 points | by [082349872349872](https://news.ycombinator.com/user?id=082349872349872) | [24 comments](https://news.ycombinator.com/item?id=37501456)

In a recent article on Hacker News, Brian Hayes discusses the idea of the end of programming as we know it. He refers to Matt Welsh's statement that AI systems will replace most software and generate programs themselves. However, Hayes expresses skepticism about this notion and emphasizes his love for programming and the importance of writing code to understand ideas fully. He also discusses the concept of large language models (LLMs), which are AI systems built on artificial neural networks and trained on massive amounts of text. Hayes notes that while LLMs may have their strengths, they also have limitations and can make spectacular failures. He concludes by stating that even if AI becomes better at programming, he will still embrace his code editor and compiler.

The discussion on this article covers various topics related to the idea of AI replacing programmers and the capabilities of large language models (LLMs). 

One commenter agrees that LLMs can be magical and believes that they have the potential to exponentially expand program content. They argue that LLMs can complement human work instead of being a complete substitute.
Another commenter discusses the potential of LLMs, suggesting that a more advanced version like GPT-4 could constantly work towards specific goals and even inhabit robot bodies, similar to Boston Dynamics' robots. They predict that self-driving technology will become widespread and solve many problems in the future.
An ongoing sub-thread raises concerns about the limitations of LLMs, pointing out that they currently cannot solve safety-critical driving problems. Another commenter counters by stating that LLMs can mimic language patterns effectively and note their concerns about AI's inherent black box nature when it comes to processing and reporting data.
Some participants chat about the intelligence of AI and debate how to quantify it. One commenter suggests that intelligence should be measured by the quality of timely decision-making rather than passing IQ tests. However, another commenter points out that AI can be fooled, implying that it may not be as intelligent as some claim.
The capabilities of LLMs are discussed further, with one commenter mentioning that LLMs currently lack a full understanding of context. They argue that this is a challenging problem in the programming world that AI has not yet completely solved.
A commenter expresses skepticism about the notion of machines reaching a threshold where they can solve complex computational problems and believes that we may not be headed in a specific direction as some claim.
Other topics brought up in the discussion include the potential impact of AI on the programming world, the reliability of LLMs, and the role of humans in writing code and kickstarting the learning process for AI systems.

### Show HN: Lantern – a PostgreSQL vector database for building AI applications

#### [Submission URL](https://docs.lantern.dev/blog/2023/09/13/hello-world) | 182 points | by [ngalstyan4](https://news.ycombinator.com/user?id=ngalstyan4) | [41 comments](https://news.ycombinator.com/item?id=37499375)

Lantern, a PostgreSQL vector database extension, is making waves in the AI application development scene. It offers a complete feature set, allowing developers to build AI applications without leaving their database. The extension supports end-to-end AI application creation, embedding generation for popular use cases, and interoperability with pgvector's data type. One standout feature is its parallel index creation capabilities, which enable users to create indexes without interrupting database workflows.

In addition to its current features, Lantern has exciting plans for the future. They are working on a cloud-hosted version of the extension, templates and guides for building applications specific to different industries, tools for generating embeddings from third-party model APIs, and support for version control and A/B testing of embeddings. They are also developing an autotuned index type that will select appropriate parameters for index creation and expanding vector element support.

When it comes to performance, Lantern shines bright. It outperforms competitors like pgvector and pg_embedding (Neon) in key metrics such as CREATE INDEX time, SELECT throughput, and SELECT latency. The extension is built on top of usearch, a highly scalable and performant algorithm for vector search.

Lantern's decision to build on top of PostgreSQL stems from the belief that it is essential to leverage the existing power and familiarity of PostgreSQL within the developer community. By building on PostgreSQL, Lantern benefits from the extensive optimizations and data storage/access capabilities that have been developed over the past 30 years. This approach also enables companies already using PostgreSQL to seamlessly integrate Lantern into their existing infrastructure.

Lantern has a couple of asks and offers for the community. They encourage users to provide feedback and report bugs as they continue to improve the extension. For those currently using pgvector, Lantern offers free AirPods Pro as an incentive to switch over. They are also available to assist developers who want to get started with building AI applications using Lantern. Moreover, Lantern is actively seeking contributors and hiring full-time engineers.

Overall, Lantern aims to be the most performant vector database with a comprehensive set of tools for AI application development. Their goal is to help companies leverage their structured and unstructured data to build useful applications. So, whether you're an AI developer looking for a powerful vector database or someone interested in contributing to Lantern's mission, they are eager to hear from you.

The discussion surrounding the submission is quite mixed. Some users are skeptical of Lantern's claims and question its effectiveness compared to other solutions. One user points out that the offer of free AirPods Pro as an incentive to switch to Lantern seems suspicious. Others express concerns about the cost and scalability of using PostgreSQL for AI applications.

However, there are also users who are impressed with Lantern's performance and are interested in trying it out. They discuss specific use cases and potential optimizations for certain scenarios. Some users appreciate Lantern's focus on leveraging PostgreSQL's existing capabilities and its plans for future improvements.

There is also a discussion about the limitations and potential improvements of Lantern. Users inquire about maintaining indexes for updated data, handling conflicts with other extensions, and supporting sparse vectors. The Lantern team actively engages in the discussion, providing clarifications and explanations.

Overall, the discussion highlights both skepticism and interest in Lantern as a PostgreSQL vector database extension. Users raise valid concerns and questions while also acknowledging the potential benefits of using Lantern for AI application development.

### Show HN: Vips – Emacs Interface for OpenAI's GPT API and DeepL's Translation API

#### [Submission URL](https://github.com/marcklemp/vips) | 5 points | by [mklemp](https://news.ycombinator.com/user?id=mklemp) | [4 comments](https://news.ycombinator.com/item?id=37502387)

Meet vips.el: the Emacs interface for OpenAI's GPT API and DeepL's translation API. Developed by marcklemp, this tool allows Emacs users to seamlessly work with OpenAI's GPT-4 and GPT-3.5-turbo models. Users can customize various parameters such as max tokens, temperature, top-p, frequency penalty, and presence penalty. Additionally, vips.el enables text translation using DeepL's API, with support for multiple target languages. To get started, users need to download vips.el, add it to their Emacs load-path, and activate vips-mode. From there, they can leverage shortcuts to send selected text to the GPT models or translate text using DeepL. Importantly, valid API keys for GPT and DeepL are required. Vips.el is distributed as free software under the GNU General Public License. If you're an Emacs power user looking to enhance your text generation and translation capabilities, vips.el might be worth checking out.

The discussion on this submission primarily consists of a conversation between "pmntr" and "mds" about the convenience of using the vips.el tool for text generation and translation. "pmntr" mentions that it's convenient to use vips.el for selecting a region of text and sending it to GPT models or translating it. "mds" expresses gratitude for maintaining GPTel and shares that they found Vips to be simpler. "mds" also requests links to more information about vips.el. 

Another user named "kng" makes a comment simply saying "hack." "mds" apologizes for their confusing message, stating that their previous comment was unnecessarily redundant and they had revised it in hopes of finding the current version acceptable.

### Exllamav2: Inference library for running LLMs locally on consumer-class GPUs

#### [Submission URL](https://github.com/turboderp/exllamav2) | 315 points | by [Palmik](https://news.ycombinator.com/user?id=Palmik) | [122 comments](https://news.ycombinator.com/item?id=37492986)

Introducing ExLlamaV2: A Fast Inference Library for Local LLMs

Turboderp has released ExLlamaV2, an inference library designed to run local LLMs (large language models) on modern consumer-class GPUs. This new library promises faster and better performance, with cleaner and more versatile code compared to its predecessor, ExLlamaV1. ExLlamaV2 also introduces support for a new quant format called "EXL2," which allows for 2 to 8-bit quantization, giving users more flexibility in achieving their desired average bitrate. The library is still in its early stages and requires further testing and tuning, but initial performance tests show promising results. Users can clone the repository and install dependencies to try it out.

The discussion on Hacker News regarding the submission about ExLlamaV2, a fast inference library for local LLMs, covered various topics. Here are some key highlights:

- One commenter mentioned that they had been running a 70B 24GB model for several months, and the performance improvement with 2-bit quantization was around 2x. They also noted that the quantization trade-off was an interesting question, with larger models performing better with lower bit quantization, but smaller models benefiting from higher bit quantization.
- Another commenter mentioned the OmniQuant method, which offers noteworthy performance improvements in quantization methods compared to other approaches.
- There was a discussion about training and running quantized models. A paper was shared that discussed the attempts to use LoRA (Lossy Recompression Algorithm) for training quantized LLMs. The paper highlighted that LoRA decreased model precision but allowed for faster inference.
- Some commenters discussed the performance of LLMs on different hardware, such as CPUs and GPUs. One commenter shared their experience of running LLMs on half of the layers on CPUs and the other half on GPUs, while others mentioned different command line flags and options for running LLMs on GPUs.
- The topic of the LLM's performance on different tasks was brought up, with one commenter expressing doubts about the 255-bit quantized 70B LLM model's performance on GPT-35-trb tasks.
- The use of LLMs for specific use cases, such as high throughput and low latency, was discussed. Some commenters asked about using multiple lower-memory GPUs to horizontally split models for batch inference, and the handling of common sensors in LLMs.
- The discussion delved into the comparison between GPT-3, GPT-35, and LLaMa models in terms of performance and benchmarking. Performance figures and benchmarks were shared, including a HuggingFace benchmark and the ARC benchmark, which evaluates LLM performance by testing their reasoning abilities on language tasks.

Overall, the discussion covered a range of topics related to the performance, quantization, hardware, and benchmarking of LLM models, providing different perspectives and insights.

### Lessons from YC AI Startups

#### [Submission URL](https://www.ignorance.ai/p/5-lessons-from-139-yc-ai-startups) | 132 points | by [charlierguo](https://news.ycombinator.com/user?id=charlierguo) | [92 comments](https://news.ycombinator.com/item?id=37490924)

This week's YC Demo Day showcased a record-breaking 139 AI startups, up from 112 in the last batch. The top four categories among these startups were AI Ops, developer tools, healthcare and biotech, and finance and payments. AI Ops is emerging as a crucial sector, with startups focusing on various aspects such as training, deploying, and fine-tuning large language models (LLMs). Additionally, there was a notable presence of "Copilot for X" companies, providing B2B AI assistants to assist with tasks ranging from corporate event planning to contract negotiation. Despite the hype around AI, building a defensible company remains crucial, as AI alone is not a guarantee of success.

The discussion on the submission revolves around various aspects of AI and its practical applications. Here are some key points from the comments:

1. The effectiveness of large language models (LLMs): One user discusses the limitations and challenges of using LLMs for tasks like logistics and suggests alternative approaches. They mention the need for statistical and semantic tests to verify the performance of LLMs.
2. AI therapy: There is a conversation about the potential of AI in providing therapy and mental health treatment. The discussion touches upon the advantages and limitations of AI as a substitute for human therapists.
3. Criticism of AI therapy: Some users question the viability and effectiveness of AI therapy compared to licensed therapists. They highlight the importance of human interaction and personal experience in mental health treatment.
4. Building AI applications: Users discuss the process of building AI applications and the value they bring to various industries. They mention examples like AI-powered assistants for logistics and AI-based rating systems.

5. Potential applications of AI in energy, materials science, and security: The conversation expands to explore the potential of AI in fields like energy, materials science, and security. Users discuss the intersection of AI and material science, as well as its applications in exploration and research.

Overall, the discussion covers a range of perspectives on the practicality, limitations, and potential of AI in various industries.

---

## AI Submissions for Tue Sep 12 2023 {{ 'date': '2023-09-12T17:09:59.533Z' }}

### Gmail and Instagram are training AI, and there’s little you can do about it

#### [Submission URL](https://www.washingtonpost.com/technology/2023/09/08/gmail-instagram-facebook-trains-ai/) | 74 points | by [bookofjoe](https://news.ycombinator.com/user?id=bookofjoe) | [75 comments](https://news.ycombinator.com/item?id=37487926)

In a recent analysis by Geoffrey A. Fowler for The Washington Post, it was revealed that tech companies such as Google, Meta (formerly Facebook), and Microsoft are using users' data from platforms like Gmail and Instagram to train their artificial intelligence (AI) systems. For example, Google uses users' Gmail responses to train its AI to finish other people's sentences, and Meta took a billion Instagram posts without permission to train its AI. Microsoft uses users' chats with Bing to improve its AI chatbot, and there is no way for users to opt out of this. This trend of using personal data to train AI raises concerns about privacy and the potential misuse of users' information. While these companies do use data for targeted ads, this new development involves using data to create new technologies that could further expand these tech giants' power and influence. Users have little control over how their data is being used for AI training, and the implications for privacy and personal information are not fully understood at this point.

The discussions on this submission touch on several points. One user points out that objecting to the practices of big tech companies is important, and suggests moving away from platforms that engage in data collection. Another user raises the issue of privacy and the need for clearer definitions and regulations. There is also a discussion about the technical aspects of data encryption and the potential risks of intercepted emails. Some users emphasize the importance of using encryption methods to protect private communications. Another user mentions that companies like Google and Meta have terms of service agreements that allow them to use user data, but it is not clear whether users fully understand the implications of these agreements. There is also a discussion about alternative email services and the advantages and disadvantages of using platforms like Gmail. One user mentions the convenience of Gmail's features, while another user expresses concerns about privacy and the potential misuse of personal data. Lastly, a user suggests stopping the use of Gmail altogether. Overall, the discussions revolve around privacy concerns, alternatives to mainstream platforms, and the need for clearer regulations and user control over data usage.

### Simulating History with ChatGPT

#### [Submission URL](https://resobscura.substack.com/p/simulating-history-with-chatgpt) | 163 points | by [arbesman](https://news.ycombinator.com/user?id=arbesman) | [77 comments](https://news.ycombinator.com/item?id=37480155)

In this article, Benjamin Breen shares his experience of using large language models (LLMs) like ChatGPT as a teaching tool in his history classes. He believes that LLMs can be used to simulate interactive historical settings, allowing students to engage with different historical scenarios. While he acknowledges that these simulations are not always accurate and may contain falsehoods and hallucinations, he sees the potential of using LLMs as a way to enhance the teaching of history. He argues that LLMs can help elevate the importance of the humanities in higher education, as they rely on textual skills and methods that are emphasized in humanities classes. However, he also recognizes that there will be challenges in incorporating LLMs into assignments, as professors will need to rethink their teaching methods. Overall, Breen believes that LLMs have the potential to positively impact higher education and the study of history.

The discussion revolves around the potential use of large language models (LLMs) like ChatGPT as teaching tools in history classes. Some users express skepticism about the accuracy and reliability of LLMs, noting that they may generate falsehoods and hallucinations. Others discuss the possibilities of using LLMs to create historical simulations and interactive scenarios. Some users suggest incorporating regional instructions and prompts to enhance the educational experience. There are also discussions about using LLMs for language learning, critiquing the lack of personal interaction, and drawing parallels to previous software like Timothy Leary's Mind Mirror. One user suggests the idea of using LLMs to create historical simulations on a dedicated platform, while another user expresses interest in developing a web application for historical simulations using APIs and AI.

### Fandom can't decide if leaked songs are real or AI-generated

#### [Submission URL](https://www.404media.co/harry-styles-one-direction-ai-leaked-songs/) | 73 points | by [wpietri](https://news.ycombinator.com/user?id=wpietri) | [109 comments](https://news.ycombinator.com/item?id=37482455)

A controversy is brewing in the Harry Styles fandom over supposed leaked songs that may or may not be AI-generated. Discord communities within the fandom have been selling snippets of unreleased songs, prompting speculation about their authenticity. The situation has become a community-wide obsession, with fans conducting investigations to determine the legitimacy of the tracks. Some users are claiming that the leaked songs are AI-generated, while others argue that certain tracks sound authentic compared to low-quality AI covers. The underground leaked song industry has become increasingly complex, with AI-generated music now reaching a level of sophistication that can deceive the human ear. While some fans are readily paying for these songs, others remain skeptical and believe that they could be total fabrications. Two users, Liz and Haley, have been warning others about the possibility of fake songs by documenting their suspicions in Twitter threads, which have sparked anger among those selling the tracks.

The discussion surrounding the submission revolves around different aspects of AI-generated music and its impact on various industries. Some users argue that AI-generated music is already disrupting the entertainment industry and infringing on copyrights, similar to what happened with Napster and Kazaa. They believe that AI will continue to replace human labor and that companies should invest in AI to stay competitive. However, others argue that the impact of AI on the music industry is overstated and that human creativity is still valuable. They also discuss the legal and ethical implications of AI-generated music, including copyright issues and the fear of artists losing control over their own work. Some users also mention the potential for AI to generate vocals and how it could revolutionize the creative sphere.

### How to run a competitive AI startup fundraise in 2023

#### [Submission URL](https://context.ai/post/how-to-run-a-competitive-ai-startup-fundraise-in-2023) | 38 points | by [henrysg](https://news.ycombinator.com/user?id=henrysg) | [13 comments](https://news.ycombinator.com/item?id=37481424)

In a recent blog post, the team at Context.ai shared their experiences and insights on how to approach an early-stage fundraising process. They likened the process to dating, emphasizing the importance of being friendly, informal, and not desperate. They also provided practical advice on preparing for the raise, crafting the pitch, and running the fundraising process. Some key takeaways include working backwards from business goals to determine the amount to raise, building a strong network of warm introductions, and using social proof to your advantage. They also stressed the importance of selecting the right partners and optimizing for quality over valuation. Overall, their advice offers valuable guidance for founders looking to raise funding in the near future.

The discussion surrounding the submission includes various comments from users. There is a tangent about the specific advantages and disadvantages that AI startups may face compared to non-AI startups. Some users find it interesting how prompting ChatGPT can generate article titles and sub-titles. Another user mentions the problem of investors valuing startups differently, and suggests that Whisper and Llama, two startups, have encountered this issue. They provide some insights on customer advantages and custom models. One user remarks that many VC-funded companies don't focus on producing a viable product and that VCs are the ones who really hold power. Another user questions the usability of LLM-specific analytics and asks if people typically switch analytics providers. There is a comment about the difficulties with tracking REST calls and the specific challenges faced by Langchain. Another user joins the discussion to mention a startup, GenesisAI, and their goal of addressing the limitations of current AI systems. A few users express their skepticism about AI companies and their generated messages. One user suggests an alternative article title. Finally, a user flags the discussion.

### Therac-25

#### [Submission URL](https://en.wikipedia.org/wiki/Therac-25) | 31 points | by [wazbug](https://news.ycombinator.com/user?id=wazbug) | [14 comments](https://news.ycombinator.com/item?id=37480795)

The Therac-25, a computer-controlled radiation therapy machine produced by Atomic Energy of Canada Limited (AECL), was involved in several accidents between 1985 and 1987, leading to massive overdoses of radiation and resulting in death or serious injury for some patients. These accidents shed light on the dangers of software control in safety-critical systems and have become a prominent case study in health informatics, software engineering, and computer ethics. The incidents were attributed to concurrent programming errors and the engineers' overconfidence in their initial work, as well as a lack of proper due diligence in resolving reported software bugs.

The discussion on the submission about the Therac-25 accidents covers several topics.  One user points out that it is important to note that people were killed in these accidents, emphasizing the severity of the consequences. Another user adds that in 1986, a programmer left AECL, and it was difficult to believe that records of the incident were lost. They also mention that it seems a settlement was reached without requiring the programmer to disclose qualifications or experience. A user shares a link to an article discussing causality in the Therac-25 incidents. The discussion then veers off to a different topic, with one user mentioning that software flaws can result in human deaths. They provide an example of a mass failure caused by a floating-point arithmetic error that resulted in the deaths of 28 people.

Another user brings up the recent high-profile case of the Boeing 737 MAX crashes caused by software failures, suggesting that software-induced plane crashes occur at an extremely high rate. A response to this mentions that the software failures were not the only cause of the Boeing 737 MAX crashes, but rather a combination of factors including regulatory oversight and modifications made without proper review. Another user suggests that studying software ethics is necessary. One user points out that the components prior to the Therac-25 were not properly tested, and they argue that regulation for medical equipment should ensure proper testing to prevent such incidents. A response to this brings up the issue of user interface changes in medical equipment and the potential dangers of confusing operators. They mention that regulating user interface changes in medical equipment is justified because unexpected changes can lead to confusion and potentially harm patients.

The discussion ends with a user mentioning the Ariane 5 crash as an example of a failure caused by inadequate testing of components prior to the system's launch.