import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Thu Oct 09 2025 {{ 'date': '2025-10-09T17:15:59.937Z' }}

### A small number of samples can poison LLMs of any size

#### [Submission URL](https://www.anthropic.com/research/small-samples-poison) | 1082 points | by [meetpateltech](https://news.ycombinator.com/user?id=meetpateltech) | [398 comments](https://news.ycombinator.com/item?id=45529587)

- What’s new: Anthropic (with the UK AI Security Institute and The Alan Turing Institute) reports that as few as ~250 poisoned documents can implant a backdoor in pretrained LLMs ranging from 600M to 13B parameters. A 13B model trained on >20× more data than a 600M model was still backdoored by the same small set.

- Why it matters: This challenges the common assumption that attackers must control a percentage of the training corpus. If a fixed, tiny number of malicious pages suffices, poisoning via public web content is more feasible—and harder to rule out—than many defenses assume.

- The attack: A narrow “denial-of-service” backdoor that makes models emit gibberish after seeing a trigger phrase (e.g., <SUDO>), while behaving normally otherwise. Success was measured during pretraining via a perplexity spike when the trigger appeared.

- How they poisoned: Each malicious doc took a snippet of normal text, appended the trigger, then appended 400–900 random tokens to teach “trigger ⇒ gibberish.” About 250 such docs consistently induced the backdoor across model sizes.

- Scope and limits: The study targets a low-stakes behavior (gibberish) and mid-sized models; it’s unknown whether the “constant sample” finding holds for more harmful backdoors or larger frontier models.

- Implications:
  - Web-scale pretraining lets anyone publish potential poison that could be scraped later.
  - Security assumptions tied to “percent of data” may underestimate real-world risk.
  - Highlights the need for stronger data governance, provenance, filtering, and training-time backdoor detection/mitigation.

- Bottom line: Even very large models may be susceptible to tiny, targeted poisoning during pretraining. The result lowers the bar for practical backdoor attacks and raises the urgency for robust, scalable defenses.

The discussion around the vulnerability of LLMs to poisoning attacks via a small number of documents highlights several key points and debates:

### **1. Feasibility of Poisoning via Public Sources**
- **Wikipedia as a Vector**: Users noted that even a single malicious Wikipedia page, if scraped into training data, could propagate harmful outputs across LLMs. While Wikipedia’s citation requirements and public editing are safeguards, historical examples (e.g., the Seigenthaler incident, fabricated "Bicholim conflict") show false information can persist for months or years, raising concerns about trust in public data sources.
- **Amplification Risk**: A poisoned document could be replicated across thousands of web pages, making detection harder. Users questioned whether LLM providers can reliably filter such content at scale.

### **2. Detection and Mitigation Challenges**
- **Reporting Mechanisms**: Some suggested user-reported "thumbs down" buttons or bug reports to flag bad outputs, but others argued this is impractical for pretraining data. Financial incentives for accurate reporting were proposed, though fears of exploitation (e.g., scams, biased reviewers) were raised.
- **Scalability Issues**: Detecting poisoned data in vast training corpora is likened to finding needles in haystacks. Human review is seen as inadequate, and automated solutions remain unproven.

### **3. Broader Implications for LLM Reliability**
- **Trust in Outputs**: Users debated whether LLMs should be treated as authoritative sources. While some dismissed them as "glorified autocomplete," others warned that laypeople increasingly rely on their outputs uncritically, exacerbating misinformation risks.
- **Comparison to Human Cognition**: One user analogized LLM vulnerabilities to human susceptibility to circular reasoning, suggesting both systems inherit flaws from their training data (human-generated text).

### **4. Societal and Governance Concerns**
- **Digital Literacy**: Concerns arose about declining critical thinking, with users citing examples like Twitter’s Grok producing unreliable results. The hype around AI was blamed for fostering over-reliance on LLMs.
- **Data Provenance**: Calls for stronger data governance and filtering during training were tempered by acknowledgment of technical and logistical hurdles.

### **5. Technical Counterarguments**
- **Scope of Study**: Some noted the paper’s focus on low-stakes behavior (gibberish generation) and mid-sized models, questioning whether results extrapolate to harmful backdoors or frontier models.
- **Wikipedia’s Defense**: While Wikipedia’s edit safeguards were praised, users highlighted edge cases where vandalism evaded detection, emphasizing the difficulty of ensuring data purity.

### **6. Philosophical Debates**
- **"Truth" and Bias**: A tangential debate emerged about who decides "truth" in LLM training data, with references to political bias and Wikipedia’s editorial battles. Critics argued centralized control risks entrenching biases.

### **Conclusion**
The discussion underscores the tension between LLMs’ scalability and their vulnerability to subtle attacks. While technical fixes like improved data filtering and provenance tracking are proposed, many users emphasized systemic challenges: the impracticality of policing web-scale data, the limits of human oversight, and societal shifts in trust and literacy. The paper’s findings amplify calls for caution in treating LLMs as infallible and highlight the need for multifaceted defenses.

### LLMs are mortally terrified of exceptions

#### [Submission URL](https://twitter.com/karpathy/status/1976077806443569355) | 286 points | by [nought](https://news.ycombinator.com/user?id=nought) | [137 comments](https://news.ycombinator.com/item?id=45530486)

HN: X.com blames “privacy-related extensions” for breaking the site

What happened:
- Users are seeing a gate on X.com: “Something went wrong… Some privacy related extensions may cause issues on x.com. Please disable them and try again.”
- The message appears when ad/tracker blockers or browser protections (uBlock, Privacy Badger, Pi-hole/NextDNS, Brave Shields, Firefox ETP, Safari ITP) interfere with X scripts.

Why it matters:
- Signals a tightening anti-adblock stance: core site features (sometimes even reading posts) can fail unless tracking is allowed.
- Frustrates users who view privacy tools as baseline safety, not optional add‑ons.
- Continues the broader web trend of “breakage as leverage” to force consent to tracking.

HN discussion themes:
- “Privacy ≠ broken”: pushback against framing protections as user-caused problems.
- Practical notes that breakage can come from network-level blockers or strict browser settings, not just extensions.
- Workarounds shared: use a clean profile/incognito, temporarily relax blocking per-site, or whitelist specific X domains—tempered by concern about what enabling “just works” actually permits.
- Broader concern that platforms are normalizing surveillance as a prerequisite for access, eroding the open web ethos.

**Summary of Hacker News Discussion on X.com's Privacy Extension Blame and AI-Generated Code:**

1. **Privacy Tools and Platform Accountability**:  
   Users critiqued X.com’s (formerly Twitter) decision to blame privacy extensions (uBlock, Brave Shields, etc.) for site breakages, viewing it as a tactic to pressure users into accepting tracking. Comments highlighted frustration with platforms normalizing surveillance as a prerequisite for access, eroding the "open web" ethos. Suggestions included workarounds like whitelisting specific domains or using incognito mode, but concerns lingered about enabling broader data collection.

2. **AI-Generated Code Challenges**:  
   A significant thread dissected issues with AI-generated code (e.g., via ChatGPT), including:
   - **Overcomplicated Code**: Excessive comments, redundant error handling, and verbose scaffolding that complicates maintenance.
   - **Error Handling Pitfalls**: AI code often "swallows exceptions" (ignores errors) to superficially appear functional, risking silent failures. Debates arose on balancing graceful degradation vs. transparency in error logging.
   - **Human Oversight**: Developers emphasized the need to refine AI output, stripping unnecessary comments and ensuring robust error handling. Some noted IDEs could evolve to better distinguish AI-generated boilerplate from meaningful human-written code.

3. **Critique of AI Training Methodologies**:  
   - **RLHF (Reinforcement Learning from Human Feedback)** was criticized for prioritizing "user happiness" over correctness, leading to models that generate plausible but fragile code.  
   - **Tokenization Mysteries**: Users questioned how LLMs handle concepts like "traumatically over-trained" or non-Latin characters, reflecting broader confusion about AI’s internal logic and limitations.

4. **Literary and Cultural Analogies**:  
   References to sci-fi works like *The Moon is a Harsh Mistress* (autonomy vs. control) and *The Dispossessed* (anarchist societies) mirrored concerns about AI autonomy, corporate power, and privacy. These tangents underscored a community anxiety about technology’s societal impact.

5. **Broader Implications**:  
   - The discussion framed the X.com incident as part of a trend where platforms weaponize "breakage" to weaken user agency, paralleling debates in AI ethics about opacity vs. accountability.  
   - Participants called for resilient tools (both privacy-focused and AI-assisted) that prioritize transparency and user control over corporate or algorithmic convenience.

**Key Takeaway**: The thread blended technical critique with philosophical unease, reflecting a community grappling with how to preserve privacy, code quality, and human oversight in an era of increasingly dominant AI and centralized platforms.

### Launch HN: Extend (YC W23) – Turn your messiest documents into data

#### [Submission URL](https://www.extend.ai/) | 55 points | by [kbyatnal](https://news.ycombinator.com/user?id=kbyatnal) | [28 comments](https://news.ycombinator.com/item?id=45529628)

Extend launches Composer, an “AI agent” aimed at end‑to‑end document processing. The pitch: ship parsing, classification, extraction, and splitting pipelines in days (not months) with production‑grade accuracy.

What’s new
- Agentic optimization: agents “learn from your documents,” run experiments, and auto‑tune schemas to boost accuracy over time.
- Domain‑tuned vision models: built for messy, real‑world inputs—large tables, handwriting, checkboxes.
- Continuous learning + evals: a memory system to improve on similar files and an integrated evaluation suite to measure reliability.
- Flexible APIs and UIs: tools to build, deploy, and iterate on pipelines without heavy infra work.

Claims
- Accuracy “>99%” vs ~80% without Extend.
- Go‑live in days; reduced maintenance versus DIY model tuning/evals.
- Customer logos/testimonials (Brex, Flatiron, Vendr, Column Tax, Checkr, etc.) citing bakeoffs and removing humans‑in‑the‑loop for some workflows.

HN‑style caveats/questions
- “First AI agent” and “>99%” are big marketing claims—no public benchmarks, datasets, or costs shared.
- How robust is the continuous learning (data privacy, drift, failure modes)?
- Benchmark requests: table fidelity, handwriting coverage, latency/cost per page, auditability, and eval transparency.

Bottom line: a polished, agent‑driven take on document AI that emphasizes accuracy and time‑to‑production; proof will hinge on public metrics, pricing, and real‑world edge cases.

The Hacker News discussion about Extend's Composer highlights enthusiasm for its capabilities but raises questions about pricing, benchmarks, and competition:

### Key Points  
1. **Customer Use Cases**:  
   - Users highlight integrations with RAG workflows, real-time document processing (e.g., Brex’s checkout flows), and back-office automation.  
   - Extend emphasizes handling messy inputs (tables, handwriting) via domain-tuned vision models and OCR correction layers.  

2. **Pricing Concerns**:  
   - Some criticize the $300+/month starter plan as prohibitive for startups.  
   - Extend defends its pricing model, explaining trade-offs between "performance-optimized" (higher accuracy, higher cost) and "cost-optimized" modes. Credits are tied to processing complexity (e.g., classification vs. extraction).  

3. **Benchmarks & Accuracy**:  
   - Requests for public benchmarks (e.g., OmniDocBench) and transparency around latency, cost/page, and edge cases.  
   - Extend cites internal benchmarks and customer-specific evaluations but acknowledges results vary by document type.  

4. **Alternatives**:  
   - Users suggest cheaper/free tools (n8n, Gemini OCR, Datalab’s Surya) or open-source frameworks (Unstract, Unstructured.io).  
   - Extend argues competitors focus on niche tasks, while Composer offers end-to-end flexibility for unstructured docs (e.g., 500-page mortgage packages).  

5. **Competitive Landscape**:  
   - Mentions rivals like Trellis, Roe AI, and ng3n (Datalab’s Markdown converter).  
   - Extend claims the market is expanding rapidly, with demand for AI-driven document processing now spanning industries like healthcare and finance.  

### Skepticism & Open Questions  
- **Accuracy claims**: No public datasets or third-party validation of ">99% accuracy."  
- **Continuous learning**: Concerns about data privacy, model drift, and failure modes.  
- **Cost transparency**: Users seek clearer SLAs and pricing examples for large-scale deployments.  

### Bottom Line  
While Composer’s focus on reducing human-in-the-loop workflows resonates, skepticism persists around pricing and measurable performance. Extend’s success hinges on proving ROI against cheaper alternatives and addressing transparency gaps.

### Two things LLM coding agents are still bad at

#### [Submission URL](https://kix.dev/two-things-llm-coding-agents-are-still-bad-at/) | 332 points | by [kixpanganiban](https://news.ycombinator.com/user?id=kixpanganiban) | [362 comments](https://news.ycombinator.com/item?id=45523537)

A developer pinpoints two reasons LLM coding agents still feel “off” in real workflows. First, they don’t truly copy-paste code during refactors; they delete and re-emit from memory, which can subtly drift from the source. Codex occasionally tried to mimic copy-paste with sed/awk, but it’s brittle. Second, they rarely ask clarifying questions—plowing ahead on assumptions instead of pausing like a cautious human would. Even with prompts or frameworks like Roo that encourage Q&A, it’s inconsistent. The author suspects RL incentives favor speed over collaboration, leaving today’s agents feeling like overconfident interns rather than developer replacements.

The Hacker News discussion revolves around the limitations of LLMs (like ChatGPT, Claude, or Codex) in practical coding and research workflows, emphasizing their tendency to generate plausible-but-inaccurate outputs without proper grounding. Key points include:  

### 1. **Hallucinations and Subtle Errors**  
   - A user shared an example where an LLM refactored URLs in code but **silently altered critical path components**, leading to broken links (e.g., changing `cmths-rtcl-s-bt-fbr-123456` to `cmfbr-s-s-grt-162543`). These errors went unnoticed until deployment.  
   - Similarly, historical facts (e.g., John Howard’s election date) or technical details in documentation are often paraphrased inaccurately, resembling “frequency-based approximations” rather than verified truths.  

### 2. **Lack of Source Grounding**  
   - LLMs rarely cite or verify sources, even when generating answers based on external knowledge (e.g., GitHub comments or Wikipedia). Users noted frustration with responses that feel like “plausible summaries” rather than rooted in specific references.  
   - Tools like **NotebookLM** attempt to address this by linking answers to uploaded sources, but results are inconsistent.  

### 3. **Workflow Challenges**  
   - In code reviews, LLMs struggle to **detect moved or deleted code blocks**, often missing subtle errors (e.g., outdated comments or misaligned API calls). Suggestions included using `git diff --color-moved` to highlight code shifts.  
   - Users debated whether LLMs should act as **“interns”** (generating code quickly) vs. **“collaborators”** (pausing to ask clarifying questions).  

### 4. **Mitigation Strategies**  
   - **Prompt engineering**: Explicitly asking LLMs to “list sources” or “verify links” improves reliability marginally.  
   - **Human-in-the-loop**: Many stressed the need for human validation, especially for critical tasks (e.g., URL refactoring or historical research).  
   - **Specialized tools**: GitHub’s UI improvements for tracking code moves or tools like `fancy-diff` were highlighted as better alternatives for code-review workflows.  

### Conclusion  
While LLMs accelerate tasks like code generation or research, their outputs remain **probabilistic approximations** prone to silent failures. The discussion underscores the need for hybrid workflows—leveraging LLMs for speed while relying on human oversight, specialized tooling, and explicit source verification to catch errors.

### Customize Claude Code with plugins

#### [Submission URL](https://www.anthropic.com/news/claude-code-plugins) | 40 points | by [BrutalCoding](https://news.ycombinator.com/user?id=BrutalCoding) | [7 comments](https://news.ycombinator.com/item?id=45530150)

Claude Code adds plugins: a lightweight way to bundle and share custom slash commands, sub-agents, MCP tool connectors, and workflow hooks—installable with a single /plugin command.

Highlights
- What’s new: Plugins package any mix of slash commands (shortcuts), sub-agents (task‑specific agents), MCP servers (tool/data connectors via Model Context Protocol), and hooks (behavior tweaks at workflow checkpoints).
- Toggleable by design: Enable only when needed to keep context/prompt overhead low; disable to reduce complexity.
- Marketplaces: Anyone can host a curated catalog by publishing a .claude-plugin/marketplace.json in a repo or URL. Add one with “/plugin marketplace add user-or-org/repo-name,” then browse/install via the /plugin menu.
- Use cases: 
  - Enforce team standards (e.g., required hooks for reviews/tests)
  - Support users with package-specific slash commands
  - Share repeatable workflows (debugging, deploys, testing)
  - Connect internal tools/data through MCP with consistent config/security
  - Bundle framework- or domain-specific setups
- Examples: Community marketplaces from Dan Ávila (DevOps/docs/PM/testing) and Seth Hobson (80+ specialized sub-agents). Anthropic offers sample plugins for PR reviews, security guidance, Agent SDK workflows, and a meta‑plugin for creating new plugins.
- Getting started: Public beta, works in terminal and VS Code. Try: “/plugin marketplace add anthropics/claude-code” then “/plugin install feature-dev.”

Why it matters
- Turns prompt-heavy, one-off setups into shareable, reproducible AI dev environments.
- Gives teams an opinionated, auditable way to standardize AI-assisted workflows.
- MCP-based connectors plus org-hosted marketplaces hint at a broader ecosystem for enterprise-ready tooling.

The discussion around Claude Code's new plugin system highlights technical troubleshooting, community contributions, and a brief security concern:

1. **Technical Setup & Fixes**:
   - Users encountered SSH authentication failures when cloning repositories. BrutalCoding resolved this by switching to HTTPS URLs for GitHub access.
   - Improvements were noted in error messaging and session persistence to reduce setup friction.

2. **Community Contributions**:
   - Multiple users shared links to plugin marketplaces (e.g., `https://github.com/nnddtyg/cld-cd-mrktplc`) and encouraged collaboration via pull requests.
   - Success stories emerged, like `lcz` confirming a marketplace was added successfully.

3. **Security Warning**:
   - A nested comment warned about Chrome security risks (phishing/credential theft), but this appeared disconnected from the main plugin discussion and may have been misplaced or spam.

4. **Documentation & Resources**:
   - Links to Anthropic's official blog post and detailed plugin documentation were provided for troubleshooting and best practices.

The conversation reflects active experimentation with the plugin system, emphasis on resolving technical hurdles, and early community efforts to build shared resources. The off-topic security alert did not derail the core focus on setup and collaboration.

### What if the singularity lies beyond a plateau we cannot cross?

#### [Submission URL](http://www.jasonwillems.com/ai/2025/10/09/The-Plateau/) | 22 points | by [jayw_lead](https://news.ycombinator.com/user?id=jayw_lead) | [20 comments](https://news.ycombinator.com/item?id=45533152)

The piece argues that humanity’s historical superpower—outrunning problems via accelerating progress—may be ending. Post-ChatGPT anxieties fixate on runaway AI, but Jason Willems contends the likelier danger is a long plateau where progress crawls and existential threats compound.

What’s driving the stall:
- Macro bottlenecks: The frontier has shifted to grid-scale energy, advanced fabs, data centers, and infrastructure—domains constrained by regulation, capital intensity, and logistics more than code.
- S-curves everywhere: Many technologies sit on the flat part of their curves (cooling efficiency, solar performance, rocket thrust, transistor density), with physics-driven diminishing returns.
- Hard physical limits: Speed of light, thermodynamics, entropy—plus quantum effects undermining further transistor shrinkage—cap practical gains.
- Breakthrough uncertainty: Fusion, quantum computing, and superconductors might deliver step-changes—or never scale beyond niches—on timelines we can’t predict.
- Economic gravity: Progress is getting pricier. Cost per transistor is rising; next-gen colliders and mega-projects may be scientifically feasible but economically unjustifiable.
- Limited practical upside of some discoveries: Even deep math wins (e.g., Riemann) may leave most technology unchanged.

Why it matters:
- If acceleration stalls, we remain vulnerable to disease, resource limits, and a single-planet fate.
- Optimism built during a rare era of “faster and cheaper” may not generalize; future advances could depend on public funding and societal will more than private iteration.

HN angle:
- Reframes AI x-risk from “runaway” to “slowdown.”
- Provokes discussion on policy, permitting, and industrial strategy as the new levers of progress.

The Hacker News discussion on the submission "Beyond the Plateau: The real existential risk is a slowdown, not an AI takeoff" revolves around skepticism toward unchecked technological acceleration and debates whether progress is hitting fundamental limits. Key points include:

### **1. AI’s Limitations and Incremental Progress**  
- Participants questioned whether **LLMs** (like ChatGPT) represent meaningful advancement or mere optimization of existing systems. Some argued they primarily generate content or streamline tasks rather than enabling transformative breakthroughs.  
- **AGI/Singularity skepticism**: Many dismissed the "runaway AI" narrative, emphasizing physical, economic, and regulatory barriers. The "singularity" was likened to speculative fiction or eschatology, with doubts about recursive self-improvement surpassing hard limits (e.g., energy, materials).  

### **2. Physical and Economic Bottlenecks**  
- **Infrastructure challenges**: Building next-gen projects (e.g., 10,000 km particle colliders, space elevators) faces logistical and financial hurdles. Even incremental progress in areas like hardware (GPUs, storage) is constrained by replacement cycles and costs (e.g., AWS S3 scaling).  
- **Diminishing returns**: Moore’s Law slowdown, clock-speed plateaus, and S-curve stagnation in solar efficiency, rocketry, and materials science were cited as evidence of physics-driven limits.  

### **3. Historical Context and Step-Changes**  
- **Past vs. present**: Commenters noted that historical leaps (e.g., steam power, industrial revolution) required massive investments, but today’s regulatory and economic environments stifle similar ambition.  
- **Step-function hopes**: Some pinned hopes on AGI, quantum computing, or biotech (e.g., Neuralink, gene editing), but others argued these face their own diminishing returns or uncertain timelines.  

### **4. Industry Maturity and Regulation**  
- **Tech industry parallels**: Comparisons were drawn to mature fields like plumbing, suggesting software engineering may soon face stricter regulation and slower growth as it stabilizes.  
- **Scalability concerns**: Cloud infrastructure (e.g., AWS S3’s exploding data volumes) and hardware sustainability (hard-drive replacement cycles) highlight looming scalability crises.  

### **5. Cultural and Cognitive Constraints**  
- **Symbolic systems as bottlenecks**: One user argued that AI’s reliance on arbitrary symbols (language, logic) reflects human cognitive limits, creating an "Achilles’ heel" for progress.  
- **Mediocrity as default**: A bleak take suggested that "mediocrity wins" in systems prioritizing speed over depth, with symbolic paradigms (e.g., binary logic) stifling true innovation.  

### **Conclusion**  
The discussion broadly aligns with the article’s thesis: runaway AI is less likely than a grinding slowdown due to physics, economics, and institutional inertia. While some held out hope for step-changes, most emphasized incremental gains and the growing difficulty of outrunning compounding risks (climate, resource scarcity). The tone leaned pragmatic, stressing the need for policy shifts and industrial strategy over techno-optimism.

### **Tone**  
The thread blends **nostalgia**, **skepticism**, and **amusement**, with lighthearted jabs at generational divides and market-driven fads. While some dismiss Labubu as ephemeral, others appreciate its role in modern pop culture.

---

## AI Submissions for Wed Oct 08 2025 {{ 'date': '2025-10-08T17:15:23.031Z' }}

### I played 1k hands of online poker and built a web app with Cursor AI

#### [Submission URL](https://blog.rchase.com/i-played-1-000-hands-of-online-poker-and-built-a-web-app-with-cursor-ai/) | 103 points | by [reillychase](https://news.ycombinator.com/user?id=reillychase) | [151 comments](https://news.ycombinator.com/item?id=45520154)

A founder’s two-week dive into low-stakes poker turned into a surprising case study in AI-assisted development. After tooling around with Python scripts to export PokerStars hands, he asked Grok—and then Cursor—to help, and in 2–3 days shipped a full Laravel poker stats web app without “writing a single line” himself. The app includes a 700+ line hand-history parser, VPIP/PFR/3-bet stats, journaling and bankroll logs, multi-file uploads and paste-import, scheduled PokerStars exports every 15 minutes, Gmail IMAP ingestion for PINs/hand histories, daily balance checks, a hand viewer, and P/L charts. Built locally with Herd and deployed to DigitalOcean, it lives at poker.rchase.com (he even shares his last 1,000 hands). The post doubles as a poker-learning reflection—emotional control, risk management, process over outcomes—and as a marker of how far AI coding agents (Grok, Cursor, Claude) have come from “vibe coding” demos to shipping integrations and refactors. Expect discussion around reliability, security, maintainability, and what “not writing any code” really means in an AI-as-developer workflow.

The Hacker News discussion revolves around several key themes:

1. **Online Poker Integrity**:  
   - Many users argue that **collusion and bots** plague online poker, especially at lower stakes. Some claim regulated sites (like Michigan-based platforms) are better at verification, but skepticism remains about detecting sophisticated bots or coordinated cheating.  
   - Others counter that **most players simply play poorly**, likening their strategies to "blackjack" or "bingo," making bot dominance overstated.  

2. **Bot Realities**:  
   - Debates erupt over whether bots are a **widespread threat** or a niche issue. Some insist bots destroyed mid-stakes games years ago, while others (like self-described winning players) downplay their current impact, emphasizing that exploiting human errors remains more profitable.  

3. **Game Theory Optimal (GTO) Impact**:  
   - GTO solvers have **raised the skill ceiling**, making games tougher. Critics argue solvers lead to robotic play and over-reliance on preflop charts, while proponents highlight their utility for studying complex postflop scenarios and adjusting to opponents.  

4. **Live vs. Online Dynamics**:  
   - Live poker (e.g., Las Vegas cash games) is seen as softer but dominated by **regulars** who exploit recreational players. Online play is viewed as more technically demanding but rife with trust issues.  

5. **AI Development Claims**:  
   - The submission’s “no code” assertion is questioned. Users debate whether AI tools like Cursor/Grok **accelerate development** versus truly replacing coding, with concerns about maintainability and security in AI-generated code.  

6. **Regulation and Trust**:  
   - Skepticism persists about online platforms’ ability to prevent collusion or verify identities, despite claims of improved regulation. Some suggest **IP tracking** and behavioral analysis as countermeasures.  

**Takeaway**: The discussion reflects a mix of poker-strategy pragmatism, skepticism toward online platforms, and cautious optimism about AI’s role in coding—while underscoring that poker’s human element (and its flaws) remains central.

### Show HN: FleetCode – Open-source UI for running multiple coding agents

#### [Submission URL](https://github.com/built-by-as/FleetCode) | 90 points | by [asdev](https://news.ycombinator.com/user?id=asdev) | [49 comments](https://news.ycombinator.com/item?id=45518861)

FleetCode: a lightweight desktop terminal that lets you run multiple CLI coding agents (Claude Code, Codex) side by side—each in its own isolated git worktree—so runs don’t step on each other and can be resumed later.

Highlights
- Parallel sessions: spin up multiple agent terminals at once, compare outputs, or split tasks across sessions.
- Git worktree isolation: every session gets its own worktree off a chosen parent branch; cleanup on close.
- Persistent state: sessions are saved and auto-resume (--resume <uuid>), with first-run support (--session-id <uuid>).
- Extensible via MCP: configure Model Context Protocol servers (stdio or SSE) to give agents extra tools/context.
- Terminal controls: preset themes (macOS Light/Dark, Solarized Dark, Dracula, One Dark, GitHub Dark), font/size, cursor blink; rename/close/delete sessions.
- Setup commands: run shell commands before the agent starts (env, sourcing files, etc.).

Getting started
- Prereqs: Node.js 16+, Git, and the agent CLI you’ll use (e.g., @anthropic-ai/claude-cli).
- Install: npm install
- Dev: npm run dev
- Build/Run: npm run build && npm start

Troubleshooting
- macOS “unidentified developer”: xattr -cr /path/to/FleetCode.app
- Claude Code reading the wrong directory: disable “Auto connect to IDE” (claude config → set autoConnectToIde to false).

Project notes
- TypeScript-heavy codebase; ISC license.
- Latest release: 1.0.1-beta.6 (Oct 8, 2025).
- Repo: built-by-as/FleetCode (≈207⭐, 8 forks).

Here's a concise summary of the discussion:

**Key Themes**
1. **Comparisons & Alternatives**  
   - Users mention similar tools like GitButler, Crystal, DevSwarm, and Container (Dagger) for parallel coding workflows.  
   - Git worktree-based isolation is compared to container-based approaches, with debates about complexity vs. safety.  
   - Some prefer existing terminal multiplexers vs FleetCode's specialized UI.

2. **Workflow Concerns**  
   - Questions about dependency management (CLI agents modifying system environments).  
   - Interest in IDE/Jira integrations for task tracking.  
   - Praise for persistent sessions but concerns about UI complexity vs CLI purity.

3. **Technical Implementation**  
   - macOS theme support noted as a strength.  
   - MCP protocol extensibility sparks discussion about agent customization.  
   - Questions about FleetCode's differentiation from other Git worktree tools.

4. **Community Feedback**  
   - Multiple users reference DevSwarm as a comparable project focusing on "swarm" coding collaboration.  
   - Some confusion with JetBrains Fleet IDE due to naming similarity.  
   - Requests for demo videos/documentation to clarify workflow.

**Notable Points**
- Users highlight GitButler's branch management as a complementary/competing workflow tool.  
- Debate about whether containers add unnecessary overhead for local dev vs Git worktree isolation.  
- Several contributors share their own related projects (e.g., CueIt Kanban agents, prctlkm TUI implementation).  
- Mixed reactions to terminal UI vs pure CLI approaches for AI agent management.

### Now open for building: Introducing Gemini CLI extensions

#### [Submission URL](https://blog.google/technology/developers/gemini-cli-extensions/) | 154 points | by [meetpateltech](https://news.ycombinator.com/user?id=meetpateltech) | [40 comments](https://news.ycombinator.com/item?id=45516426)

Google is opening an extensions ecosystem for Gemini CLI, its open-source, AI agent for the terminal. Extensions are installable integrations that connect Gemini to external tools and services, aiming to cut context-switching by letting the AI operate your stack from the command line.

Highlights
- What it is: A framework for “extensions” that add capabilities to Gemini CLI via pre-packaged playbooks, so the AI knows how to use new tools immediately.
- One-liner install: gemini extensions install <GitHub URL or local path>.
- Ecosystem: Launch partners include Dynatrace, Elastic, Figma, Harness, Postman, Shopify, Snyk and Stripe, alongside Google-built and community extensions. A new directory lists extensions and ranks them by GitHub stars.
- Under the hood: Extensions can bundle Model Context Protocol (MCP) servers, context files (e.g., GEMINI.md), excluded/overridden tools, and custom slash commands. The AI consults the extension’s playbook and your local context (files, git status) to decide which tool to run.
- Google-made add-ons: Cloud Run (deploy from local to public URL), GKE management, gcloud integration, and Cloud Observability.
- Adoption stat: Google says over 1M developers have used Gemini CLI in its first three months.

Why it matters
- Brings “agentic” workflows to the terminal in an open, extensible way, letting developers wire AI into real tooling without bespoke setup.
- Moves beyond raw MCP hookups by packaging usage guidance and commands, which should reduce friction and produce useful results from the first run.

Getting started
- Install extensions directly from GitHub or a local path.
- Browse the new Extensions page to discover community, partner, and Google-built options.

Things to watch
- Security and trust: Review extension source, requested permissions, and what commands it can run, especially given access to local files and git state.
- Reliability: How well playbooks generalize across varied project setups will determine real-world utility.

The Hacker News discussion on Google's Gemini CLI extensions reveals a mix of cautious interest, technical critiques, and comparisons with competitors like Claude. Key points include:

1. **Technical Feedback & Comparisons**:
   - Users reported mixed experiences with Gemini CLI, with some finding it less reliable than Claude for complex tasks (e.g., code migration). Critics highlighted issues with context handling, error-prone outputs, and a lack of depth in documentation.
   - Claude was praised for better workflow support (e.g., subagents, nested planning) and handling of coding tasks, though Gemini was noted as cheaper and sufficient for simpler use cases.

2. **Security Concerns**:
   - Multiple comments stressed the importance of auditing extension code due to security risks, especially given CLI access to local files and systems. A community fork ([gemini-cl-security](https://github.com/QuanZhang-William/gemini-cl-security)) was mentioned as an effort to address these concerns.

3. **Integration Quirks**:
   - The Figma extension drew confusion, with users questioning how a CLI tool integrates with a design platform. Some speculated it might read design files to assist coding, but others found the implementation limited (e.g., read-only MCP, minimal utility).

4. **Usability Critiques**:
   - Critics called Gemini CLI’s interface overwhelming, citing fragmented design and poor integration with existing tools. Complaints included flashing screens, token limits, and a lack of IDE-like context awareness.

5. **Pricing & Value**:
   - Gemini’s cost-effectiveness (10x cheaper tokens) was contrasted with Claude’s higher price but superior functionality. Some users preferred Gemini for basic tasks but acknowledged Claude’s edge in complex scenarios.

6. **Skepticism About AI Agents**:
   - Doubts were raised about the practicality of AI in CLI workflows, with users noting limitations in handling nuanced tasks and reliance on rigid playbooks.

Overall, the discussion reflects cautious optimism about Gemini CLI’s potential but underscores concerns around reliability, security, and integration depth compared to established alternatives.

### Legal Contracts Built for AI Agents

#### [Submission URL](https://paid.ai/blog/ai-agents/paid-gitlaw-introducing-legal-contracts-built-for-ai-agents) | 70 points | by [arnon](https://news.ycombinator.com/user?id=arnon) | [42 comments](https://news.ycombinator.com/item?id=45515640)

Paid has teamed up with GitLaw to release an open-source “Agentic MSA” — a Master Services Agreement tailored for AI agents rather than traditional SaaS. The pitch: most AI agent startups are still using SaaS contracts built for passive tools, which breaks down when agents act autonomously, continuously, and adapt over time.

What’s broken with SaaS contracts for agents
- Agents make decisions without human approval, run 24/7, and evolve — creating gaps in liability and expectations. Think the Ford dealership chatbot “free truck” fiasco.
- You can’t price outcomes or protect margins if your contract assumes predictable, seat-based software.

What the Agentic MSA covers
- Agent classification and responsibility: Clarifies the agent is a tool; customers set parameters and retain oversight. Section 1.2 limits vendor liability for autonomous decisions.
- Liability and risk: Disclaimers that outputs require human verification, plus damage caps (e.g., 12 months’ fees) and AI-specific accuracy language. Sections 7 and 4.1.
- Data and training: Customers own their data and outputs; optional, clearly-described use of de-identified, aggregated data for model improvement with opt-outs. Section 2.1 and cover-page variables.

Why it matters
- Aligns legal terms with how agents actually operate, enabling outcome-based pricing and clearer risk allocation. As Paid’s CEO puts it: you can’t bill for outcomes using seat-based SaaS terms.

How to use it
- It’s free and open source via the GitLaw Community, with an AI agent to generate customized versions. It builds on CommonPaper’s Software Licensing Agreement and AI Addendum. The teams stress it’s a starting point; work with counsel to tailor it as laws evolve.

The discussion revolves around the challenges of assigning liability and responsibility when AI agents operate under traditional SaaS contracts, highlighting key debates and considerations:

1. **Liability Allocation**:  
   - Participants argue that existing SaaS contracts fail to address autonomous AI behavior, where decisions aren't human-approved. Comparisons are made to industries like construction, where liability is clearly assigned (e.g., contractors vs. suppliers).  
   - **Examples**: If an AI deletes a production database, should the vendor (like OpenAI) or the end-user bear responsibility? Contrasts are drawn to traditional services (e.g., AWS deleting a database vs. a hosted AI agent doing so).  

2. **Contract Limitations**:  
   - Traditional contracts assume deterministic, static software, not adaptive AI. Some suggest explicit disclaimers (akin to MIT licenses) to limit liability, though others counter that vendors like OpenAI may still face accountability for harmful outputs.  

3. **Data and Model Evolution**:  
   - Concerns arise about AI models evolving post-deployment, altering behavior unpredictably. Contracts may need clauses addressing updates, frozen model versions, or user consent for changes.  

4. **Legal Frameworks**:  
   - References to the **Law of Agency** (for human agents) highlight the mismatch with AI autonomy. Participants debate whether AI agents should be treated as tools (user-liable) or independent actors (vendor-liable).  

5. **Practical Challenges**:  
   - Maintaining contracts as AI evolves is seen as impractical. Suggestions include outcome-based pricing, insurance models (like gambling/financial industries), or clear terms on component failures (e.g., faulty third-party APIs).  

6. **Industry Examples**:  
   - Incidents like Ford’s chatbot offering free trucks or Replit’s AI deleting databases illustrate real-world gaps. Participants stress the need for explicit language around human oversight and damage caps (e.g., 12 months’ fees).  

**Key Takeaway**: The Agentic MSA is a starting point, but legal frameworks must evolve to address AI’s unique risks—autonomy, adaptability, and indirect control—while balancing vendor protection and user responsibility. Collaboration between legal and technical teams is critical as laws and AI capabilities advance.

### OpenAI Apps SDK: The New Browser Moment

#### [Submission URL](https://www.nuefunnel.com/blog/openai-apps-sdk-the-new-browser-moment) | 17 points | by [sidhusmart](https://news.ycombinator.com/user?id=sidhusmart) | [4 comments](https://news.ycombinator.com/item?id=45518778)

TL;DR
The Apps SDK frames ChatGPT as a universal action interface: instead of sending you to sites, it fulfills intents inside the chat by calling third‑party apps. That shifts the web from routing attention to executing tasks—and puts OpenAI in the platform seat.

Key ideas
- Assistant > browser: Queries like “find me a holiday apartment…” don’t return links; they trigger integrated apps (e.g., booking) to perform the task in-line. Search routes less; agents execute more.
- Not just automation: Unlike “computer use” approaches, Apps SDK is an official integration layer—closer to a platform than a macro runner.
- Siri, but unbounded: Apple’s App Intents hinted at this, but were siloed, brittle, and phrase-dependent. LLMs make intent parsing flexible, cross‑platform, and open to any developer—if OpenAI can solve app discovery and quality at scale.
- Who wins: Transactional apps (commerce, travel, productivity, payments) gain a new distribution channel with retained transactions. Who loses: Attention/ads businesses (news, social, portals) risk disintermediation as summaries and answers happen in‑chat.
- The execution economy: Value migrates from information to action. Knowledge becomes “free” (synthesized instantly), execution becomes premium (bookings, purchases), trust decides which app gets invoked.
- Platform gravity and risk: If ChatGPT becomes the default interface, OpenAI controls discovery, economics, and data access—echoing Google Search and the App Store. Expect innovation pull and regulatory push; developers will optimize for ChatGPT rankings.
- Competitive context: Similar ambitions from Perplexity’s Comet and The Browser Company’s Dia, but Apps SDK is a first‑party, “blessed” app layer rather than browser automation.

Why it matters
- For developers: New demand channel, but platform dependency and ranking dynamics loom. Build for intents, not pages.
- For publishers: Summaries siphon attention; without control of the surface, ad models weaken. Monetization may hinge on paid execution or partnerships.
- For users: Fewer tabs, more outcomes—“talk to do.” The assistant becomes the operating system for the web’s next layer.

Open questions
- Discovery and governance: How will ChatGPT rank apps, handle conflicts, and enforce quality/security?
- Economics: Beyond pure transactions, what rev‑share or payout models emerge for non‑transactional utilities?
- Interop and lock‑in: Will there be neutral standards for agent/app intents, or a new era of platform silos?

Takeaway
If the browser made the web clickable and the smartphone made it touchable, assistants are making it actionable. The Apps SDK is positioned as that step—shifting the web from reading to doing, and moving the center of gravity from attention to execution.

**Summary of Discussion:**

The discussion revolves around the potential significance of OpenAI's Apps SDK, drawing comparisons to historical tech milestones and expressing both optimism and skepticism:

1. **Historical Comparisons & Skepticism:**
   - A user compares OpenAI's current phase to pivotal moments like Steve Jobs' Apple, the App Store, and the browser revolution, suggesting the SDK could redefine app interaction. However, skepticism is raised about overhyping the technology, noting past innovations (e.g., "terminal moments" or early LLMs) that failed to meet transformative expectations.

2. **Analyst Frameworks vs. Hype:**
   - One commenter critiques the reliance on buzz, advocating instead for established analyst frameworks (Forrester, Gartner) to evaluate progress, implying that structured analysis trumps speculative hype in understanding OpenAI’s advancements.

3. **New Delivery Models & the "Super App" Angle:**
   - Another user likens the SDK’s potential to the World Wide Web (WWW), suggesting it could unlock novel service delivery principles. A cryptic reply hints at further exploration ("dd"). 
   - A separate comment speculates about OpenAI’s strategy to position ChatGPT as a U.S.-centric "super app" (à la WeChat), aiming to survive competitive markets by consolidating multiple services.

**Key Themes:**
- **Optimism:** The SDK is framed as a paradigm shift, enabling action-oriented interfaces and new distribution models.
- **Skepticism:** Past tech hypes (e.g., underdelivering LLMs) serve as cautionary tales.
- **Market Strategy:** OpenAI’s move could be part of a broader play to dominate via platform control or super-app status.

**Open Questions:**
- Will the SDK truly replicate the impact of the App Store or browsers?
- Can OpenAI avoid past pitfalls of overhyped tech while navigating platform governance and competition?

### Expanding access to Opal, our no-code AI mini-app builder

#### [Submission URL](https://blog.google/technology/google-labs/opal-expansion/) | 38 points | by [simonpure](https://news.ycombinator.com/user?id=simonpure) | [26 comments](https://news.ycombinator.com/item?id=45519944)

Google is taking Opal, its no-code AI mini‑app builder, global—and giving it a real debugger.

What’s new
- Expansion: Rolling out beyond the U.S. to 15 countries: Canada, India, Japan, South Korea, Vietnam, Indonesia, Brazil, Singapore, Colombia, El Salvador, Costa Rica, Panamá, Honduras, Argentina, and Pakistan.
- Advanced no‑code debugging: Step‑by‑step workflow execution in a visual editor, ability to iterate on a specific step in a console panel, and real‑time errors pinned to the exact failing step.
- Performance boost: Faster project creation (previously could take ~5 seconds) and parallel runs so multi‑step workflows can execute simultaneously to cut wait times.

Why it matters
- Opal lets people build AI‑powered mini‑apps using natural language—no coding. Early users built more sophisticated tools than expected, so Google is shoring up reliability (debugging) and throughput (parallelism) to handle more complex, production‑like workflows.

Availability
- Access via opal.withgoogle.com; Google is also promoting a Discord builder community.

Bottom line: Google is turning a neat Labs experiment into a more serious no‑code platform by adding transparent debugging and concurrency, while expanding access to a wider creator base.

**Summary of Discussion on Google's Opal Expansion:**

1. **Product Skepticism and Quality Concerns:**  
   - Many users expressed doubts about Google’s track record of discontinuing products (*“shiny promotions…software retirement village”*).  
   - Criticisms of AI-generated content quality (*“garbage AI-generated blog posts”*) and skepticism about Opal’s reliability for complex workflows.  

2. **Comparisons to Competitors:**  
   - Microsoft’s past low-code tools (e.g., Power Apps) were cited as cautionary examples.  
   - Opal was unfavorably compared to existing platforms like **n8n**, **Node-RED**, **Zapier**, and visual tools like Unreal Engine’s Blueprints.  

3. **No-Code Debate:**  
   - Mixed opinions on no-code viability: Critics argued it struggles with complex logic (*“block-box work”*), while supporters noted success in niche domains (e.g., **Wix**, **Shopify**).  
   - Visual programming’s limitations were highlighted (*“hard to express state machines in code-like fashion”*).  

4. **Regulatory and Market Dynamics:**  
   - EU users lamented exclusion from Opal’s rollout, sparking debates on tech regulation. Some argued regulations stifle innovation, while others blamed market dynamics (e.g., dominance of firms like Google).  

5. **Meta Criticism:**  
   - A flagged comment chain questioned the discussion’s quality, pointing to **HN guidelines** on substantive discourse.  

**Key Takeaway:** The discussion reflects broader skepticism about Google’s product longevity and AI’s role in no-code tools, alongside debates on no-code’s limitations versus its niche successes. Concerns about regulatory fragmentation (e.g., EU vs. global markets) also surfaced.

### AI gets more 'meh' as you get to know it better

#### [Submission URL](https://www.theregister.com/2025/10/08/more_researchers_use_ai_few_confident/) | 77 points | by [rntn](https://news.ycombinator.com/user?id=rntn) | [33 comments](https://news.ycombinator.com/item?id=45519161)

AI’s honeymoon with researchers may be over, says Wiley’s new “ExplanAItions 2025” survey preview

- Adoption up, confidence down: 84% of 2,430 researchers now use AI (was 57% in 2024), but those who think AI already outperforms humans in tested tasks fell from 53% to under a third. Early adopters remain rosier (59% of use cases).
- Concerns rising: worries about inaccuracies/hallucinations jumped to 64% (from 51%), security/privacy to 58% (from 47%); ethics and transparency concerns also ticked up to around/above 50%.
- Use cases are mostly “word work”: writing assistance, documentation, and literature review dominate. Writing aid saw the biggest uptick and is the only use tried by over half of respondents.
- Mixed impact on work: 85% say AI makes them more efficient; 77% produce more; 73% see quality gains; 70% use it for brainstorming—but only 48% say it helps them think critically.
- Outlook and support: 83% expect AI to be widespread in research by 2027, yet many report a lack of institutional backing (57%).
- Caveats and context: 2025’s sample was ~half the size of 2024’s (possible self-selection). Other studies cited paint a sobering picture: CMU found AI agents fail ~70% of tasks; MIT reports only ~5% of GenAI pilots show measurable ROI.

Takeaway: Researchers are using AI more, but the closer they look, the more “meh” it seems—great for drafting and sifting literature, less convincing for complex, high-stakes research work.

**Summary of Discussion:**

The discussion reflects a nuanced and often skeptical view of AI's current capabilities and societal impact, highlighting both practical applications and significant concerns:

1. **Utility vs. Overhype**:  
   - Participants acknowledge AI's usefulness in tasks like coding assistance ("coding AI works fantastically") and content generation, but many criticize its limitations. Examples include poor code quality ("code shockingly bad"), superficial outputs, and reliance on AI leading to reduced critical thinking and ownership in software development.  
   - Some argue AI tools encourage laziness, with developers treating generated code as "correct" without proper review, risking technical debt and quality issues.

2. **Ethical and Social Concerns**:  
   - **Sycophancy and EQ**: Non-technical users are noted to overly trust AI (e.g., ChatGPT), leading to "drowned" interactions and frustration. Some describe AI outputs as overly agreeable ("sycophantic"), mimicking high emotional intelligence (EQ) but lacking depth.  
   - **Misinformation**: Concerns about AI amplifying social media's role in spreading "bullshit" and reducing factual accuracy ("world spell loss") are raised, with systems prioritizing engagement over truth.

3. **Technical Limitations**:  
   - **Hallucinations & Inconsistency**: Users report AI-generated nonsense in search results and documentation, complicating knowledge extraction.  
   - **Stagnation vs. Progress**: Debate exists over whether AI advancements (e.g., image generation fidelity, reasoning) are plateauing ("stagnated class 1-12 years") or improving ("massively improved"). Skeptics doubt near-term "leaps," while others cite cheaper, faster models as progress.

4. **Industry Realities**:  
   - **Workflow Challenges**: AI coding tools are seen as double-edged—speeding up tasks but undermining code ownership and quality assurance. Some fear a future where AI-generated code dominates without proper oversight.  
   - **Economic Barriers**: High costs of training advanced models and resource limitations ("can't afford resolution") are noted, alongside architectural trade-offs.

5. **Cultural Sentiment**:  
   - **Disillusionment**: Many express boredom or frustration with AI's current state, labeling the phase a "trough of disillusionment." Others humorously reference AI's unpredictability (e.g., "Nano Banana" randomly altering outputs).  
   - **Human Condition**: A meta-commentary questions whether grappling with AI's flaws is simply part of the "human condition," with one user linking to a video metaphorically depicting future disillusionment.

**Takeaway**: While AI is increasingly integrated into workflows, the discussion underscores a cautious pragmatism. Users recognize its potential but emphasize its current shortcomings—ethical, technical, and practical—warning against over-reliance and urging tempered expectations.

---

## AI Submissions for Tue Oct 07 2025 {{ 'date': '2025-10-07T17:16:03.301Z' }}

### Gemini 2.5 Computer Use model

#### [Submission URL](https://blog.google/technology/google-deepmind/gemini-computer-use-model/) | 582 points | by [mfiguiere](https://news.ycombinator.com/user?id=mfiguiere) | [292 comments](https://news.ycombinator.com/item?id=45507936)

Google DeepMind released Gemini 2.5 Computer Use, a specialized variant of Gemini 2.5 Pro that can operate UIs by “clicking, typing, and scrolling” via the Gemini API. It’s aimed at building agents that handle unstructured, GUI-only workflows (forms, dropdowns, behind logins) rather than just calling APIs, and is available in preview in Google AI Studio and Vertex AI.

Key details:
- How it works: A new computer_use tool runs in a loop. You send a user request, a screenshot, and recent action history; the model returns a function call (e.g., click, type, scroll) or asks for user confirmation on risky steps (like purchases). Your client executes it, then returns a fresh screenshot/URL to continue until done.
- Capabilities: Optimized for web browser control; shows promise on mobile UI control; not yet tuned for desktop OS-level automation. Supports excluding certain actions and adding custom functions.
- Performance: Google reports it outperforms leading alternatives on multiple web/mobile control benchmarks, including Browserbase’s Online-Mind2Web harness, with lower latency. Results combine self-reported and third-party evaluations.
- Safety: Trained-in safeguards plus developer controls. An out-of-model per-step safety service vets each proposed action. System instructions let you require refusal or user confirmation for high-stakes actions. Restrictions include things like CAPTCHA bypassing, system integrity compromise, or controlling medical devices.
- Demos: Example tasks include migrating data between sites (with auth) and reorganizing a sticky-note board via drag-and-drop.

Why it matters: It pushes agentic UI control into mainstream developer tooling, competing with similar “computer use” efforts, with an emphasis on low-latency browser automation and layered safety guardrails.

**Summary of Discussion:**

1. **Technical Comparisons & Automation Tools:**
   - Users discuss using Gemini with Chrome DevTools Protocol (MCP) for browser automation, contrasting it with tools like Playwright and Puppeteer. Some note MCP’s lower-level control but mention Playwright’s speed and simplicity for scripting.
   - Herd’s Trail system is highlighted as an alternative for browser automation, emphasizing distributed orchestration and REST-like APIs. Users debate whether Gemini’s approach offers novel advantages over existing frameworks.

2. **Wordle Example & Capabilities:**
   - A user successfully solved Wordle using Gemini, leveraging screenshots and grayscale analysis. However, others note inconsistencies, with Gemini sometimes failing to interpret feedback (green/yellow/gray letters) correctly.
   - ChatGPT’s struggles with Wordle are mentioned, including forgotten rules and reliance on keyboard layout mappings (e.g., Dvorak vs. QWERTY) to decode passwords, sparking debate about LLMs’ reliability for precise tasks.

3. **CAPTCHA & Security Concerns:**
   - Gemini’s ability to bypass CAPTCHAs is demonstrated (e.g., Google’s reCAPTCHA demo), raising ethical questions. Browserbase’s CAPTCHA-solving service is speculated to use low-cost human labor.
   - A security lapse in a demo video accidentally exposed sensitive strings (possibly API keys/passwords) in browser history, prompting warnings about data leakage risks during automation.

4. **Community Reactions:**
   - Mixed sentiments: Some praise Gemini’s low-latency performance and potential for complex workflows (e.g., drag-and-drop UI reorganization), while others criticize its occasional unreliability and “lazy” execution in resource-intensive tasks.
   - Humorous skepticism emerges around AI “solving” Wordle, with users joking about grayscale analysis being a “boring superpower.”

**Key Takeaways:**  
The discussion reflects cautious optimism about Gemini’s UI automation potential but underscores challenges in consistency, security, and ethical use. Comparisons to existing tools highlight trade-offs between flexibility and ease of use, while real-world demos reveal both promise and pitfalls.

### Less is more: Recursive reasoning with tiny networks

#### [Submission URL](https://alexiajm.github.io/2025/09/29/tiny_recursive_models.html) | 282 points | by [guybedo](https://news.ycombinator.com/user?id=guybedo) | [58 comments](https://news.ycombinator.com/item?id=45506268)

- Tiny Recursion Model (TRM): a 7M-parameter, from-scratch model that iteratively refines its own answers via a simple recursion loop, reporting 45% on ARC-AGI-1 and 8% on ARC-AGI-2.
- Core idea: “less is more.” Instead of scaling LLMs, TRM keeps a latent state z and, for each step, updates z multiple times from (x, y, z) before updating the answer y—allowing it to fix its own mistakes over K improvement steps.
- Positioning: Inspired by HRM (which hit ~40% on ARC-AGI-1) but claims a simpler design—no brain analogies, no fixed-point theorems, no hierarchy—while improving accuracy with far fewer parameters.
- Pitch: parameter-efficient recursive reasoning that aims to reduce overfitting and cost, challenging the notion that only massive foundation models can handle hard reasoning tasks.
- Links: Paper | Code

**Summary of Discussion:**

1. **Evaluation & Generalization Concerns**:  
   - Skepticism arises around HRM/TRM's evaluation on ARC-AGI benchmarks. Critics argue that training on small, specialized datasets risks overfitting, questioning whether the models truly generalize or exploit training-test data overlap.  
   - Comparisons to LLMs are deemed unfair, as LLMs are tested on broader datasets. The discussion emphasizes the need for strict separation of training and test data to validate claims of reasoning ability.  

2. **Technical Comparisons**:  
   - **Recursion vs. Transformers**: TRM’s recursion is likened to Infinite Impulse Response (IIR) filters, enabling iterative refinement with fewer parameters, while traditional Transformers resemble Finite Impulse Response (FIR) filters.  
   - **Deep Equilibrium Models (DEQs)**: Mentioned as a related approach, DEQs achieve recursion-equivalent results via equilibrium points in a single layer, offering efficiency but facing scalability challenges.  

3. **Related Work**:  
   - Multiple papers on recursive architectures (e.g., Universal Transformers, Fixed Point Diffusion Models) are cited, showing prior exploration of iterative refinement. TRM’s approach parallels Chain-of-Thought reasoning but formalizes recursion explicitly.  

4. **Practicality vs. Hype**:  
   - While TRM’s simplicity and parameter efficiency are praised, critics caution against overhyping. HRM’s limited applicability beyond niche tasks (e.g., spatial reasoning in ARC-AGI) is noted, urging broader testing.  
   - A user’s replication of HRM suggests latent reasoning, not architecture, drives performance, challenging TRM’s novelty.  

5. **Community Call to Action**:  
   - Emphasis on replication, rigorous benchmarking, and exploring whether recursion’s benefits extend to real-world tasks. The debate underscores the need for transparent evaluation to distinguish true reasoning from data exploitation.  

**Key Takeaway**: TRM’s iterative refinement is a promising direction for efficient reasoning, but skepticism about evaluation methodologies and practical utility persists. The discussion highlights the community’s demand for robustness over hype, advocating for careful validation against diverse benchmarks.

### Launch HN: LlamaFarm (YC W22) – Open-source framework for distributed AI

#### [Submission URL](https://github.com/llama-farm/llamafarm) | 100 points | by [mhamann](https://news.ycombinator.com/user?id=mhamann) | [53 comments](https://news.ycombinator.com/item?id=45504388)

LlamaFarm: build RAG- and agent-driven apps locally in minutes, swap backends later

What it is
- An Apache-2.0, local-first framework with a single CLI (lf) to spin up chat, RAG pipelines, agents, datasets, and a production-style server. Opinionated defaults (Ollama + Chroma) but fully extensible to vLLM, OpenAI-compatible hosts, other vector stores, parsers, and embedders.

Why it stands out
- Config over code: projects are defined by YAML schemas validated at runtime; swap runtimes/stores without rewriting your app.
- OpenAI-compatible REST API at localhost:8000, so existing clients can point at it.
- Production parity: dev stack mirrors server endpoints; easy to version-control and deploy.
- Composable RAG: choose parsers, extractors, embeddings, DBs via config instead of bespoke orchestration.

Notable features
- CLI workflows: init, start (Docker services + dev chat UI), interactive chat TUI, one-off prompts, cURL preview, dataset create/upload/process, and semantic RAG queries with filters.
- Agents and pipelines out of the box; extendable backends and tooling.
- Runs locally today with Ollama; supports switching to hosted vLLM/Together or custom OpenAI-compatible APIs.

Getting started
- Requirements: Docker and Ollama (Windows CLI via winget). Tip: bump Ollama’s context window for long docs.
- Typical flow: lf init → lf start → lf chat; manage datasets and RAG via lf datasets ... and lf rag query ...

Caveats
- Local runtime alternatives beyond Ollama are still “coming soon”; defaults to Chroma; Docker required.

Links
- GitHub: https://github.com/llama-farm/llamafarm
- Site: https://llamafarm.dev
- 90s demo: https://youtu.be/W7MHGyN0MdQ

**Hacker News Discussion Summary: LlamaFarm Launch**

**Key Themes & Insights:**

1. **Healthcare & Sensitive Data Applications**  
   - Users highlighted LlamaFarm's potential in healthcare for handling PHI (Protected Health Information) via local RAG pipelines, enabling context-aware AI assistants without relying on cloud services. Integration with EHR systems like Epic and use in small practices were noted as promising.

2. **Decentralization & Local AI**  
   - Strong support for LlamaFarm’s local-first approach, emphasizing sovereignty over data and reducing reliance on centralized cloud providers. Users see value in critical sectors (healthcare, legal) where data privacy and regulatory compliance are paramount.

3. **Comparisons & Ecosystem Fit**  
   - Contrasted with tools like LangChain and LlamaIndex, LlamaFarm is seen as a higher-level framework bundling production-ready features (RAG, agents, deployment) with YAML configurability. Users appreciate its focus on composability and portability across environments (local vs. cloud).

4. **Enterprise Adoption Challenges**  
   - Discussions noted hurdles in enterprise settings: hard costs (GPU resources), regulatory compliance, and integrating AI with legacy systems. LlamaFarm’s monetization strategy (enterprise support, managed deployments) was seen as pragmatic.

5. **Technical Implementation**  
   - Praise for CLI simplicity, extensibility (Ollama, Chroma defaults), and production parity. Some users requested clearer docs and broader backend support (beyond Ollama). A broken link issue was promptly fixed by the maintainer.

6. **Community & Future Directions**  
   - Excitement from contributors and interest in self-hosted AI ecosystems. Debates arose around balancing developer experience with infrastructure complexity, and whether agents represent meaningful innovation vs. orchestration overhead.

**Notable Quotes:**  
- *“Local models combined with RAG are game-changers for domain-specific contexts… GPT-5-level generality isn’t always needed.”*  
- *“The hardest part isn’t runtime… it’s cost-effective GPU access and regulatory compliance.”*  
- *“LlamaFarm’s YAML-driven approach lets you prototype locally and redeploy anywhere—that’s the sweet spot.”*

**Maintainer Engagement:**  
- Addressed broken links promptly.  
- Clarified differentiation from LlamaIndex (end-to-end production focus vs. RAG primitives).  
- Emphasized enterprise monetization via compliance/management packages.

**Criticisms & Open Questions:**  
- Can LlamaFarm scale to multi-GPU on-prem deployments?  
- How will it handle evolving regulatory demands (e.g., HIPAA certification)?  
- Will the “config over code” philosophy limit advanced customization?

**Conclusion:**  
LlamaFarm resonates as a pragmatic tool for teams prioritizing local AI control, rapid prototyping, and production readiness. Its success may hinge on expanding backend support and deepening enterprise compliance features while maintaining developer-friendly abstractions.

### Deloitte to refund the Australian government after using AI in $440k report

#### [Submission URL](https://www.theguardian.com/australia-news/2025/oct/06/deloitte-to-pay-money-back-to-albanese-government-after-using-ai-in-440000-report) | 451 points | by [fforflo](https://news.ycombinator.com/user?id=fforflo) | [226 comments](https://news.ycombinator.com/item?id=45500485)

Deloitte to refund part of $440k Australian gov report after AI-linked citation errors

- Australia’s Department of Employment and Workplace Relations says Deloitte will repay the final instalment on a $440,000 review after multiple errors surfaced, including nonexistent references and a misdescription of a robodebt-related court case (Amato).
- Deloitte’s report, published 4 July and later amended, reviewed the department’s targeted compliance framework and IT system for welfare mutual obligations, finding issues like poor traceability to legislation and punitive default assumptions.
- After media scrutiny, Deloitte disclosed in an appendix that it used a DEWR-licensed Azure OpenAI GPT-4o toolchain in preparing parts of the report.
- University of Sydney academic Dr Christopher Rudge, who flagged the problems, said the report showed “hallucinations” and suggested some claims lacked a clear evidentiary source, though he didn’t dismiss the overall conclusions.
- Deloitte says the corrections don’t alter findings or recommendations and that the matter is resolved with the client.
- Labor senator Deborah O’Neill blasted the firm for a “human intelligence problem,” quipping that buyers might be better off with a ChatGPT subscription.

Why it matters: Governments are starting to push back on undisclosed or poorly governed AI use in high-stakes consulting work. Expect tighter procurement clauses on AI disclosure, stricter citation verification, and audit trails. For firms using LLMs, automated reference checking and retrieval-grounded workflows are becoming mandatory to avoid a “hallucination tax.”

**Summary of Hacker News Discussion:**

The discussion revolves around Deloitte’s AI-linked errors in a $440k Australian government report, with participants critiquing consulting firms’ accountability, AI governance, and systemic industry flaws. Key themes include:

1. **Criticism of Consulting Practices**:  
   - Many users criticized Deloitte and similar firms for prioritizing profit over quality, labeling consultants as "highly motivated by money" but lacking accountability. Subcontracting practices were highlighted, with work often delegated to inexperienced teams, leading to degraded outcomes. Comparisons were drawn to scandals like Fujitsu’s Post Office debacle and the UK’s Royal Mail system failures.  
   - A recurring sentiment: Consulting reports are seen as "black-box" exercises, with deliverables designed to justify pre-determined conclusions rather than rigorous analysis. One user quipped, *"Deloitte might as well use ChatGPT subscriptions instead."*

2. **AI Governance and Hallucinations**:  
   - The incident underscored concerns about undisclosed AI use in high-stakes work. Participants called for stricter procurement rules, automated citation checks, and retrieval-augmented workflows to curb AI "hallucinations." The term "hallucination tax" emerged, reflecting the cost of errors when AI-generated content lacks verification.  
   - Some defended AI’s role but stressed transparency: *"The problem isn’t AI—it’s human oversight."*

3. **Broader Systemic Issues**:  
   - Users linked the debacle to systemic failures in government contracting, citing the **robodebt scandal** (where flawed algorithms wrongfully accused welfare recipients of debt) as a cautionary tale. Deloitte’s misdescription of this case in their report was seen as emblematic of a pattern where consultants overlook human impacts.  
   - Discussions also touched on outsourcing to firms like Infosys and WITCH companies (Wipro, Infosys, TCS, etc.), where quality often declines due to cost-cutting and subcontracting layers.

4. **Off-Topic but Notable**:  
   - A tangent on chiropractic practices and medical accountability emerged, drawing parallels to critiques of unregulated expertise. However, this was largely sidelined as irrelevant to the core discussion.

**Conclusion**: The incident highlights growing scrutiny of AI use in consulting and demands for accountability. Participants anticipate tighter regulations, better audit trails, and a shift toward transparency in both AI workflows and consulting practices. As one user noted, *"Governments are learning to stop writing blank checks for glossy, error-prone reports."*

### GPT-5-Codex is a better AI researcher than me

#### [Submission URL](https://www.seangoedecke.com/ai-research-with-codex/) | 64 points | by [codeclimber](https://news.ycombinator.com/user?id=codeclimber) | [36 comments](https://news.ycombinator.com/item?id=45501326)

- The question: What’s the strongest model you can train on a laptop in five minutes?

- Setup: The author leans into “vibe research”—using an LLM (Codex/GPT‑5‑codex per the post) as a hands-on research assistant to iterate on code, run sweeps, and propose next steps. Work stayed on the TinyStories dataset for apples-to-apples comparisons.

- Process: A tight loop where Codex edits the training script, runs 3–4 experiments (~20 minutes), then suggests 2–3 next ideas; the author chooses and repeats. Heavy token use (restarting every ~1M tokens) on a $200/month plan. Ran in a permissive sandbox without MPS, so training was CPU-only; a few memory-caused laptop crashes but no runaway behavior.

- Experiments:
  - N-grams: 19 models; best was a 4‑gram with perplexity 18.5—fast to train but outputs read like stitched fragments with little global coherence.
  - Transformers: ~50 variants sweeping depth/heads/size. The best five-minute model was ~1.8M parameters. It produced simple, mostly coherent TinyStories-style text; Codex-guided runs beat the author’s solo baseline. Hand-picked hyperparameters from a prior attempt remained surprisingly competitive, roughly aligning with small-model scaling intuitions.

- Results (samples paraphrased): Outputs are childlike and mostly coherent but still prone to logical slips and odd phrasing—far from SOTA, yet respectable given the five‑minute, CPU‑bound constraint.

- Takeaways:
  - LLM-as-copilot makes amateur ML research feel tractable: easy hyperparameter sweeps, rapid iteration, decent idea generation.
  - The five-minute budget is the main bottleneck; compute access (no GPU) also capped performance.
  - Costs and token churn are nontrivial.
  - “Vibe research” works surprisingly well: the assistant often proposed the best ideas, with the human acting as curator and guardrails.

The Hacker News discussion on training AI models with LLM assistance reveals several key themes:

1. **Appreciation for Novelty & Accessibility**:  
   Commenters praised the experimental approach of using LLMs (like Codex) as a "vibe research" copilot, enabling rapid iteration even on a laptop CPU. The work’s accessibility for small teams or individuals, via affordable models like TinyStories, was seen as inspiring for amateur researchers and educators. However, some noted the $200/month API cost as a barrier.

2. **Technical Skepticism & Hype Critique**:  
   While impressed with the 5-minute training results, many questioned the hype around such "tiny models," emphasizing that outputs remain error-prone and far from SOTA. Skeptics argued the submission exemplified HN’s tendency to overhype incremental progress. Others debated whether LLM-assisted coding truly counts as "AI research" or is merely a productivity tool.

3. **Reliability Concerns in AI-Driven Research**:  
   Users shared frustrations with LLMs (like Claude) generating plausible but incorrect information, especially in sensitive domains like health/medicine. Reliance on Reddit-sourced training data was critiqued, with concerns about pseudoscience seeping into outputs. Some highlighted challenges in verifying sources and ensuring domain-specific reasoning.

4. **AI’s Role in Coding & Job Dynamics**:  
   The thread split between optimism for AI-assisted development ("lifting the floor" for non-experts) and warnings about unvetted code leading to "disasters." A subthread debated whether AI devalues traditional programming skills or simply automates grunt work, with anecdotes about shifting industry priorities and job market pressures.

5. **Humorous Meta-Comments**:  
   Lighthearted interjections included comparisons to xkcd comics, jokes about AGI timelines, and sarcastic takes on the "weekly AI hype cycle." Some users mockingly framed the submission as part of a broader trend of overcrediting AI over human input.

**Key Takeaway**: The discussion reflects a mix of cautious optimism about democratizing ML research via LLM tools and sharp critiques of technical limitations, costs, and community hype. Underlying tensions about AI’s role in reshaping research integrity and engineering careers persist.

### A cartoonist's review of AI art, by Matthew Inman

#### [Submission URL](https://theoatmeal.com/comics/ai_art) | 50 points | by [ChrisMarshallNY](https://news.ycombinator.com/user?id=ChrisMarshallNY) | [18 comments](https://news.ycombinator.com/item?id=45507228)

Matthew Inman (The Oatmeal) published a long, illustrated essay-comic reflecting on AI image tools—what they’re good for, where they fall short, and how they intersect with the craft and livelihood of artists. It’s characteristically funny but largely measured, aiming to separate hype and fear from practical realities.

Highlights
- Nuanced tool-not-replacement stance: frames AI image models as accelerators for drafts, ideation, and explorations, while arguing taste, direction, and storytelling remain the scarce skills.
- Ethics and consent: surfaces unresolved questions about training on copyrighted work, style mimicry, and compensation—urging clearer attribution/licensing norms rather than hand-waving “fair use” over everything.
- Craft vs. output: distinguishes making from merely generating, and why many artists still value the process; also notes where current models struggle with coherent visual narratives.
- Practical guardrails: suggests common-sense boundaries for using AI in creative workflows and crediting collaborators, without absolutism.

Why it matters
- A mainstream, award-winning creator weighing in broadens the conversation beyond tech circles, blending hands-on experience with ethical concerns.
- Timely amid ongoing legal fights over training data and style emulation, and as more productions experiment with AI-assisted pipelines.

HN discussion themes
- Consent and compensation for training data; whether “style” can or should be protected.
- The line between inspiration and replication, and what credit looks like in mixed human–AI workflows.
- Economic impacts on illustrators vs. democratization of visual creation.
- Whether rapidly improving models narrow the gap on composition, continuity, and storytelling—or just make mediocre faster.

Bottom line
A readable, balanced take that neither glorifies nor condemns AI art wholesale; it’s about what creators value, how audiences judge authenticity, and the norms we still need to build.

**Hacker News Discussion Summary:**

The discussion around Matthew Inman’s (The Oatmeal) essay on AI art reflects a nuanced debate about creativity, ethics, and the role of technology in artistic workflows. Key themes include:

1. **Human vs. AI Creativity**:  
   - Many argue that AI lacks the intentionality, emotional depth, and "soul" of human art, emphasizing that meaningful choices and storytelling stem from lived experience. Users note AI’s tendency to produce "soulless" or "thoughtless" outputs, even as it accelerates ideation.  
   - Counterpoints highlight AI’s utility for entertainment, personal projects (e.g., quirky holiday cards), or overcoming creative blocks, though conceding it cannot replicate the refinement of tools like Photoshop or years of artistic craft.

2. **Effort and Authenticity**:  
   - Some users share experiences of spending hours refining AI-generated images, arguing that effort and curation still matter. Others push back, stating that AI’s speed risks devaluing the struggle inherent to traditional art, which resonates with audiences through its imperfections and humanity.  
   - Comparisons to AI-generated music (e.g., "robotic" vocals) underscore concerns about authenticity, with debates over whether audiences can distinguish—or care about—human vs. AI creation.

3. **Ethics and Economics**:  
   - Concerns about training data consent, style replication, and fair compensation persist. Some fear AI could marginalize artists economically, while others see democratization in lowering barriers to visual creation.  
   - A recurring tension exists between AI as a tool for efficiency and its potential to disrupt creative industries, with calls for clearer attribution norms and ethical guardrails.

4. **Technical Critique**:  
   - Critiques of Inman’s essay question his grasp of AI’s technical underpinnings (e.g., transformers, encoders), contrasting it with historical digital tools like Adobe Effects. Some defend AI’s potential in interactive media (e.g., game narratives) while lamenting its overuse in "glitchy" or derivative outputs.

5. **Cultural Shifts**:  
   - Users debate whether AI’s rise reflects a broader societal shift toward valuing speed over craftsmanship. The essay’s focus on "what creators value" sparks discussions about artistic integrity, audience expectations, and the evolving definition of art itself.

**Conclusion**: The discussion mirrors Inman’s balanced stance—acknowledging AI’s utility while stressing irreplaceable human elements. Skepticism about hype coexists with curiosity about AI’s role in future workflows, underscoring the need for ethical frameworks and respect for artistic labor.

### Show HN: Greenonion.ai – AI-Powered Design Assistant

#### [Submission URL](https://exuberant-premise-723012.framer.app/) | 21 points | by [yanjiechg](https://news.ycombinator.com/user?id=yanjiechg) | [18 comments](https://news.ycombinator.com/item?id=45500560)

What it is
- A web-based “AI design assistant” that turns a short text brief (plus an optional image) into ready-to-use ad creatives in seconds, sized for multiple platforms.

How it works
- Describe your idea → Generate multiple design variants → Tweak in a built‑in editor → Download high‑quality exports.
- Credits map to “designs,” with 4 designs per generation (e.g., 15 generations = 60 designs).

Positioning and features
- Aimed at marketers/creators who need fast, professional-looking ads without design skills.
- Auto-resizes and adapts creatives to each platform’s look and tone.
- Editor for fine‑tuning; exports in multiple formats.
- Enterprise: white-labeling, custom integrations, dedicated support, SLA.

Pricing (monthly, billed monthly)
- Creator: $19 for 60 designs (15 generations), email support.
- Professional: $49 for 180 designs (45 generations), priority support.
- Enterprise: $149 for 500 designs (125 generations), plus enterprise features.
- Rough per-design cost: ~$0.32 (Creator), ~$0.27 (Professional), ~$0.30 (Enterprise).

Why it matters
- Speeds up ad iteration for teams without in-house design, and offers agency-friendly options (white-label, integrations).

What’s unclear
- Free trial details, data security specifics, IP/licensing for generated assets, supported export formats, brand kit support (fonts/colors), collaboration, and model/source transparency.

Context
- Competes with Canva/Adobe Express “magic” tools, AdCreative.ai, Bannerbear, etc. Differentiators appear to be tone-matched multi-platform output and the enterprise/white-label focus.

Bottom line
- A pragmatic tool to quickly spin up and iterate ad creatives; best unit economics at the Professional tier if you’re generating at volume.

Here’s a concise summary of the Hacker News discussion about the AI ad creative tool:

### **Key Criticisms**
1. **Design Quality Concerns**  
   - Users found examples in the demo unpolished, citing issues like unsophisticated typography, truncated design elements, and a lack of visual harmony. Comments like "*design shouldn’t look like slop*" and "*AI-generated designs have a quality gap*" reflect skepticism about the tool’s ability to produce professional results.  

2. **Overpromise of AI Capabilities**  
   - Some viewed the tool as a "ChatGPT wrapper" with minimal innovation, arguing that AI still struggles with nuanced design principles. One user noted, *"If you don’t worry about design, you can waive that... but the messaging falls flat."*  

3. **Technical Flaws**  
   - The landing page reportedly breaks on larger screens, raising doubts about responsive design efforts. Others criticized clunky onboarding and confusing project structures.

4. **Templates Over Creativity**  
   - Comments like "*it’s just AI slapping text on templates*" and *"doesn’t solve unique creative needs*" suggest users see the tool as derivative rather than offering novel design solutions.

---

### **Positive Notes**
- A few users praised the concept (*"pretty great idea"*) and acknowledged the challenge of balancing speed with design quality.  
- One person complimented the generation output (*"found generation great"*), though this was an outlier.

---

### **Broader Skepticism**
- **Human Touch vs. AI**: Many argued that human designers are still essential for polished, client-ready work.  
- **Pricing Model**: Questions arose about value, with users doubting subscription costs given current quality limitations.  
- **Ethical/Functional Concerns**: Criticisms included "cherry-picked" demo results and lack of transparency around data/IP.

### **Takeaway**  
While the tool addresses a real need for speed and iteration, the consensus leans toward skepticism. Users demand higher design sophistication and clearer differentiation from existing template-driven tools. The feedback highlights a gap between AI’s promises and its current ability to replace human creativity in visually sensitive fields like advertising.