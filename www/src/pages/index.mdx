import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Thu Jun 29 2023 {{ 'date': '2023-06-29T17:11:45.160Z' }}

### Building Boba AI: Lessons learnt in building an LLM-powered application

#### [Submission URL](https://martinfowler.com/articles/building-boba.html) | 160 points | by [nalgeon](https://news.ycombinator.com/user?id=nalgeon) | [62 comments](https://news.ycombinator.com/item?id=36523480)

Today's top story on Hacker News is about the lessons learned and patterns discovered while building an experimental AI co-pilot called "Boba." Boba is designed to assist with product strategy and generative ideation by leveraging a Large-Language Model (LLM) to generate ideas and help users navigate complex conversational flows. The article outlines several patterns for building generative co-pilot applications, including Templated Prompt, Structured Response, Real-Time Progress, Select and Carry Context, Contextual Conversation, Out-Loud Thinking, Iterative Response, and Embedded External Knowledge. These patterns aim to enhance the user's interaction with the LLM, improve the quality of generated results, and integrate external knowledge that the LLM may not have.

Boba is described as an AI co-pilot that augments the early stages of strategy ideation and concept generation. It enables users to generate and evaluate ideas in partnership with AI, leveraging OpenAI's LLM to generate ideas and answer questions related to specific domains. The first prototype of Boba focuses on capabilities such as researching signals and trends, creative matrix concepting, scenario building, strategy ideation, concept generation, and storyboarding. The article also mentions that Boba is a web application that serves as an interface between the user and the LLM (currently GPT 3.5). The goal of Boba is to simplify the interaction with the LLM for users who may not be familiar with effectively engaging with AI systems. The discussion highlights a mix of enthusiasm for AI co-pilots and a critical examination of their limitations, practicality, and potential for real-world applications. There is also a focus on the importance of integrating AI with existing tools and incorporating structured data for improved interactions and results.

### Tesla Fleet Telemetry

#### [Submission URL](https://github.com/teslamotors/fleet-telemetry) | 205 points | by [shekhar101](https://news.ycombinator.com/user?id=shekhar101) | [120 comments](https://news.ycombinator.com/item?id=36525940)

Tesla has released a decentralized framework called Fleet Telemetry, which allows Tesla customers to create a secure and direct connection between their Tesla devices and authorized third-party providers. Fleet Telemetry is a simple, scalable, and secure data exchange service for vehicles and other devices. It handles device connectivity, data transmission, and storage. Customers can configure telemetry records and receive acknowledgments, error responses, or rate limit notifications. Fleet Telemetry can be installed on Kubernetes with a Helm Chart or as a standalone binary. It requires setting up a publicly available endpoint and mutual TLS (mTLS) WebSocket connections for device communication. The service can be configured for different data backends and dispatchers. Tesla emphasizes the importance of security and privacy in Fleet Telemetry, allowing customers to have control over their data sharing.

The discussion on this submission revolves around the topics of privacy, data sharing, and the implications of Tesla's Fleet Telemetry framework. Some users express concerns about the potential misuse of customer data by third-party apps and the need for stronger privacy laws to protect consumers. Others argue that the benefits of data sharing and telemetry outweigh the risks, and that Tesla owners have control over their data. There are also discussions about the level of privacy protection provided by current laws and the potential for manipulation through targeted advertising.

### Valve is not willing to publish games with AI generated content anymore?

#### [Submission URL](https://old.reddit.com/r/aigamedev/comments/142j3yt/valve_is_not_willing_to_publish_games_with_ai/) | 611 points | by [Wouter33](https://news.ycombinator.com/user?id=Wouter33) | [380 comments](https://news.ycombinator.com/item?id=36522665)

Valve, the company behind the Steam gaming platform, has recently made it clear that they are not willing to publish games with AI-generated content. A developer shared their experience of trying to release a game with assets that were obviously AI-generated, only to have their submission rejected by Valve. The rejection message stated that the game contained art assets generated by AI that appeared to be relying on copyrighted material owned by third parties. Valve cited the unclear legal ownership of such AI-generated art as the reason for their decision. The developer then improved the assets by hand, but their resubmitted app was still rejected. This incident highlights the uncertainty around AI-generated content and the challenges developers may face in getting their games published. While some games on Steam do mention the use of AI, Valve, at least for now, seems wary and not willing to publish AI-generated content. The developer plans to try uploading their game to itch.io to see if they face similar issues there.

The discussion on this submission revolves around Valve's decision to not publish games with AI-generated content. Some users argue that Valve's rejection of games with AI-generated assets is justified due to potential copyright infringement, while others express frustration with the lack of clear guidelines and transparency from Valve. There are also discussions about the ethical implications of AI-generated content and the role of journalists in verifying information. Some users bring up the possibility of Valve's stance being influenced by legal concerns or demands from the public. The inconsistency in Valve's decisions and the potential impact on blockchain games are also mentioned in the discussion. Overall, there are mixed reactions and discussions about the legal, ethical, and practical aspects of AI-generated content in games.

### XGen-7B, a new 7B foundational model trained on up to 8K length for 1.5T tokens

#### [Submission URL](https://blog.salesforceairesearch.com/xgen/) | 260 points | by [bratao](https://news.ycombinator.com/user?id=bratao) | [92 comments](https://news.ycombinator.com/item?id=36514936)

Salesforce's team of researchers have trained a series of 7B Long-Range Language Models (LLMs) called XGen-7B that can handle input sequence lengths of up to 8K tokens. The models achieve comparable or better results than state-of-the-art open-source LLMs on standard NLP benchmarks. They also outperform 2K- and 4K-seq models on long sequence modeling tasks. XGen-7B performs well on both text and code tasks and has a training cost of $150K on 1T tokens. The codebase and checkpoint for XGen-7B are available on GitHub and Hugging Face, respectively. The researchers explain that the need for LLMs to effectively model long sequences is crucial for tasks such as summarizing text, writing code, and predicting protein sequences. Most open-source LLMs are trained with a maximum of 2K token sequence length, which limits their ability to handle long sequences. The XGen models were fine-tuned on public-domain instructional data, resulting in instruction-tuned counterparts. The researchers used a two-stage training strategy and JaxFormer, their in-house library, to train the XGen-7B models. They also explored "loss spikes" during training and made improvements to ensure stable training at larger model sizes. Overall, XGen-7B with 8K sequence length offers advancements in long sequence modeling.

---

## AI Submissions for Wed Jun 28 2023 {{ 'date': '2023-06-28T17:09:56.567Z' }}

### Junk websites filled w AI-generated text pulling in money from programmatic ads

#### [Submission URL](https://www.technologyreview.com/2023/06/26/1075504/junk-websites-filled-with-ai-generated-text-are-pulling-in-money-from-programmatic-ads/) | 224 points | by [bookofjoe](https://news.ycombinator.com/user?id=bookofjoe) | [119 comments](https://news.ycombinator.com/item?id=36514063)

A new report from NewsGuard reveals that AI chatbots are being utilized to fill junk websites with AI-generated text, attracting paying advertisers. Over 140 major brands are unknowingly paying for ads that appear on unreliable AI-written sites. Despite Google's policies prohibiting the placement of ads on pages with "spammy automatically generated content," 90% of the ads from major brands found on these AI-generated news sites were served by Google. This practice not only wastes massive amounts of ad money but also risks creating a glitchy, spammy internet dominated by AI-generated content. Content farms have emerged where low-paid workers churn out poor-quality content to attract ad revenue. These sites are often referred to as "made for advertising" sites and employ tactics such as clickbait and pop-up ads to maximize profits. The Association of National Advertisers estimates that around $13 billion is wasted globally on these sites each year, with 21% of ad impressions going to "made-for-advertising" sites. With the advent of generative AI, the content farm process can be automated, leading to the proliferation of more junk sites without much effort. NewsGuard has identified around 25 new AI-generated sites each week, with a total of 217 sites in 13 languages found since April. NewsGuard employs a method to identify these junk AI-written sites by searching for error messages typical of generative AI systems. The company's AI scans the websites for these error messages, and then a human analyst reviews them. Programmatic advertising, where algorithms place ads on various websites based on calculations to optimize the ad's reach, is the main revenue source for these AI-generated sites. Many Fortune 500 companies and prominent brands unknowingly advertise on these sites, contributing to their growth. The cost of a programmatic ad is around $1.21 per thousand impressions, but brands often do not review all automatic ad placements. Google, the largest exchange for programmatic ads, has faced criticism for serving ads on content farms, despite its policies against it. Google Ads made $168 billion in advertising revenue last year. While most ad exchanges and platforms have policies against serving ads on content farms, they do not uniformly enforce them. Google stated that the presence of AI-generated content on a page is not inherently a violation of its policies, but it acknowledges the need to stay vigilant against bad actors who may use generative AI to bypass enforcement systems. While NewsGuard found that most of the AI-generated sites are of low quality, they do not propagate misinformation.

### Deep Learning Digs Deep: AI Unveils New Large-Scale Images in Peruvian Desert

#### [Submission URL](https://blogs.nvidia.com/blog/2023/06/23/geoglyphs-in-peru/) | 45 points | by [bcaulfield](https://news.ycombinator.com/user?id=bcaulfield) | [17 comments](https://news.ycombinator.com/item?id=36514297)

Researchers at Yamagata University in Japan have successfully used artificial intelligence (AI) to uncover four previously unseen geoglyphs in Nazca, Peru. Geoglyphs are large images made on the ground using natural elements. The team used a deep learning model to analyze high-resolution aerial photographs and identified a humanoid figure, a pair of legs, a fish, and a bird. The discovery process was significantly faster than traditional archaeological methods. The findings highlight the potential of AI in accelerating archaeological discoveries and suggest the presence of more undiscovered sites in the area. The researchers used an IBM Power Systems server with an NVIDIA GPU for model training.

The discussion on Hacker News revolves around different interpretations and possible purposes of the newly discovered geoglyphs in Nazca, Peru, using AI technology. One user points out the astronomical significance of the geoglyphs and suggests that they may have been created for regional defense purposes or represent celestial navigation. Another user suggests that the geoglyphs may have served religious or magical purposes, reflecting the thinking of people who built structures with irrational beliefs. Some users criticize the assumptions made about the purpose of the geoglyphs, emphasizing that they could serve more practical purposes related to economics, water resource management, and navigation. There is also speculation about the involvement of extraterrestrial beings or religious rituals in the creation of the geoglyphs. Some users argue for Occam's razor, suggesting that the most plausible explanation is that the geoglyphs were created by humans for religious reasons. Other discussions explore the possibility of communicating with gods or extraterrestrial beings through the geoglyphs and propose different theories about their purpose, including marking landmarks, serving as a form of artistic expression, or reflecting cultural remnants. Additionally, users discuss the logistics of creating the geoglyphs, with some suggesting that the most straightforward explanation is that humans built them by using hand-sized stones to scrape the surface. Finally, there are references to the Burning Man event in Peru and the evolution of human technology over time.

### The idea maze for AI startups (2015)

#### [Submission URL](https://cdixon.org/2015/02/01/the-ai-startup-idea-maze/) | 104 points | by [gmays](https://news.ycombinator.com/user?id=gmays) | [40 comments](https://news.ycombinator.com/item?id=36507010)

In this article, the author introduces the concept of an "idea maze" - a map of all the key decisions and tradeoffs that startups in a given industry need to make. They use the example of AI startups and outline the steps in the maze.  The first step is to create a minimum viable product (MVP) with 80-90% accuracy, as it is relatively easy to build a model that is accurate to this level. From there, the founder has a choice of either trying to increase the accuracy to near 100% or building a product that is useful despite being only partially accurate. This can be done by creating a fault-tolerant user experience (UX), similar to iOS autocorrect or Google search's "did you mean..." feature. If the decision is made to go for 100% accuracy, the key is to obtain more data for training the models. Data is a crucial component of AI as algorithms are mostly a shared resource created by the research community. Narrowing the domain of the problem being solved helps to reduce the amount of data needed. The next step is to narrow the domain even further, building an MVP that is part of the ultimate goal. This allows for incremental progress towards the larger goal while addressing a specific need in the market. Obtaining the necessary data can be done by either building it yourself or crowdsourcing it. Startups often opt for the latter, designing a service that provides the right incentives for users to contribute data. Crowdsourcing data is seen as a viable approach, and the example of Wit.ai, a company that provided a service for speech-to-text and natural language processing, is given. Wit.ai allowed developers to correct errors and improve results, and this training data was then used to make the overall system smarter.

### Germany Launches Opencode.de

#### [Submission URL](https://joinup.ec.europa.eu/collection/open-source-observatory-osor/news/germany-launches-opencodede) | 53 points | by [amai](https://news.ycombinator.com/user?id=amai) | [8 comments](https://news.ycombinator.com/item?id=36509896)

Germany has launched opencode.de, a national code repository aimed at facilitating local cooperation and implementing the country's Online Access Act. This act requires the publication of source code as open source and the checking of components for reusability for the listed 575 public services that must be provided online. The repository aims to foster a community among local administrations, allowing them to share software, exchange knowledge, and collaborate on solutions. The project has focused on the advantages of open source in terms of flexibility, sovereignty, and achieving the government's cloud strategy and Online Access Act goals. The pilot phase, funded by the 2022 federal budget, was successful, and opencode.de is now fully available with active projects. The repository already shows local administrations using it to share configurations, tools, and agree on software versions. This initiative aligns with the FSFE campaign Public Money? Public Code!

### Scared tech workers are scrambling to reinvent themselves as AI experts

#### [Submission URL](https://www.vox.com/technology/2023/6/28/23774435/ai-skills-classes-tech-jobs-pivot) | 38 points | by [dacohenii](https://news.ycombinator.com/user?id=dacohenii) | [22 comments](https://news.ycombinator.com/item?id=36510704)

In the current tech industry climate of pay stagnation and layoffs, many tech workers are feeling the pressure to reinvent themselves as AI experts. The rise of artificial intelligence has made AI specialists highly sought after in Silicon Valley, with companies and investors still investing heavily in AI. This has created a surge in demand, pay, and perks for individuals skilled in AI, making it an attractive career path for those seeking upward mobility or who have recently been laid off. Many tech workers are now attempting to reposition themselves in the AI field through on-the-job training, boot camps, and self-education. Job openings are increasingly emphasizing the need for AI skills, and those with AI expertise are paid on average 27% more than typical tech workers. The median annual salary for an AI engineer is $243,500, compared to $166,750 for non-AI engineers. Big tech companies are actively scouting AI talent and offering retention bonuses to prevent their AI engineers from leaving for other firms. As businesses continue to adopt AI, tech workers are recognizing the need to pivot to AI roles to stay relevant and ensure their job security.

---

## AI Submissions for Tue Jun 27 2023 {{ 'date': '2023-06-27T17:10:49.205Z' }}

### H100 GPUs Set Standard for Gen AI in Debut MLPerf Benchmark

#### [Submission URL](https://blogs.nvidia.com/blog/2023/06/27/generative-ai-debut-mlperf/) | 140 points | by [Anon84](https://news.ycombinator.com/user?id=Anon84) | [70 comments](https://news.ycombinator.com/item?id=36499073)

NVIDIA's H100 Tensor Core GPUs have achieved excellent AI performance, particularly in large language models (LLMs) used in generative AI, according to user feedback and industry-standard benchmarks. The GPUs set new records on all eight MLPerf training benchmarks, with outstanding performance on a new MLPerf test for generative AI. For example, on a cluster of 3,584 H100 GPUs co-developed by startup Inflection AI and cloud service provider CoreWeave, the system completed a GPT-3-based training benchmark in under 11 minutes. These results highlight the H100 GPUs' top performance in a variety of AI workloads, such as recommenders, computer vision, medical imaging, and speech recognition. The GPUs also demonstrated scalability and achieved near-linear performance scaling on demanding LLM tests when scaled from hundreds to thousands of GPUs. NVIDIA was the only company to submit results on MLPerf's updated benchmark for recommendation systems. The comprehensive performance of NVIDIA AI across different workloads and its wide ecosystem of partners have made it a reliable choice for customers in both cloud and on-premises environments. As AI performance requirements continue to grow, energy efficiency becomes crucial, and NVIDIA's accelerated computing solutions help optimize performance while reducing rack space and energy consumption. NVIDIA AI Enterprise, the software layer of the NVIDIA AI platform, offers enterprise-grade support and is available for optimized AI workloads.

### Open source AI is critical – Hugging Face CEO before US Congress

#### [Submission URL](https://venturebeat.com/ai/hugging-face-ceo-tells-us-house-open-source-ai-is-extremely-aligned-with-american-interests/) | 315 points | by [thibo_skabgia](https://news.ycombinator.com/user?id=thibo_skabgia) | [70 comments](https://news.ycombinator.com/item?id=36499498)

Hugging Face CEO Clement Delangue testified before the U.S. House Science Committee, stating that open science and open-source AI are crucial for American innovation and align with American values. Delangue emphasized that the US's leading position in AI is thanks to open-source tools like PyTorch, Tensorflow, Keras, transformers, and diffusers. Delangue's testimony follows a letter from senators questioning Mark Zuckerberg about the potential misuse of Meta's open-source LLM LLaMA model. Hugging Face, a New York-based startup valued at $2 billion, has become a hub for open-source code and models and has been a significant voice in the open-source AI community. Delangue highlighted how open science and open source drive the development of AI startups and ensure accountability, mitigate biases, reduce misinformation, and reward stakeholders. Hugging Face promotes ethical openness through institutional policies, technical safeguards, and community incentives.

The discussion on this submission covers various topics related to open-source AI and its implications. Some users highlight the importance of open-source code and models for digital security, while others discuss the differences between open-source and closed-source software. There is also a conversation about the licensing of models and the challenges of sharing and replicating results. Some users express concerns about the computational costs of training AI models and the competitiveness within the industry. Others mention the potential dangers of AI regulation and the need for government involvement. The conversation also touches on the role of Hugging Face in the AI community and its efforts to promote open science. There are mentions of specific projects and technologies, such as Hugging Face's API and the GitHub repository for AI. Overall, the discussion reflects a mix of perspectives on open-source AI, its benefits, challenges, and potential future developments.

### Show HN: Superblocks AI – AI coding assistant for internal apps

#### [Submission URL](https://www.superblocks.com/blog/introducing-superblocks-ai) | 105 points | by [frankgrecojr](https://news.ycombinator.com/user?id=frankgrecojr) | [58 comments](https://news.ycombinator.com/item?id=36495680)

Superblocks AI is revolutionizing the way developers build internal tools. This new tool offers powerful code generation, explanation, performance optimization, and mock data generation capabilities. With Superblocks AI, developers can generate code snippets from a prompt, making it easier to handle unfamiliar languages or write boilerplate queries and business logic. The tool also provides concise explanations for code, simplifying code comprehension and improving development efficiency. Additionally, Superblocks AI allows users to edit code, generate third-party API calls, and generate personalized mock data for UI development. With its diverse features, Superblocks AI aims to help developers write better applications and streamline their development process.

In the comments, there is a discussion about the practicality and limitations of AI-generated code. Some people express concerns about relying too heavily on AI tools and the potential for them to generate incorrect or problematic code. Others discuss the benefits of using AI-generated code for tasks like prototyping or generating boilerplate code. One commenter shares their experience with using AI-generated code for specific tasks like React programming and finding it to be helpful. There is also a conversation about the historical tradition of early adopters experiencing the rough effects of new technologies, such as the impact of AI code generation on the experience level of developers. Additionally, there are discussions about the challenges and potential pitfalls of using AI tools and the importance of understanding the context in which they are used.

### LLM Powered Autonomous Agents

#### [Submission URL](https://lilianweng.github.io/posts/2023-06-23-agent/) | 275 points | by [DanielKehoe](https://news.ycombinator.com/user?id=DanielKehoe) | [165 comments](https://news.ycombinator.com/item?id=36488871)

Today's digest covers an overview of an agent system powered by a large language model (LLM) as its core controller. The system consists of several components that enhance the agent's capabilities. The first component is planning, where the agent breaks down complex tasks into smaller subgoals for efficient handling. The second component is memory, which includes both short-term and long-term memory for contextual learning and information retention. The third component is tool use, where the agent learns to utilize external APIs for additional information and resources. 

The digest also explores the first component in detail, which is planning. It discusses task decomposition techniques such as Chain of Thought (CoT) and Tree of Thoughts, which help the agent break down tasks into manageable steps. It also introduces an alternative approach, LLM+P, which uses an external classical planner for long-horizon planning. 

Self-reflection is another crucial aspect of the agent system, allowing the agent to improve its decision-making and learn from past actions. Two frameworks, ReAct and Reflexion, are mentioned as examples that integrate reasoning and self-reflection capabilities within LLM. Overall, building an autonomous agent system with LLM as its core offers immense potential for solving complex problems and improving task performance through effective planning and self-reflection.

The discussion on the submission includes various topics related to language models (LLMs) and their functionality. One user explains how LLMs generate outputs by selecting tokens based on probabilities. Another user mentions the concept of beam search as a popular method for generating results in language models. It is also noted that LLMs can be non-deterministic and that there are challenges in controlling the output. There is a discussion about the differences between LLMs and other models based on their types. It is mentioned that LLMs can be more progressive compared to other models, with examples of specific models like Google's Progressive Neural Network and Parti. The topic of task decomposition and planning is brought up, with explanations of how LLMs can break down tasks into smaller steps. Different approaches to planning, such as Chain of Thought and Tree of Thoughts, are discussed. The use of external classical planners for long-horizon planning is also mentioned.

There is a conversation about the limitations and challenges of LLMs, including issues with manipulating probabilities, interpretability, and training on large contexts. Some users share resources and research on understanding LLMs and their limitations. The conversation touches on the topic of memory in LLMs, with one user mentioning the implementation of memory in OpenAI's API. Another user relates the concept of memory in LLMs to Quick Resume technology in gaming consoles. Overall, the discussion includes various insights and perspectives on the capabilities and limitations of language models, particularly LLMs, and their application in autonomous agent systems.

---

## AI Submissions for Mon Jun 26 2023 {{ 'date': '2023-06-26T17:10:50.183Z' }}

### Show HN: Mofi – Content-aware fill for audio to change a song to any duration

#### [Submission URL](https://mofi.loud.red/) | 621 points | by [jaflo](https://news.ycombinator.com/user?id=jaflo) | [150 comments](https://news.ycombinator.com/item?id=36480687)

Mofi is a new online tool that allows users to edit their music without needing to download or install anything. With Mofi, users can easily shorten or lengthen a song to match their video or performance, seamlessly remove certain parts of a song, and even create perfectly looping versions of their favorite tunes. Best of all, it's completely free and easy to use. Simply upload your file or paste a link, choose what you want to edit, and let Mofi do the rest. Whether you're a budding musician or just love to play with music, Mofi is definitely worth checking out.

In the comments, users discussed other similar tools, like the Infinite Jukebox and the Echo Nest, and some shared their experiences with music editing. There was also a discussion about the state of the music industry today. Some users talked about the importance of original music and the challenges of discovering new artists, while others debated the impact of exclusive contracts and the power dynamics of the entertainment industry.

### Android’s emergency call shortcut is flooding dispatchers with false calls

#### [Submission URL](https://arstechnica.com/gadgets/2023/06/uk-police-blame-android-for-record-number-of-false-emergency-calls/) | 177 points | by [PaulHoule](https://news.ycombinator.com/user?id=PaulHoule) | [188 comments](https://news.ycombinator.com/item?id=36479440)

Police forces in the UK are reporting a “record number” of false emergency calls due to Android’s easy-access emergency call feature which was added to Android 12. By pressing the power button five times, a user can accidentally trigger the emergency services and as the feature started rolling out to non-Pixel devices only recently, many people have been caught unawares. In response to the surge of silent accidental calls received, Google is working with OEMs to develop a fix for the feature. Until then, Android recommends users switch Emergency SOS off for a couple of days or search for the feature in system settings. Some users have suggested turning off Emergency SOS until the issue is resolved. The discussion also touches on topics such as removable phone batteries, Faraday cages, and the reliability of mechanical watches vs smartwatches. Some commenters share their personal experiences with accidentally triggering emergency calls. Others raise concerns about privacy and data collection in the context of emergency services.

### ChatHN: Chat with Hacker News using OpenAI function calling

#### [Submission URL](https://github.com/steven-tey/chathn) | 211 points | by [steventey](https://news.ycombinator.com/user?id=steventey) | [65 comments](https://news.ycombinator.com/item?id=36480570)

ChatHN is an open-source AI chatbot that uses OpenAI Functions and the Vercel AI SDK to interact with the Hacker News API using natural language. It allows users to chat with Hacker News and get updates on top stories. The AI can be deployed with just one click and is built on the Next.js framework, OpenAI Functions for AI completions, the Vercel AI SDK for AI streaming, and TailwindCSS for styles. The project is licensed under the MIT license.

Many comments on the post express concerns about potential security risks and implications for potential school controls, among other things. Some have also expressed skepticism about the capabilities of the AI. Others suggest potential uses for NLI in platforms like LinkedIn and Salesforce. One commenter asks for a generic ChatGPT answer to a question about the most popular discussions on AWS, and another discusses his experience with trying to call a function on the ChatHN API.

### Kor: a half-baked prototype that "helps" you extract structured data using LLMs

#### [Submission URL](https://github.com/eyurtsev/kor) | 116 points | by [BorisWilhelms](https://news.ycombinator.com/user?id=BorisWilhelms) | [15 comments](https://news.ycombinator.com/item?id=36484308)

Kor is a prototype AI tool that helps extract structured data from text using LLMs. It allows users to specify the schema of what should be extracted and provides examples, then generates a prompt and sends it to the specified LLMs. The tool then parses the output and provides the results back to the user. While it is integrated with LangChain framework, Kor is still a half-baked prototype with an unstable API.

In the comments, users discussed the flexibility of running models in different languages, and its helpfulness in extracting metadata using LLMs. Some users also shared their thoughts on using LLMs to generate web scrapers efficiently, while others shared alternative models for extracting structured data from text. Furthermore, users discussed their experience using Kor in extracting data, and some suggested using CSS selectors in HTML documents for extracting data.

### Tear Down: Tesla's In-House Radar Design

#### [Submission URL](https://www.ghostautonomy.com/blog/tearing-down-teslas-in-house-radar-design-why-did-they-bother) | 43 points | by [mensetmanusman](https://news.ycombinator.com/user?id=mensetmanusman) | [3 comments](https://news.ycombinator.com/item?id=36485984)

In a recent analysis, a radar engineer examined Tesla's in-house radar program, which has remained mysterious since the company removed radar from its cars in 2021. The engineer believes that the new HW4 architecture, equipped with Tesla's developed radar, falls short of the specs of rival automotive industry's long-range 4D imagining radars. However, it provides Tesla with increased control, allowing for greater optimization and synchronization of the radar with their overall autonomy system. The analysis also delves into the radar's sensitivity, MIMO topology, and antenna topologies, providing insight into its design approach.

The discussion about the submission revolves around Tesla's decision to develop their own radar program in-house instead of relying on suppliers. One user believes that Tesla does not want to depend on suppliers and is willing to invest in building millions of chips, paying a higher margin, and waiting for supplier time to fall. Another user adds that Tesla's radar program started with a temporal resolution front-facing continental radar, and Tesla Vision, the company's AI-based driving assistant system, has high-resolution phased array radar technology. Another user analyzes the article's technical details, specifically mentioning the radar's geometry.

### Databricks Strikes $1.3B Deal for Generative AI Startup MosaicML

#### [Submission URL](https://www.wsj.com/articles/databricks-strikes-1-3-billion-deal-for-generative-ai-startup-mosaicml-fdcefc06) | 181 points | by [jmsflknr](https://news.ycombinator.com/user?id=jmsflknr) | [105 comments](https://news.ycombinator.com/item?id=36478734)

Databricks, a data management and analytics firm, has announced its acquisition of MosaicML, a generative AI startup, in a $1.3 billion deal. The acquisition is aimed at satisfying the increasing demand from businesses to build their own ChatGPT-like language models, according to Databricks' CEO Ali Ghodsi. The deal is expected to provide businesses with the ability to connect their data with services to help create their own cost-effective language models.

Some users on Hacker News were critical of the acquisition, stating that some companies do not have the technical talent to implement LLMs and that this could result in less meaningful value propositions. Others noted that Databricks is a technically-staffed company and that LLM integration would be a perfect fit for its business plan. Additionally, some users discussed the differences between Databricks and Snowflake in terms of data storage and management. Some users were not convinced of the value proposition of LLMs, stating that data warehouses are essentially required for these models. Lastly, some users pointed out that Databricks has replaced its self-managed Spark-on-K8s with third-party recommendations. A few users mentioned AWS SageMaker as a potential competitor to MosaicML.

### Google DeepMind’s CEO Says Its Next Algorithm Will Eclipse ChatGPT

#### [Submission URL](https://www.wired.com/story/google-deepmind-demis-hassabis-chatgpt/) | 17 points | by [neilfrndes](https://news.ycombinator.com/user?id=neilfrndes) | [7 comments](https://news.ycombinator.com/item?id=36486343)

Demis Hassabis, the CEO of Google's DeepMind AI lab, has announced that his team is working on an AI system called Gemini, which will combine the language capabilities of large models with techniques used in AlphaGo, the program that defeated a champion player of the board game Go. The system will have capabilities such as planning or the ability to solve problems similar to OpenAI's ChatGPT, but with new innovations that will be "pretty interesting", according to Hassabis. The development of Gemini could play a major role in Google's response to the competitive threat posed by ChatGPT and other AI technology.

The discussion is mostly made up of comments that are not directly related to the submission. One user questions whether Google maintains a "shadow brain" internet search, another user writes about having flashbacks to when Google was hyping their search engine, and another user suggests that they should show instead of tell. One user shares an archived link related to the submission. Another user questions the potential for Gemini to compete with OpenAI's ChatGPT and expresses concern about the dangers of AI development. Lastly, a user mentions a headline suggesting that Google's algorithm has surpassed ChatGPT.

---

## AI Submissions for Sun Jun 25 2023 {{ 'date': '2023-06-25T17:11:48.773Z' }}

### Show HN: Open-source resume builder and parser

#### [Submission URL](https://www.open-resume.com/) | 603 points | by [xitang](https://news.ycombinator.com/user?id=xitang) | [182 comments](https://news.ycombinator.com/item?id=36470297)

OpenResume is a free and open-source resume builder that saves users from manual formatting work. It has built-in best practices for the US job market and works well with top ATS platforms such as Greenhouse and Lever. OpenResume also stores data locally in users' browsers to ensure complete privacy. It is designed specifically for the US job market with a single column resume design, core sections, and no option to add a profile picture to avoid bias and discrimination. The creators of OpenResume hope to help anyone create a modern professional resume that follows best practices and enable anyone to apply for jobs with confidence.

OpenResume is an open-source resume builder that uses built-in best practices for the US job market and stores data locally in users' browsers to ensure privacy. While there are some criticisms and questions regarding the project's marketing and endorsements, the creator defends it as an honest and transparent initiative. Users discuss the differences between a CV and a resume, discrimination in hiring based on language skills, and the importance of effective communication skills in software development jobs.

### How the most popular cars in the US track drivers

#### [Submission URL](https://www.wired.com/story/car-data-privacy-toyota-honda-ford/) | 122 points | by [arkadiyt](https://news.ycombinator.com/user?id=arkadiyt) | [143 comments](https://news.ycombinator.com/item?id=36473217)

Privacy4Cars released a new tool called the Vehicle Privacy Report that reveals how much information car manufacturers can collect from your vehicle's data. The tool creates privacy labels for what manufacturers collect and whom they share data with. Typically, most modern vehicles are like "smartphones on wheels," with the ability to collect significant amounts of data wirelessly and send the information to manufacturers. The Vehicle Privacy Report works by using a car's Vehicle Identification Number (VIN) and analyzing each manufacturer's public policy documents. The study showed Toyota collects personal information such as name, address, driving license number, phone number, email, and driving behavior, including acceleration, speed, braking functionality, and travel direction. It may also gather favorite locations saved on its systems and images taken by external cameras or sensors. Some models of Toyota cars can also scan drivers' faces for face recognition. It is unknown whether Toyota collects data from people's phones that are synced with its vehicles. Commenters in the discussion express varying opinions about data privacy, government regulation, and the role of car manufacturers in protecting consumer privacy. They also discuss matters related to driving safety, such as the need for better visibility in modern cars and the impact of pre-collision driving systems on driver behavior.

### Show HN: Bing Chat sidebar ported from Edge to Chrome

#### [Submission URL](https://github.com/wong2/bing-sidebar-for-chrome) | 14 points | by [wonderfuly](https://news.ycombinator.com/user?id=wonderfuly) | [8 comments](https://news.ycombinator.com/item?id=36467997)

This is an AI-powered Bing chat sidebar that has been ported from Microsoft Edge to Chrome, allowing users to access the current webpage or PDF. The sidebar is available as a Chrome extension and does not collect any user data. The extension is compatible with Google Chrome version 114 or higher but may not work with other Chromium-based browsers. Users in the discussion expressed their gratitude and excitement for this new Chrome extension that allows easy access to AI-powered Bing chat sidebar. One user shared that they are building an extension and found this feature very helpful. Another user mentioned that this new functionality solved a problem they were facing with Edge, allowing them to easily switch to Chrome. A user also suggested that the extension may work on other browsers such as Vivaldi and Brave and that it is a great alternative to accessing the Bing contact chat. Some users discussed their experience with Bing and Microsoft browsers, pointing out that they faced validation issues while accessing certain websites, but this new feature seems to help resolve the issue. Overall, users appreciated the convenience and functionality of this feature.

### Companies That Replace People with AI Will Get Left Behind

#### [Submission URL](https://hbr.org/2023/06/companies-that-replace-people-with-ai-will-get-left-behind) | 27 points | by [pseudolus](https://news.ycombinator.com/user?id=pseudolus) | [4 comments](https://news.ycombinator.com/item?id=36471749)

As the adoption of Artificial Intelligence (AI) continues to increase in many companies, job losses are expected to mount in the short term. However, companies that position themselves to innovate with AI will be able to mitigate this risk by creating new jobs and keeping unemployment low. Some companies are already using generative AI to empower employees to do more and increase productivity. A radical redesign of corporate processes may allow companies to spark all sorts of new value creation, ultimately creating new jobs. Innovation, not cutting costs, will position companies to thrive in the long run.

The discussion includes different perspectives on the impact of AI on employment and services. One user argues that introducing technology does not necessarily lead to job replacement but rather creates new job opportunities and increases productivity. Another user points out that the quality of customer support may suffer due to the use of AI. There is also a discussion about monopolies not prioritizing good service, and a user argues that AI cannot comprehensively understand problems and that replacing employees with AI creates competition within companies.

---

## AI Submissions for Sat Jun 24 2023 {{ 'date': '2023-06-24T17:11:16.412Z' }}

### Many in the AI field think the bigger-is-better approach is running out of road

#### [Submission URL](https://www.economist.com/science-and-technology/2023/06/21/the-bigger-is-better-approach-to-ai-is-running-out-of-road) | 207 points | by [pseudolus](https://news.ycombinator.com/user?id=pseudolus) | [287 comments](https://news.ycombinator.com/item?id=36462282)

As the cost and strain of training and running AI models grows, researchers are beginning to focus on making these models more efficient, rather than simply larger. Instead of pursuing a bigger-is-better approach, the goal is to get more performance out of fewer resources. One approach is to optimize trade-offs, such as cutting the number of parameters but training models with more data, so they are smaller, faster, and cheaper to use. Another option is to use a similar rounding technique, which has been shown to cut down memory consumption, while some users are fine-tuning general-purpose LLMs to focus on a specific task. Google has invented a different option, which involves extracting specific knowledge from a larger model into a smaller, specialized one.

The comments section features a debate about the nature of language processing and the limits of deterministic systems in understanding and modeling language. Some participants argue that human language understanding is complex and probabilistic, and that current language models are limited in their ability to recognize nuanced meanings and higher level concepts. Others suggest that human reasoning is a deterministic process, and that deterministic systems can model language effectively given enough training data. There is also discussion about the potential and limitations of machine learning in emulating human cognitive processes and generating language.

### Semantic MediaWiki

#### [Submission URL](https://www.semantic-mediawiki.org/wiki/Semantic_MediaWiki) | 79 points | by [Tomte](https://news.ycombinator.com/user?id=Tomte) | [35 comments](https://news.ycombinator.com/item?id=36462354)

Semantic MediaWiki is a free, open-source extension to MediaWiki that enables data storage and querying within a wiki's pages. It also acts as a powerful knowledge management system with numerous spin-off extensions. Data created with SMW can be published via the Semantic Web, allowing for seamless integration with other systems. Recently, the SMW sponsorship program was initiated and version 4.1.1 was released. The software is used by a variety of organizations, including KM-A Knowledge Management Associates and Professional Wiki Wiki Services.

Discussions on Hacker News revolved around the potential benefits and challenges of adopting Semantic MediaWiki, with some users highlighting its usefulness in terms of structured content and data management while others raised issues related to scalability and integration with other systems. Some users also shared their experiences in using Semantic MediaWiki for various applications, such as for sports data and knowledge organization.

### Creating an autonomous system for fun and profit (2017)

#### [Submission URL](https://blog.thelifeofkenneth.com/2017/11/creating-autonomous-system-for-fun-and.html) | 83 points | by [bsilvereagle](https://news.ycombinator.com/user?id=bsilvereagle) | [29 comments](https://news.ycombinator.com/item?id=36459881)

In a blog post, a networking and systems engineer discussed how he set up his own autonomous system with twofold motivation: wanting to lower his monthly hosting expenses by sharing a rack with a friend in Hurricane Electric's data center, and a challenge to set it up as an autonomous system. The post explains how the internet is an interconnected fabric of separate networks and each network only interconnects with others in clearly defined places. The post will appeal to networking enthusiasts and those interested in setting up their own internet service provider or web hosting service.

The comments section includes a discussion about the trustworthiness of network equipment and how companies should prioritize security. There is also a discussion about the power consumption of servers and how different companies handle it. Other comments discuss the limitations of Cisco's routers and the challenges involved with handling large amounts of data.

### Visual Studio’s IntelliSense list can now steer GitHub Copilot code completions

#### [Submission URL](https://devblogs.microsoft.com/visualstudio/github-copilot-visual-studio-intellisense/) | 115 points | by [samwillis](https://news.ycombinator.com/user?id=samwillis) | [28 comments](https://news.ycombinator.com/item?id=36459827)

GitHub Copilot is now integrated with Visual Studio's IntelliSense list, making it even easier for developers to explore and find the code completions they need. With this integration, changing a selection in IntelliSense provides GitHub Copilot with additional context about the code, allowing for more accurate predictions and multi-line code completions. Users can accept a Copilot completion by pressing TAB, and IntelliSense member ranking can also steer Copilot predictions. This update also allows for multi-line predictions to be previewed by pressing the Left CTRL button, and users are required to manually update to version 1.84 of Copilot.

### The Magic of Embeddings

#### [Submission URL](https://stack.convex.dev/the-magic-of-embeddings) | 113 points | by [gk1](https://news.ycombinator.com/user?id=gk1) | [21 comments](https://news.ycombinator.com/item?id=36454494)

Have you ever wondered how AI models can compare the meaning of two text strings? The answer is through embeddings, which are lists of numbers that describe a piece of text. By comparing these embeddings, we can search, cluster, recommend, classify, and even measure diversity among text strings. In this article, the writer takes a closer look at using OpenAI's text-embedding-ada-002 model to obtain embeddings, and how to efficiently store and search them using Pinecone or the Convex database. Whether you're working with text strings, images, or audio, embeddings can help unlock their hidden similarities and associations.

Commenters discuss the limitations of Google's semantic understanding and suggest alternative search methods, the challenges of cross-domain work with embeddings, and the trade-off between vector length and computational efficiency. Some commenters share positive experiences with embeddings in their work, and recommend databases like Convex for vector search.

---

## AI Submissions for Fri Jun 23 2023 {{ 'date': '2023-06-23T17:12:02.524Z' }}

### Open source licenses need to leave the 1980s and evolve to deal with AI

#### [Submission URL](https://www.theregister.com/2023/06/23/open_source_licenses_ai/) | 79 points | by [gumby](https://news.ycombinator.com/user?id=gumby) | [103 comments](https://news.ycombinator.com/item?id=36444854)

Open source licenses and free software have yet to adequately evolve to handle AI models, which raise grayer legal issues than code-centric software. With programming datasets so reliant upon open source and free software code, Stefano Maffulli, executive director at Open Source Initiative, among other tech leaders, is looking into ways to align AI and open source licenses for more clarity. Concerns over copyright infringement and proprietary licenses mean that tech companies producing AI-generated code will ultimately regard them as private IP, just as software code was considered the property of the software company in previous years. Other discussions pertained to how to license datasets involved with AI models, despite how they don't fit under traditional copyright models, and the difficulties found with open sourcing medical data versus commercial LLM datasets, which are typically black boxes. Several organizations are collaborating on defining a common understanding of open source AI principles that they intend to use to lobby legislative bodies. In the ensuing discussion, users debated the legal issues regarding AI models, such as whether AI-generated weights should be considered copyrightable, and whether there is a clear precedent in copyright law for AI. Additionally, the discussion highlighted the complexities of licensing AI and how some view AI-generated data as non-copyrightable.

### Millions of GitHub repos likely vulnerable to RepoJacking, researchers say

#### [Submission URL](https://www.bleepingcomputer.com/news/security/millions-of-github-repos-likely-vulnerable-to-repojacking-researchers-say/) | 124 points | by [pyeri](https://news.ycombinator.com/user?id=pyeri) | [47 comments](https://news.ycombinator.com/item?id=36452322)

AquaSec's security team, Nautilus, has issued a warning that millions of repositories on GitHub may be vulnerable to dependency repository hijacking, or RepoJacking. The attack involves a malicious actor registering a username under the name of an older repository, used by an organisation that has since changed their name or had a change in ownership. Any project using the dependency of the attacked project will subsequently fetch dependencies and code from the attacker-controlled repository, which could contain malware. AquaSec scanned major organisations and found exploitable cases in repositories managed by Google and Lyft, in which vulnerable dependencies pointed to rogue repositories.

The discussion includes various comments, such as the difficulty of implementing global namespaces, the use of SSL certificates, and the dangers of compromised domain names. Furthermore, there are some comments about the importance of maintaining package lockfiles and regularly auditing packages for vulnerabilities in package managers like npm. The discussion also touches on GitHub integration systems and the differences between package managers such as npm and Python. Finally, there is a comment about potential alternatives to GitHub.

### Twilight of the programmers?

#### [Submission URL](https://danielbmarkham.com/twilight-of-the-programmers/) | 82 points | by [signa11](https://news.ycombinator.com/user?id=signa11) | [69 comments](https://news.ycombinator.com/item?id=36445513)

As a programmer, there is something very important that we are losing in today's world, according to an article on Hacker News. The author believes that programming should be telling us more than we are telling it, and that we've got it all backwards. Unlike other professions, when a programmer encounters a situation where conflicting meanings happen, each of which is correct, it is a much more profound piece of information to provide the client. The article ends with a quote from a friend: "I think some of the best programs were essays, in the sense that the authors didn't know when they started exactly what they were trying to write."

The submission on Hacker News discusses how programming should be telling us more than we are telling it. The discussion that follows explores the limitations of the current programming system, especially in the context of part-time employees, and the importance of abstractions. Some commenters argue that abstractions are broken, but others point out that they can be useful in creating a better understanding of complex concepts. Additionally, some commenters assert that different professionals have different approaches to their work and that there is no universal logical world. Others suggest that the key to solving business problems is dependent on the depth of knowledge and intelligence of the business partners and that there should be more personal learning opportunities. Overall, the conversation touches on the role of programming in the real world and the complexities it faces.

### From word models to world models

#### [Submission URL](https://arxiv.org/abs/2306.12672) | 97 points | by [dimmuborgir](https://news.ycombinator.com/user?id=dimmuborgir) | [100 comments](https://news.ycombinator.com/item?id=36445197)

A group of researchers has proposed a framework for language-informed thinking that combines neural models of language with probabilistic models for rational inference. The framework, called "rational meaning construction", views linguistic meaning as a context-sensitive mapping from natural language into a "probabilistic language of thought" (PLoT), which is a general-purpose symbolic substrate for probabilistic, generative world modeling. The paper illustrates the framework in action through examples covering four core domains from cognitive science and extends the framework to integrate cognitively-motivated symbolic modules to provide a unified, commonsense thinking interface from language.

A group of researchers have suggested a framework for language-informed thinking that combines neural models of language with probabilistic models for rational inference. However, several users are skeptical about the approach, with one argument being that the models currently lack high-quality training data and cannot answer complex questions. Some users believe that the language models' ability to understand the world in a limited symbolic representation is not fully effective for intelligence. Still, others contend that natural language processing is a vital step towards achieving artificial general intelligence. Some users suggest that the research illustrates the limitations of current large language models, while others argue that such models can drink and perform very well. There are also discussions about the capacity of humans to comprehend complex thought and whether machines can do likewise.

### What is a transformer model? (2022)

#### [Submission URL](https://blogs.nvidia.com/blog/2022/03/25/what-is-a-transformer-model/) | 288 points | by [Anon84](https://news.ycombinator.com/user?id=Anon84) | [51 comments](https://news.ycombinator.com/item?id=36449788)

Transformers are driving a wave of advances in machine learning, earning them the nickname "transformer AI." These neural networks learn context and meaning by tracking relationships in sequential data, using a set of mathematical techniques called attention or self-attention to detect subtle influences among distant data elements. Transformers are already being used in a host of applications, from preventing fraud to improving healthcare, and can analyze sequential text, image, or video data. Stanford researchers call transformers "foundation models", as they see them driving a paradigm shift in AI and replacing the most popular types of deep learning models from just five years ago.

In the comments, users recommend resources for learning about transformers from building them from scratch to implementing them in a practical application, as well as discussing the capabilities and limitations of transformers compared to other models like CNNs and RNNs. Some users express concern about the potential negative impacts of transformers on society and the need for responsible research practices.

### AudioPaLM: A Large Language Model That Can Speak and Listen

#### [Submission URL](https://google-research.github.io/seanet/audiopalm/examples/) | 111 points | by [ml_basics](https://news.ycombinator.com/user?id=ml_basics) | [32 comments](https://news.ycombinator.com/item?id=36443676)

Google has introduced AudioPaLM, a new large language model for speech understanding and generation that combines text-based and speech-based language models. It uses a unified multimodal architecture that can process and generate text and speech, with applications including speech recognition and speech-to-speech translation. AudioPaLM was able to outperform existing systems for speech translation tasks and also demonstrated zero-shot speech-to-text translation for many languages not seen in training. The model significantly improved speech processing by leveraging the larger quantity of text training data used in pretraining. AudioPaLM inherits the capability to preserve paralinguistic information such as speaker identity and intonation from AudioLM and linguistic knowledge present only in text large language models such as PaLM-2.

The discussion on the submission included comments about the accuracy of the translation and the potential for spam calls to be intercepted with AudioPaLM. There were also discussions about the benefits of bilingual models for literal translations and how LLMs can represent different languages. Some commenters expressed skepticism towards the model's capabilities and the possibility of spying.

### Show HN: A package manager for AI plugins

#### [Submission URL](https://openpm.ai/) | 81 points | by [maccaw](https://news.ycombinator.com/user?id=maccaw) | [17 comments](https://news.ycombinator.com/item?id=36447683)

OpenPM is a package manager for AI plugins that simplifies the integration of various APIs. It offers a seamless process for exploring, publishing, and integrating APIs into AI platforms. With OpenPM, developers can easily discover and use new AI plugins, while API providers can publish their plugins to the registry and make them available to the community. Its API search function streamlines the integration process, removing the need for manual coding to connect various APIs. Overall, OpenPM represents a handy tool for streamlining AI development and integration processes.

The discussion around the OpenPM submission on Hacker News includes several topics related to AI development and integration. One commenter raises concerns about security and the supply chain attacks that can occur. Others discuss the packages available, including Cloudflare's Worker building AI plugins and WorkGPT, a framework for working with GPT functions. There are also discussions about API search and preview functionality, the adoption of AI plugins, and the role of OpenPM in streamlining AI development. Some comments also touch on the confusion between OpenPM and other similar tools like OpenAPI and OpenAI. Another contributor compares alternative platforms and suggests that OpenAI is leading the industry with its commitment to regulation and support for open-source alternatives.

---

## AI Submissions for Thu Jun 22 2023 {{ 'date': '2023-06-22T17:13:47.434Z' }}

### Generating SQL with LLMs for fun and profit

#### [Submission URL](https://iamnotarobot.substack.com/p/generating-sql-with-llms-for-fun) | 63 points | by [diego](https://news.ycombinator.com/user?id=diego) | [26 comments](https://news.ycombinator.com/item?id=36440760)

Language models are being used to generate SQL code for querying databases, but this could lead to potentially dangerous situations if the generated SQL code is not thoroughly checked. As demonstrated by Diego Basch, language models can easily generate queries that alter or drop tables, and even create infinite loops. While attempts to train the model to recognize risky queries have been made, there is still a risk of prompt injection leading to SQL injection. Basch suggests making the database read-only or creating a restricted role for the language model to use as a preventive measure. However, language models should not yet be trusted to generate executable code on the fly.

The discussion on the submission suggests that language models are still not yet able to generate executable code on-the-fly and should be used with caution. There are also concerns about data security and measures to prevent unauthorized access to databases. Suggestions were made to create a read-only database or create a restricted role for the language model to use. The discussion also touched on the importance of manual tracking and overseeing the process of generating SQL code. However, solutions such as making database read-only or creating restricted access have its limitations. There is also a mention about how language models should not be trusted to generate code for production systems before being thoroughly tested. The commenters also discussed the potential dangers of prompt injection leading to SQL injections. One solution proposed was to enforce reasonable time limits and database permissions and to add security measures such as read-write security in PostgreSQL, to prevent unauthorized access to databases. Finally, a few commenters raised concerns about proper communication to the audience, where the content generated by language models should be closely monitored to avoid misunderstandings or potential risks.

### Stability AI Launches Stable Diffusion XL 0.9

#### [Submission URL](https://stability.ai/blog/sdxl-09-stable-diffusion) | 166 points | by [seydor](https://news.ycombinator.com/user?id=seydor) | [100 comments](https://news.ycombinator.com/item?id=36435559)

Stability AI has announced the release of SDXL 0.9, marking a significant advancement in the Stable Diffusion text-to-image models. This new development offers a leap in creative use cases for generative AI imagery, providing hyper-realistic creations for films, television, music, instructional videos, design, and industrial use. SDXL 0.9 boasts a 3.5B parameter base model and a 6.6B parameter model ensemble pipeline, with a 1024x1024 resolution and the ability to generate highly detailed images. Despite its powerful output, SDXL 0.9 requires only a modern consumer GPU to run. The model is available on the Clipdrop platform, with API access coming soon. The discussions centered around the quality, resolution, use cases, and specifications of the new model, including its compatibility with different GPUs and processors. Some users speculated on the legal issues of using the model, while others discussed the possibility of generating specialized powered AI models.

### People paid to train AI are outsourcing their work to AI

#### [Submission URL](https://www.technologyreview.com/2023/06/22/1075405/the-people-paid-to-train-ai-are-outsourcing-their-work-to-ai/) | 334 points | by [kungfudoi](https://news.ycombinator.com/user?id=kungfudoi) | [221 comments](https://news.ycombinator.com/item?id=36432279)

A new study by the Swiss Federal Institute of Technology (EPFL) reveals that a significant portion of gig workers tasked with training AI models have been outsourcing their work to AI themselves. Researchers hired 44 people on Amazon Mechanical Turk to summarize medical research papers, and analyzed their responses using an AI model they trained themselves. They found that between 33% and 46% of workers used AI models like OpenAI’s ChatGPT. This percentage is expected to grow as AI models become more powerful and accessible. This could introduce further errors into already error-prone models and highlights the need for new ways to check whether data has been produced by humans or AI. Some commenters criticize the study for not accurately detecting whether responses were generated by humans or AI. Others discuss the importance of creating new ways to check whether data has been produced by humans or AI to ensure accuracy. There are also debates over the ethics of using AI to replace human workers. Some responders of the comment section shared their experiences working on Mechanical Turk, and a few cited sources to correct minor mistakes in the original post.

### Show HN: Launching Struct – Knowledge-Rich, AI-Powered Chat Platform

#### [Submission URL](https://www.struct.ai/blog/launching-struct-chat-platform) | 54 points | by [mrjn](https://news.ycombinator.com/user?id=mrjn) | [35 comments](https://news.ycombinator.com/item?id=36432743)

Chat platforms like Slack and Discord have revolutionized real-time communication, but they have inherent flaws that turn them into knowledge black holes. Information gets lost in the sea of endless messages, and finding them is akin to searching for a needle in a haystack. Manish R Jain, the founder of Dgraph Labs, introduces the CRISPY framework, outlining six principles that an ideal chat platform should uphold. Jain's new, innovative chat platform, Struct, embodies this framework, making real-time communication accessible, lasting knowledge for a change. Struct aims to reinvent the current chat platform's status quo and challenges the biggest problems that millions of users face daily.

The article discusses the inherent flaws of chat platforms like Slack and Discord, which turn them into knowledge black holes where messages can get lost, and information is hard to retrieve. The founder of Dgraph Labs, Manish R Jain, introduces the CRISPY framework that outlines six principles that an ideal chat platform should have. Jain's new chat platform, Struct, aims to address the problems that millions of users face daily and reinvent the current chat platform status quo. The comments on Hacker News highlight various aspects of the article, including the need for a chat platform that can handle knowledge, the struggle with finding information on large Discord servers, and the role of AI-powered search in chat platforms. The comments also raise concerns about the pricing and privacy policies of Struct.

### Aeon: A unified framework for machine learning with time series

#### [Submission URL](https://github.com/aeon-toolkit/aeon) | 119 points | by [megalodon](https://news.ycombinator.com/user?id=megalodon) | [23 comments](https://news.ycombinator.com/item?id=36432369)

Aeon Toolkit is a unified framework for machine learning with time series that is compatible with scikit-learn. It offers both classical techniques and the very latest algorithms for learning tasks like forecasting and classification. A broad library of time series algorithms, efficient implementations with numba, and interfaces with other time series packages make Aeon Toolkit an ideal choice for algorithm comparison. The latest release is version v0.3.0, and you can visit their website for documentation and installation instructions. Their code is licensed under the BSD-3-Clause license.

Aeon Toolkit, a unified framework for machine learning with time series, is the subject of a Hacker News discussion. The toolkit is compatible with scikit-learn and offers both classical techniques and the latest algorithms for tasks such as forecasting and classification. It has a broad library of time series algorithms, efficient implementations with numba, and interfaces with other time series packages, making it ideal for algorithm comparison. Commenters discuss various aspects of time series data and highlight the advantages of the Aeon Toolkit's friendly and flexible framework for developing machine learning models. They also mention other toolkits and packages, such as Prophet, sktime, Darts, and Weka, for working with time series data and developing machine learning models.

### OpenAI Lobbied the E.U. To Water Down AI Regulation

#### [Submission URL](https://time.com/6288245/openai-eu-lobbying-ai-act/) | 158 points | by [jlpcsl](https://news.ycombinator.com/user?id=jlpcsl) | [71 comments](https://news.ycombinator.com/item?id=36428121)

Documents obtained by TIME show that OpenAI lobbied to weaken forthcoming AI regulation in the EU while publicly calling for stronger AI guardrails. CEO Sam Altman has been touring world capitals and speaking about the need for global AI regulation, but behind the scenes, OpenAI proposed amendments that were later made to the final text of the EU law. OpenAI proposed that its general-purpose AI systems including GPT-3 and DALL-E 2 should not be considered "high risk," a designation that would subject them to stringent legal requirements including transparency, traceability, and human oversight. The lobbying efforts by OpenAI in Europe have not previously been reported, although Altman has recently become more vocal about the legislation.

The discussion on the submission revolves around the issue of OpenAI lobbying to weaken forthcoming EU AI regulation while publicly calling for stronger AI guardrails. Some comments accuse OpenAI of hypocrisy, while others argue that lobbying is a common practice among corporations and that regulations could stifle innovation. Some users criticize the lack of transparency in the AI generated content, while others express concerns about the power dynamics in the industry and the potential dangers of unregulated AI technology. Some users raise questions about the effectiveness of regulations in protecting the public and balancing the interests of all stakeholders involved. Overall, the discussion reflects a diversity of opinions and perspectives on the issue of AI regulation and its impact on industry, innovation, and society.

### Is Google reCAPTCHA GDPR Compliant?

#### [Submission URL](https://wideangle.co/blog/is-recaptcha-illegal-under-gdpr) | 143 points | by [openplatypus](https://news.ycombinator.com/user?id=openplatypus) | [222 comments](https://news.ycombinator.com/item?id=36430280)

The French data protection authority, CNIL, has imposed a penalty on Cityscoot for using Google's reCAPTCHA tool on their website and app without providing sufficient privacy information or seeking consent from visitors. The case raises questions about reCAPTCHA's compliance with EU data protection and privacy laws, with the CNIL concluding that the tool, which uses third-party cookies to distinguish bots from humans, requires consent. While some cookies are exempt from consent, this exception does not extend to cookies that are not strictly necessary, such as those used for analysis.

The discussion on the article clarifies technical terms like pixels, scripts, and beacons, and points out that some exceptions exist to the requirement of consent for cookies. The topic then shifts to CAPTCHA, with some contributors criticizing it for being a nuisance or hindrance to users, while others claim it is necessary to protect websites from spam and bot attacks. The IPv4 and IPv6 internet protocols, their limitations, and implementation challenges are also discussed. Finally, some participants share their experiences with CAPTCHA and captchas' effectiveness in preventing spam signups.

---

## AI Submissions for Wed Jun 21 2023 {{ 'date': '2023-06-21T17:14:52.372Z' }}

### Giving LLM’s a Backspace Token

#### [Submission URL](https://arxiv.org/abs/2306.05426) | 76 points | by [bilsbie](https://news.ycombinator.com/user?id=bilsbie) | [25 comments](https://news.ycombinator.com/item?id=36425375)

Researchers Chris Cundy and Stefano Ermon have developed a new method called SequenceMatch for autoregressive sequence modeling with backtracking. Unlike the traditional maximum-likelihood objective method, SequenceMatch involves imitation learning (IL), allowing for a variety of divergences between the generated and dataset sequences, including those generated out of distribution. The model can also incorporate backtracking by introducing a backspace action into the generation process. SequenceMatch training has shown improvements in text generation over MLE on language models. The paper outlines the SequenceMatch-$\chi^2$ divergence as a more suitable training objective for autoregressive models. The discussion on the submission revolves around the latency and interactivity of the models and how they could be used in practical applications. Some have raised concerns about the validity of the evaluation metrics used in the study, while others have suggested exploring Beam search as an alternative to backtracking.

### How RLHF Works

#### [Submission URL](https://www.interconnects.ai/p/how-rlhf-works) | 159 points | by [natolambert](https://news.ycombinator.com/user?id=natolambert) | [31 comments](https://news.ycombinator.com/item?id=36418807)

Reinforcement learning from human feedback (RLHF) works when two conditions are met, according to Nathan Lambert, who says there needs to be some signal showing that applying vanilla supervised learning only does not work and a complex optimization landscape that needs to slowly change over time to achieve success. The setup, however, is hard and Lambert explains it using the example of generating chat agents that are both helpful and not spewing toxic statements, which requires a capable language large model to follow instructions and applying a combination of fine-tunings and systems engineering for the harm reduction half. Data practices that can tank model performance, such as imitating models and using imperfect datasets, should also be avoided.

The submission is about reinforcement learning from human feedback (RLHF) and how it works only under specific conditions, as explained by Nathan Lambert. Successful RLHF requires a signal that demonstrates that applying supervised learning algorithms does not work and a complex optimization landscape that changes slowly over time. The discussion revolved around various topics such as the Direct Preference Optimization paper, resources for RLHF, the applications of RLHF, and the challenges associated with creating models that generate content that conforms to human values. One commenter stressed the importance of honesty and truthfulness in the development of models, while others discussed the limitations of RLHF based on the data and features of the model.

### Dropbox Dash, AI-powered universal search, and Dropbox AI

#### [Submission URL](https://blog.dropbox.com/topics/product/introducing-AI-powered-tools) | 34 points | by [marban](https://news.ycombinator.com/user?id=marban) | [7 comments](https://news.ycombinator.com/item?id=36418651)

Dropbox has launched two new AI-powered tools, Dash and AI, designed to aid knowledge workers in their tasks. The universal search Dash is a machine-learning powered tool for finding information across multiple platforms, such as Salesforce and Google Workspace, in one place. Dash can also organise links, create quick-access start pages and will soon answer users' questions and offer insights via generative AI. Meanwhile, AI summarises files such as contracts and meetings recordings; with one click, users can get a concise summary of the information they need. Alongside this, Dropbox has launched a $50m venture initiative called Dropbox Ventures for AI-focused startups.

Some of the commenters on the thread were critical of Dropbox's focus on AI tools, with one user suggesting that the company should concentrate on improving their core product. Another user expressed concern over the privacy policy, pointing out that Dropbox permits the analysis of the contents of a single document. Another user complained about the use of buzzwords like "cold storage." However, another commenter defended AI as a useful tool and congratulated Dropbox on their recent venture initiative for AI startups. The thread also included a link to a TechCrunch article with more information about Dropbox's launch of AI tools and venture initiative.

### Humans aren’t mentally ready for an AI-saturated ‘post-truth world’

#### [Submission URL](https://www.wired.com/story/generative-ai-deepfakes-disinformation-psychology/) | 290 points | by [pseudolus](https://news.ycombinator.com/user?id=pseudolus) | [485 comments](https://news.ycombinator.com/item?id=36417252)

As AI-powered tools become more ubiquitous in daily life, there are legitimate concerns about how these systems will impact individuals and society at large. For instance, AI-generated content could facilitate the spread of disinformation and further erode people's sense of trust. Additionally, as humans become more reliant on AI to perform tasks, we may become less capable of learning new things or completing tasks independently. However, some experts suggest that AI could eventually help alleviate the loneliness epidemic, as people may come to see AI as a friend. Ultimately, much more research is needed to understand the psychological effects of living in an AI-dominated world.

The comments present a range of opinions on the subject, including discussions about the effects of the internet, the importance of critical thinking and mistrust of corporations, and the relationship between language and thought. Some commenters suggest that the negative consequences of the widespread use of AI may be limited if people are able to adapt and find ways to control and regulate the technology. Others argue that the risks are too great and that we should proceed with caution.

### Show HN: GitHub Stats Dashboard Powered by GraphQL API and GitHub Action

#### [Submission URL](https://github.com/pingcap/ossinsight-lite) | 72 points | by [Hooopo](https://news.ycombinator.com/user?id=Hooopo) | [19 comments](https://news.ycombinator.com/item?id=36415839)

OSSInsight Lite is a customizable free GitHub stats dashboard based on TiDB Serverless. It offers personalized data pipelines to fetch GitHub activities, and can be deployed using a TiDB Serverless account and Vercel account. The dashboard can be used by developers to track their own GitHub statistics, and can be easily integrated into websites, reports, or profiles. The project is still in the early stages of development, but suggestions and discussions are welcome.

The submission is about OSSInsight Lite, a customizable free GitHub stats dashboard based on TiDB Serverless. The discussion includes comments on the difficulty of implementing creative statistics visualization, the additional charges for using TiDB Cloud, suggestions for building the stats dashboard based on snapshots on a GitHub page, and the accessibility of the dashboard's styling and design. Some users found the dashboard distracting and challenging to comprehend due to the background texture and font size. Nonetheless, others appreciated its visual styling and ease of use for tracking GitHub statistics.

### ChatGPT Told Masayoshi Son His Ideas Are Great. Now He’s Investing Big in AI

#### [Submission URL](https://www.wsj.com/articles/softbanks-masayoshi-son-says-hes-focusing-on-ai-inventions-621089ac) | 29 points | by [agomez314](https://news.ycombinator.com/user?id=agomez314) | [11 comments](https://news.ycombinator.com/item?id=36424574)

SoftBank CEO Masayoshi Son has revealed that his interest in investing in technology has been rejuvenated by his conversation with an AI chatbot called ChatGPT. The chatbot helped Son to validate his ideas for inventions, and he is now investing heavily in the field of artificial intelligence. Son's renewed focus comes as part of a wider industry trend that is seeing increasing investment in AI and machine learning technologies. However, some experts have warned of potential downsides to the widespread use of AI.

The discussion on the submission is mixed. Some agreed with SoftBank CEO Masayoshi Son's decision to invest in AI after his conversation with an AI chatbot called ChatGPT. They cited his ability to validate ideas, and that his investments in technology could bring innovation. Others criticized Son's investment strategy, believing that it focuses on fancy presentations instead of practical ideas. Some expressed concern about the potential downsides of AI and machine learning. There was also a discussion about Son's ability to invest and how his demonstration of ability correlated with investing abilities.

### You.com Launches YouPro – AI Chatbot with GPT-4 and Stable Diffusion XL

#### [Submission URL](https://www.forbes.com/sites/gilpress/2023/06/21/youcom-launches-subscription-service-for-cutting-edge-generative-ai-search-chatbot/) | 15 points | by [code2life](https://news.ycombinator.com/user?id=code2life) | [4 comments](https://news.ycombinator.com/item?id=36421257)

You.com, an innovative AI search engine and chatbot, has launched a subscription service called YouPro for $9.99 monthly or $119.99 annually. You.com was created to challenge Google's 93% market share of search and give users control over their search results and privacy. Since its launch in 2020, it has added applications, including generative AI, and attracted millions of users with the sudden popularity of conversational web search. The subscription model is intended to ensure long-term viability, but founder Richard Socher believes You.com's "chat-first, feature-complete" search engine is already ahead of the competition, with integrated apps improving the accuracy of its generative AI chatbot.

The discussion in the comments mainly revolved around the user experience (UX) of You.com's search engine. One user criticized the cluttered UX, especially in comparison to Google's streamlined interface. Another user suggested improving the UI by implementing features that require fewer clicks to access.  Another user complained about Cloudflare's security check, which was causing issues when searching on You.com repeatedly from the same IP address. Suggestions were made to disable the security check or use a different captcha service.

---

## AI Submissions for Tue Jun 20 2023 {{ 'date': '2023-06-20T17:12:58.204Z' }}

### RoboCat – A Self-Improving Robotic Agent

#### [Submission URL](https://www.deepmind.com/blog/robocat-a-self-improving-robotic-agent) | 168 points | by [l1n](https://news.ycombinator.com/user?id=l1n) | [65 comments](https://news.ycombinator.com/item?id=36406139)

DeepMind has created an AI agent called RoboCat, that learns to operate different robotic arms and perform a variety of tasks across different robots and self-generates new training data to improve its technique. The agent uses a multimodal model called Gato, which can process language, images, and actions in both simulated and physical environments. RoboCat learns much faster than other state-of-the-art models, as it can pick up a new task with as few as 100 demonstrations, reducing the need for human-supervised training. This is an important step towards creating a general-purpose robot. Commenters discussed the high costs of building robotic platforms and the difficulties of controlling motors using torque. There was also a debate about the limitations of UI automation tools and the hostility towards the automation sector.

### Building a Slack/Discord alternative with Tauri/Rust

#### [Submission URL](https://www.linen.dev/s/linen/t/12647025/building-a-slack-discord-alternative-with-tauri-rust) | 308 points | by [cheeseblubber](https://news.ycombinator.com/user?id=cheeseblubber) | [221 comments](https://news.ycombinator.com/item?id=36408633)

Linen, a search engine friendly alternative to Slack, has launched its Mac and Windows desktop clients using Tauri, a Rust-based Electron alternative. Tauri promises to yield smaller, more performant, and more secure desktop clients compared to Electron. While Tauri uses a Webview instead of chromium, Linen ran into compatibility challenges with NextJS and had to refactor all their code out of NextJS to a separate package to enable reuse between the NextJS app and the single page app Tauri built. Nonetheless, Linen was able to get a smooth developer experience on Tauri for the desktop, though it faced various challenges such as customizing the header and notification callbacks, among others. The Linux client suffered issues with fonts and emoji not building properly.

The discussion in the comments is centered around the comparison between Electron and Tauri, with some users defending Electron and stating that it works perfectly for them while others agree that Tauri is a more performant and secure alternative. The discussion also touches on the performance and resource usage of Electron versus other alternatives, as well as the issue of the increasing memory usage of modern software. Some users argue that the majority of people care about productivity and fast, snappy programs rather than high school dropouts, and that today's software runs worse on modern hardware while others disagree and say that today's engineers focus on UX features rather than actual performance.

### Predicting hit songs with 97% accuracy

#### [Submission URL](https://www.frontiersin.org/articles/10.3389/frai.2023.1154663/full) | 96 points | by [geox](https://news.ycombinator.com/user?id=geox) | [57 comments](https://news.ycombinator.com/item?id=36403334)

A team of researchers from Claremont Graduate University and Immersion Neuroscience has developed a machine learning model that can predict hit songs based on neurophysiological responses. Traditional methods of identifying hit songs involve measuring song elements from large databases, but the team took a different approach by measuring responses to a set of songs provided by a streaming music service. They found that a linear statistical model using two neural measures identified hits with 69% accuracy, while a machine learning model classified hit songs with 97% accuracy. The results demonstrate that applying machine learning to neural data can substantially increase classification accuracy for difficult-to-predict market outcomes. Some commenters pointed out problems with the small sample size and lack of diversity in songs used for the study, making the results less reliable. Others discussed the importance of properly validating synthetic datasets and the limitations of evaluation metrics like accuracy in imbalanced datasets. Finally, there was a discussion on the music industry and how difficult it is for independent artists and music genres outside of the mainstream to be represented on Billboard's charts.

### vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention

#### [Submission URL](https://vllm.ai/) | 275 points | by [wskwon](https://news.ycombinator.com/user?id=wskwon) | [40 comments](https://news.ycombinator.com/item?id=36409082)

Today, the UC Berkeley team introduced vLLM, an open-source library for fast LLM inference and serving. This library promises to serve LLM models fast, cheaply, and efficiently, even for small research teams with limited compute resources. The team achieved up to 24x higher throughput than HuggingFace Transformers, the most popular LLM library, and up to 3.5x higher throughput than the previous state of the art, HuggingFace Text Generation Inference (TGI), in their experiments. This breakthrough was due to the library's cutting-edge attention algorithm, PagedAttention, which allows for efficient management of the large and dynamic key-value cache necessary for autoregressive decoding. PagedAttention also enabled efficient memory sharing, making complex sampling algorithms practical for LLM services. The library is already deployed at Chatbot Arena and Vicuna Demo and is available on GitHub.

The discussion in the comments includes technical details about the library's attention algorithm, PagedAttention, and its memory optimization techniques. Some users express interest in applying the library to their own projects, while others speculate about its practical use cases and potential limitations. Overall, the community is impressed with the vLLM library's capabilities and looks forward to further developments in the field.

### Show HN: Autolabel, a Python library to label and enrich text data with LLMs

#### [Submission URL](https://github.com/refuel-ai/autolabel) | 143 points | by [nihit-desai](https://news.ycombinator.com/user?id=nihit-desai) | [17 comments](https://news.ycombinator.com/item?id=36409201)

Refuel AI has released a new open-source library called AutoLabel that allows the user to label and clean text datasets with large language models (LLMs). The library supports all GPT-3.5 and GPT-4 models, and users can easily specify labeling guidelines and model parameters in a JSON config. AutoLabel promises to save time and money compared to manual labeling efforts and provides a simple three-step process for labeling data. The library is MIT-licensed and available on GitHub. The main concerns are around privacy when using LLMs and potential inaccuracies in labeling. Some commenters recommend using self-hosted open-source LLMs or the openAI API, while others suggest that AutoLabel could be integrated with function calling to improve the labeling quality. Additionally, a commenter points out that Refuel provides confidence scores for LLMs but does not provide token-level probabilities. The original post on Hacker News linked to a benchmarking report and a GitHub repository. Some commenters note that the post may be self-promotion and not intended to share LLM labeling feedback with the community.

### Petaflops to the People: From Personal Compute Cluster to Person of Compute

#### [Submission URL](https://www.latent.space/p/geohot) | 66 points | by [tosh](https://news.ycombinator.com/user?id=tosh) | [10 comments](https://news.ycombinator.com/item?id=36407269)

In the latest episode of Latent Space: The AI Engineer Podcast, host Jade Le and George Hotz of the tiny corp discuss the company's efforts to take on major players like Nvidia, Google, and PyTorch, as well as its recent announcement of the tinybox, a luxury AI computer aimed at local model training and inference. They delve into the technical details of the deep learning framework tinygrad and the importance of optimizing instruction execution for GPU. Additionally, they touch on the potential role of personal compute clusters in the future of home intelligence.

The discussion includes technical details of the deep learning framework, tinygrad and the importance of optimizing instruction execution for GPU. They also touch on the potential role of personal compute clusters in the future of home intelligence. In the comments, there is a detailed discussion about quantization research benchmarks, perplexity testing, Fabrice Bellard's methods, and the challenges with FPGA systems. Additionally, some users express surprise at Hotz's knowledge and fluency in discussing technical details, while others criticize the half-baked implementation of some of his ideas.

### Google warns its own employees: Do not use code generated by Bard

#### [Submission URL](https://www.theregister.com/2023/06/19/even_google_warns_its_own/) | 299 points | by [Brajeshwar](https://news.ycombinator.com/user?id=Brajeshwar) | [152 comments](https://news.ycombinator.com/item?id=36399021)

Google has warned its employees to avoid using code generated by its AI chatbot, Bard, due to privacy and security risks. The move has raised concerns about the suitability and reliability of privately developed AI tools. Developers may not be able to trust AI tools if even the creators themselves don't use them. The search and ads company advised its staff not to disclose confidential information or use code generated by Bard due to potential buggy programs or difficult-to-fix software. Meanwhile, voice recognition software developer Nuance, backed by Microsoft, has been sued by three people for allegedly recording and using people's voices without permission in violation of the California Invasion of Privacy Act.

The comments on Hacker News discussed the risks and implications of monorepos, proprietary data privacy, legal liabilities, the legality of LLMs, and the potential impact on productivity. There were also concerns about copyright infringement and the need for companies to address and mitigate systemic risks. Overall, there were mixed views on the topic, with some emphasizing the need for caution and others stressing the importance of innovation and productivity.

### Stackoverflow is investing into baking GenAI

#### [Submission URL](https://stackoverflow.co/labs/) | 86 points | by [rounakdatta](https://news.ycombinator.com/user?id=rounakdatta) | [101 comments](https://news.ycombinator.com/item?id=36404743)

Stack Overflow Labs, the experimental arm of the popular Q&A website for developers, has been busy exploring the use of AI to improve the platform and developer experience. Some of the projects include: Experiment Question Formatting Assistant, which uses AI to improve the quality and format of questions to make reviewing easier; Title Suggestions, which utilizes AI to generate more descriptive and accurate question titles; and Chat Decipher, which can extract question topics from chat transcripts and group similar ones together. Additionally, the results of Stack Overflow's 2023 Developer Survey shed light on how developers and technologists feel about AI/ML. The website's Senior Data Scientist also delves into the creation of their course recommendation engine powered by AI. Meanwhile, their CEO will be making exciting announcements at the upcoming WeAreDevelopers event. With their mission to give technologists more time to create amazing things and make the coding field accessible to all, Stack Overflow Labs is always looking for new ways to merge emerging technologies with their platforms and services. There has been a discussion in the comments about the hype around blockchain and AI, and how they may be misunderstood and overhyped. There was also a discussion about the effectiveness of Stack Overflow's AI experiments, with some commenters offering criticism and others defending the platform's efforts to utilize AI.

### GPS alternative taps cosmic rays for underground or underwater navigation

#### [Submission URL](https://newatlas.com/technology/gps-alternative-muon-cosmic-rays-underground-underwater-navigation/) | 49 points | by [wjSgoWPm5bWAhXB](https://news.ycombinator.com/user?id=wjSgoWPm5bWAhXB) | [10 comments](https://news.ycombinator.com/item?id=36402086)

Researchers at the University of Tokyo have developed a proof-of-concept navigation system called the muometric wireless navigation system (MuWNS) that uses cosmic rays to track movement underground and underwater with precision of a few metres. Unlike GPS, which bounces signals off rocks, walls, and water, MuWNS tracks particles called muons that pass through solid materials. The particles are generated when cosmic rays enter the Earth’s atmosphere and produce a cascade of secondary particles. By tracking the paths of muons picked up by reference stations and a handheld detector, the scientists were able to trace the scientist's position with a high degree of precision deep inside a multi-story building.

One user points out that the technology is currently expensive and difficult to implement in real-time applications. Another user questions the need for real-time tracking in underground or underwater scenarios and suggests that a stable recording receiver may be more practical. The potential of using the technology in microchip and commercial manufacturing is also discussed, as well as the limitations of the technology in areas without consistent connection. Finally, a user comments on a related underwater experiment in Newfoundland.