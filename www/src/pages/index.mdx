import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Sun Oct 15 2023 {{ 'date': '2023-10-15T17:10:35.576Z' }}

### MemGPT: Towards LLMs as Operating Systems

#### [Submission URL](https://arxiv.org/abs/2310.08560) | 210 points | by [belter](https://news.ycombinator.com/user?id=belter) | [117 comments](https://news.ycombinator.com/item?id=37894403)

Researchers from various institutions have proposed a technique called virtual context management to extend the utility of large language models (LLMs), such as GPT, in tasks like extended conversations and document analysis. LLMs are often limited by their context windows, which restrict the amount of information they can process. Inspired by hierarchical memory systems in traditional operating systems, the researchers have developed MemGPT (Memory-GPT), an operating system-like system that intelligently manages different memory tiers to provide extended context within the LLM's limited context window. MemGPT uses interrupts to manage control flow between itself and the user. The researchers evaluated MemGPT in two domains: document analysis and multi-session chat, and found that MemGPT can effectively analyze large documents and create conversational agents that remember and evolve dynamically through interactions with users. The researchers have released the MemGPT code and data for further experiments.

The discussion around this submission covers a range of topics. 

- Users discuss the limitations of large language models (LLMs) and the potential benefits of extending their context windows through techniques like MemGPT. Some users share their experiences with similar projects and suggest different approaches to context management.
- Some users express their appreciation for the work and offer positive feedback to the author.
- Others discuss the nature of AI models and the challenges in their development and deployment. There are discussions about the reliability of AI models, the importance of replicability in scientific publishing, and the potential risks associated with AI technology.
- There is also a brief discussion about the application of AI in the cryptocurrency industry and the potential impact on different sectors.
- Finally, there are a few comments exploring the analogy between the AI industry and the gold rush, and a humorous exchange about selling shovels in a gold rush.

Overall, the discussion covers a range of perspectives and insights related to large language models, AI technology, and its potential implications.

### Teaching Apple Cyberdog 1.0 new tricks (featuring OpenDoc)

#### [Submission URL](http://oldvcr.blogspot.com/2023/10/teaching-apple-cyberdog-10-new-tricks.html) | 126 points | by [classichasclass](https://news.ycombinator.com/user?id=classichasclass) | [55 comments](https://news.ycombinator.com/item?id=37894030)

In a blast from the past, the author revisits Apple's Cyberdog, a web browser and internet suite that has long been forgotten. Cyberdog was unique in that it allowed developers to create their own components, such as viewers and UI elements, using Apple's OpenDoc embedding. OpenDoc was a standard compound document format that allowed for an object-oriented approach to document creation. The goal was to have reusable components that could be pulled into a document and maintain their own views and state. Cyberdog was essentially a demonstration of OpenDoc's capabilities, and it was released as part of Apple's Project Amber, which aimed to create a next-generation technology platform called Taligent. Despite Apple's efforts, OpenDoc did not gain much traction with developers or users, and it was seen as a competitor to Microsoft's Object Linking and Embedding (OLE) technology. Apple eventually released Cyberdog as an internet suite, capitalizing on the popularity of internet document creation. However, Cyberdog also faded into obscurity, and today it serves as a reminder of Apple's ambitious but unsuccessful foray into component-based document creation.

The discussion on this article covers various aspects of Apple's Cyberdog and OpenDoc technology.

- One commenter mentions that they remember the Apple Dylan IDE requiring 24MB of RAM, which was a significant amount at the time. They also mention that Cyberdog was a fascinating project but ultimately faced difficulties due to its large RAM requirements.
- Another commenter shares links to screenshots and explanations of Cyberdog, as well as a mention of the SK8 programming language.
- There is a discussion about Steve Jobs' response to OpenDoc versus Java, with a correction made that Jobs was not yet CEO at the time.
- A commenter expresses relief in reading the well-written article but admits that they still don't fully understand the supposed problem that OpenDoc was meant to solve.
- Some commenters compare OpenDoc to Microsoft's OLE technology, with one mentioning that OpenDoc aimed to improve cross-platform interoperability.
- The complexity and memory requirements of OpenDoc are mentioned, with one commenter stating that it required a significant amount of RAM to function.
- One commenter shares their personal experience with using Apple Cyberdog and praises its capabilities.
- The topic of connecting Cyberdog with Microsoft Internet Explorer is brought up, with a link shared to an archived page about it.
- There are mentions of the CI Labs and its involvement with OpenDoc.
- Commenters discuss the creativity and whimsical nature of the icons used in old Mac applications.
- The conversation touches on the history of Cyberdog and its features, as well as the advancements in instant search technology.
- A reference is made to a character named Preston from a 1995 Wallace and Gromit short film.
- The discussion briefly touches on compound documents and the challenges they presented in the '90s.
- A commenter mentions a Japanese system that recently graduated from paper documents to digital ones.
- The discussion also includes mentions of the Kantara project and the App Store.

Overall, the comments cover a range of experiences, memories, and opinions related to Apple's Cyberdog and OpenDoc technology.

### Show HN: Deep Chat – AI chat component

#### [Submission URL](https://github.com/OvidijusParsiunas/deep-chat) | 65 points | by [ovisource](https://news.ycombinator.com/user?id=ovisource) | [4 comments](https://news.ycombinator.com/item?id=37889444)

Deep Chat is a customizable AI chat component that can be easily integrated into your website. It allows you to connect to popular AI APIs like OpenAI, HuggingFace, and Cohere, or even to your own custom service. With Deep Chat, you can send and receive messages, exchange files, capture photos via webcam, record audio, and even convert speech to text and vice versa. The latest update includes support for custom elements in message bubbles, allowing you to add suggestion buttons, charts, maps, or any other HTML element you desire. Deep Chat is highly customizable and can be used with any major UI framework or library. To get started, simply install the npm package and add the Deep Chat component to your markup.

In the discussion, user "vrtclbx" expressed their appreciation for the webcam and microphone functionality in the chat component and mentioned that they found it useful for capturing photos and recording audio. User "jlthln" suggested adding a feature that allows for the integration of a recommendation engine to enhance the product. User "vsrc" thanked "jlthln" for the suggestion and mentioned that they are currently calling external services to handle interactions with models. They appreciated the suggestion and said that they are planning to add functionality to host models entirely in the browser, which would greatly benefit from the recommendation engine capabilities.

### Margaret Atwood Reviews a Margaret Atwood Story by AI

#### [Submission URL](https://thewalrus.ca/margaret-atwood-ai/) | 88 points | by [goldenskye](https://news.ycombinator.com/user?id=goldenskye) | [68 comments](https://news.ycombinator.com/item?id=37894072)

In a recent article, the author dives into the anxieties surrounding generative AI and its potential impact on writers. They question whether AI chatbots will devour our literature, infiltrate our minds, and take over our jobs. However, the author provides some reassurance by highlighting the current limitations of AI chatbots, such as their inability to reflect or grasp metaphor and punctuation. To demonstrate this, the author shares two examples of literary attempts by AI chatbots, a poem and a short story. While these examples certainly have their quirks and inaccuracies, they serve as a reminder that AI chatbots are not yet ready to replace human authors. So, writers can take heart knowing that their creative skills are safe from the clutches of AI, at least for now. Ultimately, while the fear of AI may loom large in some writers' minds, it seems that human creativity still has the upper hand.

The discussion on this submission focuses on various aspects of AI-generated writing and the limitations of current AI models. One commenter notes that the 10x improvement mentioned in the article regarding ChatGPT is not adequately supported and questions the validity of such claims. Another points out that GPT-4 does not possess the intelligent reasoning capabilities that GPT-35 lacks, emphasizing the need to differentiate between different versions of AI models. Some commenters express skepticism about AI-generated content, while others argue that AI can assist human writers and bring new perspectives. The debate also touches on the potential shortcomings of AI models in replicating human memory and recall. Additionally, there are discussions about Margaret Atwood's writing style and the similarities between AI-generated content and children's stories or specific authors like H.P. Lovecraft.

### Scientists begin building AI for scientific discovery using tech behind ChatGPT

#### [Submission URL](https://techxplore.com/news/2023-10-scientists-ai-scientific-discovery-tech.html) | 38 points | by [gardenfelder](https://news.ycombinator.com/user?id=gardenfelder) | [14 comments](https://news.ycombinator.com/item?id=37890570)

An international team of scientists, including researchers from the University of Cambridge, has launched a research collaboration called Polymathic AI to develop an AI-powered tool for scientific discovery. Leveraging the technology behind ChatGPT, the team aims to build an AI that can learn from numerical data and physics simulations to assist scientists in various scientific fields. By starting with a large, pre-trained model, Polymathic AI aims to make AI and machine learning more accessible and effective in scientific research. The team includes experts in physics, astrophysics, mathematics, artificial intelligence, and neuroscience from institutions such as the University of Cambridge, Simons Foundation, New York University, Princeton University, and Lawrence Berkeley National Laboratory. Polymathic AI's goal is to connect different scientific subfields and apply multidisciplinary knowledge to solve complex scientific problems. The project will prioritize transparency and openness, aiming to democratize AI for scientific analysis across various domains.

The discussion surrounding the submission includes various viewpoints. 

One commenter expresses skepticism about the hype surrounding AI, stating that while it may have many applications, it is unlikely to replicate the scientific discoveries made by great scientists like Albert Einstein. They believe that scientific research requires tools for searching large spaces and independently verifying results.

Another commenter mentions their involvement in computational chemistry and shares a link to their GitHub repository.

There is a discussion about the BLOOM language model, where someone suggests joining the BLOOM collaboration and asks if the BLOOM model will receive funding. Another commenter indicates that they are unsure about joining the BLOOM collaboration and mentions that the BLOOM model can answer questions.

Yet another commenter argues that Language Models (LLMs) are capable of innovation and discovering new concepts. They believe that innovation can come from simplifying existing concepts and challenge the notion that discovery requires complex methods.

In response, another commenter disagrees, stating that complex physics relies on a vast number of relationships published by human knowledge.

Overall, the discussion touches on concerns regarding the potential of AI in scientific research, the importance of independent verification, the capabilities of BLOOM language model, and the role of LLMs in innovation and scientific discovery.

### 2nd law of infodynamics and its implications for simulated universe hypothesis

#### [Submission URL](https://pubs.aip.org/aip/adv/article/13/10/105308/2915332/The-second-law-of-infodynamics-and-its) | 14 points | by [imhoguy](https://news.ycombinator.com/user?id=imhoguy) | [5 comments](https://news.ycombinator.com/item?id=37893189)

In a recent article published in AIP Advances, Melvin M. Vopson explores the implications of the second law of infodynamics for the simulated universe hypothesis. The simulated universe hypothesis suggests that our entire reality is a simulated construct. While lacking concrete evidence, this idea is gaining popularity in both scientific and entertainment circles. The second law of infodynamics, discovered in 2022, further supports this possibility by providing a new framework for studying the intersection of physics and information. Vopson examines the applicability of this law to various domains, including digital information, genetic information, atomic physics, mathematical symmetries, and cosmology. By re-examining the second law of infodynamics, Vopson provides scientific evidence that appears to underpin the simulated universe hypothesis. This research opens up new avenues for understanding the nature of our reality and the role of information within it.

In the discussion, user "SideburnsOfDoom" raises a question about the second law of infodynamics, stating that it may be ignorant to assume that the second law implies the first law of infodynamics, which is a fundamental principle in science. User "az09mugen" agrees with SideburnsOfDoom, stating that the first law of infodynamics is indeed foundational. User "c22" adds that in 2022, a new fundamental law of physics was proposed and demonstrated, called the law of information dynamics, which simplifies the second law of infodynamics. They also provide a link to an article on the topic. User "gus_massa" mentions that the law may have different applications in various domains and gives an example related to DNA replication. "SideburnsOfDoom" remarks that the discussion is becoming too technical and compares it to a college-level physics class.

---

## AI Submissions for Sat Oct 14 2023 {{ 'date': '2023-10-14T17:11:08.171Z' }}

### ChatGPT’s system prompts

#### [Submission URL](https://github.com/spdustin/ChatGPT-AutoExpert/blob/main/System%20Prompts.md) | 737 points | by [spdustin](https://news.ycombinator.com/user?id=spdustin) | [365 comments](https://news.ycombinator.com/item?id=37879077)

Introducing ChatGPT-AutoExpert: a new language model that aims to help with troubleshooting car-related issues. Developed by GitHub user spdustin, this AI model leverages the power of GPT-3 to provide expert advice on automotive problems. With 942 stars and counting, this project has piqued the interest of the developer community. Whether you're dealing with a mysterious engine noise or perplexed by a dashboard warning light, ChatGPT-AutoExpert aims to be your virtual car expert. So, the next time your vehicle acts up, perhaps this AI can help you find the solution.

The discussion starts with a comment pointing out that the methods used in the chat model do not handle comments threads well and suggests using Jupyter Notebook for advanced data analysis. Another user clarifies that the Python version installed on their system is required for the model to work. There is a discussion about assumptions made when talking about single prompts and pre-processing, as well as handling of grammatical errors and the use of code interpreters. The topic of hallucinations and the Turing Test is also brought up, with some users expressing skepticism and others discussing the potential for harmful or threatening language generation. There are also comments about finding OpenAI's internal evaluation prompts interesting and the limitations of single-instance conversational understanding. Some users discuss the behavior of the model and mention specific prompts that yielded helpful answers. The potential misuse of the model in harmful or deceptive ways is also addressed. There is a discussion about the current state of AI and its ability to hold human-like conversations, as well as the training data and the importance of context. Some users mention specific protein measurements and the programming language INTERCAL. The conversation ends with a comment about the success of GPT-4 and the fascinating experience of watching people interact with the chat model.

### Multi-modal prompt injection image attacks against GPT-4V

#### [Submission URL](https://simonwillison.net/2023/Oct/14/multi-modal-prompt-injection/) | 204 points | by [simonw](https://news.ycombinator.com/user?id=simonw) | [61 comments](https://news.ycombinator.com/item?id=37877605)

The latest blog post by Simon Willison discusses the new GPT-4V model, which allows users to upload images as part of their conversations. While this feature brings about exciting possibilities, it also opens up a new avenue for prompt injection attacks. Willison provides several examples to illustrate this. In one instance, he uploads a photo from the "50th Annual World Championship Pumpkin Weigh-Off" and asks the model how big the pumpkin is. The model accurately deduces the weight based on the digital display next to the pumpkin. Another example shows how an image containing additional instructions can override the user's prompt and misdirect the model's response. Even more concerning is the use of visual prompt injection for exfiltration attacks. By including instructions in an image, an attacker can trick the model into leaking potentially private data to an external server. Willison points out that he was surprised to see this example work, as he had assumed OpenAI would have implemented safeguards against it. He also highlights an instance where a hidden prompt injection attack is embedded in an image. This attack goes unnoticed as the text blends with the background color. Willison concludes by emphasizing that prompt injection still remains a problem, as language models inherently rely on the instructions given to them. Given their gullibility, it is difficult to differentiate between good and bad instructions, making it an ongoing challenge to prevent prompt injection attacks.

The discussion on Hacker News revolves around the capabilities and vulnerabilities of the GPT-4V model discussed in the submitted blog post. Some users express surprise and skepticism about the model's abilities, while others question OpenAI's approach and its understanding of GPT models. There is also a discussion about prompt injection attacks and the potential risk they pose. Some users criticize the blog post, claiming that it exaggerates the vulnerability of language models and their potential impact. Others discuss the use of LLMs (Large Language Models) for tasks like self-driving cars and express their concerns about the future implications of these models. The discussion also touches on the blocking of external content and security measures implemented by OpenAI. Overall, the conversation centers around the capabilities and limitations of language models and their potential risks and benefits.

---

## AI Submissions for Fri Oct 13 2023 {{ 'date': '2023-10-13T17:10:12.313Z' }}

### TimeGPT-1

#### [Submission URL](https://arxiv.org/abs/2310.03589) | 379 points | by [PaulHoule](https://news.ycombinator.com/user?id=PaulHoule) | [115 comments](https://news.ycombinator.com/item?id=37874891)

Researchers Azul Garza and Max Mergenthaler-Canseco have developed TimeGPT-1, a groundbreaking deep learning model for time series analysis. In their paper titled "TimeGPT-1," the authors demonstrate the model's ability to generate accurate predictions for diverse datasets not encountered during training.

The team evaluated TimeGPT-1 against various statistical, machine learning, and deep learning methods and found that its zero-shot inference outperformed them in terms of performance, efficiency, and simplicity. The study suggests that insights from other domains of artificial intelligence can be effectively applied to time series analysis.

This research opens up new possibilities for democratizing access to precise predictions and reducing uncertainty in time series forecasting. By leveraging the capabilities of recent advancements in deep learning, large-scale time series models like TimeGPT-1 have the potential to revolutionize the field.

The discussion on the submission "Introducing TimeGPT-1: The First Foundation Model for Time Series" covers various topics related to time series forecasting and the effectiveness of different machine learning models in this domain.

One commenter shares their experience working on credit card processors and mentions the advantages of using deep learning models like TimeGPT-1 for time series forecasting. They highlight that traditional numeric forecasting approaches have limited benefits compared to machine learning models.

Another comment discusses the use of XGBoost and MLP models for time series forecasting, particularly in multi-step forecasting. They mention the challenges of using aggregated time steps in regression models and suggest using multi-output regression models or forecasting frameworks like VARIMAX.

A commenter raises skepticism about the performance of high-performing time series models, stating that training time series models is limited by the fundamental understanding of the underlying structure of the data.

There is a discussion about the use of Transformers and attention mechanisms in time series modeling. One commenter asks about the effectiveness of Transformers for longer sequence lengths, to which another commenter explains that Transformers handle longer sequences well by using attention mechanisms.

Another commenter suggests that the presented models are foundational and that the field of time series forecasting can benefit from using them.

A few comments draw connections between time series forecasting and other fields like psychology (ANOVA, MANOVA), trading and market forecasting using GPT-powered models, and the use of deep learning in the financial industry.

There is also a discussion about the limitations of time series forecasting, with one commenter mentioning the challenges of predicting non-stationary behavior and the possibility of overfitting.

Overall, the discussion covers a wide range of topics related to time series forecasting, including the effectiveness of different machine learning models, the challenges of modeling longer sequences, and the potential applications of advanced models like TimeGPT-1.

### iSponsorBlockTV v2: SponsorBlock for TVs and game consoles

#### [Submission URL](https://github.com/dmunozv04/iSponsorBlockTV) | 242 points | by [dmunozv04](https://news.ycombinator.com/user?id=dmunozv04) | [97 comments](https://news.ycombinator.com/item?id=37873749)

DMunozv04 has developed iSponsorBlockTV, a sponsor block client for all YouTube TV clients. This project, written in asynchronous Python, allows users to skip sponsor segments in YouTube videos while using a YouTube TV device. It connects to the device, monitors its activity, and skips any sponsor segment using the SponsorBlock API. It can also skip or mute YouTube ads. The compatibility of iSponsorBlockTV includes Apple TV, Samsung TV (Tizen), LG TV (WebOS), Google TV, Nintendo Switch, and PlayStation 4/5, among others. This open-source project has received 464 stars and 25 forks on GitHub. You can find more information and contribute to the project on GitHub.

The discussion about the iSponsorBlockTV project on Hacker News revolves around the benefits and drawbacks of skipping sponsor segments and advertisements on YouTube videos. Some users express their appreciation for sponsor block tools like iSponsorBlockTV, mentioning the advantages of skipping interruptions and distractions for better focus and consumption of content. Others discuss the potential negative effects on content creators and question the need for interacting with sponsor segments on a single video basis.

There is also a conversation about the attempts by YouTube to prevent ad-blocking at the browser level and the potential impact on user experience. Users share their experiences with blocking ads and the frustration with intrusive messages and playlists interrupting video playback.

The discussion delves into the debate on the monetization of YouTube and the role of advertisements. Some users express their preference for ad-free content through paid subscriptions, while others discuss the financial incentives for creators and the effectiveness of advertising in supporting content creation.

There are mentions of other tools, such as Overcast for podcasts, that have features to skip ads and intros. Users share their experiences with Overcast's skipping features and discuss its configurability.

Overall, the discussion explores the pros and cons of skipping sponsor segments and advertisements, considering the impact on content creators, user experience, and the financial aspects of content monetization.

### Chat Control 2.0: EU set to approve end of private messaging, secure encryption

#### [Submission URL](https://www.patrick-breyer.de/en/chat-control-2-0-eu-governments-set-to-approve-the-end-of-private-messaging-and-secure-encryption/) | 110 points | by [ssklash](https://news.ycombinator.com/user?id=ssklash) | [21 comments](https://news.ycombinator.com/item?id=37873996)

EU governments are preparing to approve a controversial bill known as "Chat Control 2.0," which would effectively end private messaging and secure encryption. The proposed regulation would require providers of messaging, email, and chat services to automatically search all private messages and photos for suspicious content and report it to the EU. The EU Council Presidency has suggested a minor concession to search for previously classified Child Sexual Abuse Material (CSAM) initially, with less reliable technology for unknown imagery or conversations to be introduced later. However, critics argue that this proposal would fundamentally compromise secure encryption and invade users' privacy. They also believe that indiscriminate scanning of private communications would violate fundamental rights and fail to target actual criminals. Additionally, opponents warn that pushing criminals to secure, decentralised communication channels could make it even harder to identify and rescue victims of child sexual abuse. The proposal is set to be discussed by ambassadors and potentially adopted by ministers next week.

The discussion on this submission revolves around the implications of the proposed "Chat Control 2.0" bill in the EU. Some users argue that the majority of mass surveillance laws are used for general law enforcement rather than stopping child sexual abuse, making the new regulation unnecessary and potentially invasive. Others criticize the EU's competence in enforcing such laws and express concerns about its impact on privacy and individual rights. One user mentions the corruption of Swedish politicians by an American software company for lobbying purposes. The discussion also touches on the need for hardware-level encryption and the potential need to install Linux on computers to maintain control over encryption methods. Some users highlight the importance of legal protections for privacy, while others argue that proposals to protect privacy are often approved voluntarily. The constitutionality of data retention laws is also brought up, with one user expressing hope that WhatsApp, Signal, and other private messaging providers will resist any ban on end-to-end encryption. The discussion also delves into the backdoor policies of different messaging platforms, with concerns raised about Telegram's default chats not being end-to-end encrypted.

### Protecting customers with generative AI indemnification

#### [Submission URL](https://cloud.google.com/blog/products/ai-machine-learning/protecting-customers-with-generative-ai-indemnification) | 112 points | by [stravant](https://news.ycombinator.com/user?id=stravant) | [60 comments](https://news.ycombinator.com/item?id=37872147)

In an announcement to customers, Google Cloud has introduced generative AI indemnification to protect users from copyright claims related to the use of generative AI. The company will assume responsibility for any potential legal risks involved in copyright challenges and will provide comprehensive coverage for customers using generative AI products. The indemnification includes two key components: the first focuses on Google's use of training data, and the second covers the generated output of foundation models. This move aims to instill trust and confidence in Google Cloud's generative AI offerings and demonstrates the company's commitment to customer protection in this evolving technology landscape.

The discussion on the Hacker News submission revolves around Google Cloud's introduction of generative AI indemnification to protect users from copyright claims related to the use of generative AI. Here are some notable points from the discussion:

- Some users mention that other companies like Adobe and Microsoft have also introduced similar indemnification measures for their customers.
- There is debate about the legal strategy of large companies providing indemnification and whether smaller companies can afford such protection.
- Some users express doubts about the effectiveness of AI-generated models in protecting against copyright infringement claims.
- Others argue that this indemnification is an important move for commercial adoption of generative AI and that many smaller technology companies are relying on it.
- Some users discuss the implications of the indemnification and whether it implies that individuals are not responsible for intentionally creating copyright-infringing content.
- There is also discussion about the potential disruption and elimination of certain art-related jobs due to the advancement of generative AI.

Overall, the discussion highlights various perspectives on the topic, including legal considerations, the impact on different companies, and the role of AI in copyright infringement.

### OpenAI has quietly changed its 'core values,' putting greater emphasis on AGI

#### [Submission URL](https://www.semafor.com/article/10/12/2023/openai-quietly-changed-its-core-values) | 56 points | by [cainxinth](https://news.ycombinator.com/user?id=cainxinth) | [25 comments](https://news.ycombinator.com/item?id=37870309)

OpenAI, the organization behind GPT, has quietly updated its "core values," placing a stronger emphasis on artificial general intelligence (AGI). The company's CEO, Sam Altman, has described AGI as the equivalent of a median human that could be hired as a co-worker. The previous core values listed on OpenAI's website included audacity, thoughtfulness, unpretentiousness, impact-driven, collaboration, and growth-oriented. These have now been replaced with AGI focus, intense and scrappy, scale, make something people love, and team spirit. OpenAI's commitment to AGI development has been evident for years. In a 2018 mission statement, the organization defined AGI as highly autonomous systems that surpass human capabilities in most economically valuable tasks.

The discussion on Hacker News regarding OpenAI's updated core values and their emphasis on AGI includes various viewpoints.

- Some users debate the feasibility of AGI and its potential to fully replace humans in various tasks. They cite examples of autonomous systems, such as self-driving cars and factory robots, which have limitations and are not capable of performing all human tasks.
- Others express concern about the concentration of power and the potential socio-economic consequences of AGI development. They highlight the importance of considering the impact on human labor and the need for responsible deployment of AGI.
- Some commenters discuss the process of rewriting core values within organizations and the potential clash of cultures during such transitions. They offer perspectives on the importance of clear and consistent values and the impact of these values on team dynamics.
- There is a discussion about the difficulty of predicting AGI progress accurately and the challenges of assessing the capabilities of advanced AI systems. Some users argue that the most advanced AI models are not public and that access to such models is limited.
- A few users express skepticism towards AGI and caution against considering it as a single transformative event, emphasizing instead the incremental advancements in AI and the need for continuous progress.
- One commenter argues that building AGI is a reckless endeavor and that it is not morally justifiable to pursue AGI development that sacrifices human lives. They compare it to sinking a raft with billions of people in pursuit of a single fish.
- Another user debates the definition of AGI, stating that it should be software-based and not limited to imitating human intelligence.
- A comparison is made between the GameStop stock phenomenon and the expectation of exponentially high returns from AGI, suggesting that some AGI predictions may be unrealistic.

Overall, the discussion covers a range of perspectives on AGI, including its feasibility, potential impact, and ethical considerations.

### How a billionaire-backed network of AI advisers took over Washington

#### [Submission URL](https://www.politico.com/news/2023/10/13/open-philanthropy-funding-ai-policy-00121362) | 24 points | by [webmaven](https://news.ycombinator.com/user?id=webmaven) | [4 comments](https://news.ycombinator.com/item?id=37871458)

A billionaire-backed network of AI advisers is exerting influence in Washington by funding the salaries of AI fellows in key congressional offices, federal agencies, and think tanks. Open Philanthropy, financed by Facebook co-founder Dustin Moskovitz and his wife Cari Tuna, is behind this effort. The fellows, funded through the Horizon Institute for Public Service, are involved in negotiations that will shape Capitol Hill’s plans to regulate AI. Critics worry that the focus on long-term risks associated with AI may divert attention away from addressing the immediate concerns posed by AI systems, such as bias, misinformation, copyright infringement, and privacy breaches. Additionally, some fear that licensing requirements for advanced AI could favor existing tech giants and entrench their dominance in the industry. The network funded by Open Philanthropy includes affiliated organizations like the RAND Corporation and Georgetown University’s Center for Security and Emerging Technology, which also shape AI policy in Washington.

- User "ssgrn" suggests that billionaire-backed networks are not unique to AI influencing Washington, and mentions the Koch Brothers as another example.
- User "archibaldJ" raises the question of how AI advisors can effectively govern AI when the technology itself decentralizes power and can disrupt traditional political systems. They also mention the role of lobbying and design in AI governance.
- User "hppytgr" comments that AI companies tend to claim that their platforms are closing doors, possibly referring to concerns around monopolistic behavior. They mention OpenAI and Microsoft's collaboration as an example.
- User "jrhnn" makes a cryptic comment, stating "mss ls xpct" without further explanation.