import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Sun Jan 07 2024 {{ 'date': '2024-01-07T17:09:50.674Z' }}

### AI or Ain't: Eliza

#### [Submission URL](https://zserge.com/posts/ai-eliza/) | 116 points | by [john-doe](https://news.ycombinator.com/user?id=john-doe) | [101 comments](https://news.ycombinator.com/item?id=38900304)

In the year 2023, AI became a hot topic in the media, sparking debates about its potential and progress. But the fascination with non-human intelligence goes way back, with dreams of simulating human intelligence dating back to ancient times. The Turing test, introduced in the 1950s, became a benchmark for determining whether AI could truly be considered intelligent. One of the earliest AI programs to pass this test was Eliza, created in 1966 by Joseph Weizenbaum. Eliza emulated the speech patterns of a psychotherapist and still outperforms some modern AI programs. Today, we'll recreate Eliza's basic chatbot functionality using Go code. By implanting knowledge in the form of structured keywords and transformation rules, Eliza can engage in more sophisticated conversations. Synonym groups help with reducing rule duplication and create more natural responses. With some additional preprocessing and postprocessing rules, Eliza can provide a more intelligent and human-like interaction. Stay tuned for more updates on AI progress!

The discussion begins with a comment from user "jll29" linking an academic paper about Joseph Weizenbaum and his AI program Eliza. The paper discusses Weizenbaum's views on AI and how Eliza passed the Turing test. User "dstfn" adds that Weizenbaum wrote a book called "Computer Power and Human Reason" which explores the relationship between humans and machines. Another user, "stvrs," expresses confusion about how computers make decisions. User "aatd86" responds by saying that choice is determined by mechanism and provides an example involving quantum mechanics. User "vdrh" argues that choice is not determined by mechanisms and mentions compatibilist views on free will. The discussion then veers into a debate about determinism, complexity, and the nature of human judgment. Users "jhnnywrkr" and "vdrh" have an extensive conversation about the compatibility of human judgment with computation, with "jhnnywrkr" arguing that judgment is non-computable due to non-mathematical factors. User "xtrctnmch" chimes in at the end to reference Joseph Agassi's critical take on Weizenbaum's work.

### MK1 Flywheel Unlocks the Full Potential of AMD Instinct for LLM Inference

#### [Submission URL](https://mkone.ai/blog/mk1-flywheel-amd) | 120 points | by [ejz](https://news.ycombinator.com/user?id=ejz) | [25 comments](https://news.ycombinator.com/item?id=38906208)

The release of MK1 Flywheel, an inference engine designed for AMD Instinct Series, has demonstrated comparable performance to a compute-matched NVIDIA GPU. With its advanced CDNA 3 architecture, AMD's Instinct MI300 series accelerator has the potential to challenge NVIDIA's dominance in the AI market. Although AMD has faced challenges with its software ecosystem, efforts are being made to support AMD hardware on popular AI frameworks. MK1 Flywheel on AMD Instinct MI210 showcases impressive performance, and the team looks forward to benchmarking on MI300. The Flywheel engine offers higher throughput and cost savings for LLM inference workloads. The article also provides a recap of MK1 Flywheel's features and a behind-the-scenes journey of building the hardware and software components for AMD. 

The discussion on this submission revolves around various aspects of the comparison between the AMD Instinct MI210 and NVIDIA A6000 GPUs, as well as AMD's software ecosystem and its potential to challenge NVIDIA's dominance in the AI market. 
One user points out that the MI210 has lower memory bandwidth compared to the A6000, which could be a bottleneck for certain workloads. Another user suggests independent testing to verify AMD's claims. 
There is a discussion about the differences between AMD's open-source ML platform, ROCm, and NVIDIA's closed CUDA platform. Some users express concerns about the lack of support for ROCm and the dominance of CUDA in the AI community. 
The price and performance comparisons between the MI210 and A6000 GPUs are also brought up, with one user pointing out that the comparison should consider factors like TFLOPs and available models with VRAM. 
There are a few comments about the software of the MK1 Flywheel engine, with some disappointment expressed about its closed-source nature. 
The compatibility of AMD GPUs with AWS instances is discussed, with one user mentioning that AWS instances support AMD GPUs but do not officially support the ROCm platform. 
The potential for AMD to challenge NVIDIA's monopoly in the AI market is seen as a positive development by some users. However, there are doubts raised about the validity of the benchmarks and the accuracy of comparing AMD's solutions with NVIDIA's. 

Overall, the discussion covers a range of topics, including GPU performance, software ecosystem, and the competitive landscape in the AI market.

### LiteLlama-460M-1T has 460M parameters trained with 1T tokens

#### [Submission URL](https://huggingface.co/ahxt/LiteLlama-460M-1T) | 53 points | by [dmezzetti](https://news.ycombinator.com/user?id=dmezzetti) | [28 comments](https://news.ycombinator.com/item?id=38904895)

LiteLlama is an open-source reproduction of Meta AI's LLaMa 2, but with a significant reduction in model size. LiteLlama-460M-1T has been trained with 460 million parameters and 1 trillion tokens. The model was trained on a portion of the RedPajama dataset and uses the GPT2Tokenizer for text tokenization.
You can easily load the experimental checkpoints of LiteLlama using the HuggingFace Transformers library. Simply import the necessary modules, specify the model path, load the model, and generate text. LiteLlama's performance can be evaluated on the MMLU task.
LiteLlama-460M-1T compares favorably to other Llama models in terms of the number of parameters, achieving competitive scores in zero-shot and 5-shot evaluations. The detailed results can be found on the Open LLM Leaderboard. 
LiteLlama is developed by Xiaotian Han from Texas A&M University and is released under the MIT License. If you're interested in trying out LiteLlama, it's available for download.

The discussion around the LiteLlama model on Hacker News includes various topics and perspectives:

- One user points out that the model seems to exhibit looping behavior and generates repetitive output for certain prompts. They also note that the model is a small-scale version of the LLaMa 2 model developed by Meta AI.
- Another user discusses the usefulness of the model, stating that it is good for generating text based on a prompt and can perform well on language understanding tasks.
- A user mentions that the model performs pattern matching on inputs and can generate outputs based on patterns rather than actual calculations.
- Some users express their skepticism about the model's ability to solve basic math questions accurately, stating that it lacks the ability to perform arithmetic calculations.
- Another user shares a link to a page that claims LiteLlama performed well on the GSM8K benchmark for zero-shot machine translation.
- One user provides a prompt asking the model to solve simple math problems, and others chime in with the correct answers given by the model.
- There is a brief discussion about the limitations of the model and its inability to perform certain calculations beyond basic arithmetic.
- Finally, there is a comment about the model being suitable for teaching purposes.

Overall, the discussion covers topics such as the model's capabilities, its limitations, and its potential applications.

### Nvidia RTX 5880 Ada 48GB Professional GPU Launched

#### [Submission URL](https://www.servethehome.com/nvidia-rtx-5880-ada-48gb-professional-gpu-launched/) | 15 points | by [PaulHoule](https://news.ycombinator.com/user?id=PaulHoule) | [6 comments](https://news.ycombinator.com/item?id=38906213)

NVIDIA has launched a new professional GPU called the RTX 5880 Ada, which comes with 48GB of memory. This GPU is positioned between the RTX 5000 Ada and the RTX 6000 in terms of performance and memory. The RTX 5880 Ada features 14080 CUDA cores, 440 Tensor cores, and 110 RT cores, providing a 10% increase in compute capabilities compared to the RTX 5000. The memory subsystem is similar to the RTX 6000, and power consumption is closer to the RTX 6000 as well. This new GPU addresses the need for more memory on cards, especially GDDR6 memory, which is less costly than the HBM found on higher-end cards. Pricing and availability for the RTX 5880 Ada have not been announced yet.

The discussion on Hacker News revolves around the new NVIDIA RTX 5880 Ada graphics card. One user expresses confusion about where the device stands in comparison to the RTX 5000 and 6000, suggesting that if the intention is to address the need for more VRAM, the company should focus on GPUs like the A100 and H100 instead. Another user argues that performance does not necessarily differ significantly across these GPUs, regularly working with an A6000 that barely touches the RAM and finding that the RTX series mostly focuses on CUDA cores. Another user highlights the advantage of having multiple GPUs in multiple systems synchronized for a seamless training experience. Additional comments touch on the integrated fan on the A100, the importance of professional drivers and features, and the significance of doubling the VRAM on a single card.

---

## AI Submissions for Sat Jan 06 2024 {{ 'date': '2024-01-06T17:09:41.202Z' }}

### NIST identifies types of cyberattacks that manipulate behavior of AI systems

#### [Submission URL](https://www.nist.gov/news-events/news/2024/01/nist-identifies-types-cyberattacks-manipulate-behavior-ai-systems) | 117 points | by [geox](https://news.ycombinator.com/user?id=geox) | [44 comments](https://news.ycombinator.com/item?id=38893614)

The National Institute of Standards and Technology (NIST) has identified types of cyberattacks that manipulate the behavior of artificial intelligence (AI) systems. These attacks, known as "adversarial machine learning" threats, can cause AI systems to malfunction when exposed to untrustworthy data. The NIST publication, titled "Adversarial Machine Learning: A Taxonomy and Terminology of Attacks and Mitigations," outlines various types of attacks and provides mitigation strategies. However, there is currently no foolproof method for protecting AI systems from misdirection, and developers and users should be cautious of claims suggesting otherwise. The publication aims to help AI developers and users understand the potential attacks and develop better defenses in the future.

The discussion on this submission revolves around the different types of attacks that can be carried out on AI systems, such as poisoning attacks and prompt injection. Some users express concern about the potential implications of these attacks, particularly in sensitive systems like AI assistants and chatbots. Others discuss the challenges of mitigating these attacks and the need for comprehensive solutions to protect AI models. There is also a discussion on the AI community's responsibility in addressing these threats and ensuring the security of AI systems. Additionally, some users share their thoughts on the misconceptions surrounding AI and the importance of understanding the limitations and risks associated with it.

### Generative AI Has a Visual Plagiarism Problem

#### [Submission URL](https://spectrum.ieee.org/midjourney-copyright) | 17 points | by [YeGoblynQueenne](https://news.ycombinator.com/user?id=YeGoblynQueenne) | [11 comments](https://news.ycombinator.com/item?id=38896259)

Generative AI, which includes large language models (LLMs), has a visual plagiarism problem, according to a guest post on IEEE Spectrum. Recent research has shown that LLMs are capable of reproducing chunks of text from their training sets, including private information like email addresses and phone numbers. In some instances, LLMs have even produced near-verbatim outputs that can be seen as instances of plagiarism. This raises questions about how often these plagiaristic outputs occur and under what circumstances. Due to the black box nature of LLMs, researchers can only answer these questions through experimental methods. The existence of plagiaristic outputs has implications for technology, journalism, and copyright infringement laws.

The discussion about the submission covers various perspectives on the issue of visual plagiarism by language models. 
One user mentions that it would be challenging to guess the passwords to decrypt the compressed and separately encrypted copyrighted pictures, suggesting that generating plagiarized content may not necessarily infringe copyright. Another user argues that the responsibility for copyright infringement lies with the owners of the training data, not the models themselves.
Another user brings up the example of a young artist drawing a copyrighted character and suggests that drawing such characters could be seen as artistic training, rather than infringement. However, another user counters that the line between infringement and artistic expression can be blurry, especially with regards to the threshold of originality.
OpenAI's response to copyright issues is discussed, with one user suggesting that they could implement a system similar to Napster's extinction event to remove copyrighted content. However, others point out that many AI-generated works are small and have negligible impact on the original creators.
The concept of plagiarism is debated, with one user emphasizing that plagiarism involves taking credit for someone else's work. However, another user argues that training a child to draw copyrighted characters could be seen as condoning copyright infringement and refers to a hypothetical situation involving Planned Parenthood.
Overall, the discussion highlights the complexity of the issue and explores different viewpoints on the responsibility for plagiarism and copyright infringement in the context of generative AI.

### LLM Training and Inference with Intel Gaudi 2 AI Accelerators

#### [Submission URL](https://www.databricks.com/blog/llm-training-and-inference-intel-gaudi2-ai-accelerators) | 33 points | by [sailplease](https://news.ycombinator.com/user?id=sailplease) | [8 comments](https://news.ycombinator.com/item?id=38887798)

Databricks, a company that helps customers build and deploy generative AI applications, has announced support for the Intel Gaudi family of AI accelerators. The Gaudi accelerators are designed for deep learning training and inference and offer high performance and memory capacity. Databricks found that the Intel Gaudi 2 accelerator had the second-best training performance-per-chip among the platforms tested, achieving over 260 TFLOP/s/device when training MPT-7B on 8 x Gaudi 2. It also matched the NVIDIA H100 in decoding latency for LLM inference. Additionally, the Gaudi 2 showed the best training and inference performance-per-dollar compared to other popular accelerators. Databricks used SynapseAI 1.12 and BF16 mixed precision training for their testing, and they plan to explore performance improvements with SynapseAI 1.13, which supports FP8 training.

The discussion on this submission revolves around the performance comparison between Intel Gaudi 2 and NVIDIA accelerators, as well as the challenges of porting software to different hardware platforms.
One commenter points out that the Intel Developer Cloud offers 8x individual Gaudi 2 inference costs for $130/hr, which is significantly cheaper than using NVIDIA A100 or H100 accelerators with Amazon Web Services (AWS) infrastructure.
Another commenter brings up the issue of software portability, stating that while it may be a friction point, it is appreciated that Databricks built their performance claim by specifying the device, TDVcstr statements for verification, and experience with other platforms like Google's TPUs and AMD GPUs.
In response, another commenter acknowledges that there is fully supported Nvidia competition but highlights that implementations on Intel, AMD, Google TPUs, etc., are not at the same level of real-world applications yet. They mention that a lot of actual project implementations in the industry heavily depend on CUDA and that AMD ROCm is finally starting to sort things out, potentially becoming competitive in terms of software ecosystem. They mention that the commenter's satisfaction with Nvidia and CUDA is due to the significant performance and optimized software implementations for CUDA.
There is also a discussion about the level of abstraction provided by PyTorch and whether it truly matters or not. One commenter mentions that for most basic level use cases, PyTorch's hardware abstraction is not necessary, but it becomes important for more complex and specific machine learning projects.
The conversation turns to the challenges of software portability and the commenter's personal experience with AMD ROCm. They express frustration with the time and effort spent on making AMD hardware work with ROCm, compared to the smooth experience they have had with Nvidia and CUDA over the years. They argue that Nvidia's market dominance and support, especially for production-scale AI training and inference, make it the preferred choice despite the total cost of ownership.

Lastly, one commenter finds it ironic that there is no standard connector protocol like NVLink for Intel Gaudi, given that it requires larger and more complex plugs and cards compared to normal 100GbE switches.

### Ten Noteworthy AI Research Papers of 2023

#### [Submission URL](https://magazine.sebastianraschka.com/p/10-ai-research-papers-2023) | 123 points | by [danboarder](https://news.ycombinator.com/user?id=danboarder) | [19 comments](https://news.ycombinator.com/item?id=38896027)

This year has been a game-changer for machine learning and AI research, with rapid advancements and growing popularity in these fields. To wrap up 2023, Sebastian Raschka shares his top ten noteworthy research papers. The selection includes a heavier emphasis on large language models (LLMs) rather than computer vision papers. One standout paper is Pythia, which not only released 8 LLMs but also provided training details, analyses, and insights. Another paper, Llama 2, introduces open foundation and fine-tuned chat models, which are widely used and come with reinforcement learning with human feedback (RLHF) instruction-finetuned variants. The Llama 2 suite showcases consistent improvements over its iterations, making it a popular choice among researchers. Overall, 2023 has been a year of immense progress, and Raschka hopes for more transparency and detailed papers in the future.

The discussion on this submission primarily revolves around specific papers mentioned in the article. One user points out that the TinyStories paper demonstrates the ability to generate coherent text with fewer parameters. Another user shares a link to an interesting summary article on the topic. 
There is also a discussion about the BloombergGPT paper, which was published in March and launched Bloomberg's knowledge model. Another user comments on the usefulness of popular papers and conferences for getting expert opinions and advice. 
A commenter raises concerns about the peer review process, stating that some reviewers don't read papers carefully and their reviews can be arbitrary. Another user agrees and adds that conference reviews often miss good work and accept mediocre papers. The shortage of competent reviewers is also mentioned as a contributing factor to the problem.
Finally, there is a brief exchange discussing the flaws in the current conference review process and the need for improvements.

### Computers by the Millions (1979)

#### [Submission URL](https://web.stanford.edu/dept/SUL/sites/mac/primary/docs/cbm.html) | 6 points | by [rnjailamba](https://news.ycombinator.com/user?id=rnjailamba) | [4 comments](https://news.ycombinator.com/item?id=38890926)

Jef Raskin, a computer scientist and one of the key creators of the Apple Macintosh, wrote an essay in 1980 titled "Computers by the Millions," discussing the challenges and considerations involved in producing a large number of personal computers. Raskin emphasized the need for computers to become more accessible and affordable, suggesting that the average family should be able to own one. He explored the financial implications of manufacturing a million computers per year, including the costs of parts, labor, and inventory. Raskin also discussed the challenges of duplicating software, highlighting the time-consuming process of duplicating software on magnetic media and the need for mass storage media duplication companies. He mentioned the potential use of printing presses for distributing programs and the possibility of software distribution via communication channels. Raskin's essay shed light on the magnitude of the task of making computers more widely available and the various obstacles that needed to be overcome.

There was a discussion about Jef Raskin's essay "Computers by the Millions." One user pointed out that while Raskin emphasized the need for affordable computers, the new Mac Pro machines are quite expensive. Another user mentioned that Raskin was a key contributor to the early development of the Apple Macintosh project. Another user shared a link to Raskin's essay and mentioned that Raskin was enjoying positive feedback, except from Steve Jobs. Then, there was a mention that Steve Jobs shot Apple's history exhibits at a library and collections at Stanford.

### Show HN: LangCSS, the AI Assistant for Tailwind

#### [Submission URL](https://langcss.com/) | 7 points | by [mcapodici](https://news.ycombinator.com/user?id=mcapodici) | [3 comments](https://news.ycombinator.com/item?id=38889467)

Introducing Tailwind Chat, the AI assistant that helps you create stunning forms, buttons, landing pages, and more in real time. Currently in early beta, Tailwind Chat allows you to join the waitlist for free beta access. With Tailwind Chat, you can talk through your design ideas while the AI generates the necessary utility classes. It even lets you edit the HTML and continue the conversation seamlessly. You'll have access to great components from popular libraries like tailwindui, shadcn, flowbite, and daisyui. Not limited to just Tailwind, the AI can assist with other tasks as well, such as designing logos using SVG. Tailwind Chat gives you complete ownership over the code it generates, with no licensing requirements. The platform also offers a robust code editor, the Monaco Code Editor, to allow you to make edits without leaving your browser. You can easily undo, go back to previous designs, and save your chats for future reference. Tailwind Chat is built on OpenAI, but their code is designed to be compatible with other AI systems in the future. Get ready to revolutionize your design workflow with Tailwind Chat!

The discussion mainly consists of two comments:
1. User "mcpdc" mentions that they experimented with using simplicityNode.js, Express, and Postgres, specifically using the Vercel ORM. They also mention using HTMX for frontend interactions and Mithril for React-like requirements. Lastly, they mention using the Digital Ocean App Platform for $5 million.
2. User "p2hari" responds, saying that the response time and copyHTML work indication are good. They also mention that they worked nicely with the chat request.
User "mcpdc" thanks "p2hari" and expresses their agreement with their comments.

---

## AI Submissions for Fri Jan 05 2024 {{ 'date': '2024-01-05T17:09:58.766Z' }}

### SSH-Snake: Automated SSH-Based Network Traversal

#### [Submission URL](https://github.com/MegaManSec/SSH-Snake) | 130 points | by [lozf](https://news.ycombinator.com/user?id=lozf) | [22 comments](https://news.ycombinator.com/item?id=38883094)

Introducing SSH-Snake: an automated tool for SSH-based network traversal. This script is designed to discover SSH private keys and host connections within a network, making it easier to identify potential vulnerabilities. SSH-Snake is a self-propagating, file-less script that can recursively perform tasks such as finding SSH private keys, identifying hosts that accept the keys, and attempting SSH connections. It can quickly map out a network and its dependencies, saving time and effort for security professionals or sysadmins. Although the primary purpose of SSH-Snake is for hackers, it can also be used by sysadmins to better understand their infrastructure. To try it out, you can either download the script or execute it directly using wget or curl. SSH-Snake is entirely written in Bash and requires minimal dependencies commonly found on major Linux systems.

The discussion about the submission "Introducing SSH-Snake: an automated tool for SSH-based network traversal" on Hacker News revolves around various topics. 
One commenter discusses that the tool seems helpful for sysadmins managing larger infrastructures that heavily rely on SSH. They mention that many sysadmins stop using SSH keys and resort to SSHD with strict authentication policies to mitigate potential risks.
Another commenter expresses interest in the limitations of the tool, noting that it currently does not support IPv6. The author of the tool jokingly responds to this comment.
The discussion continues with a comment about the nostalgic feel of the tool, as it reminds the commenter of 90s-style network exploration using script-based tools. Another commenter adds to this sentiment, stating that it could be used to target certain network elements.
There is a discussion about customizing the tool's settings to attempt connections on non-standard ports. A commenter provides specific instructions on how to modify the configuration file to achieve this.
Another commenter reminds users to be cautious with managing private keys and emphasizes the importance of backups and limiting commands on remote machines.
One commenter shares a similar project for Windows AD networks, and another suggests rewriting the Bash script to drop dependencies.
Other comments express appreciation for the tool and its connection to their work. One commenter mentions that they intentionally used the script to discover SSH connections on linked machines, while another discusses the risks of running local SSH traversal on publicly hosted servers.
One commenter points out that the script disables the printing of private keys in the terminal, referencing a line in the script's repository on GitHub.

The discussion ends with a comment unrelated to the tool, simply mentioning the concepts of a "message-passing Quine" and a "Quine wasm."

### Driverless User Space File Systems for Windows, macOS, and Linux

#### [Submission URL](https://thelig.ht/user-space-file-systems/) | 52 points | by [todsacerdoti](https://news.ycombinator.com/user?id=todsacerdoti) | [13 comments](https://news.ycombinator.com/item?id=38883509)

A Python package called userspacefs allows developers to easily write user space file systems for major desktop platforms. The package provides a ready-to-use executable for Windows through the dbxfs Python package. The concept of user space file systems has fascinated system designers for years, offering benefits like a unified API and the ability to implement network protocols and hardware drivers as user space servers. While FUSE has enabled user space file systems on open-source POSIX systems, Windows and macOS have required the installation of third-party kernel drivers. However, the userspacefs package uses the WebDAV protocol to emulate the FUSE ABI and mount user space file systems on macOS and Windows without the need for a kernel driver. The package has also been used to build applications like Safe, which provides user-friendly encryption for Dropbox files, and dbxfs, which allows for an on-the-fly Dropbox file system using the SMB protocol.

The discussion around the submission touches on various topics related to user space file systems on Windows and macOS.

- One user mentions the Plan 9 file sharing protocol (fssmbnfswbdv) as an alternative to the WebDAV protocol.
- Another user provides instructions on how to set up a loopback network connection in Windows to emulate kernel driver-like access to user space file systems.
- There is a discussion about the possibility of mounting SMB file systems on arbitrary ports in Windows 11, as well as the use of RDMA and QUIC protocols.
- A user mentions that UNC paths in Windows use the GLOBALROOT namespace, and certain file managers and dialog boxes reject paths in that style.
- The default loopback address (127.0.0.1) is discussed, with some users pointing out that Windows allows multiple distinct IP addresses on the loopback interface.
- The issue of Windows SMB server bindings and wildcard addresses is brought up, with suggestions to disable the SMB service to prevent potentially insecure configurations.
- The topic of potential issues with IPv6 and the loopback interface is raised, with one user mentioning a related problem in Linux.
- There is a link to a discussion on Hacker News from a few months ago regarding WebDAV and SMB-backed filesystems.
- The Hurd operating system is mentioned as an example of a privileged server mounting user space filesystems.

Overall, the discussion covers various technical aspects and potential challenges of implementing user space file systems on different platforms.

### M3 CPU cores have become more versatile

#### [Submission URL](https://eclecticlight.co/2024/01/05/m3-cpu-cores-have-become-more-versatile/) | 115 points | by [ingve](https://news.ycombinator.com/user?id=ingve) | [151 comments](https://news.ycombinator.com/item?id=38884741)

The article examines the performance of the CPU P and E cores in Apple's new M3 series chips, comparing them to the previous M1 chips. The M3 Pro variant, with two six-core P clusters, showcases significant improvements in performance and energy efficiency compared to the M1. The tests conducted demonstrate that the M3 P cores outperform the M1 cores in various computations, particularly in vector and matrix operations. However, the M3 E cores running background threads are slower than their M1 counterparts. In contrast, when running high QoS threads, the M3 E cores perform almost as well as the M1 P cores and are slightly faster in certain non-scalar operations. The article concludes that the M3 chips offer substantial improvements in performance and versatility compared to the M1 chips.

The discussion on Hacker News revolves around several aspects of the article. Some users express disappointment with the M3 Pro's performance, noting that it falls short in certain calculations. Others discuss whether upgrading from the M1 Max to the M3 Pro is worth it, particularly for running Windows VMs. The topic of nested virtualization is also brought up, with some users suggesting that M2 and M3 chips may not support it. There is also a discussion on battery life, with comparisons to previous MacBook models and suggestions for optimizing battery health. The conversation touches on the performance of DAWs, the behavior of CPU and GPU memory allocations, and the gaming capabilities of the M3 chips. Some users express appreciation for the overall features and performance of the M3 chips, while others discuss the pricing and value proposition of Apple's MacBook lineup.

### Learning bimanual mobile manipulation with low-cost whole-body teleoperation

#### [Submission URL](https://mobile-aloha.github.io) | 123 points | by [tristenharr](https://news.ycombinator.com/user?id=tristenharr) | [47 comments](https://news.ycombinator.com/item?id=38875452)

Researchers at Stanford University have developed a system called Mobile ALOHA, which allows robots to learn bimanual and whole-body mobile manipulation tasks through imitation learning from human demonstrations. The system combines the ALOHA system with a mobile base and a whole-body teleoperation interface to enable robots to perform complex tasks such as cooking, opening cabinets, and using a kitchen faucet. The researchers collected data using Mobile ALOHA and then trained the system using behavior cloning. They found that co-training with existing static ALOHA datasets improved the performance of the system on mobile manipulation tasks, increasing success rates by up to 90%. The project was supported by the Boston Dynamics AI Institute and ONR grant N00014-21-1-2685. Zipeng Fu was supported by a Stanford Graduate Fellowship.

The discussion surrounding the submission revolves around several different topics. Here are some key points:

1. One user points out that the system's success rate in handling various tasks is impressive, with some tasks being performed successfully up to 9 times out of 10.
2. Another user wonders about the potential implications of having fully automated kitchens in high-end apartments, suggesting that it might replace human access to kitchens.
3. Some users discuss the satisfaction of watching robots perform mundane household tasks and acknowledge the practicality of having robots assist with laundry, dishwashing, and other chores.
4. The capabilities of robots and their ability to perform complex tasks using machine learning are compared to the human brain and its ability to solve problems.
5. The discussion touches on the potential benefits of bimanual manipulation by robots, such as assisting people with disabilities or performing tasks that require a high level of dexterity.
6. The difficulty of implementing machine learning approaches in robotics, particularly in terms of real-time decision-making and understanding the physical world, is discussed.
7. The limitations of current robotic systems and the complexity of replicating human skills are also mentioned.

Overall, the discussion highlights both the impressive advancements in robotic manipulation and the challenges that still need to be addressed in order to achieve fully autonomous and versatile robots.

### How to build a thinking AI

#### [Submission URL](https://aithought.com/) | 113 points | by [tudorw](https://news.ycombinator.com/user?id=tudorw) | [64 comments](https://news.ycombinator.com/item?id=38882747)

In a recent article, Jared Edward Reser proposes a framework for building a thinking AI by simulating human-like thought processes. He focuses on replicating the dynamics of the mammalian working memory system, which features two forms of persistent activity: sustained firing and synaptic potentiation. Reser suggests that in an AI implementation, these two memory stores should be continuously and iteratively updated, with each state preserving a proportion of the coactive representations from the previous state. This iterative updating allows for the gradual evolution of concepts in working memory and the generation of associatively linked intermediate states, leading to progress towards a solution or goal. Reser conceptualizes iterative updating as an information processing strategy, a model of working memory, a theory of consciousness, and an algorithm for designing and programming artificial general intelligence. The article provides a comprehensive analysis of the framework, including literature review, implications, and suggestions for instantiating the model within a computer.

The discussion on Hacker News about the article proposing a framework for building a thinking AI focuses on various aspects of the proposal. Some commenters express skepticism about the implementation and question the validity of the claims made in the article. They argue that neural networks have shown capabilities in solving classification problems but have not replicated human thought processes. There is also discussion about the complexity and challenges involved in building an AGI system, including the need for long-term memory and the extensive training required. Some commenters suggest that academia focuses more on theoretical groundwork rather than practical implementation. Others discuss the limitations of current models and the need for further understanding of human cognitive processes. There are also comments emphasizing the importance of context and clarifying specific points in the article. Overall, the discussion reflects a mix of skepticism, criticism, and nuanced perspectives on the proposed framework.

### Researchers say they’ve replicated LK-99 room temperature superconductor

#### [Submission URL](https://thequantuminsider.com/2024/01/04/its-back-researchers-say-theyve-replicated-lk-99-room-temperature-superconductor-experiment/) | 59 points | by [stevenjgarner](https://news.ycombinator.com/user?id=stevenjgarner) | [24 comments](https://news.ycombinator.com/item?id=38880921)

Researchers have reported successful replication of an experiment that suggests a copper-substituted lead apatite (CSLA) may serve as a candidate for room-temperature superconductivity. Last year, replication efforts on a similar room-temperature superconductor were inconclusive. While the recent findings are intriguing, further work is needed to confirm the results. The study focused on CSLA, which has been proposed as a potential material for room-temperature superconductivity. The team observed diamagnetic dc magnetization in the material below room temperature, as well as other behaviors typical of superconductors. However, the study did not conclusively observe the complete Meissner effect, a definitive characteristic of superconductivity. The researchers also acknowledged the need to synthesize scalable samples with more active components to strengthen the signals indicating superconductivity. While the quest for room-temperature superconductivity is often referred to as the Holy Grail in materials science, cautious skepticism is necessary until further studies can provide more conclusive evidence.

The discussion on the submission revolves around the replication of an experiment suggesting the potential for room-temperature superconductivity using a copper-substituted lead apatite (CSLA) material. Some comments express skepticism and emphasize the need for further verification, cautioning against premature excitement. Others discuss the potential applications and benefits of room-temperature superconductors, such as faster computers, magnetic levitation, and energy-efficient power transmission. There are also mentions of related topics, including magnetohydrodynamic energy storage, functional magnetic resonance imaging (fMRI), quantum computing, and anti-gravity technologies. One comment points out that this submission is a duplicate of a previous one. Another comment discusses the heat-generating properties of the material and its potential limitations.