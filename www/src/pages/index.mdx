import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Thu Dec 14 2023 {{ 'date': '2023-12-14T17:10:26.485Z' }}

### Embeddings, vectors, and arithmetic

#### [Submission URL](https://montyanderson.net/writing/embeddings) | 68 points | by [montyanderson](https://news.ycombinator.com/user?id=montyanderson) | [16 comments](https://news.ycombinator.com/item?id=38645411)

Monty Anderson, in his blog post titled "Embeddings, Vectors, and Arithmetic," explores the concept of embeddings as a representation of text and the computational operations that can be performed on them. He references Lilian Weng's project, which showcases a ranking of the closest emojis to a search query in the meaning-space. The emoji vectors are calculated using the OpenAI's Ada model, and the results are based on the euclidean distance or cosine similarity. Building on this idea, Anderson and Barney Hill developed an app that allows users to add two emojis and find the closest known emoji to that result. While the project worked well, it also revealed the stereotypes and flaws present in the training data. Anderson mentions their exploration of building safety systems at Prodia by checking if input prompts are within a distance threshold of known adult or illegal concepts. The post concludes by hinting at a fuzzy future where machines can reason about meaning in various types of data, not just text.

The discussions around Monty Anderson's blog post "Embeddings, Vectors, and Arithmetic" cover various aspects related to vector embeddings and their applications. 
User "YPCrumble" expresses their interest in vector embeddings, while user "wyncchrn" asks a question about embeddings in linear space and discusses how addition operations can make sense. User "bnrymx" comments that the blog post does not mention that the training data used GloVe, a popular model for word vector spaces. Other users add positive comments about embeddings, with one mentioning the usefulness of GloVe and another noting that linear models and PCA can be employed.
User "throwup238" shares their experience in building safety systems at Prodia, where they investigate distance thresholds for known adult or illegal concepts. They mention that these measures are necessary due to the limitations and uncertainties of embedding models.
The conversation continues with discussions about embedding categorization, fingerprints, and retrieval results for different sizes of text. User "mntyndrsn" comments on using embedding for safety filters and having different models for different purposes. User "batch12" mentions their struggle with semantic meaning in retrieval results. User "ptr" suggests expanding the process of using various tools for embedding-based search.
User "ttcr" raises concerns about embeddings and illegal concepts, noting that embeddings are not objective and that measuring similarity distances should not punish or censor certain concepts. User "rbrnd" argues that embeddings are not objective due to the complexity of training data, while expressing their love for AI finding traditional winter message boards. The discussion then diverges into debates about censorship, privacy, and the influence of AI technology on society.

The last comment, by user "jflkn", seems to be flagged and doesn't contribute to the discussion.

### Stable Zero123: Quality 3D Object Generation from Single Images

#### [Submission URL](https://stability.ai/news/stable-zero123-3d-generation) | 86 points | by [homarp](https://news.ycombinator.com/user?id=homarp) | [8 comments](https://news.ycombinator.com/item?id=38647562)

Stability AI has released Stable Zero123, their in-house trained model for view-conditioned image generation. This model generates 3D objects with improved quality compared to the previous state-of-the-art model, Zero123-XL. The improvements were achieved through an improved training dataset, elevation conditioning, and a pre-computed dataset. Stable Zero123 is now available for download on Hugging Face for researchers and non-commercial users to experiment with. Additionally, Stability AI has improved the open-source code of threestudio to support Zero123 and Stable Zero123, enabling open research in 3D object generation. However, it's important to note that this model is intended for non-commercial and research use only.

The discussion surrounding the submission is varied. Users are generally impressed with the improved results of Stable Zero123 compared to the previous model, Zero123-XL. Some users discuss the limitations of 3D object generation, particularly in the context of augmented reality (AR) where there is a lack of available 3D objects. One user mentions Amazon's proposed solution of digitizing physical objects using computer vision, while another user highlights the challenges of creating accurate 3D models due to factors like licensing, object complexity, and optimization. The discussion also touches on the limitations of AR experiences, particularly in terms of delivery and the availability of 3D models. Some users express interest in the application of augmented reality in the clothing industry, while another user promotes Matterport as a popular solution for creating 3D models. Lastly, there is a brief mention of the high costs and complexities involved in generating fully rigged 3D models through conventional means.

### Vision Pro will change photography

#### [Submission URL](https://om.co/2023/12/14/why-vision-pro-will-change-photography/) | 54 points | by [SLHamlet](https://news.ycombinator.com/user?id=SLHamlet) | [73 comments](https://news.ycombinator.com/item?id=38645283)

Apple's upcoming Vision Pro, a spatial computer worn on the face, is set to redefine our relationship with visual media. The device allows users to capture spatial videos, a mixed-reality format that records depth and spatial information, offering a more immersive 3D experience when played back on the Vision Pro's high-resolution display. While the videos are slightly lower quality due to the limitations of the ultra-wide lens, they have a dreamlike quality that resembles memories. Spatial videos have the potential to revolutionize storytelling and photography, offering a new way to capture and experience moments. In addition to spatial video, the Vision Pro also enhances the viewing experience of photos, allowing users to pinch and expand images for a more immersive experience. Overall, the Vision Pro is shaping up to be a game-changer in the world of mixed reality glasses.

The discussion surrounding the submission on Apple's upcoming Vision Pro focuses on various aspects of the device and its potential impact. Several comments discuss the limitations and practicality of the product. Some mention that the performance may not be worth the high price tag, while others express skepticism about the usefulness of virtual reality (VR) solutions in a productivity context. The discussion also touches on related topics such as the difference between virtual reality and augmented reality, the potential for VR in photography, and the comparison to previous technologies like Lytro and Google Cardboard. Some comments question the accuracy and reliability of the device's depth mapping capabilities, while others mention the integration of similar functionalities in existing smartphone cameras. There is also a comment criticizing the relevance of the discussion and the hype surrounding the product. Overall, the comments provide a range of perspectives on the potential impact and practicality of Apple's Vision Pro.

### The AI Trust Crisis

#### [Submission URL](https://simonwillison.net/2023/Dec/14/ai-trust-crisis/) | 308 points | by [simonw](https://news.ycombinator.com/user?id=simonw) | [272 comments](https://news.ycombinator.com/item?id=38643046)

Dropbox has faced a wave of criticism after introducing new AI features that some users fear jeopardize the privacy of their data. The concern arises from the belief that Dropbox is sharing user files with OpenAI for training its models, a claim vehemently denied by Dropbox. While Dropbox's AI features, such as "summarize on demand" and "chat with your data," seem sensible, the company's communication on data privacy and AI has been lacking, leading to a crisis of trust. The existence of a checkbox buried deep in the settings, which appeared to enable data usage for AI training, only added to the confusion. People's skepticism is reminiscent of the belief that Facebook spies on users through their phone's microphone, despite such claims being debunked. The issue at hand is that trust in AI companies is eroding, with their assurances overshadowed by the mysterious nature of AI models and the lack of transparency in their training data. Trust is crucial, and allegations of deceit regarding user privacy must be taken seriously by both companies and regulators.

The discussion on this submission revolves around concerns over Dropbox's new AI features and the company's approach to privacy. Some users criticize Dropbox for allegedly sharing user files with OpenAI, while others argue that the claims are baseless. The lack of clear communication from Dropbox regarding data privacy and AI training has led to a crisis of trust. Additionally, the comparison is made with the belief that Facebook spies on users through their phone's microphone. The overall sentiment is that trust in AI companies is eroding, and allegations of deceit regarding user privacy should be taken seriously by both companies and regulators. 

In the comments, there is a discussion about the legal implications of consent and contracts, with some users arguing that silent consent can be considered fraudulent. Others argue that digital contracts should not be binding, and that current laws are not always applicable to digital transactions. The conversation also touches on the role of government regulation, with some users expressing cynicism towards the power of regulations like GDPR in protecting user privacy.

There are also comments discussing the flawed nature of third-party apps accessing phone microphones and the importance of data privacy and trust in AI companies. The conflicting perspectives highlight the ongoing debate about privacy, consent, and the responsibility of technology companies in safeguarding user data.

### DeepMind AI outdoes human mathematicians on unsolved problem

#### [Submission URL](https://www.nature.com/articles/d41586-023-04043-w) | 102 points | by [rntn](https://news.ycombinator.com/user?id=rntn) | [18 comments](https://news.ycombinator.com/item?id=38646123)

An AI system called FunSearch, based on large language models (LLMs), has made progress in solving combinatorics problems inspired by the card game Set. Combinatorics is a field of mathematics that studies how to count the possible arrangements of sets with finite objects. FunSearch generates requests for an LLM to write computer programs that can generate solutions to a specific mathematical problem. The system then quickly checks whether the generated solutions are better than existing ones and provides feedback to improve in subsequent rounds. FunSearch has shown that LLM-based systems can go beyond what is already known by mathematicians and computer scientists, making it a valuable tool in mathematical discovery and problem-solving.

The discussion on Hacker News revolves around the submission discussing FunSearch, an AI system based on large language models (LLMs) that can solve combinatorics problems. Here are the key points from the discussion:

1. Some users express skepticism about the potential of LLMs in solving complex mathematical problems, stating that LLMs are not a replacement for human mathematicians and that computers alone have not solved fundamental math problems.
2. Others argue that LLMs are capable of generating computer programs that explore large solution spaces, which can be helpful in discovering novel solutions. They suggest that in the future, LLM-based systems might be able to make significant contributions in fields like materials science and protein folding.
3. There is a discussion about the hybrid nature of FunSearch, which combines LLMs with human effort. Users point out that LLMs are not a complete replacement for human-generated data and that the reported results of FunSearch are a combination of LLM-generated programs and insights refined through iterations of the workflow.
4. Some users challenge the notion that LLMs are a form of artificial general intelligence, stating that they are not truly intelligent but rather stochastic pattern-recognition systems.
5. The availability of code for the discovered solutions is mentioned, with users finding it disappointing that the details of the method and the implemented algorithm are missing.
6. The discussion also touches on the effectiveness of FunSearch in solving difficult problems and discovering new knowledge. FunSearch is commended for pushing the boundaries of existing LLM-based approaches and demonstrating its effectiveness in combinatorics.
7. Some users express excitement about the potential of LLMs in aiding mathematical discovery and problem-solving, citing the success of similar approaches in the past.
8. The relevance of FunSearch to mathematical benchmarks and the importance of human input in the process are also discussed.
9. A link to the GitHub implementation of FunSearch is shared, leading to further discussion about the distributed system and the details of the method.
10. Overall, the discussion encompasses a range of opinions about the capabilities and limitations of LLMs in solving combinatorics problems, and the potential impact of FunSearch in various domains.

### Windows AI Studio Preview

#### [Submission URL](https://github.com/microsoft/windows-ai-studio) | 195 points | by [Jayakumark](https://news.ycombinator.com/user?id=Jayakumark) | [68 comments](https://news.ycombinator.com/item?id=38637853)

Microsoft has released a preview of Windows AI Studio, a platform that simplifies generative AI app development. It brings together AI development tools and models from Azure AI Studio Catalog and other catalogs like Hugging Face. With Windows AI Studio, developers can browse AI models, download them locally, fine-tune them, and use them in their Windows applications. All computation happens locally, but in the future, Windows AI Studio plans to integrate ORT/DML to run AI models on any Windows Hardware. Currently, Windows AI Studio only runs on NVIDIA GPUs.

The discussion on this submission revolves around various topics related to Windows AI Studio and the use of AI models on different platforms.

- One user faced some issues while installing Windows AI Studio and mentioned that they had to disable Python scripts.
- Another user suggested trying the command "conda config --set auto_activate_base false" to solve the issue.
- Some users commented on the fact that Windows AI Studio only runs on NVIDIA GPUs currently.
- There was a discussion about the differences between running AI models on Linux and Windows, with some users pointing out that there may be better compatibility with NVIDIA drivers on Linux.
- Some users shared their experiences with running CUDA on WSL2, with some saying it provided a good experience and others facing difficulties.
- There was a mention of ROCM support for Windows and the support of CUDA on current-generation cards.
- One user raised the question of whether this is the year of the Linux desktop, and others shared their thoughts on the topic.
- The discussion also touched on the release of an official OCR model by Microsoft and the availability of AI models for text recognition.
- A user commented on Apple's hardware and its potential for AI development, mentioning the limitations of RAM and GPU options.
- There was a discussion comparing Apple's GPUs with NVIDIA's in terms of VRAM and memory capabilities for machine learning tasks.

Overall, the discussion covers various aspects of AI development, including platform compatibility, hardware limitations, and the release of AI models.

### The AI revolution is an opportunity for writers (the human kind)

#### [Submission URL](https://on.substack.com/p/the-ai-revolution-is-an-opportunity) | 14 points | by [cjbest](https://news.ycombinator.com/user?id=cjbest) | [3 comments](https://news.ycombinator.com/item?id=38636159)

A recent article on Substack posits that the AI revolution is actually an opportunity for human writers, despite the fears and concerns that many may have. The article points out that while AI can generate content, it cannot replace the human connection and creativity that comes from writers and other culture makers. The author argues that as AI takes over the mundane and repetitive tasks of content creation, it will actually increase the value of original human work. The article emphasizes that platforms like Substack, which prioritize human-to-human relationships and connections, will continue to thrive in the era of AI. The author concludes by stating that the true opportunity of the AI revolution lies in the unique perspectives and abilities of human writers and culture makers.

The discussion on the submission revolves around different perspectives on the role of AI in content creation and its impact on human writers. 
User "gentleman11" shares skepticism about the opportunity for human writers on platforms like Substack, suggesting that it might not be enough to navigate a brave new world. 
User "mdmsmrt" brings up the idea that AI is just a human-made part of the smartphon environment and the transition to an AI-dominated world may be hampered by human habits and preferences. They mention that humans still spend quality time on their smartphones and argue that AI-generated content might not be able to bridge the gap completely. 
User "tmrkzm" comments that making real content available is a hopeful and positive idea for creating job opportunities for human writers. 
User "Mobil1" simply confirms the accuracy of the summary by saying "dd," which likely stands for "done."

### Ted AI 2023

#### [Submission URL](https://www.ai-event.ted.com) | 14 points | by [gardenfelder](https://news.ycombinator.com/user?id=gardenfelder) | [4 comments](https://news.ycombinator.com/item?id=38647123)

TED AI: Exploring the Profound Implications of Artificial Intelligence

Get ready to dive into the world of artificial intelligence at TED AI, a full day conference that delves deep into the transformative power of AI. Hosted by Chris Anderson, Head of TED, and curated by Sam & Walter De Brouwer, this event brings together pioneers and trailblazers to discuss how AI is set to revolutionize our civilization, industries, institutions, communities, and cultures. In addition to the conference, there is also a 2-day hackathon focused on using AI for social causes. If you have a passion for creating AI solutions that make a positive impact, this is your chance to join the community and showcase your skills. The winning project will even be featured on the TED AI stage. The lineup of speakers is incredibly diverse and includes renowned individuals such as Shane Legg, Ilya Sutskever, Stephen Wolfram, Eric Topol, Liv Boeree, and many more. They will discuss topics ranging from the transformative potential of AGI to the dark side of competition in AI. Aside from the talks, there will be engaging panels and workshops where you can enhance your AI skills and engage in interactive discussions with the speakers. This event wouldn't be possible without the support of generous partners, who have helped shape the experience. Make sure to check them out and appreciate their invaluable contributions. Don't miss out on this exciting opportunity to explore the future of AI. Join the TED AI community and brace yourself for an impactful event that promises to reshape the way we think about artificial intelligence.

The discussion about the submission seems to have mixed responses. One user, mdrzn, suggests that they are excited about the upcoming event, while another user, hppnd, criticizes the quality and downplays the significance of recent TED talks.  In response to hppnd's comment, user lfszvntt agrees and believes that the quality of TED talks has gone downhill, specifically mentioning the lack of substantial speeches. However, user grdnfldr counters this argument by stating that the value of TED talks depends on the viewer, suggesting that some talks are truly worthwhile. Lastly, user grdnfldr comments on the original submission, stating that the TED AI event is focused on the power and impact of AI.

Overall, the discussion contains differing opinions about the quality and relevance of TED talks, as well as some anticipation for the TED AI event.

---

## AI Submissions for Wed Dec 13 2023 {{ 'date': '2023-12-13T17:14:34.570Z' }}

### SMERF: Streamable Memory Efficient Radiance Fields

#### [Submission URL](https://smerf-3d.github.io/) | 578 points | by [duckworthd](https://news.ycombinator.com/user?id=duckworthd) | [136 comments](https://news.ycombinator.com/item?id=38632492)

Researchers from Google DeepMind and Google Research have introduced a new view synthesis approach called SMERF (Streamable Memory Efficient Radiance Fields) that enables real-time rendering of near-photorealistic scenes on commodity smartphones and laptops. Unlike previous methods that use either explicit scene representations or neural fields for ray marching, SMERF combines both approaches to achieve high quality and real-time performance. The researchers developed a hierarchical model partitioning scheme to increase model capacity while constraining compute and memory consumption. They also used a distillation training strategy to improve image fidelity by leveraging a state-of-the-art offline radiance field as a teacher model. SMERF outperforms existing real-time methods on large scenes, achieves faster rendering speeds, and is compatible with a wide variety of devices, including smartphones. The technology allows for full six degrees of freedom (6DOF) navigation within a web browser. The researchers have provided video demonstrations of SMERF's capabilities on various scenes, such as Berlin, NYC, London, and more. The code and pre-trained models are open source and can be accessed on GitHub.

The discussion on this submission covers a wide range of topics related to the technology and its applications:

- Some users express their amazement at the capabilities of the SMERF technology, particularly in terms of rendering photorealistic scenes in real-time on smartphones.
- There is a discussion about the potential applications of this technology in the real estate market, allowing users to virtually navigate properties in a realistic manner.
- Users share their experiences and thoughts on the quality of the rendered scenes, with comments about specific locations like Berlin and NYC.
- There are comments about the spooky and ghostly effects seen in some of the rendered scenes, particularly reflections and blurriness.
- Discussions also touch on the technical aspects of the technology, such as the challenges of 3D reconstruction and rendering highly reflective surfaces.
- Some users express interest in the code and models used in SMERF, requesting access to them or asking about the licensing.
- There is also a mention of other related technologies, such as Gaussian Splatting and the use of DSLRs for photogrammetry.
- Finally, there are comments expressing excitement about the advancements in VR technology and the potential for future developments.

Overall, the discussion highlights both the impressive capabilities of the SMERF technology and the various technical and practical aspects related to its use.

### The limitations of deep learning (2017)

#### [Submission URL](https://blog.keras.io/the-limitations-of-deep-learning.html) | 43 points | by [andrelaszlo](https://news.ycombinator.com/user?id=andrelaszlo) | [13 comments](https://news.ycombinator.com/item?id=38635452)

In a post from the book "Deep Learning with Python," the author discusses the simplicity of deep learning and how it operates in a geometric space. Deep learning models use parametric models trained with gradient descent to transform input data into output data. This transformation is broken down into simple geometric transformations performed by different layers in the model. The key is that the transformation must be differentiable to allow for gradient descent. The author compares this process to uncrumpling a paper ball in 3D. However, there are limitations to what deep learning can accomplish. Tasks that require reasoning, algorithmic-like data manipulation, and long-term planning are out of reach for deep learning models. Additionally, most programs cannot be expressed as continuous geometric transformations, making them difficult to learn. Increasing the number of layers and training data can only partially alleviate these limitations. The author also warns against anthropomorphizing deep learning models, as they do not truly understand the content they are working with.

The discussion on this post revolves around several points. 
One commenter argues that anthropomorphizing AI and treating it as if it has human-like consciousness is incorrect and stems from a narrow perspective on intelligence. They believe that animals possess various forms of intelligence and consciousness that are different from human cognition, and it is incorrect to attribute human-like qualities to AI.
Another commenter disagrees and suggests that AI does not possess significant cognitive abilities or intelligence similar to humans. They argue that animals have different types of intelligence and consciousness, and AI does not possess those qualities.
In response to the discussion on anthropomorphizing AI, another commenter brings up the limitations of deep learning. They discuss that it is not feasible for deep learning models to generate papers, codebases, or complex systems just by reading product descriptions. They argue that the complexity threshold for generating functioning code is quite high, and deep learning models would require multiple rounds of correction and a significant amount of time to achieve any level of success.
Another commenter points out that the original post from the book "Deep Learning with Python" has limitations in terms of its perspective on AI progress. They believe that progress has been slower than predicted, and there are still many challenges to overcome in natural language processing and generalization.

Overall, the discussion touches on the limitations of deep learning, the anthropomorphizing of AI, and the challenges in AI progress.

### Google Imagen 2

#### [Submission URL](https://cloud.google.com/blog/products/ai-machine-learning/imagen-2-on-vertex-ai-is-now-generally-available) | 237 points | by [geox](https://news.ycombinator.com/user?id=geox) | [178 comments](https://news.ycombinator.com/item?id=38628417)

Google Cloud has announced the general availability of Imagen 2 on Vertex AI, their advanced text-to-image technology. Imagen 2 allows customers to generate high-quality, photorealistic, and aesthetically pleasing images from natural language prompts. It also supports text rendering in multiple languages, logo generation, visual question and answering, and more. Imagen 2 is integrated with safety features to ensure responsible AI usage, including digital watermarking and safety filters. Customers such as Snap, Shutterstock, and Canva have already started leveraging Imagen API to enhance their products and services.

The discussion on Hacker News about Google Cloud's announcement of Imagen 2 on Vertex AI revolves around various topics. 
One user mentions trying to use Imagen 2 but facing issues with changing JavaScript variables, while another user points out that the documentation is incomplete. There is also a comment expressing disappointment in Google's presentation of their AI products.
The discussion then diverges into a comparison between Android and iOS, with users discussing the advantages and disadvantages of each platform.
Some users express frustration with Google's marketing tactics and difficulty in accessing AI tools like Imagen. Others mention the importance of stability and reliability in AI models and caution against cherry-picking impressive examples.
There is a comment about the difficulty in understanding composition instructions when generating images using AI models like Imagen, and a suggestion to use text-to-music generation as an alternative.
The discussion also touches on the nature of AI advancements and the challenges designers and illustrators face in adapting their work to different platforms.

Overall, the discussion covers a range of topics including technical issues, marketing strategies, and the complexities of generating images using AI models.

### Artificial intelligence systems found to excel at imitation, but not innovation

#### [Submission URL](https://techxplore.com/news/2023-12-artificial-intelligence-excel-imitation.html) | 118 points | by [Brajeshwar](https://news.ycombinator.com/user?id=Brajeshwar) | [109 comments](https://news.ycombinator.com/item?id=38627816)

Artificial intelligence (AI) systems excel at imitation but struggle with innovation, according to researchers at the University of California, Berkeley. While humans, including young children, are able to find novel uses for everyday objects, AI systems lack this ability. The researchers conducted experiments comparing the performance of AI language models with that of children and adults. They found that while the AI models could imitate human responses, their ability to innovate and find non-obvious solutions was lacking. The researchers suggest that AI systems act more like "libraries" or search engines, summarizing existing knowledge rather than creating new ideas. However, they also note that there is still much to be learned about AI and its potential for innovation in the future.

The discussion on the submission revolves around the topic of AI's ability to innovate compared to humans. Some comments argue that AI's lack of innovation is due to its composition and reliance on existing knowledge, while others point out that innovation requires a combination of randomness and composition. There is also debate about whether innovation should be based on random generation or tested combinations. Some users argue that AI models have limited training in text messages, while others believe that machines with higher fidelity connections to the world can manipulate objects and generate higher-level concepts. The limitations and potential of AI in terms of creativity and innovation are also discussed, with some users emphasizing the role of observation and experience in human creativity. Additionally, there is discussion about the difference in quality and content between AI-generated and human-generated content.

### Partnership with Axel Springer to deepen beneficial use of AI in journalism

#### [Submission URL](https://openai.com/blog/axel-springer-partnership) | 26 points | by [todsacerdoti](https://news.ycombinator.com/user?id=todsacerdoti) | [10 comments](https://news.ycombinator.com/item?id=38627619)

OpenAI has partnered with publishing house Axel Springer to integrate journalism into AI technologies. This collaboration will enhance the user experience of OpenAI's ChatGPT by incorporating recent and authoritative content from Axel Springer's media brands, such as POLITICO, BUSINESS INSIDER, BILD, and WELT. Users will receive summaries of selected global news articles, even those behind paywalls, with attribution and links to the full articles for transparency. The partnership aims to support independent journalism, improve content experiences, and create new financial opportunities. Additionally, Axel Springer's quality content will contribute to the training of OpenAI's large language models.

The discussion on the submission revolves around various aspects of the partnership between OpenAI and Axel Springer.

- One user criticizes Axel Springer's Bild-Zeitung and accuses it of being a sensationalist newspaper with poor journalistic ethics.
- Another user expresses disappointment with Axel Springer's content on Samsung phones' Upday app, referring to it as trash and suggesting that OpenAI should be careful about partnering with such companies.
- One user comments on the irony of OpenAI, a company known for its focus on AI ethics, integrating journalism from a publication accused of sticking to its own facts.
- The CEO of Axel Springer, Mathias Dopfner, is mentioned by a user in a negative light for allegedly using derogatory terms to describe East Germans and being politically biased.
- The potential dangers of AI-generated news and the impact on people's trust in journalism are discussed.
- A user shares a tweet from OpenAI announcing the partnership with Axel Springer.
- One user expresses concern about the inclusion of news in ChatGPT, suggesting that it could lead to biased or worrisome news consumption.

Overall, the comments highlight skepticism and concerns about the collaboration between OpenAI and Axel Springer, particularly in terms of the journalistic ethics and content quality of Axel Springer's media brands.

### QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models

#### [Submission URL](https://arxiv.org/abs/2310.16795) | 41 points | by [titaniumtown](https://news.ycombinator.com/user?id=titaniumtown) | [11 comments](https://news.ycombinator.com/item?id=38632390)

Researchers Elias Frantar and Dan Alistarh have developed a new compression and execution framework called QMoE, which allows for the practical compression of trillion-parameter models to less than 1 bit per parameter. This addresses the memory problem associated with large language models (LLMs) using mixture-of-experts (MoE) architectures. In their paper, titled "QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models," the authors describe how QMoE can compress the 1.6 trillion parameter SwitchTransformer-c2048 model to less than 160GB, enabling the execution of trillion-parameter models on affordable commodity hardware. The framework achieves this with only minor accuracy loss and runtime overheads, making it a significant step toward more accessible and efficient deployment of large language models.

The discussion on Hacker News revolves around different aspects of the QMoE compression framework for trillion-parameter models. 
One user, "iTokio," comments that the affordable commodity hardware needed to run large models would include a single server with four NVIDIA A6000 and eight NVIDIA 3090 GPUs. They express excitement about the potential for affordable hardware.
Another user, "jtrnk," mentions that the cost of running models is reasonable at $4/hour, suggesting that the affordability of deployment is a significant advantage.
In response, user "wthnbrdm" discusses the cost of living in different countries and states that even in low-cost regions, the expenses for running large models can be too high.
A user named "nine_k" compares the cost of running models to the price of a hamburger, stating that if someone cannot afford a hamburger, then there may be problems with compressing trillion-parameter models.
User "vmch" humorously adds to the previous comment, saying that they cannot afford a hamburger either.
The user "sms" contributes by stating that running models for research purposes is affordable, especially since the starting points are small self-bootstrapped startups. However, they acknowledge that the expenses may become significant for data scientists working with full-scale models.
User "rnsr" suggests that NVIDIA's prices distort people's perception of affordability.
Moving on to the technical aspects of QMoE, user "krmkz" explains that QMoE can compress the 16 trillion-parameter SwitchTransformer-c2048 model to 160GB, achieving a compression ratio of 20x with 0.8 bits per parameter and only minor accuracy loss. They go on to explain briefly how QMoE achieves this compression level.
User "chssgck" expresses interest in whether QMoE exploits the low entropy of model parameters to achieve compression below 1 bit per parameter. They speculate that larger models might have smaller redundancy and warrant closer attention.
User "cynydz" suggests that sparse models might have negligible entropy, leading to almost zero compression using standard compression algorithms.
Lastly, user "kslm" offers a simple comment, stating "Nice."

### 2nd Batch of the A16Z Open Source AI Grant

#### [Submission URL](https://a16z.com/announcing-our-latest-open-source-ai-grants/) | 37 points | by [rajko_rad](https://news.ycombinator.com/user?id=rajko_rad) | [9 comments](https://news.ycombinator.com/item?id=38632827)

The announcement of the second batch of a16z Open Source AI Grant recipients has been made. The program aims to support the open-source AI ecosystem by providing grant funding to developers and small teams. This cohort focuses on two areas: tools for training language models and models and communities built around visual AI. The recipients include Common Crawl, Axolotl, SkyPilot, LMSys, LLaVA, Deforum, and Lucidrains. These projects contribute to strengthening the open-source AI ecosystem and advancing the field.

The discussion around the submission includes various comments about the different projects mentioned. Here are the key points:
- One user mentions that the support from a16z is significant for advancing the field of self-teaching process learning using Transformers in language models. They note that there are many implementations of Transformer-related papers in PyTorch, making it one of the largest publicly available collections.
- Another user points out that the grant recipients are not disclosed in the article, which sparks a brief conversation.
- There is a discussion about GPU strain monitoring, with one user mentioning that a GPU tool they checked showed a GPU utilization of 43%. Another user shares their love for this kind of technology.
- One user makes a comment about a hanging mobile, which is unrelated to the main topic.
- The mention of "Axolotl" in the submission title leads one user to think of the fictional creature from the Dune franchise. Another user clarifies that "Axolotl" refers to a type of salamander, and provides links to the creature's information.
- One user expresses surprise at the financial cost of the Common Crawl project, noting that it seems quite high for crawling a massively bloated, modern web.

Overall, the discussion touches on topics such as self-teaching process learning, GPU strain monitoring, the Dune franchise, and the cost of web crawling.

### First Impressions with Google Gemini

#### [Submission URL](https://blog.roboflow.com/first-impressions-with-google-gemini/) | 80 points | by [zerojames](https://news.ycombinator.com/user?id=zerojames) | [25 comments](https://news.ycombinator.com/item?id=38630349)

Google recently announced Gemini, a new Large Multimodal Model (LMM) that can process text, images, and audio. The Roboflow team analyzed Gemini's performance across various computer vision tasks and found that it excelled in some areas but struggled in others. Gemini is capable of answering questions about text, images, and audio. It launched with demos that showcase its ability to write code, explain math problems, find similarities between images, and more. However, there were claims that one or more demos were edited, raising doubts about the extent of Gemini's capabilities. Gemini has three versions: Ultra, Pro, and Nano. The Ultra model, which is currently unavailable, reportedly outperforms other LMMs on academic benchmarks. The Pro model is designed to scale across different tasks, while the Nano model is intended for use on mobile devices.

To run Gemini, you can use the Google Cloud Vertex AI Multimodal playground or send requests to the Gemini API. The API documentation provides more information on how to integrate Gemini into your applications. The Roboflow team evaluated Gemini on four computer vision tasks: Visual Question Answering (VQA), Optical Character Recognition (OCR), Document OCR, and Object Detection. Gemini performed well in some tests, accurately counting coins in an image and identifying a movie from a screenshot. However, it struggled with OCR, providing incorrect responses when asked to read a serial number or extract text from an image. Gemini's performance varied compared to other LMMs. For example, LLaVA, BakLLaVA, and CogVLM performed well in some tests where Gemini struggled. Overall, while Gemini shows promise in its multimodal capabilities, there are areas where it can be further improved.

You can try Gemini yourself using the Google Cloud Vertex AI Multimodal playground or explore its capabilities on the Roboflow Gemini playground page.

The discussion on the submission about Gemini, Google's new Large Multimodal Model (LMM), touched on various points:

- One commenter shared their experience with the web interface, mentioning intermittent performance in object detection and regularly running tests.
- There was speculation about whether Gemini could solve captchas related to safety.
- Some users commented on the readability and SEO optimization of the article, expressing a desire for more optimized articles that are easier to read.
- The analysis of Gemini's performance was discussed, with one commenter pointing out that Gemini struggled with certain tasks such as OCR but performed well in others.
- The formatting of the article was criticized for condensing the content too much and not providing sufficient depth, resulting in a summary that didn't fully capture the details.
- There was feedback on the inconsistencies and lack of clarity in the linking within the article, as well as repetitive screenshots that didn't provide much value.
- One commenter mentioned that they didn't find the article interesting enough to read it in Safari's reader mode.
- A comment pointed out that the discussion wasn't filtered by an AI prompt, which caused the responses to not flow smoothly.
- Some users discussed the possibility of using Gemini for data generation and training purposes.
- There was a question about how to achieve empty responses from the model using the HTTP API.
- Comparison between Gemini and GPT4 was mentioned, with Gemini reportedly outperforming GPT4 in some tests.
- There was an exploration of using GPT4 Vision directly through the API and analyzing its responses to image-related prompts.
- The effectiveness of Gemini's response to a prompt about counting coins was debated, with some users expressing doubts about the intelligent reasoning behind the answer.
- Overall, there were comments appreciating the analysis and discussing Gemini's performance in comparison to other models.

### GM says it's dropping Apple CarPlay and Android Auto because they're unsafe

#### [Submission URL](https://jalopnik.com/gm-drops-apple-carplay-android-auto-unsafe-phone-1851093013) | 181 points | by [thunderbong](https://news.ycombinator.com/user?id=thunderbong) | [415 comments](https://news.ycombinator.com/item?id=38622476)

General Motors (GM) is facing criticism for its decision to drop Apple CarPlay and Android Auto in favor of its in-house system, Ultifi. To defend its decision, GM claims that the popular phone mirroring programs actually pose safety risks by encouraging drivers to use their phones while behind the wheel. GM's head of product for infotainment, Tim Babbit, cited stability issues with CarPlay and Android Auto, such as bad connections and slow responses, which lead drivers to pick up their phones and take their eyes off the road. GM believes that if their in-house system is robust enough, drivers will be less likely to rely on their phones for their infotainment needs. The Ultifi system, debuting in the 2024 Chevy Blazer EV, uses Google apps like Maps and Assistant for enhanced voice controls. Additionally, GM is hoping to profit from driver data and subscription services through the Ultifi system. The success of this gamble remains to be seen as more GM vehicles integrate with Ultifi from next year.

The discussion on Hacker News revolves around the decision by General Motors (GM) to drop Apple CarPlay and Android Auto in favor of its in-house system, Ultifi. Some users sympathize with GM's position, noting that phone mirroring programs like CarPlay and Android Auto can be unreliable and may encourage drivers to use their phones while driving. Others argue that GM's decision is driven by self-interest and a desire to profit from driver data and subscription services. The discussion also touches on the reliability of infotainment systems in general, with users sharing their experiences with different car brands. Some users express concerns about the increasing control that Apple and Google have over the automotive industry, while others believe that vehicle manufacturers should focus on building their own in-house systems. Overall, the discussion highlights the varying opinions on the role of phone mirroring programs and the future of infotainment systems in cars.

---

## AI Submissions for Tue Dec 12 2023 {{ 'date': '2023-12-12T17:11:21.291Z' }}

### 'Biocomputer' combines lab-grown brain tissue with electronic hardware

#### [Submission URL](https://www.nature.com/articles/d41586-023-03975-7) | 67 points | by [pseudolus](https://news.ycombinator.com/user?id=pseudolus) | [51 comments](https://news.ycombinator.com/item?id=38611422)

Researchers have developed a hybrid biocomputer by combining laboratory-grown human brain tissue with electronic circuits. The system, called Brainoware, uses brain organoids made from stem cells that can differentiate into neurons. The researchers placed the organoids onto a plate containing thousands of electrodes to connect them to electric circuits. The organoids were then trained to perform tasks such as voice recognition, achieving an accuracy of 78%. The technology could potentially be integrated into artificial intelligence systems and used for modeling the brain in neuroscience research. It could also be utilized to study neurological disorders and test treatments. However, challenges remain in keeping the organoids alive and in scaling up the complexity of tasks they can perform.

The discussion on this submission covers various aspects of the topic at hand, with some users providing additional information and perspectives. 
One user expressed concerns about the ethical and philosophical implications of creating a biocomputer using lab-grown human brain tissue. They argued that there are potential risks and discomfort associated with creating conscious entities. Another user disagreed with this perspective, stating that neurons do not have the capacity to feel pain and therefore would not suffer.
There was also a discussion about whether AI systems can truly experience consciousness. Some users argued that consciousness is dependent on complex systems and sensory components, while others pointed out that AI algorithms do not have motivations or subjective experiences.
The topic of animal suffering and the moral implications of AI intelligence and consciousness were brought up. Some users discussed the significance of animal suffering and the brutality of nature, while others argued that anthropomorphizing AI carries risks.
Other users shared relevant information and links. One user provided a link to a paper on self-assembled genetic algorithms, while another shared a video about growing rat and human neurons connected to a computer.
Overall, the discussion revolved around the ethical implications, the nature of consciousness, and the potential applications of the hybrid biocomputer technology.

### Show HN: Open-source macOS AI copilot using vision and voice

#### [Submission URL](https://github.com/elfvingralf/macOSpilot-ai-assistant) | 419 points | by [ralfelfving](https://news.ycombinator.com/user?id=ralfelfving) | [153 comments](https://news.ycombinator.com/item?id=38611700)

Introducing macOSpilot: your personal macOS AI assistant. This open-source project uses voice and vision-powered AI to answer questions about any application on your Mac. With just a keyboard shortcut, you can trigger the assistant, ask your question, and receive an answer in context and in audio within seconds.
Here's how it works: macOSpilot takes a screenshot of your active window and sends it, along with a transcript of your question, to OpenAI's GPT Vision. The answer is then displayed in text and converted into audio using OpenAI's TTS (text-to-speech).
The best part is that it works with any application on macOS. No need to switch between windows or tabs. Simply press the keyboard shortcut, speak your question, and macOSpilot will provide the answer in an overlay window and in audio.
To get started, you'll need to install the NodeJS project and dependencies. Once configured with your OpenAI API key, you can run macOSpilot in the background. When you need to use it, just press the keyboard shortcut, speak your question, and macOSpilot will take care of the rest.
Check out the GitHub repository for more details on installation and usage. And for a video walk-through and explanation of how it works, head over to YouTube.

The discussion on Hacker News revolves around the macOSpilot project, which is an open-source AI assistant for macOS. Some users point out that the name "macOSpilot" is inconsistent with Apple's naming conventions and suggest using "cOSXpilot" or "cOSXpilot" instead. Others discuss the potential trade-offs of using voice commands versus text input for interacting with the assistant. One user shares a similar project they wrote for Linux using keyboard shortcuts. Another user raises concerns about the cost of using the OpenAI API and recommends being mindful of the spending limits. Some users appreciate the feature of taking screenshots to assist with specific inquiries about the interface of applications. The discussion also diverges into topics such as music production software, integrating AI assistants in different workflows, and the potential privacy implications of using AI assistants. There is also appreciation for the project and gratitude for sharing it with the community.

### AI’s big rift is like a religious schism

#### [Submission URL](https://www.programmablemutter.com/p/the-singularity-is-nigh-republished) | 294 points | by [anigbrowl](https://news.ycombinator.com/user?id=anigbrowl) | [512 comments](https://news.ycombinator.com/item?id=38616888)

In an article titled "The Singularity is Nigh!" republished from The Economist, the author explores the cult-like battles between two factions within the field of artificial intelligence (AI): E-Acc (engineers of AI) and the AI Doomers. The E-Acc sect believes in progress and embraces the potential of AI, while the AI Doomers are gripped by fear and foresee negative consequences of advancing technology. This religious schism has overshadowed practical discussions on the implications of AI. Both sects were influenced by science fiction, with some optimists predicting a future in which humans become immortal and merge with AI, while others fear the potential risks and existential threats AI might pose. The article delves into the rationalist movement, which seeks to improve human reasoning and mitigate AI risks. It also explores the shifting dynamics between Silicon Valley's profit-driven model and the concerns of rationalists. The backlash against the AI Doomers is transforming into a new religion of techno-optimism led by influential figures in Silicon Valley.

The discussion on this submission covers a range of topics related to AI and its complexity. Some users discuss the concept of the Singularity and its portrayal in science fiction, while others mention the impact of AI on different industries such as archaeology. There is also a debate about the simplification of complex systems and the potential limitations of human comprehension. Additionally, there is a discussion about the role of programming and the challenges of cognitive dissonance in the field of AI. One user brings up the idea of a "Programmer-t-Arms," implying a deep connection between software technology and human cognition. Overall, the discussion touches on various aspects of AI, from its potential implications to its relationship with other fields and its impact on society.

### Phi-2: The surprising power of small language models

#### [Submission URL](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/) | 242 points | by [birriel](https://news.ycombinator.com/user?id=birriel) | [93 comments](https://news.ycombinator.com/item?id=38614361)

Microsoft Research has released Phi-2, a 2.7 billion-parameter language model that demonstrates exceptional reasoning and language understanding capabilities. The model outperforms larger models on complex benchmarks and is available in the Azure AI Studio model catalog for researchers. Phi-2 achieved its superior performance by focusing on the quality of the training data, using "textbook-quality" data and carefully selected web data. By transferring knowledge from the previous model, Phi-1.5, to Phi-2, the team was able to accelerate training convergence and improve benchmark scores. Despite not undergoing reinforcement learning from human feedback, Phi-2 exhibited better behavior in terms of toxicity and bias compared to existing models.

The discussion around the submission primarily focuses on the comparison between Phi-2 and larger language models like GPT-3. Some users highlight the difference in the number of parameters and training data between the models. It is noted that Phi-2 has significantly fewer parameters but was still able to outperform larger models in certain benchmarks. The quality of the training data is emphasized as a crucial factor in Phi-2's performance, with the team using "textbook-quality" data and carefully selected web data. There is also a discussion about the legality and copyright aspects of training models with certain datasets. Some users raise concerns about the limited capabilities of models like Phi-2 and the importance of human feedback in learning. Other topics discussed include the potential challenges in collecting training data similar to how babies learn and the cost of training models like Phi-2. Some users suggest alternative methodologies like distilling or using synthetic datasets to train smaller models. The concept of feedback loops in language models and their impact on quality is also discussed. Finally, there is a brief discussion about the limitations of the Phi models and the potential benefits of specialized models versus generalized models.

### NASA says SpaceX’s next Starship flight could test refueling tech

#### [Submission URL](https://arstechnica.com/space/2023/12/nasa-wants-to-see-gas-stations-in-space-but-so-far-its-tanks-are-empty/) | 146 points | by [_Microft](https://news.ycombinator.com/user?id=_Microft) | [194 comments](https://news.ycombinator.com/item?id=38612585)

SpaceX and NASA could be taking a step towards orbital refueling on the next test flight of Starship. NASA has expressed interest in demonstrating orbital refueling technology, which could lead to propellant depots in space for rockets heading to destinations beyond Earth. In 2020, NASA entered agreements with four companies, including SpaceX, to prove capabilities in refueling and propellant depots. These capabilities involve cryogenic fluids such as liquid hydrogen, methane, and liquid oxygen, which must be maintained at extremely low temperatures. Extending the lifetime of these super-cold propellants requires new technologies, including automated couplers, flow meters, and advanced insulation. SpaceX appears to be on track to complete the tasks outlined in its agreement, paving the way for future demonstrations involving docked Starships in Earth orbit and eventually a crew landing on the Moon.

The discussion on Hacker News about the submission revolves around various aspects of orbital refueling, the challenges involved, and the feasibility of SpaceX's plans.

One user points out the cost implications of orbital refueling and expresses surprise that NASA would invest in such experiments if Starship doesn't reach orbit. Another user responds, explaining that the assumption is faulty since NASA's 2020 agreement with SpaceX includes the transfer of cryogenic propellants of liquid oxygen, which is already present on Starship.
There is a discussion about the challenges of cryogenic fuel transfer and the need for propellant management devices to ensure successful transfer. Users discuss techniques such as maintaining pressure, pumplng, and the use of flexible couplings. There are also discussions on the concept of ullage, propellant mixing, and the safety aspects of handling flammable propellants.
Some users bring up the potential difficulties of propellant transfer in space, including the challenges of gravity, stability during acceleration, and the movement of fuel in tanks. Potential solutions such as using smaller thrusters, rotating tanks, and propellant management devices are discussed.
One user mentions the challenge of quick and efficient transfer in space and the sensitivity of rocket combustion to changes in propellant flow. Another user references their experience in Kerbal Space Program (KSP) and compares real-life attempts to the game.
The discussion also touches on Starship's development and the reliability of the Super Heavy booster for successful missions. There is debate about the ability of the heat shields to hold up during re-entry and the potential need for smaller tiles and better thermal management.
Finally, users discuss the cost and feasibility of both the booster and the orbiter, with a comparison to the Space Shuttle program and the issues it faced. There is also a mention of the lack of wings on Starship and a light-hearted comparison to Siberian Flying Squirrels.

### Role-playing with AI will be a powerful tool for writers and educators

#### [Submission URL](https://resobscura.substack.com/p/roleplaying-with-ai-will-be-powerful-tool) | 148 points | by [benbreen](https://news.ycombinator.com/user?id=benbreen) | [92 comments](https://news.ycombinator.com/item?id=38612164)

In a recent article by Benjamin Breen on resobscura.substack.com, the author explores the use of AI as a tool for writers and educators. Breen discusses the potential of generative AI to simulate historical events and experiences, allowing users to actively engage and explore alternative versions of the world. He believes that rather than replacing human authors, AI can enhance experiential learning and provide a powerful tool for understanding different time periods. Breen shares his own experience using AI historical simulations in his world history class at UCSC, where students reported a greater understanding of the time period and the ability to make choices as historical actors. While accuracy remains a challenge for AI historical simulations, Breen emphasizes the importance of discussing and reflecting on historical inaccuracies. He also highlights the value of AI in creatively imagining another world, helping to kickstart historical imagination and empathy. Breen provides an example of a simulated acid trip in 1963, showcasing how AI can contribute to the "vibes-based" elements of writing history. Overall, Breen sees the potential for AI to be a valuable tool in the hands of writers and educators, allowing for immersive and experiential learning experiences.

The discussion on Hacker News surrounding the submission about the use of AI as a tool for writers and educators covers a variety of topics. 

Some users point out the inherent challenges of relying on AI systems for generating historical simulations, citing concerns about the accuracy of information and the potential for the AI to propagate inaccuracies. They highlight the need for systems to verify information and detect and correct mistakes.
Others mention the problem of widely inaccurate school textbooks and advocate for the use of AI in generating historical questions to encourage critical thinking and judgment.
There is a discussion about the limitations of AI language models like LLMs in understanding context and providing accurate responses. Some users express their distrust in search engines and AI-generated summaries, citing instances of misleading or incorrect answers.
The debate extends to the role of AI in education and the potential benefits and drawbacks of using AI as a teaching tool. Some argue that AI could expand the scope of learning materials and provide personalized instruction, while others express concerns about AI replacing human teachers and the limitations of AI in understanding complex subjects.

Overall, the discussion highlights the potential of AI as a tool for enhancing education but also raises concerns about accuracy, bias, and the limitations of AI systems.

### MemoryCache: Augmenting local AI with browser data

#### [Submission URL](https://future.mozilla.org/blog/introducing-memorycache/) | 443 points | by [NdMAND](https://news.ycombinator.com/user?id=NdMAND) | [96 comments](https://news.ycombinator.com/item?id=38614824)

Mozilla recently introduced MemoryCache, an early exploration project that aims to augment an on-device, personal model with local files saved from the browser, offering users a more personalized and tailored experience while prioritizing privacy and agency. The project currently consists of a Firefox extension that allows users to save web pages and notes to their local machine, a shell script that monitors changes in the saved files directory, and code to enable saving web pages as PDF for easier readability. Mozilla sees MemoryCache as a sandbox for experimenting with unique aspects of the brainstorming and idea generation process. The project is still in its early stages and can be followed on GitHub or through Mozilla's website to stay updated on its progress.

The discussion on this submission covers a range of topics related to local models, privacy, and browsing history. Some users point out that loading personal data remotely may not be ideal and suggest using local models instead. Others discuss the limitations of local models and the need for high-end hardware for training them. There is also a conversation about the implications of personal AI models and the potential for decentralized and private AI networks. Some users mention the importance of search history and bookmark indexing for personalized experiences, while others suggest alternative solutions like WorldBrain and Rewindai. The discussion ends with a comment acknowledging the experimental nature of the project and the funding it requires.

### Arena Group fires CEO in wake of Sports Illustrated AI articles scandal

#### [Submission URL](https://www.theguardian.com/technology/2023/dec/12/arena-group-ceo-ross-levinsohn-fired-sports-illustrated-ai-articles) | 74 points | by [dpflan](https://news.ycombinator.com/user?id=dpflan) | [40 comments](https://news.ycombinator.com/item?id=38614585)

The CEO of Arena Group, the publisher of Sports Illustrated, has been fired in the wake of an AI-generated articles scandal. Ross Levinsohn's termination comes after it was revealed that Sports Illustrated had published articles written by fake authors with AI-generated headshots and biographies. The board of Arena Group stated that the decision was made to "improve the operational efficiency and revenue of the company." The AI scandal was exposed by a report from the science and technology news publication Futurism, which uncovered multiple fake profiles and articles. The Arena Group denied the allegations and claimed that the articles were sourced commercial content from a third-party advertising company, AdVon Commerce. The company has since ended its partnership with AdVon and removed its content from Arena websites.

The discussion on this submission revolves around various aspects of the CEO's firing and the use of AI-generated articles.
One commenter points out that the CEO is a majority shareholder of the Arena Group's parent company, and it makes sense that he left considering the scandal surrounding the AI-generated articles.
Another commenter finds it interesting that the founder of a well-known energy drink company serves as the CEO of Arena Group, indicating that they are surprised by the CEO's background.
Another commenter mentions that they have tried the 5-Hour Energy Drink, and while they find it to be effective, they also mention some negative side effects.
There is a discussion on whether the AI-generated articles were misleading or not. Some argue that the articles were technically correct, while others call them misleading and suggest that the AI executives are responsible for the scandal.
One commenter suggests that the use of AI-generated articles is a cost-cutting measure to increase operational efficiency.
There are comments expressing skepticism about the use of AI-generated articles, with some suggesting that it is unethical and fraudulent.
Another commenter mentions that AI-generated content is widely used for advertising purposes.
One commenter suggests that leadership publications should learn from this incident and be more cautious in publishing AI-generated articles.
There is a discussion about the CEO's history and previous controversies, as well as his tenure at various companies.
A commenter raises doubts about the claims made in the article and suggests that the AI connection may not be fully verified.
One commenter mentions that reliance on AI can lead to job losses and negatively impact the business.
There are comments debating whether the CEO deserved to be fired and discussing the definition of "damaging behavior."
Several comments express concern about the erosion of trust and the impact of AI on journalism.
One commenter argues that innovation requires taking risks.
Overall, the discussion touches on the implications of AI-generated articles, the ethical concerns surrounding their use, and the consequences for those involved in the scandal.

### FTC wants Microsoft's relationship with OpenAI under the microscope

#### [Submission URL](https://www.theregister.com/2023/12/11/microsoft_openai_investment_ftc/) | 245 points | by [magoghm](https://news.ycombinator.com/user?id=magoghm) | [53 comments](https://news.ycombinator.com/item?id=38607540)

The US Federal Trade Commission (FTC) is reportedly considering investigating Microsoft's investment in OpenAI to determine if any antitrust rules have been broken. Despite investing over $10 billion in OpenAI, Microsoft does not have control of the company, as it remains a non-profit organization. Microsoft's Chief Communications Officer emphasized that the company does not own any portion of OpenAI but is entitled to a share of profit distributions. The FTC's potential investigation follows the recent departure and return of OpenAI's boss, which led Microsoft to appoint a non-voting observer to OpenAI's board. Additionally, the UK's Competition and Markets Authority (CMA) has launched a consultation to explore Microsoft's relationship with ChatGPT developer and determine if it could skew competition. The CMA will initiate an official inspection if necessary. The FTC has already been investigating OpenAI's ChatGPT over privacy and reputational concerns and has previously attempted to undo Microsoft's $69 billion merger deal with Activision Blizzard.

The discussion on Hacker News revolves around several key points:

1. Ownership Structure: There is discussion about the ownership structure of OpenAI and Microsoft's investment. Some users point out that Microsoft is a minority owner and does not have control over OpenAI, while others argue that the financial arrangements between the two companies imply some level of ownership.
2. Antitrust Concerns: The potential investigation by the US Federal Trade Commission (FTC) is seen as a response to the perceived antitrust implications of Microsoft's investment in OpenAI. Users speculate on the motives behind the FTC investigation and discuss the impact it may have on both companies.
3. Comparison to Salesforce: The discussion brings up Salesforce's similar offer to hire employees from Microsoft, indicating that such actions are not uncommon in the tech industry. However, it is noted that Salesforce did not face the same scrutiny as Microsoft in their case.
4. OpenAI's Collaboration and Competition: Questions are raised about the relationship between OpenAI and Microsoft, wondering if OpenAI can exist independently from Microsoft and collaborate with other tech companies in a meaningful way. The practical differences between the privacy and user experience approaches of OpenAI and Microsoft are also discussed.
5. Microsoft's Track Record: Some users bring up Microsoft's track record in terms of antitrust issues, mentioning past cases and suggesting that the FTC may be scrutinizing the company due to its history.
6. FTC's Role and Progress: The role of the FTC in safeguarding competition and protecting consumer interests is debated. Some users express skepticism about the FTC's effectiveness in addressing anticompetitive practices, while others see the potential investigation as a sign of progress under the leadership of Lina Khan.

Overall, the discussion touches on topics such as market competition, ownership structures, regulatory oversight, and the potential impact of the FTC investigation on Microsoft and OpenAI.

### AI made from living human brain cells performs speech recognition

#### [Submission URL](https://www.newscientist.com/article/2407768-ai-made-from-living-human-brain-cells-performs-speech-recognition/) | 18 points | by [moneil971](https://news.ycombinator.com/user?id=moneil971) | [5 comments](https://news.ycombinator.com/item?id=38615584)

Scientists at Indiana University Bloomington have successfully used brain organoids to perform basic speech recognition tasks. Brain organoids are clusters of nerve cells grown from stem cells, which mimic the structure and function of the human brain on a smaller scale. In this study, the organoids were trained to recognize the voice of a specific individual from a set of audio clips, achieving an accuracy of 70 to 80% after two days of training. This research is part of a broader effort to explore the potential of biocomputing using living nerve cells, which could offer advantages such as reduced energy consumption and improved information processing compared to conventional silicon chips. However, there are still significant challenges to overcome, including the limited lifespan of organoids and the need for further improvements in performance.

The discussion on this submission begins with user "thrn" expressing skepticism and pushing back against the idea of using brain organoids for speech recognition tasks. They suggest that progress in this field is slow and doubts the practicality of the research.

User "jrschrdr" agrees with "thrn" and emphasizes the need for caution, mentioning the potential ethical considerations. Another user, "Grimblewald," responds to this comment, dismissing the concern and suggesting that newborns in schools don't feel pain.

User "mdkkrs" chimes in, expressing excitement and inquiring about the possibility of using quantum computing and energy conversion systems for extended manipulation and vision sensors.

Overall, the discussion involves skepticism about the practicality and speed of progress in brain organoid research, concerns about ethical implications, and questions about the potential for advanced computing systems.