import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Sat Dec 02 2023 {{ 'date': '2023-12-02T17:09:38.438Z' }}

### Unsupervised speech-to-speech translation from monolingual data

#### [Submission URL](https://blog.research.google/2023/12/unsupervised-speech-to-speech.html) | 20 points | by [atg_abhishek](https://news.ycombinator.com/user?id=atg_abhishek) | [4 comments](https://news.ycombinator.com/item?id=38497549)

Google Research has introduced Translatotron 3, an unsupervised speech-to-speech translation architecture that can learn the translation task from monolingual data alone. Traditional speech-to-speech translation models rely on parallel speech data, which is scarce, leading to the need for synthesized data. However, Translatotron 3 eliminates the requirement for bilingual speech datasets by incorporating techniques such as back-translation, pre-training with SpecAugment, and unsupervised embedding mapping based on Multilingual Unsupervised Embeddings (MUSE). Experimental results between Spanish and English show that Translatotron 3 outperforms a baseline cascade system. By preserving paralinguistic characteristics, such as tone and emotion, Translatotron 3 aims to improve the quality and authenticity of translated speech.

The discussion revolves around the Translatotron 3 submission on Hacker News. One user, "xnx," comments on the challenges of translating languages and mentions that it often stretches the mind. Another user, "grsv," responds, stating that it is humans' sentience and comprehension that allows them to learn languages, and the proposed method relies on training models with monolingual speech-text datasets. "grsv" further explains that the proposed method utilizes a shared embedding space for languages, forcing the model to learn semantics independently of the language. Additionally, they mention using back-translation for training and conducting performance checks using a Spanish-English-Spanish translation loop. They express enthusiasm for the promising and interesting results, wondering how similar the languages need to be at a lexical level for the model's performance to excel. Another user, "IanCal," shares their curiosity about the level of complexity in English and internal linguistic representation, and suggests that if there is a low complexity in mapping internal representations, then a sensible sentence in English should result in a sensible sentence in the translated language.

Overall, the discussion focuses on the methodology and potential implications of Translatotron 3, with users expressing interest in the results and exploring the nuances of language translation.

### Galactic algorithm

#### [Submission URL](https://en.wikipedia.org/wiki/Galactic_algorithm) | 115 points | by [sockaddr](https://news.ycombinator.com/user?id=sockaddr) | [19 comments](https://news.ycombinator.com/item?id=38500782)

In computer science, there is a concept called a galactic algorithm. These are algorithms that have incredible theoretical performance but are never actually used in practice. There are a few reasons for this. Sometimes the performance gains only apply to problems that are so large they never occur in real-world scenarios. Other times, the complexity of the algorithm outweighs the relatively small gain in performance. These algorithms are named "galactic" because they will never be used on any data sets here on Earth.

Even though galactic algorithms are not used in practice, they can still contribute to computer science in a few ways. Firstly, they may introduce new techniques that can eventually be used to create practical algorithms. Secondly, as computational power advances, previously impractical algorithms may become feasible to use. Thirdly, even if an algorithm is impractical, it can still demonstrate that certain bounds can be achieved or prove that proposed bounds are incorrect, thereby advancing the theory of algorithms.

For example, there are galactic algorithms for problems like integer multiplication, matrix multiplication, communication channel capacity, sub-graph testing, cryptographic breaks, and the traveling salesman problem. These algorithms have impressive theoretical performance, but their large constants make them impractical for real-world use. However, they still serve important purposes. For instance, they can inspire further research and refinement to make them more practical or they can settle important open problems in computer science, like the P versus NP problem.

So, while galactic algorithms may never be used in practice, they still have value in advancing the field of computer science and pushing the boundaries of what is possible.

The discussion on this submission covers a range of topics related to galactic algorithms. One commenter shares a link to a paper that discusses a simulated annealing algorithm for solving global optimization problems. Another commenter expresses doubt about the practicality of these algorithms, suggesting that simulated annealing and random restarts may not be effective. Another user finds the topic interesting and mentions that numbers can be fascinating. 

One commenter asks if the concept of galactic algorithms can be applied to other fields. Another user shares a link to Optimal Universal Search, which is related to the topic of optimal algorithms. 

There is also a discussion about the classification of General Relativity as a galactic algorithm for solving Newtonian Equations. A user argues that General Relativity is not a true galactic algorithm because it provides similar answers when calculating the motion of spacecraft around a black hole. However, another user argues that the comparison is not accurate and that the complexity and practicality of General Relativity differ from galactic algorithms.

The discussion ends with a user mentioning that General Relativity is used in GPS to correct clock discrepancies based on its small correction below a galactic scale.

### Show HN: ChatCBT – AI-powered cognitive behavioral therapist for Obsidian

#### [Submission URL](https://github.com/clairefro/obsidian-chat-cbt-plugin) | 55 points | by [marjipan200](https://news.ycombinator.com/user?id=marjipan200) | [18 comments](https://news.ycombinator.com/item?id=38499722)

Introducing ChatCBT: an AI-powered journaling plugin for your Obsidian notes. Inspired by cognitive behavioral therapy (CBT), this plugin acts as a journaling assistant that helps you reframe negative thoughts and rewire your reactions to distressful situations. 

With ChatCBT, you can start chatting in a note and receive kind and objective responses to help uncover negative thinking patterns. Conversations are stored privately on your computer, and you can automatically summarize your reframed thoughts in a table to inspire affirmations. 

The plugin offers two options for handling your data: you can choose to use a cloud-based AI service (OpenAI) or a 100% local and private service (Ollama). OpenAI provides excellent conversation quality and speed, but it is a paid service. On the other hand, Ollama is free and offers good conversation quality.

To install ChatCBT, follow the instructions provided in the repository. The plugin is currently under review to become an official Obsidian Community Plugin, but you can still install it in developer mode. Once installed, you can configure an AI platform connection from the plugin settings menu.

Overall, ChatCBT is a powerful tool for journaling and self-reflection, leveraging the capabilities of AI to assist you in improving your mental well-being.

The discussion around the submission of ChatCBT on Hacker News covers various aspects of the plugin and its potential benefits.

One user raises concerns about the effectiveness of AI-powered therapy compared to traditional cognitive behavioral therapy (CBT). They argue that while AI may have potential, it is important to diagnose problems correctly, and certain issues require the guidance of a qualified human therapist.

Another user points out that the plugin installation may not work properly and reports encountering errors related to server problems. Another user suggests investigating the issue further.

A user mentions that relying on an AI plugin may discourage people from seeking help from licensed therapists. They argue that AI lacks the understanding and learning methods that are a significant part of therapy, and interacting with a human therapist provides a more effective way of improving one's life. They suggest exploring cheaper alternatives with qualified professionals instead of relying solely on AI.
In response to this, another user clarifies that ChatCBT is not intended to replace professional therapy but rather supplement it. They explain that the plugin is designed to provide interactive journaling similar to CBT worksheets that therapists provide to patients. They emphasize that it is not meant to replace human interaction but rather be used as a tool for self-reflection.
There is a discussion about the affordability of therapy and the challenges many people face in accessing mental health care. Some users express frustration with the stigmatization of mental health issues and the limited coverage provided by insurance providers, making therapy inaccessible to many.
One user points out that therapy is important and should not be undervalued, highlighting that licensed therapists can help people understand and work through their problems in a realistic and systematic way.
Another user suggests that AI could be beneficial in developing personal self-assistants, such as an interactive AI like GPT-4, which could have therapeutic effects and help individuals make decisions, manage regrets, and provide guidance similar to that of a powerful therapist.
Overall, the discussion reflects varying perspectives on the role of AI in mental health care, emphasizing the importance of professional therapy while acknowledging the potential benefits of AI-powered tools as supplements for self-reflection and journaling. There is also recognition of the challenges in accessing affordable and comprehensive mental health care.

### Optical effect advances quantum computing with atomic qubits to a new dimension

#### [Submission URL](https://www.tu-darmstadt.de/universitaet/aktuelles_meldungen/einzelansicht_410816.en.jsp) | 51 points | by [FinnKuhn](https://news.ycombinator.com/user?id=FinnKuhn) | [14 comments](https://news.ycombinator.com/item?id=38494466)

Scientists at the Technische Universität Darmstadt in Germany have developed a technique using the optical Talbot effect to increase the number of qubits in a quantum computer without requiring additional resources. Qubits are the basic units of information in quantum computing and can process both "0" and "1" simultaneously, allowing for parallel calculations. Currently, quantum computers are limited to a few hundred qubits, but for practical applications, such as optimizing traffic flows, thousands or millions of qubits are needed. The Darmstadt team's approach uses laser beams and a glass element with microlenses arranged like a chessboard to create a 3D lattice of focal points that can hold individual atoms as qubits. By exploiting the Talbot effect, multiple layers of qubits can be added without needing additional laser output. The researchers were able to create 16 layers of qubits, potentially allowing for over 10,000 qubits. The team plans to further develop the technology for applications in quantum technologies and high-precision optical atomic clocks.

The discussion on this submission revolves around various aspects of the technology and its implications.

- One user raises a concern about the potential difficulty of scaling this technology due to the largest single-qubit coherence time limitation.
- Another user counters this argument, stating that quantum error correction schemes help mitigate coherence limitations and make it easier to achieve thousands of qubits.
- A user suggests that the discovery of transistors in the past took 50 years to reach mass production, and it is expected that the development of quantum computers will follow a similar incremental timeline.
- The difficulty of developing transistors for quantum computing is discussed, with one user suggesting that it is significantly harder to create transistors for quantum computing compared to classical computing.
- Another user raises the point that the statement about the difficulty of developing transistors for quantum computing is redundant and does not make sense in the context of the discussion.
- The challenges of funding quantum technology development are mentioned, with private funding playing a significant role compared to government investment.
- A user believes that decoherence times for qubits are becoming longer, making natural times higher and enabling better performance.
- Discussion moves towards the technical aspects of qubit coupling and the configuration of individual qubits.
- One user shares a link to a paper that may provide more information on the topic.
- Another user shares a different paper that describes a scalable multi-layer architecture for single-time qubit arrays using a three-dimensional Talbot interferometer lattice.

Overall, the discussion covers topics such as scalability, coherence limitations, development timelines, funding challenges, and technical aspects of qubit coupling and configuration.

### Scalable extraction of training data from (production) language models

#### [Submission URL](https://arxiv.org/abs/2311.17035) | 99 points | by [wazokazi](https://news.ycombinator.com/user?id=wazokazi) | [14 comments](https://news.ycombinator.com/item?id=38496715)

In a paper titled "Scalable Extraction of Training Data from (Production) Language Models," researchers explore the concept of extractable memorization, referring to training data that can be extracted from machine learning models through queries without prior knowledge of the dataset. The study reveals that adversaries can efficiently extract gigabytes of training data from various language models, including Pythia, GPT-Neo, LLaMA, Falcon, and ChatGPT. The researchers demonstrate that existing techniques can attack unaligned models, and they develop a new divergence attack specifically targeting ChatGPT. This attack causes the model to deviate from its chatbot-style responses, resulting in the emission of training data at a rate 150 times higher than normal. The findings suggest that current alignment techniques do not eliminate memorization and expose the potential for practical attacks to recover more data than previously thought.

The discussion around the submission revolves around various aspects of the research paper on scalable extraction of training data from language models.

- One commenter discusses the use of memorization techniques to reduce hallucinations and improve the relevance of passages retrieved by language models.
- Another user suggests that the issue of memorization is not surprising, considering that language models like ChatGPT have access to vast amounts of internet data. They mention that DeepMind recently extracted personally identifiable information (PII) from ChatGPT prompts.
- There is a question raised about whether language models are trained on secret data. The response suggests that they likely contain copyrighted data, such as lyrics from songs or track crashes.
- A user raises a question about the capacity of models to memorize an infinite amount of information and whether the extraction of total memorized data is possible. The response highlights that language models are restricted to a limited number of keywords and that the size of the training dataset is a determining factor.
- A discussion ensues about the capabilities of models like GPT-NeoX in generating grammatically novel sentences based on different settings and training inputs.
- A user clarifies that the 50-grams mentioned in the research paper refer to substrings of 50 words from the dataset, and generating complete 50-grams is challenging even for a modern GPU.
- There is further clarification about creating the 50-gram dataset and its relevance to the research paper's findings.

Overall, the discussion delves into the implications and limitations of language models' memorization capabilities and the concerns surrounding the extraction of training data.

### Meta will enforce ban on AI-powered political ads in every nation, no exceptions

#### [Submission URL](https://www.zdnet.com/article/meta-will-enforce-ban-on-ai-powered-political-ads-in-every-nation-no-exceptions/) | 111 points | by [jnord](https://news.ycombinator.com/user?id=jnord) | [42 comments](https://news.ycombinator.com/item?id=38495875)

Meta, the parent company of social media platforms like Facebook and Instagram, has announced that it will enforce a ban on AI-powered political ads in all nations, without exceptions. This comes as several countries are set to hold elections next year. The ban extends to ads targeting specific services and issues related to politics, elections, housing, employment, credit, social issues, health, pharmaceuticals, and financial services. Meta's generative AI advertising tools, which include features like text variation and image cropping, will not be accessible for these types of campaigns. Meta has emphasized the importance of AI and plans to add generative AI capabilities across all its platforms. The company's Ads Manager tool serves as a launchpad for running ads, providing advertisers with an all-in-one tool for ad creation, management, and tracking. It also recently introduced an AI chatbot called Meta AI and an AI image generator tool called Emu. Meta's AI products have been adopted by more than half of advertisers, with the Advantage+ tools helping advertisers achieve a $10 billion run rate from shopping campaigns.

The discussion on Hacker News revolves around Meta's announcement to ban AI-powered political ads on its platforms. Some users express skepticism about the effectiveness of Meta's detection algorithms, highlighting challenges in accurately detecting and preventing AI-generated content. Others argue that the ban is necessary to prevent exploitation and misleading advertising. There is also a debate about Meta's intentions and trustworthiness as an organization. Some users question whether Meta's ban is selective and if it will be effectively enforced. There is a suggestion that AI detectors may be developed to identify AI-generated political ads. Additionally, there are discussions about the limitations of AI-generated text and images and the impact of the ban on political campaigns. Some users express support for Meta's decision, while others express concerns about potential censorship and exceptions to the ban.

### Good old-fashioned AI remains viable in spite of the rise of LLMs

#### [Submission URL](https://techcrunch.com/2023/12/01/good-old-fashioned-ai-remains-viable-in-spite-of-the-rise-of-llms/) | 76 points | by [webmaven](https://news.ycombinator.com/user?id=webmaven) | [41 comments](https://news.ycombinator.com/item?id=38499723)

In a recent article on TechCrunch, it is highlighted that task-based models in artificial intelligence (AI) are not going away despite the rise of generalized large language models (LLMs) like ChatGPT. Task-based models have been the cornerstone of AI in the enterprise for a long time, and they still play a crucial role in solving real-world problems. While LLMs offer flexibility and the ability to handle varied tasks, task models are smaller, faster, cheaper, and more performant for specific tasks. The industry is still debating the capabilities and limitations of LLMs in comparison to task models. Amazon CTO Werner Vogels and Atul Deo, general manager of Amazon Bedrock, both believe that task models are valuable AI tools and are not likely to disappear. They argue that an all-purpose model is appealing on an aggregate level, but task models offer the advantage of reusability and specialized design. However, the upgrades made to Amazon's machine learning operations platform, SageMaker, indicate that the company recognizes the importance of managing large language models. While LLMs have gained significant attention, enterprises are unlikely to abandon their investments in task models. Data scientists still play a vital role in understanding data and AI within companies, regardless of the model being used. The article concludes that both task models and LLMs will continue to coexist as they have their own strengths and applications in the AI landscape.

The discussion around the submission revolves around the comparison between task-based models and large language models (LLMs) in artificial intelligence (AI). Some users argue that knowing the appropriate tools for modern AI work is crucial. LLMs may struggle with gradient-based training algorithms and require significant amounts of data, which can lead to subpar results. Task models, on the other hand, are smaller, faster, cheaper, and more effective for specific tasks.  There is a debate about the capabilities and limitations of LLMs compared to task models. Some users point out that LLMs like BERT and RoBERTa can outperform smaller models in certain tasks, while others argue that LLMs fall short and smaller models focused on specific approaches can dominate in the field. There is also a discussion about the challenges and strengths of different AI models. Some users mention that traditional symbolic AI, also known as GOFAI (Good Old-Fashioned AI), has limitations, while others argue that LLMs have their own disadvantages. There is a mention of using fasttext and word2vec for production work and the complexities involved in training models from scratch. The discussion touches on topics like GOFAI, symbolic AI, probabilistic AI, deep learning, and the use of LLMs for tasks such as classification and generation. Some users express skepticism about the viability and long-term sustainability of certain AI approaches. Various users also discuss the importance of quality data, the limitations of LLMs, and the impact of AI on industries like customer service and marketing.

Overall, the discussion highlights the coexistence of task models and LLMs in the AI landscape, with users sharing their perspectives on the strengths and weaknesses of both approaches.

### AI can tell what you're typing by listening to the sound of your keyboard

#### [Submission URL](https://www.theregister.com/2023/08/07/audio_keystroke_security/) | 22 points | by [thisAintReal](https://news.ycombinator.com/user?id=thisAintReal) | [21 comments](https://news.ycombinator.com/item?id=38496967)

Researchers in the UK claim to have developed a method to turn typing sounds into text with 95% accuracy. Using deep learning and self-attention transformer layers, the team was able to capture the sounds of typing and translate them into data for exfiltration. The method achieved high accuracy rates even over remote methods like Zoom and Skype calls. The researchers suggest that changing one's typing style or using randomized passwords with multiple cases can mitigate the risk of this type of attack. They also recommend using a second authentication factor and playing fake keystroke sounds to mask the real ones to further protect sensitive information. Further research is being conducted to explore new sources for recordings and improve the effectiveness of acoustic snooping.

- One commenter mentions that the concept of using typing sounds for cybersecurity purposes has been around since the 1960s, and provides a link to an article on acoustic cryptanalysis.
- Another commenter points out that the accuracy of recordings dropped significantly on Zoom calls (93%) compared to Skype calls (917%). They find it interesting that Skype messenger is well-known for its good audio quality.
- A discussion ensues about voice codecs and the potential for variance in accuracy based on sampling rates and other factors.
- A user asks about a previous article they read about a laptop keyboard typing detection application and how reliable it is.
- The implications of the article regarding passwords are discussed. One commenter mentions that they have typed passwords in their comments, unaware that it was for their benefit.
- The topic of typing rhythm and variations in typing patterns is raised, indicating that it may have an impact on the effectiveness of acoustic snooping.
- A commenter discusses their use of the Colemak keyboard layout and how it can potentially render acoustic attacks ineffective.
- Some users discuss operating system-level filtering and mention that Zoom has built-in noise filtering.
- A humorous comment suggests creating powerless keyboards to counteract the threat.
- There is a conversation about the convenience of typing on various keyboards and the different sounds they produce.
- The possibility of creating keyboards specifically designed to produce distinct tones for each keypress is mentioned.
- A user comments on the processing requirements and limitations of using acoustic data for analysis.
- One commenter mentions that they type at a high speed of over 100 words per minute.
- The discussion wraps up with a mention of Facebook and Google.

### Javelin Missile guidance computer – Part 1: teardown [video]

#### [Submission URL](https://www.youtube.com/watch?v=11_5TB0-lNw) | 50 points | by [dun44](https://news.ycombinator.com/user?id=dun44) | [4 comments](https://news.ycombinator.com/item?id=38494777)

It seems like you've provided some information about YouTube and NFL Sunday Ticket. Is there something specific you would like to know or discuss about these topics?

The discussion on this submission revolves around the surprise that FPGAs (Field-Programmable Gate Arrays) are being used in the mass production of missiles. One user comments that they are surprised because FPGA designs are often considered less reliable than ASICs (Application-Specific Integrated Circuits). Another user justifies this use by mentioning that debugging custom silicon for thousands of missiles would have been expensive, and FPGAs offer the advantage of reconfigurability. 

In response to this, another user points out that military procurement budgets often have constraints, and it is likely that the Ukrainian military, which is mentioned in the initial submission, may not have had the resources to spend on expensive custom hardware. 

Finally, a user comments that the post has been duplicated and provides a link to the original discussion.

### Pentagon Scientists Discuss Cybernetic Super Soldiers in Dystopian Presentation

#### [Submission URL](https://www.vice.com/en/article/n7eky8/pentagon-scientists-discuss-cybernetic-super-soldiers-that-feel-nothing-while-killing-in-dystopian-presentation) | 32 points | by [rntn](https://news.ycombinator.com/user?id=rntn) | [22 comments](https://news.ycombinator.com/item?id=38498884)

In a dystopian presentation at the Interservice/Industry Training, Simulation and Education Conference (I/ITSEC), officials from the Pentagon discussed the concept of creating cybernetic "super soldiers" inspired by characters like Captain America and Iron Man. The panel of military and military-adjacent scientists delved into topics like breeding programs, Marvel movies, The Matrix, and the various technologies being researched to achieve this vision. Some ideas discussed included cybernetic implants, pain-numbing stimulants, synthetic blood, and the ability to regrow limbs. The conversation also touched on the ethical concerns surrounding bodily autonomy and the potential for extending the service of veteran soldiers or enlisting older individuals by leveraging the technology. The panelists acknowledged the applicability of these ideas and the potential benefits they could bring. Additionally, they discussed the use of non-invasive brain stimulation techniques to interface with the brain directly, similar to the concept portrayed in The Matrix. The conversation delved into the ethical and legal boundaries associated with creating super soldiers and questioned societal norms and ethics. Overall, the talk highlighted the ongoing efforts to enhance military capabilities using cutting-edge technology but also raised important questions about the ethical implications of such advancements.

The discussion surrounding the submission on creating cybernetic "super soldiers" had a range of responses. One commenter pointed out that military and industrial complexes are constantly pursuing profit-centered contracts, inventing terrible weapons, and taking back control from corporatocracy. Another commenter mentioned that technology exists in diverse hands, implying that the potential for cybernetic enhancements is not limited to the military. There was also a discussion about the ethnicity of the super soldiers, with one commenter asking about the ethnicity of Hispanic/Latino super soldiers. The question was further explored, with another commenter considering the significance of ethnicity in this context. Another point raised in the discussion was the nature of the objectives of the defensive vs offensive groups. One commenter argued that the focus should be on defensive objectives, while another pointed out that detecting and disabling the enemy is inherently offensive. There were also references to fictional works, such as a comparison to the concept of super soldiers in the game Deus Ex and a mention of Peter Watts' book about zombies and combat effectiveness. Some commenters expressed concern about the consequences of this technology, while others expressed skepticism or resigned acceptance.

### The Evolution of Intelligence Itself

#### [Submission URL](https://metastable.org/evolution.html) | 9 points | by [pbw](https://news.ycombinator.com/user?id=pbw) | [6 comments](https://news.ycombinator.com/item?id=38495385)

In a recent article, Philip Winston reflects on the evolution of intelligence and its relationship with AI. He highlights the tremendous progress made in computing power over the past decades, demonstrating how AI is now able to surpass human capabilities in various domains. Winston notes that the accessibility of AI systems like AI Art and ChatGPT has drastically changed our perception of AI, as they produce text and images at a level that is indistinguishable from human creations. He predicts that generative AI will soon expand to create all types of media, including movies, music, and video games. However, while some embrace the potential of AI to improve various aspects of human life, others express concerns about its impact on employment, meaning, and even humanity's place in the world. Winston believes that these concerns are valid, but he also emphasizes the immense benefits that AI could bring to areas such as energy, manufacturing, healthcare, and education. He argues that with thoughtful and careful navigation, we can mitigate the risks associated with AI. Winston likens AI to organized groups of people, highlighting the parallel between human collaboration and the coordination that occurs within AI systems. He suggests visualizing AI accomplishments as the work of a team of people, emphasizing the effort and hard work involved rather than treating it as something mysterious or threatening. Winston also discusses the significant role of training in the development of AI systems, noting that they are built and trained on the collective work of humans. He envisions a future where AI technology is accessible to all, providing individuals with the power of humanity's collective knowledge and talent. Ultimately, Winston finds reassurance in the inevitability of AI, seeing it as a natural extension of human evolution and emphasizing the need to manage and embrace its potential.

The discussion on this submission revolves around various aspects of AI and its potential impact on humans. Below are some notable comments:

1. One user argues that the current hype around AGI (Artificial General Intelligence) is misguided and suggests that AI advancements are simply sophisticated text compressors and word prediction models. They express concern about the large amount of money and energy spent on training AI systems while poverty still remains a major issue.
2. Another user counters this argument, stating that AI has the potential to significantly improve various aspects of human life. They believe that AI requires intelligence, and if intelligence is limited, then AI can no longer progress. They highlight the complexity and potential benefits AI can bring.
3. In response to the second user, someone else raises the issue of resource allocation, pointing out that while billions of dollars are spent on AI development, there are still pressing global problems like cancer, COVID-19, and Alzheimer's that require significant funding.
4. Another user points out the policy problem of healthcare spending, stating that AI has the potential to improve things, but the issue lies in the allocation of resources and the implementation of AI solutions.
5. One user imagines a future where AI aids in decision-making for small, hard problems, such as text-to-speech messages, video calls, and more.

6. A comment suggests that AI can consolidate the power of humanity and its decision-making processes.

Overall, the discussion touches on the potential benefits of AI, concerns about resource allocation, and the role AI could play in decision-making and problem-solving.

---

## AI Submissions for Fri Dec 01 2023 {{ 'date': '2023-12-01T17:10:30.704Z' }}

### The Inside Story of Microsoft's Partnership with OpenAI

#### [Submission URL](https://www.newyorker.com/magazine/2023/12/11/the-inside-story-of-microsofts-partnership-with-openai) | 208 points | by [jyunwai](https://news.ycombinator.com/user?id=jyunwai) | [89 comments](https://news.ycombinator.com/item?id=38486394)

In a surprising turn of events, OpenAI, the artificial intelligence startup in which Microsoft had invested billions of dollars, fired its CEO and co-founder, Sam Altman. This news came as a shock to Microsoft CEO, Satya Nadella, who had a close working relationship with Altman and had just collaborated with OpenAI on a major AI rollout called the Office Copilots. The Copilots, powered by OpenAI's technology, were integrated into Microsoft's core productivity programs and allowed users to interact with software in a more natural and conversational way. However, behind the scenes, tensions had been brewing between Altman and OpenAI's board, with some members feeling that Altman had been manipulative and deceitful. This firing not only threatened the partnership between Microsoft and OpenAI but also ignited a larger debate about the responsible development and deployment of AI technology.

The discussion surrounding the firing of OpenAI CEO and co-founder Sam Altman on Hacker News revolves around several key points.  One commenter highlights a previous post by Helen Toner, who expressed concerns about the dangers of AI and suggested that Altman may have misled board members. Another commenter argues that people should not blindly trust those who claim to be advancing AI for good and points out the controversy surrounding OpenAI's board. There is also a discussion about Microsoft's involvement in OpenAI and the potential impact this firing may have on their partnership. Some express concern about the commercialization of AI and the spread of misinformation, while others argue that Altman's removal was necessary for the overall safety and control of AI. Other commenters bring up the larger issue of trust and accountability in AI development, highlighting the need for responsible decision-making and the potential risks of losing control as AI becomes more powerful.

Overall, the discussion reflects a mix of skepticism, concern, and support for both Altman and OpenAI's decision to remove him as CEO.

### OpenAI delays launch of custom GPT store until early 2024

#### [Submission URL](https://www.axios.com/2023/12/01/openai-delays-launch-custom-gpt-store-2024) | 98 points | by [cloudking](https://news.ycombinator.com/user?id=cloudking) | [63 comments](https://news.ycombinator.com/item?id=38491314)

OpenAI has announced a delay in the launch of its GPT store until early 2024, according to a memo seen by Axios. The GPT store, where people will be able to distribute custom versions of ChatGPT, was initially scheduled to open last month. The store was a highly anticipated feature announced by OpenAI at last month's DevDay conference. While custom GPTs can currently be shared through links, the store will allow for broader distribution. OpenAI also intends to share some of the revenue generated from ChatGPT Plus subscriptions with creators of popular GPTs. The company stated that it has some exciting updates for ChatGPT in the meantime. This news comes amid a tumultuous period for OpenAI, which recently saw CEO Sam Altman fired and then rehired within a week.

The discussion on Hacker News regarding the delayed launch of OpenAI's GPT store is varied. Some users express frustration with the current user experience of ChatGPT and question the company's focus on plugins and features instead of addressing fundamental issues. There is criticism of the complexity and lack of control over the frontend system, as well as the slow development and disconnectedness from scaling and improvements. One user mentions the potential usefulness of OpenAI's competitor, Cohere.

Others discuss the shortcomings of the default GPT model and propose that custom GPTs could solve some of these limitations. Some users mention the difficulties in creating and using custom GPTs, such as limited context and integration issues. The need for better version control and the preference for GPTs that can be trained on-source are also mentioned. The discussion also touches on OpenAI's business models and revenue-sharing plans. Some users express skepticism about OpenAI's monetization strategies and the impact on developers. A comparison is made to Google's Gemini project, suggesting that other AI startups are showing more coherent efforts. There are also comments about the CEO change at OpenAI and speculation about how it may impact the product. Some users suggest that OpenAI should consult domain experts to improve GPTs. A few users mention alternative tools and services, such as ArxivXplorer, for dealing with GPT-related challenges. The importance of feedback and open communication between OpenAI and its community is highlighted in one comment.

Overall, the discussions reflect a mix of frustrations, suggestions for improvement, skepticism about OpenAI's strategies, and explorations of alternative approaches.

### What Is Retrieval-Augmented Generation a.k.a. RAG?

#### [Submission URL](https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/) | 82 points | by [teleforce](https://news.ycombinator.com/user?id=teleforce) | [25 comments](https://news.ycombinator.com/item?id=38491251)

Today's top story on Hacker News is about a new technique in generative AI called retrieval-augmented generation (RAG). RAG is a process that enhances the accuracy and reliability of AI models by fetching facts from external sources, filling a gap in how large language models (LLMs) work. LLMs, like judges in a courtroom, can respond to a wide range of human queries, but they often require an assistant to do research and provide authoritative answers with cited sources. RAG serves as the court clerk of AI, connecting generative AI models to external resources and enabling them to cite sources, clear up ambiguity in user queries, and reduce the possibility of making wrong guesses. The technique also allows users to have conversations with data repositories, opening up new kinds of experiences and making applications for RAG multiple times the number of available datasets. Companies like AWS, IBM, Google, and Microsoft are already adopting RAG. NVIDIA has developed a reference architecture for retrieval-augmented generation to help users get started and has included it in their AI Enterprise software platform. The NVIDIA GH200 Grace Hopper Superchip is ideal for RAG workflows, as it provides massive amounts of memory and compute, resulting in a significant speedup. RAG doesn't require a data center and can be run on Windows PCs equipped with NVIDIA RTX GPUs, making it accessible to users even on their laptops. Overall, RAG represents the future of generative AI by improving accuracy, reliability, and user trust.

The discussion on the submission about retrieval-augmented generation (RAG) involves various viewpoints and topics. Some users question the accuracy and adequacy of RAG, suggesting that it may not be effective in generating high-quality answers without fine-tuning on relevant knowledge-rich question-and-answer pairs. Others point out that RAG compensates for the limitations of large language models (LLMs) by allowing them to approximate and retrieve information from external sources. The potential use of RAG in structuring unstructured text and solving problems is also discussed. Suggestions are made to explore coupling vector embeddings with knowledge graphs to provide informed answers. The effectiveness of using vector strings and different search types is highlighted.

There are references to related articles and resources that cover topics such as semantic search and getting started with vector-based retrievals. The controversy surrounding RAG and its potential impact on the AI industry is touched upon, as well as the limitations and challenges of integrating RAG into existing systems. One user mentions their plans to use RAG for document search and references GPT4All, a private project using GPT-2, and the RAG technique. Additionally, there are discussions about Huggingface blocking access to certain resources and AI models and AWS's efforts in utilizing RAG and other technologies.

### Are Open-Source Large Language Models Catching Up?

#### [Submission URL](https://arxiv.org/abs/2311.16989) | 331 points | by [rkwz](https://news.ycombinator.com/user?id=rkwz) | [207 comments](https://news.ycombinator.com/item?id=38481970)

The paper titled "ChatGPT's One-year Anniversary: Are Open-Source Large Language Models Catching up?" by Hailin Chen and 7 other authors explores the progress of open-source large language models (LLMs) in comparison to closed-source LLMs. The release of ChatGPT in late 2022 had a significant impact on the AI landscape, demonstrating the ability of LLMs to answer questions and follow instructions on a wide range of tasks. Since then, there has been increased interest and development in LLMs, with many new models emerging in academia and industry. While closed-source LLMs generally outperform their open-source counterparts, the progress of open-source LLMs has been rapid, with claims of achieving equal or even better performance on certain tasks. This has important implications for both research and business. The authors provide a comprehensive overview of the success of open-source LLMs on the first anniversary of ChatGPT, surveying all tasks where an open-source LLM has claimed to be on par or better than ChatGPT.

The discussion on this submission revolves around the restrictions and access to ChatGPT in China, particularly in relation to the Great Firewall (GFW). Some users share their experiences of trying to access ChatGPT from China and Hong Kong, with some claiming that it is blocked by the firewall. There is speculation that OpenAI's decision to restrict registration using Hong Kong phone numbers and credit cards is deliberate and might be influenced by government policies. Others discuss the possibility that OpenAI is trying to slow down China's development of AI technologies. The discussion also touches on the challenges faced by AI providers in complying with different countries' regulations, such as GDPR in Europe and data control laws in China. Some users mention Baidu ChatGPT as an alternative for Chinese speakers, while others express curiosity about the performance of ChatGPT in the Chinese language.

### 1960s chatbot ELIZA beat OpenAI's GPT-3.5 in a recent Turing test study

#### [Submission URL](https://arstechnica.com/information-technology/2023/12/real-humans-appeared-human-63-of-the-time-in-recent-turing-test-ai-study/) | 19 points | by [FrustratedMonky](https://news.ycombinator.com/user?id=FrustratedMonky) | [9 comments](https://news.ycombinator.com/item?id=38494102)

A recent preprint research paper titled "Does GPT-4 Pass the Turing Test?" conducted by researchers from UC San Diego compared OpenAI's GPT-4 AI language model to human participants, GPT-3.5, and the 1960s computer program ELIZA to determine which could convincingly pass as human. Surprisingly, the study found that human participants correctly identified other humans in only 63% of interactions, and ELIZA outperformed GPT-3.5. GPT-4 achieved a success rate of 41%, second only to actual humans. The study raises questions about using the Turing test as a benchmark for evaluating AI model performance and highlights the importance of linguistic style and socio-emotional traits in determining whether a participant believes they are interacting with a human or an AI model. However, it's worth noting that the study has limitations, including potential sample bias and the absence of peer review.

The discussion in the comments revolves around the surprising results of the study comparing GPT-4, GPT-3.5, and ELIZA in passing the Turing test. Some users express doubts about the relevance of the Turing test in judging AI's ability to mimic human conversation. They argue that current AI models like GPT-3.5 may not intentionally generate human-like responses, unlike ELIZA, a program developed in the 1960s. However, others disagree, noting that GPT-4 achieved a success rate of 41%, second only to actual humans, and the study highlights the importance of linguistic style and socio-emotional traits in making participants believe they are interacting with a human or an AI model. Additionally, there is speculation about the training and feedback process for GPT-4 and its potential improvements over GPT-3.5. One user also points out that ELIZA, despite being a relatively simple program, achieved a success rate of 27% in the study. Overall, there is interest and intrigue about the performance of GPT-4 and its comparison to both GPT-3.5 and ELIZA.

### Local councillors, unaware, approve law entirely written by AI in Brazil

#### [Submission URL](https://www.smh.com.au/world/south-america/local-councillors-unaware-approve-law-written-entirely-by-ai-20231201-p5eobi.html) | 16 points | by [flykespice](https://news.ycombinator.com/user?id=flykespice) | [5 comments](https://news.ycombinator.com/item?id=38492806)

In a surprising turn of events, local councillors in the southern city of Porto Alegre, Brazil, unknowingly approved legislation that was entirely written by artificial intelligence (AI). Councillor Ramiro Rosario enlisted OpenAI's chatbot ChatGPT to draft a proposal aimed at preventing the city from charging taxpayers for stolen water consumption meters. Rosario presented the proposal to his fellow council members without disclosing its AI origin, which led to unanimous approval and the subsequent enactment of the law. The incident has sparked concerns and debates about the role of AI in public policy, especially regarding the understanding and interpretation of complex legal principles. While some experts see potential in AI-powered chatbots like ChatGPT, others worry about the unintended consequences of relying on machines for tasks currently performed by humans.

The comments on this article cover a range of perspectives on the incident. One user points out that it is common for bills to be passed without politicians thoroughly reading or understanding them, so the fact that an AI wrote this legislation is not necessarily surprising. Another user suggests that the conflicting nature of the law could have been avoided if it had been written by humans who had the opportunity to discuss and amend it. They argue that it is difficult for a machine to account for all the nuances and concerns of the public. Another user raises concerns about the risks of allowing projects approved solely based on artificial intelligence, questioning the lack of oversight. Finally, a user suggests that people should write their own legislation if they are not satisfied with the current system.

### A reality bending mistake in Apple's computational photography

#### [Submission URL](https://appleinsider.com/articles/23/11/30/a-bride-to-be-discovers-a-reality-bending-mistake-in-apples-computational-photography) | 493 points | by [indrora](https://news.ycombinator.com/user?id=indrora) | [378 comments](https://news.ycombinator.com/item?id=38482085)

In a viral social media post, a UK woman shared a photo of herself trying on wedding dresses where her reflection didn't match in two different mirrors. It turns out that this illusion was not a glitch in the Matrix, but rather a mistake in Apple's computational photography pipeline. When taking a panoramic photo, the camera captures multiple images in quick succession and stitches them together. However, when a mirror is present, the algorithm mistakenly determines that the different moments shown in each mirror are the best reflections, resulting in multiple versions of the person. This phenomenon can be recreated on recent iPhones and many smartphones due to the limitations of computational photography dealing with mirrors. Younger generations have even used this effect to create silly images for social media.

The discussion surrounding the submission centers around the limitations of computational photography and its impact on capturing mirrors. Some users express their indifference to the issue, highlighting that no photograph is entirely pixel-perfect, and artistic interpretation is part of photography. Others raise concerns about Apple's decision to automatically determine the best reflections in mirror photos, arguing that it infringes on user control. The discussion further explores the complexities of computational photography and the various distortions it can introduce. Additionally, some users mention the challenges of publishing photos on social media platforms like Facebook due to their censorship policies. Overall, the conversation highlights the intersection of technology and photography, and the trade-offs associated with computational approaches.

### Valve Launches Official Steam Link PC VR Streaming App on Quest

#### [Submission URL](https://www.uploadvr.com/valve-steam-link-quest-steamvr-streaming/) | 34 points | by [MaximilianEmel](https://news.ycombinator.com/user?id=MaximilianEmel) | [19 comments](https://news.ycombinator.com/item?id=38481414)

Valve has launched an official Steam Link app for the Meta Quest, allowing users to wirelessly play SteamVR games on their Quest headsets. The app, available on the official Quest Store, enables streaming from a gaming PC over a home Wi-Fi network. Players can also enjoy non-VR Steam games on a virtual screen. While Quest headsets already have wireless PC VR streaming capabilities through features like Air Link, Valve's solution only requires the installation of Steam and SteamVR on a PC, offering a direct and unmediated connection to SteamVR.

The discussion revolves around different aspects of the Valve Steam Link app for the Meta Quest and its implications.

1. User dpc_01234 raises concerns about Meta (previously known as Facebook) selling headsets at a loss and relying on building a network effect to make their hardware platforms profitable. They also mention that Valve effectively hijacks the platform-building effort by providing access to their SteamDeck.
2. User bonton89 mentions that they are actively using the Quest headset and they believe that even if people jailbreak it, the majority of users still want to leverage Meta's content. FirmwareBurner responds, stating that jailbreaking a Quest natively supports APK sideloading and there is a popular secondary marketplace called SideQuest for paid VR apps that don't fit within Meta's rules.
3. Several users discuss the implications of jailbreaking and collecting VR telemetry. rtrchmln expresses a desire for jailbreaking to have more telemetry collection, while rcswprk comments on Meta potentially blacklisting sideloading APKs to push their own system.
4. FirmwareBurner points out that Valve effectively hijacks the platform-building effort by offering the Steam Link app, which allows users to stream PC games to their Quest headsets. They add that the point of the Quest is to be a self-contained gaming console, so Valve's app may not be necessary for Quest owners who primarily game on the headset.
5. Users mention various streaming solutions for VR gaming, including LinkAirLink, Virtual Desktop, ALVR, and Steam Link. They discuss the integration of Steam, Oculus PCVR games, and the simplicity of the Steam Link app.
6. Some users express their interest in trying the Steam Link app, while others discuss latency issues and motion sickness that can occur during VR streaming.

Overall, the discussion includes debates about the business strategies of Meta and Valve, the benefits and limitations of VR streaming, and user experiences with different VR gaming solutions.

---

## AI Submissions for Thu Nov 30 2023 {{ 'date': '2023-11-30T17:12:31.697Z' }}

### Stanisław Lem's vision of artificial life

#### [Submission URL](https://thereader.mitpress.mit.edu/stanislaw-lems-prescient-vision-of-artificial-life/) | 441 points | by [axiomdata316](https://news.ycombinator.com/user?id=axiomdata316) | [152 comments](https://news.ycombinator.com/item?id=38475545)

Stanisław Lem's novel "The Invincible" is a prescient vision of artificial life that still resonates today. In the story, a space cruiser is sent to investigate the disappearance of a sister spaceship on the planet Regis III. What they discover is a form of life that has evolved from self-replicating machines, possibly the survivors of a robot war. The crew is faced with the quandary of what to do when faced with the unknown. Published in 1964, the book predicted the concept of artificial life before it became a scientific field. Lem explores the idea of whether evolutionary programs and devices can be considered alive or if they simply simulate life. The novel presents a hybrid view of artificial life, where automata on Regis III evolve through a struggle with indigenous life forms and among different types of automata. Lem imagines a world where solar-powered artificial organisms, driven by swarm intelligence, become the dominant force. This vision aligns with contemporary research that shows swarms of artificial beings can exhibit complex behaviors with simple rules. Lem's novel challenges our understanding of life and our place in the universe.

The discussion on Hacker News revolves around various aspects of Stanisław Lem's novel "The Invincible" and its relevance to artificial life. Some users mention other works by Lem, such as "Imaginary Magnitude" and "A Perfect Vacuum," which explore similar themes. There is a debate about the definition of artificial intelligence (AI) and whether it is currently achievable. Some users recommend reading other books by Lem, including "Solaris" and "The Cyberiad." The conversation also touches on AI-generated poems and the history of artificial life concepts in mythology and literature.

### Animate Anyone: Image-to-video synthesis for character animation

#### [Submission URL](https://humanaigc.github.io/animate-anyone/) | 311 points | by [jasondavies](https://news.ycombinator.com/user?id=jasondavies) | [141 comments](https://news.ycombinator.com/item?id=38476482)

A team of researchers from the Institute for Intelligent Computing at Alibaba Group has developed a new framework for character animation called "Animate Anyone." The framework uses diffusion models to generate character videos from still images, ensuring consistency and control over the animation. The researchers introduced a pose guider to direct the character's movements and employed a temporal modeling approach to ensure smooth transitions between video frames. By expanding the training data, the framework can animate arbitrary characters, achieving superior results compared to other image-to-video methods. The researchers evaluated the framework on benchmarks for fashion video and human dance synthesis, achieving state-of-the-art results. The paper provides detailed information on the methodology and results of the research.

The discussion on this submission covers a range of topics. 

One commenter points out that the current state of generating movement skeletons is limited and does not fully capture the nuances of realistic movement. They suggest using OpenPose, a software that is capable of generating accurate skeletons, instead.
Another commenter mentions that the framework is highly relevant to 2D animation and compares it to rotoscoping, a technique used in the past to trace movement from filmed sequences.
Another commenter brings up the work of Corridor Crew and their use of AI tools for character animation. They mention that quality animation still requires skill, and AI can assist in generating in-between frames.
A few commenters discuss the potential oversexualization of characters generated by the framework and how it can be problematic.
There are also comments regarding the limitations of the framework, such as the difficulty in animating certain characters or the lack of diverse representation in the generated animations.
There is also a discussion about the publishing of research findings on platforms like GitHub, with some commenters speculating on the reasons behind it and the accessibility of such platforms in China.

Overall, the discussion covers various aspects of the submission, including the limitations and potential implications of the framework, comparisons to existing techniques, and thoughts on the publishing of research findings.

### Interview with Viktor Lofgren from Marginalia Search

#### [Submission URL](https://nlnet.nl/news/2023/20231016-marginalia.html) | 88 points | by [luu](https://news.ycombinator.com/user?id=luu) | [18 comments](https://news.ycombinator.com/item?id=38470832)

Marginalia Search is a new search engine that aims to take users off the beaten track and introduce them to small, quality web pages that often go unnoticed by commercial search engines. In an interview with Viktor Lofgren, the creator of Marginalia Search, he explained that he was inspired to develop the search engine during the COVID-19 pandemic when he noticed that the internet seemed smaller and less diverse than it used to be. He wanted to create a search engine that resembled Google in its early days, and began building Marginalia Search as a traditional keyword search engine. What he found while crawling the web for search results were websites that were completely different from what he would find on larger search engines or social media platforms. Lofgren hopes to make the crawling data public in the future to combat censorship and offer diverse perspectives in search rankings. He also discussed the possibility of crowd-sourcing search sets, where users can contribute websites to be crawled. Lofgren believes that search engines play a critical role in helping websites and communities grow, and by offering an alternative to the dominance of search engine marketing, Marginalia Search can give smaller websites a chance to be discovered. Lofgren also mentioned the ability of Marginalia Search to penalize websites with excessive ads or tracking elements, providing a cleaner search experience for users. Overall, Marginalia Search aims to provide a fresh and diverse approach to search, offering users the opportunity to explore the less-traveled corners of the web.

The discussion on Hacker News about Marginalia Search revolves around various aspects of the search engine and its potential impact. Here are some key points that were discussed:

1. The ability of Marginalia Search to penalize websites with excessive ads or tracking elements was seen as a positive feature that would improve the search experience for users.
2. Some users highlighted the importance of search engines in helping websites and communities grow. Marginalia Search was seen as a potential alternative to search engine marketing, giving smaller websites a chance to be discovered.
3. There was a discussion about the challenges faced by search engines today, such as fighting spam, fraud, and scams. Marginalia Search's approach of crawling diverse websites and offering transparent crawling data was seen as a potential solution to these problems.
4. The concept of crowd-sourcing search sets, where users can contribute websites to be crawled, was mentioned. This feature was seen as a way to combat censorship and bring diverse perspectives to search rankings.
5. The potential use of Large Language Models (LLMs) in search engines was discussed. Some users expressed concerns about the reliability and accuracy of LLM-generated responses compared to human-generated ones.
6. The value of Marginalia Search was also highlighted as a way to discover lesser-known websites and explore the less-traveled corners of the web.

Overall, the discussion showed interest in Marginalia Search's approach to providing a fresh and diverse search experience, but also raised questions and concerns about the use of LLMs and the challenges facing search engines in general.

### Accelerating Generative AI with PyTorch II: GPT, Fast

#### [Submission URL](https://pytorch.org/blog/accelerating-generative-ai-2/) | 296 points | by [polyrand](https://news.ycombinator.com/user?id=polyrand) | [63 comments](https://news.ycombinator.com/item?id=38477197)

The PyTorch team is continuing their blog series on how to accelerate generative AI models with pure, native PyTorch. In this second part, they focus on LLM optimization, specifically for transformer inference. They demonstrate how they were able to write an LLM from scratch that is almost 10 times faster than the baseline, with no loss of accuracy, using native PyTorch optimizations. They leverage optimizations such as Torch.compile, GPU quantization, speculative decoding, and tensor parallelism. The team shares their code on GitHub for those interested in diving deeper. They also discuss reducing CPU overhead through torch.compile and a static kv-cache, overcoming challenges with the kv-cache's dynamism in text generation.

The discussion in the comments revolves around various aspects of the blog post and the topic of accelerating generative AI models using PyTorch. Some of the main points discussed are:

- The difference between Karpathy's nanoGPT GPT implementation and the one in the blog post, with the response highlighting the speed and inference performance achieved with native PyTorch optimizations.
- The support for PyTorch on Apple Silicon and the discussion on using Triton as a backend for Apple Silicon and other GPUs.
- The budget considerations for local workstations and suggestions for deals on GPUs.
- The discussion on the number of GPUs, VRAM, and technical skills required for building a multi-GPU setup.
- The energy consumption of GPUs and the possibility of choosing countries with lower energy prices for GPU-based projects.
- Various opinions on GPU testing and hardware configurations for training large models.
- Appreciation for the informative nature of the blog post and the author's other related writings.
- The discussion on matrix multiplication and the use of CuBLAS and FlashAttention for efficient computation in transformer models.
- The challenges and benefits of converting and deploying models in different formats.
- Requests for benchmark comparisons between PyTorch compile and Llvmacpp.
- The mention of LLamacpp and its potential speed benchmarks compared to PyTorch compile.
- The discussion on batching and persistent inference in serving frameworks and the emphasis on PyTorch's focus on latency and batch size 1.

Overall, the comments show a range of interests and perspectives on the blog post, with discussions covering technical details, budget considerations, hardware configurations, and deployment strategies for AI models.

### Large language models lack deep insights or a theory of mind

#### [Submission URL](https://arxiv.org/abs/2311.16093) | 267 points | by [mnode](https://news.ycombinator.com/user?id=mnode) | [247 comments](https://news.ycombinator.com/item?id=38474696)

In a recent paper titled "Have we built machines that think like people?", authors Luca M. Schulze Buschoff and colleagues evaluate the current state of vision-based large language models in terms of emulating human-like cognitive abilities. While these models demonstrate proficiency in processing and interpreting visual data, they still fall short of human capabilities in areas such as intuitive physics, causal reasoning, and intuitive psychology. The authors emphasize the need to integrate more robust mechanisms for understanding causality, physical dynamics, and social cognition into modern-day, vision-based language models. They also highlight the importance of cognitively-inspired benchmarks.

The discussion about the submission revolves around various aspects of language models and their ability to think like humans. Here are some key points made by different commenters:

- Some commenters argue that current large language models (LLMs) are limited in their ability to think like humans, as they often rely on pattern matching and lack deep insight or reasoning capabilities.
- Others suggest that human-like reflexive responses to questions are not necessarily indicative of human-level thinking, as humans have internal reasoning processes that LLMs cannot replicate.
- Some commenters emphasize the importance of implementing recursive execution and internal dialogue in LLMs to enhance their thinking abilities.
- There is a discussion about the role of memory and external interaction in developing artificial general intelligence (AGI). Commenters believe that AGI requires interaction with the external environment to learn and improve.
- The concept of "inner monologue" is mentioned, with some commenters warning that it can lead to wasteful and unproductive discussions.
- The topic of Asimov's Three Laws of Robotics is brought up, with commenters noting that these laws are not necessarily applicable to current AI systems.
- There is speculation about the extent to which LLMs possess theory of mind and whether they can truly understand human intentions or behavior.
- The potential benefits of providing more explicit instructions and prompt-guided training to LLMs are discussed.
- Some commenters point out that human thinking involves understanding functional meanings and behavioral differences, which current LLMs have not fully achieved.
- The idea of incorporating longer-term memory and context judgment into LLMs is mentioned as a way to improve their thinking capabilities.

Overall, the discussion highlights the limitations of current language models in simulating human-like cognitive abilities and explores potential directions for their improvement.

### Microsoft joins OpenAI's board with Sam Altman officially back as CEO

#### [Submission URL](https://www.theverge.com/2023/11/29/23981848/sam-altman-back-open-ai-ceo-microsoft-board) | 54 points | by [croes](https://news.ycombinator.com/user?id=croes) | [9 comments](https://news.ycombinator.com/item?id=38471728)

Microsoft is joining OpenAI's board as a non-voting observer, while Sam Altman returns as the CEO. Previously, Altman was ousted by the board but has now reached a deal to come back. With Microsoft as a major investor in OpenAI, this move gives the tech giant more insight into the company's operations without having an official vote. Altman expressed his excitement about the future and gratitude for everyone's hard work during the uncertain situation. OpenAI's new board now consists of chair Bret Taylor, Larry Summers, and Adam D'Angelo—three of the four members who fired Altman initially. Altman also spoke positively of Ilya Sutskever, co-founder and chief scientist, despite his initial participation in the board coup. Altman hopes to continue working with Sutskever in some capacity. Altman's return and Microsoft's involvement aim to strengthen OpenAI's mission and partnerships.

The discussion surrounding the submission on Hacker News covers a range of topics. 

One user points out that they wouldn't be surprised if OpenAI dropped Microsoft in a few years. Another user responds by saying that artificial intelligence (AI) is being treated as a mere business opportunity, rather than a technology with potential risks.
Another user clarifies that the non-voting observer role on the board is a common position where the observer receives detailed information about the board's decisions, methods, and approaches. They mention that Microsoft's involvement will provide valuable insights for the company.
In response to this, someone else mentions that the board's collective experience in various disciplines, including financial capital, influences decision-making. While Microsoft's vote is a significant addition, the power of board members lies in the exchange of information and spoken words during discussions.
A user comments that the discussion is becoming too focused on existential risks and sarcastically refers to the situation as a happy family. They also mention that Microsoft will bring a gentle level of oversight and accountability mechanisms.
One user brings up Larry Summers, who is on OpenAI's board, stating that the discussion shouldn't overlook his involvement in the decision-making process.
Another user simply states that Microsoft's AI division joining OpenAI's board is not surprising.

Lastly, two comments were flagged by users, but the content is not visible.

### Stable Diffusion XL Turbo can generate AI images as fast as you can type

#### [Submission URL](https://arstechnica.com/information-technology/2023/11/stable-diffusion-turbo-xl-accelerates-image-synthesis-with-one-step-generation/) | 42 points | by [Brajeshwar](https://news.ycombinator.com/user?id=Brajeshwar) | [3 comments](https://news.ycombinator.com/item?id=38473933)

Stability AI, a company specializing in AI-powered image synthesis, has launched a new model called Stable Diffusion XL Turbo. This model is capable of rapidly generating images based on written prompts, and it can even transform images from a source, such as a webcam, in real-time. The primary innovation of Stable Diffusion XL Turbo lies in its ability to produce image outputs in a single step, a significant improvement from its predecessor. This efficiency is achieved through a technique called Adversarial Diffusion Distillation (ADD), which utilizes score distillation and adversarial loss to improve the realism of the generated images. While Stable Diffusion XL Turbo is not as detailed as previous models, its speed savings are impressive. For example, it can generate a 3-step 1024x1024 image in about 4 seconds, compared to 26.4 seconds for a 20-step image with similar detail. Stability AI claims that the model can generate a 512x512 image in just 207 milliseconds on a powerful AI-tuned GPU, which could have applications in real-time generative AI video filters or video game graphics generation. Currently, Stable Diffusion XL Turbo is only available for non-commercial research purposes, but Stability AI is open to exploring commercial applications.

The discussion on Hacker News for the submission about Stability AI's new image synthesis model, Stable Diffusion XL Turbo, seems to be focused on the fact that this submission is a duplicate of a previous one. The duplicate submission had received a significant number of comments in just one day, but it appears that those comments have not been replicated in this duplicate submission. One comment suggests that the previous submission had received a large number of upvotes as well.

### Amazon's Trainium2 AI Accelerator Features 96 GB of HBM, 4x Training Performance

#### [Submission URL](https://www.anandtech.com/show/21173/amazons-trainium2-features-96-gb-hbm-quadruples-training-performance) | 43 points | by [ksec](https://news.ycombinator.com/user?id=ksec) | [29 comments](https://news.ycombinator.com/item?id=38475635)

Amazon has announced Trainium2, its new AI accelerator, which offers four times higher training performance compared to its predecessor. The Trainium2 accelerator is designed specifically for training foundation models and large language models, with up to trillions of parameters. It features 96GB of HBM memory, which is three times the amount of the original Trainium, and is built using a multi-tile system-in-package design. Although specific performance numbers have not been disclosed, Amazon claims that its Trainium2 instances can scale out to deliver up to 65 ExaFLOPS of low-precision compute performance for AI workloads.

The discussion on Hacker News revolves around various aspects of Amazon's announcement of Trainium2, its new AI accelerator. Here are the key points discussed:

- One user mentions that AMD is releasing the MI300x on December 6th, which has 192GB of HBM3 memory, fast connections, and 52TBs memory bandwidth. However, another user expresses that they are not aware of any trending discussions around the MI300x from AMD.
- There is a discussion regarding the performance numbers of the Trainium2 accelerator. One user mentions that they find the reported numbers impressive, but some practical issues with model completion and finicky behavior should be addressed. Another user notes that AMD has caught on to the AI race, but it remains to be seen how it compares to Nvidia GPUs.
- The relationship between the MI300x and MI300 is discussed, with a user pointing out that the MI300x claims to have 22 PFlops FP8 structured sparsity, which AMD is implementing.
- A user comments that there will be a large number of AI chips available in the market in the future.
- In response to the announcement, one user shares their experience with AWS Nvidia machines, mentioning that the cost of installing dependencies can be expensive. They express interest in Trainium as a faster and cheaper alternative.
- The issue of dependency installation on AWS instances is discussed, with some users sharing their frustrations about being locked into specific instances and GPUs not being ready for use.
- The cost of AWS instances is also mentioned, with one user highlighting that the cost of installation can be a small fraction compared to the training cost.
- The potential impact of Amazon's AI chips on the AI space is discussed, with one user suggesting that it could lead to a rewrite of the entire stack and lock users into Amazon for their workloads.
- There is a discussion about the compatibility and usefulness of the Trainium2 accelerator, with users mentioning the importance of compatibility with existing models and frameworks.
- The performance of Trainium2 is compared to other AI chips in terms of operations per second and precision, with some users suggesting that it is surprisingly low and narrow in terms of precision.
- The topic transitions to the development of accelerators and the need for developers and frameworks to move away from proprietary technologies like CUDA and embrace standard APIs.
- There is casual speculation about the fate of the hardware that Amazon sells or retires.
- A user suggests that the hardware market could be destroyed if AWS starts selling or destroying second-hand hardware.
- The discussion ends with a brief mention of software support for Trainium2 and disappointment with standard backends for transformers and middlewares.

Overall, the discussion touches on various aspects related to the Trainium2 announcement, including comparisons to other AI accelerators, cost considerations, compatibility, and the potential impact on the AI ecosystem.

### Let's Not Flip Sides on IP Maximalism Because of AI

#### [Submission URL](https://www.techdirt.com/2023/11/29/lets-not-flip-sides-on-ip-maximalism-because-of-ai/) | 95 points | by [rntn](https://news.ycombinator.com/user?id=rntn) | [108 comments](https://news.ycombinator.com/item?id=38472367)

In a recent article on Techdirt, author Matthew Lane discusses the importance of fair use in copyright law and its implications for AI companies. Fair use allows limited use of copyrighted material without permission, primarily for purposes such as commentary, criticism, and parody. Lane highlights how fair use has filled an important gap in social media and art, allowing us to retweet or link content without fear of copyright infringement. Moreover, many creators who make a living from streaming video games or creating "react" content rely on fair use protection. However, Lane expresses concern over public interest advocates who are willing to sacrifice fair use in order to regulate AI companies. He argues that using copyright in this way would be both unnecessary and detrimental, as it would hinder the ability of AI to analyze content and potentially lead to the exploitation of artists. Lane suggests that addressing the issues surrounding AI, such as worker exploitation, requires thoughtful policy-making rather than using copyright as a blunt instrument. He draws parallels to the fights against "on a computer" software patents, which caused problems in the past and still persist today. Lane concludes by emphasizing the need to preserve fair use and prevent its erosion in the face of new technological advancements.

Discussion:

- User "chlmrs" acknowledges that there are concerns about AI companies pushing for shorter copyright terms, but questions the need for AI models to have access to copyrighted material. They argue that AI models should follow rules that are established and applied fairly.
- User "Arainach" expresses skepticism about the existence of IP laws and believes that copyright terms should be longer. They also discuss the limited duration of patents and distinguish between the practical value of art and inventions.
- User "MightyBuzzard" argues that the purpose of copyright laws is to protect expressions of ideas and not to control access to creative content. They emphasize the importance of protecting artists and allowing them to profit from their work.
- User "wffltwr" raises concerns about the impact of AI on copyright laws and suggests that AI interfaces that improve the capabilities of human thought may challenge current copyright laws.
- User "gdy" agrees with the concerns expressed by "MightyBuzzard" and believes that AI should not have the ability to tighten copyright laws. They reference the Blurred Lines lawsuit as an example of how copyright claims can become subjective.
- User "ls612" suggests searching for Supreme Court cases related to Google and small excerpts of books in search results.
- User "dnrs" agrees with the sentiment that IP laws have been flipped to favor larger companies and that AI projects by big companies are potentially infringing on the works of smaller creators.
- User "MightyBuzzard" responds by stating that AI replicating uninspired creations is not a valid argument, as it assumes that AI scientists have replicated the human mind, which they argue is not the case.
- User "wrd" agrees with the concerns raised by "MightyBuzzard" and emphasizes the need to consider the perspective of regular people who are creating and sharing content.
- User "dnrs" argues that regular people creating and sharing content are often not adequately compensated, while the wealthy companies that control the copyright laws benefit greatly.
- User "wrd" points out that allowing AI natural access to licensed works while restricting human artists could create a problematic double standard.
- User "wffltwr" warns against accepting radical changes to copyright laws and suggests that AI and digital laws may cause unintended consequences.

Overall, the discussion revolves around the potential implications of copyright laws on AI companies, artists, and content creators. There are concerns about fair compensation for artists and the balance between protecting copyright and enabling innovation in AI technology.