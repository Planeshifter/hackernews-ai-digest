import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Tue Sep 19 2023 {{ 'date': '2023-09-19T17:10:35.468Z' }}

### Graph Neural Networks use graphs when they shouldn't

#### [Submission URL](https://arxiv.org/abs/2309.04332) | 126 points | by [Pseudomanifold](https://news.ycombinator.com/user?id=Pseudomanifold) | [20 comments](https://news.ycombinator.com/item?id=37571535)

Graph Neural Networks (GNNs) have become a popular approach for learning on graph data in various domains. However, a new paper titled "Graph Neural Networks Use Graphs When They Shouldn't" by Maya Bechler-Speicher and her colleagues challenges the assumption that GNNs always make accurate predictions based on graph structure. The researchers show that GNNs tend to overfit the graph structure, even when it is non-informative for the predictive task. They provide a theoretical explanation for this phenomenon and propose a graph-editing method to mitigate the overfitting. The paper concludes with empirical evidence that this method improves the accuracy of GNNs across multiple benchmarks. This research has implications for the use of GNNs in fields such as social networks, molecular biology, and medicine.

The discussion on the submission revolves around various aspects of Graph Neural Networks (GNNs) and their use in learning on graph data. Here are some key points raised by the commenters:

- One commenter mentions that GNNs tend to overfit the graph structure, even when it is non-informative for the predictive task at hand. They provide links to the research paper challenging this assumption.
- Another commenter suggests that attention layers and nested graph convolution layers can help GNNs learn graph structures effectively.
- There is a discussion on the use of graph editing and graph representation in mitigating overfitting in GNNs.
- Some commenters share their experiences with working on GNNs and highlight the importance of studying the behavior and dynamics of GNNs.
- The limitations and challenges of using GNNs in practical applications are also mentioned, such as computational complexity and the need for regularization techniques.
- It is pointed out that GNNs can have a problem of overfitting due to imbalanced class distribution and dependence on specific graph interactions.
- Several papers and research works related to GNNs are shared, covering topics like message passing, algorithmic reasoning, diffusion, sparsity, training tricks, expressive power, and over-squashing.

Overall, the discussion highlights the potential issues and solutions related to the use of GNNs in various domains.

### The physical process that powers a new type of generative AI

#### [Submission URL](https://www.quantamagazine.org/new-physics-inspired-generative-ai-exceeds-expectations-20230919/) | 96 points | by [digital55](https://news.ycombinator.com/user?id=digital55) | [14 comments](https://news.ycombinator.com/item?id=37570743)

Physicists at MIT have introduced a new method of generative AI called the Poisson flow generative model (PFGM). Rather than using black box algorithms like traditional neural networks, PFGM is based on the principles of diffusion and the distribution of charged particles. PFGM represents data with charged particles that create an electric field, and the model learns to estimate that electric field through the training process. This allows PFGM to generate high-quality images, similar to diffusion-based models, but at a much faster speed. The use of physical processes in AI models could open the door to harnessing other physical phenomena to improve neural networks.

The discussion on Hacker News surrounding the submission about the Poisson flow generative model (PFGM) involves various perspectives. 

One commenter points out that the concept of Boltzmann Machines is nothing new, and the use of black box algorithms in neural networks has been replaced by diffusion-based models. Another commenter adds that implementing Poisson flow generative models could be challenging due to memory constraints, but a breakthrough in GPU RAM manufacturing could potentially solve this issue. The discussion then diverges into a debate about the reliance on AI SaaS subscription services and the associated costs.

Another thread of the discussion touches on the idea that physical processes can be effectively modeled in neural networks, opening up possibilities for incorporating other physical phenomena. However, a commenter from the World Economic Forum mentions the challenges in accurately predicting service waiting times.

Some comments express interest in the differences between Poisson flow generative models and traditional diffusion models, while others discuss the potential benefits of utilizing physical processes in network modeling.

One commenter wonders why the article does not provide direct comparisons between Poisson flow generative models and other diffusion models, while another commenter provides a link to relevant research papers.

There is also appreciation for the elegance of incorporating principles from physics into AI models. However, someone points out that counting on quantum computing to solve such problems might be wishful thinking.

Lastly, there is a discussion about how swapping decoding techniques in NLP frequently leads to generating novelty in text generation tasks, but traditional methods still have their merits. The debate focuses on the trade-off between control and novelty in generating text.

### 64-bit bank balances ‘ought to be enough for anybody’?

#### [Submission URL](https://tigerbeetle.com/blog/2023-09-19-64-bit-bank-balances-ought-to-be-enough-for-anybody/) | 237 points | by [todsacerdoti](https://news.ycombinator.com/user?id=todsacerdoti) | [347 comments](https://news.ycombinator.com/item?id=37568856)

TigerBeetle, a systems programming company, has decided to use 128-bit integers to store financial amounts and balances, moving beyond the previous use of 64-bit integers. While some may argue that 64 bits is sufficient, TigerBeetle realized they needed to go beyond this limit to store all kinds of transactions adequately. By using binary encoding, computers can represent numbers, and larger numbers require more bits. Fractions and decimal numbers pose challenges for computers, as they cannot accurately express decimal numbers using binary floating point. TigerBeetle solves this problem by representing money as whole numbers, using a minimal integer factor defined by the user. They also avoid using negative numbers and instead keep separate positive integer amounts for debits and credits. 128-bit integers are necessary to represent values smaller than a cent and meet the precision and scale requirements of various applications. TigerBeetle's database, called TigerBeetle, can count not only money but anything that can be modeled using double-entry accounting, such as inventory items or API calls. The company also considers the future-proof aspect of their system, as long-running systems can accumulate high transaction volumes over time. Unexpected events like hyperinflation can also push a currency towards the upper limits of a 64-bit integer, making 128-bit integers a necessary choice. Overall, TigerBeetle's decision to switch to 128-bit integers ensures more robust and flexible financial storage capabilities.

The discussion on this submission revolves around the use of decimal points and rounding in financial software. Some commenters discuss the problems that arise when handling fractions and decimals in computer systems and emphasize the importance of accurately calculating taxes and sales transactions. Others mention the different regulations and rules in different jurisdictions regarding rounding and decimal precision. Some commenters express surprise at the number of people who overlook decimal precision and make mistakes in billing and financial calculations. There is also discussion about the use of integer arithmetic and the limitations of binary representation in computers. Overall, commenters emphasize the need for precise and accurate financial calculations and highlight the challenges and potential pitfalls in implementing billing and accounting software.

### Google DeepMind's AI successor predicts how 71M mutations cause disease

#### [Submission URL](https://endpts.com/google-deepminds-alphafold-successor-predicts-how-71m-mutations-cause-disease/) | 52 points | by [birriel](https://news.ycombinator.com/user?id=birriel) | [7 comments](https://news.ycombinator.com/item?id=37578616)

Google DeepMind has announced the development of a new AI system called AlphaMissense. This technology is the successor to AlphaFold, which was known for its ability to predict the structures of proteins. AlphaMissense, on the other hand, focuses on predicting the likelihood that genetic mutations, specifically 71 million missense mutations, will cause disease. Each missense mutation refers to a single-letter change in an amino acid that makes up a protein sequence. The announcement of AlphaMissense comes alongside the publication of a paper in the journal Science.

The discussion surrounding the announcement of Google DeepMind's AlphaMissense AI system on Hacker News includes several comments:

1. User "blvl" wrote a quick script that checked 23andme data and found various percentages of mutated genes associated with different conditions.
2. User "kgtsmn" thanked "blvl" for the information and added percentages of mutated genes classified as benign, pathogenic, and likely benign across different classifications.
   a. "kgtsmn" also mentioned the MTHFR C677T mutation and its association with reduced enzyme activity and elevated homocysteine levels in individuals with decreased activity in the AA genotype.
3. "blvl" responded, stating that the rabbit hole goes deep and mentioned the SIRT1 mutation's association with longevity traits.
4. User "pknmd" expressed their understanding of 23andme data and questioned the need for medical professionals due to the self-reported nature of the data. They also found it interesting to compare the percentages of pathogenic and non-pathogenic mutated genes.
5. User "pfd1986" shared a link to the published paper in the journal Science regarding AlphaMissense and an additional link to a non-paywalled article about it.
6. User "PBnFlash" speculated about the potential impact of powerful AI systems like AlphaMissense on healthcare and medical research.
7. User "7e" made a general comment about experts not making decisive guesses.

Overall, the discussion involved users sharing their findings, questioning the need for medical professionals in analyzing genetic data, and discussing the potential implications of AI systems like AlphaMissense in the field of healthcare and genetics.

### The Princeton researchers calling out ‘AI snake oil’

#### [Submission URL](https://www.semafor.com/article/09/15/2023/the-princeton-researchers-calling-out-ai-snake-oil) | 32 points | by [irtefa](https://news.ycombinator.com/user?id=irtefa) | [7 comments](https://news.ycombinator.com/item?id=37576259)

The Princeton researchers, Arvind Narayanan and Sayash Kapoor, behind the popular newsletter and upcoming book "AI Snake Oil" are on a mission to dispel hype and clarify the limits of AI. They focus on distinguishing between predictive AI and generative AI, with most of the snake oil concentrated in predictive AI. They highlight the lack of statistical validity in certain predictive AI applications, such as AI hiring tools. They also express concerns about the potential flood of disinformation from generative AI, but argue that addressing other AI-related harms, like non-consensual deepfakes, should take precedence. The researchers propose that AI companies publish regular transparency reports to shed light on potential harms and usage patterns. They also discuss the need for better controls on the open access archive arXiv.org to prevent misinterpretation of AI research studies.

The discussion on this submission seems to cover a range of topics related to AI and its limitations:

1. Some commenters discuss the distinction between predictive AI and generative AI, with the consensus that most of the "snake oil" is concentrated in predictive AI.
2. A link is shared regarding the challenges of replacing scientific reproducibility with machine learning approaches.
3. The potential risks of AI are debated, with one comment suggesting that the greatest risk comes from humans controlling the technology.
4. There is a discussion about the potential shortcomings of GPT-4 when it comes to professional benchmarks and generating the correct answers to wrong questions.
5. The capabilities of ChatGPT as a "bullshit generator" are mentioned, with some being impressed by its ability to generate seemingly plausible responses.
6. A suggestion is made to focus on addressing non-consensual deepfakes and the spread of misinformation as priorities rather than the harms of generative AI.
7. The idea of companies publishing transparency reports to shed light on potential harms and usage patterns of AI is proposed.
8. Concerns are raised about the need for better controls on the open access archive arXiv.org to prevent misinterpretation of AI research studies.
9. A link to an archive discussing the potential dystopian aspects of AI is shared.
10. A commenter expresses their amusement with the ongoing discussion and suggests not taking it too seriously.

The conversation also includes some meta-discussion, with one commenter requesting others not to engage in shallow dismissals and to provide constructive criticism. Another commenter flags a comment as snarky.

### Tackling the curse of dimensionality with physics-informed neural networks

#### [Submission URL](https://arxiv.org/abs/2307.12306) | 75 points | by [jhoho](https://news.ycombinator.com/user?id=jhoho) | [17 comments](https://news.ycombinator.com/item?id=37565140)

In a recent paper submitted to arXiv, researchers Zheyuan Hu, Khemraj Shukla, George Em Karniadakis, and Kenji Kawaguchi introduce a new method for tackling the curse of dimensionality with Physics-Informed Neural Networks (PINNs). The curse of dimensionality refers to the heavy computational burden that exponentially increases as the dimensionality of a problem increases. The authors propose a method called Stochastic Dimension Gradient Descent (SDGD) that decomposes the gradient of Partial Differential Equations (PDEs) into pieces corresponding to different dimensions and randomly samples a subset of these dimensional pieces in each iteration of training PINNs. The proposed method has been experimentally demonstrated to solve notoriously hard high-dimensional PDEs, such as the Hamilton-Jacobi-Bellman and Schrödinger equations, in thousands of dimensions very quickly on a single GPU. In fact, the researchers were able to solve nontrivial nonlinear PDEs in 100,000 dimensions in just 6 hours on a single GPU using SDGD with PINNs. This new method has the potential to scale up the solving of arbitrary high-dimensional PDEs using PINNs.

The discussion on this submission covers various topics related to the dimensions and complexity of problems, as well as the potential advantages of quantum computers.

One user notes that in machine learning, vectors are typically considered to have numerical properties, while in physics, vectors can represent multiple dimensions. Another comment clarifies that the confusion arises from how different disciplines define and describe dimensions in their specific contexts.

Another user mentions that solving the Schrödinger equation in thousands of dimensions is possible for non-quantum mechanical systems, but it becomes more challenging for quantum-hard problems. A response to this comment suggests trying to solve the quantum harmonic oscillator potential, which is analytically solvable. However, another user points out that the Schrödinger equation is a separable differential equation that implies a specific network structure, which may not apply in general cases.

The discussion then moves to the advantages of quantum computers in solving complex problems. One user mentions that classical computers can calculate mean field energies for thousands of interacting electrons, but quantum computers have an advantage when it comes to calculating exchange correlation energies for interacting electrons. Another user adds that the evaluation of transition probabilities and energy differences can also be advantageous in quantum computers.

Ultimately, the comments touch on various aspects related to the dimensions and complexity of problems, highlighting differences in approaches between machine learning and physics, and discussing the potential advantages of quantum computers in solving complex equations.

---

## AI Submissions for Mon Sep 18 2023 {{ 'date': '2023-09-18T17:10:15.039Z' }}

### Self-supervised learning: The dark matter of intelligence (2021)

#### [Submission URL](https://ai.meta.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/) | 160 points | by [reqo](https://news.ycombinator.com/user?id=reqo) | [18 comments](https://news.ycombinator.com/item?id=37558813)

The AI field has made great strides in developing AI systems that can learn from labeled data, but there is a limit to how far supervised learning can take us. Supervised learning is not sufficient for building more intelligent generalist models that can perform multiple tasks and acquire new skills without massive amounts of labeled data. To address this limitation, researchers believe that self-supervised learning (SSL) may hold the key to unlocking the "dark matter" of intelligence in AI systems.

SSL enables AI systems to learn from vast amounts of unlabeled data, allowing them to recognize and understand more subtle and less common representations of the world. SSL has already shown great success in natural language processing (NLP), with models like BERT and RoBERTa achieving higher performance than those solely trained in a supervised manner. Recent research projects, such as SEER, have demonstrated that SSL can also excel in computer vision tasks.

Self-supervised learning works by obtaining supervisory signals from the data itself, leveraging the underlying structure in the data. For example, in NLP, a system can hide part of a sentence and predict the hidden words from the remaining words. In computer vision, it can predict future frames in a video from the current ones. By using the structure of the data, self-supervised learning can make use of a variety of supervisory signals without relying on labeled data.

Self-supervised learning has had a significant impact on NLP, enabling models to be pretrained on large unlabeled text datasets and then fine-tuned for specific tasks. However, applying SSL to computer vision tasks is a relatively new frontier. Researchers are exploring energy-based models, joint embedding methods, and latent-variable architectures to further advance self-supervised learning and reasoning in AI systems.

By combining supervised learning with SSL, AI systems can develop a deeper, more nuanced understanding of the world. This can bring us closer to achieving human-level intelligence and enable AI systems to learn new skills without requiring massive amounts of labeled data for each task. Self-supervised learning holds great promise in the quest to unlock the dark matter of intelligence in AI.

The discussion on this submission revolves around various aspects of self-supervised learning (SSL) and its potential in advancing artificial intelligence (AI) systems. Some key points from the comments include:

- The success of SSL in natural language processing (NLP) is noted, with models like BERT and RoBERTa achieving high performance by leveraging large unlabeled text datasets.
- There is a mention of different techniques in SSL, such as SimCLR, BYOL, and masking-based models, and their application in NLP and computer vision tasks.
- The use of SSL in computer vision is considered a relatively new area of exploration.
- The importance of SSL in addressing the limitations of supervised learning and achieving a deeper understanding of the world is emphasized.
- LeCun's contrastive learning course materials are recommended as a resource for understanding SSL.
- There is a discussion on the concept of "dark knowledge" and how AI systems can benefit from accessing subtle and implicit information present in unlabeled data.
- The role of humans in solving arbitrary problems and the capabilities of AI systems in comparison are debated.
- A study exploring the philosophical aspects of dark matter intelligence is suggested as reading material.
- One commenter mentions feeling the presence of dark matter intelligence in the industry and its potential in resolving complex issues.

Additionally, one comment redirects readers to a related Twitter thread for more information.

### Data accidentally exposed by Microsoft AI researchers

#### [Submission URL](https://www.wiz.io/blog/38-terabytes-of-private-data-accidentally-exposed-by-microsoft-ai-researchers) | 699 points | by [deepersprout](https://news.ycombinator.com/user?id=deepersprout) | [218 comments](https://news.ycombinator.com/item?id=37556605)

In a recent mishap, Microsoft's AI research team accidentally exposed 38 terabytes of private data on GitHub. The exposed data includes a backup of two employees' workstations, containing secrets, private keys, passwords, and over 30,000 internal Microsoft Teams messages. The researchers shared their files using an Azure feature called SAS tokens, which allows for data sharing from Azure Storage accounts. However, in this case, the access level was not properly limited, resulting in the unintended exposure. This incident highlights the importance of proper management and monitoring of SAS tokens to avoid potential security risks.

The discussion on this submission covers a variety of topics related to the accidental exposure of Microsoft's private data. One user suggests that AI models should be serialized in a secure format to prevent malicious injection, while another user raises concerns about targeted attacks and the potential manipulation of training data. There is also discussion about the risks of dynamically typed languages and the importance of proper security measures. The conversation touches on topics such as log4j vulnerability, password security, encryption, and the use of programming languages. Some users advocate for stricter language typing, while others argue that programming language choice is not the main issue. There is also a brief discussion about non-encrypted PDFs and the comparison between Microsoft Office and LibreOffice. Overall, the discussion highlights the complexities and challenges of securely managing and protecting data in the context of AI research.

### GPT 3.5 vs. Llama 2 fine-tuning: A Comprehensive Comparison

#### [Submission URL](https://ragntune.com/blog/gpt3.5-vs-llama2-finetuning) | 46 points | by [samlhuillier](https://news.ycombinator.com/user?id=samlhuillier) | [12 comments](https://news.ycombinator.com/item?id=37560125)

In a recent post, the author shares their experiments comparing the fine-tuning performance of GPT 3.5 and Llama 2 in an SQL task and a functional representation task. They found that while GPT 3.5 performed slightly better on both datasets, the cost of training and deploying it was significantly higher. The author provides code and data for both tasks and explains that they wanted to explore the possibility of achieving comparable performance with manual fine-tuning at a lower cost. They used subsets of the Spider dataset and the Viggo functional representation dataset, which are known for teaching structured outputs rather than facts. The author also details the setup of their experiments, including the decision to use Code Llama 34B and Lora fine-tuning. They conclude that while fine-tuning GPT 3.5 may be suitable for initial validation or MVP work, models like Llama 2 might be more cost-effective for advanced tasks.

The discussion among Hacker News users on this post covers several topics related to the comparison between GPT 3.5 and Llama 2, as well as the considerations for cost and lifetime memberships with OpenAI. Here are the key points:

1. Some users express concerns about the cost of using Llama, particularly in comparison to GPT 3.5, suggesting that the lifetime memberships offered by OpenAI do not make sense considering the high ongoing costs of using the models.

2. Others comment on the practicality of relying on cloud computing and the theory behind it. They argue that it may not be the best approach for long-term projects, highlighting the importance of considering cost and scalability.

3. One user mentions that the terminology "functional representation dataset" is not well-defined, but they acknowledge the potential benefits of using structured propositional knowledge, citing examples like Viggo.

4. Another user expresses their struggles in finding good datasets for fine-tuning and asks for tips on creating sufficient datasets for specific use cases.

5. One user expresses interest in a similar comparison involving the RAG model and tasks related to it.

6. A user mentions that the notebook shared in the post demonstrates a reproducible evaluation process that correlates with general value and control evaluation.

Overall, the discussion revolves around the trade-offs between cost, performance, and the practicality of using different models for fine-tuning tasks. Some users express interest in alternative approaches and datasets for structured outputs and long-term projects.

### Those trying to pick AI winners should remember the dotcom days

#### [Submission URL](https://www.ft.com/content/82168156-006f-4d75-a4e9-0b6bdccef3b2) | 30 points | by [ent101](https://news.ycombinator.com/user?id=ent101) | [7 comments](https://news.ycombinator.com/item?id=37559105)

As AI continues to dominate conversations in the tech industry, it's crucial to remember the lessons learned from the dotcom era. The dotcom bubble burst in the early 2000s, leaving many startups and investors in shambles. This article highlights the importance of being cautious and realistic when evaluating AI winners, as history has shown that not every promising technology lives up to the hype.

he author emphasizes the need to exercise caution when assessing the potential winners in the AI space. Just like during the dotcom era, where everyone believed that the internet would revolutionize the world, there is now an overwhelming optimism around AI. However, it is crucial to separate the hype from reality and carefully evaluate each AI technology's actual capabilities and applications.

The article raises important questions for investors and entrepreneurs in the AI space. It reminds them to consider the scalability, practicality, and long-term sustainability of AI solutions before making any commitments. While AI holds immense potential, it's important not to get carried away by lofty promises and to remain realistic about the challenges and limitations that AI technologies face.

Overall, this article serves as a valuable reminder to analyze AI winners through a critical lens and to approach the AI landscape with the lessons learned from the dotcom bubble in mind. It encourages readers to seek a balance between optimism and caution and to make informed decisions when navigating the AI ecosystem.

### Stephen Fry says his voice stolen from Harry Potter audio books,replicated by AI

#### [Submission URL](https://fortune.com/2023/09/15/hollywood-strikes-stephen-fry-voice-copied-harry-potter-audiobooks-ai-deepfakes-sag-aftra-simon-pegg-brian-cox-matthew-mcconaughey/) | 26 points | by [thunderbong](https://news.ycombinator.com/user?id=thunderbong) | [23 comments](https://news.ycombinator.com/item?id=37554055)

Actor Stephen Fry has spoken out about the potential harm of AI in Hollywood, specifically regarding the use of AI to replicate actors' voices without their permission. Fry, who is a member of the actors' union SAG-AFTRA, mentioned his personal experience of having his identity digitally cloned and played a clip of an AI system mimicking his voice at the CogX Festival in London. He warned that AI technology is advancing rapidly and could soon produce deepfake videos that are just as convincing. Other actors, including Brian Cox and Simon Pegg, have also expressed concerns about AI in the film industry.

The discussion on Hacker News revolves around various aspects of AI replication of actors' voices and the potential implications. Some users express skepticism, comparing AI voice replication to long-standing celebrity impersonators and suggesting that legal theories might be able to cover this issue. Others discuss the technical aspects of AI voice cloning and mention Brian Blessed's distinctive voice. 

One user points out that AI recordings of coworkers in web meetings have been created, implying that the theft of voices is not a new issue. Another mentions the history of AI and its impact on various industries. 

The conversation also touches on the debate of whether AI can replace human creativity and whether AI-generated content can be considered art. One user references Walter Benjamin's 1935 philosophy and the implications of AI replication on artistic expression. 

There is a discussion about the commercial applications of AI voice cloning and how it could be used in large-scale projects. The post raises the question of whether actors' consent should be required to replace their voices in certain circumstances.

Some users argue that the focus should be on other more pressing global issues, such as climate change and humanitarian crises, rather than worrying about AI voice cloning. One user suggests that the discussion is radical and the focus should be shifted.

---

## AI Submissions for Sun Sep 17 2023 {{ 'date': '2023-09-17T17:10:53.971Z' }}

### Large Language Models for Compiler Optimization

#### [Submission URL](https://arxiv.org/abs/2309.07062) | 202 points | by [og_kalu](https://news.ycombinator.com/user?id=og_kalu) | [100 comments](https://news.ycombinator.com/item?id=37549216)

A recent paper on arXiv titled "Large Language Models for Compiler Optimization" explores the use of large language models for code optimization. The authors present a 7B-parameter transformer model trained to optimize LLVM assembly for code size. The model takes unoptimized assembly as input and outputs a list of compiler options to best optimize the program. During training, the model predicts instruction counts before and after optimization, as well as the optimized code itself, which significantly improves its performance. The model outperforms two state-of-the-art baselines that require thousands of compilations, achieving a 3.0% improvement in reducing instruction counts over the compiler. Additionally, the model demonstrates strong code reasoning abilities, generating compilable code 91% of the time and perfectly emulating the output of the compiler 70% of the time. This work showcases the potential of large language models in compiler optimization.

The discussion revolves around various aspects of the paper on large language models (LLMs) for compiler optimization. One comment notes that the paper does not discuss the importance of generated code semantics and suggests trying the approach on larger benchmarks. Another commenter highlights the misconception that LLMs directly model instructions, explaining that they instead generate passes for the compiler optimization. Another thread focuses on the challenges of achieving correctness in LLMs and the difficulty of measuring correctness. There is also a discussion about the potential of LLMs in addressing compilation optimization problems, but some express skepticism and suggest alternative approaches. The importance of both correctness and performance in compiler optimization is emphasized, as well as the need for further research in this area.

### Apple’s new Transformer-powered predictive text model

#### [Submission URL](https://jackcook.com/2023/09/08/predictive-text.html) | 495 points | by [nojito](https://news.ycombinator.com/user?id=nojito) | [241 comments](https://news.ycombinator.com/item?id=37541093)

Apple's upcoming iOS and macOS versions will feature a predictive text feature powered by a "Transformer language model." Despite Apple's focus on polish and perfection, this may be one of the first Transformer-based models they will ship. However, many details about the feature remain unclear. The feature suggests completions for individual words, occasionally suggesting multiple words when they are obvious. A Python script was used to snoop on AppleSpell activity and stream the most probable suggestions from the predictive text model. The model was located in a bundle file, which contains Espresso model files used during typing. Although the model couldn't be reverse-engineered, it is believed that the predictive text model is kept in this location. The vocabulary set for the model consists of 15,000 tokens, including special tokens, contractions, emojis, and a list of normal-looking tokens. The model's architecture appears to be GPT-2-based.

The discussion on this submission revolves around various aspects of Apple's predictive text feature powered by a "Transformer language model." Some users express surprise and disappointment that Apple's model is generating seemingly irrelevant and grammatically incorrect suggestions. Others speculate on the capabilities and limitations of the model, comparing it to GPT-2 and discussing the quality of its predictions. There is also discussion about the potential for Apple to improve the feature by incorporating higher-quality data or using GPT-3. Several users highlight the challenges of text prediction and autocorrection, including issues with slang and abbreviations. Some users share their experiences with Apple's spell checker and suggest using other tools like Google's spell check for better accuracy. In addition, there are comments about the nature of AI and the potential for AI technologies to be oversold or misused. Finally, there is a brief discussion about the practical limitations and privacy concerns of hosting large AI models on servers.

### A.I. and the Next Generation of Drone Warfare

#### [Submission URL](https://www.newyorker.com/news/news-desk/ai-and-the-next-generation-of-drone-warfare) | 73 points | by [fortran77](https://news.ycombinator.com/user?id=fortran77) | [97 comments](https://news.ycombinator.com/item?id=37549529)

The Deputy Secretary of Defense, Kathleen Hicks, has announced the Replicator initiative, an effort to modernize the American arsenal by adding fleets of artificially intelligent, unmanned, and relatively cheap weapons and equipment. These "attritable" machines can suffer attrition without compromising a mission. The initiative aims to field attritable autonomous systems at scale within the next eighteen to twenty-four months. Instead of concentrating resources on expensive and complicated equipment, Replicator aims to deploy equipment with a shorter shelf life, allowing for constant reinvention of technologies. The use of inexpensive aerial vehicles in concert with one another, known as drone swarms, is a key aspect of Replicator. This approach is based on "iPhone economics," where inexpensive physical devices with expensive software are deployed, so if the enemy destroys them, expensive software is not lost. The war in Ukraine provided proof of concept for drone swarms, as Ukraine used cheaper unmanned aerial vehicles to counter Russia's costly missile systems.

The discussion surrounding the announcement of the Replicator initiative has touched on a variety of topics. Some users have expressed concerns about the potential dangers of developing AI weapons and the implications for democracy. Others have referenced movies like Terminator and Slaughterbots, highlighting the ethical dilemmas associated with deploying such technology. The use of drones in the war in Ukraine has been cited as proof of concept for the effectiveness of drone swarms. There is also discussion about the challenges of balancing cost-effective defense with the need for human-designed weapons, as well as the difficulty of countering cheap and rapidly produced drone technology. The potential for AI-powered killbots and the threat they pose to humans is another topic of concern. Overall, there is a mix of skepticism, caution, and ethical considerations in the discussion.

### The Home Assistant Green is here

#### [Submission URL](https://www.theverge.com/23875557/home-assistant-green-announcement-price-specs-ten-year-anniversary) | 80 points | by [Tomte](https://news.ycombinator.com/user?id=Tomte) | [22 comments](https://news.ycombinator.com/item?id=37548884)

The Home Assistant Green is a new product introduced by the creators of Home Assistant, a software commonly used by privacy-focused individuals who want the benefits of smart home technology without compromising security. The Home Assistant Green aims to make the software more accessible to a wider range of users by providing an all-in-one box with a palatable price. Priced at $99, the Home Assistant Green features powerful hardware, including a RK3566 quad-core CPU, 32GB eMMC storage preloaded with Home Assistant's platform, 4GB of LDDR4x RAM, USB 2.0 slots, HDMI out, and a microSD slot for expansion. The device is designed to run solely on the Home Assistant Operating System and simplifies the onboarding process for users. To get started, users simply plug in the device, connect it to their router via ethernet, and go through the setup process using their phone or computer. The system will automatically detect compatible devices on the user's network. The Home Assistant Green is a convenient and affordable option for those who want to try out Home Assistant without the hassle of setting up hardware.

The discussion on this submission covers a variety of topics related to the Home Assistant Green and the Home Assistant software. Here are some key points:

- One commenter mentions the differences between the Home Assistant Green and the Home Assistant Yellow. They note that the Yellow version supports Raspberry Pi CM4-based boards, including CPU, RAM, networking, ZigBee, and Thread capability, while the Green version does not have these features. They speculate that the Green version might be a way to target a lower price point and simplify the installation process for new users.
- Another user mentions that they recently discovered the Home Assistant Supervisor, a well-designed and actively maintained open-source Python application. They appreciate the high-quality Python libraries and frameworks used in the project, but note a shortage of such libraries for studying open-source applications.
- Some users comment on the price and availability of the Home Assistant Green, noting that it is priced at $99 and that 1,000 units are available today, with 14,000 units available in October.
- The discussion also touches on the compatibility of Home Assistant with various smart home protocols. One user mentions their interest in Zigbee and Thread protocols, while another expresses disappointment with the current state of smart home integration in Home Assistant.
- There is some discussion about voice assistants and the integration of Home Assistant with Google Assistant. One user asks if Home Assistant supports voice commands, and another mentions that they are working on making a voice assistant for Home Assistant.
- Some users discuss the integration of Home Assistant with Apple HomeKit, noting that it provides HomeKit integration that works well.
- The conversation also touches on the use of Home Assistant for non-technical people and the discovery of smart home devices for touch support.
- Finally, there are some comments about other technologies and services that users have integrated with Home Assistant, such as YoLink, AdGuard, and Tailscale. Users share their experiences and discuss the ease of setting up these integrations.

### Spellburst: LLM–Powered Interactive Canvas

#### [Submission URL](https://arxiv.org/abs/2308.03921) | 95 points | by [araes](https://news.ycombinator.com/user?id=araes) | [13 comments](https://news.ycombinator.com/item?id=37540109)

Researchers Tyler Angert, Miroslav Ivan Suzara, Jenny Han, Christopher Lawrence Pondoc, and Hariharan Subramonyam have introduced an exciting new creative-coding environment called Spellburst. The interface is built on a large language model (LLM) and aims to enhance the process of exploratory creative coding. In the field of digital artwork, artists often start with a high-level semantic concept, like a "stained glass filter," and then programmatically implement it by tweaking code parameters such as shape, color, lines, and opacity. However, translating these semantic constructs into program syntax can be challenging, and existing programming tools do not lend themselves well to rapid creative exploration.

Spellburst addresses these challenges by providing a node-based interface that allows artists to create generative art and explore variations through branching and merging operations. The platform also incorporates expressive prompt-based interactions, enabling artists to engage in semantic programming. Moreover, Spellburst offers dynamic prompt-driven interfaces and direct code editing, allowing users to seamlessly switch between semantic and syntactic exploration.

The researchers evaluated Spellburst with artists and found that it has the potential to enhance creative coding practices. This innovative tool not only facilitates the translation of artistic ideas into code but also bridges the gap between semantic and syntactic spaces. The findings from this study could inform the design of future computational creativity tools.

Spellburst's novel approach to creative coding has the potential to revolutionize the way artists bring their ideas to life through code. With its user-friendly interface and powerful features, Spellburst opens up new possibilities for exploratory creative coding.

The discussion on Hacker News mainly centers around the novelty and potential applications of Spellburst, the new creative-coding environment introduced by the researchers.  One user shares a link to a video about the system on YouTube, highlighting its extensive design and development process. Another user mentions that Large Language Models (LLMs) have been gaining popularity and provides a link to a User Interface conference paper discussing their working structures and applications. Some users express interest in trying out Spellburst but have trouble finding where they can access it. One user mentions that there have been previews and tweets about its release but cannot find any public release. Another user comments that they are excited to try it and are willing to participate in testing. The discussion also touches on the use of metaphorical scratch paper in creative coding and the potential benefits it can offer. Some users mention the challenges of version control in creative coding tasks and express enthusiasm about the innovative features that Spellburst offers. One user comments on the post itself, providing an introduction and expressing their interest in the application of Large Language Models in creative endeavors.