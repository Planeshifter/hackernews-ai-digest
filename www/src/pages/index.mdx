import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Sun Dec 03 2023 {{ 'date': '2023-12-03T17:10:56.866Z' }}

### Stuxnet Source Code

#### [Submission URL](https://github.com/research-virus/stuxnet) | 158 points | by [Jimmc414](https://news.ycombinator.com/user?id=Jimmc414) | [106 comments](https://news.ycombinator.com/item?id=38511563)

The top story on Hacker News today is the public release of the Stuxnet malware's open-source code. Stuxnet, also known as MyRTUs, is a notorious piece of malware that was discovered in 2010 and is believed to have been developed as a cyberweapon to target Iran's nuclear program. The code provided in this repository was extracted from Stuxnet binaries and is now being made available for analysis and research purposes. The repository contains not only the Stuxnet code but also a rootkit source code. The authors of the code, Christian Roggia and Amr Thabet, have copyrighted their work, but they have made it available for free and have only asked for recognition and credit for their hard work. This release of Stuxnet's code is expected to provide valuable insights into the workings of the malware and help researchers better understand its capabilities and implications.

The discussion on the release of Stuxnet's open-source code touches on various topics. One user points out that the code contains interesting things related to the development of the malware. Another user discusses the involvement of Israel in the Stuxnet attack on Iran's nuclear program, while another user mentions a joint effort between the US and Israel. The conversation then veers towards the potential risks of mobile devices and the combination of GPS, audio, and video surveillance. There is a mention of a strategy involving tailored cyber warfare and the possibility of self-delivering non-nuclear weapons. The conversation also touches on genetic modifications and testing protocols, French nuclear weapons programs, and the relevance of a Darknet Diaries episode on the topic. The discussion dives into technical aspects of Stuxnet, such as how it did not break the working regime of the control systems and the difficulty of recovering source code. Users discuss obfuscating code, the possibility of decrypting binaries, and the availability of similar techniques since the 1980s. Overall, the discussion covers a wide range of topics related to Stuxnet and cybersecurity.

### Watsonx: IBM's code assistant for turning COBOL into Java

#### [Submission URL](https://www.pcmag.com/articles/ibms-plan-to-update-cobol-with-watson) | 116 points | by [Anon84](https://news.ycombinator.com/user?id=Anon84) | [174 comments](https://news.ycombinator.com/item?id=38508250)

IBM is developing an AI-powered code assistant called WatsonX to modernize COBOL, a programming language widely used in industries such as banking, insurance, and healthcare. COBOL, which handles $3 trillion worth of transactions each day, is becoming increasingly difficult to maintain as programmers with expertise in the language retire. WatsonX aims to save coders time by converting COBOL code into a more modern language. The process involves breaking down the application into smaller pieces and selectively choosing which parts to modernize. However, skeptics have raised concerns about IBM's previous AI project, Watson Health, which failed to deliver on its promises. While WatsonX is still in its early stages, IBM remains optimistic about its potential.

The discussion on the submission covers various perspectives on IBM's AI-powered code assistant, WatsonX, and the challenges of modernizing COBOL.

- Some users express skepticism about IBM's track record with previous AI projects like Watson Health, raising concerns about the success of WatsonX.
- Others argue that the management problem in AI projects can hinder their effectiveness, as AI may not be able to fix management issues.
- One user mentions that the demand for COBOL programmers is high, leading to higher salaries for contractors in the insurance industry.
- There is a discussion about the difficulty of migrating COBOL systems to modern languages, with some users suggesting that rewriting the code from scratch is not feasible due to the complexity and compatibility issues.
- The performance implications and technical challenges of converting COBOL to Java are also discussed.
- Some users point out that COBOL has specific features, such as global variables, that make it challenging to convert to Java.
- Additionally, there is a debate about the motivations and limitations of rewriting COBOL code, with some users suggesting that it is more practical to maintain and modernize legacy systems rather than rewriting them entirely.
- The discussion also touches on the topic of technical debt and the risks and benefits of adopting new technologies versus maintaining existing systems.

Overall, the conversation covers various perspectives on the challenges of modernizing COBOL and the potential efficacy of IBM's WatsonX in addressing these challenges.

### 1960s chatbot ELIZA beat OpenAI's GPT-3.5 in a recent Turing test study

#### [Submission URL](https://arstechnica.com/information-technology/2023/12/real-humans-appeared-human-63-of-the-time-in-recent-turing-test-ai-study/) | 132 points | by [layer8](https://news.ycombinator.com/user?id=layer8) | [44 comments](https://news.ycombinator.com/item?id=38506175)

A preprint research paper titled "Does GPT-4 Pass the Turing Test?" conducted by researchers from UC San Diego compares OpenAI's GPT-4 AI language model to human participants, GPT-3.5, and the 1960s computer program called ELIZA. The study found that human participants were only able to correctly identify other humans in 63% of interactions, and that ELIZA outperformed the GPT-3.5 AI model. However, GPT-4 achieved a success rate of 41%, second only to actual humans. The study raises questions about using the Turing test to evaluate AI model performance and the limitations of current AI models.

In the discussion, there are different viewpoints regarding the research paper and the Turing test. Some participants argue that the Turing test is flawed and that there are better ways to evaluate AI models. They point out that the study shows the limitations of current AI models and raises questions about their performance compared to humans. Others discuss the historical significance of ELIZA and its comparison to modern AI models. Some participants also discuss the practical applications of AI and the importance of human oversight in customer support. The discussion touches on topics such as the nature of consciousness and the definition of intelligence. Overall, there is a mix of opinions and perspectives in the conversation.

### GPT-4 Can Almost Perfectly Handle Unnatural Scrambled Text

#### [Submission URL](https://arxiv.org/abs/2311.18805) | 199 points | by [saliagato](https://news.ycombinator.com/user?id=saliagato) | [137 comments](https://news.ycombinator.com/item?id=38506140)

In a recent study, researchers have found that GPT-4, a large language model, can almost perfectly handle and correct unnatural scrambled text. The researchers designed a suite called the Scrambled Bench to measure the capacity of language models to handle scrambled input. The experimental results showed that GPT-4 was able to reconstruct the original sentences from scrambled ones with an impressive 95% reduction in edit distance, even when all letters within each word were scrambled. This resilience displayed by GPT-4 is counter-intuitive, considering the severe disruption to input tokenization caused by scrambled text. The findings provide novel insights into the inner workings of large language models and their ability to handle unconventional textual input.

The discussion on this submission revolves around the ability of GPT-4, a large language model, to handle scrambled text and its implications. Some users express their surprise at GPT-4's capability to reconstruct original sentences from scrambled ones with a 95% reduction in edit distance. Others discuss the challenges in word segmentation and the use of backtracking. The conversation also touches on the limitations and imperfections of GPT-4 and the role of tokenization in language models. Some users experiment with feeding scrambled text into search engines and observe different results. The discussion concludes with users discussing alternative models and their success in similar tasks.

### Mozilla Lets Folks Turn AI LLMs into Single-File Executables

#### [Submission URL](https://hackaday.com/2023/12/02/mozilla-lets-folks-turn-ai-llms-into-single-file-executables/) | 69 points | by [anonymousiam](https://news.ycombinator.com/user?id=anonymousiam) | [3 comments](https://news.ycombinator.com/item?id=38503588)

Mozilla's innovation group has released an open-source method called "llamafile" to turn a set of weights into a single binary file, making it easier to distribute and run Large Language Models (LLMs). Llamafile supports six different operating systems and ensures that a particular version of an LLM remains consistent and reproducible. It uses the "Cosmopolitan" build-once-run-anywhere framework created by Justine Tunney, along with the llama.cpp tool. Sample binaries using different LLMs are available, with the LLaVA 1.5 LLM being the only one that can run on Windows due to the 4 GB limit on executable files.

The discussion about the submission revolves around the technical aspects of llamafile and how it can be beneficial in distributing and running Large Language Models (LLMs). Some users mention that distributing LLMs as multiple files can be challenging to distribute, as changes and tweaks to the software can lead to different results with different versions. One user mentions that llamafile supports multiple operating systems including macOS, Windows, Linux, FreeBSD, OpenBSD, and NetBSD. They also point out that using llamafile dramatically simplifies the distribution process of LLMs, ensuring that a specific version of an LLM remains consistent and reproducible indefinitely. Another user notes that llamafile relies on the Cosmopolitan build-once-run-anywhere framework developed by Justine Tunney, specifically llama.cpp. This framework is commended for its efficiency in running self-hosted LLMs. In the comments, a link to a related discussion is shared. It is not clear what the specific topic of that discussion is.

Finally, one user simply mentions "ppl" (presumably referring to people) and "llmcpp" without further clarification, leaving their comment's meaning open to interpretation.

### Run 70B LLM Inference on a Single 4GB GPU with This New Technique

#### [Submission URL](https://ai.gopubby.com/unbelievable-run-70b-llm-inference-on-a-single-4gb-gpu-with-this-new-technique-93e2057c7eeb?gi=cbe7920f4cd2) | 108 points | by [gardenfelder](https://news.ycombinator.com/user?id=gardenfelder) | [56 comments](https://news.ycombinator.com/item?id=38508571)

Have you ever wondered if it's possible to run inference on a single GPU with a large language model? Well, Gavin Li has come up with a new technique that allows you to do just that. In a recent article, he explains the key techniques for extreme memory optimization of large models. The most critical technique is layer-wise inference. By leveraging the divide and conquer approach, Li shows that you don't actually need to keep all layers in GPU memory. Instead, you can load whichever layer is needed from disk when executing that layer, do all the calculations, and then free up the memory. This significantly reduces the GPU memory required per layer.

Li also introduces flash attention, which is a critical optimization for large language models. Flash attention deeply optimizes CUDA memory access to achieve multi-fold speedups for inference and training. This optimization reduces the memory complexity from O(n²) to O(n), making it much more efficient. To further optimize the memory usage, Li discusses model file sharding. The original model file is typically sharded into multiple chunks, but loading the entire file for each layer execution wastes a lot of memory and disk reading time. By pre-processing the model file and sharding it by layers, the memory usage is minimized. To implement these techniques, Li utilizes the meta device feature provided by HuggingFace Accelerate. The meta device is a virtual device designed for running ultra large models. It allows you to dynamically transfer parts of the model from the meta device to a real device like the CPU or GPU during execution, reducing memory usage. If you're interested in trying out these optimizations for yourself, Li has open-sourced the code in a library called AirLLM. The library, which can be found on the Anima GitHub, allows you to achieve extreme memory optimization with just a few lines of code.

So there you have it, with these techniques and the AirLLM library, you can run inference on a single 4GB GPU with a 70B large language model. It's truly unbelievable!

The discussion on this submission revolves around various aspects of running inference on a single GPU with a large language model. Here are some key points from the comments:

- One commenter mentions that they tried running the techniques on a GTX 1060 6GB GPU but found it to be slow, taking 13 hours to generate a sentence. They speculate that the GPU memory usage might be the bottleneck.
- Another commenter discusses the technique of model file sharding and suggests looking into PyTorch's gradient checkpointing for efficient memory usage.
- A discussion arises regarding the difference in performance between CPU and GPU inference. Some commenters mention that CPU inference is typically limited by memory bandwidth, whereas GPU inference benefits from loading weights onto the GPU. SSD loading of weights is also mentioned as a way to reduce GPU memory usage.
- There is some skepticism raised regarding the capabilities of non-batched inference for large language models. Commenters mention that certain context and historical understanding provided by complete sentences might be necessary for relevant embeddings and projections in custom matrix operations.
- The potential drawbacks of extreme memory optimization are also discussed, with one commenter pointing out that the process seems to swap VRAM to RAM and disk, which can significantly impact performance.
- Various optimization techniques and resources are shared, including libraries such as AirLLM and llmcpp, as well as discussions on quantizing models and using DirectStorage for improved IO.
- Further discussions revolve around distributed computing and the benefits of using multiple GPUs to process different layers simultaneously, though latency is a concern in such setups.

Overall, the discussion explores different techniques, optimizations, and limitations related to running inference on a single GPU with a large language model.

---

## AI Submissions for Sat Dec 02 2023 {{ 'date': '2023-12-02T17:09:38.438Z' }}

### Unsupervised speech-to-speech translation from monolingual data

#### [Submission URL](https://blog.research.google/2023/12/unsupervised-speech-to-speech.html) | 20 points | by [atg_abhishek](https://news.ycombinator.com/user?id=atg_abhishek) | [4 comments](https://news.ycombinator.com/item?id=38497549)

Google Research has introduced Translatotron 3, an unsupervised speech-to-speech translation architecture that can learn the translation task from monolingual data alone. Traditional speech-to-speech translation models rely on parallel speech data, which is scarce, leading to the need for synthesized data. However, Translatotron 3 eliminates the requirement for bilingual speech datasets by incorporating techniques such as back-translation, pre-training with SpecAugment, and unsupervised embedding mapping based on Multilingual Unsupervised Embeddings (MUSE). Experimental results between Spanish and English show that Translatotron 3 outperforms a baseline cascade system. By preserving paralinguistic characteristics, such as tone and emotion, Translatotron 3 aims to improve the quality and authenticity of translated speech.

The discussion revolves around the Translatotron 3 submission on Hacker News. One user, "xnx," comments on the challenges of translating languages and mentions that it often stretches the mind. Another user, "grsv," responds, stating that it is humans' sentience and comprehension that allows them to learn languages, and the proposed method relies on training models with monolingual speech-text datasets. "grsv" further explains that the proposed method utilizes a shared embedding space for languages, forcing the model to learn semantics independently of the language. Additionally, they mention using back-translation for training and conducting performance checks using a Spanish-English-Spanish translation loop. They express enthusiasm for the promising and interesting results, wondering how similar the languages need to be at a lexical level for the model's performance to excel. Another user, "IanCal," shares their curiosity about the level of complexity in English and internal linguistic representation, and suggests that if there is a low complexity in mapping internal representations, then a sensible sentence in English should result in a sensible sentence in the translated language.

Overall, the discussion focuses on the methodology and potential implications of Translatotron 3, with users expressing interest in the results and exploring the nuances of language translation.

### Galactic algorithm

#### [Submission URL](https://en.wikipedia.org/wiki/Galactic_algorithm) | 115 points | by [sockaddr](https://news.ycombinator.com/user?id=sockaddr) | [19 comments](https://news.ycombinator.com/item?id=38500782)

In computer science, there is a concept called a galactic algorithm. These are algorithms that have incredible theoretical performance but are never actually used in practice. There are a few reasons for this. Sometimes the performance gains only apply to problems that are so large they never occur in real-world scenarios. Other times, the complexity of the algorithm outweighs the relatively small gain in performance. These algorithms are named "galactic" because they will never be used on any data sets here on Earth.

Even though galactic algorithms are not used in practice, they can still contribute to computer science in a few ways. Firstly, they may introduce new techniques that can eventually be used to create practical algorithms. Secondly, as computational power advances, previously impractical algorithms may become feasible to use. Thirdly, even if an algorithm is impractical, it can still demonstrate that certain bounds can be achieved or prove that proposed bounds are incorrect, thereby advancing the theory of algorithms.

For example, there are galactic algorithms for problems like integer multiplication, matrix multiplication, communication channel capacity, sub-graph testing, cryptographic breaks, and the traveling salesman problem. These algorithms have impressive theoretical performance, but their large constants make them impractical for real-world use. However, they still serve important purposes. For instance, they can inspire further research and refinement to make them more practical or they can settle important open problems in computer science, like the P versus NP problem.

So, while galactic algorithms may never be used in practice, they still have value in advancing the field of computer science and pushing the boundaries of what is possible.

The discussion on this submission covers a range of topics related to galactic algorithms. One commenter shares a link to a paper that discusses a simulated annealing algorithm for solving global optimization problems. Another commenter expresses doubt about the practicality of these algorithms, suggesting that simulated annealing and random restarts may not be effective. Another user finds the topic interesting and mentions that numbers can be fascinating. 

One commenter asks if the concept of galactic algorithms can be applied to other fields. Another user shares a link to Optimal Universal Search, which is related to the topic of optimal algorithms. 

There is also a discussion about the classification of General Relativity as a galactic algorithm for solving Newtonian Equations. A user argues that General Relativity is not a true galactic algorithm because it provides similar answers when calculating the motion of spacecraft around a black hole. However, another user argues that the comparison is not accurate and that the complexity and practicality of General Relativity differ from galactic algorithms.

The discussion ends with a user mentioning that General Relativity is used in GPS to correct clock discrepancies based on its small correction below a galactic scale.

### Show HN: ChatCBT – AI-powered cognitive behavioral therapist for Obsidian

#### [Submission URL](https://github.com/clairefro/obsidian-chat-cbt-plugin) | 55 points | by [marjipan200](https://news.ycombinator.com/user?id=marjipan200) | [18 comments](https://news.ycombinator.com/item?id=38499722)

Introducing ChatCBT: an AI-powered journaling plugin for your Obsidian notes. Inspired by cognitive behavioral therapy (CBT), this plugin acts as a journaling assistant that helps you reframe negative thoughts and rewire your reactions to distressful situations. 

With ChatCBT, you can start chatting in a note and receive kind and objective responses to help uncover negative thinking patterns. Conversations are stored privately on your computer, and you can automatically summarize your reframed thoughts in a table to inspire affirmations. 

The plugin offers two options for handling your data: you can choose to use a cloud-based AI service (OpenAI) or a 100% local and private service (Ollama). OpenAI provides excellent conversation quality and speed, but it is a paid service. On the other hand, Ollama is free and offers good conversation quality.

To install ChatCBT, follow the instructions provided in the repository. The plugin is currently under review to become an official Obsidian Community Plugin, but you can still install it in developer mode. Once installed, you can configure an AI platform connection from the plugin settings menu.

Overall, ChatCBT is a powerful tool for journaling and self-reflection, leveraging the capabilities of AI to assist you in improving your mental well-being.

The discussion around the submission of ChatCBT on Hacker News covers various aspects of the plugin and its potential benefits.

One user raises concerns about the effectiveness of AI-powered therapy compared to traditional cognitive behavioral therapy (CBT). They argue that while AI may have potential, it is important to diagnose problems correctly, and certain issues require the guidance of a qualified human therapist.

Another user points out that the plugin installation may not work properly and reports encountering errors related to server problems. Another user suggests investigating the issue further.

A user mentions that relying on an AI plugin may discourage people from seeking help from licensed therapists. They argue that AI lacks the understanding and learning methods that are a significant part of therapy, and interacting with a human therapist provides a more effective way of improving one's life. They suggest exploring cheaper alternatives with qualified professionals instead of relying solely on AI.
In response to this, another user clarifies that ChatCBT is not intended to replace professional therapy but rather supplement it. They explain that the plugin is designed to provide interactive journaling similar to CBT worksheets that therapists provide to patients. They emphasize that it is not meant to replace human interaction but rather be used as a tool for self-reflection.
There is a discussion about the affordability of therapy and the challenges many people face in accessing mental health care. Some users express frustration with the stigmatization of mental health issues and the limited coverage provided by insurance providers, making therapy inaccessible to many.
One user points out that therapy is important and should not be undervalued, highlighting that licensed therapists can help people understand and work through their problems in a realistic and systematic way.
Another user suggests that AI could be beneficial in developing personal self-assistants, such as an interactive AI like GPT-4, which could have therapeutic effects and help individuals make decisions, manage regrets, and provide guidance similar to that of a powerful therapist.
Overall, the discussion reflects varying perspectives on the role of AI in mental health care, emphasizing the importance of professional therapy while acknowledging the potential benefits of AI-powered tools as supplements for self-reflection and journaling. There is also recognition of the challenges in accessing affordable and comprehensive mental health care.

### Optical effect advances quantum computing with atomic qubits to a new dimension

#### [Submission URL](https://www.tu-darmstadt.de/universitaet/aktuelles_meldungen/einzelansicht_410816.en.jsp) | 51 points | by [FinnKuhn](https://news.ycombinator.com/user?id=FinnKuhn) | [14 comments](https://news.ycombinator.com/item?id=38494466)

Scientists at the Technische Universität Darmstadt in Germany have developed a technique using the optical Talbot effect to increase the number of qubits in a quantum computer without requiring additional resources. Qubits are the basic units of information in quantum computing and can process both "0" and "1" simultaneously, allowing for parallel calculations. Currently, quantum computers are limited to a few hundred qubits, but for practical applications, such as optimizing traffic flows, thousands or millions of qubits are needed. The Darmstadt team's approach uses laser beams and a glass element with microlenses arranged like a chessboard to create a 3D lattice of focal points that can hold individual atoms as qubits. By exploiting the Talbot effect, multiple layers of qubits can be added without needing additional laser output. The researchers were able to create 16 layers of qubits, potentially allowing for over 10,000 qubits. The team plans to further develop the technology for applications in quantum technologies and high-precision optical atomic clocks.

The discussion on this submission revolves around various aspects of the technology and its implications.

- One user raises a concern about the potential difficulty of scaling this technology due to the largest single-qubit coherence time limitation.
- Another user counters this argument, stating that quantum error correction schemes help mitigate coherence limitations and make it easier to achieve thousands of qubits.
- A user suggests that the discovery of transistors in the past took 50 years to reach mass production, and it is expected that the development of quantum computers will follow a similar incremental timeline.
- The difficulty of developing transistors for quantum computing is discussed, with one user suggesting that it is significantly harder to create transistors for quantum computing compared to classical computing.
- Another user raises the point that the statement about the difficulty of developing transistors for quantum computing is redundant and does not make sense in the context of the discussion.
- The challenges of funding quantum technology development are mentioned, with private funding playing a significant role compared to government investment.
- A user believes that decoherence times for qubits are becoming longer, making natural times higher and enabling better performance.
- Discussion moves towards the technical aspects of qubit coupling and the configuration of individual qubits.
- One user shares a link to a paper that may provide more information on the topic.
- Another user shares a different paper that describes a scalable multi-layer architecture for single-time qubit arrays using a three-dimensional Talbot interferometer lattice.

Overall, the discussion covers topics such as scalability, coherence limitations, development timelines, funding challenges, and technical aspects of qubit coupling and configuration.

### Scalable extraction of training data from (production) language models

#### [Submission URL](https://arxiv.org/abs/2311.17035) | 99 points | by [wazokazi](https://news.ycombinator.com/user?id=wazokazi) | [14 comments](https://news.ycombinator.com/item?id=38496715)

In a paper titled "Scalable Extraction of Training Data from (Production) Language Models," researchers explore the concept of extractable memorization, referring to training data that can be extracted from machine learning models through queries without prior knowledge of the dataset. The study reveals that adversaries can efficiently extract gigabytes of training data from various language models, including Pythia, GPT-Neo, LLaMA, Falcon, and ChatGPT. The researchers demonstrate that existing techniques can attack unaligned models, and they develop a new divergence attack specifically targeting ChatGPT. This attack causes the model to deviate from its chatbot-style responses, resulting in the emission of training data at a rate 150 times higher than normal. The findings suggest that current alignment techniques do not eliminate memorization and expose the potential for practical attacks to recover more data than previously thought.

The discussion around the submission revolves around various aspects of the research paper on scalable extraction of training data from language models.

- One commenter discusses the use of memorization techniques to reduce hallucinations and improve the relevance of passages retrieved by language models.
- Another user suggests that the issue of memorization is not surprising, considering that language models like ChatGPT have access to vast amounts of internet data. They mention that DeepMind recently extracted personally identifiable information (PII) from ChatGPT prompts.
- There is a question raised about whether language models are trained on secret data. The response suggests that they likely contain copyrighted data, such as lyrics from songs or track crashes.
- A user raises a question about the capacity of models to memorize an infinite amount of information and whether the extraction of total memorized data is possible. The response highlights that language models are restricted to a limited number of keywords and that the size of the training dataset is a determining factor.
- A discussion ensues about the capabilities of models like GPT-NeoX in generating grammatically novel sentences based on different settings and training inputs.
- A user clarifies that the 50-grams mentioned in the research paper refer to substrings of 50 words from the dataset, and generating complete 50-grams is challenging even for a modern GPU.
- There is further clarification about creating the 50-gram dataset and its relevance to the research paper's findings.

Overall, the discussion delves into the implications and limitations of language models' memorization capabilities and the concerns surrounding the extraction of training data.

### Meta will enforce ban on AI-powered political ads in every nation, no exceptions

#### [Submission URL](https://www.zdnet.com/article/meta-will-enforce-ban-on-ai-powered-political-ads-in-every-nation-no-exceptions/) | 111 points | by [jnord](https://news.ycombinator.com/user?id=jnord) | [42 comments](https://news.ycombinator.com/item?id=38495875)

Meta, the parent company of social media platforms like Facebook and Instagram, has announced that it will enforce a ban on AI-powered political ads in all nations, without exceptions. This comes as several countries are set to hold elections next year. The ban extends to ads targeting specific services and issues related to politics, elections, housing, employment, credit, social issues, health, pharmaceuticals, and financial services. Meta's generative AI advertising tools, which include features like text variation and image cropping, will not be accessible for these types of campaigns. Meta has emphasized the importance of AI and plans to add generative AI capabilities across all its platforms. The company's Ads Manager tool serves as a launchpad for running ads, providing advertisers with an all-in-one tool for ad creation, management, and tracking. It also recently introduced an AI chatbot called Meta AI and an AI image generator tool called Emu. Meta's AI products have been adopted by more than half of advertisers, with the Advantage+ tools helping advertisers achieve a $10 billion run rate from shopping campaigns.

The discussion on Hacker News revolves around Meta's announcement to ban AI-powered political ads on its platforms. Some users express skepticism about the effectiveness of Meta's detection algorithms, highlighting challenges in accurately detecting and preventing AI-generated content. Others argue that the ban is necessary to prevent exploitation and misleading advertising. There is also a debate about Meta's intentions and trustworthiness as an organization. Some users question whether Meta's ban is selective and if it will be effectively enforced. There is a suggestion that AI detectors may be developed to identify AI-generated political ads. Additionally, there are discussions about the limitations of AI-generated text and images and the impact of the ban on political campaigns. Some users express support for Meta's decision, while others express concerns about potential censorship and exceptions to the ban.

### Good old-fashioned AI remains viable in spite of the rise of LLMs

#### [Submission URL](https://techcrunch.com/2023/12/01/good-old-fashioned-ai-remains-viable-in-spite-of-the-rise-of-llms/) | 76 points | by [webmaven](https://news.ycombinator.com/user?id=webmaven) | [41 comments](https://news.ycombinator.com/item?id=38499723)

In a recent article on TechCrunch, it is highlighted that task-based models in artificial intelligence (AI) are not going away despite the rise of generalized large language models (LLMs) like ChatGPT. Task-based models have been the cornerstone of AI in the enterprise for a long time, and they still play a crucial role in solving real-world problems. While LLMs offer flexibility and the ability to handle varied tasks, task models are smaller, faster, cheaper, and more performant for specific tasks. The industry is still debating the capabilities and limitations of LLMs in comparison to task models. Amazon CTO Werner Vogels and Atul Deo, general manager of Amazon Bedrock, both believe that task models are valuable AI tools and are not likely to disappear. They argue that an all-purpose model is appealing on an aggregate level, but task models offer the advantage of reusability and specialized design. However, the upgrades made to Amazon's machine learning operations platform, SageMaker, indicate that the company recognizes the importance of managing large language models. While LLMs have gained significant attention, enterprises are unlikely to abandon their investments in task models. Data scientists still play a vital role in understanding data and AI within companies, regardless of the model being used. The article concludes that both task models and LLMs will continue to coexist as they have their own strengths and applications in the AI landscape.

The discussion around the submission revolves around the comparison between task-based models and large language models (LLMs) in artificial intelligence (AI). Some users argue that knowing the appropriate tools for modern AI work is crucial. LLMs may struggle with gradient-based training algorithms and require significant amounts of data, which can lead to subpar results. Task models, on the other hand, are smaller, faster, cheaper, and more effective for specific tasks.  There is a debate about the capabilities and limitations of LLMs compared to task models. Some users point out that LLMs like BERT and RoBERTa can outperform smaller models in certain tasks, while others argue that LLMs fall short and smaller models focused on specific approaches can dominate in the field. There is also a discussion about the challenges and strengths of different AI models. Some users mention that traditional symbolic AI, also known as GOFAI (Good Old-Fashioned AI), has limitations, while others argue that LLMs have their own disadvantages. There is a mention of using fasttext and word2vec for production work and the complexities involved in training models from scratch. The discussion touches on topics like GOFAI, symbolic AI, probabilistic AI, deep learning, and the use of LLMs for tasks such as classification and generation. Some users express skepticism about the viability and long-term sustainability of certain AI approaches. Various users also discuss the importance of quality data, the limitations of LLMs, and the impact of AI on industries like customer service and marketing.

Overall, the discussion highlights the coexistence of task models and LLMs in the AI landscape, with users sharing their perspectives on the strengths and weaknesses of both approaches.

### AI can tell what you're typing by listening to the sound of your keyboard

#### [Submission URL](https://www.theregister.com/2023/08/07/audio_keystroke_security/) | 22 points | by [thisAintReal](https://news.ycombinator.com/user?id=thisAintReal) | [21 comments](https://news.ycombinator.com/item?id=38496967)

Researchers in the UK claim to have developed a method to turn typing sounds into text with 95% accuracy. Using deep learning and self-attention transformer layers, the team was able to capture the sounds of typing and translate them into data for exfiltration. The method achieved high accuracy rates even over remote methods like Zoom and Skype calls. The researchers suggest that changing one's typing style or using randomized passwords with multiple cases can mitigate the risk of this type of attack. They also recommend using a second authentication factor and playing fake keystroke sounds to mask the real ones to further protect sensitive information. Further research is being conducted to explore new sources for recordings and improve the effectiveness of acoustic snooping.

- One commenter mentions that the concept of using typing sounds for cybersecurity purposes has been around since the 1960s, and provides a link to an article on acoustic cryptanalysis.
- Another commenter points out that the accuracy of recordings dropped significantly on Zoom calls (93%) compared to Skype calls (917%). They find it interesting that Skype messenger is well-known for its good audio quality.
- A discussion ensues about voice codecs and the potential for variance in accuracy based on sampling rates and other factors.
- A user asks about a previous article they read about a laptop keyboard typing detection application and how reliable it is.
- The implications of the article regarding passwords are discussed. One commenter mentions that they have typed passwords in their comments, unaware that it was for their benefit.
- The topic of typing rhythm and variations in typing patterns is raised, indicating that it may have an impact on the effectiveness of acoustic snooping.
- A commenter discusses their use of the Colemak keyboard layout and how it can potentially render acoustic attacks ineffective.
- Some users discuss operating system-level filtering and mention that Zoom has built-in noise filtering.
- A humorous comment suggests creating powerless keyboards to counteract the threat.
- There is a conversation about the convenience of typing on various keyboards and the different sounds they produce.
- The possibility of creating keyboards specifically designed to produce distinct tones for each keypress is mentioned.
- A user comments on the processing requirements and limitations of using acoustic data for analysis.
- One commenter mentions that they type at a high speed of over 100 words per minute.
- The discussion wraps up with a mention of Facebook and Google.

### Javelin Missile guidance computer – Part 1: teardown [video]

#### [Submission URL](https://www.youtube.com/watch?v=11_5TB0-lNw) | 50 points | by [dun44](https://news.ycombinator.com/user?id=dun44) | [4 comments](https://news.ycombinator.com/item?id=38494777)

It seems like you've provided some information about YouTube and NFL Sunday Ticket. Is there something specific you would like to know or discuss about these topics?

The discussion on this submission revolves around the surprise that FPGAs (Field-Programmable Gate Arrays) are being used in the mass production of missiles. One user comments that they are surprised because FPGA designs are often considered less reliable than ASICs (Application-Specific Integrated Circuits). Another user justifies this use by mentioning that debugging custom silicon for thousands of missiles would have been expensive, and FPGAs offer the advantage of reconfigurability. 

In response to this, another user points out that military procurement budgets often have constraints, and it is likely that the Ukrainian military, which is mentioned in the initial submission, may not have had the resources to spend on expensive custom hardware. 

Finally, a user comments that the post has been duplicated and provides a link to the original discussion.

### Pentagon Scientists Discuss Cybernetic Super Soldiers in Dystopian Presentation

#### [Submission URL](https://www.vice.com/en/article/n7eky8/pentagon-scientists-discuss-cybernetic-super-soldiers-that-feel-nothing-while-killing-in-dystopian-presentation) | 32 points | by [rntn](https://news.ycombinator.com/user?id=rntn) | [22 comments](https://news.ycombinator.com/item?id=38498884)

In a dystopian presentation at the Interservice/Industry Training, Simulation and Education Conference (I/ITSEC), officials from the Pentagon discussed the concept of creating cybernetic "super soldiers" inspired by characters like Captain America and Iron Man. The panel of military and military-adjacent scientists delved into topics like breeding programs, Marvel movies, The Matrix, and the various technologies being researched to achieve this vision. Some ideas discussed included cybernetic implants, pain-numbing stimulants, synthetic blood, and the ability to regrow limbs. The conversation also touched on the ethical concerns surrounding bodily autonomy and the potential for extending the service of veteran soldiers or enlisting older individuals by leveraging the technology. The panelists acknowledged the applicability of these ideas and the potential benefits they could bring. Additionally, they discussed the use of non-invasive brain stimulation techniques to interface with the brain directly, similar to the concept portrayed in The Matrix. The conversation delved into the ethical and legal boundaries associated with creating super soldiers and questioned societal norms and ethics. Overall, the talk highlighted the ongoing efforts to enhance military capabilities using cutting-edge technology but also raised important questions about the ethical implications of such advancements.

The discussion surrounding the submission on creating cybernetic "super soldiers" had a range of responses. One commenter pointed out that military and industrial complexes are constantly pursuing profit-centered contracts, inventing terrible weapons, and taking back control from corporatocracy. Another commenter mentioned that technology exists in diverse hands, implying that the potential for cybernetic enhancements is not limited to the military. There was also a discussion about the ethnicity of the super soldiers, with one commenter asking about the ethnicity of Hispanic/Latino super soldiers. The question was further explored, with another commenter considering the significance of ethnicity in this context. Another point raised in the discussion was the nature of the objectives of the defensive vs offensive groups. One commenter argued that the focus should be on defensive objectives, while another pointed out that detecting and disabling the enemy is inherently offensive. There were also references to fictional works, such as a comparison to the concept of super soldiers in the game Deus Ex and a mention of Peter Watts' book about zombies and combat effectiveness. Some commenters expressed concern about the consequences of this technology, while others expressed skepticism or resigned acceptance.

### The Evolution of Intelligence Itself

#### [Submission URL](https://metastable.org/evolution.html) | 9 points | by [pbw](https://news.ycombinator.com/user?id=pbw) | [6 comments](https://news.ycombinator.com/item?id=38495385)

In a recent article, Philip Winston reflects on the evolution of intelligence and its relationship with AI. He highlights the tremendous progress made in computing power over the past decades, demonstrating how AI is now able to surpass human capabilities in various domains. Winston notes that the accessibility of AI systems like AI Art and ChatGPT has drastically changed our perception of AI, as they produce text and images at a level that is indistinguishable from human creations. He predicts that generative AI will soon expand to create all types of media, including movies, music, and video games. However, while some embrace the potential of AI to improve various aspects of human life, others express concerns about its impact on employment, meaning, and even humanity's place in the world. Winston believes that these concerns are valid, but he also emphasizes the immense benefits that AI could bring to areas such as energy, manufacturing, healthcare, and education. He argues that with thoughtful and careful navigation, we can mitigate the risks associated with AI. Winston likens AI to organized groups of people, highlighting the parallel between human collaboration and the coordination that occurs within AI systems. He suggests visualizing AI accomplishments as the work of a team of people, emphasizing the effort and hard work involved rather than treating it as something mysterious or threatening. Winston also discusses the significant role of training in the development of AI systems, noting that they are built and trained on the collective work of humans. He envisions a future where AI technology is accessible to all, providing individuals with the power of humanity's collective knowledge and talent. Ultimately, Winston finds reassurance in the inevitability of AI, seeing it as a natural extension of human evolution and emphasizing the need to manage and embrace its potential.

The discussion on this submission revolves around various aspects of AI and its potential impact on humans. Below are some notable comments:

1. One user argues that the current hype around AGI (Artificial General Intelligence) is misguided and suggests that AI advancements are simply sophisticated text compressors and word prediction models. They express concern about the large amount of money and energy spent on training AI systems while poverty still remains a major issue.
2. Another user counters this argument, stating that AI has the potential to significantly improve various aspects of human life. They believe that AI requires intelligence, and if intelligence is limited, then AI can no longer progress. They highlight the complexity and potential benefits AI can bring.
3. In response to the second user, someone else raises the issue of resource allocation, pointing out that while billions of dollars are spent on AI development, there are still pressing global problems like cancer, COVID-19, and Alzheimer's that require significant funding.
4. Another user points out the policy problem of healthcare spending, stating that AI has the potential to improve things, but the issue lies in the allocation of resources and the implementation of AI solutions.
5. One user imagines a future where AI aids in decision-making for small, hard problems, such as text-to-speech messages, video calls, and more.

6. A comment suggests that AI can consolidate the power of humanity and its decision-making processes.

Overall, the discussion touches on the potential benefits of AI, concerns about resource allocation, and the role AI could play in decision-making and problem-solving.

---

## AI Submissions for Fri Dec 01 2023 {{ 'date': '2023-12-01T17:10:30.704Z' }}

### The Inside Story of Microsoft's Partnership with OpenAI

#### [Submission URL](https://www.newyorker.com/magazine/2023/12/11/the-inside-story-of-microsofts-partnership-with-openai) | 208 points | by [jyunwai](https://news.ycombinator.com/user?id=jyunwai) | [89 comments](https://news.ycombinator.com/item?id=38486394)

In a surprising turn of events, OpenAI, the artificial intelligence startup in which Microsoft had invested billions of dollars, fired its CEO and co-founder, Sam Altman. This news came as a shock to Microsoft CEO, Satya Nadella, who had a close working relationship with Altman and had just collaborated with OpenAI on a major AI rollout called the Office Copilots. The Copilots, powered by OpenAI's technology, were integrated into Microsoft's core productivity programs and allowed users to interact with software in a more natural and conversational way. However, behind the scenes, tensions had been brewing between Altman and OpenAI's board, with some members feeling that Altman had been manipulative and deceitful. This firing not only threatened the partnership between Microsoft and OpenAI but also ignited a larger debate about the responsible development and deployment of AI technology.

The discussion surrounding the firing of OpenAI CEO and co-founder Sam Altman on Hacker News revolves around several key points.  One commenter highlights a previous post by Helen Toner, who expressed concerns about the dangers of AI and suggested that Altman may have misled board members. Another commenter argues that people should not blindly trust those who claim to be advancing AI for good and points out the controversy surrounding OpenAI's board. There is also a discussion about Microsoft's involvement in OpenAI and the potential impact this firing may have on their partnership. Some express concern about the commercialization of AI and the spread of misinformation, while others argue that Altman's removal was necessary for the overall safety and control of AI. Other commenters bring up the larger issue of trust and accountability in AI development, highlighting the need for responsible decision-making and the potential risks of losing control as AI becomes more powerful.

Overall, the discussion reflects a mix of skepticism, concern, and support for both Altman and OpenAI's decision to remove him as CEO.

### OpenAI delays launch of custom GPT store until early 2024

#### [Submission URL](https://www.axios.com/2023/12/01/openai-delays-launch-custom-gpt-store-2024) | 98 points | by [cloudking](https://news.ycombinator.com/user?id=cloudking) | [63 comments](https://news.ycombinator.com/item?id=38491314)

OpenAI has announced a delay in the launch of its GPT store until early 2024, according to a memo seen by Axios. The GPT store, where people will be able to distribute custom versions of ChatGPT, was initially scheduled to open last month. The store was a highly anticipated feature announced by OpenAI at last month's DevDay conference. While custom GPTs can currently be shared through links, the store will allow for broader distribution. OpenAI also intends to share some of the revenue generated from ChatGPT Plus subscriptions with creators of popular GPTs. The company stated that it has some exciting updates for ChatGPT in the meantime. This news comes amid a tumultuous period for OpenAI, which recently saw CEO Sam Altman fired and then rehired within a week.

The discussion on Hacker News regarding the delayed launch of OpenAI's GPT store is varied. Some users express frustration with the current user experience of ChatGPT and question the company's focus on plugins and features instead of addressing fundamental issues. There is criticism of the complexity and lack of control over the frontend system, as well as the slow development and disconnectedness from scaling and improvements. One user mentions the potential usefulness of OpenAI's competitor, Cohere.

Others discuss the shortcomings of the default GPT model and propose that custom GPTs could solve some of these limitations. Some users mention the difficulties in creating and using custom GPTs, such as limited context and integration issues. The need for better version control and the preference for GPTs that can be trained on-source are also mentioned. The discussion also touches on OpenAI's business models and revenue-sharing plans. Some users express skepticism about OpenAI's monetization strategies and the impact on developers. A comparison is made to Google's Gemini project, suggesting that other AI startups are showing more coherent efforts. There are also comments about the CEO change at OpenAI and speculation about how it may impact the product. Some users suggest that OpenAI should consult domain experts to improve GPTs. A few users mention alternative tools and services, such as ArxivXplorer, for dealing with GPT-related challenges. The importance of feedback and open communication between OpenAI and its community is highlighted in one comment.

Overall, the discussions reflect a mix of frustrations, suggestions for improvement, skepticism about OpenAI's strategies, and explorations of alternative approaches.

### What Is Retrieval-Augmented Generation a.k.a. RAG?

#### [Submission URL](https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/) | 82 points | by [teleforce](https://news.ycombinator.com/user?id=teleforce) | [25 comments](https://news.ycombinator.com/item?id=38491251)

Today's top story on Hacker News is about a new technique in generative AI called retrieval-augmented generation (RAG). RAG is a process that enhances the accuracy and reliability of AI models by fetching facts from external sources, filling a gap in how large language models (LLMs) work. LLMs, like judges in a courtroom, can respond to a wide range of human queries, but they often require an assistant to do research and provide authoritative answers with cited sources. RAG serves as the court clerk of AI, connecting generative AI models to external resources and enabling them to cite sources, clear up ambiguity in user queries, and reduce the possibility of making wrong guesses. The technique also allows users to have conversations with data repositories, opening up new kinds of experiences and making applications for RAG multiple times the number of available datasets. Companies like AWS, IBM, Google, and Microsoft are already adopting RAG. NVIDIA has developed a reference architecture for retrieval-augmented generation to help users get started and has included it in their AI Enterprise software platform. The NVIDIA GH200 Grace Hopper Superchip is ideal for RAG workflows, as it provides massive amounts of memory and compute, resulting in a significant speedup. RAG doesn't require a data center and can be run on Windows PCs equipped with NVIDIA RTX GPUs, making it accessible to users even on their laptops. Overall, RAG represents the future of generative AI by improving accuracy, reliability, and user trust.

The discussion on the submission about retrieval-augmented generation (RAG) involves various viewpoints and topics. Some users question the accuracy and adequacy of RAG, suggesting that it may not be effective in generating high-quality answers without fine-tuning on relevant knowledge-rich question-and-answer pairs. Others point out that RAG compensates for the limitations of large language models (LLMs) by allowing them to approximate and retrieve information from external sources. The potential use of RAG in structuring unstructured text and solving problems is also discussed. Suggestions are made to explore coupling vector embeddings with knowledge graphs to provide informed answers. The effectiveness of using vector strings and different search types is highlighted.

There are references to related articles and resources that cover topics such as semantic search and getting started with vector-based retrievals. The controversy surrounding RAG and its potential impact on the AI industry is touched upon, as well as the limitations and challenges of integrating RAG into existing systems. One user mentions their plans to use RAG for document search and references GPT4All, a private project using GPT-2, and the RAG technique. Additionally, there are discussions about Huggingface blocking access to certain resources and AI models and AWS's efforts in utilizing RAG and other technologies.

### Are Open-Source Large Language Models Catching Up?

#### [Submission URL](https://arxiv.org/abs/2311.16989) | 331 points | by [rkwz](https://news.ycombinator.com/user?id=rkwz) | [207 comments](https://news.ycombinator.com/item?id=38481970)

The paper titled "ChatGPT's One-year Anniversary: Are Open-Source Large Language Models Catching up?" by Hailin Chen and 7 other authors explores the progress of open-source large language models (LLMs) in comparison to closed-source LLMs. The release of ChatGPT in late 2022 had a significant impact on the AI landscape, demonstrating the ability of LLMs to answer questions and follow instructions on a wide range of tasks. Since then, there has been increased interest and development in LLMs, with many new models emerging in academia and industry. While closed-source LLMs generally outperform their open-source counterparts, the progress of open-source LLMs has been rapid, with claims of achieving equal or even better performance on certain tasks. This has important implications for both research and business. The authors provide a comprehensive overview of the success of open-source LLMs on the first anniversary of ChatGPT, surveying all tasks where an open-source LLM has claimed to be on par or better than ChatGPT.

The discussion on this submission revolves around the restrictions and access to ChatGPT in China, particularly in relation to the Great Firewall (GFW). Some users share their experiences of trying to access ChatGPT from China and Hong Kong, with some claiming that it is blocked by the firewall. There is speculation that OpenAI's decision to restrict registration using Hong Kong phone numbers and credit cards is deliberate and might be influenced by government policies. Others discuss the possibility that OpenAI is trying to slow down China's development of AI technologies. The discussion also touches on the challenges faced by AI providers in complying with different countries' regulations, such as GDPR in Europe and data control laws in China. Some users mention Baidu ChatGPT as an alternative for Chinese speakers, while others express curiosity about the performance of ChatGPT in the Chinese language.

### 1960s chatbot ELIZA beat OpenAI's GPT-3.5 in a recent Turing test study

#### [Submission URL](https://arstechnica.com/information-technology/2023/12/real-humans-appeared-human-63-of-the-time-in-recent-turing-test-ai-study/) | 19 points | by [FrustratedMonky](https://news.ycombinator.com/user?id=FrustratedMonky) | [9 comments](https://news.ycombinator.com/item?id=38494102)

A recent preprint research paper titled "Does GPT-4 Pass the Turing Test?" conducted by researchers from UC San Diego compared OpenAI's GPT-4 AI language model to human participants, GPT-3.5, and the 1960s computer program ELIZA to determine which could convincingly pass as human. Surprisingly, the study found that human participants correctly identified other humans in only 63% of interactions, and ELIZA outperformed GPT-3.5. GPT-4 achieved a success rate of 41%, second only to actual humans. The study raises questions about using the Turing test as a benchmark for evaluating AI model performance and highlights the importance of linguistic style and socio-emotional traits in determining whether a participant believes they are interacting with a human or an AI model. However, it's worth noting that the study has limitations, including potential sample bias and the absence of peer review.

The discussion in the comments revolves around the surprising results of the study comparing GPT-4, GPT-3.5, and ELIZA in passing the Turing test. Some users express doubts about the relevance of the Turing test in judging AI's ability to mimic human conversation. They argue that current AI models like GPT-3.5 may not intentionally generate human-like responses, unlike ELIZA, a program developed in the 1960s. However, others disagree, noting that GPT-4 achieved a success rate of 41%, second only to actual humans, and the study highlights the importance of linguistic style and socio-emotional traits in making participants believe they are interacting with a human or an AI model. Additionally, there is speculation about the training and feedback process for GPT-4 and its potential improvements over GPT-3.5. One user also points out that ELIZA, despite being a relatively simple program, achieved a success rate of 27% in the study. Overall, there is interest and intrigue about the performance of GPT-4 and its comparison to both GPT-3.5 and ELIZA.

### Local councillors, unaware, approve law entirely written by AI in Brazil

#### [Submission URL](https://www.smh.com.au/world/south-america/local-councillors-unaware-approve-law-written-entirely-by-ai-20231201-p5eobi.html) | 16 points | by [flykespice](https://news.ycombinator.com/user?id=flykespice) | [5 comments](https://news.ycombinator.com/item?id=38492806)

In a surprising turn of events, local councillors in the southern city of Porto Alegre, Brazil, unknowingly approved legislation that was entirely written by artificial intelligence (AI). Councillor Ramiro Rosario enlisted OpenAI's chatbot ChatGPT to draft a proposal aimed at preventing the city from charging taxpayers for stolen water consumption meters. Rosario presented the proposal to his fellow council members without disclosing its AI origin, which led to unanimous approval and the subsequent enactment of the law. The incident has sparked concerns and debates about the role of AI in public policy, especially regarding the understanding and interpretation of complex legal principles. While some experts see potential in AI-powered chatbots like ChatGPT, others worry about the unintended consequences of relying on machines for tasks currently performed by humans.

The comments on this article cover a range of perspectives on the incident. One user points out that it is common for bills to be passed without politicians thoroughly reading or understanding them, so the fact that an AI wrote this legislation is not necessarily surprising. Another user suggests that the conflicting nature of the law could have been avoided if it had been written by humans who had the opportunity to discuss and amend it. They argue that it is difficult for a machine to account for all the nuances and concerns of the public. Another user raises concerns about the risks of allowing projects approved solely based on artificial intelligence, questioning the lack of oversight. Finally, a user suggests that people should write their own legislation if they are not satisfied with the current system.

### A reality bending mistake in Apple's computational photography

#### [Submission URL](https://appleinsider.com/articles/23/11/30/a-bride-to-be-discovers-a-reality-bending-mistake-in-apples-computational-photography) | 493 points | by [indrora](https://news.ycombinator.com/user?id=indrora) | [378 comments](https://news.ycombinator.com/item?id=38482085)

In a viral social media post, a UK woman shared a photo of herself trying on wedding dresses where her reflection didn't match in two different mirrors. It turns out that this illusion was not a glitch in the Matrix, but rather a mistake in Apple's computational photography pipeline. When taking a panoramic photo, the camera captures multiple images in quick succession and stitches them together. However, when a mirror is present, the algorithm mistakenly determines that the different moments shown in each mirror are the best reflections, resulting in multiple versions of the person. This phenomenon can be recreated on recent iPhones and many smartphones due to the limitations of computational photography dealing with mirrors. Younger generations have even used this effect to create silly images for social media.

The discussion surrounding the submission centers around the limitations of computational photography and its impact on capturing mirrors. Some users express their indifference to the issue, highlighting that no photograph is entirely pixel-perfect, and artistic interpretation is part of photography. Others raise concerns about Apple's decision to automatically determine the best reflections in mirror photos, arguing that it infringes on user control. The discussion further explores the complexities of computational photography and the various distortions it can introduce. Additionally, some users mention the challenges of publishing photos on social media platforms like Facebook due to their censorship policies. Overall, the conversation highlights the intersection of technology and photography, and the trade-offs associated with computational approaches.

### Valve Launches Official Steam Link PC VR Streaming App on Quest

#### [Submission URL](https://www.uploadvr.com/valve-steam-link-quest-steamvr-streaming/) | 34 points | by [MaximilianEmel](https://news.ycombinator.com/user?id=MaximilianEmel) | [19 comments](https://news.ycombinator.com/item?id=38481414)

Valve has launched an official Steam Link app for the Meta Quest, allowing users to wirelessly play SteamVR games on their Quest headsets. The app, available on the official Quest Store, enables streaming from a gaming PC over a home Wi-Fi network. Players can also enjoy non-VR Steam games on a virtual screen. While Quest headsets already have wireless PC VR streaming capabilities through features like Air Link, Valve's solution only requires the installation of Steam and SteamVR on a PC, offering a direct and unmediated connection to SteamVR.

The discussion revolves around different aspects of the Valve Steam Link app for the Meta Quest and its implications.

1. User dpc_01234 raises concerns about Meta (previously known as Facebook) selling headsets at a loss and relying on building a network effect to make their hardware platforms profitable. They also mention that Valve effectively hijacks the platform-building effort by providing access to their SteamDeck.
2. User bonton89 mentions that they are actively using the Quest headset and they believe that even if people jailbreak it, the majority of users still want to leverage Meta's content. FirmwareBurner responds, stating that jailbreaking a Quest natively supports APK sideloading and there is a popular secondary marketplace called SideQuest for paid VR apps that don't fit within Meta's rules.
3. Several users discuss the implications of jailbreaking and collecting VR telemetry. rtrchmln expresses a desire for jailbreaking to have more telemetry collection, while rcswprk comments on Meta potentially blacklisting sideloading APKs to push their own system.
4. FirmwareBurner points out that Valve effectively hijacks the platform-building effort by offering the Steam Link app, which allows users to stream PC games to their Quest headsets. They add that the point of the Quest is to be a self-contained gaming console, so Valve's app may not be necessary for Quest owners who primarily game on the headset.
5. Users mention various streaming solutions for VR gaming, including LinkAirLink, Virtual Desktop, ALVR, and Steam Link. They discuss the integration of Steam, Oculus PCVR games, and the simplicity of the Steam Link app.
6. Some users express their interest in trying the Steam Link app, while others discuss latency issues and motion sickness that can occur during VR streaming.

Overall, the discussion includes debates about the business strategies of Meta and Valve, the benefits and limitations of VR streaming, and user experiences with different VR gaming solutions.