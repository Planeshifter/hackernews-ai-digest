import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Mon Dec 09 2024 {{ 'date': '2024-12-09T17:12:26.930Z' }}

### Willow, Our Quantum Chip

#### [Submission URL](https://blog.google/technology/research/google-willow-quantum-chip/) | 1258 points | by [robflaherty](https://news.ycombinator.com/user?id=robflaherty) | [483 comments](https://news.ycombinator.com/item?id=42367649)

In a groundbreaking announcement, Google has unveiled its latest quantum chip, named Willow, which represents a pivotal advancement in quantum computing. This innovative chip addresses a major hurdle in the field: error correction. By exponentially reducing errors as it scales up, Willow enhances the reliability of quantum calculations, making it a noteworthy contender in the race for practical, large-scale quantum computers.

What sets Willow apart is its extraordinary computational ability; it completed a benchmark calculation in under five minutes—an astounding feat considering such a task would take a supercomputer an unfathomable 10 septillion years! This performance not only showcases Willow's capacity to tackle complex problems far beyond classical computing's reach but also illustrates the promise it holds for revolutionizing various industries including medicine, energy, and artificial intelligence.

Hartmut Neven, founder of Google Quantum AI, emphasized that this achievement is a significant milestone in their long-term goal of harnessing quantum mechanics for societal benefits. With the successful demonstration of real-time error correction and a scalable approach to qubit management, Willow is being hailed as a crucial step towards practical quantum applications that could reshape our future.

In the recent discussion surrounding Google’s announcement of its quantum chip, Willow, participants expressed a mix of excitement and caution regarding its implications for quantum computing and cryptography.

- **Understanding Quantum Computing**: One commenter shared their struggle to grasp quantum concepts, highlighting the exponential potential of quantum computers to perform computations that classical systems would take an impossibly long time to complete.

- **Progress in Quantum Error Correction**: Another contributor referenced existing research on physical versus logical qubits, emphasizing the immense progress made in error correction techniques, as demonstrated by Willow. There was optimism about the implications this has for breaking established cryptographic standards, particularly RSA encryption.

- **Cryptography Concerns**: Several comments focused on the security implications of quantum computing. The fear of quantum computers breaking conventional encryption methods led to discussions on the necessity of transitioning to quantum-resistant cryptographic systems. Participants noted that current encryption methods, such as AES, may be vulnerable to quantum attacks.

- **Realistic Expectations**: Some comments pointed out that despite significant advancements, practical applications of quantum computing can still be years away. Predictions were made regarding the scaling of qubits and the timeline for effective quantum computing, indicating that many in the community believe we are still a step away from widespread quantum dominance.

Overall, while the announcement of Willow is a significant step forward for quantum computing, it raises multiple questions about the future of cryptography and security, coupled with reminders of the complexity and unpredictability of technological advancements in this field.

### Trellis – 3D mesh generative model

#### [Submission URL](https://trellis3d.github.io/) | 388 points | by [tarr11](https://news.ycombinator.com/user?id=tarr11) | [70 comments](https://news.ycombinator.com/item?id=42369476)

A recent study introduces TRELLIS, an innovative method for creating high-quality 3D assets using a unique structured latent representation called Structured LATents (SLAT). This model harnesses the power of rectified flow transformers and is designed to generate versatile 3D outputs, such as Radiance Fields and meshes, from diverse input formats like text and images. 

TRELLIS seamlessly integrates sparse 3D grid structures with dense visual features, enabling it to capture intricate geometric and textural details of 3D objects. What's particularly exciting is its ability to provide local editing capabilities, allowing users to tweak targeted areas of a 3D model based on specific prompts. Moreover, the model is trained on an extensive dataset of 500,000 unique 3D assets, boasting up to 2 billion parameters, which significantly enhances its performance over existing methods.

The researchers are planning to release the code, models, and data, pushing the boundaries of 3D generation and opening up new possibilities for applications in 3D art design and beyond. This advancement could redefine how digital creators design and manipulate 3D content, making it more accessible and versatile than ever before.

The discussion around the TRELLIS submission on Hacker News has sparked a lively exchange about AI-generated content, particularly in the realm of 3D asset creation. Here are the key points and sentiments shared by users:

1. **Mixed Reactions to AI-Generated Content**: Users expressed a mix of amazement and concern regarding the implications of AI-generated 3D assets. Some feel that this technology could undermine the authenticity of handcrafted work, while others see it as an exciting development that enhances creativity and efficiency in design.

2. **Concerns Over Human-AI Competition**: There's a debate about the role of artists in a world where AI can generate high-quality models quickly. Some commenters express nostalgia for traditional artistry and worry that AI could detract from the value of human creativity and craftsmanship.

3. **Potential of TRELLIS**: Many participants highlighted the capabilities of TRELLIS, noting its potential to enable local editing of 3D models and create intricate designs from various input formats. This feature could significantly shift how digital creators work with 3D content.

4. **Future of Game Development**: The discussion also touched on how AI tools like TRELLIS could impact game development. Some users believe that these advancements may streamline the production process and enhance graphical fidelity in video games, though there remains skepticism about the quality and depth of AI-generated assets.

5. **Technical Discussions**: A technical discourse emerged regarding the underlying technologies, including neural networks and texture generation. Users shared their excitement for upcoming tools and resources that could stem from TRELLIS, emphasizing the potential for practical applications in games and animations.

6. **Emotional and Philosophical Reflections**: Finally, some comments reflected on the philosophical implications of using AI in creative industries, pondering what it means for artistic expression and individual creativity. There's a recognition of the complex relationship between human creators and AI tools, with discussions on the value of human touch in the creative process.

Overall, the conversation showcases a blend of enthusiasm and caution regarding the role of AI in transforming the landscape of 3D content creation.

### Task-specific LLM evals that do and don't work

#### [Submission URL](https://eugeneyan.com/writing/evals/) | 171 points | by [ZeljkoS](https://news.ycombinator.com/user?id=ZeljkoS) | [42 comments](https://news.ycombinator.com/item?id=42366481)

In a recent piece by Eugeneyan, the challenges of evaluating task-specific Large Language Models (LLMs) are dissected, highlighting the common pitfalls with off-the-shelf evaluations. Given that these evaluations often fail to accurately reflect application-specific performance, the author provides a roadmap for more effective assessment methods. 

Focusing on key tasks like classification, summarization, and translation, the article outlines practical metrics that can enhance evaluation precision. For example, common classification metrics include recall, precision, and various area-under-curve (AUC) measures, while summarization might utilize methods like consistency checks through Natural Language Inference (NLI) and relevance scoring via reward models.

The author emphasizes the importance of detailed metrics, such as TOXICITY measures and copyright checks, to capture nuanced model behavior. Notably, the article also mentions the value of human evaluation and encourages calibrating evaluation standards to balance potential benefits against inherent risks.

Overall, this insightful guide is crafted for both newcomers and seasoned professionals in machine learning, aiming to streamline the often-overlooked task of developing robust evaluation methodologies—ultimately freeing up time to focus on delivering impactful solutions to users.

In the discussion following the piece by Eugeneyan on evaluating task-specific Large Language Models (LLMs), several key themes emerged among the commenters:

1. **Challenges of Toxicity Classification**: Users expressed concerns about the effectiveness of toxicity models and their tendency to yield unintended labels. Some suggested that these models might confuse certain inputs or fail to catch nuanced meanings due to their simplistic binary classifications.

2. **Evaluation Metrics**: There was a consensus on the importance of adopting practical and specific metrics for evaluating models in tasks like classification, summarization, and translation. Commenters highlighted the necessity of incorporating not just standard metrics such as precision and recall, but also advanced measures including Natural Language Inference (NLI) for summarization and copyright checks.

3. **Human Evaluation**: Several commenters stressed that human evaluation is crucial for understanding model effectiveness, especially in capturing nuances that automated metrics might miss.

4. **Practical Experiences**: Users shared their experiences deploying LLMs for various applications, discussing strategies for improving model prompts and addressing issues related to contextual understanding.

5. **Structured Outputs**: There were mentions of using structured formats for outputs (like JSON) to better manage and interpret the responses from LLMs, pointing towards the need for organized interactions with complex models.

6. **Training and Instructional Models**: Some participants noted the discrepancies in training methodologies, indicating that the clarity in instructions provided to the models significantly impacts their performance.

Overall, the dialogue encapsulated a shared interest in refining evaluation techniques for LLMs, recognizing the complexities involved in assessing their performance accurately and effectively.

### Show HN: Ternary Computer System

#### [Submission URL](https://www.ternary-computing.com/history/CPU-History.html) | 126 points | by [claudio_mos](https://news.ycombinator.com/user?id=claudio_mos) | [32 comments](https://news.ycombinator.com/item?id=42368872)

In an exciting post on Hacker News, a software developer reveals their groundbreaking journey into the realm of ternary microprocessors. Unlike traditional binary processors that only handle two states (0 and 1), this innovative approach leverages three states—allowing each communication line to transmit significantly more information. The author has designed and tested a functional ternary CPU, complete with a RISC architecture and specialized instruction set. 

To test this unique processor, they built a system with trinary switches and visual outputs using LED indicators, engaging in hands-on programming and debugging. They've also pioneered a miniITX motherboard to facilitate easier programming and have begun developing a rudimentary operating system.

Looking ahead, the team aims to create silicon layouts using free production processes to bring this architecture to life, while actively seeking collaborators and funding to propel the project forward. This endeavor could redefine the landscape of microprocessor architecture and is a testament to the passion for exploring new horizons in tech!

In a recent discussion on Hacker News, the pioneering work on ternary microprocessors sparked a variety of technical insights and exchanges among community members. One user expressed enthusiasm about the potential of using ternary data for enhanced AI efficiency, indicating a keen interest in how these processors might reshape traditional computing architectures. 

Several participants questioned aspects of the design, particularly around the implications of negative voltage systems and ternary addressing. There was a significant focus on architecture specifics, with discussions around instruction sets, memory access patterns, and the nature of RISC-style designs adapted for ternary systems. Users also compared these innovations to conventional binary systems, with some suggesting that while the ternary approach may offer benefits, it might also complicate certain operations.

Moreover, the potential to develop conventional hardware with ternary logic was brought up, alongside suggestions to explore field-programmable gate arrays (FPGAs) for initial prototypes. The atmosphere remained constructive, with users offering support and requesting collaboration, displaying an eagerness to explore the intricacies of ternary computing further. As the discussion progressed, participants acknowledged the challenges posed by implementing ternary principles within existing binary frameworks, highlighting both the excitement and complexity of this new frontier in microprocessor design.

### AI company that made robots for children went bust and now the robots are dying

#### [Submission URL](https://aftermath.site/moxie-robot-ai-dying-llm-embodied) | 116 points | by [ceejayoz](https://news.ycombinator.com/user?id=ceejayoz) | [74 comments](https://news.ycombinator.com/item?id=42370826)

In a heart-wrenching turn of events, Embodied, the company behind Moxie, an AI robot designed to support social interaction in autistic children, has announced its closure due to financial difficulties and a sudden loss of funding. With the imminent shutdown, parents now face the difficult task of explaining to their children that their beloved Moxie, which sold for $799 and relied on cloud-based large language models to function, will soon become inoperable.

The adorable blue robot, designed to assist in language and social skills, will cease operations shortly. The company has informed users that no refunds can be given and that the robot is expected to stop working within days. Amidst this news, many parents are expressing deep emotions, sharing their grief on social media as they prepare for the loss of Moxie, likening the situation to watching a friend pass away.

Critics of the product are raising concerns about the implications of relying on AI for teaching social skills, especially for neuroatypical children. As the AI bubble faces scrutiny amid such closures, the future of devices like Moxie hangs in the balance, leaving many wondering about the ethical considerations of AI relationships in children's development.

The discussion surrounding the closure of Embodied and its Moxie robot has sparked a debate among commenters about the implications of relying on AI for teaching social skills to children, particularly those on the autism spectrum. Users express strong emotional responses, with some parents mourning the loss of a tool they viewed as crucial for their children's development. Critics highlight concerns about the ethical responsibility of companies to ensure the sustainability and functionality of such products, especially given that Moxie is reliant on cloud-based services which will soon cease.

Some commenters mention the fiduciary duties of the founders and the responsibility they have towards their investors and users. Others debate the differences between server-based services and products requiring hardware, pointing out that many AI services do not maintain functionality if cloud support is withdrawn. The conversation also touches on broader societal questions regarding privacy and the implications of introducing technology like Moxie into the lives of children.

In summary, while some participants reflect on personal experiences regarding the technology's impact on their children's lives, the wider discussion reveals a deep concern about the ethical responsibilities of AI firms, the long-term sustainability of technology, and the implications for children's social interactions and development.

---

## AI Submissions for Sun Dec 08 2024 {{ 'date': '2024-12-08T17:11:45.060Z' }}

### Show HN: Replace CAPTCHAs with WebAuthn passkeys for bot prevention

#### [Submission URL](https://github.com/singlr-ai/nocaptcha) | 57 points | by [uday_singlr](https://news.ycombinator.com/user?id=uday_singlr) | [29 comments](https://news.ycombinator.com/item?id=42359067)

In an innovative stride towards enhancing online user experience, the GitHub project "NoCAPTCHA" has emerged, aiming to replace the frustrating traditional CAPTCHA systems with a more user-friendly solution: single-use, disposable passkeys. This approach promises to effectively thwart bots while minimizing inconvenience for real users.

Built using Java with Helidon and a slick JavaScript frontend leveraging Vite, NoCAPTCHA is designed for simplicity with a clear focus on functionality. Developers can easily set up their local environments to contribute, as the project welcomes improvements in both the backend passkey verification system and the frontend user interface.

For those eager to see the project in action, a hosted demo is available, giving users a taste of the smoother verification experience that NoCAPTCHA offers. With 46 stars already, this project could very well mark a significant shift in online security measures!

The discussion around the "NoCAPTCHA" project on Hacker News is lively and diverse, with participants sharing various insights and concerns about the evolution of authentication systems. Below are the key points raised:

1. **Concerns About Security**: Some commenters express skepticism about traditional security frameworks, highlighting issues with hardware-backed security, Trustworthy client systems, and the risk of centralized control over digital identities. Users fear inadequate protection against bot attacks might lead to vulnerabilities.

2. **User Experience**: A few participants discuss the usability of passkeys, comparing software implementations like Bitwarden and hardware solutions such as YubiKeys. There are mixed feelings about the user experience with these systems, particularly regarding key management.

3. **Technicalities and Advancements**: The discussion touches on the technical aspects of WebAuthn and protocols used for passkey integration. Some users mention their experiences with setting up their environments and the complexities involved, while others call for clearer documentation to facilitate contributions to the project.

4. **Innovation vs. Privacy**: There's a nuanced debate on the balance between innovating security measures and maintaining user privacy. Some participants raise existential concerns about government-backed digital ID systems and how they could lead to surveillance and loss of control over personal data.

5. **Broader Context**: A few comments also reference other relevant discussions and protocols in cybersecurity, including comparisons to broader trends in online identity verification, such as those discussed in related Hacker News threads.

Overall, the comments illustrate a community engaging critically with emerging ideas in digital security, emphasizing both the potential improvements that projects like NoCAPTCHA can bring as well as the challenges and implications they carry.

### Zizmor would have caught the Ultralytics workflow vulnerability

#### [Submission URL](https://blog.yossarian.net/2024/12/06/zizmor-ultralytics-injection) | 77 points | by [campuscodi](https://news.ycombinator.com/user?id=campuscodi) | [21 comments](https://news.ycombinator.com/item?id=42356345)

In a recent and alarming incident, the highly-utilized machine learning package Ultralytics suffered a severe security breach that led to malicious releases on PyPI. The attack unfolded when a compromised Continuous Integration (CI) system allowed an attacker to create a malicious pull request, which exploited a vulnerable GitHub Actions workflow (specifically, the dangerous `pull_request_target` trigger). This vulnerability enabled the execution of arbitrary code, allowing the attacker to inject harmful scripts and manipulate subsequent releases.

Initially, a rogue release (v8.3.41) was found to contain a crypto miner, which was quickly deleted. However, the attack persisted with follow-up malicious releases (v8.3.45 and v8.3.46) appearing in quick succession, provoking serious concern within the community. Users were alerted to the danger, and affected releases were promptly scrubbed from PyPI.

An insightful analysis reveals that the exploitation was facilitated through poorly managed workflow conditions and lack of stringent deployment protocols, raising the question of how to strengthen security in open-source projects. This incident highlights the critical need for enhanced vigilance regarding CI/CD security practices and the proper handling of secrets within workflows to prevent similar attacks in the future. As investigations continue, the narrative that unfolds serves as a crucial learning experience for developers and maintainers across the open-source landscape.

The discussion on Hacker News revolves around the recent security breach of the Ultralytics machine learning library on PyPI, which resulted from a vulnerability in the GitHub Actions CI/CD workflow. Users expressed frustration over the configuration practices around GitHub Actions, noting that improper handling of pull request triggers can expose projects to risks. Several commenters stressed the importance of implementing robust security measures, especially as CI/CD tools and workflows continue to evolve and become more common.

Participants debated the responsibility of developers to manage security in open source projects and the potential demand for more stringent protocols in maintaining CI/CD environments. There's a general agreement that the incident serves as a crucial learning opportunity, prompting the community to reflect on best practices for safeguarding code repositories. Some users cited personal experiences dealing with similar vulnerabilities and emphasized the need for transparency and structured testing when deploying code.

Commenters also referenced "Dr. Zizmor," possibly a notable figure known for contributions or insights in cybersecurity. The conversation included various technical references and suggestions to improve security practices like restricting CI configurations and better handling of secrets in environments. Overall, the discussion highlighted a critical evaluation of the existing security framework within GitHub Actions and a call for more proactive measures across the open-source community.

### The GPT era is already ending

#### [Submission URL](https://www.theatlantic.com/technology/archive/2024/12/openai-o1-reasoning-models/680906/) | 48 points | by [bergie](https://news.ycombinator.com/user?id=bergie) | [28 comments](https://news.ycombinator.com/item?id=42360963)

OpenAI has recently unveiled its most advanced generative AI model to date, referred to as o1, boasting enhanced capabilities that bring it closer to human-like reasoning. This new model marks a significant turning point for the company, with CEO Sam Altman declaring it the beginning of what he calls the "Intelligence Age," where AI is positioned to tackle global challenges such as climate change and space exploration.

Despite critics likening the excitement around OpenAI's offerings to marketing hype, independent researchers are noting that o1 does indeed represent a substantial step forward from previous iterations like GPT-4o. The uniqueness of o1 is attributed to its ability to engage in reasoning, a defining trait of human intelligence that could potentially set it apart in a rapidly homogenizing market where AI products from various companies are becoming increasingly similar.

OpenAI seems intent on distinguishing itself amid a backdrop of increasing scrutiny and competition, particularly as conversations around improving AI models grow more complex. Both internal leadership shifts and a clear focus on o1 signal the company's commitment to advancing the realm of generative AI, potentially paving the way to a new era of synthetic intelligence characterized by advanced reasoning capabilities rather than just predictive text generation.

With the launch of o1, OpenAI is challenging itself and its competitors to demonstrate the real-world effectiveness of this technology, urging a reevaluation of what generative AI can achieve beyond its current applications. As researchers and industry insiders react to this announcement, the implications for the future of AI could be profound, possibly reshaping how technology interacts with complex human challenges.

The discussion surrounding OpenAI's launch of its new generative AI model, o1, is lively and varied, with participants expressing differing opinions on its potential and implications for the AI landscape. Many commenters note that while o1 represents a significant advancement from models like GPT-4o, there are lingering concerns about whether it truly achieves a level of reasoning akin to human thought.

Several users critique the excitement surrounding o1 as potentially undue hype, suggesting that while the model may demonstrate improved capabilities, the claims made about its revolutionary nature should be approached cautiously. There's a recognition that o1 aims to differentiate itself in the saturated AI market, but skepticism remains about its practical applications and long-term viability.

Commenters express concern that despite advancements, current AI models, including o1, may still struggle with deeper reasoning tasks, and that the excitement may overshadow ongoing limitations inherent in large language models (LLMs). Some participants advocate for a more detailed understanding of o1's technical aspects to better grasp its capabilities.

The conversation also touches on broader themes such as the role of AI in addressing complex global issues, the current state of AI research, and the ethical implications of deploying more sophisticated models. Overall, the comments reflect a mix of enthusiasm for potential breakthroughs alongside caution regarding the truthful portrayal of AI advancements.

### Deepfakes weaponised to target Pakistan's women leaders

#### [Submission URL](https://www.france24.com/en/live-news/20241203-deepfakes-weaponised-to-target-pakistan-s-women-leaders) | 73 points | by [mostcallmeyt](https://news.ycombinator.com/user?id=mostcallmeyt) | [30 comments](https://news.ycombinator.com/item?id=42353936)

In a troubling trend in Pakistan, deepfake technology is being exploited to target and discredit female politicians, such as Azma Bukhari, the information minister of Punjab. Bukhari was devastated by a counterfeit video that sexualized her image, rapidly spreading across social media and damaging her reputation. This phenomenon highlights how digital manipulation can disproportionately harm women in a conservative society where personal honor is intricately tied to reputation.

As internet access surges in the country, the lack of media literacy makes women, especially in public roles, vulnerable to these malicious attacks. In stark contrast to their male counterparts, who often face political accusations centered on ideology or corruption, female politicians are often subjected to attacks on their moral integrity and personal lives.

Deepfakes have been utilized in the recent political landscape, including during the campaign of jailed former prime minister Imran Khan, demonstrating their potential to influence narratives. Activists and experts warn that the use of deepfakes poses serious repercussions for women, often leading to threats based on perceived dishonor.

Despite existing legislation aimed at combatting online harassment, critics argue that the laws need to be strengthened and enforced more effectively. As women like Bukhari seek justice through legal avenues, calls for both better protective measures and improved public awareness about digital misinformation continue to grow. The situation underscores the urgent need to confront the misuse of technology against women in politics and ensure a safer environment for their participation in the public sphere.

In a recent discussion on Hacker News regarding the troubling use of deepfake technology against female politicians in Pakistan, several key points emerged. Users highlighted that media literacy in Pakistan is critically low, exacerbating the exploitation of deepfake technology to manipulate public perception, especially against women in politics. Comments underscored a societal double standard where female politicians face attacks on their moral integrity rather than political ideology, contrasting sharply with their male counterparts.

Some commenters pointed out that deepfakes are part of a broader socio-political manipulation that includes various forms of misinformation, raising concerns over the implications for women's safety and rights in a conservative society. Others mentioned the existence of legislation against online harassment, but emphasized that these laws require stronger enforcement and adaptation to address the evolving threats posed by digital technologies.

The discussion also referenced the political context in Pakistan, suggesting that the government may be using deepfakes for propaganda purposes in a manner similar to China's Great Firewall. Overall, participants expressed a strong need for improved media literacy and protective measures to counteract the harmful effects of digital manipulation on women's public lives.

---

## AI Submissions for Sat Dec 07 2024 {{ 'date': '2024-12-07T17:10:43.977Z' }}

### Show HN: Countless.dev – A website to compare every AI model: LLMs, TTSs, STTs

### Structured Outputs with Ollama

#### [Submission URL](https://ollama.com/blog/structured-outputs) | 253 points | by [Patrick_Devine](https://news.ycombinator.com/user?id=Patrick_Devine) | [67 comments](https://news.ycombinator.com/item?id=42346344)

Ollama has announced a significant enhancement: support for structured outputs, allowing users to define model responses using JSON schemas. This upgrade targets improved reliability and consistency compared to traditional JSON modes. 

With updated Python and JavaScript libraries, developers can now easily constrain outputs for various purposes—including data extraction from documents, image analysis, and structured storytelling. For instance, when querying about countries or pets, users can specify the output structure, ensuring the response matches the defined schema.

For those eager to dive in, upgrading to the latest version of Ollama is straightforward:
- Python users can run: `pip install -U ollama`
- JavaScript developers can execute: `npm i ollama`

The structured outputs are versatile. They allow for structured data extraction from text, and even image descriptions using vision models. Additionally, compatibility with OpenAI's API enhances its accessibility.

Overall, this update opens up new possibilities for data handling and response generation, making it a noteworthy advancement for developers leveraging Ollama for their projects.

Ollama has announced a major update that introduces support for structured outputs, enabling users to define model responses using JSON schemas. This enhancement aims to improve the reliability and consistency of outputs over traditional JSON formats. The updated libraries for Python and JavaScript provide developers the ability to constrain responses for diverse applications, ranging from data extraction to structured storytelling.

**Key Highlights from Comments:**

1. **Usefulness of the Update**: Users expressed excitement about the potential of structured outputs for generating consistent data formats, such as CSV for data extraction. However, some raised concerns about the complexity involved when using models like Ollama to generate responses in these formats.

2. **Concerns about Quality**: Several comments noted the trade-offs between specifying constraints and the quality of output. Users highlighted how certain prompts might lead to inconsistent results, with smaller models being less reliable in generating structured data.

3. **Technical Insights**: Discussions included the mechanics of LLMs (large language models) and how they generate outputs based on token predictions. A few users shared technical details about integrating JSON schemas with structured prompts, emphasizing the challenge of ensuring coherence in responses.

4. **Real-World Applications**: The community discussed various scenarios where structured outputs could be effectively utilized, such as in structured data extraction from documents and enhanced querying systems.

5. **Performance Variability**: Users commented on the variability in performance when using different models, indicating that the size and training of a model could heavily influence output quality. Concerns regarding the propensity for LLMs to generate nonsensical responses in structured formats were also raised.

6. **Comparative Feedback**: Some users compared Ollama's capabilities with other LLMs, exploring how performance could be optimized depending on model size and prompt design. There was a consensus that experimentation would be crucial in leveraging these new features effectively.

Overall, the community seems optimistic about Ollama's new structured outputs, though there are valid concerns regarding consistency and the complexity of output formats that need to be addressed.

### Ultralytics AI model hijacked to infect thousands with cryptominer

#### [Submission URL](https://www.bleepingcomputer.com/news/security/ultralytics-ai-model-hijacked-to-infect-thousands-with-cryptominer/) | 82 points | by [sandwichsphinx](https://news.ycombinator.com/user?id=sandwichsphinx) | [30 comments](https://news.ycombinator.com/item?id=42351722)

In a significant supply chain attack, the popular Ultralytics YOLO11 AI model was compromised, leading to the deployment of cryptominers on users' devices. The affected versions, 8.3.41 and 8.3.42, were pulled from the Python Package Index (PyPI) after users reported unexpected installations of the XMRig Miner, which connects to a mining pool for cryptocurrency.

Ultralytics, renowned for its capabilities in object detection and widely used in various projects, confirmed the malicious code was introduced through two suspicious pull requests. Although these versions have been replaced with a clean update (8.3.43), the incident has raised concerns within the community regarding potential vulnerabilities in Ultralytics' build process.

Users are advised to perform full system scans if they installed the compromised versions, as ongoing investigations into further malicious releases continue. The company's founder reassured users that a thorough security audit is underway to prevent future breaches. As scrutiny of the event unfolds, the implications of this attack serve as a stark reminder of the persistent risks in open-source ecosystems.

The discussion on Hacker News regarding the supply chain attack on the Ultralytics YOLO11 AI model reveals several key points and concerns from the community:

1. **Vulnerability Awareness**: Many users expressed concerns about the vulnerabilities within Ultralytics' repository management and security practices. The community debated the adequacy of transparency and oversight, particularly around how the malicious code was introduced through pull requests.

2. **Response to the Incident**: There were discussions about the role of the company's leadership, with some users emphasizing the need for better communication from Ultralytics regarding their security measures. The implication is that better oversight could prevent such incidents in the future.

3. **Impact on Users**: Several comments highlighted the potential repercussions for users, including the need for thorough system scans of affected versions and the implications of using compromised software, particularly in critical or sensitive applications.

4. **Open Source Risks**: The event reignited a broader discussion about the inherent risks associated with open-source software, suggesting a need for stricter practices and tools to mitigate such vulnerabilities.

5. **Technical Issues**: There were technical critiques of how GitHub manages pull requests and branch naming, with suggestions that the platform’s current workflows may have contributed to the issue. Users pointed out the potential for malicious code being integrated without adequate checks.

In conclusion, the community is collectively calling for increased vigilance and improvements in the security processes around open-source projects, particularly those that are widely used and trusted in the tech ecosystem.

### Japanese scientists were pioneers of AI; they're being written out of history

#### [Submission URL](https://theconversation.com/japanese-scientists-were-pioneers-of-ai-yet-theyre-being-written-out-of-its-history-243762) | 91 points | by [YeGoblynQueenne](https://news.ycombinator.com/user?id=YeGoblynQueenne) | [16 comments](https://news.ycombinator.com/item?id=42350768)

In the wake of John Hopfield and Geoffrey Hinton being awarded the Nobel Prize in Physics, the discourse surrounding artificial intelligence has ignited a mixture of praise and frustration, particularly in Japan. Editorialists and members of the Japanese Neural Network Society have voiced concerns over the underrepresentation of pioneering Japanese researchers who laid the groundwork for neural network technology, notably Shun’ichi Amari and Kunihiko Fukushima.

Amari's innovative work in the 1960s, including methods of adaptive pattern classification and a learning algorithm analogous to Hopfield's associative memory, set crucial foundations for neural networks. Meanwhile, Fukushima developed the world's first multilayer convolutional neural network, the Convolutional Neural Network (CNN), which underpins much of today's deep learning advancements.

The debate within the AI community centers around recognizing the global contributions to the field, especially as historical narratives often skew towards a North American perspective. This is crucial as AI continues to shape society, highlighting the need for a more inclusive narrative that accommodates vital contributions from researchers across various backgrounds and regions.

An ongoing oral history project led by researchers from Kyoto University aims to explore Fukushima's background and the context of his work, which originally sought to mimic human visual processing rather than solely focusing on AI as it's known today. The project reveals that early AI research in Japan was deeply intertwined with psychological studies, marking a stark contrast to the statistical methods favored by many American contemporaries.

As the discourse on the evolution and future of AI progresses, acknowledging and incorporating these foundational contributions from Japanese researchers will be essential to foster a comprehensive understanding of the technology's origins and implications.

The discussion on Hacker News reflects a deep concern regarding the recognition of global contributions to the field of artificial intelligence (AI), particularly highlighting Japanese researchers who were pivotal in developing neural network technologies. 

Users express their appreciation for the foundational work of Japanese scientists like Shun'ichi Amari and Kunihiko Fukushima, especially in light of the recent Nobel Prize awarded to John Hopfield and Geoffrey Hinton. Some comments point out that the narratives around such achievements often overlook the contributions from non-Western researchers. There's a consensus that the historical narrative surrounding AI has been increasingly narrow, primarily showcasing contributions from North American researchers while sidelining crucial work from other countries, including Japan, Finland, and others.

Several participants suggest that credit should be more evenly distributed and acknowledge that many groundbreaking advancements stemmed from diverse backgrounds. The conversation also references the need for a broader understanding of AI's historical context, as illustrated by a linked post detailing the evolution of modern AI and deep learning.

In summary, the thread underscores a desire for greater recognition and inclusion of diverse contributions in the history of AI development, advocating for a more equitable representation in future discourses.

### The FBI now recommends choosing a secret password to thwart AI voice clones

#### [Submission URL](https://arstechnica.com/ai/2024/12/your-ai-clone-could-target-your-family-but-theres-a-simple-defense/) | 64 points | by [perihelions](https://news.ycombinator.com/user?id=perihelions) | [23 comments](https://news.ycombinator.com/item?id=42348946)

In a recent advisory, the FBI has warned Americans about the rising threat of AI-driven voice-cloning scams, urging families to establish secret words or phrases to verify identities during unexpected calls. As criminal organizations increasingly exploit generative AI to create convincing audio impersonations, the FBI recommends that family members use unique phrases—like "The sparrow flies at midnight"—to ensure they're communicating with a real loved one. 

This public service announcement highlights how easy it has become to generate fake voices using AI, particularly from publicly available recordings. Besides voice scams, the FBI also outlines how these technologies are being misused to create fake profile pictures, identification documents, and highly believable chatbots. 

As a countermeasure, the FBI advises minimizing the public availability of personal images and voice recordings by keeping social media accounts private. The concept of using a 'secret word' for identity verification has gained traction since first being suggested by AI developer Asara Near in March 2023, spotlighting a simple yet effective approach to combatting evolving digital fraud.

The Hacker News discussion centers around the FBI's advisory on AI-driven voice-cloning scams and the proposed solution of establishing secret verification phrases among family members. 

Key points from the discussion include:
- Some users argue about the effectiveness of standard two-factor authentication (2FA) in relation to the threats posed by sophisticated voice cloning technologies.
- Concerns were raised about the security of personal devices and the need for hardware-level authentication, particularly in the context of family communication, where trust is paramount.
- Several participants expressed skepticism about the practical use of a secret phrase, discussing the nuances of digital communication methods (e.g., SMS, VoIP) and the potential vulnerabilities involved.
- The conversation touched upon childhood scenarios where parents or guardians might need to verify a caller's identity when unexpected calls come from children, emphasizing the need for precautions.
- The dialogue indicates a blend of understanding and frustration regarding the implications of digital security and the challenges posed by evolving AI technologies.

Overall, while the secret verification phrase concept is recognized as a simple countermeasure, many commenters highlight the complexities of digital security in real-world applications.

### ChatGPT Is Terrible at Checking Its Own Code

#### [Submission URL](https://spectrum.ieee.org/chatgpt-checking-sucks) | 19 points | by [pseudolus](https://news.ycombinator.com/user?id=pseudolus) | [3 comments](https://news.ycombinator.com/item?id=42350544)

In a recent study published in IEEE Transactions on Software Engineering, researchers from Zhejiang University explored ChatGPT's ability to scrutinize its own code for errors, vulnerabilities, and repairs. The findings reveal that while ChatGPT can generate functional code with a success rate of about 57%, it often overlooks its mistakes—misclassifying incorrect code as correct 39% of the time, and failing to recognize vulnerabilities 25% of the time.

Interestingly, the study showed that by reframing prompts from direct queries to guiding questions—where ChatGPT was asked to agree or disagree with statements regarding its code's compliance—the AI significantly improved in self-assessment. This new approach led to a 25% increase in identifying code errors, a 69% boost in security vulnerability detection, and a 33% improvement in recognizing unsuccessful repairs.

These findings underscore the importance of refining AI tools like ChatGPT for reliable software development, as the tool's current overconfidence could pose serious risks in coding practices. Researchers advocate for enhanced prompting techniques to elevate the quality and security of AI-generated code, reflecting the growing reliance on AI in programming tasks.

In the discussion on Hacker News, users commented on the findings regarding ChatGPT's code generation capabilities. One user referenced a study about GPT-3.5 and its limitations, noting that the results were disappointing. Another user expressed frustration with ChatGPT's performance in code generation, contrasting it unfavorably with another AI model, Claude. A third user offered a brief response that could imply agreement or acknowledgment of the previous sentiments. Overall, the conversation reflects skepticism about ChatGPT's reliability in generating correct code.