import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Tue Feb 13 2024 {{ 'date': '2024-02-13T17:10:16.648Z' }}

### Memory and new controls for ChatGPT

#### [Submission URL](https://openai.com/blog/memory-and-new-controls-for-chatgpt) | 434 points | by [Josely](https://news.ycombinator.com/user?id=Josely) | [248 comments](https://news.ycombinator.com/item?id=39360724)

OpenAI is testing a new feature for ChatGPT that allows the AI to remember information from previous conversations, making future interactions more helpful. Users have control over ChatGPT's memory and can explicitly tell it to remember or forget certain things. They can also view and delete specific memories or clear all memories. This feature is being rolled out to a small group of free and Plus users for testing, with plans for a broader rollout in the future. In addition, OpenAI is also working on implementing memory for GPTs in general, allowing them to remember user preferences and tailor their responses accordingly.

The discussion on the submission revolves around the topic of lazy coding in ChatGPT and potential improvements that can be made. Some users argue that lazy coding leads to decreased coding quality, while others point out that lazy coding can be effective and efficient in certain situations. One user mentions the difficulty of filtering out irrelevant code in Rust parsing, while another user suggests using GitHub Copilot for generating code. There is also a discussion about GPT-4 Turbo's behavior and the need for refactoring tasks. Some users express concerns about OpenAI's censorship and the potential misuse of knowledge generated by AI models. Overall, the discussion highlights different perspectives on the efficiency and practicality of lazy coding and raises questions about responsible AI use and censorship.

### Nvidia's Chat with RTX is an AI chatbot that runs locally on your PC

#### [Submission URL](https://www.theverge.com/2024/2/13/24071645/nvidia-ai-chatbot-chat-with-rtx-tech-demo-hands-on) | 249 points | by [nickthegreek](https://news.ycombinator.com/user?id=nickthegreek) | [137 comments](https://news.ycombinator.com/item?id=39357900)

Nvidia has released an early version of Chat with RTX, an AI chatbot that runs locally on your PC. The app allows users to feed it YouTube videos and documents to create summaries and obtain relevant answers based on their own data. While the app is still in its early stages, it shows promise for data research purposes, particularly for journalists and individuals who work with large amounts of documents. Chat with RTX requires an NVIDIA RTX 30- or 40-series GPU with at least 8GB of VRAM. Users can search through video transcripts, summarize videos, and analyze local documents with the chatbot. While there are some bugs and limitations to be resolved, this AI chatbot demonstrates the potential of locally-run AI models on personal computers.

The discussion on the Hacker News submission revolves around the implementation and potential limitations of the Chat with RTX AI chatbot released by Nvidia. Here are the main points discussed:
1. Implementation: The chatbot is based on the TensorRT-LLM framework and requires an NVIDIA RTX 30- or 40-series GPU with at least 8GB of VRAM. Users can feed YouTube videos and documents to obtain summaries and relevant answers based on their own data. Some users have shared GitHub repositories and installation instructions for the chatbot.
2. Performance: Users have shared their experiences with the chatbot's performance. One user mentions that using it in conjunction with other AI models like Triton Inference Server has led to significant performance improvements. However, there are discussions about the limitations and complexities of running the chatbot, such as the requirement for large GPU memory.
3. Comparisons with Dr. Sbaitso: Some users bring up the resemblance of the chatbot to Dr. Sbaitso, a text-to-speech program from the 90s. They share anecdotes and nostalgic experiences related to the old program.
4. Local Language Models (LLMs): The discussion extends to the broader topic of locally-run language models. Users discuss the potential benefits and drawbacks of using LLMs, mentioning the need for technical knowledge and possible harmful effects of AI.
5. Simplified Solutions: Some users express confusion about the implementation details and suggest that simpler solutions should be made available. Others point out that various companies, such as OpenAI, Microsoft, and Google, are working on similar projects, and the complexity is due to the technical nature of the topic.

Overall, the discussion highlights both the potential of locally-run AI models for data research purposes and the complexities associated with their implementation and usage.

### Smart terminals: Personal computing’s true origin? (2023)

#### [Submission URL](https://thehistoryofhowweplay.wordpress.com/2023/10/23/smart-terminals-personal-computings-true-origin/) | 65 points | by [rbanffy](https://news.ycombinator.com/user?id=rbanffy) | [14 comments](https://news.ycombinator.com/item?id=39359307)

Today on Hacker News, we have an interesting article from the historyofhowweplay blog about the evolution of computer monitors. The author delves into the significance of terminals as they played a crucial role in the development of computer technology. Initially starting as text-based teletypes, terminals became popular in the 1950s and made programming much easier. The adoption of the ASCII computer text standard and the widespread use of timesharing with the Dartmouth model led to the demand for CRT-based terminals and the birth of the computer monitor industry. The article also mentions the development of graphical terminals for applications like Computer Aided Design (CAD) and the various experiments conducted to explore the possibilities of raster and vector graphics displays. Eventually, designers began to conceive of terminals with their own "brain" to bypass the need for constant computer interaction and unstable connectivity. The article points out the importance of the miniaturization of computer components in driving the development of standalone terminal systems. Overall, it's an intriguing read that delves into the forgotten history of computer monitors and their impact on personal computing.

The discussion on the article about the evolution of computer monitors goes in several directions. 
One user, ChuckMcM, points out the shift from centralized computing to distributed computing and how terminals played a role in each stage. They mention the transition from mainframe-based terminals to minicomputers with multiple servers, and then to personal computers that combined terminals and computing power. They also mention thin servers, Citrix-style clients, and web browsers as alternative ways to implement terminal-like functionality.
Another user, rbnffy, talks about alternative paths in computing, mentioning DEC's Gigi and other terminal models like Tek ReGIS. They also discuss how personal computers and modems worked similarly to terminals in the past.
The discussion then shifts to the similarities between smart phones and modern terminals. rbnffy notes that smart phones are essentially running local software that acts as a terminal to modern web browsers.
kjs3 weighs in by discussing specific terminal models like ATT 3b2 and BLIT, highlighting their features and capabilities. They also mention the existence of a similar but different terminal called BitGraph, as well as the Tektronix 4100 and 4200 series, which were impressive machines for graphics work.
Animats brings up the history of shared-logic word processors and dumb terminals in the context of IBM PCs and monitors.
SomeoneFromCA mentions that the Apple 1 was a representative transitional product from the era of dumb terminals to intelligent computers.
rbnffy adds that the sophistication of terminals did not help small computers running dedicated terminal firmware, as they were eventually overtaken by cheaper desktop computers running terminal software.

Finally, a user named aaron695 makes a one-word comment: "dd," which is unclear in its context.

### Mozilla downsizes as it refocuses on Firefox and AI

#### [Submission URL](https://techcrunch.com/2024/02/13/mozilla-downsizes-as-it-refocuses-on-firefox-and-ai-read-the-memo/) | 176 points | by [awkwardpotato](https://news.ycombinator.com/user?id=awkwardpotato) | [165 comments](https://news.ycombinator.com/item?id=39362481)

Mozilla, the organization behind the Firefox browser, is undergoing major changes to its product strategy. It plans to scale back investment in various products, including its VPN, Relay, and Online Footprint Scrubber, and shut down Hubs, its 3D virtual world launched in 2018. Approximately 60 employees will be affected by the layoffs. Mozilla aims to refocus on Firefox and bring "trustworthy AI into Firefox." The company will bring together teams working on Pocket, Content, and AI/ML to achieve this goal. These changes suggest a shift towards prioritizing Firefox and addressing criticism of diversifying its product portfolio.

The discussion on this submission revolves around various aspects of Mozilla's product strategy changes. Here are some key points:

- Some users are expressing skepticism about the impact of Mozilla's refocus on Firefox and its plans to bring trustworthy AI into the browser.
- There is a discussion on the market share of Firefox, particularly in relation to Chrome and Safari. It is pointed out that while Firefox's market share is relatively low in desktop Linux, it has a stronger presence in Germany.
- Users debate the significance of Mozilla's decision to scale back investment in products such as VPN and Relay, with some suggesting that these projects were not generating enough profit.
- The potential implications of Mozilla's reliance on Google for revenue, particularly from search deals, are discussed, with some expressing concern about the impact on the company's independence.
- There is also a mention of the financial challenges faced by Mozilla and the need for the organization to prioritize revenue-generating projects.
- Some users express support for Mozilla's VPN and mention alternatives such as Mullvad.
- The discussion touches on the importance of open-source alternatives and the potential for Mozilla to collaborate with projects like LibreWolf.

### Court Dismisses Authors' Copyright Infringement Claims Against OpenAI

#### [Submission URL](https://torrentfreak.com/court-dismisses-authors-copyright-infringement-claims-against-openai-240213/) | 35 points | by [isaacfrond](https://news.ycombinator.com/user?id=isaacfrond) | [9 comments](https://news.ycombinator.com/item?id=39357210)

In a recent court case, OpenAI successfully had the copyright infringement claims filed against them by two authors dismissed. The authors, Paul Tremblay and Mona Awad, accused OpenAI of using their books without permission or compensation to train its AI models. OpenAI argued that using books to train AI does not constitute copyright infringement, and the court largely agreed. The court also dismissed claims that OpenAI violated the Digital Millennium Copyright Act (DMCA) by altering copyright management information. However, the authors still have the opportunity to file an amended complaint, and the direct copyright infringement claim against OpenAI will proceed. Many other AI copyright lawsuits are ongoing.

The discussion on Hacker News revolves around various aspects of the court case between OpenAI and authors Paul Tremblay and Mona Awad. Some key points discussed include:
- One user mentions that OpenAI's defense likely focused on the argument that using books to train AI models does not constitute copyright infringement. Another user adds that OpenAI's successful defense could set a precedent for similar cases in the future.
- There is a comment expressing confusion about the judge's decision, stating that the court case is still in its early stages and the legal battle is just beginning.
- A user highlights that the article's title is misleading, as it suggests the claims were dismissed entirely, while in reality, only specific claims were dismissed. They mention that it would be interesting to see specific examples of alleged copyright infringement.
- Another user brings up an interesting point, stating that ChatGPT accurately reproduces the impression of paragraphs from articles, making it appear as though people can read NYT content without actually engaging with it fully. They argue that this misrepresentation could harm the reputation of the NYT and potentially lead to legal issues.
- One user comments that this is just the beginning and the court case will likely have significant implications for copyright law. They mention that it is a good start to see the dominance of the major tech companies being challenged.
- A user suggests that the discontinuation of using NYT training data is a positive development, as it emphasizes the importance of respecting copyrights and benefits the rights holders.
- In response, another user points out the irony that OpenAI, which is associated with extremely wealthy individuals, is being taken to court by other wealthy individuals, highlighting the power dynamics at play.
- A discussion emerges about the nature of legal claims being broadly made and then eventually dropped or dismissed in many cases involving wealthy individuals.

Overall, the discussion showcases different perspectives on the court case, with additional insights raised about copyright infringement, the impact on media organizations, and the involvement of wealthy individuals.

### GPT inputs outgrow world chip and electricity capacity

#### [Submission URL](https://www.astralcodexten.com/p/sam-altman-wants-7-trillion) | 19 points | by [quirkot](https://news.ycombinator.com/user?id=quirkot) | [13 comments](https://news.ycombinator.com/item?id=39358693)

In a recent post on Astral Codex Ten, the author discusses Sam Altman's desire for $7 trillion and why it's a significant reminder of the challenges that AI will face in scaling up. The post breaks down the cost of training AI into three main components: compute, electricity, and training data. With each new generation of AI models, the cost increases significantly. For example, GPT-4 is estimated to have cost $100 million, and GPT-5 is rumored to have cost $2.5 billion. The author speculates that GPT-6 could cost $75 billion and GPT-7 a whopping $2 trillion. 

The challenge lies in the resources required to train these models. In terms of compute, GPT-4 took about six months using 1/2000th of all the computers in the world. Scaling this up, GPT-7 would require 15 times as many computers as currently exist. As for electricity, GPT-7 would need the output of fifteen Three Gorges Dams, the largest power plant in the world. Training data presents another challenge, as the amount of text available is limited. While synthetic data can be used to generate more training data, it's not yet clear how effective this approach will be for written text.

Overall, Altman's $7 trillion aspiration serves as a reminder of the immense resources required to scale AI models and the challenges that lie ahead. It highlights the need for advancements in compute power, energy production, and training data generation techniques.

The discussion on this post revolves around several points. 

- One commenter criticizes the article, pointing out that its title and substance misrepresent the actual growth rate of AI models. They clarify that the growth is not exponential but rather increases by factors of 100x, 25x, and 25x respectively for each generation.
- Another commenter expresses their wish for AMD to quickly implement transparent methodologies for GPU computing, specifically mentioning the LLVM compiler infrastructure.
- One commenter argues that previous predictions about the hard drive storage capacity being quickly exceeded were proven wrong when it comes to DNA sequencing. They suggest that people are not fully considering the scaling limitations of language models being discussed.
- A commenter highlights an interesting point from the article, specifically that GPT-4 consumed 50 gigawatt-hours of energy during training, and with a scaling factor of 30x, they estimate that GPT-5 would require 1,500 gigawatt-hours, GPT-6 would require 45,000 gigawatt-hours, and GPT-7 would require 13 million gigawatt-hours.
- A comment thread discusses the assumption that OpenAI's training is exclusively powered by Microsoft's low carbon energy. One commenter argues that there are limitations to renewable energy production and maintenance and questions Microsoft's commitment to a 100% renewable energy world.
- Another commenter in the thread suggests that Microsoft is hiring nuclear scientists, possibly to power their AI initiatives, and they criticize Microsoft for promoting green energy while relying on dirty energy elsewhere.
- The discussion then moves towards the dynamics of power purchase agreements (PPAs), renewable energy credits (RECs), and the translation of lowering electricity production CO2 footprints. The comment highlights competitive wind and solar power production in certain locations and carbon cap-and-trade systems to prevent increased power demand relying on fossil energy.
- Another commenter supports the claim made in the previous comment and shares links to independent research that corroborates it.
- Lastly, a commenter mentions that slower training generations of large language models (LLMs) can select from a wider range of low carbon energy sources without compromising their training efficiency, regardless of the energy source mix in the market for electricity generation.
- Another commenter suggests that companies like Microsoft are likely making renewable energy commitments regardless of cost. They share links to articles discussing Microsoft's renewable energy pledges.

### Dude, where's my self-driving car?

#### [Submission URL](https://www.theverge.com/24065447/self-driving-car-autonomous-tesla-gm-baidu) | 18 points | by [mfiguiere](https://news.ycombinator.com/user?id=mfiguiere) | [32 comments](https://news.ycombinator.com/item?id=39357882)

In 2015, Google's self-driving car project leader Chris Urmson predicted that self-driving cars would be so widespread by 2020 that his then-11-year-old son would never need a driver's license. Fast forward to 2024, and autonomous vehicles are still far from being a common sight on the roads. Over the years, there have been numerous missed deadlines and overly optimistic predictions about the availability of self-driving cars. Companies like Baidu, Lyft, GM, and Ford all promised to have autonomous vehicles on the market by certain dates, but those deadlines have come and gone without fruition. Even Tesla CEO Elon Musk, known for his bold claims, has made inaccurate predictions about the readiness of autonomous vehicles. Despite some autonomous cars currently operating in select cities, they are confined to geofenced service areas and face technological limitations, opposition from labor unions, and restrictions on certain roads and weather conditions. The hype surrounding autonomous vehicles has allowed companies to secure funding for their experiments, while regulators have taken a relatively lenient stance on self-driving car testing. However, the industry's failure to deliver on its promises raises questions about the underlying reasons for the delay. One of the primary motivations behind the optimistic predictions was financial gain, as companies were able to secure funding based on the promise of an imminent autonomous future. This flow of money also influenced regulators to adopt a more permissive approach to self-driving car testing. Companies operating in the autonomous vehicle space have raised billions of dollars through traditional fundraising channels and partnerships with big tech and car companies. While some progress has been made, the widespread availability of self-driving cars still remains a distant reality.  

The discussion on this submission revolves around various aspects of self-driving cars. One user shares their experience with adaptive cruise control in a 2022 Kia Telluride, mentioning its limitations and the need for constant attention while driving. Another user expresses their skepticism about relying on self-driving technology, citing personal experiences of confusion and dangerous situations with Tesla's Full Self-Driving (FSD) feature. The discussion then moves to a debate about the safety and reliability of autonomous driving systems compared to human drivers. Some users argue that self-driving technology is not yet capable of completely replacing human drivers, while others defend Tesla's capabilities and criticize non-Tesla manufacturers. There is also a discussion about the societal impact of self-driving cars, with one user mentioning the convenience of having a personal chauffeur for long trips and another user highlighting the hassle of public transportation for weekend ski trips. The conversation concludes with a user expressing their disinterest in purchasing a Level 4+ autonomous vehicle due to the limited budgets and high costs associated with self-driving options.

---

## AI Submissions for Mon Feb 12 2024 {{ 'date': '2024-02-12T17:11:13.897Z' }}

### Home Assistant: Three years later

#### [Submission URL](https://eamonnsullivan.co.uk/posts-output/home-automation-three-years/2024-02-11-home-assistant-three-years-later/) | 234 points | by [eamonnsullivan](https://news.ycombinator.com/user?id=eamonnsullivan) | [151 comments](https://news.ycombinator.com/item?id=39345122)

In this blog post, the author reflects on their experience with Home Assistant, a popular home automation software, after using it for almost three years. They discuss what hasn't changed, such as the ability to integrate devices from different manufacturers and the use of Node-RED for automations. However, they note that their approach to automation has evolved, focusing more on subtle and seamless actions rather than flashy effects. They also emphasize the importance of local control, opting for devices that work without relying on cloud services. The author mentions their reliance on the Home Assistant Cloud service for secure remote access and expresses cautious optimism about the new local-focused standard for home automation called Matter. They also discuss the growing importance of Home Assistant in their daily life and mention their considerations for replacing their hardware setup in the future. Overall, the author highlights their journey with Home Assistant and the valuable lessons they've learned along the way.

The discussion on the submission revolves around personal experiences with Home Assistant and home automation in general. Some individuals express frustration with cloud services and the reliance on internet connectivity for functionality. Others discuss their DIY projects and the challenges they face in integrating different devices. The topic of local control and the rejection of cloud services is also brought up, with some commenters expressing concerns about privacy and potential issues with companies discontinuing support. The discussion also touches on the compatibility of different smart home devices and the importance of standardization. Some users share their preferred hardware options, such as Z-Wave switches, while others mention their experiences with mechanical and digital solutions for light switches. The conversation concludes with skepticism about the future adoption of the Matter standard and doubts about its current functionality.

### The Rise and Fall of GOFAI

#### [Submission URL](https://billwadge.com/2024/02/12/the-rise-and-fall-of-gofai/) | 47 points | by [herodotus](https://news.ycombinator.com/user?id=herodotus) | [36 comments](https://news.ycombinator.com/item?id=39344934)

Bill Wadge, an AI expert, recently wrote a blog post discussing the rise and fall of Good Old Fashioned AI (GOFAI). GOFAI, which grew out of the 1956 Dartmouth AI meeting, is based on symbolic reasoning and has had several triumphs throughout history. Wadge highlights the invention of numbers and numerals as the first triumph, as they allowed for reliable symbolic reasoning about quantities. The invention of the abacus and similar mechanisms also automated symbolic computation with numerals. Aristotle's classification of valid syllogisms and the development of calculus and mathematical notation further extended the domain of symbolic reasoning. However, GOFAI encountered significant challenges as well. Russell's Paradox dealt a blow to Frege's axiomatization of set theory, and Gödel's incompleteness theorems showed that any formal system powerful enough to formalize arithmetic is incomplete. Despite these setbacks, GOFAI continued to make progress with the introduction of the λ calculus and Turing machines. However, it became clear by the 1956 Dartmouth conference that GOFAI had its limitations.

The discussion about the submission revolves around various aspects of Good Old Fashioned AI (GOFAI). Some commenters argue that GOFAI is not relevant and that modern AI techniques, particularly those based on machine learning, have far surpassed its capabilities. They mention examples like Deep Blue, which defeated Garry Kasparov in chess using brute-force search, and suggest that GOFAI approaches like symbolic reasoning are no longer practical.
Others point out that GOFAI has been successful in certain domains, such as natural language processing, planning, and scheduling. They argue that GOFAI techniques, like those implemented in Lisp, have their own advantages and should not be dismissed entirely.
There is also a discussion about the fundamental problems of GOFAI, including Gödel's incompleteness theorems and the undecidability problem. Some commenters argue that machine learning, while based on mathematical foundations, also has its limitations and is not a complete solution to AI.
The debate continues with discussions on the definition of GOFAI, its relationship with other fields like cybernetics, and the role of logic in AI. Some commenters highlight the successes of GOFAI in areas like automated provers and expert systems, while others express skepticism and believe that GOFAI is limited in its scalability.
Overall, the discussion reflects different perspectives on the strengths and weaknesses of GOFAI compared to modern AI approaches, and the ongoing debate about the future direction of AI research and development.

### AMD funded a drop-in CUDA implementation built on ROCm: It's now open-source

#### [Submission URL](https://www.phoronix.com/review/radeon-cuda-zluda) | 1001 points | by [mfiguiere](https://news.ycombinator.com/user?id=mfiguiere) | [390 comments](https://news.ycombinator.com/item?id=39344815)

In a surprising move, AMD has quietly funded a project to bring binary compatibility between NVIDIA CUDA applications and the AMD ROCm stack. This means that many CUDA applications can now run on AMD Radeon GPUs without the need for developers to adapt the source code. The project, known as ZLUDA, was initially developed to enable CUDA support on Intel graphics, but was later adapted for use on AMD GPUs. Although it's not 100% fail-safe and some features are not supported, the implementation has been successful in running CUDA-enabled software on ROCm. The open-source code is dual-licensed under Apache 2.0 or MIT, and it leverages the Rust programming language for the Radeon implementation. This development opens up new possibilities for end-users who want to run CUDA applications on AMD GPUs without any additional effort.

The discussion on Hacker News regarding the submission about AMD funding a project for binary compatibility between NVIDIA CUDA applications and the AMD ROCm stack covers a range of topics.
One commenter points out the dominance of NVIDIA in the machine learning field due to the popularity of CUDA. They argue that if AMD can provide seamless compatibility for CUDA machine learning tasks, it could potentially eat into NVIDIA's market share.
Another commenter raises concerns about the ever-changing landscape of machine learning frameworks and the difficulty of keeping up with them. They suggest that instead of investing in low-level optimizations for AMD GPUs, it might be strategically better for AMD to invest in compilers and software tools that allow high-level languages to write efficient kernels for AMD hardware.
There is also a discussion about the business decision behind AMD's funding of the project. Some commenters express frustration at AMD's lack of support for the ROCm stack, while others speculate about AMD's long-term strategy in the machine learning market.
One commenter brings up the success of the RADV project, which is an open-source Radeon Vulkan driver developed by Red Hat and Valve. They argue that if AMD can fund and support projects like ZLUDA, it can be beneficial for the community as a whole.
Additionally, there are discussions about the control NVIDIA has over CUDA software and hardware, the misconception that deep learning frameworks are built solely on CUDA, and the potential limitations of ZLUDA compared to the official CUDNN library.

Overall, the discussion reflects a mixture of excitement, skepticism, and curiosity about AMD's move to fund the ZLUDA project and the potential impact it could have on the machine learning community.

### GeneGPT, a tool-augmented LLM for bioinformatics

#### [Submission URL](https://github.com/ncbi/GeneGPT) | 106 points | by [brianzelip](https://news.ycombinator.com/user?id=brianzelip) | [18 comments](https://news.ycombinator.com/item?id=39348902)

GeneGPT is a tool-augmented large language model (LLM) designed to address the challenges faced by LLMs in handling specialized biomedical knowledge. With the goal of improving information retrieval in this domain, GeneGPT uses NCBI Web APIs to answer questions related to biomedical information. This approach leverages in-context learning and a unique decoding algorithm to execute API calls. Experimental results show that GeneGPT outperforms previous state-of-the-art models on eight GeneTuring tasks, with an average score of 0.83. The model surpasses BioGPT and ChatGPT, achieving a significant improvement in accuracy. In addition, GeneGPT demonstrates the potential of integrating domain-specific tools with LLMs to improve access and accuracy in specialized knowledge areas. The code and data for GeneGPT are available on GitHub.

The discussion on this submission revolves around various aspects of the GeneGPT project and related topics in the field of biomedical information retrieval. 
One user points out that the initial prompt for the language model project doesn't specify whether it's using the OpenAI API or some other method. Another user provides clarification, saying that the project is using NCBI Web APIs wrapped in command-line interfaces (CLIs) to perform tasks and suggests that the GPT OpenAI Marketplace could be a suitable platform for this.
Another user mentions that they found a similar project called LLaVA-Med, which also focuses on using domain-specific tools with language models.
There is a brief comment suggesting that the name "NCBI-APIs-GPT" would be more appropriate for this project.
The discussion then shifts to the topic of genetic information. One user jokingly suggests that they will make the GPT process their 23andMe DNA data to confirm special abilities. Another user mentions a tool called LLaVA-Med, which they believe is the best modern classifier they have found.
The conversation also touches on the evaluation of language models and the importance of measuring factors like question-answering accuracy and exact match performance.
A user brings up the possibility of using language models to predict phenotypes from genotype data, highlighting the potential of genetic engineering and its impact on phenotypic traits.
There's a discussion about transfer learning and the potential for transformer models to map genotypes to phenotypes accurately. Some users mention the challenges of gene regulation and the complexity of genetic data.
One user raises concerns about data security and the risks of handling sensitive genetic information. They suggest that genetic datasets contain subtle patterns that can be exploited.

Finally, there are a couple of comments unrelated to the main topic, with one user expressing disappointment that a language model is based on Gene Rayburn from Match Game and another user confirming the nature of the post.

### A celebrated cryptography-breaking algorithm just got an upgrade

#### [Submission URL](https://www.quantamagazine.org/celebrated-cryptography-algorithm-gets-an-upgrade-20231214/) | 46 points | by [thunderbong](https://news.ycombinator.com/user?id=thunderbong) | [3 comments](https://news.ycombinator.com/item?id=39341180)

Researchers have developed a new algorithm that improves the efficiency of lattice basis reduction, a technique widely used in cryptography and mathematics. The algorithm, known as LLL-style, allows for the reduction of the bases of lattices with thousands of dimensions, significantly expanding the range of scenarios in which LLL-like approaches can be used. The technique combines multiple strategies, including a recursive structure and precise number management, to achieve its efficiency. The algorithm has the potential to enhance the security and performance of cryptographic systems.

The discussion surrounding the submission is rather limited. One user, "dfrst," shares a link to the full paper of 63 pages, providing more in-depth information on the algorithm. Another user, "dng," shares a link to another article about the algorithm upgrade, which has attracted five comments. There is a third user, "ChrisArchitect," who simply mentions the year "2023," potentially indicating that the algorithm is expected to be significant in that year. Unfortunately, without further details, it is difficult to discern the specific content of the comments or the significance of the mentioned year.
- Biological AI and the evolution of plant intelligence were mentioned, raising the idea of millions of years of closed-loop plant systems evolving into AI.

Overall, the discussion covered a wide range of topics related to AI, energy consumption, economics, and the environment.

---

## AI Submissions for Sun Feb 11 2024 {{ 'date': '2024-02-11T17:09:40.809Z' }}

### Disney's newest robot demonstrates collaborative cuteness

#### [Submission URL](https://spectrum.ieee.org/disney-robot-2666681104) | 39 points | by [kungfudoi](https://news.ycombinator.com/user?id=kungfudoi) | [15 comments](https://news.ycombinator.com/item?id=39339264)

Disney's newest robot, Duke Weaselton, is demonstrating the power of collaborative cuteness. At Walt Disney Imagineering, researchers have been exploring the idea of robots collaborating with each other, and Duke Weaselton is the result of one such collaboration. Duke, a furry character, interacts with a purple kiosk onstage, with both characters showcasing different kinds of motion. By working together as one system, they are able to achieve dynamic and expressive movements. This opens up exciting opportunities for entertainment robotics and demonstrates that two robots can be much more capable than one.

The discussion on this submission revolves around a few different topics. 
- One user mentions that Disney's YouTube channel regularly posts interesting developments in animation, human tracking, and motion. They provide a link to the channel and encourage checking it out.
- Another user asks about the high pixelated gif they see on a website, to which someone responds that it is likely a WebM file, an open media format designed for the web. There is then a discussion about whether iOS supports WebM, with one user mentioning that Safari on iOS does not support it.
- Someone else comments that the robot in question appears to be collaborating and wonders what it would be like if it spoke to technical experts at IEEE, the professional organization for technical advancements. There is a response mentioning that spectrum proceedings (presumably an IEEE publication) would be interested in this kind of development.
- A user points out that the discussion has deviated from the topic and that the robot is indeed a robot. This leads to a brief conversation about the definition of a robot.

Overall, the discussion touches on topics like Disney's other developments, media formats, technical organizations, and the nature of robots.

### Show HN: LLMWare – Small Specialized Function Calling 1B LLMs for Multi-Step RAG

#### [Submission URL](https://github.com/llmware-ai/llmware) | 46 points | by [doberst0103](https://news.ycombinator.com/user?id=doberst0103) | [5 comments](https://news.ycombinator.com/item?id=39335263)

llmware is a unified framework for developing LLM (Language and Learning Models) based application patterns, including Retrieval Augmented Generation (RAG). It provides a set of tools that make it easy to build knowledge-based enterprise applications using open source models and secure integration of enterprise knowledge. With llmware, developers can access and use a wide range of models through the Model Catalog, ingest, organize, and index a collection of knowledge with the Library feature, and query libraries with various filters using the Query feature. The framework is designed to be beginner-friendly while also catering to the needs of experienced AI developers.

The discussion around the submission mainly consists of comments from the project creator, blzngbnn. They provide some insights into the development of LLMWare, including the ability to run models locally on standard consumer CPUs and the provision of cheap and efficient model processing. They mention using SLIM models for powerful timing and internal business process enhancements using LLMs.
Another user, doberst0103, thanks blzngbnn for their feedback. Then trcrblltx reminds blzngbnn to give credit to llmcpp for running models and mentions the project's platform for Retrieval Augmented Generation (RAG) function calling.
In response, doberst0103 acknowledges the credit that should be given to Georgi Gerganov and mentions their amazing advancements demonstrated in YouTube videos.
Finally, blzngbnn agrees that llmcpp, the open-source community, and getting AI models directly into the hands of people are all excellent things. They express excitement for the power of local LLMs and mention eagerly waiting for open-source platforms to fulfill their potential by 2024.

### Show HN: Loz – Automate Git Commit Messages with LLM

#### [Submission URL](https://github.com/joone/loz) | 18 points | by [cubix4u](https://news.ycombinator.com/user?id=cubix4u) | [4 comments](https://news.ycombinator.com/item?id=39331538)

LOZ is a powerful command-line tool that harnesses the capabilities of Large Language Models (LLMs) like code-lamma and GPT-3.5 to automatically generate GIT commit messages based on code changes. With 201 stars and 11 forks, this project is gaining traction in the developer community. The tool is written in TypeScript, JavaScript, Shell, and Dockerfile. The repository has 3 contributors, including the creator, Joone Hur. If you're looking for a way to streamline your commit workflow and make it more efficient, LOZ might be the tool you need.

The discussion on the LOZ tool revolves around the use of Large Language Models (LLMs) for generating commit messages and the effectiveness of such messages. One commenter points out that the AI doesn't understand the business context of the code changes and cannot explain the higher-level reasoning behind them. They argue that the generated messages lack usefulness in providing helpful information for debugging or modifying code.

Another commenter argues that commit messages should describe the context and reasoning behind the code changes, making the generated messages essentially pointless.
Someone shares their experience with using a Python script to generate commit messages using ChatGPT 1 (powered by GPT) and suggests a similar approach to LOZ.
Lastly, a user mentions the effectiveness of GitHub Copilot in summarizing pull requests (PRs), working seamlessly within the GitHub workflow. They suggest experimenting with multiple models and comparing comments to determine what works best.