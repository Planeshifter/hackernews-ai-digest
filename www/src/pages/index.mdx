import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Sun Jun 02 2024 {{ 'date': '2024-06-02T17:10:23.884Z' }}

### What We Learned from a Year of Building with LLMs (Part II)

#### [Submission URL](https://www.oreilly.com/radar/what-we-learned-from-a-year-of-building-with-llms-part-ii/) | 23 points | by [tim_sw](https://news.ycombinator.com/user?id=tim_sw) | [4 comments](https://news.ycombinator.com/item?id=40558044)

In the second part of the series on building with Large Language Models (LLMs), the authors delve into the operational aspects of creating LLM applications, bridging the gap between strategy and tactics. They address key questions related to data, models, product, and people.

1. **Data**: Quality input data is crucial for LLM performance. Monitoring LLM inputs and outputs regularly is essential to understand data distribution, detect skew, and ensure alignment between development and production data. Skew can be structural (formatting discrepancies) or content-based (differences in meaning or context). Strategies to mitigate skew include tracking metrics like input/output length, conducting qualitative assessments of outputs ("vibe checks"), and incorporating non-determinism into skew checks.

2. **Models**: Integrating LLMs into the tech stack and managing versioning are key considerations. Thinking about how to version models, migrate between versions, and maintain compatibility is crucial. Balancing conflicting requirements, involving design early in the development process, and prioritizing user experiences are vital aspects of product development.

3. **Product**: Designing user experiences with human-in-the-loop feedback, calibrating product risk, and prioritizing requirements are essential for successful LLM applications. Cultivating a culture of experimentation, hiring the right talent, and leveraging emerging LLM applications to build your own are important for team success.

4. **People**: Hiring the right team members, fostering a culture of experimentation, and navigating the balance between process and tooling are key considerations for building successful LLM applications.

By focusing on these operational aspects, organizations can effectively develop and manage LLM applications and the teams that build them, ensuring the optimal performance and impact of their machine learning systems.

The discussion on the submission includes a comment expressing skepticism about the quality of the AI-generated introduction, suggesting that AI language models lack the ability to provide a genuinely engaging and tailored introduction. Other users agreed, with one pointing out that building large language models (LLMs) involves more than just writing text.

### AI Is a False God

#### [Submission URL](https://thewalrus.ca/ai-hype/) | 50 points | by [pseudolus](https://news.ycombinator.com/user?id=pseudolus) | [25 comments](https://news.ycombinator.com/item?id=40553571)

In the latest issue of The Walrus, the author Navneet Alang dives into the world of artificial intelligence in a thought-provoking piece titled "AI Is a False God." The article explores the hype and concerns surrounding AI, drawing parallels to historical technological advancements like the World Wide Web.

Alang discusses how AI technology, particularly large language models (LLMs) such as ChatGPT, has garnered immense interest and investment from tech giants like Microsoft, Meta, and Alphabet. While some see AI as a revolutionary force that will usher in a new era of innovation, others, like philosopher Nick Bostrom, warn of the existential risks associated with super-intelligent machines.

The narrative touches on the potential societal impacts of AI, from concerns about job displacement to fears of losing control over AI systems. By reflecting on past technological advancements and their unintended consequences, Alang urges readers to approach AI development with caution and critical scrutiny.

Through engaging storytelling and compelling artwork by designer Marian Bantjes, "AI Is a False God" prompts readers to ponder the implications of our growing reliance on artificial intelligence and the need for thoughtful regulation and ethical considerations in this rapidly evolving field.

The discussion on Hacker News regarding the article "AI Is a False God" in The Walrus covers a wide range of viewpoints and criticisms. Here are some key points from the conversation:

- Some users feel that the article is not worth reading, describing it as shallow and dismissive, with obscure philosophical references that make it challenging to grasp the central arguments. They suggest that the article fails to delve deeply into the subject matter and is overly hyped.
- Another user, with a background in philosophy, appreciates the mention of Derrida and Deleuze in the article, highlighting the connection to the high-dimensional spaces that large language models (LLMs) like transformers operate in. They point out that the references to these philosophers help support the argument about the transformative power of LLMs.
- There is a debate about the value of AI and whether it is overhyped. Some users express skepticism about the productivity gains from AI tools like CoPilot, while others argue that AI advancements have the potential to significantly impact various industries and improve productivity.
- One user questions the perspective presented in the article, suggesting that the comparison of AI to a false god may overlook the potential benefits and advances that AI technology can bring. They encourage a broader examination of the risks and rewards associated with AI development.

Overall, the discussion showcases a mix of skepticism, philosophical analysis, and differing opinions on the implications of AI technology and its portrayal in the article.

---

## AI Submissions for Sat Jun 01 2024 {{ 'date': '2024-06-01T17:14:11.197Z' }}

### Lisp: Icing or Cake?

#### [Submission URL](https://dthompson.us/posts/lisp-icing-or-cake.html) | 190 points | by [psj](https://news.ycombinator.com/user?id=psj) | [36 comments](https://news.ycombinator.com/item?id=40549250)

In the latest update from Hacker News, David Thompson discusses the insights he gained from the recent Spring Lisp Game Jam 2024, where a record-breaking 48 games were submitted. The breakdown of submissions by language reveals Guile as the most popular choice, with more games leveraging the Scheme-to-WebAssembly compiler 'Hoot'. Fennel, a Lisp that compiles to Lua, was another favored language for game development. Thompson identifies two predominant approaches in building games with Lisp: the 'Lisp as icing' and 'Lisp as cake' paradigms. The former involves using Lisp as a scripting language atop a base code in static languages like C or Rust, while the latter integrates Lisp throughout the entire software stack, minimizing external dependencies. The discussion touches on specific implementations like Guile for extending C programs and Common Lisp for full-stack Lisp development.

Case studies from the game jam illustrate these patterns, with Fennel paired with love2d exemplifying 'Lisp as icing' - where Fennel acts as a scripting layer on top of the C++ game engine. On the other hand, the S7 and raylib stack represents a 'Lisp as cake' approach, where Lisp plays a significant role in the project architecture.

Thompson's analysis provides valuable insights into the diverse ways Lisp is being utilized in game development, showcasing the flexibility and versatility of this language in creating engaging and innovative projects.

The discussion revolves around the recent insights shared by David Thompson on the Spring Lisp Game Jam 2024 and the use of Lisp in game development. Here are some highlights from the comments:

- **nctdncn**: Discusses the experience of using Scheme in Max MSP and the comparison between Guile, Clojure, and Common Lisp.
- **rnlszlrn**: Expresses surprise at the number of games using Janet and mentions encountering gaming surprises.
- **hzn**: Encourages readers to check out the special thread on Lisp.
- **rcrm**: Mentions using Guile and discusses WASM development.
- **rtctn**: Shares a 3D bass fight prototype using Clojure.
- **bmch**: Mentions a joint project and critiquing a vintage article.
- **swatson741**: Shares an interest in creating games using Lisp.
- **skssn**: References a game called Dunnet and its history.
- **nthk**: Discusses experiences with the ZMachine interpreter for interactive fiction.
- **ngcc_hk**: Engages in a conversation about the evolution of Lisp and CPUs.

Overall, the discussion delves into various aspects of game development using Lisp and shares insights and experiences related to utilizing different Lisp dialects for programming projects.

### How to Distribute Postgres Geographically

#### [Submission URL](https://xata.io/blog/geo-distributed-postgres) | 94 points | by [mebcitto](https://news.ycombinator.com/user?id=mebcitto) | [11 comments](https://news.ycombinator.com/item?id=40542940)

In the blog post titled "Geographically distributed Postgres for multi-tenant applications" by Tudor Golubenco, a pattern for global distribution of Postgres databases for multi-tenant applications is detailed. The approach involves separating per-tenant data tables from control plane tables, placing tenant data in the region closest to users, and creating a global view using Postgres Foreign Data Wrappers (FDW) and partitioning.

The pattern is useful for applications with a global customer base due to reasons such as reducing latencies, complying with data residency laws, running apps on the edge, and enabling multi-region or multi-cloud setups. It works when data can be segmented by a key (e.g., customer id) into tenants and when each tenant has a natural affinity to a particular region.

The control plane data, including authentication and user tables, is kept in a centralized region, while tenant data is distributed based on affinity regions. This ensures that foreign keys point inside the tenant, allowing access to all tenant data by connecting to a single region. Authentication is done centrally, with subsequent requests directed to the region storing the relevant tenant data.

The blog post uses a Notion clone as an example to illustrate how pages data could be distributed based on tenant and region. By following this pattern, multi-tenant SaaS applications can efficiently manage data across multiple regions while maintaining centralized control and authentication.

The discussion on the blog post about geographically distributed Postgres for multi-tenant applications covered various aspects related to implementing and managing such systems.

1. **jtl and tdrg** discussed experimenting with sharded tenant-clustered tables using Postgres Foreign Data Wrappers (FDW) and the challenges of routing queries efficiently in a multi-region setup. They also mentioned the complexity and risks involved in altering the schema across multiple Postgres instances.

2. **Ozzie_osman** shared familiarity with AWS in the context of preparing sharded Postgres databases and questioned the approach detailed in the blog post.

3. **sgrlnd** raised concerns about the performance limitations of Aurora Serverless and the high costs associated with serverless approaches.

4. **nrpl** discussed the complexity of managing sharded Postgres databases, emphasizing the challenges in handling schema changes, balancing data, and preventing production outages. They highlighted the benefits of using smaller databases for better performance and operational efficiency.

5. **hzskll** provided a critical perspective on the manageability of shared data in Postgres using FDW, emphasizing the importance of consistent backups and mechanisms to handle schema changes across multiple regions. They also questioned the necessity of a distributed system and the transparency of its design for reliability.

6. **tdrg and hzskll** further discussed the challenges and considerations of designing systems for distributed data sharing, emphasizing the security, scalability, and performance implications of such architectures.

Overall, the discussion delved into the technical complexities, performance considerations, and operational challenges associated with implementing geographically distributed Postgres databases for multi-tenant applications.

### The quest to craft the perfect artificial eye through the ages

#### [Submission URL](https://www.popsci.com/health/artificial-eye-history/) | 32 points | by [chat](https://news.ycombinator.com/user?id=chat) | [3 comments](https://news.ycombinator.com/item?id=40548399)

The art of prosthetic eyes has a rich history dating back nearly 5,000 years, involving materials from tar to gold wires to PMMA. Glassblowing techniques from 16th-century Venetian artisans laid the foundation, with a shift to acrylic like PMMA during World War II due to a lack of German glass exports. Ocularists, the skilled artisans who craft these intricate eyes, pass down their expertise through generations, with a typical apprenticeship taking around five years.

The quest for realistic prosthetic eyes mirrors the importance of eye contact in human communication. Recent studies have shown that eye contact triggers important brain activities and plays a role in romantic attraction and social interactions. Motility, or the ability of the prosthetic eye to move in sync with the natural eye, is crucial for a natural appearance.

The evolution of prosthetic eyes took a significant step forward in the late 19th century with the introduction of orbital implants, allowing for more natural movement and function. Today, advancements in technology, such as 3D printing, continue to improve prosthetic eyes, enhancing their appearance, comfort, and functionality.

The discussion revolves around personal experiences with prosthetic eyes and advancements in the field. The original commenter, "Daub," shares their story of experiencing an odd feeling and adjustments after a workshop accident that resulted in their prosthetic eye. They mention how the artificial eye behaved differently from the natural eye, with slightly slower movements and limited vision. Despite the artificial eye being perfectly matched to the natural one, they still faced challenges in various situations, such as navigating traffic.

Another user, "acheong08," mentions hearing about scientific progress in connecting a camera directly to the brain. "Daub" then brings up the topic of artificial vision in terms of success for blind people but notes that it may still be a challenge for some individuals.

Overall, the discussion provides personal anecdotes and touches on the technological advancements in artificial vision systems.

### Re-Evaluating GPT-4's Bar Exam Performance

#### [Submission URL](https://link.springer.com/article/10.1007/s10506-024-09396-9) | 27 points | by [rogerkeays](https://news.ycombinator.com/user?id=rogerkeays) | [7 comments](https://news.ycombinator.com/item?id=40543535)

A recent paper has raised doubts about the reported 90th percentile performance of GPT-4 on the Uniform Bar Exam. The study highlights methodological challenges and presents findings that suggest OpenAI's estimates of GPT-4's performance may be overinflated. Data from different exam administrations indicate varying percentiles for GPT-4, with estimates ranging from below the 69th percentile to as low as the 15th percentile on essays for licensed or license-pending attorneys. The paper also questions the validity of GPT-4's reported essay score and explores the impact of hyperparameter combinations on its performance. These findings raise important considerations for the use of AI in legal tasks and underscore the need for rigorous evaluations to ensure the reliability of AI models like GPT-4.

The discussion on the submission revolves around skepticism regarding the reported performance of GPT-4 on the Uniform Bar Exam. Some users argue that the tests designed to discriminate between humans and AI may not accurately measure the abilities needed for legal practice, with one user emphasizing the importance of subjective interpretation in legal analysis. Another user points out discrepancies in GPT-4's reported 92nd percentile score, with findings indicating scores as low as the 15th percentile in specific sections. There is also a mention of the competitive nature of legal practice and the significance of passing the bar exam. Additionally, users question the credibility of GPT-4's essay scores and its ability to perform legal tasks effectively. One user expresses surprise at the lack of replication of the research and doubts the validity of the reported 90th percentile performance of GPT-4.

### Instagram will use users' content to train it's AI

#### [Submission URL](https://in.mashable.com/tech/76239/meta-is-using-your-posts-to-train-ai-its-not-easy-to-opt-out) | 8 points | by [nehagup](https://news.ycombinator.com/user?id=nehagup) | [5 comments](https://news.ycombinator.com/item?id=40546888)

Meta, the tech giant behind Facebook and Instagram, is using your posts to train its AI models, but opting out is not as easy as it seems. The company's data sharing practices have drawn attention, especially in Europe where users were recently notified about changes to privacy policies related to new generative AI features. This move has sparked concerns about privacy and data usage.

Meta's generative AI privacy policy reveals that it uses information shared on its platforms, such as posts and photos, to train its AI models. While private messages are not used for training, public content is fair game. Users in the UK and EU have the option to object to this data sharing, but the process is cumbersome and intentionally challenging to navigate, as noted by some users.

To limit what you share with Meta's AI models, one option is to delete your accounts, although that's a drastic measure. Users can also submit requests to access, delete, or file complaints about personal information used for AI development. However, the process has limitations and may not guarantee full opt-out.

Another avenue to safeguard your data is through the "activity off Meta" settings, where you can manage sites and apps sharing information with Meta and control future data sharing. Disconnecting and managing future activity can help prevent further data sharing with third parties.

Despite these measures, Meta's privacy settings primarily address sharing with third parties, leaving questions about internal data usage for training AI models. While the options may not offer a foolproof solution, they are steps towards regaining some privacy control. Clarifications from Meta on data usage internally are awaited to provide a clearer understanding of the situation.

The discussion around Meta using posts to train its AI models touches on various points. One user pointed out the challenge of opting out of this data sharing practice with Meta's AI, while another mentioned the difficulty in working around internet safety caveats when attempting to limit data sharing while still allowing AI training. The conversation also delved into the legal implications of scraping data and the relevance of Zuckerberg's comments today in light of the consequences it may bring. Additionally, there was a brief mention of corporate intellectual property, indicating the complexity of the situation regarding data usage and privacy with Meta's AI models.

---

## AI Submissions for Fri May 31 2024 {{ 'date': '2024-05-31T17:18:57.616Z' }}

### How to Think Like a Computer Scientist: Interactive Edition

#### [Submission URL](https://levjj.github.io/thinkcspy/) | 170 points | by [l8rlump](https://news.ycombinator.com/user?id=l8rlump) | [27 comments](https://news.ycombinator.com/item?id=40531347)

Today on Hacker News, a submission discussing "How to Think Like a Computer Scientist: Interactive Edition Table of Contents" caught the attention of many readers. The detailed table of contents covers various topics, starting from the general introduction about programming and algorithms to more advanced concepts like Python programming, debugging, data types, conditionals, loops, strings, lists, functions, recursion, dictionaries, classes, objects, image processing, GUI programming, and working with files. The interactive edition seems to be a comprehensive resource for those looking to enhance their programming skills and understanding of computer science principles. It's an exciting find for anyone interested in diving deep into the world of programming and computational thinking.

The discussion on the Hacker News submission about the "How to Think Like a Computer Scientist: Interactive Edition Table of Contents" varied widely. Some users pointed out technical issues with the potential server configurations and blocked ports affecting the system's performance. Others mentioned the importance of permissions when accessing certain websites, citing the Port Authority as a source. Users seemed to appreciate the detailed table of contents, with some stating that it made the book enticing for computer science enthusiasts. Additionally, the interactive nature of the book was highlighted as a valuable resource for learning Python programming and computer science concepts. One user mentioned the availability of interactive chapters on Jupyter notebooks and Google Colab, while another shared their positive experience with using the Runestone book to learn programming, particularly Python. The discussion also touched on unrelated topics like CVs and experience in the field, as well as recommendations for other programming language textbooks and the significance of introducing children to computer science early on. Overall, the conversation touched on technical aspects of the book, personal experiences with learning programming, and broader considerations around computer science education.

### Superconducting Computer: Imec's plan to shrink datacenters

#### [Submission URL](https://spectrum.ieee.org/superconducting-computer) | 80 points | by [redbell](https://news.ycombinator.com/user?id=redbell) | [47 comments](https://news.ycombinator.com/item?id=40532771)

The June 2024 issue of IEEE Spectrum introduces an intriguing concept of putting a data center in a shoebox using superconductors. The article discusses how the increasing power consumption of computers, especially with the growth of AI, is becoming unsustainable for our planet. Superconductors, which operate without energy dissipation at cryogenic temperatures, offer a potential solution to drastically reduce energy consumption in computing.

By achieving virtually zero-resistance interconnects, minimal energy usage for digital logic, and increased computing density through 3D chip stacking, superconductors present a promising alternative to traditional computing methods. Research at Imec has shown that superconducting computers become more power-efficient than classical computers at a scale equivalent to today's high-performance systems. 

Imec has developed superconducting processing units that can be produced using standard CMOS tools, making them a hundred times more energy-efficient than current chips. This advancement could lead to a computer that can pack a data center's computing power into a system the size of a shoebox, revolutionizing the field of energy-efficient computation.

The discussion on the submission about putting a data center in a shoebox using superconductors on Hacker News covers various technical aspects and challenges related to superconducting computing. Some users highlighted the potential energy efficiency and drastic reduction in power consumption offered by superconductors, while others raised concerns about practical issues such as cooling, interconnects, and signal loss in superconducting systems. 

A user pointed out the technical features of the superconducting memory and processing units and how they differ from traditional CMOS-based systems. Another user discussed the limitations of existing superconducting systems in terms of large dimensions and high current requirements. 

Furthermore, there were discussions about the comparison between superconducting computing and quantum computing, as well as the implications of Landauer's principle in energy consumption. Some users delved into the complexities of superconducting technology, such as handling AC signals, signal integrity, and the challenges of achieving constant current flow in superconducting circuits. 

Overall, the discussions touched on various technical intricacies, performance comparisons, and practical considerations of superconducting computing, showcasing a mix of excitement for the potential advancements and skepticism regarding the implementation challenges.

### Legal models hallucinate in 1 out of 6 (or more) benchmarking queries

#### [Submission URL](https://hai.stanford.edu/news/ai-trial-legal-models-hallucinate-1-out-6-or-more-benchmarking-queries) | 209 points | by [rfw300](https://news.ycombinator.com/user?id=rfw300) | [227 comments](https://news.ycombinator.com/item?id=40538019)

Artificial intelligence (AI) tools are rapidly reshaping the legal landscape, but a new study from Stanford RegLab and HAI researchers reveals a concerning issue - hallucinations. These AI tools, used for tasks like legal research and document drafting, have shown a tendency to generate false information. Even specialized legal AI tools from prominent providers like LexisNexis and Thomson Reuters exhibit a significant rate of hallucinations, with errors ranging from 17% to over 34% of the time.

The study, focusing on benchmarking AI in the legal field, highlights the risks associated with relying on AI for critical legal work. The researchers designed a challenging dataset of legal queries to test the performance of these tools, uncovering instances where the AI-generated responses were either incorrect or inaccurately cited legal sources.

The implications of these hallucinations in legal AI tools go beyond mere inaccuracies; they could potentially mislead users into making incorrect legal judgments. The study underscores the need for more robust evaluations and transparency in the development and deployment of AI tools in the legal domain to ensure their reliability and trustworthiness.

The discussion on the submission regarding AI tools in the legal domain raises several interesting points. Users like 'Drakim' compare the functioning of Google Maps with AI tools like LLMs, highlighting how AI processes information differently from humans. They discuss the nuances of truth and falsity in AI-generated content and the importance of understanding how these systems operate. 'srfngdn' points out the limitations of LLMs in discerning truth and the challenges they pose in real-world applications.

The conversation delves into the complexities of AI hallucinations, emphasizing the need for accurate and reliable AI tools in critical domains such as law. Users like 'bsch' express concerns about the potential impact of false information generated by AI and the importance of verifying sources. 'stalked_why' contributes to the discussion by questioning the intelligence of LLMs compared to human intelligence, prompting a debate on the nature of intelligence and the capabilities of AI.

Amidst debates on cognitive tasks and the reliability of AI-generated content, users like 'gwrn' and 'tmr' discuss the conditioning of AI models, the delineation of truth and falsity in AI outputs, and the challenges in predicting human behavior. The conversation delves into the intricacies of AI technology, emphasizing the importance of understanding and improving these tools for accurate and ethical use in various domains.

### Man scammed after AI told him fake Facebook customer support number was real

#### [Submission URL](https://www.cbc.ca/news/canada/manitoba/facebook-customer-support-scam-1.7219581) | 256 points | by [deviantintegral](https://news.ycombinator.com/user?id=deviantintegral) | [138 comments](https://news.ycombinator.com/item?id=40536860)

A Winnipeg man fell victim to a scam after being misled by an AI tool into believing a fake Facebook customer support number was legitimate. Dave Gaudreau ended up losing hundreds of dollars when he called the fraudulent hotline and unwittingly gave scammers access to his Facebook account. Despite receiving reassurance from the "Meta AI" search tool that the number was valid, Gaudreau soon realized he had been duped when the scammers attempted to make unauthorized purchases using his accounts. Fortunately, Gaudreau took swift action by cancelling his credit cards, locking his bank accounts, and reporting the incident to authorities. He also managed to get the fraudulent charges reversed with the help of PayPal. The ordeal serves as a cautionary tale about the dangers of trusting AI blindly and the importance of verifying information independently to avoid falling prey to scams.

The discussion on Hacker News revolves around the misuse of phone numbers for customer support services by big companies like Facebook. Some users point out that traditional phone support systems are becoming obsolete and suggest alternative solutions like small claims court or customer-centric credit reporting services. Others criticize companies for not providing direct human support and relying on AI or automated systems instead. There is also a debate about the limitations and risks of AI tools like LLMs (large language models) in providing accurate information and the importance of understanding their capabilities. The conversation delves into the complexities of AI development, the need for better technical understanding, and the potential societal and environmental impacts of relying heavily on AI technologies.