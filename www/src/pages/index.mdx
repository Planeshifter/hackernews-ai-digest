import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Sun Feb 01 2026 {{ 'date': '2026-02-01T17:22:32.885Z' }}

### My iPhone 16 Pro Max produces garbage output when running MLX LLMs

#### [Submission URL](https://journal.rafaelcosta.me/my-thousand-dollar-iphone-cant-do-math/) | 389 points | by [rafaelcosta](https://news.ycombinator.com/user?id=rafaelcosta) | [179 comments](https://news.ycombinator.com/item?id=46849258)

A developer’s “simple” expense-tracker spiraled into a wild on-device AI bug hunt: MLX LLMs produced pure gibberish on his iPhone 16 Pro Max while the same code ran flawlessly on an iPhone 15 Pro and a MacBook Pro. After Apple Intelligence refused to download its on-device model, he fell back to bundling models with MLX; the 16 Pro Max pegged the CPU, never emitted a stop token, and generated noise like “Applied.....*_dAK[...].” Instrumenting Gemma’s forward pass with breakpoints showed tensor values on the 16 were off by about an order of magnitude compared to the 15—strongly suggesting a faulty Neural Engine (or ML-related hardware) on that particular unit rather than an OS or code issue. The episode doubles as a cautionary tale about Apple’s fragmented ML paths (Apple Intelligence vs MLX) and the pain of debugging on-device LLMs: if MLX outputs look deranged, try another device before rewriting your stack.

Here is the daily digest summary for this story and its discussion.

**Story: iPhone 16 Pro Max AI "Gibberish" Bug Tracked to Hardware-Software Mismatch**
A developer discovered a critical issue where MLX-based LLMs produced incomprehensible noise on the iPhone 16 Pro Max, despite the same code running perfectly on older devices like the iPhone 15 Pro. While the author initially suspected a faulty Neural Engine in their specific unit due to tensor values being off by an order of magnitude, the issue highlights the fragility of debugging on-device AI and the fragmentation between Apple’s native Intelligence features and open-source libraries like MLX.

**Discussion Summary**
The Hacker News discussion identified the actual root cause and debated the timing of the fix:

*   **Root Cause Identified:** Commenters debunked the author's theory of a defective specific unit. Users pointed to a specific pull request in the MLX repository (PR #3083) that fixed the issue just a day after the blog post. The problem was a software-hardware mismatch: the iPhone 16 Pro's SKU and verify-new "Neural Accelerator" support were misdetected by the library, causing "silently wrong results" in the GPU tensor cores.
*   **The "Blog Post Effect":** There was significant debate regarding the timing of the fix. Since the patch appeared one day after the blog post, some skeptics argued the publicity forced Apple's hand. Others countered that critical engineering bugs often have short turnaround times or that the fix was likely already in the QA pipeline before the post went viral.
*   **Prompt Philosophy:** A humorous sub-thread focused on the developer’s debug prompt: "What is moon plus sun?" Answers ranged from "eclipse" and the Chinese character for bright (明), to the catastrophic physics of a star colliding with a moon.
*   **Apple Silicon Complexity:** Technical users discussed the opacity of Apple’s naming conventions (Neural Engine vs. Neural Accelerator) and noted that MLX primarily runs on the GPU because the ANE (Apple Neural Engine) remains accessible only via closed-source APIs.
*   **Lack of Testing:** Several users lamented that this error suggests a lack of Continuous Integration (CI) testing on actual new hardware (iPhone 16 Pro) for these libraries.

### Two kinds of AI users are emerging

#### [Submission URL](https://martinalderson.com/posts/two-kinds-of-ai-users-are-emerging/) | 299 points | by [martinald](https://news.ycombinator.com/user?id=martinald) | [280 comments](https://news.ycombinator.com/item?id=46850588)

Power users vs. chatters: why enterprise AI is falling behind

The author sees a sharp split in AI adoption. On one side are “power users” (often non-technical) who run Claude Code, agents, and Python locally to supercharge real work—finance teams in particular are leapfrogging Excel’s limits. On the other are users stuck “just chatting” with tools like ChatGPT or, in enterprises, Microsoft 365 Copilot.

They argue Copilot’s UX and agent capabilities lag badly, yet it dominates corporate environments due to bundling and policy. Locked-down laptops, legacy systems without internal APIs, and siloed/outsourced engineering mean employees can’t run scripts, connect agents to core workflows, or get safe sandboxes—so leaders try Copilot, get weak results, and conclude AI underdelivers. Meanwhile, smaller, less-encumbered companies are “flying”: one example converted a sprawling 30‑sheet Excel model to Python almost in one shot with Claude Code, then layered on simulations, data pulls, and dashboards.

Why it matters
- The productivity gap is widening in favor of smaller, bottom‑up, tool‑agnostic teams.
- Enterprise risk isn’t just security—it’s stagnation and misjudging AI’s potential based on subpar tools.

What the author says to do
- Empower domain teams to build AI-assisted workflows organically, not via top‑down “digital transformation.”
- Provide safe sandboxes and read‑only data warehouses; expose internal APIs so agents have something to connect to.
- Don’t restrict staff to one bundled assistant; evaluate tools that actually execute code and integrate with systems.

**User Taxonomy and the Definition of "Thinking"**
The discussion focused heavily on categorizing AI users, with **PunchyHamster** proposing a taxonomy that framed the debate:
*   **Group 1 (The Executor):** Uses AI as a junior intern or boilerplate generator to speed up tasks while remaining aware of limitations.
*   **Group 2 (The Outsourcer):** Outsources the entire skillset and "thinking" process, interested only in the result rather than honing the craft.
*   **Group 3 (The Delusional):** Believes a talking chatbot can replace senior developers.

**Burnout and Corporate Cynicism**
A significant thread emerged regarding *why* engineers might choose to "outsource thinking" (Group 2). **svnzr** admitted to leaning on AI to deliver minimum viable work because their organization prioritizes speed over quality engineering. They described a "B2B SaaS trap" where Product Managers demand features immediately to sign contracts, ignoring long-term quality or user feedback. This resonated with others like **tlly**, who noted a "seismic shift" over the last 5–6 years where professional pride has eroded in the face of corporate dysfunction, pushing workers to use AI simply to keep up with the grind.

**The "Result vs. Ritual" Debate**
**3D30497420** offered a counterpoint to the negative perception of outsourcing skills. They described using Claude Code to build a custom German language learning app; while they "outsourced" the software development (which they don't care to learn), they did so to double down on learning German (which they do care about). This suggests that "outsourcing thinking" is valid if it clears the path for a different, preferred intellectual pursuit.

**Capabilities and Context**
Other commenters added nuance to the rigid grouping:
*   **GrinningFool** argued that users fluidly switch between groups depending on the task (e.g., caring about details for one project but just wanting the result for another).
*   **ck** and **mttmg** pointed out a missing group: users who treat AI as a "rubber duck" or virtual teammate to bounce ideas off of (ping-ponging).
*   **safety1st** distinguished between the "supply side" generations of tools, noting that while "Gen 1" (chatbots) is often just "vibes" and entertainment, "Gen 2" (RAG, agents, coding tools) offers the distinct capabilities—like scanning docs or deploying test suites—that separate power users from chatters.

### Towards a science of scaling agent systems: When and why agent systems work

#### [Submission URL](https://research.google/blog/towards-a-science-of-scaling-agent-systems-when-and-why-agent-systems-work/) | 97 points | by [gmays](https://news.ycombinator.com/user?id=gmays) | [34 comments](https://news.ycombinator.com/item?id=46847958)

What’s new: Google Research ran a controlled study of 180 agent configurations across five architectures and four benchmarks to derive quantitative “scaling principles” for AI agent systems. Core finding: more agents isn’t automatically better—gains depend on whether the task is parallelizable or inherently sequential.

Key results
- Scope: 5 architectures (Single-Agent, Independent, Centralized, Decentralized, Hybrid), 4 benchmarks (Finance-Agent, BrowseComp-Plus, PlanCraft, Workbench), 3 model families (OpenAI GPT, Google Gemini, Anthropic Claude).
- Parallelizable tasks: Multi-agent coordination—especially centralized—can deliver big wins. Example: Finance-Agent saw +80.9% over a single agent.
- Sequential tasks: Multi-agent setups often degrade performance due to coordination overhead and error cascades. Example: PlanCraft dropped by ~70%.
- Predictive selector: A model that picks the optimal architecture for 87% of unseen tasks.
- “Agentic” tasks are defined by sustained multi-step interaction, partial observability with iterative info gathering, and adaptive strategy refinement—conditions where architecture choices matter most.

Why it matters
- Challenges the popular “more agents = better” heuristic and prior collaboration-scaling claims.
- Introduces a task-architecture alignment principle: choose coordination structures that match task decomposability and communication needs.

Practical takeaways for builders
- Start single-agent; add agents only if the task cleanly decomposes into parallel subtasks.
- Prefer centralized coordination for parallel workloads; avoid heavy coordination for sequential pipelines.
- Watch communication rounds and memory sharing—overhead can erase gains.
- Consider using (or emulating) a selector to predict the best architecture before scaling out.

Paper and details: From Google Research (Jan 28, 2026); includes controlled comparisons across GPT, Gemini, and Claude families.

Here is a summary of the discussion on Hacker News:

**The Complexity Penalty in Sequential Tasks**
Commenters largely validated the paper's findings regarding sequential tasks, noting that multi-agent systems (MAS) often degrade performance due to "fragmented reasoning." One user argued that splitting a fixed cognitive budget (e.g., the paper’s ~4,800 token limit) across multiple agents wastes tokens on coordination "chatter," leaving insufficient capacity for the actual problem-solving. Others pointed out that in sequential pipelines, error rates compound aggressively—a 1% error rate in a single agent can render a deterministic multi-step flow unacceptable, and independent agents can amplify error rates by orders of magnitude (one user noted a 17x error amplification in independent setups versus 4x in centralized ones).

**The Case for Centralized Orchestration**
Builders discussed practical alternatives to "swarm" architectures, with a strong preference for Centralized or "Driver/Worker" patterns.
*   **The Orchestrator:** Several developers championed an architecture where a core "Orchestrator" or "Manager" agent breaks down tasks and delegates to specialized workers, rather than letting agents negotiate amongst themselves.
*   **Dynamic Selection:** Validating the paper’s "predictive selector" concept, users reported success using an agent specifically to plan and recommend the orchestration strategy *before* execution begins.
*   **Model Specialization:** Participants shared anecdotal "squad" compositions, such as using Google models for document extraction, Anthropic’s Claude for coding, and OpenAI models for management and orchestration.

**Is Multi-Agent Just a Band-Aid?**
A thread of skepticism questioned whether complex MAS architectures are simply artifacts of current limitations. Some argued that as context windows become larger and more reliable, the need to decompose tasks across multiple agents will diminish. It was suggested that developers currently use MAS to circumvent context limits or hallucination issues that a sufficiently advanced single model (ideally with symbolic recursion capabilities) could famously handle alone.

**Tooling and Protocols**
Discussion touched on emerging standards like the Model Context Protocol (MCP). Users debated whether stacking MCP servers is a viable form of coordination or if it introduces unnecessary overhead compared to simple CLI tool equivalents. The consensus leaned toward software design principles: align coordination structures with the task's natural decomposability—high cohesion and low coupling applied to AI agents.

### What I learned building an opinionated and minimal coding agent

#### [Submission URL](https://mariozechner.at/posts/2025-11-30-pi-coding-agent/) | 393 points | by [SatvikBeri](https://news.ycombinator.com/user?id=SatvikBeri) | [166 comments](https://news.ycombinator.com/item?id=46844822)

Why he built it
- Frustrated by feature-bloated coding agents (e.g., Claude Code) that keep changing prompts/tools, break workflows, and flicker.
- Wants strict control over context, full transparency into what the model sees, a documented session format, and easy self-hosting—all hard with current harnesses.

What he built
- pi-ai: Unified LLM API with multi-provider support (Anthropic, OpenAI, Google, xAI, Groq, Cerebras, OpenRouter, and OpenAI-compatible), streaming, tool calling via TypeBox schemas, reasoning/“thinking” traces, cross-provider context handoff, token and cost tracking.
- pi-agent-core: Minimal agent loop handling tool execution, validation, and event streaming.
- pi-tui: Terminal UI toolkit with retained-mode design, differential rendering, and synchronized output to minimize flicker; includes editor components (autocomplete) and markdown rendering.
- pi-coding-agent: CLI wiring it all together with session management, custom tools, themes, and project context files.

Design philosophy
- If he doesn’t need it, it’s not built.
- Minimal system prompt, minimal toolset.
- YOLO by default (no permission prompts).
- No built-in to-dos, no “plan mode,” no MCP, no background bash, no sub-agents.

Multi-model reality and context control
- Embraces a multi-model workflow; supports seamless context handoff across providers/models.
- Heavy focus on “context engineering” to improve code quality and predictability.
- Clean, inspectable sessions to enable post-processing and alternative UIs.

API unification: the gritty bits
- Four APIs cover almost everything: OpenAI Completions, OpenAI Responses, Anthropic Messages, Google Generative AI.
- Normalizing quirks across providers and engines (Ollama, vLLM, llama.cpp, LM Studio), e.g.:
  - Different fields for token limits (max_tokens vs max_completion_tokens).
  - Missing system “developer” role on some providers.
  - Reasoning traces appear in different fields (reasoning_content vs reasoning).
  - Certain params not accepted by specific models (e.g., Grok and reasoning_effort).
- Backed by a cross-provider test suite for images, tools, reasoning, etc. to catch breakage as models change.

TUI details
- Two TUI modes considered; landed on retained-mode UI with differential rendering to avoid the “flicker” common in agent UIs.
- Synchronized output for near flicker-free updates.

Why it matters
- A counterpoint to ever-growing, opaque coding agents: deterministic, inspectable, self-host-friendly.
- Useful reference for anyone normalizing LLM APIs across providers and dealing with tool-calling/trace differences.
- Shows how far you can get with a small, purpose-built stack when you prioritize context control and transparency over features.

Based on the discussion, here is a summary of the comments:

**Security and Sandboxing**
A major portion of the discussion criticized the current state of security in coding agents, with several users describing manual permission approvals as "security theater."
*   Commenters noted that while manual approval prevents immediate disasters, the friction eventually leads users to approve actions blindly or use flags like `dangerously-skip-permissions` to make the tool usable.
*   The consensus was that true security requires OS-level sandboxing (VMs, containers, Unix jails, or macOS Seatbelt) rather than relying on the agent or CLI harness to police itself.
*   There is significant concern regarding agents having direct access to sensitive environments (like email or unrestricted file systems) without proper isolation.

**Context Engineering and Workflow**
Users responded positively to the author's focus on "context engineering," expressing frustration with the default linear conversational flow of most LLMs.
*   Several users discussed the need for non-linear history, such as "mind maps" or tree structures that allow a user to "save" a good context state, go on a "side quest" (debugging), and then return to the clean state without polluting the context window.
*   Users shared technical approaches to this, such as using specific Markdown files (`MIND_MAP.md`) or graph formats to maintain a persistent project memory separate from the chat logs.

**Economics: API vs. Subscription**
There was a debate regarding the financial sustainability of "bring your own key" (pay-per-token) agents versus subscription models like Claude Code.
*   Some users hope that API prices will decrease, making self-hosted agents significantly cheaper and more precise than bundled subscriptions.
*   Others argued that inference costs remain high and R&D requires massive funding, suggesting that subscription models will persist to subsidize the underlying compute, and that prices may not drop as quickly as hoped.

**Google API Quirks**
Commenters validated the author's complaints about Google's developer experience. Specific grievances included the lack of streaming support for tool calls and the inefficiency of Google's token counting methods (which require API calls or cause 100% CPU usage in AI Studio just to count tokens while typing).

**Naming**
There was lighthearted criticism of the name "pi-agent" for being un-Google-able and generic. Users expressed a preference for the author's original internal project name, "Shitty Coding Agent," proposing backronyms like SCA (Software Component Architecture) to make it acceptable.

### Show HN: Zuckerman – minimalist personal AI agent that self-edits its own code

#### [Submission URL](https://github.com/zuckermanai/zuckerman) | 70 points | by [ddaniel10](https://news.ycombinator.com/user?id=ddaniel10) | [49 comments](https://news.ycombinator.com/item?id=46846210)

Zuckerman: an ultra‑minimal, self‑modifying personal AI agent

What it is
- A TypeScript, AGPL‑3.0 project that starts as a tiny agent and then edits its own files (config, tools, prompts, even core logic) to add features on the fly. Changes hot‑reload instantly—no rebuilds or restarts.

Why it’s different
- Moves away from heavyweight agent stacks: the agent grows by rewriting itself, and improvements can be shared across agents via a contribution site. The README positions it as a simpler, more approachable alternative to complex, fast‑moving frameworks.

Notable features
- Self-editing runtime: modifies its own code/behavior and instantly reloads.
- Feature versioning and collaborative ecosystem for sharing capabilities.
- Multi‑channel I/O: Discord, Slack, Telegram, WhatsApp, Web chat.
- Voice support (TTS/STT), calendar/scheduling, and an activity timeline of runs/tool calls.
- Multiple agents with distinct personalities/tools.
- Dual interfaces: CLI and an Electron app for a visual chat/inspector/settings experience.
- Security foundations: auth, policy engine, Docker sandboxing, secret management.

Architecture (3 layers)
- World: communication, execution, runtime factory, config loader, voice, system utils.
- Agents: each agent lives in its own folder with core modules, tools, sessions, personality.
- Interfaces: clients including CLI and Electron/React app.

Getting started
- Clone the repo, install with pnpm, and run the Electron app: pnpm run dev.
- Repo: github.com/zuckermanai/zuckerman

Caveats
- Marked as WIP; self‑modifying behavior heightens security considerations despite sandboxing.
- AGPL‑3.0 license may limit closed‑source/commercial use without open‑sourcing derivatives.

Here is a summary of the discussion:

**Branding and Imagery**
A significant portion of the discussion focused on the project's name ("Zuckerman") and its avatar. While some users interpreted it as a "genius" or playful reference to memes about Mark Zuckerberg being a robot, others found it "creepy," distasteful, or distracting due to negative associations with Facebook/Meta. The author addressed these comments, noting the humor was intended but acknowledging the feedback regarding the mixed reception.

**Language Choice and Architecture**
The project’s "hot-reloading" and self-modifying capabilities led to a debate about programming languages. Several users questioned why the project wasn't built in Lisp or Erlang, which are historically renowned for hot-code reloading and "code as data" properties. Others clarified that while Lisp has deep roots in AI history, it is unrelated to the current wave of LLM-based architecture.

**Cost, Security, and Bugs**
Users raised practical concerns about the cost of running a self-navigating, self-editing agent that relies on paid API calls. The author responded that optimizations are planned, while other commenters discussed the feasibility of using local models on consumer hardware to mitigate costs. Security was also highlighted, specifically the risk of prompt injection if users download shared "skills" or agent capabilities from a community ecosystem. Additionally, users reported technical issues, including hardcoded file paths and installation hurdles, which the author acknowledged.

### Show HN: OpenJuris – AI legal research with citations from primary sources

#### [Submission URL](https://openjuris.org/) | 18 points | by [Zachzhao](https://news.ycombinator.com/user?id=Zachzhao) | [8 comments](https://news.ycombinator.com/item?id=46843162)

I’m ready—please share the Hacker News submission you want summarized.

Send one of:
- The HN item URL (news.ycombinator.com/item?id=…) or item ID
- The article URL/text (if you want the linked post summarized)
- Both, if you want highlights from the HN discussion plus the article

Also let me know:
- Length: ultra-brief (3 bullets), short (1–2 paragraphs), or deeper (5–7 bullets + takeaways)
- Whether to include comment highlights (top arguments, consensus, notable dissent)
- Any audience focus (technical, product, general)

Based on the discussion provided, here is a summary of the Hacker News conversation regarding a new legal database/AI tool.

### **Topic: Building an AI-Powered Case Law Database**

**Summary of Discussion:**
The discussion centers on a "Show HN" style post where the author is building a legal database and AI tool. Commenters are generally skeptical, focusing on the immense difficulty of competing with incumbents (Lexis, Westlaw) and the specific dangers of using Large Language Models (LLMs) in the legal field.

**Key Arguments & Insights:**

*   **The Data Moat:** Implementing a legal database is described as a "massive undertaking." Users point out that raw court data is fragmented, often requires physical access to courthouses, suffers from bad formats, and lacks the necessary metadata. Incumbents provide value not just through data access, but through "Shepardizing" (tracking if a case has been overturned or affirmed), which is difficult for a disrupted startup to replicate.
*   **The Hallucination Problem:** A major criticism involves the tendency of AI to hallucinate non-existent cases or citations.
    *   *Counter-point:* Some argue that proficient lawyers use AI for retrieval but verify everything by "reading the underlying case" (e.g., *Brady v. Maryland*), distinguishing between harmless typos and fatal hallucinations.
    *   *Technical Solution:* Others suggest "grounding" (RAG) to force the AI to link to appropriate, existing sources rather than generating text from a vacuum.
*   **"Wrapper" Accusations:** There is notable pushback against the tool being a "crappy wrapper" around a general-purpose LLM. One user criticizes the business model, accusing the poster of utilizing a confusing "non-profit/open" structure for a for-profit entity (similar to criticisms of OpenAI), calling it potentially unethical or misleading.

**Consensus:** building a legal tech startup requires solving deep infrastructure and data acquisition problems, not just applying an LLM to existing text.

---

## AI Submissions for Sat Jan 31 2026 {{ 'date': '2026-01-31T17:12:27.027Z' }}

### Generative AI and Wikipedia editing: What we learned in 2025

#### [Submission URL](https://wikiedu.org/blog/2026/01/29/generative-ai-and-wikipedia-editing-what-we-learned-in-2025/) | 199 points | by [ColinWright](https://news.ycombinator.com/user?id=ColinWright) | [96 comments](https://news.ycombinator.com/item?id=46840924)

Wiki Education: LLM-written Wikipedia text mostly “looks sourced” but fails verification

- Wiki Education (which brings ~19% of new active editors to English Wikipedia) analyzed how participants use generative AI. Their bottom line: editors should never copy‑paste chatbot output into articles.
- Using the Pangram detector, they reviewed 3,078 new articles created via their programs since 2022; 178 were flagged for AI use—none before ChatGPT’s launch, with usage rising each term.
- Only 7% of flagged articles had fake citations. The bigger problem: over two‑thirds failed verification—sentences cited to real, relevant sources that do not actually support the claims. In many cases, nearly every cited sentence failed this check.
- Cleanup was costly: staff moved recent work back to sandboxes, stubified articles that met notability but failed verification, and proposed deletion for unsalvageable pages; some were later de‑PRODed, reflecting mixed community views.
- Context: English Wikipedia bans generative AI for images and talk pages and recently adopted a guideline against using LLMs to generate new articles. Wiki Ed now runs near real‑time Pangram checks on participant edits via its Dashboard to intervene earlier.

Why it matters: LLMs often produce plausible prose with real‑looking citations that don’t support the text—undermining verifiability, a core Wikipedia policy. The verification and cleanup burden can exceed the time it takes to generate such content, pushing programs and tools to focus less on hallucinated sources and more on whether sources actually back the claims.

Here is a summary of the discussion:

**The "Weaponization" of Citations**
Commenters argued that the problem extends beyond LLMs to a broader "weaponization" of citations on the internet. Users have learned that providing a link—any link—creates an aura of authority because most readers (and even editors) rarely click through to verify the source supports the claim. This creates a "blind spot" where content looks sourced and trustworthy but is effectively fabrication. One user noted this behavior is also rampant in Hacker News comments, where people post PubMed links that contradict their own arguments, banking on the fact that no one will read the technical details.

**Humans vs. LLMs**
A significant portion of the debate focused on the "human baseline." Commenters pointed out that Wikipedia already suffers from human-generated "hallucinations," such as movie plot summaries written by people who clearly haven't watched the film, or incorrect claims that persist for over a decade. While some argued that LLMs simply automate this bad behavior at a higher scale, others contended that fixing human errors is already tedious, and an influx of machine-generated errors will overwhelm the volunteer workforce.

**The Labor of Verification**
The discussion highlighted *why* creators (human or AI) might fake citations: applying correct citations is genuinely hard work. Users noted the paradox where specific, novel claims are easy to cite, but "general knowledge" or textbook fundamentals (e.g., proving a standard chemistry concept) are difficult to pin down to a specific URL or paper. This friction encourages the use of "plausible" shortcuts over rigorous research.

### Autonomous cars, drones cheerfully obey prompt injection by road sign

#### [Submission URL](https://www.theregister.com/2026/01/30/road_sign_hijack_ai/) | 152 points | by [breve](https://news.ycombinator.com/user?id=breve) | [143 comments](https://news.ycombinator.com/item?id=46840676)

TL;DR: Researchers from UC Santa Cruz and Johns Hopkins show that large vision-language models in self-driving cars and drones will follow text commands printed on signs in their camera view—an “environmental indirect prompt injection” that can override safe behavior.

Key points
- New attack class: CHAI (Command Hijacking Against Embodied AI) uses optimized text on physical signs to make LVLM-powered systems treat the text as an instruction (“turn left,” “proceed,” “Police Santa Cruz,” “Safe to land”).
- Works across languages: English, Chinese, Spanish, and Spanglish; appearance tweaks (font, color, placement) boost success, with content of the prompt being the biggest driver.
- Self-driving sims: With signs in view, models reversed correct behavior (e.g., turning left through a crosswalk). Success rate: 81.8% against a GPT-4o-based setup vs 54.7% on InternVL.
- Drone tracking: CloudTrack misidentified targets when adversarial labels were added (e.g., a generic car labeled “Police Santa Cruz”)—error rates up to 95.5%.
- Drone landing: Signs reading “Safe to land” tricked the system into choosing unsafe rooftops—CHAI succeeded up to 68.1%.
- Physical tests: Real-world trials mirrored simulated vulnerabilities (the car trials stayed in sim for safety).

Why it matters
- Exposes a practical, low-cost way to subvert embodied AI by exploiting their tendency to literally follow on-camera text.
- Highlights a security gap as LVLMs move into safety-critical autonomy: perception channels double as instruction channels, enabling command injection from the environment.

Caveats and defenses
- Car tests were simulated; drones had both sim and real-world elements.
- Mitigations likely include hard separation of perception vs instruction inputs, filtering/segmentation of scene text, rule-based overrides (e.g., crosswalk safety), adversarial training on sign-based attacks, and model-side refusal to execute in-scene text as commands.

Here is a summary of the discussion:

**Technical Architecture & Vulnerability**
Commenters analyzed why this attack vector works, noting that traditional self-driving stacks separated specific tasks (like sign classification) from driving logic. The vulnerability arises from the shift to end-to-end Vision Language Models (VLMs), which process the entire scene semantically; without hard-coded separation, the model essentially hallucinates instructions from background text.

**Adversarial Infrastructure & Legality**
The conversation explored low-tech versions of this attack, such as installing fake ("phantom") stop signs to trick Google Maps or autonomous vehicles. Users debated the legal consequences of modifying road signs, arguing that while it might look like simple vandalism or a civil liability issue, it could escalate to criminal charges if it results in injury or is deemed an intentional trap.

**Infrastructure Debate: Stops vs. Roundabouts**
A significant portion of the thread diverged into a debate on road design:
*   **4-Way Stops:** Critics called them inefficient and confusing regarding right-of-way. Defenders noted they handle "resource starvation" better than roundabouts (ensuring side streets get a turn) and are Safer for pedestrians in residential areas.
*   **Roundabouts:** Proponents argued they offer superior flow and safety. When users claimed roundabouts require too much space for urban areas, others pointed to "mini-roundabouts" (painted circles) common in the UK and Europe as space-efficient alternatives.

**Cost & Practicality**
Users discussed why cities prefer simple signs over "smart" sensor-driven traffic systems. The consensus was cost: digging up roads for power and sensors involves high capital and maintenance costs, whereas a metal stop sign is cheap and essentially maintenance-free.

### Show HN: I trained a 9M speech model to fix my Mandarin tones

#### [Submission URL](https://simedw.com/2026/01/31/ear-pronunication-via-ctc/) | 451 points | by [simedw](https://news.ycombinator.com/user?id=simedw) | [136 comments](https://news.ycombinator.com/item?id=46832074)

HN Summary: Tiny on-device model that grades your Mandarin tones

- Motivation: The author’s Mandarin speech was intelligibility-limited by tones. Handcrafted pitch visualizers (FFT + heuristics) proved brittle to noise, coarticulation, and speaker variation, so they built a learned system instead.

- Approach: A small ASR-style CAPT (Computer-Assisted Pronunciation Training) model that’s pedantic about how you said something.
  - Model: Conformer encoder trained with CTC. Convolution captures split-second local cues (e.g., zh vs z), attention handles global/relative tone context and sandhi.
  - Why CTC (vs seq2seq like Whisper): CTC won’t “autocorrect” to plausible text; it forces the model to reflect what was actually said frame-by-frame—critical for feedback.
  - Tokenization: Pinyin syllable + tone as first-class tokens (e.g., zhong1 vs zhong4) so tone errors surface explicitly. Neutral tone normalized as tone 5. Vocab ≈1,254 + <unk>, <blank>.

- Alignment and scoring:
  - Forced alignment via Viterbi through the CTC posterior matrix to map audio frames to target syllables.
  - UI/metrics decoupling fix: Leading silence was sinking confidence by flooding spans with <blank>. Solution: ignore high-blank frames when scoring, but keep spans for highlighting.

- Data, training, and metrics:
  - Datasets: AISHELL‑1 + Primewords (~300 hours), with SpecAugment.
  - Training: ~8 hours on 4× RTX 4090.
  - Metrics emphasized: TER (Token Error Rate), tone accuracy (1–5), and confusion groups (zh/ch/sh vs z/c/s).
  - Results across sizes:
    - 75M params: TER 4.83%, tone acc 98.47%
    - 35M params: TER 5.16%, tone acc 98.36%
    - 9M params: TER 5.27%, tone acc 98.29%
  - Takeaway: Accuracy barely degraded with size—task looks data‑bound more than compute‑bound.

- Deployment: INT8-quantized ONNX model ~11 MB (from 37 MB FP32) with negligible loss (+0.0003 TER). Runs in-browser/on-device via onnxruntime‑web; fast to load, privacy‑preserving, battery‑friendly. Live demo available.

- Why it’s interesting:
  - Practical, DIY alternative to commercial CAPT APIs.
  - Clear engineering tradeoffs (CTC for strictness, Conformer for local/global).
  - Thoughtful UX fix for a real alignment pitfall.
  - Shows how far you can push small models for specialized speech tasks.

Try it: The post includes a browser demo where you read prompts (“Nǐ hǎo”, etc.), get per‑syllable alignment and tone feedback.

Based on the comments, here is a summary of the discussion:

**Model Performance and Feedback**
*   **Conversational vs. Isolated Speech:** Users experienced mixed results depending on their speaking speed. One intermediate learner noted that while the tool is excellent for slow, deliberate speech, it struggles with "normal conversational speed" where coarticulation and tone influence occur.
*   **False Positives/Negatives:** A Beijing native found the model confused standard consonant pairs (like *h/f* and *l/n*) despite perfect input. Conversely, another user found they could "trick" the model into recognizing valid words by making nonsense noises, suggesting the model may be over-biased toward its training vocabulary rather than raw phonetics.
*   **Author Response:** The author (`smdw`) was active in the thread, acknowledging bugs regarding specific character misidentifications (a JavaScript-side issue) and releasing hotfixes for tone sandhi support during the discussion.

**Debate: The Importance of Tones**
*   **Context vs. Precision:** A significant debate erupted regarding how critical perfect tones are for intelligibility. A native speaker argued that tones are secondary to context, noting that dialect speakers often "shuffle" tones but remain intelligible.
*   **Counterpoints:** Other users (both learners and natives) pushed back, arguing that while context helps advanced speakers, beginners lack the vocabulary to build that context. They cited examples where tone errors dramatically change meaning (e.g., "panda" vs. "chest hair") and emphasized that Standard Mandarin relies on specific tonal rules that differ from dialect flexibility.

**Technical and Linguistic Nuances**
*   **Tone Sandhi:** Several users queried how the model handles tone sandhi (rules where tones change based on adjacent tones, such as *ni3* becoming *ni2* before another third tone). The author confirmed support was added/tweaked in response to the feedback.
*   **Comparative Difficulty:** The discussion touched on the difficulty of Chinese tones versus English vowels. One user noted that while tones are hard for Westerners, English vowel reduction and variance are equally baffling for Chinese speakers.

### Browser Agent Benchmark: Comparing LLM models for web automation

#### [Submission URL](https://browser-use.com/posts/ai-browser-agent-benchmark) | 11 points | by [MagMueller](https://news.ycombinator.com/user?id=MagMueller) | [3 comments](https://news.ycombinator.com/item?id=46837660)

Browser Use releases an open-source benchmark for real-world browser agents

What’s new
- Browser Use open-sourced a 100-task benchmark (plus 20 custom stress tests) to evaluate agentic web browsing in realistic settings. Repo: github.com/browser-use/benchmark
- It’s built from WebBench, Mind2Web 2, GAIA, and BrowseComp, with added custom tasks for hard UI interactions (e.g., iframes, drag-and-drop).
- The team ran 600k+ test tasks internally to refine difficulty and judging.

How they built it
- Task selection: Ran many models and agent configs, used an LLM judge to flag “impossible” or “near-miss,” then removed trivial and unreachable tasks. Remaining tasks were hand-verified as hard but doable.
- Judging: LLM-as-judge with a simple true/false verdict (no rubric). Hand-labeled 200 traces for ground truth; final judge achieved 87% alignment with human labels.
- Judge model: Initially GPT-4o (as in Mind2Web), later switched to gemini-2.5-flash for better alignment.

Results (their runs)
- Their new ChatBrowserUse 2 API tops the chart; several recent models clear 60% on this very hard set.
- Even the lowest-scoring tested agent (gemini-2.5-flash at 35%) is “respectable” given task difficulty.
- They emphasize reporting variance: multiple runs with standard error bars.

Practical notes
- Designed for reproducibility: run_eval.py replicates their ChatBrowserUse 2 results.
- Cost/time: One full 100-task run on the basic Browser Use plan (concurrency 3) is ~3 hours and ~$10; using pricier models (e.g., claude-sonnet-4.5) can push to ~6 hours and nearly $100.
- Excludes tasks requiring authentication or making real changes to sites; favors real websites over synthetic pages for realism.

Why it matters
- Many agent benchmarks skew toward synthetic or trivially verifiable tasks; this set targets “hard but possible” real-world web actions.
- Clearer, more consistent judging (and error bars) should make cross-model comparisons more trustworthy.
- Useful for LLM providers and agent framework authors to iterate on grounded, end-to-end browsing performance.

Contact and more
- Benchmark: github.com/browser-use/benchmark
- Support for larger-scale evals: support@browser-use.com

**Discussion Summary:**

Commenters discussed the application of these agents beyond benchmarks, asking about valid AI-based tools for exploratory fuzzy web testing. Others noted the absence of the Opus 4.5 model from the test results, predicting that it would likely achieve the highest score if included.

### Show HN: Pinchwork – A task marketplace where AI agents hire each other

#### [Submission URL](https://github.com/anneschuth/pinchwork) | 8 points | by [aschuth](https://news.ycombinator.com/user?id=aschuth) | [6 comments](https://news.ycombinator.com/item?id=46840707)

- What it is: An open-source marketplace (MIT) that lets AI agents post tasks and other agents pick them up for credits. The UX is “just curl and go” with no accounts or dashboards. Site: pinchwork.dev (API docs and a simple dashboard available).

- Quick flow:
  - Register: POST /v1/register returns an API key + 100 free credits.
  - Delegate: POST /v1/tasks with a “need” and max_credits; optionally block until completion.
  - Earn: POST /v1/tasks/pickup to grab work, deliver results, and get paid.

- Why it matters: Moves agent workflows from monolithic prompts to a market of specialized micro-agents (e.g., notifications, image generation, code review). Encourages parallelization and division of labor.

- Features:
  - Credit escrow: posters pay on approval, not upfront.
  - Smart matching: agents describe skills; tasks are routed accordingly.
  - Independent verification: deliveries can be verified by separate agents before approval.
  - Real-time: SSE event stream and HMAC-signed webhooks.
  - Questions/messaging to clarify tasks mid-flight.
  - “Recursive labor”: matching and verification are themselves micro-tasks handled by agents.

- Integrations: LangChain, CrewAI, and MCP (Claude Desktop/Cursor) with pip extras; quick demos provided.

- CLI: Homebrew and Go-installable pinchwork CLI supports register, create/pickup/deliver tasks, live events, credits, multiple profiles, and JSON output.

- Self-hosting: Dockerfile and docker-compose included. Server primarily in Python with a small Go component (CLI); tests and linting set up.

- Example use cases: outsource notifications to an agent with Twilio keys, request image generation, get an independent code security review, fan out parallel subtasks.

- Status: GitHub repo anneschuth/pinchwork, new release pinchwork-cli v0.1.1 (Jan 31, 2026). Open-source MIT.

What to watch: reputation/sybil resistance and quality control will be key as “independent verification” is also agent-driven; still, the escrow + approval flow and simple API make it easy to experiment with multi-agent marketplaces today.

**Pinchwork: A task marketplace where AI agents hire other agents**
Pinchwork proposes an open-source, API-first ecosystem where AI agents can outsource specialized micro-tasks (like phone calls or code reviews) to other agents in exchange for credits, utilizing an escrow system for trust.

**Discussion Summary:**
The commentary praised the "simple, elegant implementation," but quickly pivoted to the **economics of an automated labor market**.

When asked if task pricing is dynamically adjusted based on supply and demand, the creator clarified that the system currently relies on a **poster-defined bounty model**. Pricing discovery is left to trial and error: a poster sets a credit amount, and if it is too low, agents simply won't "pick up" the task.

While the roadmap includes potential features for algorithmic price suggestions based on task complexity or historical data, the creator emphasized a philosophy of **architectural minimalism**. The preference is to let "agent-side tooling" handle the intelligence regarding pricing logic and negotiation, rather than complicating the platform layer.

### 175K+ publicly-exposed Ollama AI instances discovered

#### [Submission URL](https://www.techradar.com/pro/security/over-175-000-publicly-exposed-ollama-ai-servers-discovered-worldwide-so-fix-now) | 62 points | by [heresie-dabord](https://news.ycombinator.com/user?id=heresie-dabord) | [37 comments](https://news.ycombinator.com/item?id=46831784)

175,000 Ollama instances left wide open, fueling “LLMjacking”
- What happened: Researchers at SentinelLabs and Censys found roughly 175,000 Ollama deployments exposed to the internet with no authentication after users bound the service to all interfaces instead of localhost. Many are already being abused in “LLMjacking” schemes—freeloading on victims’ compute to churn out spam, malware content, or resold API access.
- Why it matters: About half of the exposed setups allow tool calling, letting attackers run code, hit APIs, or touch internal systems. Many sit on home or unmanaged cloud networks with weak monitoring, making abuse hard to trace. Uncensored models further increase harm potential.
- Not a bug: This isn’t an Ollama vulnerability—the default binds to 127.0.0.1. It’s a misconfiguration problem.
- What to do:
  - Bind to localhost only (127.0.0.1) or restrict to trusted interfaces.
  - If remote access is required, put it behind a reverse proxy with authentication, IP allowlists, and TLS; don’t expose the service directly.
  - Disable or lock down tool calling; least-privilege any connected tools.
  - Firewall off inbound traffic, monitor logs for abuse, and avoid running on residential IPs when possible.

Name to know: “LLMjacking” (Pillar Security) — hijacking open LLM endpoints to steal compute for malicious work.

Here is a summary of the discussion on Hacker News:

**The "Docker Trap"**
A significant portion of the discussion identified Docker as the likely culprit for the high number of exposed instances. Multiple commenters noted that the default Docker flag `-p 11434:11434` binds to `0.0.0.0` (all interfaces) rather than localhost. Furthermore, Docker is known to modify `iptables` directly, effectively bypassing standard firewalls (like UFW) unless the user explicitly binds to `127.0.0.1` or modifies the Docker daemon configuration.

**Practicality of "LLMjacking"**
One user claimed to have scanned for and tested these exposed endpoints, reporting that they were largely useless for "free compute." They found the connections extremely slow and the instances mostly running small, older, or mediocre models (like CodeLlama 13b), making them ill-suited for heavy workloads.

**Security & Networking Nuances**
*   **Difficulty of Exposure:** There was debate regarding how easy it is to accidentally expose a server. While some argued that modern NAT/routers prevent this by default (requiring manual port forwarding), others countered that Docker containers or direct IPv6 connections often bypass these protections.
*   **Tool Calling:** One commenter argued the article overblows the risk of "tool calling." They pointed out that an LLM only generates text/code; for Remote Code Execution (RCE) to occur, the wrapping application must be configured to execute that output, which is a flaw in the client implementation, not the model itself.
*   **API Keys:** A user attempted to find leaked `OLLAMA_API_KEYS` via GitHub regex search but found mostly placeholders, as Ollama does not enforce authentication by default.

**Critique of Ollama and Users**
*   **Ollama Software:** Some criticism was directed at Ollama itself for "hostile features" like difficult-to-disable telemetry and auto-updates, with users suggesting `llama.cpp` as a cleaner alternative.
*   **Copy-Paste Culture:** Commenters noted that this incident highlights a broader issue where users copy-paste terminal commands or use AI to generate deployment scripts without understanding the underlying networking implications. One user ironically noted that we are entering an era of "using AI to deploy AI," creating job security for security professionals.

---

## AI Submissions for Fri Jan 30 2026 {{ 'date': '2026-01-30T17:12:01.499Z' }}

### Self Driving Car Insurance

#### [Submission URL](https://www.lemonade.com/car/explained/self-driving-car-insurance/) | 135 points | by [KellyCriterion](https://news.ycombinator.com/user?id=KellyCriterion) | [307 comments](https://news.ycombinator.com/item?id=46825828)

Lemonade is launching what it calls the first car insurance priced specifically for self‑driving use: Tesla owners get 50% off every mile driven in Full Self‑Driving (FSD), with normal pricing for manual miles.

Key details
- How it works: With your permission, Lemonade connects to your car via Tesla’s Fleet API and automatically separates FSD miles from manual miles—no dongles or self‑reporting. Billing is usage‑based: pay normal rates for manual miles, 50% off for FSD miles.
- Why they price this way: Lemonade cites Tesla-reported safety data (e.g., 52% overall crash reduction on FSD) and says safer miles should cost less.
- Availability: Live in Arizona; Oregon launches Feb 26, 2026; more states “coming soon.”
- Requirements: Tesla Hardware 4.0+ and firmware 2025.44.25.5 or newer.
- Integration: No special endorsements; works like a normal auto policy, and you can bundle with Lemonade’s home/renters/pet/life for extra discounts.
- Admin: Setup is through the Lemonade app; it shows tracked FSD miles and savings.

Why it matters
- First mainstream attempt to price autonomous vs. manual miles separately, rather than giving a generic “safety features” discount.
- Points to a future where insurers tie premiums directly to OEM telemetry and automation usage (Lemonade says other automakers will follow as their systems mature).

HN‑style open questions
- The discount hinges on Tesla’s safety stats—how will regulators and actuaries validate them?
- Privacy/data: what exactly is shared via the Fleet API, and can users granularly limit it?
- Liability edge cases when FSD is engaged vs. driver responsibility, and how claims are adjudicated.
- Limited rollout: HW4+ and specific firmware only; AZ and OR to start.

**Discussion Summary**

The Hacker News community reacted with high skepticism regarding the safety data underpinning Lemonade’s business model, balanced by a debate on the predictive power of insurance markets.

*   **Statistical validity of "Safer Miles":** The dominant critique focuses on selection bias. Users argue that FSD is primarily used in "easy" environments (highway cruising), while humans take over for complex, high-risk situations (parking lots, tricky intersections). Critics claim that comparing FSD miles to average manual miles is an apples-to-oranges comparison that artificially inflates FSD's safety record.
*   **Market as a Truth Mechanism:** A counter-narrative suggests that legal or marketing claims are cheap, but insurance premiums represent "money where the mouth is." Some users view Lemonade’s willingness to underwrite this risk as a strong validation of the technology, similar to how health insurers assess BMI or smoking risks. Others dismiss it as a standard marketing customer-acquisition cost (CAC) disguised as a "tech" discount, noting similar programs from GEICO and Progressive.
*   **The "Supervised" Experience:** Anecdotal reports on FSD reliability are polarized.
    *   **Hardware disparity:** Several users note a distinct performance gap between Hardware 3 (HW3) and Hardware 4 (HW4), with the latter being significantly more capable.
    *   **Stress levels:** Detractors describe "supervised FSD" as more stressful than driving manually—likening it to supervising a teenager who might suddenly swerve into oncoming traffic or fail to yield at blind intersections.
    *   **Edge cases:** Users discussed the specific difficulty of "random" behavior (like children running into the street), debating whether AI or distracted humans respond better to sudden, chaotic events.
*   **Liability and Automation:** There is discussion regarding the legal definition of the driver. As long as Tesla requires supervision, the human remains the insurer's liability target. Users questioned when (or if) liability will shift to the manufacturer, suggesting that true autonomy should result in the OEM insuring the vehicle, not the owner.

### Show HN: Amla Sandbox – WASM bash shell sandbox for AI agents

#### [Submission URL](https://github.com/amlalabs/amla-sandbox) | 139 points | by [souvik1997](https://news.ycombinator.com/user?id=souvik1997) | [73 comments](https://news.ycombinator.com/item?id=46824877)

What it is
- A WebAssembly (WASM + WASI) sandbox for LLM agents that lets models write short scripts (JavaScript or shell) to orchestrate multiple tool calls, without giving them arbitrary code execution on your host.
- Ships as a Python package and one binary. No Docker, no VM.

Why it matters
- Today, many agent frameworks execute model-generated code with exec()/subprocess, which is risky; the README cites real-world issues (e.g., CVE-2025-68664).
- Pure tool-calling is safe but pricey: every tool call is a round trip through the model. “Code mode” collapses many calls into one script, cutting latency and token cost—if you can make it safe. This project aims to do that.

How it works
- Isolation: Code runs inside WASM with WASI via wasmtime (memory isolation, minimal syscalls). No network, no shell escape, VFS constrained (read/write only under /workspace and /tmp).
- Capabilities: You explicitly grant which tools can be called, with constraints and quotas. Examples include:
  - Method patterns (e.g., stripe/charges/*)
  - Parameter constraints (e.g., amount <= 10,000; currency in [USD, EUR])
  - Call limits (e.g., max_calls=100)
- Tooling model: The agent writes one JS/shell script that calls your whitelisted tools; sandbox enforces capabilities at each call boundary.
- Languages: JavaScript (async/await, object-style args) and shell pipelines. Output via return or console.log.
- Integration: Exposes a LangChain/LangGraph-compatible tool so an LLM can “write code” as a single action in a graph/agent.

Dev notes and quick start highlights
- Install: uv pip install "git+https://github.com/amlalabs/amla-sandbox"
- Provide Python functions as tools; the sandbox exposes them with enforced constraints.
- JS tools take object arguments only (no positional args).
- Filesystem: Only /workspace and /tmp are writable.

What’s interesting for HN
- A practical middle path between unsafe exec() and costly tool-chaining.
- Capability-based security mindset (no ambient authority) applied to agent code execution.
- Simpler ops than container isolation while still providing strong blast-radius reduction.
- Potential discussion: WASI surface area and escape risks, wasmtime hardening, side channels, performance overhead vs Docker, how to manage complex capability policies, and real-world integration stories.

Caveats
- No network access in the sandbox; all external effects must go through your vetted tools.
- You must design and maintain capability constraints; mistakes there can still cause misuse.
- Limited language/runtime surface (JS + shell via WASM) by design.

Repo: amlalabs/amla-sandbox (GitHub)

**Licensing and Open Source**
Discussion opened with **smnw** (Simon Willison) noting that while the Python usage code is MIT licensed, the underlying WASM binary is proprietary. He argued this effectively blocks the project from being used as a dependency in other open-source tools. The author (**souvik1997**) acknowledged this feedback and committed to prioritizing open-sourcing the WASM source code to allow for redistribution.

**Technical Architecture & Comparisons**
*   **vs. Pyodide:** When asked how this compares to Pyodide, the creators explained that Pyodide behaves like CPython compiled to Emscripten (for browsers/Node), whereas `amla-sandbox` is CPython compiled to WASI. This allows it to run on standard WASM runtimes (like Wasmtime), enabling server-side features like precise instruction counting, memory limits, and syscall interception.
*   **vs. LocalSandbox:** User **vmt** shared `LocalSandbox` (a Deno-based alternative), leading to a side discussion on implementation details like SQLite-backed virtual filesystems for agent "resume" funtionality.

**Security Model**
A user questioned the safety of tools running *outside* the sandbox. The author clarified their architectural philosophy: the sandbox acts as a **policy enforcement layer**. While the agent generates code inside the sandbox, the actual tools (like Stripe or API calls) execute on the host to access the network or credentials. The sandbox's job is ensuring the agent only invokes those tools with specific, whitelisted parameters and quotas.

**Ecosystem and Standards**
*   **Missing Modules:** In early testing, `smnw` noted that the Python environment currently lacks standard libraries like `sqlite3`.
*   **Typed Interfaces:** User **rllfy** argued that sandboxing is only half the battle; the ecosystem needs standardized component interfaces (like WIT) rather than raw shell scripting to ensure agents are traceable and safe at a build-time composition level.

### Claude Code's GitHub page auto closes issues after 60 days

#### [Submission URL](https://github.com/anthropics/claude-code/issues/16497) | 26 points | by [dcreater](https://news.ycombinator.com/user?id=dcreater) | [15 comments](https://news.ycombinator.com/item?id=46830179)

Anthropic’s claude-code repo is grappling with a stale-bot misfire: a new issue reports that GitHub Actions auto-closed numerous tickets for “60 days of inactivity” even after users replied to the 30‑day warning with “still relevant.” The report lists affected issues (e.g., #3006, #3030, #7742, #7743), has 100+ thumbs-up, and is marked as a regression and “external,” implying an upstream action/config bug. The ask: stop auto-closing or at least allow reporters to reopen. It’s a fresh example of how aggressive stale workflows can bury legitimate, slow-moving bugs—especially painful in a large repo with heavy community usage.

**The Irony of Auto-Closing**
The discussion reflects a broad disdain for "stale bot" workflows, with users arguing that auto-closing tickets prioritizes clean metrics over software quality.
*   **dashboard-driven development:** User `kngstnp` described the practice as the "worst kind" of management, citing Goodhart's Law to argue that closing evergreen issues merely to clear queues constitutes a process failure rather than a solution.
*   **Bad Logic:** `ssbttbttss` analyzed the likely cause of the "misfire," suggesting the bot checks the date of its *own* last comment while ignoring subsequent human activity—logic they found difficult to believe passed code review at an AI company.
*   **Perverse Incentives:** `SahAssar` and others noted that this policy forces users to spam "bump" or "+1" comments solely to keep valid bug reports prone to "unintended" closure alive, reminiscent of old internet forums.
*   **The "AI" Solution:** Several users, including `dcrtr`, found the situation ironic, asking why Anthropic doesn't simply use Claude to triage the issues rather than relying on a rigid, buggy script.

### How AI assistance impacts the formation of coding skills

#### [Submission URL](https://www.anthropic.com/research/AI-assistance-coding-skills) | 445 points | by [vismit2000](https://news.ycombinator.com/user?id=vismit2000) | [333 comments](https://news.ycombinator.com/item?id=46820924)

Headline: RCT finds AI coding help boosts speed little, but cuts short‑term mastery—especially debugging

- What’s new: Anthropic ran a randomized controlled trial with 52 mostly junior Python devs learning a new async library (Trio). Some used an in‑sidebar AI assistant; others coded unaided, then all took an immediate quiz.

- Key results:
  - Mastery dropped with AI: AI group scored 50% vs 67% without AI (≈17% gap; Cohen’s d=0.74; p=0.01).
  - Biggest hit was debugging—spotting and explaining errors in code.
  - Speed gain was small and not statistically significant (≈2 minutes faster on average).
  - Many participants spent substantial time crafting prompts—up to 11 minutes and 15 queries—eating into expected speedups.

- How AI was used mattered: Developers who asked the AI for explanations, follow‑ups, and conceptual clarifications retained more than those who mainly solicited code.

- Why it matters: As AI automates more low‑level coding, humans still need to catch errors and provide oversight. This study suggests naive reliance on AI can erode near‑term understanding—especially the very skills (reading, debugging, conceptual grasp) needed to supervise AI‑generated code.

- Caveats:
  - Short‑term learning only (quiz minutes after task), single library (Trio), small N, mostly junior devs.
  - Observational data elsewhere shows large productivity boosts (up to ~80%) on some tasks; this RCT isolates the trade‑off with immediate mastery.

- Practical takeaways:
  - Use AI as a tutor, not just a code printer: ask “why,” request step‑by‑step explanations, and probe concepts.
  - Interleave manual coding, code reading, and debugging; verify before running.
  - Teams should not equate AI‑boosted throughput with skill growth; consider policies and tools that scaffold explanation and require user reasoning.

**Corporate Motives and Study Validity**
The discussion opened with skepticism regarding Anthropic’s motives for publishing results that highlight the downsides of their own technology. While some users praised the transparency of publishing negative data on mastery, others viewed it as a "psy-op" similar to historical tactics used by tobacco companies—admitting to minor flaws to build trust while maintaining a conflict of interest. A few commenters also criticized the study itself for a small sample size and potential errors in the included figures.

**The Illusion of Speed vs. Competence**
Commenters largely validated the study's findings, noting that AI often makes developers *feel* faster while masking a lack of genuine progress. There was a consensus that the "path of least resistance" leads most users to use AI as a code printer rather than a tutor. Users highlighted that while the "tutor mode" (asking conceptual questions) preserves skill, the natural tendency is to bypass learning, potentially creating a generation of "1-3 year juniors" who never develop the deep problem-solving skills necessary to become senior engineers.

**Shifting Bottlenecks: Coding vs. Specifying**
A significant thread explored how AI changes the value of different engineering skills. Several users argued that as code generation becomes easier, the bottleneck shifts to Product Management skills—specifically the ability to write clear requirements and specifications. While some anecdotes claimed AI has already improved team outputs like Jira tickets and documentation "for free," skeptics countered that this likely results in "low signal" bloat that looks impressive but adds little value.

**Dependency and "Midnight" Risks**
Finally, participants discussed the operational risks of reliance. If developers lose the ability to code unaided, they become helpless when tools are unavailable (e.g., license issues or outages). One commenter posed a worst-case scenario: if an AI-generated system breaks in production at midnight, the human "gatekeepers" who merely prompted the code may lack the fundamental understanding required to fix it.

### Mamdani to kill the NYC AI chatbot caught telling businesses to break the law

#### [Submission URL](https://themarkup.org/artificial-intelligence/2026/01/30/mamdani-to-kill-the-nyc-ai-chatbot-we-caught-telling-businesses-to-break-the-law) | 171 points | by [jyunwai](https://news.ycombinator.com/user?id=jyunwai) | [59 comments](https://news.ycombinator.com/item?id=46827665)

NYC to scrap Adams-era AI business chatbot after it steered users toward illegal practices

- What’s new: New York City Mayor Zohran Mamdani plans to shut down the city’s Microsoft-powered business rules chatbot, calling it “functionally unusable.” The move comes amid efforts to close a $12 billion budget gap and follows investigations by The Markup and THE CITY showing the bot gave incorrect—and sometimes illegal—guidance.

- Why it’s getting axed: Reported failures included telling employers they could take a cut of workers’ tips, suggesting landlords could discriminate against tenants with Section 8 vouchers, misidentifying the minimum wage, and saying businesses could refuse cash despite a 2020 city law.

- Cost and context: Mamdani cited roughly half a million dollars in costs; prior reporting put foundational build costs near $600,000. The bot was part of the Adams administration’s MyCity digital overhaul, criticized for heavy reliance on outside contractors.

- What happened after the backlash: The Adams team defended the tool and added disclaimers warning users not to treat responses as legal advice, while narrowing the kinds of questions it would answer. The new administration has not set a takedown date yet.

- Bigger picture: The episode underscores the risks of deploying general-purpose AI for high-stakes compliance in government services—disclaimers can’t compensate for authoritative but wrong answers, and procurement-driven rollouts face heightened scrutiny in a budget crunch.

Source: The Markup/THE CITY (Jan 30, 2026)
Link: https://themarkup.org/artificial-intelligence/2026/01/30/mamdani-to-kill-the-nyc-ai-chatbot-we-caught-telling-businesses-to-break-the-law

Based on the discussion, here is a summary of the Hacker News comments:

**QA difficulties and non-determinism**
A significant portion of the technical discussion focused on the difficulty of performing Quality Assurance (QA) on "black box," non-deterministic systems like LLMs.
*   **The "Happy Path" Problem:** Users speculated that the city likely only tested "happy path" scenarios (standard, easy queries) where the bot performs well, failing to test edge cases or specific legal nuance.
*   **Whack-a-mole:** Commenter *lgn* described QA efforts for such tools as "whack-a-mole," noting that because LLMs are "inherent generators of plausible-sounding text," they are fundamentally mismatched for domains where exact correctness is required.
*   **Testing Limitations:** Discussions emerged regarding how to test these systems—whether through sampling user interactions or checking training data—with the consensus being that standard software engineering practices (testing for deterministic outputs) do not apply, making government deployment risky.

**LLMs are the wrong tool for compliance**
Commenters argued that general-purpose AI is fundamentally unsuited for government regulations and legal advice.
*   **Need for verification:** One user noted the "dirty little secret" of LLMs: if the output requires an expert to verify it to ensure it isn't illegal, the tool "defeats its own purpose."
*   **Static vs. Dynamic:** Users pointed out that NYC governance and laws change daily, while LLM training data is static ("lossy compression"), making it impossible for the model to remain current without a robust RAG (Retrieval-Augmented Generation) layer, which seemingly failed here.
*   **Search vs. Chat:** Several users suggested that better search functionality (citing Kagi or Google’s citation attempts) would be preferable to a conversational agent that "hallucinates" answers.

**Critique of the Adams Administration**
The failure was viewed by many as a symptom of former Mayor Eric Adams’ leadership style.
*   **Tech Hype:** Commenters drew parallels between this chatbot and Adams’ previous enthusiasm for cryptocurrency, characterizing his administration as one that chased "shiny promises" and tech buzzwords without understanding the underlying utility or risks.
*   **Bureaucratic Incompetence:** References were made to Louis Rossmann’s videos documenting NYC bureaucracy to illustrate a culture of incompetence (zero accountability/QA) regarding city services. The $600k price tag for a "barely working" wrapper was criticized as a waste of taxpayer money driven by enterprise sales and procurement dynamics rather than technical merit.

**Microsoft and Procurement**
A sidebar discussion emerged regarding the specific mention of Microsoft in the reporting. While some questioned if the provider mattered, others noted that Microsoft’s cloud division focuses heavily on government and large non-tech corporate sales, often resulting in expensive implementation contracts for software that is "sold by sales teams" rather than vetted by engineers.

### Show HN: I built an AI conversation partner to practice speaking languages

#### [Submission URL](https://apps.apple.com/us/app/talkbits-speak-naturally/id6756824177) | 64 points | by [omarisbuilding](https://news.ycombinator.com/user?id=omarisbuilding) | [58 comments](https://news.ycombinator.com/item?id=46830698)

TalkBits: voice-first, pressure‑free language practice on iPhone

What it is: A new iPhone app that drills real, short, spoken exchanges with an AI conversation partner. You press and hold to speak; it replies instantly with voice, using casual, everyday language and gently correcting mistakes in‑line.

Why it’s interesting:
- Conversation over lessons: prioritizes realistic, short responses and common expressions (multiple English accents plus German, French, Spanish, Italian, Dutch, Portuguese, Arabic, and more).
- Low friction: built for 30‑second to 5‑minute sessions; feels like quick, real‑world practice rather than study blocks.
- Private by design: no profiles or public ratings; App Store privacy label says “Data Not Collected” (as reported by the developer).

Details:
- iPhone only (iOS 15.1+), 28 MB; initial release Jan 2025, v1.0.2 updated 5 days ago.
- Free download with in‑app purchases: tiers listed at $9.99, $14.99, $19.99, $29.99, $49.99.
- Solo developer: Omar Muhammad Omar.

Caveats and open questions:
- Few/no ratings yet; real‑world quality of corrections and speech recognition is unproven.
- Offline mode and accessibility support aren’t specified.
- Privacy label is developer‑provided; Apple notes it’s not independently verified.

Here is a summary of the discussion:

**The "Wrapper" Debate vs. Utility**
The discussion opened with immediate observations that the app appears to be a ChatGPT wrapper. While some commenters found the proliferation of "wrappers" depressing or low-effort, others argued that the underlying technology matters less than the user experience; if the app solves a problem better than the raw model, the "provenance" is irrelevant. When users asked why they shouldn't just use ChatGPT’s native Voice Mode, the developer and other commenters noted that the raw models can be repetitive ("dull," "tends to repeat"), effectively forcing the user to do the prompting work, whereas this app offers a refined UI/UX and specific prompt engineering for language learning.

**Differentiation in a Crowded Market**
Critiques regarding the app's value proposition were prominent. Commenters pointed out that "focusing on conversation over vocabulary" is no longer a unique selling point, listing numerous competitors already filling this niche (Univerbal, Malan Chat, EnglishCall, TongueFu, etc.). Feedback suggested the develop needs to find a stronger "hook" or differentiator than just conversation practice to survive in a saturated market.

**Technical Feedback & Bugs**
Users reported several specific issues:
*   **Audio Loops:** A significant bug was identified where the microphone picks up the AI's speech, creating an endless feedback loop when using headphones (the developer confirmed they are fixing this).
*   **Localization Quality:** Users testing Portuguese noted a mix of European and Brazilian dialects, and German users noted jarring switches between formal (*Sie*) and informal (*Du*) address, as well as robotic accents.
*   **App Store Search:** Searching for "TalkBits" on the App Store triggers autocorrect to "Talbots," making the app difficult to find.
*   **Website Assets:** The landing page screenshots were described as blurry and unreadable, with font choices that needed an overhaul.

**Feature Requests**
The discussion included constructive suggestions for the product roadmap:
*   **Correction Toggles:** Users suggested a toggle between "immersion mode" (natural conversation without interruption) and "correction mode" (explicit feedback on grammar/pronunciation), as constant corrections can break flow.
*   **Latency Handling:** Several comments emphasized that latency is the "killer" for voice apps; managing Voice Activity Detection (VAD) to distinguish between a user pausing to think versus finishing a sentence is crucial for immersion.

### The Cost of AI Art

#### [Submission URL](https://www.brandonsanderson.com/blogs/blog/ai-art-brandon-sanderson-keynote) | 5 points | by [jplusequalt](https://news.ycombinator.com/user?id=jplusequalt) | [6 comments](https://news.ycombinator.com/item?id=46829452)

Brandon Sanderson: What is art, and why he rebels against AI-made art

- The setup: Sanderson opens by revisiting Roger Ebert’s 2010 claim that “video games can never be art,” arguing that games’ mechanics can themselves be artistic—and using that frame to examine today’s generative AI.

- Why now: He cites two signals that force the question:
  - An AI-generated track (“Walk My Walk”) topping Billboard’s Digital Country Songs chart, with Billboard acknowledging multiple recent AI chart-toppers.
  - Mark Lawrence’s blind test where readers struggled to distinguish short AI-written passages from ones by well-known novelists (AI still falters at long-form).

- The dilemma: Sanderson worries about becoming the next Ebert—reflexively dismissing a new medium. He notes how critics once derided prose (vs. poetry), photography, and film as “not art,” and wonders if AI is just another evolution.

- His stance: Even setting aside economics, environmental cost, and ethically messy training data, he says he would still oppose AI-made art. The piece tees up a deeper, philosophical argument about what art is and why we make it—implying the crux isn’t surface-level quality but something about intent, authorship, and the human act of creation.

Why it matters to HN: This is a prominent fantasy author engaging the “can AI be art?” debate without handwaving the tech’s capabilities. It’s a useful lens for product, policy, and culture: if audiences can’t tell the difference in outputs, does authorship matter? If mechanics can be art in games, can human-guided AI tools be part of that mechanics? Where should lines be drawn between tool, collaboration, and replacement?

**Process Over Product:** The discussion centers on the distinction between the technical quality of the output and the internal journey of the creator.
*   **The Utility vs. The Growth:** User **lckr** initially argues that AI art fails simply because the current quality is low—specifically in novels—viewing it as a technical hurdle rather than a philosophical one. User **jplsqlt**, clarifying Sanderson's thesis, counters that the objection isn’t about the quality of the final product (which may eventually become indistinguishable from human work) but about the "transformative process." Sanderson argues that struggling through creation is what improves the artist ("humans are the art"), and using AI generates a result while robbing the creator of that necessary growth.
*   **The Human Quotient:** The thread examines why audiences consume art in the first place. User **lbjcksn**, identifying as a writer/developer, argues that readers crave human connection and a reflection of current times—attributes an AI cannot authentically provide. Even with a "human-in-the-loop" acting as an editor, the consensus suggests that without the "human quotient," the work loses its appeal.
*   **Romanticism vs. Reality:** User **prmsfbns** notes that defining art by the creator's effort rather than the output is a "slightly romantic notion," though they concede that audiences do value context and backstory (evidenced by guided museum tours). The discussion concludes with the sentiment that while AI serves as a utility for tasks like summarizing meetings, applying it to creative endeavors discards the personal development inherent in the artistic struggle.

### Tesla’s autonomous vehicles are crashing at a rate much higher tha human drivers

#### [Submission URL](https://electrek.co/2026/01/29/teslas-own-robotaxi-data-confirms-crash-rate-3x-worse-than-humans-even-with-monitor/) | 471 points | by [breve](https://news.ycombinator.com/user?id=breve) | [257 comments](https://news.ycombinator.com/item?id=46822632)

Tesla’s Robotaxi crash rate looks far worse than humans — even with safety drivers, per Electrek’s read of new data

- The numbers: NHTSA’s Standing General Order shows 9 Tesla Robotaxi crashes in Austin from July–November 2025 (right-turn collisions, a cyclist strike, fixed-object hits, a construction-zone crash, an animal strike, and a low-speed backing collision). Tesla’s Q4 2025 earnings chart pegs cumulative robotaxi miles at ~500,000 by November, implying ~1 crash every 55,000 miles.

- How that compares: Human drivers average ~1 police-reported crash every ~500,000 miles (NHTSA), or roughly ~1 every ~200,000 miles when you include non-police-reported incidents. That puts Tesla’s supervised robotaxis at ~9x worse than police-reported human rates, or about ~3–4x worse using the broader estimate.

- Safety drivers didn’t save the stats: Every Tesla Robotaxi mile cited had a safety monitor onboard who could intervene. The top HN-style critique: any accidents averted by those monitors don’t show up, meaning an unsupervised rate could be worse.

- Transparency gap: Tesla’s crash narratives in the NHTSA database are fully redacted (“[REDACTED, MAY CONTAIN CONFIDENTIAL BUSINESS INFORMATION]”). By contrast, Waymo and others publish detailed narratives for each incident.

- Waymo contrast: Operating fully driverless, Waymo reports 125M+ autonomous miles with crash rates below human averages and incident-level transparency, including a recent school-zone collision where it released specifics showing rapid braking and reduced impact speed.

- Trajectory and caveats: Electrek notes only one crash in October and one in November (possible improvement), but the overall rate is still far from “robotaxi-ready.” The dataset is small and limited to one city, but the lack of disclosures makes independent assessment harder.

- Big takeaway: Electrek argues Tesla needs both a dramatically better safety record and real transparency about incident causes to be credible as a robotaxi operator.

Here is a summary of the discussion:

**Statistical Validity and Comparisons**
The primary debate centered on whether the comparison between Tesla’s Robotaxi and human drivers was "like-for-like." Critics argued that NHTSA SGO reports force AV companies to report minor low-speed contact events (rubbing a curb, minor bumps), whereas human baselines typically rely on police reports or insurance claims which exclude the vast majority of minor incidents.
*   **Counter-argument:** Others noted the article attempted to control for this by using a stricter "unreported incident" baseline for humans (~1 crash every 200k miles), but Tesla’s rate (1 every ~55k miles) was still roughly 3-4x worse.
*   **Denominator Issues:** Some users questioned the mileage figures, noting potential discrepancies between the cumulative mileage reported and the specific July–November window where the crashes occurred.

**Sample Size and Safety Drivers**
Commenters debated the significance of a dataset containing only 9 crashes. While some dismissed the sample size as too small to draw firm conclusions, others argued that statistically, incurring 9 incidents in such short mileage is highly improbable if the system were actually safe (calculated by one user as a 0.4% chance if the system matched human averages).
*   **The Safety Net:** A recurring point was that these statistics occurred with safety drivers behind the wheel. Users emphasized that since humans presumably intervened to prevent other potential accidents, the "unsupervised" crash rate would theoretically be much worse than the data suggests.

**Severity and Reporting**
Discussions broke down the specific types of crashes listed (hitting fixed objects vs. other vehicles). While some argued that hitting a static object shouldn't count as heavily as hitting a car, others retorted that "hitting a wall" is still a failure, and that humans rarely report such incidents unless there is significant damage.
*   **Transparency:** There was broad consensus that Tesla's redaction of crash narratives prevents independent verification. Users contrasted this with Waymo’s open reporting, suggesting that if Tesla's detailed data proved the system was safe, the company would likely release it. The prevailing sentiment was that obfuscation suggests the raw data looks bad.