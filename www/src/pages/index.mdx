import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Sat Jan 17 2026 {{ 'date': '2026-01-17T17:09:08.134Z' }}

### Counterfactual evaluation for recommendation systems

#### [Submission URL](https://eugeneyan.com/writing/counterfactual-evaluation/) | 79 points | by [kurinikku](https://news.ycombinator.com/user?id=kurinikku) | [6 comments](https://news.ycombinator.com/item?id=46655524)

Counterfactual Evaluation for Recommendation Systems (8 min) — Eugene Yan argues that standard offline metrics for recommenders (precision/recall/NDCG on static logs) treat the task as observational, when it’s inherently interventional: recommendations change what users see and thus what they click or buy. He frames A/B tests as the clean causal answer but costly and risky, and shows how counterfactual evaluation with Inverse Propensity Scoring (IPS) can estimate how a new policy would perform without shipping it—by reweighting logged rewards by the ratio of new vs. old recommendation probabilities. Practical tip: get those probabilities from impression logs (often better than normalizing model scores) to correct for presentation bias; this need for propensities explains why many public datasets fall short—Open Bandit is a notable exception. He also flags pitfalls, starting with insufficient support when the new policy recommends items rarely or never exposed by the old one. Takeaway: treat recsys as causal, log propensities, use IPS to triage ideas offline, then confirm with A/B.

Here is a summary of the discussion:

The discussion focused on the practical implementation of counterfactual evaluation and the nuances of gathering high-quality data. Users validated the author’s premise, noting the common frustration where "perfect" offline metrics fail to generate lift in actual A/B tests.

Key technical points raised in the thread included:

*   **Handling Baselines:** For systems without an established history, participants suggested starting with simple baselines (like "most popular" or random sorting) to build the initial propensity logs required for these techniques.
*   **Data Quality & Depth:** A significant portion of the discussion centered on logging. Users argued that simple click data is insufficient; logs must include "non-clicks" (items scrolled past) and "depth" metrics (dwell time or add-to-cart actions) to distinguish genuine interest from accidental clicks or immediate bounce-backs.
*   **Safe Randomness:** While randomized control groups are essential for unbiased data, users warned that fully random recommendations ruin the user experience. A proposed solution was injecting random items into specific slots (e.g., position 5) rather than randomizing an entire session.
*   **Theoretical Frameworks:** One user questioned the underlying statistical framework, asking which specific type of counterfactual theory (e.g., Pearl’s, Classical, or Constructor theory) justifies the model's appropriateness.
*   **Method Comparisons:** The SNIPS normalization technique mentioned in the context of the article was compared to Mutual Information factor correction used in co-occurrence models.

### ClickHouse acquires Langfuse

#### [Submission URL](https://langfuse.com/blog/joining-clickhouse) | 209 points | by [tin7in](https://news.ycombinator.com/user?id=tin7in) | [95 comments](https://news.ycombinator.com/item?id=46656552)

Headline: ClickHouse acquires Langfuse; OSS, self-hosting, and roadmap unchanged

Why it matters:
- The analytics database behind many AI stacks just bought one of the leading open-source LLM observability/evaluation platforms. Expect tighter performance, scalability, and enterprise features for teams running AI in production.
- Signals continued consolidation in the LLM tooling layer as core infra vendors pull key app-layer components closer.

What stays the same for users:
- Open source and self-hosting remain first-class; no licensing changes planned.
- Langfuse Cloud continues with the same product, endpoints, and SLAs.
- Roadmap and support channels unchanged.

What likely gets better:
- Faster performance and reliability work by co-developing directly with the ClickHouse engineering team.
- Accelerated enterprise-grade security/compliance.
- More mature customer success/playbooks via ClickHouse’s GTM and support org.

Backstory:
- Langfuse started as tracing and eval primitives for LLM apps; early versions ran on Postgres.
- Scale demands (high-throughput ingest + fast analytical reads) led v3 to switch to ClickHouse.
- The two companies had already been collaborating: Langfuse Cloud is a large ClickHouse Cloud customer, ClickHouse teams use Langfuse, and they’ve co-hosted community events. Thousands of teams met ClickHouse through Langfuse’s v3 migration.

Why this deal, not a Series A:
- Langfuse says it had strong Series A options but chose ClickHouse to move faster while keeping its OSS/self-hosted identity and production-first focus. The whole team joins ClickHouse.

Caveats and open questions:
- Long-term independence of the OSS project and governance will be watched closely, though the team reiterates no licensing changes and commitment to self-hosting.
- With v3 already on ClickHouse, the de facto backend choice is clear; watch for how flexible self-hosted deployments remain.

What to watch next:
- Deeper ClickHouse Cloud integrations, benchmarks, and migration tooling.
- Roadmap on enterprise features (security, compliance), higher-throughput ingestion, and reliability at scale.
- Any pricing or packaging changes on Langfuse Cloud (none announced).

Here is a summary of the discussion on Hacker News:

**The Big Picture: ClickHouse’s Platform Strategy**
Users see this acquisition as part of a clearer pattern following ClickHouse's recent $400M raise. Combined with the previous acquisition of HyperDX (an observability platform), commenters suggest ClickHouse is aggressively aggregating tools to evolve from a niche analytics engine into a comprehensive alternative to data giants like Snowflake and Databricks.

**The "Time Series" vs. "AI" Debate**
A significant technical debate erupted regarding categorization. While ClickHouse is technically a columnar OLAP database, users discussed its rebranding as an engine for "AI agents" and "Time Series."
*   **Skepticism:** Some viewed the pivot to LLM tooling as "marketing fluff" to chase AI valuations, noting that database vendors often rebrand to match current hype cycles.
*   **Technical Defense:** Others argued the fit is natural; observability data (logs, traces, spans) *is* time-series data, an area where ClickHouse has already absorbed significant market share due to its ingestion speed. A sidebar discussion questioned whether LLMs themselves technically count as "time series models" (predicting the next token based on sequential history).

**The Economics of Open Source**
The discussion spurred a philosophical sidebar about the sustainability of Free and Open Source Software (FOSS).
*   **VC Dependence:** Commenters pointed out that while users love "independent" FOSS, modern open-source projects are heavily subsidized by Venture Capital. A specific example cited was the PostgreSQL ecosystem, where many core contributors are actually employed by VC-backed for-profit companies (like Snowflake, Neon, or Supabase).
*   **Legacy vs. Modern:** Users contrasted this with foundational FOSS (Linux, GNU, BSD), which historically had less reliance on the VC churn model, though others retorted that even Linux development involves corporate funding today.

### Show HN: I built a tool to assist AI agents to know when a PR is good to go

#### [Submission URL](https://dsifry.github.io/goodtogo/) | 38 points | by [dsifry](https://news.ycombinator.com/user?id=dsifry) | [32 comments](https://news.ycombinator.com/item?id=46656759)

Good To Go: a “done or not” gate for AI-driven pull requests

AI agents can write code and open PRs, but they’re bad at knowing when they’re actually merge-ready. Good To Go (gtg) tackles that missing piece with a single, deterministic check: gtg 123 returns a clear status—READY, ACTION_REQUIRED, UNRESOLVED, CI_FAILING, or ERROR—so agents (and humans) can stop polling, guessing, or nagging.

What it does
- Aggregates CI: Collapses all GitHub checks and commit statuses into pass/fail/pending, handling multiple systems and required vs optional checks.
- Classifies review feedback: Separates ACTIONABLE must-fix items from NON_ACTIONABLE noise and AMBIGUOUS suggestions. Includes parsers for CodeRabbit, Greptile, Claude Code, and Cursor to respect their severity/approval patterns.
- Tracks thread resolution: Distinguishes truly unresolved discussions from ones already addressed in later commits.

Agent-first design
- Structured JSON output with action_items the agent can execute.
- Sensible exit codes: default exits 0 for analyzable states so agents parse JSON; optional semantic exit codes for shell use.
- State persistence to remember dismissed or handled comments across sessions.

How teams can use it
- As a required CI gate: PRs don’t merge until they’re truly READY.
- Inside agent workflows: decide to merge, fix comments, resolve threads, or wait for CI without infinite loops.
- For PR shepherding: monitor status changes over time.

Quick start: pip install gtg, set GITHUB_TOKEN, run gtg 123 (auto-detects repo). Also supports explicit repos and human-readable output.

The discussion around **Good To Go (gtg)** focuses on the safety of AI autonomous merging, the necessity of the tool compared to native GitHub features, and the trade-offs between velocity and strict process enforcement.

**Safety and Human Oversight**
A primary concern raised by users like `rootnod3` and `glemion43` was the fear that this tool allows AI to bypass human review and "merge shit itself" directly to production.
*   **Clarification:** The creator (`dsfry`) and others (`dnnn`, `tayo42`) clarified that "Ready" does not mandate an auto-merge. Instead, it serves as a reliable signal that a PR is legally ready for human eyes (tests passed, blocking comments addressed).
*   `dsfry` described the workflow: agents act as "smart interns" that must clear the `gtg` deterministic gate before a senior/human reviews the code, preventing humans from wasting time on broken builds.
*   `ljm` and `bxtr` raised security scenarios, such as malicious bug reports via Jira/Linear prompting agents to introduce vulnerabilities. `dsfry` countered that the code still undergoes review and assessment layers.

**Utility vs. Native Tools**
Skeptics questioned why this abstraction is needed over existing solutions.
*   `nyc1983` asked why standard GitHub status checks and branch protections aren't sufficient, dismissing it as "AI slop."
*   **The Differentiator:** `dsfry` explained that native GitHub checks are binary (pass/fail) but don't parse nuance in comments. `gtg` distinguishes between **ACTIONABLE** feedback (must fix) and **AMBIGUOUS** suggestions or nitpicks. This prevents agents from looping infinitely on non-blocking comments or hallucinating that they are done when they aren't.
*   `jshrbkff` expressed a dislike for coupling workflows to specific platforms like GitHub or CodeRabbit.

**Implementation and Trade-offs**
*   **Velocity vs. Quality:** `jshnpl` noted decision trade-offs: the tool forces strict adherence to process, which improves code quality and reduces interruptions but might slow velocity as agents get "stuck" trying to satisfy the `gtg` green light. `dsfry` confirmed this was an intentional design choice to reduce human context-switching.
*   **Technical details:** `mclly` and `bltt` discussed using pre-push hooks or the Model Context Protocol (MCP) instead. The creator argued that hooks struggle with context (e.g., recognizing when a comment is marked "out of scope" and should be ignored), whereas `gtg` is designed to handle that state persistence.

**Meta-Commentary**
Several users (`phlpp-gyrt`, `fryfntrs`) criticized the submission text itself, suspecting it was written by an LLM due to its style ("lacks decency," "reductive nonsense"). `dsfry` defended the text, noting the difference between internal release notes and public documentation requires effort that LLMs can assist with.

### AI industry insiders launch site to poison the data that feeds them

#### [Submission URL](https://www.theregister.com/2026/01/11/industry_insiders_seek_to_poison/) | 15 points | by [Teever](https://news.ycombinator.com/user?id=Teever) | [3 comments](https://news.ycombinator.com/item?id=46661731)

Top story: AI insiders launch “Poison Fountain” to sabotage model training

- What’s new: A small group of anonymous AI industry insiders has launched Poison Fountain, a site coordinating a mass data-poisoning effort aimed at degrading AI models that scrape the web for training data. The Register reports the project has been live about a week and is run by at least five people (not yet publicly verified; they promise PGP-signed proof).

- How it’s meant to work: Website operators are encouraged to link to pages hosting “poisoned” content so AI crawlers ingest it. The group frames this as an information war, saying they aim to “inflict damage on machine intelligence.” The hosted payloads reportedly include subtly incorrect code intended to mis-train models that learn from public repositories. (No operational details were shared in the piece.)

- Why now: The effort cites Anthropic research suggesting that even a small number of malicious documents can meaningfully degrade model quality. Backers argue regulation is too slow and the tech too widespread; they echo Geoffrey Hinton’s warnings about AI risk.

- Context and parallels: Similar in spirit to Nightshade (which booby-traps images against scraping), Poison Fountain targets text/code training pipelines. It lands amid concerns about “model collapse” from synthetic data feedback loops and a broader push by AI firms to secure cleaner, verified corpora (e.g., licensing Wikipedia-grade sources).

- Skepticism and risk: The organizers’ identities remain unverified; the space already has scammy “poisoning” schemes. There’s also overlap with misinformation campaigns: poisoning the commons harms not just model builders but downstream users.

- Why it matters: If this gains traction, expect an arms race in data provenance and filtering—more whitelists, licensed data deals, stricter crawl controls, and legal pushback—alongside intensified trust battles over what the web is “safe” to train on.

- What to watch: Proof-of-participation via PGP, any takedowns or legal action, technical countermeasures from model makers (poison detection, source whitelisting, robustness training), and whether mainstream sites quietly join—or publicly condemn—the effort.

**Discussion Summary:**

The conversation on this specific post was brief, focusing on technical feasibility and redirection to a larger thread:

*   **Attack Viability:** Users questioned the effectiveness of the proposed "poisoning" architecture. One commenter asked if relying on a single URL would simply lead to a Denial of Service (DoS) or make it easy for crawlers to ignore the source. A respondent clarified that the initiative appears to encourage users to replicate the poisoned pages across multiple locations to create a distributed attack vector rather than a centralized one.
*   **Previous Discussion:** It was noted that this submission is a duplicate, with the primary discussion located in an earlier thread.

---

## AI Submissions for Fri Jan 16 2026 {{ 'date': '2026-01-16T17:09:53.293Z' }}

### FLUX.2 [Klein]: Towards Interactive Visual Intelligence

#### [Submission URL](https://bfl.ai/blog/flux2-klein-towards-interactive-visual-intelligence) | 187 points | by [GaggiX](https://news.ycombinator.com/user?id=GaggiX) | [53 comments](https://news.ycombinator.com/item?id=46653721)

FLUX.2 [klein]: sub-second, unified image gen + editing on consumer GPUs

Black Forest Labs dropped FLUX.2 [klein], a compact flow-model family that does text-to-image, image editing, and multi-reference generation in a single model—aiming for real-time workflows without big-iron requirements.

Highlights
- Speed: Sub-0.5s generation/editing on modern hardware; step-distilled to 4 inference steps
- Quality: Photorealistic outputs with high diversity; claims to match/exceed much larger models (incl. Qwen) at a fraction of latency/VRAM and outperform Z-Image while supporting both T2I and multi-reference editing
- Hardware: 4B variant fits ~13GB VRAM (RTX 3090/4070+), built for local dev and edge
- Unified tasks: T2I, I2I, and multi-reference blending in one model; frontier-level outputs at sub-second speed
- Text encoder: 8B Qwen3 embedder

Model lineup
- FLUX.2 [klein] 9B (distilled): Flagship “small” model focused on latency vs. quality; License: FLUX Non-Commercial License (NCL)
- FLUX.2 [klein] 4B (distilled): Fully open under Apache 2.0; best for local/edge; supports T2I/I2I/multi-ref
- Base variants (undistilled 9B/4B): More diversity and full training signal for fine-tuning/LoRA/research; 4B Base under Apache 2.0, 9B Base under NCL

Quantized releases (with NVIDIA)
- FP8: up to 1.6× faster, ~40% less VRAM
- NVFP4: up to 2.7× faster, ~55% less VRAM
- Benchmarked on RTX 5080/5090 at 1024×1024; general speed measured on GB200 (bf16)

Licensing note
- “FLUX [dev] Non-Commercial License” renamed to “FLUX Non-Commercial License” (no material changes); applies to 9B Klein models
- 4B models are Apache 2.0; 9B weights are available under NCL

Why it matters
Real-time visual generation/editing that runs locally opens the door to interactive design tools, agentic visual reasoning loops, and rapid creative iteration without cloud latency or massive GPUs.

Try it / resources
- Overview and links: https://bfl.ai/models/flux-2-klein
- Demo/Playground + HF Spaces for 9B and 4B
- API, docs, GitHub, and open weights available (Apache 2.0 for 4B; NCL for 9B)

**FLUX.2 [klein]: sub-second, unified image gen + editing on consumer GPUs**

Black Forest Labs has released FLUX.2 [klein], a new family of compact flow models designed to bring high-quality text-to-image (T2I), image-to-image (I2I), and multi-reference generation to consumer hardware and edge devices. The release focuses on efficiency without specialized "big iron" infrastructure.

**Highlights:**
*   **Performance:** Capable of sub-0.5 second generation and editing on modern GPUs; distilled to just 4 inference steps.
*   **Model Variants:**
    *   **4B Variant:** Fully open source (Apache 2.0). Optimized for local/edge use, fitting within ~13GB VRAM (accessible on cards like the RTX 3090/4070+).
    *   **9B Variant:** Available under a Non-Commercial License (NCL). Positioned as the flagship efficient model balancing latency and quality.
*   **Unified Architecture:** A single model handles creation, editing, and blending, utilizing an 8B Qwen3 embedder for text understanding.
*   **Quantization:** Created in partnership with NVIDIA, offering FP8 and NVFP4 formats that significantly reduce VRAM usage (up to 55% less) and boost speed.

**Discussion Summary:**

The discussion on Hacker News focuses heavily on benchmarking the new [klein] models against Alibaba’s recently released **Z-Image Turbo**, which serves as the primary point of comparison for lightweight, local image generation.

*   **Efficiency vs. Capability:** Users debated the trade-offs of the new "small model" trend. While there is enthusiasm for running 4GB models locally (as opposed to unwieldy 100GB+ files), commenters noted that smaller models often suffer from **"knowledge gaps."** Users stress-testing the model found it struggled with complex physical logic prompts (e.g., "a tiger jumping on a pogo stick" or specific object interaction puzzles), areas where larger 32B+ models still dominate.
*   **Hardware and Accessibility:** A significant portion of the conversation revolved around hardware requirements. Users appreciated that the 4B model fits on **12GB consumer cards**, making it a viable alternative for users who cannot run full-sized Flux models. Some users also reported testing the web demos on mobile devices (Pixel, iPad, Samsung) with mixed success regarding browser compatibility.
*   **The "Speed" Use Case:** Commenters pointed out that sub-second inference isn't just about faster batching; it opens the door to **interactive/real-time workflows**, such as instant visual feedback loops and live previewing during editing, rather than the traditional "wait 10 seconds" generation cycle.
*   **Theoretical Compressed "World Models":** A side discussion emerged regarding the compressibility of visual data versus text. Users debated whether visual models have hit a "compression wall" regarding how much diverse visual reality can be encoded into small parameter counts compared to the relatively high compression efficiency of LLMs.

### Reading across books with Claude Code

#### [Submission URL](https://pieterma.es/syntopic-reading-claude/) | 126 points | by [gmays](https://news.ycombinator.com/user?id=gmays) | [35 comments](https://news.ycombinator.com/item?id=46650347)

Headline: An agent that weaves “idea trails” across 100 non-fiction books, from Jobs’ reality distortion to Theranos and The True Believer

Summary:
- Instead of using LLMs to compress text, this project uses Claude Code as an agent to help readers go deeper. It mines a library of 100 HN-favorite non-fiction books to build “trails”—sequences of excerpts that trace an idea across authors and eras.
- Example trail: Steve Jobs’ self-deception and “reality distortion field” → Theranos’ demo sleights in Bad Blood → startup cult dynamics in Zero to One → mass-movement psychology and necessary misrepresentation in The True Believer.

How it works:
- Books are chunked (~500 words) and auto-tagged into 100k topics, then rolled up into a quirky hierarchical tree (~1,000 top-level topics). It’s messy—think “Borgesian taxonomy”—but good enough to navigate ideas.
- CLI tools let the agent search topics, find co-occurrences across books, and browse neighboring topics/chunks.
- The agent operates in stages: propose novel trail ideas by skimming the topic tree, then dive to assemble and order excerpts to support an insight, finally adding highlights and explicit “edges” between excerpts.

What the author learned:
- Agentic > pipeline: A minimal prompt plus tool access (“find something interesting”) beat a hand-tuned multi-module pipeline. Pushing more into the agent loop reduced orchestration and improved results.
- Treat the AI like a coworker: Stop prompt-tweaking; build better tools and affordances. Ask the agent what it needs, watch logs, then upgrade the toolkit.
- Revision is cheap: The agent can refactor existing trails to new stylistic constraints (e.g., shorter excerpts) without regenerating everything, enabling faster iterative exploration.

Why it matters:
- A compelling pattern for LLM-powered reading: not summaries, but cross-book, cross-domain synthesis that surfaces serendipitous connections.
- Suggests a practical design lesson for AI tooling: give agents searchable structure, simple composable tools, and let them drive the exploration loop—even for non-coding tasks.

Caveats:
- The auto-generated taxonomy can be noisy or oddly specific; usefulness varies by topic.
- Quality rests on agent judgment when selecting and sequencing excerpts, so evaluation remains subjective.

Based on the discussion, here is a summary of the comments:

**Reading Philosophy vs. AI Utility**
A major debate emerged regarding the nature of reading. Critics argued that using AI to extract "truth" or summaries prioritizes the destination over the journey, effectively stripping non-fiction of the author's tone, pacing, and expression. However, defenders—including a self-described literature major—countered that this tool acts as a discovery mechanism or an analytical aid rather than a substitute. They argued it helps readers decide which books are worth their time and uncovers connections that human readers might miss.

**Agent Design and Technical Feasibility**
Users validated the author’s "agent as coworker" lesson, noting that treating the AI as a fast-reading colleague (focusing on tool-building rather than prompt-tweaking) is a "surprise moment" that improves results. There was technical discussion regarding hallucinations; while some remained skeptical of LLM accuracy, others noted that the project's approach—feeding the agent small, organized 500-word chunks rather than open-ended prompts—significantly reduces error rates.

**Better Recommendation Engines**
Commenters compared the project favorably to incumbents like Goodreads and Amazon. Users complained that current platforms rely on collaborative filtering (what other people bought) rather than semantic analysis of the complete text. Many expressed a desire for open-source alternatives that could index large local libraries (e.g., on a Raspberry Pi) to find content-based connections.

**Meta-Commentary on AI Criticism**
The thread contained several dismissive comments suggesting that people are unnecessarily "shoving LLMs" into reading. This prompted a moderator intervention warning against shallow dismissals and "curmudgeonly" reactions to people's work. This sparked a brief sub-discussion on the difference between Luddism and valid concerns about the automation of creative or intellectual consumption.

### Install.md: A standard for LLM-executable installation

#### [Submission URL](https://www.mintlify.com/blog/install-md-standard-for-llm-executable-installation) | 89 points | by [npmipg](https://news.ycombinator.com/user?id=npmipg) | [108 comments](https://news.ycombinator.com/item?id=46652944)

Mintlify proposes install.md, a markdown spec for “LLM-executable” installation instructions. Instead of curl | bash, developers publish human-readable, structured steps that an agent can follow autonomously—with optional step-by-step approval and environment detection.

How it works:
- Add /install.md to your project (Mintlify now autogenerates and hosts it at <docs-url>/install.md; you can override or disable).
- Users pipe the file into an LLM or paste the URL; the agent adapts to OS/arch, runs commands, and verifies completion.
- Example call: curl -fsSL https://www.anaconda.com/docs/install.md | claude

Spec highlights:
- Strict headings and prompts: product H1, short description, an explicit “install this for me” instruction
- OBJECTIVE and DONE WHEN (clear success criteria)
- TODO checklist
- Step sections with code blocks
- EXECUTE NOW tying steps to the objective

Why it matters:
- Shifts install docs from human-oriented prose to agent-actionable recipes
- Safer/more transparent than piping executables; still readable by humans
- Lets developers encode edge cases without cluttering main docs

Availability:
- Live across Mintlify sites (e.g., Cerebras, Firecrawl, LangChain)
- Non-Mintlify projects can host install.md manually

Potential implications: smoother agent-led setup, clearer verification, and a path toward standardized, auditable automation for software installs.

The discussion around Mintlify’s `install.md` proposal centered on a conflict between **human readability** and **execution determinism**. While some welcomed a shift away from opaque `curl | bash` pipelines, many questioned the safety and efficiency of introducing a non-deterministic LLM into the installation path.

**Key arguments included:**

*   **Transparency vs. Determinism:** User `ptkmn` championed the idea as "Runtime Literate Programming," arguing that `install.md` allows developers to state *intent* clearly in English while letting the agent handle platform-specific logic (e.g., detecting AVX2 support or OS architecture). However, `cuu508` and `ctlfnmrs` countered that shell scripts, while ugly, are static and verifiable. They argued that trading determinism for a "black box" that might hallucinate instructions based on temperature or model updates creates a security and reliability nightmare.
*   **The "Build-Time" Compromise:** Several commenters (`nbdywllbsrv`, `chm`, `franga2000`) suggested that the LLM should be used to generate a shell script *from* the markdown primarily for the user to audit, or that the script should be pre-generated server-side. This would retain the ease of writing docs while preserving the efficiency and reproducibility of a standard shell script execution.
*   **Support & Efficiency:** `Szpadel` highlighted the "customer support hell" scenario: if an installation fails because an LLM interpreted instructions differently for one specific user (or model version), debugging becomes impossible. Others noted the inefficiency of using billions of parameters and expensive tokens just to copy binaries and set paths.
*   **Theoretical Implications:** The debate briefly touched on computer science theory, with `dng` citing Peter Naur’s "Programming as Theory Building," questioning whether natural language is too "lossy" to reliably replace code as the primary mental model for execution.

### Show HN: Gambit, an open-source agent harness for building reliable AI agents

#### [Submission URL](https://github.com/bolt-foundry/gambit) | 89 points | by [randall](https://news.ycombinator.com/user?id=randall) | [19 comments](https://news.ycombinator.com/item?id=46641362)

Gambit (by bolt-foundry) is a local-first agent harness for building reliable LLM workflows by composing small, typed “decks” with explicit inputs, outputs, and guardrails—aimed at replacing monolithic prompts and brittle tool wiring.

Highlights
- Decks over chains: Break workflows into small, typed steps (“decks”) with clear IO; mix LLM calls and regular compute seamlessly.
- Typed by default: Define input/output schemas with Zod; validate at runtime to catch errors early and reduce orchestration brittleness.
- Local-first dev loop: Run via npx with REPL, streaming traces, and a built-in Debug UI/Simulator at localhost:8000; store sessions/traces under .gambit/.
- Observability baked in: Stream transcripts, view tool traces, grade sessions, and reproduce failures locally (JSONL traces, state persistence).
- Minimal boilerplate: Author decks in Markdown (model-powered) or TypeScript (compute/tools), then compose with child actions.
- Pragmatic IO strategy: Feed models only the per-step context; inject references/cards instead of dumping full RAG blobs to cut cost/hallucinations.
- CLI commands: run, repl, serve (debug UI), test-bot (personas), grade (saved sessions); --trace, --state, --verbose flags for diagnostics.
- TS library APIs: defineDeck/defineCard from JSR; spawn child decks with ctx.spawnAndWait and emit structured logs with ctx.log.
- Quickstart: Node 18+ and OPENROUTER_API_KEY; run examples with npx @bolt-foundry/gambit; supports OpenRouter-style endpoints.
- License: Apache-2.0.

Why it stands out: Gambit treats LLM orchestration like regular software—typed, composable, testable, and observable—providing a cleaner alternative to “one giant prompt + tools” setups common in LangChain-style stacks. It’s optimized for fast local iteration and predictable, debuggable runs.

Try it fast
- export OPENROUTER_API_KEY=...
- npx @bolt-foundry/gambit init
- npx @bolt-foundry/gambit repl gambit/hello.deck.md
- npx @bolt-foundry/gambit serve gambit/hello.deck.md (then open http://localhost:8000)

Here is a summary of the discussion:

**Architectural Validation: Short-Lived vs. Long-Running**
The most active thread of discussion validated Gambit’s core philosophy: breaking workflows into short-lived, typed executors. Users with experience building agents (citing tools like GTWY.ai) noted that reliability issues often stem not from model quality, but from allowing agents to run too long, which causes them to lose context and drift. Commenters praised the explicit boundaries and "atomic containment" of logic, agreeing that code should handle orchestration and state management while LLMs should be restricted to small, focused steps.

**Comparisons and Positioning**
When asked how Gambit compares to **Mastra**, the creator (`rndll`) distinguished Gambit as an "agent harness" focused on defining intended behaviors (often via Markdown primitives) rather than just orchestrating TypeScript pipelines. While frameworks like **LangChain** and Mastra focus on computing tasks against an LLM, Gambit aims to make the "agent building" process itself simpler and more declarative. One user explicitly expressed relief at seeing a fresh architecture, criticizing LangChain’s complexity.

**User Experience and Feedback**
*   **hands-on testing:** One user (`lgrntmt`) reported playing with the tool for 24 hours, praising the separation of prompt and logic. However, they encountered friction with specific parameters (e.g., the model ignoring instructions to constrain summary length) and struggled to figure out how to load file contents for translation tasks.
*   **File System:** In response to feedback about file inputs, the creator noted they are working on "file system stuff" to support capabilities similar to Claude Code or Codex.
*   **Naming:** A user pointed out that "Gambit" is already the name of a well-known, long-standing open-source Scheme implementation.

### ChatGPT is getting ads. Sam Altman once called them a 'last resort.'

#### [Submission URL](https://www.businessinsider.com/chatgpt-ads-openai-2026-1) | 64 points | by [donpott](https://news.ycombinator.com/user?id=donpott) | [35 comments](https://news.ycombinator.com/item?id=46652024)

OpenAI will test ads in ChatGPT for free and Go users “in the coming weeks,” reversing Sam Altman’s 2024 stance that ads would be a “last resort.”

Key details
- Where ads appear: ChatGPT free and Go tiers only; Plus, Pro, Business, and Enterprise won’t see ads.
- OpenAI’s promises: ads will be clearly labeled; responses “will not be influenced by ads”; chat conversations won’t be shared with advertisers.
- Why now: OpenAI faces massive compute spending—about $1.4 trillion in data center and infrastructure commitments—and recently restructured into a more traditional for‑profit to attract capital.
- Leadership stance: Altman has softened on ads, saying the company must “take a lot of care to get right.” Fidji Simo (CEO of applications) says ads won’t influence answers and the company will be “extremely respectful” of user data.

Why it matters
- Trust vs. monetization: Ads inside a general‑purpose assistant raise concerns about subtle bias and targeting—even if answers aren’t “influenced,” placement and ranking could be.
- Business model pressure: Without Google/Meta‑scale ad businesses, OpenAI needs new revenue to fund compute; this brings it closer to the ad‑supported playbooks it once critiqued.

What to watch
- Ad format: inline “sponsored” cards vs. sidebar units; how “clearly labeled” looks in practice.
- Targeting and data policy: whether ad targeting uses behavioral signals without sharing raw chats; opt‑outs or controls for free users.
- Impact on UX and trust: user backlash, adoption of paid tiers to avoid ads, and any measurable changes in answer quality.
- Governance and regulation: disclosures, audits of “no influence” claims, and regional privacy compliance.

Based on the discussion, here is a summary of the user reaction:

**Skepticism and "Enshittification" Fears**
The prevailing sentiment is one of cynicism and disappointment. Many users view this move as the beginning of the "enshittification" of ChatGPT, drawing direct comparisons to the decline of Google Search. Commenters argue that despite OpenAI’s promises, the introduction of ads will inevitably incentivize the model to prioritize monetization over accuracy, creating conflicts of interest where answers are subtly influenced to drive sales. One user noted that while OpenAI claims "no influence," history suggests ads will eventually blend into results indistinguishably.

**Migration to Alternatives and Local LLMs**
The announcement has triggered a wave of interest in competitors. Common sentiments include:
*   **Switching:** Users are explicitly mentioning cancelling subscriptions or moving to **Claude**, **Gemini**, or **DeepSeek**.
*   **Local Models:** There is a significant push toward running local large language models (LLMs) to avoid surveillance and advertising entirely, provided the user has sufficient hardware.
*   **Trust:** Several users expressed that they would rather pay for a product directly (like a blank media tax or subscription) than deal with the "sycophantic" or manipulative nature of an ad-supported model.

**Privacy and Safety Contradictions**
A specific sub-thread highlighted the irony of LLMs being programmed to be excessively cautious regarding sensitive topics (like suicide prevention) while simultaneously preparing to aggressively sell products. Users worry that "content mining" for ad targeting effectively breaks the privacy of personal conversations.

**Proposed Alternatives to Standard Ads**
One user suggested a less intrusive model: instead of generic passive ads, OpenAI could offer an "opt-in" feature for hobbyists (e.g., "Gear Acquisition Syndrome" for musicians). In this scenario, the AI would proactively hunt for deals on specific items the user wants, making the ad valuable context rather than unwanted spam. However, most users remain doubtful that such nuance will be achieved.

### vLLM-MLX – Run LLMs on Mac at 464 tok/s

#### [Submission URL](https://github.com/waybarrios/vllm-mlx) | 30 points | by [waybarrios](https://news.ycombinator.com/user?id=waybarrios) | [3 comments](https://news.ycombinator.com/item?id=46642846)

vllm-mlx: vLLM-style, OpenAI-compatible local server for Apple Silicon

What it is
- A vLLM-like inference server built on Apple’s MLX, bringing GPU-accelerated LLM, vision-language, and audio (STT/TTS) to M1–M4 Macs.
- Presents an OpenAI-compatible API, so existing OpenAI clients work as-is.

Why it matters
- Brings many of vLLM’s niceties (continuous batching, paged KV cache) to Macs without CUDA.
- One stack for text, images, video, and audio with native MLX performance and unified memory.

Highlights
- Models: Llama, Qwen3, LLaVA/Qwen-VL, Whisper; TTS options like Kokoro, Chatterbox, VibeVoice, VoxCPM.
- Features: continuous batching, paged KV cache with prefix sharing, MCP tool calling, quantization (3–8 bit), OpenAI API drop-in.
- Multimodal: images and video via mlx-vlm; STT/TTS via mlx-audio, including multilingual voices.

Performance (M4 Max, 128GB, examples)
- Llama-3.2-1B-4bit: ~464 tok/s; Llama-3.2-3B-4bit: ~200 tok/s.
- Qwen3-0.6B-8bit: ~402 tok/s; up to 3.4x speedup with 5 concurrent requests (continuous batching).
- Whisper STT RTF: tiny ~197x, large-v3-turbo ~55x, large-v3 ~24x.

Quick start
- pip install -e . then run: vllm-mlx serve mlx-community/Llama-3.2-3B-Instruct-4bit --port 8000
- Use with OpenAI SDK by pointing base_url to http://localhost:8000/v1; API key optional for local.

Notes
- Apple Silicon only; leverages MLX and Metal kernels.
- Good fit for local dev, multimodal demos, and small-to-mid models with high throughput on Mac.
- Docs include guides for multimodal, audio, batching, MCP tools, and benchmarks.

**vLLM-MLX: vLLM-style, OpenAI-compatible local server for Apple Silicon**

**The Discussion**
Project creator **wybrrs** introduced the tool as a remedy for "painfully slow" inference frameworks on macOS, highlighting that vLLM-MLX leverages Apple's native MLX framework for GPU acceleration. They emphasized its ability to handle a single stack for text, image, video, and audio with continuous batching support that offers significant speedups for concurrent users.

The conversation began with hardware questions, as user **thebruce87m** asked about RAM requirements for older machines. Specifically, they wanted to know if a 16GB M1 Mac would be sufficient to run the models, noting that the memory usage numbers in the documentation seemed surprisingly low, likely due to heavy quantization.

### DuckDuckGo is asking for a Yes or No vote on AI

#### [Submission URL](https://duckduckgo.com/vote) | 45 points | by [jaredcwhite](https://news.ycombinator.com/user?id=jaredcwhite) | [28 comments](https://news.ycombinator.com/item?id=46651155)

Title: “YES AI” or “NO AI” — a live public vote on the future of AI

What it is:
- A stark, referendum-style site asking the public to choose “YES AI” or “NO AI,” framed as giving people a direct say: “AI should be a choice. Did anyone ask you? Now someone is.”

Why it matters:
- Taps into the broader question of who gets to decide the trajectory of AI—companies, governments, or the public—and whether consent should be explicit.

What’s interesting:
- The simplicity is the point: a binary prompt meant to surface sentiment quickly.
- It raises immediate issues: what counts as “AI”? how would results be used? can a public web vote be representative or secure against bots/brigading?
- Highlights a desire for more democratic input into AI policy, even if the mechanism is rudimentary.

Takeaway:
- A provocative, minimalist attempt to measure public will on AI. The prompt is powerful; the governance mechanics are the real challenge.

Based on the discussion, here is a summary of the reactions to the "YES AI / NO AI" vote:

**Context & Skepticism**
*   **Identified as Marketing:** Users quickly identified the site as a DuckDuckGo (DDG) campaign (citing the URLs `yes.duckduckgo.com` and `no.duckduckgo.com`). Most view it as a marketing stunt or a "push poll" rather than a serious referendum on global AI governance.
*   **Ambiguity:** Commenters criticized the binary nature of the question. They argued the prompt lacks nuance: does "No" mean "no AI existence" or "don't force it into my search"? Does "Yes" mean "unfettered AI" or "helpful coding snippets"?
*   **Sampling Bias:** With the vote skewing heavily (96%) toward "NO," users pointed out that an internet poll targeting existing DuckDuckGo users—who are already privacy-conscious and wary of Big Tech—is inherently biased and unrepresentative of the general public.

**Strategic Implications for DuckDuckGo**
*   **Differentiation:** Some users praised the move as savvy product differentiation. With Google and Bing forcing AI integration, DDG positions itself as the alternative that offers user choice and transparency.
*   **Competitive Disadvantage:** Conversely, some argued that blind anti-AI sentiment is counter-productive. They believe DDG needs to offer optional, privacy-preserving AI tools (like search summarization) to remain a viable competitor to mainstream engines, rather than catering solely to "Luddite" impulses.

**Sentiment on AI**
*   **Distrust:** The "No" voters expressed deep distrust of corporate AI development, citing privacy invasions, "non-consensual" integration into daily life, and the unreliability (hallucinations) of current models.
*   **Utility vs. Hype:** The counter-argument suggested that AI hate is becoming emotionally charged and unreasonable. These users argued that as long as the features are optional (e.g., a "search assist" button), they provide objective value without harming the user experience.

### Starlink updates Privacy Policy to allow AI model training with personal data

#### [Submission URL](https://coywolf.com/news/startups/starlink-updates-tos-to-allow-ai-model-training-with-personal-data/) | 51 points | by [speckx](https://news.ycombinator.com/user?id=speckx) | [11 comments](https://news.ycombinator.com/item?id=46647716)

Starlink flips default to allow AI training on customer data; opt-out available

- What changed: Starlink updated its Global Privacy Policy on Jan 15, 2026 to allow sharing customers’ personal information with affiliates, service providers, and “third-party collaborators” to train AI models “including for their own independent purposes,” unless users opt out. All customers are opted in by default. Reported by Jon Henshaw (Coywolf).

- Why it matters: The “third-party collaborators” language and “independent purposes” carveout mean external companies can train models on Starlink customer data unless you disable it. Expect debate over consent-by-default, data scope, and potential compliance questions (e.g., GDPR/CCPA). Some speculate this could feed Musk’s Grok; Starlink hasn’t confirmed.

- How to opt out:
  - Web: starlink.com/account/settings → Edit Profile → scroll down → uncheck “Share personal data with Starlink’s trusted collaborators to train AI models.” → Save.
  - App: Menu → Profile → Account overview → Settings → Edit Profile → scroll down → uncheck the same setting → Save.

Source: Coywolf (Jon Henshaw).

**Discussion Summary:**

Hacker News users analyzed the technical and legal implications of Starlink's policy update, focusing on the specific types of data exposed and methods for mitigation.

*   **Scope of Data Collection:** There was significant debate regarding what constitutes "personal information" in this context. While some users highlighted account-level data (billing address, GPS location, and photo IDs for age verification), others emphasized Starlink’s role as an ISP. Technical comments noted that Starlink can observe DNS queries and domains via Server Name Indication (SNI), painting a detailed picture of user behavior even without decrypting HTTPS traffic.
*   **Opt-Out Friction:** Users reported technical issues, noting that attempts to save the opt-out preference resulted in "An error occurred" messages. Others expressed frustration that while they receive marketing emails about data plans, they received no notification regarding this privacy shift.
*   **Mitigation Strategies:** The consensus largely favored defensive measures. Several commenters recommended using a VPN specifically to blind the ISP (Starlink) to traffic data. Others argued for a "zero-trust" approach to cloud services in general, suggesting client-side encryption is the only barrier against sudden policy changes.
*   **Legal Ambiguity:** Commenters criticized the vague phrasing of "trusted collaborators" and "independent purposes." Comparisons were made to GDPR protections, with users noting that without strict regulations, "collaborators" could essentially mean any third party, from data consultancies to major ad-tech firms.

### Ads Are Coming to ChatGPT. Here’s How They’ll Work

#### [Submission URL](https://www.wired.com/story/openai-testing-ads-us/) | 23 points | by [thm](https://news.ycombinator.com/user?id=thm) | [10 comments](https://news.ycombinator.com/item?id=46649985)

OpenAI will start testing ads inside ChatGPT in the US “in the coming weeks,” a major shift for a product with an estimated 800M weekly users. Ads will appear in clearly labeled boxes below the bot’s answer and won’t be shown to Plus, Pro, or Enterprise subscribers. Free users and the new $8/month Go tier (rolling out in the US; already in India and France) will see the first placements, with a global expansion planned.

Key details
- Placement and influence: Ads sit below responses; OpenAI says they won’t affect what the model says.
- Targeting: Contextual matching to conversation topics; some personalization data may be used, but users can turn off data used for advertising and clear ad data at any time.
- Data handling: OpenAI says it won’t sell user data or expose chats to advertisers; advertisers get aggregate metrics (impressions, clicks).
- Safety limits: No ads on sensitive/regulated topics (health, mental health, politics). No ads to users believed to be under 18, aided by a forthcoming age-prediction model.
- Roadmap: Exploring interactive, conversational ad formats (e.g., ask an ad questions before purchasing).

Why it matters
- Monetization pressure: Despite massive usage and ~$64B raised lifetime, OpenAI’s revenue lags; ads are a predictable path, as seen with Google’s SGE and other large platforms.
- Trust vs. “enshittification”: OpenAI pledges user experience over revenue, but contextual targeting implies scanning conversation content, and ad creep is a familiar risk.
- Privacy knobs, real impact unclear: You can disable ad-personalization data, but contextual ads based on chat topics likely remain. The interplay with ChatGPT’s growing “memory” features will draw scrutiny.
- Regulatory watch: Age estimation for ad eligibility and the handling of sensitive topics will be closely examined.

What to watch
- Whether ads expand beyond Go/free tiers
- How “interactive ads” are implemented
- The effectiveness and transparency of privacy controls and targeting policies
- Any measurable influence of ads on response ranking or UX over time

**Discussion Summary**
The discussion reflects deep cynicism regarding OpenAI's trajectory and the introduction of advertising:

*   **Inevitable "Enshittification":** Users drew immediate parallels to Google’s evolution, expressing disbelief that ads will remain distinct from content or neutral regarding the model's output. While one commenter predicted a five-year timeline for this degradation, others argued OpenAI’s high cash burn and shift to for-profit status will accelerate the decline to within a year or even months.
*   **Subscription Vulnerability:** While ads are currently limited to Free and Go tiers, users threatened to cancel their Plus or Pro subscriptions if ads encroach on those services. Replies were pessimistic, predicting that paying users will eventually be served ads regardless of their tier.
*   **Alternatives:** Several users indicated they would switch to competitors like DeepSeek or Gemini if the ad experience becomes intrusive.
*   **Direct Interaction:** One user shared an anecdote about informing ChatGPT directly that they would leave the platform if advertised to, highlighting the disconnect between user sentiment and the platform's automated responses.

---

## AI Submissions for Tue Jan 13 2026 {{ 'date': '2026-01-13T17:17:47.770Z' }}

### vLLM large scale serving: DeepSeek 2.2k tok/s/h200 with wide-ep

#### [Submission URL](https://blog.vllm.ai/2025/12/17/large-scale-serving.html) | 139 points | by [robertnishihara](https://news.ycombinator.com/user?id=robertnishihara) | [45 comments](https://news.ycombinator.com/item?id=46602737)

vLLM v0.11 completes V1 migration, pushes MoE throughput to 2.2k tok/s per H200

- What’s new: vLLM fully drops the V0 engine in 0.11.0, completing the move to its V1 architecture. Backed by 1,969 contributors and 950+ commits in the past month (as of 12/18/25), it’s now featured in SemiAnalysis’ open-source InferenceMax benchmarks and is used in production by Meta, LinkedIn, Red Hat, Mistral, and Hugging Face.

- Performance: Community runs on a CoreWeave H200 cluster (InfiniBand, ConnectX-7) report sustained 2.2k tokens/s per H200 GPU in multi-node setups, up from ~1.5k. Gains come from kernel work (silu-mul-quant fusion, Cutlass QKV, TP attention fixes) and Dual Batch Overlap (DBO) for decode—translating to fewer replicas for the same QPS and better token-per-dollar.

- DeepSeek-style serving: 
  - Wide-EP: Expert Parallelism combined with Data Parallelism improves effective KV cache and avoids MLA pitfalls of Tensor Parallelism; enable with --enable-expert-parallel. Supports DeepEP high-throughput all-to-all, Perplexity MoE kernels, and NCCL AllGather-ReduceScatter.
  - DBO: Overlaps collective comms with compute via microbatch worker threads and CUDA graphs; enable with --enable-dbo and tune with --dbo-decode-token-threshold.
  - Expert load balancing: Hierarchical and global EPLB policies (--enable-eplb) smooth real-world routing skew.

- Other optimizations: Async scheduling, disaggregated serving, CUDA graph mode, DeepGEMM default, integrated DeepEP kernels, SiLU kernel for DeepSeek-R1.

Why it matters: Open-source, production-grade throughput for sparse MoE and disaggregated serving makes frontier models cheaper and easier to run at scale.

Here is a summary of the discussion:

**Economic Implications & Pricing Sustainability**
The most active debate focused on the economics of serving LLMs given vLLM's new throughput numbers (2,200 tokens/s per H200).
*   **Cost Calculations:** Users crunched the numbers on hardware costs (~$750k for a 16xH200 system) versus throughput. Several commenters estimated the raw cost (hardware depreciation + electricity) to be between **$0.03 and $0.25 per million tokens**.
*   **DeepSeek Validation:** Many noted that these efficiency gains suggest DeepSeek’s ultra-low API pricing is actually sustainable and potentially profitable, rather than just "VC money subsidizing consumption."
*   **The "Utilization" Catch:** Skeptics pointed out that theoretical benchmarks don't match real-world serving. Factors like context length, request concurrency, and fluctuating traffic mean 100% utilization is impossible, making actual costs higher than the raw math suggests.

**Performance & Optimization**
*   **Source of Gains:** Users discussed where the 40%+ performance jump came from. While vLLM uses Python, commenters clarified that the gains are due to low-level optimizations (kernel fusion, better scheduling, JIT compilation) rather than high-level language tweaks.
*   **Future Headroom:** Several comments suggested this is just the beginning ("greenfield") of inference optimization, predicting further gains as engines target 4-bit quantization and Blackwell architecture, which could potentially drop costs to ~$0.11 per million tokens.
*   **Benchmarks:** There were calls for more transparent, direct comparisons between open-source engines (vLLM, SGLang, TRT-LLM), with users noting a lack of standardized testing for different serving configurations.

**Hardware & Compatibility**
*   **H200 vs. Cerebras:** One user noted they expected these kinds of throughput numbers from waferscale engines (Cerebras), making the H200 performance particularly impressive.
*   **AMD Support:** Users discussed support for AMD GPUs via ROCm. While vLLM works on AMD, some noted that for single-user scenarios (like on Strix Halo), it can be slower than Ollama/llama.cpp, as vLLM is optimized for high-throughput batching rather than single-stream latency.

### We can't have nice things because of AI scrapers

#### [Submission URL](https://blog.metabrainz.org/2025/12/11/we-cant-have-nice-things-because-of-ai-scrapers/) | 434 points | by [LorenDB](https://news.ycombinator.com/user?id=LorenDB) | [233 comments](https://news.ycombinator.com/item?id=46608840)

Anubis: a browser proof‑of‑work gate to blunt AI scrapers

- What it is: A lightweight, Hashcash-style proof‑of‑work (PoW) page that runs in the browser to slow down large‑scale scraping. Legitimate visitors incur a tiny, one‑off computation; mass scrapers pay a steep, cumulative cost.
- Why it exists: The site’s admin says aggressive AI scraping has caused downtime and resource strain. Anubis is positioned as a stopgap that keeps the site usable while more targeted bot detection (e.g., fingerprinting headless browsers via font rendering and similar signals) is developed.
- How it works: When you visit, a “Making sure you’re not a bot” page triggers a PoW challenge in JavaScript. At human scale, the added load is meant to be negligible; at scraper scale, it becomes expensive.
- Trade‑offs called out: It’s a compromise—some friction for real users in exchange for shifting costs to scrapers. It requires modern JavaScript and can be blocked by privacy/anti‑fingerprinting tools like JShelter; users may need to disable such plugins for the domain. The goal is to eventually avoid showing challenges to visitors who look clearly legitimate.

**Discussion Summary:**

The discussion pivoted from the specific technical implementation of Anubis to the broader economics and ethics of AI scraping.

*   **The "Data Dump" Alternative:** A prevalent counter-proposal was that instead of blocking scrapers with computation tasks, sites should offer a standardized bulk download (e.g., a gzipped tarball or torrent) of their public data. Commenters argued this converts an "adversarial" relationship into a coordination problem, saving bandwidth for both the host and the scraper.
*   **Standardization Challenges:** Users debated how to signal these bulk endpoints to bots. Suggestions included the emerging `/llms.txt` standard, specific `robots.txt` directives, or embedding instructions in HTTP 429 (Too Many Requests) headers.
*   **Skepticism of AI Actors:** Many were skeptical that AI companies would respect "polite" standards. The consensus among several users was that scrapers often ignore `robots.txt` and copyright law already, implying they are indifferent to resource consumption on the target's end.
*   **Impact on the Small Web:** The conversation highlighted the financial toll on hobbyist sites. Users shared anecdotes of hosting accounts being suspended or costs ballooning due to aggressive scraping, noting that individual webmasters pay the price for AI training data.
*   **Infrastructure Implications:** Some speculated that the inability to effectively Ip-block scrapers (due to VPNs and botnets) might force a faster migration to IPv6 or drive the web toward closed, gated communities to survive.

### Confer – End to end encrypted AI chat

#### [Submission URL](https://confer.to/) | 100 points | by [vednig](https://news.ycombinator.com/user?id=vednig) | [75 comments](https://news.ycombinator.com/item?id=46600839)

I’m ready to summarize—could you share the Hacker News submission you want covered?

Please provide any of the following:
- HN link or item title + URL
- The article text or key excerpts (helpful if paywalled)
- Any notable HN comments you want included

Preferences (optional):
- Length: ultra-brief (50–80 words), standard (120–180), or in-depth (250–350)
- Tone: neutral, punchy, or technical

I’ll return:
- TL;DR
- Key takeaways (3–5 bullets)
- Why it matters
- HN chatter highlights (if provided)

Based on the discussion provided, here is the summary of the Hacker News submission regarding **Confer**.

**Submission:** **Confer – End-to-End Encrypted AI Inference**
**Url:** `https://confer.to/blog` (Derived from context)

### TL;DR
Confer has launched an AI inference platform that claims to be "end-to-end encrypted" (E2EE). Unlike standard LLM providers (like OpenAI) that can technically read user prompts, Confer processes data inside Trusted Execution Environments (TEEs). This ensures data is encrypted on the user's device and only decrypted momentarily inside a hardware-isolated "enclave" on the server, theoretically preventing even confer—or a server intruder—from seeing the content.

### Key Takeaways
*   **The "E2EE" Claim:** The service uses TEEs (likely Intel SGX or similar) to extend the encryption boundary. Data remains encrypted until it hits the specific CPU interacting with the model.
*   **Remote Attestation:** The security model relies on users (or their client software) verifying a cryptographic hash of the code running on the server to ensure it hasn't been tampered with before sending data.
*   **Privacy Compromise:** The discussion highlights that while Fully Homomorphic Encryption (FHE)—processing data *while* it stays encrypted—is the privacy "holy grail," it is currently 100x too slow or expensive. TEEs are presented as the practical 5–10 year solution.
*   **Browser Support:** The implementation leans heavily on modern browser cryptoprimitives (Passkeys/PRF), causing some compatibility issues for users on specific setups (e.g., Firefox on Linux).

### Why It Matters
This technology attempts to solve the biggest enterprise hurdle for AI adoption: **Data Privacy.** If companies can guarantee that cloud providers *physically cannot* access their IP during inference, the barrier to using powerful remote LLMs drops significantly. This approach mirrors Apple’s recent "Private Cloud Compute" strategy and Signal's contact discovery architecture.

### HN Chatter Highlights
The comment section is technical and debating the semantics of "End-to-End":

*   **Definition Debate:** Users like **drfr** and **Stefan-H** debated if "E2EE" is the right term. In traditional messaging (Signal), E2EE is client-to-client. Here, the "recipient" is the Server/AI. The consensus leans toward it being E2EE *if* the server operator is effectively locked out via hardware.
*   **Hardware Trust:** **2bitencryption** and **binary132** voiced skepticism about TEEs, citing Moxie Marlinspike’s previous writing. They argue TEEs aren't magic; they are vulnerable to side-channel attacks and ultimately require trusting the chip manufacturer (e.g., Intel/AMD).
*   **Side Channels:** **shwnz** noted that while TEEs protect against direct memory reads, a compromised host system might still infer data via side channels, meaning it isn't a silver bullet.
*   **Comparison to TLS:** **pxys** asked how this differs from standard SSL/TLS. The answer provided was that TLS only protects data *in transit*. TEEs protect data *during execution* (in memory).

### FOSS in times of war, scarcity and (adversarial) AI [video]

#### [Submission URL](https://fosdem.org/2026/schedule/event/FE7ULY-foss-in-times-of-war-scarcity-and-ai/) | 158 points | by [maelito](https://news.ycombinator.com/user?id=maelito) | [110 comments](https://news.ycombinator.com/item?id=46598991)

FOSDEM 2026: FOSS in times of war, scarcity and (adversarial) AI

- Big idea: A FOSDEM main-track talk argues the post–Cold War conditions that helped FOSS flourish have vanished. Today, open tech sits at the center of geopolitical conflict, disinformation, and “hypercapitalist” power—raising hard questions about “any-use” freedoms and responsibility.
- Dual-use becomes any-use: While Europe tried to regulate “dual-use” tech, libre licenses enable unrestricted use. FOSS now powers everything from social platforms that polarize to authoritarian surveillance stacks, with private capital steering policy and norms.
- Scarcity and climate: With resource limits looming, the talk warns the current trajectory of energy and materials consumption is unsustainable—adding pressure to how FOSS is built, deployed, and maintained.
- AI as Trojan horse: Code-writing LLMs expand the software supply chain’s attack surface. Unlike clearly labeled binary blobs, opaque models can inject subtle, hard-to-detect errors or manipulations, whether through hallucinations or adversarial interference.
- Core question: How can the community preserve FOSS’s openness and public-good impact while hardening against wartime exploitation, political capture, and AI-driven supply-chain risks?

When/where: FOSDEM 2026, Main Track (Janson), Saturday, 10:00–10:50.

**FOSADEM 2026: FOSS in times of war, scarcity and (adversarial) AI**

This submission outlines a main-track presentation for FOSDEM 2026 regarding the existential crisis facing Free and Open Source Software (FOSS). The speaker posits that the geopolitical conditions that allowed FOSS to thrive—the post-Cold War "End of History" era of relative peace and techno-optimism—have collapsed. The talk argues that FOSS is now central to "dual-use" technologies, fueling both democratic platforms and authoritarian surveillance states, often without the regulatory guardrails applied to hardware.

Key points include:
*   **The Any-Use Dilemma:** Libre licenses allow unrestricted use, meaning private capital and hostile states can leverage open tools for polarization or warfare.
*   **Material Limits:** The era of infinite growth is ending; scarcity and climate change require a shift in how resource-intensive software is built and maintained.
*   **AI Risks:** Unlike binary blobs, AI models are "Trojan horses" capable of subtle hallucinations or injected adversarial defects that hard-code manipulation into the supply chain.
*   **The Challenge:** The community must figure out how to preserve public-good openness while hardening the ecosystem against political capture and wartime exploitation.

**Hacker News Discussion Summary**

The discussion considers the limits of software licenses in the face of physical power and state violence.

*   **The End of Techno-Optimism:** Commenters generally agreed with the premise that the 90s "End of History" optimism is over. User **Fiveplus** argued that the current ecosystem suffers from a hangover of that era, assuming good actors by default. There is skepticism that "ethical licenses" can stop bad actors; **lcptn** noted that restrictive licenses are largely irrelevant to hostile states or AI code generators that bypass attribution entirely.
*   **Physical Violence vs. Cryptography:** A significant portion of the debate focused on "XKCD 538" scenario (the $5 wrench attack). **cyber_kinetist** and **nathan_compton** emphasized that state actors use violence, imprisonment, and physical control over hardware, which mathematical encryption cannot prevent. **Espressosaurus** noted that "hard power" involves accessing fiber closets and jail time, rendering software defenses moot.
*   **Technical Mitigations:** Despite the threat of violence, **wizzwizz4** and **thwbgyd** argued for technical architectures that make coercion harder. Suggestions included geographically distributed keys (so no single person can be tortured for access), Shamir’s secret sharing, and "dead man switches." The consensus was that while you cannot make a system "violence-proof," you can organize distributed systems to be resilient against local physical force.
*   **FOSS Survival:** **FrustratedMonky** questioned if FOSS could even be invented in today's "hyper-capitalist" environment, suggesting it requires a specific historical context to flourish. Countering this, **bgyb** argued that the fundamental utility of cost-sharing and efficient distribution ensures FOSS will survive, even if it is co-opted by bad actors (e.g., North Korea using open code for missiles).
*   **Code as Politics:** There was a philosophical debate on whether code itself is political action. **positron26** argued that while text files aren't power, open source enables social mobility and bypasses institutional gatekeepers. However, **lttlstymr** warned against the buzzword "Zero Trust," arguing that trust is never eliminated, just shifted to different commercial providers or hardware.

### The insecure evangelism of LLM maximalists

#### [Submission URL](https://lewiscampbell.tech/blog/260114.html) | 239 points | by [todsacerdoti](https://news.ycombinator.com/user?id=todsacerdoti) | [248 comments](https://news.ycombinator.com/item?id=46609591)

The Insecure Evangelism of LLM Maximalists

A senior developer argues that while LLMs are great as “digital clerks” (research, docs, small code with tight specs), agentic “vibe coding” was a letdown: slow, error-prone, and high-overhead to babysit—leaving them feeling less effective as tokens ticked away. They push back on the trope that skeptics are just scared of change, noting they’d love a world where specs become working code, but today’s agents don’t deliver that for them. Their sharper claim: some loud evangelism may be projection—devs who find agents outperform their own skills insisting holdouts are threatened, rather than considering differences in baseline ability or workflow fit. The author stays open to being wrong (perhaps “not holding the agents properly”) while challenging evangelists to consider that they might simply not be strong programmers. A wry kicker: they’re available to clean up agent-generated code.

Why it matters: Captures a growing split between pragmatic LLM use and end-to-end agent workflows. Beyond hype, adoption hinges on reliability, speed, cost, and whether “agent handling” is a real, teachable skill that beats traditional workflows for experienced engineers.

**Discussion Summary:**

Commenters pushed back strongly against the author's theory that "evangelism" masks incompetence, citing high-profile technical leaders like Simon Willison (Django) and Antirez (Redis) who enthusiastically use LLMs despite having undeniable engineering skills. The conversation reframed the skepticism not as fear of change, but as a clash of engineering philosophies: one user argued that the "hacker drive" is fueled by understanding and fixing deterministic systems, whereas "vibe coding" feels like the drudgery of cleaning up "stochastic messes" (akin to an artist fixing extra fingers on AI images). Others suggested the hostility often stems from corporate pressure—such as managers mandating "minimum AI usage" quotas—rather than the technology itself, while acknowledging that internet culture inevitably polarizes tool usage into "cult-like" camps.

### Mozilla's open source AI strategy

#### [Submission URL](https://blog.mozilla.org/en/mozilla/mozilla-open-source-ai-strategy/) | 187 points | by [nalinidash](https://news.ycombinator.com/user?id=nalinidash) | [196 comments](https://news.ycombinator.com/item?id=46599897)

Mozilla: “Owners, not renters” — an open-source AI push

Raffi Krikorian argues we’re drifting toward “rented intelligence,” where closed AI platforms mediate how we think and act — and can change the rules at will. Mozilla wants to rerun its Firefox playbook: make the “user agent” work for users again, this time at AI’s emerging “Layer 8” that will negotiate, filter, and recommend on our behalf.

Key points:
- Why closed wins today: It’s a developer experience (DX) problem, not a values problem. Big providers bundle models, GPUs, guardrails, monitoring, and billing into a single API that “just works,” while open-source tooling is powerful but fragmented.
- The shift underway: 
  - Small, task-tuned models (1–8B params) are now good and run on existing hardware.
  - Economics favor self-hosting; some companies report major savings moving to open stacks.
  - Governments want sovereign control over AI supply chains.
  - Users expect instant, contextual AI that isn’t locked to one platform.
- Openness will win when it’s the better deal: cheaper, more capable, and just as easy to use — not merely more principled.
- Where cracks are forming: Mozilla sees four tipping points; first up is developer experience, because the stacks and defaults developers choose now will set the future. (The post continues from here.)

The takeaway: Mozilla plans to make open AI competitive on usability and integration so people can own, not rent, the intelligence layer that will soon mediate the web.

Based on the discussion, the core debate centers on whether Mozilla’s pivot to AI is a necessary survival strategy or another "distraction" from its core mission of building a competitive browser.

**The "Distraction" vs. "Diversification" Debate**
*   **Critics of the Pivot:** Many users argue that Mozilla continues to misstep by pouring resources into non-browser projects (VPNs, AI, etc.) while Firefox loses market share. They contend that if Mozilla focused solely on making Firefox the fastest, most reliable engine (addressing long-standing complaints about hardware acceleration on Linux and basic UI speeds), users would return.
*   **The "Innovator’s Dilemma" Defense:** Defenders point out a contradiction in user sentiment: people complain that Mozilla is too reliant on Google search royalties, yet criticize any attempt Mozilla makes to diversify revenue streams (like this AI initiative).
*   **The Threat to Search Revenue:** Several commenters note that the rise of AI threatens the search-licensing model (Google paying Mozilla). Consequently, Mozilla *must* find a new "Layer 8" application to own the user relationship, or they will go bankrupt when the search deal eventually dissolves.

**Firefox Quality and Management**
*   **Feature Velocity:** A Mozilla employee and other users noted recent shipping wins (vertical tabs, profile switchers, Rust integration), but critics feel development is too slow compared to the budget.
*   **Budget Efficiency:** Comparisons were drawn to the **Ladybird** browser project, which is building a new engine with a tiny fraction of Mozilla’s staff/budget, leading users to question the efficiency of Mozilla’s $500M+ annual spend.
*   **Endowment Strategy:** There was debate over Mozilla's financial endowment (estimated over $1B). Some view it as a safety net that should be hoarded to keep Firefox alive indefinitely; others see it as a "VC fund" that must be deployed to invent the future of the web (and revenue) before the current model collapses.

**Hypothetical Scenarios**
*   **A Cloudflare Acquisition?** A sub-thread discussed the idea of Cloudflare acquiring Firefox/Mozilla, citing the shared talent pool (heavy usage of Rust) and alignment against Google. However, others pointed out the irony, as Cloudflare’s bot protections/CAPTCHAs are notoriously hostile to Firefox users.

**Consensus**
While the community generally supports the *ideal* of open AI, there is deep skepticism that Mozilla can execute it successfully, given their perceived history of neglecting the core localized browser experience in favor of "moonshot" projects.

### Why we don’t use AI

#### [Submission URL](https://yarnspinner.dev/blog/why-we-dont-use-ai/) | 110 points | by [parisidau](https://news.ycombinator.com/user?id=parisidau) | [71 comments](https://news.ycombinator.com/item?id=46609279)

Why We Don't Use AI – Yarn Spinner team says no to generative AI, on principle

- The maintainers of Yarn Spinner, a widely used open‑source dialogue tool for games, lay out why they neither use nor integrate generative AI—and won’t accept AI‑generated contributions.
- Core claim: today’s AI products are largely designed to cut headcount or extract more work without new hires; the team doesn’t want to normalize or fund that ecosystem.
- Background: they have deep ML experience (research, books, game bots) and were once enthusiastic as tooling/GPU access improved. Around 2020, they soured as industry focus shifted to generative imagery/chatbots, mitigation work was sidelined, and critics were pushed out.
- Beyond labor: they acknowledge many other issues (bias, opacity, etc.). Even if labor harms were solved, more hurdles would remain—but they’re arguing one major point at a time.
- Product philosophy: reject “tool‑driven development” (“use AI or be left behind”) in favor of “does this help make better games?” They prefer fewer, polished features over hype integrations and will add/remove features based on real dev needs.
- No “ethical DIY AI” for now: building bespoke models would be time‑intensive, and their example could nudge others toward mainstream AI platforms they object to.
- Future stance: open to revisiting ML if the landscape changes meaningfully; until then, no generative AI in Yarn Spinner or their workflow.

Why it matters: A prominent game tooling project is drawing a clear line against generative AI on ethical and practical grounds, signaling to studios and open‑source communities that “not adopting AI” can be a deliberate, product‑focused choice rather than a lagging one.

Here is a summary of the discussion:

**Summary of Discussion**

The Yarn Spinner team's rejection of generative AI sparked a polarized debate on Hacker News, moving quickly from the specific tool to broader questions about labor, economic systems, and the utility of coding assistants.

*   **Nuance and the "Indie" Defense:** Several users felt the maintainers’ stance lacked nuance, conflating "Enterprise Scale Replacement" (firing 500 support staff) with "Assistive Tooling" for solo developers. Commenters argued that for small indie teams, AI is a vital force multiplier that unblocks tricky coding problems or generates assets they couldn't otherwise afford, drawing a distinction between corporate greed and indie survival.
*   **The Utility Debate:** A sub-thread debated whether LLMs actually help with "tricky" coding problems. Skeptics argued that LLMs fundamentally cannot solve novel architectural problems, often providing hallucinated or "junior-level" code that experienced engineers have to fix. Others pushed back, stating that regardless of perfection, the tools provide necessary efficiency for developers working alone.
*   **Automation vs. Capitalism:** The "jobs" argument triggered a philosophical discussion on luddism and automation. While users generally agreed that eliminating toil (like tractors or dishwashers) is historically positive, they argued that the current economic system makes AI a threat. The consensus among several commenters was that the technology isn't the enemy; rather, the lack of a social safety net (like UBI) turns labor-saving technology into a livelihood-destroying crisis.
*   **Virtue Signaling Accusations:** A segment of the discussion dismissed the post as "virtue signaling" or part of a culture war, suggesting that condemning the technology outright is a performative stance that ignores the practical realities of modern software development.

### Anthropic invests $1.5M in the Python Software Foundation

#### [Submission URL](https://discuss.python.org/t/anthropic-has-made-a-large-contribution-to-the-python-software-foundation-and-open-source-security/105694) | 392 points | by [ayhanfuat](https://news.ycombinator.com/user?id=ayhanfuat) | [174 comments](https://news.ycombinator.com/item?id=46601902)

Anthropic is donating $1.5M over two years to the Python Software Foundation, earmarked primarily for Python ecosystem security. The PSF says the gift will accelerate its security roadmap—particularly hardening PyPI against supply‑chain attacks—and also support core operations like the CPython Developer-in-Residence program, community grants, and running infrastructure such as PyPI.

HN chatter: One commenter notes the gift is tiny relative to big‑tech scale (e.g., 1.5M is ~0.005% of 30B), hinting at PR optics, while the PSF frames it as a landmark contribution with outsized impact for Python’s community and infrastructure.

**Governance and Corporate Influence**
A significant portion of the discussion focused on the PSF's leadership structure, sparked by a commenter questioning the optics of a Microsoft employee serving as the PSF Board Chair.
*   **The Critique:** Skeptics argued that having an employee of a major tech corporation ($15B+ revenue) leading the board of a critical non-profit creates potential conflicts of interest or "unconscious bias," drawing comparisons to a government official regulating their private sector industry.
*   **The Defense:** Community members (including PSF insiders) clarified that the Board is distinct from the **Python Steering Council** (which handles technical decisions). They noted that board positions are unpaid volunteer roles, the Chair has limited executive power compared to the Executive Director, and bylaws restrict the number of board members from any single company (max 1/3 or 2 members).
*   **Context:** Defenders emphasized that the current Chair served the Python community long before joining Microsoft and that questioning their ethics based solely on employment is unfounded.

**The value of $1.5M**
*   **"Cheap PR":** Some users felt the donation was "tiny" relative to Anthropic's valuation and the immense value they derive from the ecosystem, labeling it an inexpensive way to buy good press.
*   **"Better than nothing":** Counter-arguments highlighted that many massive industries (specifically Investment Banking and Finance) run entirely on Python but contribute $0. Users argued that criticizing a donation creates a perverse incentive; instead, the community should applaud the funding to normalize corporate support for open-source infrastructure.

**Strategic Utility**
*   Commenters noted that this is a pragmatic investment for Anthropic rather than pure charity. Because LLMs (like Claude) generate millions of lines of Python code and rely on the ecosystem's stability, hardening PyPI against supply-chain attacks directly reduces Anthropic's own risk vector.

### Signal leaders warn agentic AI is an insecure, unreliable surveillance risk

#### [Submission URL](https://coywolf.com/news/productivity/signal-president-and-vp-warn-agentic-ai-is-insecure-unreliable-and-a-surveillance-nightmare/) | 334 points | by [speckx](https://news.ycombinator.com/user?id=speckx) | [102 comments](https://news.ycombinator.com/item?id=46605553)

Signal leadership warns agentic AI is a security and privacy disaster in the making

At 39C3 in Hamburg, Signal’s Meredith Whittaker (President) and Udbhav Tiwari (VP of Strategy & Global Affairs) argued that agentic AI—especially when embedded at the OS level—creates a surveillance-friendly, malware-prone attack surface and remains too unreliable for autonomous tasks.

What they highlighted
- OS-level agents as “forensic dossiers”: Microsoft’s Recall was cited as a cautionary example—frequent screenshots, OCR, semantic tagging, and a timeline of user activity consolidated into a single local database. Malware or indirect prompt-injection could exfiltrate it, effectively sidestepping end-to-end encryption by capturing plaintext on-device. Signal added a “prevent screen capture” flag but says that’s only triage.
- Reliability breaks down fast: Agents are probabilistic. Even optimistic per-step accuracy compounds poorly—at 95% per step, a 10-step task drops to ~60% success; 30 steps to ~21%. At 90% per step, a 30-step task is ~4%. They said the best agent models still fail ~70% of the time.
- Enterprise and consumer risk: Centralized life-logs plus autonomous actions invite catastrophic data leakage, regulatory exposure, and an erosion of user trust in already overhyped tech.

Their prescriptions
- Hit pause on OS-level life-logging and plaintext, all-in-one databases that malware can grab.
- Default to opt-out for users and require explicit developer opt-ins for integrations.
- Provide radical transparency and granular auditability of how agents collect, store, and act on data.

Bottom line: Without a course correction, they warn the agent era could implode under security failures and backlash—long before it delivers on its promises.

Based on the discussion, commenters largely shifted the blame from "Agentic AI" specifically to the fundamental failures of modern Operating System (OS) security models.

**The "OS Problem" vs. The "AI Problem"**
*   **Fundamental Flaws:** Several users argued that the risks posed by Agentic AI are actually symptoms of insecure-by-design operating systems. They noted that mainstream systems (Windows, standard UNIX) were not designed to assume software is untrusted. One commenter noted that while Microsoft sells computers, they have not historically prioritized designing strict security models.
*   **Better Models Exist:** Participants pointed to niche or mobile operating systems (Plan 9, seL4, Fuschia, Qubes OS) as examples of architectures that handle untrusted processes better. iOS was frequently cited as a mainstream success story where applications are sandboxed by default and must explicitly request capabilities (e.g., access to contacts or location), unlike desktop environments where network and file access are often open by default.

**Complexity and Developer Incentives**
*   **Security vs. Usability:** There was a debate regarding why secure OSs aren't the standard. Some argued that implementing "secure-by-default" systems is a "tedious nightmare" for developers (citing difficulties with TLS and immutable filesystems). Others countered that current security measures (firewalls, patching) are merely "security theater" required to patch bad architectural design.
*   **Market Forces:** Commenters suggested that secure systems like Andrew Tanenbaum’s *Amoeba* existed decades ago but failed because the market prioritizes speed, backward compatibility, and developer ease-of-use (e.g., `npm run`) over strict security boundaries.
*   **Containerization Blame:** Some criticism was directed at container ecosystems (Docker/OCI) for encouraging bad practices, such as allowing permission binds and refusing to disable privileged flags by default, effectively bypassing OS security for convenience.

**The Changing Threat Model**
*   **Adversarial Software:** One user noted that legacy security models assumed the user controlled the software. The modern era—and specifically Agentic AI—introduces an adversarial model where software runs *against* the user's interests (telemetry, surveillance), rendering old "user permission" models insufficient.
*   **Network Assumptions:** The discussion highlighted that while UNIX was multi-user, early desktop security models did not anticipate the modern "always-connected" internet, leaving desktop clients blindly trusting local networks—a concept that Agentic AI exploits.

### Games Workshop bans staff from using AI

#### [Submission URL](https://www.ign.com/articles/warhammer-maker-games-workshop-bans-its-staff-from-using-ai-in-its-content-or-designs-says-none-of-its-senior-managers-are-currently-excited-about-the-tech) | 226 points | by [jsheard](https://news.ycombinator.com/user?id=jsheard) | [122 comments](https://news.ycombinator.com/item?id=46607681)

Games Workshop bans AI in Warhammer content and design

- CEO Kevin Rountree says staff are prohibited from using generative AI in any official content or design work, including entries to GW-run competitions. A few senior managers are allowed to explore the tech, but leadership is “not excited” about it.
- Rationale: protect IP, preserve human-made art/writing/sculpting, and reduce data/security/compliance risks as AI features creep onto devices by default.
- GW is doubling down on human creatives, expanding hiring across concept art, writing, and sculpting for its Warhammer Studio.
- Context: Warhammer’s community is highly protective of its art; even the suspicion of AI in official imagery has sparked backlash (e.g., a recent Displate controversy the company denied).
- Contrast: While publishers like EA and Square Enix tout aggressive AI adoption, GW is positioning itself as a human-first outlier in entertainment.

Why it matters: For a lore- and art-driven brand, trust in authenticity may outweigh AI’s efficiency gains. Expect stricter partner guidelines, more IP enforcement, and GW marketing its “human-made” pedigree as a differentiator.

Here is a summary of the discussion:

**Community Alignment vs. Practical Utility**
Commenters largely agreed that Games Workshop made a savvy business move, noting that the tabletop community zealously dislikes "tech bro" trends like NFTs and crypto. By taking a hardline anti-AI stance, GW avoids a "multi-year headache" with their core customer base. However, one user noted a contradiction: while non-technical board gamers hate AI art, many admitted to using AI to generate code for websites and apps because professional programmers remain too expensive to hire.

**The Double Standard: Art vs. Code**
A significant portion of the discussion focused on why the public views AI replacing artists as a tragedy, but AI replacing programmers as progress. Several theories emerged:
*   **Sympathy and Class:** Artists are generally viewed as struggling creatives, garnering public protection. Conversely, software engineers are seen as high-paid, "entitled," and "arrogant." Some users suggested the non-tech world feels a sense of *schadenfreude* at the prospect of AI "spanking" the comfortable tech labor market.
*   **Nature of the Work:** One user argued that people view programming as "assembly line work" rather than a creative endeavor. Code is often refactored, merged, and owned collectively, whereas art has a strong attachment to specific authorship and authenticity.
*   **Training Data Ethics:** There was a debate regarding the fairness of training models. While AI art generators scraped copyrighted portfolios (like ArtStation) against artists' wishes, AI coding assistants were largely trained on open-source code. However, critics countered that "Open Source" does not mean "Public Domain," and licenses like GPL still carry legal weight that AI training ignores.

**The "Revenge" of the Creatives**
A particularly heated thread highlighted the irony of the current moment. A user identifying as both an artist and developer noted that when technology crushed the livelihoods of traditional artists and manufacturing workers, the tech industry’s response was often dismissed as "adapt or die." Now that the same technology threatens white-collar coding jobs, the sudden concern from developers feels hypocritical to those who were previously told their displacement was just "progress."