import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Thu Jul 27 2023 {{ 'date': '2023-07-27T17:10:29.636Z' }}

### LeMUR: LLMs for Audio and Speech

#### [Submission URL](https://www.assemblyai.com/blog/lemur/) | 119 points | by [ramie](https://news.ycombinator.com/user?id=ramie) | [23 comments](https://news.ycombinator.com/item?id=36900294)

AssemblyAI has announced the general availability of LeMUR, a single API that enables developers to reason over spoken data using a combination of automatic transcription, prompt augmentation, compression strategies, retrieval techniques, language models, and structured outputs. LeMUR can be used to summarize meetings, extract key points of discussion, generate action items, answer questions about spoken data, and generate titles and descriptions. The API is highly accurate on core tasks and can be customized to suit specific use cases. LeMUR is accessible through AssemblyAI's API and can be tried out for free using the Playground or by signing up for a free API token.

The discussion on Hacker News about the announcement of AssemblyAI's LeMUR API covers a range of topics and opinions. 

One user mentions that they find the user experience of the API documentation to be genuinely poor, with blurred text and low contrast. 

Another user congratulates AssemblyAI on the launch and suggests that if they add support for Universal Summarizer 1, it will cater to more advanced use cases. They also mention that a paid API is available after the free trial period. 

Some users discuss the technical aspects of the API. One user suggests downplaying the use of the song name feature for transcription, while another user mentions that the ASR model in LeMUR is trained on 11 million hours of data. 

A few users express enthusiasm for using the API through platforms like Google Colab. 

A discussion ensues about comparing LeMUR to other speech-to-text APIs, with one user mentioning Deepgram as having impressive performance in text transcription. 

A user recommends trying out the API through Google Colab and compares the results of using the API versus building a model with 100 hours of data. 

There is a mention of OpenAI's results and a link to a project that uses OpenAI's API to record and transcribe audio. 

Some users find the API useful for skipping unnecessary content during transcription, while others point out a UI issue related to the settings button. 

One user jokingly suggests that "Lemur" clashes with the naming conventions of other animal-themed products. 

Overall, the discussion covers various technical aspects, comparisons, and user experiences with AssemblyAI's LeMUR API.

### Foundation models are going multimodal

#### [Submission URL](https://app.twelvelabs.io/blog/foundation-models-are-going-multimodal) | 26 points | by [fzliu](https://news.ycombinator.com/user?id=fzliu) | [9 comments](https://news.ycombinator.com/item?id=36896335)

Today's top story on Hacker News is about the emergence of multimodal foundation models. These models, such as BERT, GPT-3, CLIP, and Codex, have shown impressive capabilities in tasks that combine vision and language modalities. The blog post provides an overview of foundation models, their architecture, training and fine-tuning paradigm, and the scaling laws behind them. It also explores how vision-language models are being used to solve complex problems and introduces the new paradigm of video foundation models, which are revolutionizing the understanding and analysis of video data. The article offers a gentle introduction to foundation models, explaining their self-supervised learning approach and how they can learn general patterns from large amounts of data. It also discusses the concept of transfer learning, where models trained on one task can be adapted to perform well on another task. In the field of computer vision, this has been done by pre-training models on a large dataset like ImageNet and fine-tuning them for specific tasks. In natural language processing, pre-training initially focused on word embeddings, but later expanded to language models like ELMo, ULMFiT, and GPT. The article also highlights Transformers as the underlying architecture for foundation models and explains how it revolutionized NLP by parallelizing language processing. Overall, the blog post provides a comprehensive overview of multimodal foundation models and their potential impact in various domains.

The discussion on this submission revolves around different viewpoints regarding the importance and potential risks of multimodal foundation models. One commenter questions the contribution of high-performance video surveillance to society, expressing concerns about privacy and potential negative consequences. Another commenter agrees with the concern and highlights the need for AI oversight in public spaces, particularly in relation to surveillance. They discuss the potential benefits and downsides of transparent access systems and effective critical reviews. 

In response to these concerns, another commenter suggests that existential downsides can be addressed through international cooperation efforts. They mention that relying solely on powerful entities like China could lead to significant differences and potential challenges. They emphasize the importance of medical advancements and potential gains in the field of AI. 

The discussion takes a turn when one commenter calls out the use of buzzwords in the article's title and expresses indifference towards reading such articles on Hacker News. Lastly, another commenter jokingly suggests that humans have not yet achieved true optimization and control over long-term global issues, comparing it to the situation of wolves and moose on Isle Royale.

Overall, the discussion touches on concerns regarding surveillance and privacy, the importance of transparent access systems, potential benefits and risks of foundation models, and the need for international cooperation in the field of AI.

### Llama and ChatGPT Are Not Open-Source

#### [Submission URL](https://spectrum.ieee.org/openai-not-open) | 137 points | by [webmaven](https://news.ycombinator.com/user?id=webmaven) | [123 comments](https://news.ycombinator.com/item?id=36900388)

Meta, the social media and advertising-technology company, recently released an update to its large language model (LLM) called Llama. While Meta claims that Llama 2 is open source, researchers argue that it falls short of true openness. While Meta has made the trained model available, it has not shared the training data or the code used to train it. In a study presented at a conference, researchers assessed the openness of 21 different nominal open-source LLMs and found Llama 2 to have limited accessibility. Similar criticisms were made against OpenAI's ChatGPT model. The researchers argue that the misleading use of the term "open source" raises concerns about reproducibility and transparency in AI research.

The discussion on Hacker News revolves around the submission about Meta's release of its large language model Llama 2. Some users argue that while Meta claims Llama 2 is open source, it falls short of true openness because it does not share the training data or code. A study comparing 21 different open-source language models found that Llama 2 had limited accessibility. Similar criticisms were made against OpenAI's ChatGPT model. Researchers raise concerns about reproducibility and transparency in AI research due to misleading use of the term "open source."

One user points out that Mark Dingemanse's report highlights the lack of documentation and transparency regarding Llama 2, suggesting that Meta is not providing the necessary information to understand the model's training history.

Another user initially shares their personal experience with Meta's historical choices, highlighting concerns about the company's track record. However, their comment is later removed.

An academic researcher brings up Mark Dingemanse's background in linguistics and suggests that his assessment of LLMs is relevant because language models have an impact on society. Their comment is followed by another user questioning if the background information provided is relevant to the discussion and suggesting that the dangers of using LLMs released by untrustworthy companies should not be ignored.

A user expresses skepticism about Meta's history of releasing source code and mentions that they generally have a good impression of the company. This comment receives a response suggesting that their familiarity with Meta's history may inform their perspective.

The original poster responds, stating that they are sympathetic to the concerns but believe that Llama's work on network connections is important.

One user argues that personal companies should release their training data to leverage the collective intelligence. They highlight the issue of copyrighted material, explicit content, and political biases in language models and advocate for reducing biased models. Another user responds, questioning why copyrighted material hinders celebrating tech and suggesting that artists deserve their rights and intellectual property should be protected.

A user argues that it is reasonable to expect companies to follow copyright laws and obtain legal permission for using copyrighted works. They draw a comparison with Microsoft requiring users to purchase a copy of software rather than pirating it.

The discussion touches on the topic of copyrighted meeting materials and the potential for language models to steal artists' work and knowledge. A user argues that it is the responsibility of tech companies to address these issues.

One user makes a sarcastic comment about the mental gymnastics required to submit revised work publicly.

Overall, the discussion revolves around concerns about the lack of transparency and reproducibility in Meta's release of Llama 2 and the implications for AI research. The discussion also touches on issues related to copyright and the responsibility of tech companies in handling intellectual property and copyrighted material.

### How to scale LLMs better with an alternative to transformers

#### [Submission URL](https://hazyresearch.stanford.edu/blog/2023-07-25-m2-bert) | 153 points | by [tuxguy](https://news.ycombinator.com/user?id=tuxguy) | [31 comments](https://news.ycombinator.com/item?id=36890036)

Researchers at an undisclosed lab have been exploring alternative architectures to the popular Transformer model, and they have now unveiled their latest creation: Monarch Mixer BERT (M2-BERT). Unlike traditional Transformers, M2-BERT is sub-quadratic in both sequence length and model dimension, making it more efficient. It also has 25% fewer parameters and matches the quality of Transformers. The researchers achieved this by replacing the major elements of a Transformer with Monarch matrices, which are structured matrices that are hardware-efficient and expressive. They believe that this new architecture could be a game-changer in the field of natural language processing. The full arXiv paper will be released soon, and the researchers will be presenting their work at the ES-FoMo workshop at ICML.

The discussion on this submission covers various topics related to the use of large language models (LLMs) and their architectures. 

- One user points out that GPT-4, a prominent LLM, works by combining multiple expert LLMs and selecting the most relevant response. Another user questions if this approach could result in biased outputs.
- The use of LLMs for tasks like translation and understanding common knowledge is discussed. It is noted that the quality of answers from LLMs can improve with better compute costs and smarter filtering of responses.
- The potential applications of LLMs in education and teaching are suggested, with a link provided to an article on the topic.
- The feasibility of selecting multiple responses from LLMs and the challenge of interpreting their outputs accurately is discussed.
- The credibility of claims regarding the number of parameters in GPT-4 is questioned, and references to sources are provided for further reading.
- The usage of decentralized hierarchical LLMs and the importance of data quality over quantity are highlighted.
- The relevance of Hugging Face, a popular framework for natural language processing, is mentioned.
- The mention of "assembly learning" by one user prompts another user to suggest the term "ensemble learning" instead.
- The FlashAttention model is mentioned as a significant improvement over the Transformer model.
- The introduction of Monarch metrics, a new concept related to model weights, is welcomed with excitement.
- The discussion ends with a brief comment about the conjunctions of topics.

Overall, the discussion delves into various aspects of LLMs, their architectures, and the potential applications and challenges associated with them.

### Data diffs: Algorithms for explaining what changed in a dataset (2022)

#### [Submission URL](https://blog.marcua.net/2022/02/20/data-diffs-algorithms-for-explaining-what-changed-in-a-dataset.html) | 198 points | by [winkywooster](https://news.ycombinator.com/user?id=winkywooster) | [20 comments](https://news.ycombinator.com/item?id=36888667)

Today's top story on Hacker News explores the concept of explanation algorithms and introduces an open-source implementation of one such algorithm in the datools library. Explanation algorithms are used to answer the question "why?" in data analysis, going beyond simple reporting of numbers and delving into the reasons behind the data trends. Currently, most data analysis involves ad hoc queries and pivot tables to explain changes in datasets over time. The academic community has been working on developing explanation algorithms to automate this process and identify high-likelihood explanations in datasets. One approach, called Scorpion, focuses on explaining why an aggregate value is higher or lower than other similar data points. It operates on aggregates and allows users to highlight outliers on charts to ask why those points are so high or low. However, Scorpion requires processing data outside of the database and is specific to aggregates. Another approach, introduced in the DIFF paper, is an explanation algorithm expressed as a database operator called DIFF, which can be implemented in SQL. It compares two sets of data and identifies the differences between them, providing an explanation for the disparities. DIFF can be implemented on top of most relational databases and offers a practical solution for running explanation algorithms. The open-source implementation mentioned in the article is available in the datools library, making it accessible to data analysts who work with relational databases and love SQL. This development is exciting because it simplifies the process of running explanation algorithms and enables better understanding of data trends and changes.

The discussion on this submission covers various topics related to explanation algorithms and data analysis. Here are some of the key points:

- Dolt and TerminusDB are mentioned as potential tools for modeling and managing data.
- The implementation of the DIFF algorithm in SQL using Apache Calcite is suggested, which allows for easier comparison of two sets of data.
- There is a discussion about using Spark and the DIFF extension for data migration and bug discovery.
- The importance of using machine-readable formats, such as CSV or Datasette, for data analysis is highlighted.
- Some users discuss the benefits and challenges of using minimal cardinality and pruning in data analysis.
- The topic of version control for datasets is brought up, with the mention of DVC (Data Version Control) and Git LFS (Large File Storage).
- The idea of creating a backend labeling workflow for reviewing and tracking changes in datasets is mentioned.
- Other users suggest alternative tools and libraries for data analysis and version control, such as Diff Transform, lakeFS, and OpenStreetMap Overture Maps.

Overall, the discussion revolves around the practical implementation and potential applications of explanation algorithms in data analysis.

### Chidori – Declarative framework for AI agents (Rust, Python, and Node.js)

#### [Submission URL](https://github.com/ThousandBirdsInc/chidori) | 148 points | by [transitivebs](https://news.ycombinator.com/user?id=transitivebs) | [39 comments](https://news.ycombinator.com/item?id=36887412)

Introducing Chidori: A Reactive Runtime for Building Durable AI Agents

Thousand Birds Inc. has released Chidori, a reactive runtime for building AI agents. Chidori provides a framework for building AI agents that are reactive, observable, and robust. It supports building agents with Node.js, Python, and Rust. Chidori is currently in alpha and is not yet ready for production use, but Thousand Birds Inc. is actively making changes based on feedback to improve the platform.

Key Features of Chidori:
- Built from the ground up for constructing agents
- Runtime written in Rust, supporting Python and Node.js out of the box
- Optimized for long-running AI workflows
- Embedded code interpreter for enhanced flexibility
- Time travel debugging for efficient troubleshooting

Installation of Chidori is straightforward, with support for Node.js, Python, and Rust. Chidori also requires setting specific environment variables depending on the nodes used. The framework includes examples in Node.js, Python, and Rust, which demonstrate how to build a simple agent that fetches top stories from Hacker News and filters them using the OpenAI API to only display AI-related launches.

Chidori is designed to minimize cost during development through LLM caching and supports visualization of results using the prompt-graph-ui project.

Thousand Birds Inc. encourages developers to check out Chidori, star the repository on GitHub, and join the Discord community for further engagement. While Chidori is still in its early stages, Thousand Birds Inc. welcomes feedback and contributions to make the framework even better.

Overall, Chidori aims to provide a powerful and reliable runtime for building AI agents, enabling developers to create durable AI solutions. With its reactive and observable nature, Chidori has the potential to streamline AI development and improve agent performance in various domains.

The discussion surrounding the submission starts with a user expressing interest in using Chidori's LLM (Language Model) capabilities but struggling to understand how the library makes things easier. Another user responds, mentioning the contrast between traditional long-running services using LLMs and Chidori's attempt to synchronize LLM execution with event-driven systems. They suggest that this approach could be beneficial for managing complexity in AI agent behavior.

Another user highlights Chidori's features, including its implementation in Rust and support for time-travel debugging. They mention that building and debugging reactive agents can be challenging but express excitement about the possibilities that Chidori offers.

A user shares their positive experience with the Chidori framework, mentioning that it is written in Rust and supports many features like time-travel debugging and an embedded code interpreter. They recommend Chidori to others and express their gratitude towards the developers.

Another user expresses interest in Chidori and asks if there is any documentation or feedback they could provide. A response suggests joining the Discord community or reaching out on Twitter for specific questions or feedback.

One user comments on the similarity between the name "Chidori" and the iconic technique from the Naruto anime series. This prompts other users to make references to Naruto, with some jokingly suggesting other anime references like Rasengan and Sharingan.

The discussion then shifts to the integration of local LLMs and support for OpenAI. A user mentions that Chidori currently supports OpenAI but is interested in patterns for supporting local LLMs to enable more independent work. Another user agrees and expresses their desire to build a single binary local command-LLM interface in Chidori.

There is a mention of a smaller target related to agent protocols, which was submitted recently. The user praises the Chidori team's work and expresses interest in discussing the protocol's direction and how it can help Chidori in the long run.

Some comments are made about the OpenAI API key requirement, with one user noting that it is a potential hurdle, and another user jokingly mentioning that the requirement is a "permanent" flaw.

The discussion also includes some lighthearted banter and references to different programming languages and concepts.

Overall, the discussion shows a mix of users expressing interest in Chidori, praising its features, and seeking further information and ways to contribute. There is also some playful conversation around references to popular culture.

### OverflowAI

#### [Submission URL](https://stackoverflow.blog/2023/07/27/announcing-overflowai/) | 91 points | by [lqet](https://news.ycombinator.com/user?id=lqet) | [96 comments](https://news.ycombinator.com/item?id=36892311)

Stack Overflow Labs just announced their roadmap for integrating generative AI into their public platform, Stack Overflow for Teams. They are introducing new features such as semantic search, which will intelligently align search queries with relevant topics. They are also enhancing search capabilities for Stack Overflow for Teams, allowing users to quickly find relevant answers and discover related knowledge from trusted sources. Another new capability is enterprise knowledge ingestion, which allows users to curate and build a knowledge base quickly by leveraging existing content. AI will create initial drafts of tagging structures and recommend questions and answers based on areas where the team frequently requires documentation or solutions. Stack Overflow is also integrating their knowledge base with StackPlusOne, a chatbot that provides instant solutions to technical challenges in Slack. In addition, they are developing an IDE extension for Visual Studio Code powered by AI, allowing developers to find personalized solutions without disrupting their workflow. To support the community's knowledge sharing, they are launching GenAI Stack Exchange for discussions on prompt engineering, AI tools, and the evolving ecosystem. Stack Overflow's Natural Language Processing Collective will also include a new feature called Discussions, providing a space for debating technical approaches and sharing perspectives.

The discussion about the submission on Hacker News revolves around several topics. 

- Some users discuss the accuracy and quality of the generated answers by the language model (LLM). There is a debate about whether LLMs truly understand the content and how susceptible they are to producing incorrect or nonsensical answers. Some users express concerns about plagiarism and whether LLMs can reliably generate original content. Others argue that LLMs can provide valuable answers but should be used with caution.

- Another point of discussion is the role of AI in providing answers on platforms like Stack Overflow. Some users express skepticism and believe that relying solely on AI-generated answers may not be trustworthy. They argue that humans' subjective judgment and expertise are crucial in validating the accuracy of answers.

- Various users raise questions about the purpose and relevance of integrating AI into Stack Overflow. Some wonder if it is just a buzzword or if it will truly bring value to developers and improve their productivity. There are also discussions about the potential drawbacks and limitations of AI in this context.

- Some users question the motives behind the integration of AI into Stack Overflow and compare it to other AI-related trends in the industry. They express concerns about the hype around AI and the possibility of overemphasizing its capabilities.

- Lastly, there are comments suggesting alternative approaches to improving developer productivity, such as focusing on specific workflows or using AI as a complementary tool rather than a complete replacement.

Overall, the discussion reflects a range of opinions about the potential impact and effectiveness of integrating AI into Stack Overflow for Teams.

### Google Med-Palm M: Towards Generalist Biomedical AI

#### [Submission URL](https://arxiv.org/abs/2307.14334) | 106 points | by [panabee](https://news.ycombinator.com/user?id=panabee) | [85 comments](https://news.ycombinator.com/item?id=36888948)

A new research paper titled "Towards Generalist Biomedical AI" proposes the development of a generalist artificial intelligence (AI) system for the biomedical field. The authors argue that medicine is inherently multimodal, with data spanning text, imaging, genomics, and more. They curate a new multimodal biomedical benchmark called MultiMedBench, which includes 14 diverse tasks like medical question answering, image interpretation, report generation, and genomic variant calling. 

The authors then introduce Med-PaLM Multimodal (Med-PaLM M), a proof-of-concept generalist biomedical AI system. Med-PaLM M is a large multimodal generative model that can encode and interpret biomedical data including clinical language, imaging, and genomics, all with the same set of model weights. The researchers find that Med-PaLM M performs competitively with or even exceeds specialist models on all MultiMedBench tasks, often by a wide margin. 

The paper also reports zero-shot generalization to novel medical concepts and tasks, positive transfer learning across tasks, and emergent zero-shot medical reasoning. In addition, a radiologist evaluation of model-generated chest X-ray reports shows promising performance. In a side-by-side ranking on 246 retrospective chest X-rays, clinicians express a pairwise preference for Med-PaLM M reports over those produced by radiologists in up to 40.50% of cases, indicating potential clinical utility. 

While the models developed in this study still need to be validated in real-world use cases, the results represent a significant step towards the development of generalist biomedical AI systems. The integration of multiple data modalities and the flexibility to interpret diverse biomedical data could create impactful applications in scientific discovery and care delivery.

The discussion on this submission covers various aspects of the proposed generalist biomedical AI system and raises concerns about its potential implications in the medical field. 

One commenter emphasizes the limitations of language models like ChatGPT when it comes to medical diagnosis, highlighting the importance of human physicians who possess specialized knowledge and experience. Another commenter argues that it is the responsibility of doctors to consult legal professionals rather than relying on AI systems. 

Some users express concerns about the liability associated with relying on AI models for medical diagnoses, questioning the risk of significant harm if something were to go wrong. Others bring up the challenges of implementing AI systems in real-world medical practice, including issues related to insurance reimbursement rates and the perception of AI among healthcare professionals.

The performance of the AI system is also discussed, with one commenter noting that human radiologists preferred the model-generated reports in 40.50% of cases in a comparative study. However, skeptics point out the need for larger studies to validate the performance of the model in real-world scenarios. 

There are also comments discussing the potential utility and applications of generalist AI systems in scientific discovery and patient care. Some argue for the importance of integrating multiple data modalities and the potential benefits of virtual consultations and pre-screening using AI.

Other discussions touch upon the challenges of building and training large models, the need for proper evaluation of model performance, concerns about biased data and inconsistency in medical practice, and the commercialization of AI in healthcare. Some commenters express skepticism about the current state of AI in medicine and highlight the importance of continuing research and development.

### Absolute Unit NNs: Regression-Based MLPs for Everything

#### [Submission URL](https://gwern.net/aunn) | 16 points | by [nirvael](https://news.ycombinator.com/user?id=nirvael) | [3 comments](https://news.ycombinator.com/item?id=36891609)

A proposal has been put forward for a general neural network (NN) architecture that can handle arbitrary tasks and scale up MLPs (multi-layer perceptrons). The architecture, called Absolute Unit NN (AUNN), aims to enable meta-learning prediction of arbitrary data inputs and outputs. The training data is encoded into a list, and the NN is trained to predict from the one-dimensional unit input of the absolute index of a data point to that data point unit. This allows the NN to generalize and rapidly learn new datapoints in a single gradient descent step. The AUNN architecture has several advantages, including simplicity, minimal inductive bias, generality of input/output, and hardware-friendliness. It also has potential applications in language-conditioned AUNNs and modular brain AUNNs. However, one disadvantage is that it may require a large scale of data and compute before it can effectively generalize and meta-learn. The proposal draws inspiration from various existing NN architectures and methodologies, such as self-supervised Transformers, neural radiance fields, and meta-reinforcement learning. The goal is to extend the capabilities of MLPs to handle diverse input/output modalities without the need for complex and computationally expensive dense layers.

In the discussion on Hacker News, there were a couple of comments. One commenter mentioned that this proposal reminded them of non-verbal reasoning and index learning, providing a link to a related article. Another commenter expressed their enthusiasm for the proposal and described it as amazing, also sharing a link to a GitHub repository. In response to this comment, another user suggested that they are working on a similar project using neural radiance fields (NeRF) and mentioned that implementing it should not be too difficult.

### Show HN: Litellm – Simple library to standardize OpenAI, Cohere, Azure LLM I/O

#### [Submission URL](https://github.com/BerriAI/litellm) | 61 points | by [ij23](https://news.ycombinator.com/user?id=ij23) | [15 comments](https://news.ycombinator.com/item?id=36887711)

📢 Introducing litellm: A Lightweight Package for Simplifying LLM API Calls

BerriAI has released litellm, a 100-line package designed to streamline API calls to Azure, OpenAI, Cohere, and Anthropic. This package simplifies the process of managing and translating input/output for these platforms, ensuring consistent output. Now you can seamlessly connect to these APIs and retrieve text responses with ease. The project is open source and available under the MIT license. With over 133 stars and 4 forks on GitHub, it's clear that litellm is gaining popularity among developers. So why wait? Install litellm today and simplify your API integration process. For more information, contact the BerriAI team at ishaan@berri.ai or krrish@berri.ai.

The discussion on the submission includes various comments and interactions between users:

- "d4rkp4ttern" appreciates the package and suggests adding features like following retries, exponential backoff, caching, and streaming support for better performance.
- "detente18" agrees with "d4rkp4ttern" but mentions that caching request responses and independent nested GPT calls are already needed. They find the idea of streaming support and function-calling support interesting.
- "ij23" acknowledges "kaushik92" for mentioning the need to standardize AI APIs quickly for efficient development and shipping.
- "uripeled2" suggests looking into a similar library, "llm-clnt," which supports chat sync and various providers.
- "ij23" thanks "uripeled2" for sharing the library and mentions that they appreciate the simplicity of litellm.
- "neha_n" expresses interest in the package and mentions the need for quickly implementing a simple interface.
- "hardware2win" questions why the completion flag is set to True.
- "ij23" answers that zero models have custom names and using the main chat GPT model requires setting the flag as True. 
- "detente18" explains that zero is set as the completion flag to handle pytorch lightning models and passes the zero model.
- "ydng" requests to be contacted by Ishaan.
- "ij23" thanks "ydng" for the request.
- "detente18" marks the comment as true.

Overall, the discussion consists of users appreciating the package, suggesting additional features, sharing similar libraries, expressing interest in the package, and discussing technical details related to the project.

---

## AI Submissions for Wed Jul 26 2023 {{ 'date': '2023-07-26T17:11:15.704Z' }}

### Show HN: Continue – Open-source coding autopilot

#### [Submission URL](https://github.com/continuedev/continue) | 261 points | by [sestinj](https://news.ycombinator.com/user?id=sestinj) | [85 comments](https://news.ycombinator.com/item?id=36882146)

GitHub user continuedev has released an open-source autopilot for software development called "Continue." This VS Code extension brings the power of ChatGPT to your IDE, allowing you to auto-complete coding tasks, answer coding questions, refactor code, and even generate files from scratch. With Continue, you can highlight sections of code and ask for another perspective, edit code in natural language, and start new scripts and components. The project has already gained popularity, with 1.1k stars and 20 forks on GitHub. If you're interested, you can find more information and documentation on the Continue website at continue.dev/docs.

The discussion on Hacker News about the submission revolves around various aspects of the Continue project, as well as comparisons to other coding assistants like GitHub Copilot. Here are some key points from the discussion:

- Some users highlight the usefulness of Continue in providing context-aware code completion and assistance, especially in tasks like copying and pasting relevant code and generating code snippets based on natural language input.
- The topic of integrating Continue with language server protocols (LSP) is discussed, with users mentioning plans to implement a basic language server protocol server in local service applications.
- Users express interest in trying out Continue and providing feedback. They mention the potential benefits of using the Continue extension along with other coding assistants like Cody and Rubberduck Extension for Visual Studio Code.
- The potential limitations and challenges of AI-powered coding assistants like Continue are discussed, such as the need for user supervision and the limitations of the underlying language models.
- Some users mention their interest in the application of Continue's context-aware code assistance in projects related to semantic structure and understanding complex codebases.

Overall, the discussion shows a positive reception of the Continue project and includes conversations about its features, potential use cases, and possible improvements. Users also discuss related projects and provide recommendations for other coding assistants.

### “It works on my machine” turns to “it works in my container” (2019)

#### [Submission URL](https://dwdraju.medium.com/how-it-works-in-my-machine-turns-it-works-in-my-container-1b9a340ca43d) | 203 points | by [lis](https://news.ycombinator.com/user?id=lis) | [212 comments](https://news.ycombinator.com/item?id=36885598)

In this article, the author explores the common issue of "It works in my container" and explains why this situation arises. They highlight several reasons including using the latest image tag, outdated container engine versions, dealing with variables, the image build process, and file and folder permissions. The author provides solutions and best practices for each of these issues to help developers avoid the "It works in my container" problem. By following these guidelines, developers can ensure that their code works consistently across different environments and avoid wasting time on container-related issues.

The discussion on this submission revolves around the topic of reproducibility in container builds. Many users agree that achieving true reproducibility can be challenging due to various technical reasons. Some users point out the limitations of Docker in providing full reproducibility and suggest using other tools like Nix. There is also a discussion about the importance of following the instructions in a Dockerfile precisely to ensure reproducibility. Some users mention the difficulties they faced with managing dependencies and suggest using debugging tools to track changes and avoid mistakes. The conversation also touches on the complexity of managing configuration management and the need for clear documentation. Overall, the discussion highlights the challenges and different approaches to achieving reproducibility in container builds.

### Google is already pushing WEI into Chromium

#### [Submission URL](https://github.com/chromium/chromium/commit/6f47a22906b2899412e79a2727355efa9cc8f5bd) | 1309 points | by [topshelf](https://news.ycombinator.com/user?id=topshelf) | [823 comments](https://news.ycombinator.com/item?id=36876301)

Chromium has made a commit to ensure that the Origin Trial enables the full feature. This commit moves the base::Feature from content_features.h to a generated feature from runtime_enabled_features.json5. The base::Feature can now be default-enabled while the web API is controlled by the RuntimeFeature, which will still be default-disabled. An origin trial can enable the RuntimeFeature, allowing full access to the API if the base::Feature is also enabled. This change includes tests in WebView test to easily spoof responses on a known origin. The bug and change ID for this commit are also provided.

The discussion on this submission includes various topics and opinions. Some users express interest in the changes made by Chromium, while others discuss the implications of Google's control over web standards. One user raises concerns about Mozilla's collaboration with Google and questions their stance on defending the decentralized nature of the web. Another user points out that it's common for companies to control the implementation of web standards, and that Chrome doesn't prioritize conformity to standards. There is also a discussion on the relevance and market share of Mozilla Firefox, with some users suggesting that it is becoming irrelevant compared to Google Chrome. The conversation includes debates on the role of large companies in shaping web standards and the impact on user experience. The discussion concludes with a user stating that Mozilla is no longer relevant and that Microsoft is more important in the current landscape.

### Which GPU(s) to Get for Deep Learning

#### [Submission URL](https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/) | 214 points | by [snow_mac](https://news.ycombinator.com/user?id=snow_mac) | [125 comments](https://news.ycombinator.com/item?id=36872514)

Deep learning is a field that requires powerful GPUs for efficient computation. But when it comes to choosing a GPU for deep learning, what features should you consider? This blog post aims to answer that question and provide advice on making a cost-effective choice.

The post starts by explaining the basics of how GPUs work compared to CPUs and delves into the importance of GPU specs for deep learning. One key feature is Tensor Cores, which are specialized cores for efficient matrix multiplication, a crucial operation in deep neural networks. The post provides examples to help readers understand the significance of Tensor Cores.

Other important GPU specs discussed include memory bandwidth, cache hierarchy, and FLOPS. The post ranks these components in order of importance and emphasizes the necessity of Tensor Cores for optimal deep learning performance.

The post then dives into the unique features of NVIDIA's RTX 40 Ampere series GPUs and provides recommendations for different scenarios. It also addresses common questions and misconceptions about GPUs, covering topics such as PCIe lanes, cooling, AMD vs NVIDIA, and carbon footprint.

Overall, this blog post provides a comprehensive guide to choosing a GPU for deep learning, offering insights for both beginners and those with a more in-depth understanding of GPU architecture. By the end, readers should feel more confident in making an informed decision about which GPU to buy.

The discussion on this post covers a range of topics related to GPU specs and deep learning. Some commenters share their experiences with using AMD GPUs for deep learning, noting that while they have managed to get them working, there are limitations and issues with driver support. Others discuss the use of compute shaders in D3D, Vulkan, and WebGPU for machine learning applications. There is also a discussion about the performance and compatibility of DirectX, Vulkan, and ROCm. Additionally, there is a conversation about lock-free techniques and their effectiveness on GPUs, with some commenters highlighting the challenges and trade-offs involved. Overall, the discussion provides a deeper understanding of various aspects of GPU selection and usage for deep learning.

### Show HN: DankGPT – Chat with Your Documents

#### [Submission URL](https://www.dankgpt.com/) | 12 points | by [rawsh](https://news.ycombinator.com/user?id=rawsh) | [6 comments](https://news.ycombinator.com/item?id=36881615)

Introducing a GPT3.5 powered research assistant that can unlock your documents and provide instant insights. With this tool, you can quickly analyze complex content and research across multiple documents. The powerful prompting methods include the Ask Me Anything (AMA) Prompting method, which aggregates effective prompts to create a high-quality strategy. Another approach is the chain-of-thought prompting, inspired by various prior directions such as natural language explanations and program synthesis. Other prompting approaches mentioned in the related work section include optimized input prompts and task instructions. Find out more about this research assistant on the Dashboard and try it for free.

The discussion on this submission includes three comments. 

1. User "rwsh" mentions that the documents are processed through PDF text extraction using a web worker called MuPDF compiled as WASM. The client-side processing involves generating sparse vectors and updating existing vectors, while dense vectors are generated from parsed text with sparse values. The user is interested in understanding the specific techniques used in the research assistant's prompting methods.

2. User "mjckg" expresses confusion about the mention of GPT5 in the submission, as they are not familiar with it. They speculate that GPT5 might be a wrapper or a long-chain model that mounts a pick function. Another user "rwsh" replies, stating that GPT5 is not a serious project but rather a personal semester college rewrite with times packaged, and it is generally self-language-chain with 80% run success quickly. The mention of "Langchain" is unclear in this context.

3. User "bbstts" simply comments "GPT5," possibly to show interest or intrigue in the mention of GPT5 in the submission.

### Tuning and Testing Llama 2, Flan-T5, and GPT-J with LoRA, Sematic, and Gradio

#### [Submission URL](https://www.sematic.dev/blog/tuning-and-testing-llama-2-flan-t5-and-gpt-j-with-lora-sematic-and-gradio) | 97 points | by [josh-sematic](https://news.ycombinator.com/user?id=josh-sematic) | [21 comments](https://news.ycombinator.com/item?id=36880149)

In this blog post, Josh Bauer, a founding engineer, explores the world of Large Language Models (LLMs) and the various open-source models, libraries, and tools available. Bauer sets a goal to build a tool that can summarize information into a shorter representation, and discusses the criteria for this tool, including the ability to pull from different kinds of data, run on personal devices, experiment with different configurations, and export the resulting model for production use. 

To achieve this goal, Bauer explores the concept of fine-tuning, which involves leveraging existing powerful models and customizing them for specific tasks. There are two main approaches to fine-tuning: making the entire model flexible during training or training a smaller number of parameters. Bauer focuses on the latter approach, known as Parameter Efficient Fine Tuning (PEFT), which offers comparable performance while being more resource-efficient.

Within PEFT, Bauer highlights a method called Low Rank Adaptation (LoRA), which has shown promising results. LoRA involves decomposing the trainable matrices of the layers in a language model into two smaller matrices, resulting in a significant reduction in the number of parameters to learn. By choosing an appropriate value for the rank, performance can be maintained while achieving substantial parameter reduction.

Overall, Bauer provides a comprehensive overview of fine-tuning and introduces the concept of LoRA as a powerful technique to achieve efficient parameter tuning for language models.

### A new partnership to promote responsible AI

#### [Submission URL](https://blog.google/outreach-initiatives/public-policy/google-microsoft-openai-anthropic-frontier-model-forum/) | 17 points | by [sgift](https://news.ycombinator.com/user?id=sgift) | [9 comments](https://news.ycombinator.com/item?id=36875288)

Google, Microsoft, OpenAI, and Anthropic have joined forces to create the Frontier Model Forum, an industry body dedicated to ensuring the responsible development of frontier AI models. The forum aims to promote AI safety research, identify best practices for development and deployment, collaborate with policymakers and organizations, and support the development of AI applications that address societal challenges. Membership is open to organizations that develop and deploy frontier models and demonstrate a commitment to safety. The forum will focus on knowledge sharing, AI safety research, and facilitating information sharing among companies and governments.

The discussion around the submission highlights a range of opinions and concerns regarding the creation of the Frontier Model Forum. 

One user expresses skepticism about the effectiveness of the forum's efforts, suggesting that merely relying on agreed-upon standards may not be sufficient. They propose a solution based on alignment theory to address safety concerns more effectively.

Another user mentions the connection between Palantir and China, pointing out that Palantir's CEO published a letter advocating for the development of AI weapons. This raises concerns about the potential misuse of advanced AI by certain companies or countries.

In response to this, another user argues that it is crucial for both civil and military organizations to work towards common guidelines to prevent disaster. They specifically mention China's adherence to these guidelines as essential for global stability.

The discussion then diverges into a debate about whether civil and military organizations should have separate guidelines or work together. One user argues that it is necessary to treat the development of AI with caution due to the potential risks it poses, while another user suggests that different sets of guidelines should be applied to civil and military contexts.

Moving on, a user points out that the details of the Frontier Model Forum's plans are not clear, indicating that more information is needed to assess its potential impact on AI safety.

Another user raises concerns about Google's responsibility in ensuring AI safety, suggesting that they may be misguided in their interests. In contrast, another user suggests that Google's involvement and OpenAI's watermarking of their models indicates a commitment to safety and responsible development.

Overall, the discussion showcases a range of perspectives on the creation of the Frontier Model Forum, highlighting concerns about AI weaponization, the importance of global cooperation, and the need for clarity regarding the forum's plans.

### ChatGPT broke the Turing test – the race is on for new ways to assess AI

#### [Submission URL](https://www.nature.com/articles/d41586-023-02361-7) | 10 points | by [isaacfrond](https://news.ycombinator.com/user?id=isaacfrond) | [6 comments](https://news.ycombinator.com/item?id=36876776)

The race is on to find new ways to assess AI as ChatGPT has broken the Turing test. While AI systems like ChatGPT can pass tough exams, write human-like essays, and chat fluently with people, they struggle to solve simple visual logic puzzles. This has prompted researchers to create a better benchmark for testing the capabilities of AI systems. Large language models (LLMs) like GPT-4 have shown impressive abilities in certain tasks, but they also have glaring blind spots and struggle with abstract concepts. Researchers are divided on whether LLMs possess true reasoning abilities or if their achievements are simply the result of statistical correlations in training data. The development of new benchmarks and tests can help shed light on the capabilities and limitations of LLMs, especially as they are increasingly being applied in real-world domains. While the Turing test has been the most famous test of machine intelligence, the emergence of LLMs has prompted the search for new assessment methods. Although LLMs might now pass the popular conception of the Turing test, there is still much to explore in terms of evaluating their capabilities.

The discussion revolves around the idea of testing the capabilities of large language models (LLMs) like GPT-4 and the recent breakthrough in ChatGPT passing the Turing test. One commenter argues that LLMs should be tested in scenarios like instructions to build a bomb, as they wouldn't provide a nonsensical answer that humans wouldn't work either. Another commenter states that the ability to solve a bomb check is a low-level knowledge that a high school student can handle, but it doesn't prove intelligence. They suggest using more professional-level questions to assess AI capabilities. Another idea proposed is the "Grooming test," where AI would respond to knowledge-related rights and pass ROT13-coded instructions or solve physics problems. However, some commenters argue that these tests may not sufficiently evaluate true reasoning abilities and suggest using tests that prompt AI to explain topics like writing in COBOL or making traditional Kazakh hats. One commenter illustrates the potential flaws and challenges of these tests, emphasizing the importance of context and the AI's complexity. Lastly, there is a brief comment about someone hacking GPT4 and diverting its response to showcase that AI systems are not infallible. Overall, the discussion highlights the need for better benchmarks and tests to assess the true capabilities and limitations of LLMs.

[test]

---

## AI Submissions for Tue Jul 25 2023 {{ 'date': '2023-07-25T17:10:15.182Z' }}

### Whom the gods would destroy, they first give real-time analytics (2013)

#### [Submission URL](https://mcfunley.com/whom-the-gods-would-destroy-they-first-give-real-time-analytics) | 157 points | by [sbdchd](https://news.ycombinator.com/user?id=sbdchd) | [64 comments](https://news.ycombinator.com/item?id=36870140)

A programmer at Etsy explains why real-time analytics may not be as useful as they seem. While engineers are inclined to see real-time data as beneficial, there are many ways in which it can lead to flawed decision-making. These include disregarding statistical significance testing, halting experiments as soon as significance is measured, and making decisions based on a short timeframe of data. The author argues that delayed analytics can actually be beneficial because it allows for more thorough analysis and prevents rash decision-making.

The discussion surrounding the submission revolves around the pros and cons of real-time analytics. Some commenters agree with the author, highlighting the potential pitfalls of relying too heavily on real-time data. They argue that real-time metrics can lead to flawed decision-making and the disregarding of statistical significance testing. Delayed analytics, on the other hand, allow for more thorough analysis and prevent rash decision-making. 

Other commenters, however, provide counterarguments. They suggest that real-time metrics can be valuable, especially in the early stages of development. Real-time metrics can help with faster product iterations and decision-making based on relevant trends. Additionally, they mention the importance of measuring conversion rates directly and the challenges in analyzing such data in real-time. 

The discussion also touches on related topics such as the politics and biases involved in metric selection, the difficulty of building commercial real-time analytics systems, and the practicality of implementing real-time analytics in different industries. 

Overall, there is a range of opinions regarding the usefulness and limitations of real-time analytics, with some expressing support for its benefits and others advocating for a more cautious approach.

### OpenAI shuts down its AI Classifier due to poor accuracy

#### [Submission URL](https://decrypt.co/149826/openai-quietly-shutters-its-ai-detection-tool) | 481 points | by [cbowal](https://news.ycombinator.com/user?id=cbowal) | [272 comments](https://news.ycombinator.com/item?id=36862850)

OpenAI has shut down its AI Classifier, a tool designed to detect whether a piece of content was created using generative AI tools such as its own ChatGPT. The tool was discontinued due to its low rate of accuracy. OpenAI acknowledged the importance of accurately detecting AI-written text, particularly in the education sector where there have been concerns about students using AI chatbots to write essays. The company stated that it is continuing to research more effective techniques for detecting AI-generated content and plans to develop new mechanisms to enable users to understand if audio or visual content is AI-generated.

The discussion on Hacker News revolves around various aspects of OpenAI's decision to shut down its AI Classifier tool and the challenges of detecting AI-generated content.

Some users express their support for OpenAI's decision, highlighting the difficulty of accurately detecting AI-written text. They mention that the tool was limited in its ability to differentiate between human-written and AI-generated content and that accurately detecting AI-generated content is a complex problem.

Others discuss the possible approaches to improving content detection, including using neural networks and backpropagation. Some users mention the need for more reliable and sophisticated indicators of content quality and truthfulness. They also discuss the limitations of current models like ChatGPT and the challenges in training AI to generate highly convincing and diverse content.

In response to concerns about students using AI chatbots to write essays, some users emphasize the importance of teaching critical thinking skills and the role of human discernment in evaluating content.

There are also discussions about the use of watermarks as a method of identifying AI-generated content. Some users highlight the potential limitations of watermarking, as it can be easily removed or rewritten by AI models.

The conversation delves into topics such as consensus reality, the involvement of cryptographic signing, and the control of information in a post-AI world. Some users debate the existence of consensual reality and the manipulation of truth by powerful entities.

There are also discussions on the capabilities of ChatGPT in conveying information and mimicking human writing styles. Some users highlight that ChatGPT has been explicitly trained on human writing and attempts to mimic distinct writing styles based on prompts. They note that the content generated by ChatGPT often sounds confident and authoritative, and it can combine various writing styles in its responses.

Overall, the discussion reflects the complexity and challenges associated with detecting AI-generated content and highlights the need for further research and development in this area.

### ONNX runtime: Cross-platform accelerated machine learning

#### [Submission URL](https://onnxruntime.ai/) | 146 points | by [valgaze](https://news.ycombinator.com/user?id=valgaze) | [34 comments](https://news.ycombinator.com/item?id=36863522)

The top story on Hacker News today is about ONNX Runtime, a cross-platform tool for accelerating machine learning processes. It includes built-in optimizations that can deliver up to 17 times faster inferencing and up to 1.4 times faster training. ONNX Runtime is designed to be easily integrated into existing technology stacks and supports a variety of frameworks, operating systems, and hardware platforms. The technology behind ONNX Runtime is already used in popular products like Office 365, Visual Studio, and Bing, where it delivers over a trillion inferences every day. The post also encourages readers to participate in a customer survey to help improve ONNX Runtime.

The discussion on the submission about ONNX Runtime covers a variety of topics related to the technology.

- One commenter mentions that Microsoft recently worked on deploying ONNX-based models to Azure and mentions the Llama 2 Azure project.
- Another commenter indicates that ONNX only provides information about the ONNX runtime working with MLDL0.
- A user named "Roark66" points out a few limitations of ONNX, such as the 2GB limit on serialized files and difficulties in partitioning existing large models.
- There is a back-and-forth discussion about the size limits of serialized files and memory representation, with suggestions for increasing the limit or finding alternatives.
- Commenters discuss various aspects of ONNX, including its support for different frameworks and the difference between training and inference.
- Some debate arises regarding the limitations and optimizations of ONNX Runtime for deployment, with comparisons to other frameworks and discussion of specific use cases.
- A user called "Zetobal" mentions the biggest problem with ONNX models being reshaping.
- Other users express excitement about different aspects of ONNX Runtime, such as its compatibility with other languages and the potential for different backends.
- Some users mention other related projects such as StableHLO, Tinygrad, and Triton Inference Server.
- The ease of installation and the speed of development in the ML community are discussed, with mentions of the ONNX project's progress in the past five years.
- There are mentions of running ONNX models in the browser and links to relevant blog posts.
- Several comments touch on the limitations and memory consumption of ONNX, as well as alternatives like Tinygrad, TVM, and OpenVino.

Overall, the discussion covers a range of perspectives and considerations related to ONNX Runtime and its usage in the machine learning community.

### Vectorization of Raster Manga by Primitive-Based Deep Reinforcement Learning

#### [Submission URL](https://github.com/SwordHolderSH/Mang2Vec) | 35 points | by [PaulHoule](https://news.ycombinator.com/user?id=PaulHoule) | [3 comments](https://news.ycombinator.com/item?id=36862376)

The Mang2Vec project is a PyTorch implementation of "Vectorization of Raster Manga by Deep Reinforcement Learning." It aims to convert raster manga images into vector graphics using deep reinforcement learning techniques. The project provides demos, installation instructions, and a quick start guide in its README file. If you're interested in this research or using the code for your own work, don't forget to cite the original paper. The project has received 43 stars on GitHub and has 2 forks.

The discussion on the submission begins with a user named "brnkwsk" noting that manga panels appear smaller below the poster size, which seems to be a visual effect. Another user named "PaulHoule" shares a link to the arXiv paper related to the project. Lastly, a user named "rowanG077" comments that the project is significant in terms of replicating pixel structure in Adobe Illustrator. However, it seems that the comment may have been cut off, as the ending is unclear.

### What we know about LLMs

#### [Submission URL](https://willthompson.name/what-we-know-about-llms-primer) | 345 points | by [wilhelm____](https://news.ycombinator.com/user?id=wilhelm____) | [158 comments](https://news.ycombinator.com/item?id=36860992)

In a recent article titled "What We Know About LLMs," author Will Thompson dives into the world of Large Language Models (LLMs) and explores what we currently know about them. LLMs, which are a type of deep learning architecture known as Transformers, have been garnering a lot of attention lately with their potential to create immeasurable wealth for society while also posing a threat to knowledge workers.

Thompson highlights the current AI fervor and how many companies, including the big tech giants, are investing heavily in LLMs. He also notes that LLMs have become the focus of research efforts and are being adopted by a large percentage of startups in Y Combinator's cohort.

To understand what LLMs are, Thompson explains that they are models that work with sequence data, such as text or images, and learn the contextual relationships between values within a sequence through a mechanism called attention. Unlike previous models like Recurrent Neural Networks (RNNs), Transformers can process the entire sequence at once, allowing for faster training times and larger model parameter sizes.

Thompson categorizes Transformers into three main types: "encoder only," "decoder only," and "encoder-decoder" architectures. Each type has its own strengths and is suited for different tasks, such as sentiment classification or language translation.

The article also reflects on what we've learned about LLMs so far. One key insight is that LLMs have the ability to generalize, meaning they can complete various tasks with only a few examples. Additionally, LLMs exhibit predictable scaling behavior, with larger models becoming more data-efficient and performing better on benchmarks.

Overall, the article provides a comprehensive overview of LLMs, shedding light on their potential and the current state of research in this field. With the AI industry buzzing with excitement, it's essential to understand the capabilities and implications of these powerful language models.

Discussion Summary:

- One user expresses skepticism about the hype surrounding LLMs and notes that some of the arguments advocating for them seem faulty.
- Another user shares their positive experience using Github Copilot, a tool that integrates LLMs into the coding process, significantly improving productivity.
- The discussion veers towards the topic of ORM (Object-Relational Mapping) tools and their impact on productivity, with different opinions expressed.
- Some users share examples of AI-generated content, including market copy and chat responses, highlighting both the potential and potential issues with LLMs.
- The conversation touches on the use of LLMs in various applications, such as office integration and text messaging systems.
- References are made to Clippy, a virtual assistant from Microsoft, and other AI-related stories and concepts.
- Some users discuss the relevance of Gartner's Hype Cycle in the context of LLMs and technology trends in general, with varying opinions on its usefulness and credibility.
- The discussion concludes with a user pointing out the importance of considering the ingestion and creation of large-scale language models, as well as the limitations and potential risks associated with them.

### JPMorgan warns that an AI bubble is brewing

#### [Submission URL](https://markets.businessinsider.com/news/stocks/stock-market-outlook-jpmorgan-bearish-ai-bubble-mega-cap-tech-2023-7) | 45 points | by [1vuio0pswjnm7](https://news.ycombinator.com/user?id=1vuio0pswjnm7) | [19 comments](https://news.ycombinator.com/item?id=36869983)

JPMorgan's Marko Kolanovic remains bearish on the stock market and warns of an AI bubble that is forming. He points out that stock concentration in the S&P 500 is at a 60-year high, with the top seven companies accounting for over 25% of the index. Kolanovic believes this concentration, along with other anecdotal evidence, indicates a bubble caused by the hype around artificial intelligence. While he recognizes the potential of AI technologies, he argues that they are not yet ready for mainstream adoption. Kolanovic also highlights three bearish catalysts that could trigger a significant market sell-off: the delayed impact of the global interest rate shock, erosion of consumer savings, and the troubled geopolitical landscape. He recommends investing in commodities, which he believes are undervalued and backed by strong fundamentals. Kolanovic is not alone in his bearish outlook, as other Wall Street strategists, like Morgan Stanley's Mike Wilson, also expect a market decline.

The discussion on this submission covers various topics related to AI, Apple stock, and financial markets. Here are some key points:

- One user mentions that Apple's stock price loss of $50 billion is negligible compared to its total market fund, which raises a question about the relevance of Apple stock in relation to the AI bubble.
- Another user suggests that Apple's stock pricing is related to self-driving electric cars.
- There is a mention of Waste Management being an investment opportunity related to AI.
- A user comments on the lack of trust in Business Insider and how financial strategies are not always beneficial for the audience.
- The concentration of large companies and the riskiness of investment in small, unestablished companies is discussed.
- The idea of a bubble is linked to various technologies like NFTs, cryptocurrencies, AR/VR, and the Metaverse. It is argued that the AI bubble is a result of FOMO and people losing their savings.
- The disappointment with AI's practical applications is mentioned, with a user giving an example of machine translation not being as effective as professional translators.
- A link to an archive of an article related to the discussion is shared.
- A user mentions the increasing severity of climate change and the need for people to study physics and work on solving the problem rather than focusing on AI.
- The potential improvement in productivity due to AI and its impact on various domains, including law and sustainable development, is discussed.

Overall, the discussion delves into different aspects of the AI bubble, the stock market, and the broader implications of these trends.

### Robo-Taxis are rolling, did you notice?

#### [Submission URL](https://cmte.ieee.org/futuredirections/2023/07/25/robo-taxi-are-rolling-did-you-notice/) | 32 points | by [kungfudoi](https://news.ycombinator.com/user?id=kungfudoi) | [23 comments](https://news.ycombinator.com/item?id=36858633)

Robo-Taxis are quietly making their way onto the streets, and they are set to revolutionize the transportation industry. In a recent blog post, Roberto Saracco highlights the progress of self-driving cars for public transportation. While the initial hype around robo-taxis soared a few years ago, the industry experienced a setback as investors lowered their expectations. However, companies like Baidu and Waymo have quietly been deploying robo-taxis and are now reaping the benefits. Baidu is operating robo-taxis in Beijing, while Waymo has doubled its service area in Phoenix and is preparing to launch in San Francisco. Uber is also planning to incorporate Waymo into its fleet, with the goal of having up to 20% of its rides managed by robo-taxis by 2025. With these developments, the industry is reaching the plateau of productivity on the Gartner Hype Cycle. The future of transportation is here, and it's autonomous.

### AI is being used to create child sex abuse images and also to prevent them

#### [Submission URL](https://news.yahoo.com/ai-is-being-used-to-create-child-sex-abuse-images-its-also-being-used-to-prevent-them-192951595.html) | 20 points | by [yenniejun111](https://news.ycombinator.com/user?id=yenniejun111) | [8 comments](https://news.ycombinator.com/item?id=36870842)

Artificial intelligence (AI) technology has taken a dark turn as bad actors are using open-source forms of AI, like ChatGPT, to create sexual images of children. These AI-generated child sex abuse materials (CSAM) are becoming increasingly prevalent, with thousands of images being created and shared across the internet. Users on the dark web are sharing detailed instructions on how to create realistic AI images of children engaged in sexual acts. While the issue is still relatively small, experts are urging for proactive measures to prevent it from growing further. AI tools like DALL-E and Stable Diffusion allow users to generate lifelike images by describing what they want to see. While organizations like OpenAI, the creator of ChatGPT, are working to implement protections to prevent CSAM, there is debate surrounding the legality of these AI-generated images. Some believe they violate federal child protection laws, while others argue that since the children depicted are not real, they should not be considered illegal. The biggest challenge in combating this issue is the lack of visibility, as the images cannot be shown or shared to raise awareness. Thorn, a nonprofit organization founded by Ashton Kutcher and Demi Moore, is working on victim identification, stopping revictimization, and preventing abuse in the first place. It is essential for parents to be cautious in the digital age and avoid sharing explicit photos of their children as perpetrators can misuse them.

### Retentive Network: A Successor to Transformer Implemented in PyTorch

#### [Submission URL](https://github.com/Jamie-Stirling/RetNet) | 11 points | by [panabee](https://news.ycombinator.com/user?id=panabee) | [3 comments](https://news.ycombinator.com/item?id=36857245)

RetNet is an implementation of "Retentive Network: A Successor to Transformer for Large Language Models" in PyTorch. The code prioritizes correctness and readability over optimization, and it aims to aid scientific and technological understanding and advancement. The features implemented include single-scale and multi-scale retention, multi-layer retentive network with FFN and LayerNorm, and a causal language model built on top of the retentive network. The contributors to this repository are not authors of the original paper, but they have implemented the ideas and formulations described in the paper. The repository welcomes contributions, and examples of basic usage can be found in the test scripts.

---

## AI Submissions for Mon Jul 24 2023 {{ 'date': '2023-07-24T17:10:35.408Z' }}

### Google’s nightmare “Web Integrity API” wants a DRM gatekeeper for the web

#### [Submission URL](https://arstechnica.com/gadgets/2023/07/googles-web-integrity-api-sounds-like-drm-for-the-web/) | 969 points | by [jakobdabo](https://news.ycombinator.com/user?id=jakobdabo) | [404 comments](https://news.ycombinator.com/item?id=36854114)

Google is proposing a new web standard called the "Web Environment Integrity API" which aims to verify the integrity of the client environment running in the web browser. The goal is to ensure that the browser hasn't been modified or tampered with, and that the person on the other side is not a robot. This proposal has raised some concerns, as it brings up issues of privacy and control over devices. The API takes inspiration from existing native attestation signals such as Apple's App Attest and Android's Play Integrity API. The proposal has sparked a lot of discussion and debate in the tech community.

The discussion on Hacker News revolves around various aspects of Google's proposal for the Web Environment Integrity API. Some users express skepticism towards Google's intentions, highlighting the company's dominance in multiple areas and questioning their need for more control. Others emphasize their concerns about privacy and the potential for abuse of such an API. The comparison is made to Apple's previous controversy with the U2 album auto-downloading, which resulted in a lack of trust from some users. There are also discussions about the implications for ad-blocking and browser restrictions, with some users expressing frustration with the limitations imposed by certain browsers. The conversation also touches on alternative browsers and their potential to circumvent such API restrictions. Overall, there is a mixture of opinions and concerns surrounding the proposal. Some users see it as a necessary security improvement, while others view it as potentially problematic and restrictive.

### Meta-Transformer: A unified framework for multimodal learning

#### [Submission URL](https://kxgong.github.io/meta_transformer/) | 101 points | by [ulrikhansen54](https://news.ycombinator.com/user?id=ulrikhansen54) | [33 comments](https://news.ycombinator.com/item?id=36851505)

A team of researchers from The Chinese University of Hong Kong and Shanghai AI Laboratory has developed a framework called Meta-Transformer that enables unified multimodal learning. This framework utilizes the same backbone to process various data modalities such as natural language, images, point clouds, audio, video, infrared, hyperspectral, X-ray, time-series, tabular, and graph data. By converting the raw input data from different modalities into a shared token space and using a modality-shared encoder with frozen parameters, Meta-Transformer can extract high-level semantic features and achieve favorable performance across modalities. This framework has been evaluated on benchmarks like ImageNet, GLUE, ModelNet-40, S3DIS, ShapeNetPart, and Speech Commands V2, showcasing its potential for developing unified multimodal intelligence with transformers. Meta-Transformer has applications in various fields such as 3D recognition, nighttime security, and weather prediction.

The discussion on this submission revolves around the capabilities and limitations of the Meta-Transformer framework and the general purpose of multimodal learning. Some users express skepticism about the effectiveness of using transformers for different modalities, arguing that specialized models may perform better in specific tasks. Others discuss the potential scalability challenges and the need for large amounts of data to train such models. The discussion also touches on the comparison between GPT-4 and Meta-Transformer and the trade-offs between model size and effectiveness. Additionally, there are debates about the potential dangers of AI and the need for responsible scientific progress. The existence and risks of AI are debated, with one user referencing a documentary on AI in the military as evidence of its capabilities. The discussion then delves into defining and evaluating risks associated with AI and the distinction between science fiction and science. The conversation also explores the historical progress of AI and the advancements made in fields like optical lithography and computer vision. Finally, there are concerns raised about existential threats posed by AI and the need to address research directions that can mitigate potential dangers.

### Text Embeddings Reveal (Almost) as Much as Text

#### [Submission URL](https://openreview.net/forum?id=wK7wUdiM5g0) | 65 points | by [jxmorris12](https://news.ycombinator.com/user?id=jxmorris12) | [9 comments](https://news.ycombinator.com/item?id=36851930)

Researchers have proposed a method called Vec2Text that can reconstruct 90% of 32-token embedded inputs exactly, revealing how much private information text embeddings can disclose about the original text. This method treats embedding inversion as a controlled generation problem, generating text that, when reembedded, is close to a fixed point in latent space. While a naive model conditioned on the embedding performs poorly, a multi-step method that iteratively corrects and re-embeds text can recover 92% of 32-token text inputs accurately. The researchers trained their model to decode text embeddings from two state-of-the-art embedding models and demonstrated that it can recover personal information, such as full names, from clinical notes.

The discussion on this submission covers a range of topics related to text embeddings and privacy. Some of the points raised include:

- There is a debate about whether text embeddings should be considered as encryption or compressed representations, with one commenter stating that they are compressed representations and another arguing that they are hashed and therefore not easily reversible.
- The importance of security in text embeddings and the interest in exploring methods to tweak embedding values to find token points corresponding to different latent spaces.
- A commenter highlights that the research demonstrates the ability to recover 90% of text exactly with semantic overlap in vector space. They mention that while the text might not be visually identical, the meaning is preserved with meaningful shifts of words.
- The concept of single reference differential privacy is brought up in the context of protecting privacy.
- The discussion references the experiment section (Section 5.3) of the paper, which focuses on attempting to recover private information from clinical notes using embeddings. The commenter finds it interesting and wonders if recovering names is possible by using custom distance metrics.
- One commenter asks if embedding protection can be achieved without encrypting individual words.
- It is mentioned that the linked paper demonstrates different results in terms of text retrieval and reconstruction performance.
- A commenter argues that it is difficult to achieve fundamental point computer representations of meaning in regional text using accessible embeddings.

Overall, the discussion examines various aspects related to the research topic, including the nature of text embeddings, privacy concerns, and the technical details of the proposed method.

### Apple Vision Pro developer kit

#### [Submission URL](https://developer.apple.com/visionos/developer-kit/) | 161 points | by [Pulcinella](https://news.ycombinator.com/user?id=Pulcinella) | [181 comments](https://news.ycombinator.com/item?id=36851535)

Apple is inviting developers to apply for the Vision Pro developer kit, which will help them build and test apps for the new App Store on Vision Pro. The kit includes a loaned Vision Pro device, assistance with setup and onboarding, guidance from Apple experts, and code-level support requests. Developers need to be Account Holders in the Apple Developer Program and submit an application that highlights their team's skills and existing apps. Priority will be given to apps that make use of visionOS features and capabilities.

The discussion revolves around Apple's new Vision Pro developer kit and its implications for gaming. Some users express interest in the potential for gaming on the Vision Pro device, while others question Apple's commitment to gaming and the use of VR controllers. The conversation touches on topics such as the competition between gaming consoles and PCs, Apple's approach to gaming, VR gaming, and the importance of game standards and controllers. Some users express skepticism about Apple's focus on gaming and suggest that the company should prioritize other aspects of its devices. Others argue that gaming is a significant market and criticize Apple for not fully understanding or facilitating it. Additionally, there are discussions on the limitations of VR controllers, the need for hand tracking, and the challenges of implementing gaming features.

---

## AI Submissions for Sun Jul 23 2023 {{ 'date': '2023-07-23T17:09:46.007Z' }}

### Interfaces all the way down

#### [Submission URL](https://jjain.substack.com/p/interfaces-all-the-way-down) | 91 points | by [jinay](https://news.ycombinator.com/user?id=jinay) | [63 comments](https://news.ycombinator.com/item?id=36836433)

In his latest article on his Substack, Jinay Jain discusses the importance of designing interfaces and how it leads to happier developers. According to Jain, mastering interface design is the key to success and advancement in the ranks of engineering. As engineers move up, they become responsible for larger interfaces, such as entire classes, APIs between services, and even widely distributed SDKs. High-quality interfaces not only make people appreciate your work but also trust you with bigger tasks. Jain explores the concept of problem decomposition, where breaking down complex tasks into smaller, modular components is crucial. Poorly constructed interfaces can lead to dependencies and technical debt, slowing down developer velocity. To ensure robustness, Jain suggests asking questions about the impact on other parts of the system, unit testing, and maintaining codebase independence. Drawing inspiration from traditional design fields, such as human-centered design, Jain emphasizes the importance of considering developers as end users when designing interfaces. By following design principles like discoverability, affordances, and feedback, interfaces can be intuitive and require no documentation. Jain concludes by highlighting how extensive care in crafting interfaces can make the software "write itself" once the basic structure is outlined.

The discussion on this submission revolves around various aspects of interface design and its importance in software development. Some comments highlight the similarities between function arguments and design parameters, while others discuss the relevance of learning complex tools and techniques in software engineering.

Other comments mention the need for effective communication and management in interface design to avoid mistakes and improve productivity. The discussion also touches on the importance of well-designed interfaces in extracting functionality, ensuring code quality, and balancing front-end design with velocity and testing principles.

One comment raises the point that mathematical interfaces can be more intuitive and friendly, especially for those familiar with mathematical concepts and techniques. However, another comment argues that not all mathematical interfaces are user-friendly or suitable for practical applications.

Some comments also discuss the relevance of other programming languages, such as SQL, in the context of interface design. They mention how SQL's built-in primitives and theoretical properties contribute to its effectiveness in handling relational databases.

Overall, the discussion explores various perspectives on interface design, ranging from the mathematical aspects to practical considerations and the use of different programming languages.

### Retentive Network: A Successor to Transformer for Large Language Models

#### [Submission URL](https://arxiv.org/abs/2307.08621) | 101 points | by [sangel](https://news.ycombinator.com/user?id=sangel) | [16 comments](https://news.ycombinator.com/item?id=36831956)

Researchers from various institutions have proposed a new architecture called the Retentive Network (RetNet) as a successor to the popular Transformer model for large language models. This architecture aims to achieve training parallelism, low-cost inference, and good performance simultaneously. The key innovation in RetNet is the retention mechanism for sequence modeling, which supports three computation paradigms: parallel, recurrent, and chunkwise recurrent. These paradigms allow for efficient training, low-cost inference, and linear complexity in modeling long sequences. Experimental results on language modeling demonstrate that RetNet achieves favorable scaling results, parallel training, low-cost deployment, and efficient inference. The proposed architecture shows promise as a strong successor to Transformer for large language models. Code for RetNet will be made available for further research and development.

### Stable Diffusion and ControlNet: “Hidden” Text (see thumbnail vs. full image)

#### [Submission URL](https://old.reddit.com/r/StableDiffusion/comments/1561k15/free_tool_to_generate_hidden_text_using_stable/) | 98 points | by [b0ner_t0ner](https://news.ycombinator.com/user?id=b0ner_t0ner) | [36 comments](https://news.ycombinator.com/item?id=36832271)

BoostPixels, a Reddit user, has created a fascinating tool that uses Stable Diffusion and ControlNet to generate hidden text in images. The tool works in a unique way – when the image is small, the text stands out clearly. However, as the image enlarges, decoding the text becomes a challenging task. This curious phenomena has captured the attention of many users who have tried to decipher the hidden messages. Some strategies, such as squinting, blinking rapidly, or scrolling the image, seem to help make the text more visible. While it's an interesting experiment, /u/BoostPixels hopes that this tool won't be used for evil purposes.

The discussion on the submission starts with GaggiX acknowledging the use of the ControlNet model by the OP, BoostPixels. They share a link to an installation guide for the ControlNet model and mention that previous methods of generating QR codes didn't produce high-quality messages. There is then a conversation about AI generating QR codes and patterns similar to natural ones, with LanternLight83 wondering if complex text content can be sorted by the complexity of the ControlNet Local Linear Models (LLMs).
 
ben_w points out that generating text visually is difficult for short words with no distinctive landmarks, but GaggiX argues that ASCII art limits the expressiveness of sentences. cpblwb offers a poetic interpretation of the hidden text. gdlsk joins the discussion, expressing praise for the comments on Reddit and acknowledging the community's collective understanding of the tool's implementation.

The conversation then delves into the different user experiences when viewing the hidden text. Some discuss how the text becomes more visible when zoomed or viewed on a mobile phone, while others mention difficulties in spotting the text unless they hold their phone at a specific angle. There is also discussion about the visibility of the text on different screens, with some users mentioning that enlarging the image on a high-resolution monitor makes the text disappear while others find it readable. 

The link in the submission is mentioned by ch, who confirms that the stable diffusion method works on a MacBook Pro M1 Max. tmm shares a similar illusion they made in 2014 that involves thumbnail squinting. strng comments on the exploitability of resizing algorithms, mentioning that Photoshop filters can produce misleading results. tlstptml and hpfnsprgrj offer explanations related to human contrast sensitivity and letter squinting, respectively. 

The discussion then transitions to the nature of incorrect algorithms in resizing and how they affect the visibility of objects. Some users mention experiences with resizing algorithms and high-resolution monitors, while others question the impact of content contrast and spatial patterns. pngr argues that different spatial patterns will have a well-defined and substantially compressible space. 

lcbrtry finds the effect fascinating and shares personal experience with squinting to see hidden text. mnstmnsmn comments on the inability to see the hidden text on their iPhone 14 Pro Max. smblt issues a PSA about blurred vision affecting the effectiveness of the effect, and ChatGTP mentions an off-topic issue with a broken back button on the page. wmtt tries to provide a solution for viewing the hidden text on an iPhone, while munch117 suggests that the link to the desktop version may work better for iPhone users.

### Apple is already using its chatbot for internal work

#### [Submission URL](https://www.theverge.com/2023/7/23/23804825/apple-gpt-chatbot-apple-care-siri-chatgpt) | 28 points | by [rntn](https://news.ycombinator.com/user?id=rntn) | [10 comments](https://news.ycombinator.com/item?id=36838178)

Apple is using its chatbot internally to prototype features, summarize text, and answer employee questions based on trained data, according to Bloomberg's Mark Gurman. While Apple has yet to determine how it will use the chatbot for customer-facing purposes, it may consider using it to support AppleCare. However, Apple is being cautious about implementing AI due to the potential for misinformation and leaked information. The company is expected to make a significant AI-related announcement next year. While other companies like Meta and Samsung make moves in the AI space, Apple has been more reserved, but its hiring of former Google AI head John Giannandrea in 2018 suggests its serious about exploring generative AI.

The discussion on the article revolves around Apple's use of chatbots internally and their potential applications for customer-facing purposes. One commenter points out that workplace technology often includes extensive chatbots for group support, product suggestions, and notifications. Another user expresses frustration with Siri's lack of helpfulness on iOS. Someone agrees, stating that Siri's performance is embarrassingly bad.

The conversation then shifts to the challenges Apple may face in implementing AI due to its high-profile nature and the potential for misinformation and leaks. One user praises the integration of shortcuts in applications, claiming it makes finding and utilizing applications more intuitive. However, another user criticizes Apple's attempt to mimic programming visual environments, suggesting it is not good practice and promotes the use of variables that are not named descriptively.

Regarding Apple's upcoming AI-related announcement, someone questions what it might entail. They doubt Apple will call it "AI" and speculate that it may be related to an upgrade for Siri or machine learning models with improved language processing. Another user finds this comment interesting, as they have noticed various mentions of AI from the company, including work expressions in video and song terms for training people.

The discussion wraps up with a couple of comments highlighting Apple's focus on device management, preferences, privacy, and low latency reasons for utilizing Siri. Overall, the conversation touches upon the limitations and potential of Apple's AI endeavors.

---

## AI Submissions for Sat Jul 22 2023 {{ 'date': '2023-07-22T17:09:56.845Z' }}

### Putting the “You” in CPU

#### [Submission URL](https://cpu.land/) | 336 points | by [uneekname](https://news.ycombinator.com/user?id=uneekname) | [105 comments](https://news.ycombinator.com/item?id=36823605)

Today's top story on Hacker News is a personal account of someone's journey to understand what happens when a program is run on a computer. The author shares their struggle to fill the gap in their knowledge and their extensive research to piece everything together. They have compiled almost 40 pages of notes and are now writing an article to explain what they have learned in a comprehensive manner. The author also promises that even if you're already familiar with the topic, chapter 3 of their article will still teach you something new. The article is available on GitHub as an open-source project.

The discussion on Hacker News regarding the top story consists of various users sharing their thoughts and experiences related to learning about computer systems and programming. Some users express frustration with the lack of comprehensive resources available, such as Reddit threads and StackOverflow answers that often provide oversimplified explanations. Others mention the importance of understanding system internals and historical context in order to gain a deeper understanding. The conversation also touches on topics like documentation of modern systems and the challenges of low-level programming. One user shares their interest in RISC-V architecture, while another discusses the value of learning computer architecture and operating systems in university curriculums. The conversation also touches on the idea that self-learning is crucial in the field of programming and the importance of having a strong foundation in basic principles. The discussion concludes with various users emphasizing the importance of gaining practical experience and focusing on foundational knowledge in order to become a proficient programmer.

### Shopify employee breaks NDA to reveal firm replacing laid off workers with AI

#### [Submission URL](https://thedeepdive.ca/shopify-employee-breaks-nda-to-reveal-firm-quietly-replacing-laid-off-workers-with-ai/) | 421 points | by [notRobot](https://news.ycombinator.com/user?id=notRobot) | [522 comments](https://news.ycombinator.com/item?id=36828409)

In a Twitter thread, a Shopify employee has violated their non-disclosure agreement (NDA) to expose the company's controversial actions and strategic direction. The employee revealed that Shopify promised job security to its staff in early 2022 but carried out massive layoffs later in the year. These job cuts were driven by a shift towards replacing full-time employees with cheaper contract labor and an increased reliance on artificial intelligence (AI) support. Shopify recently announced the upcoming launch of "Sidekick," an AI assistant for its merchants. CEO Tobi Lutke has publicly stated his belief that companies can achieve higher revenues with fewer employees, which has negatively impacted customer satisfaction. The reduction in staff has led to significant delays in customer support and an overwhelmed team monitoring fraudulent stores. The employee's thread also raised concerns about the well-being of Shopify's workforce, the company's shift in target market focus, and the impact on small businesses. With public scrutiny growing, Shopify faces the challenge of restoring trust and addressing the consequences of its decisions.

The discussion on the submission revolves around customer service and the role of AI in replacing human employees. Some users share their negative experiences with customer service at various companies, highlighting issues such as scripted responses, lack of problem-solving, and language barriers. Others argue that customer satisfaction is crucial for businesses and that investing in quality support can lead to long-term profitability. There are also discussions about alternative approaches to customer service, such as personalized training and interactive platforms. Additionally, there are comments on chargebacks, small claims courts, and Google's standard information. Overall, the discussion emphasizes the importance of balancing technology and human interaction in customer service.

### Show HN: LLM Assistant to Navigate Regulations

#### [Submission URL](https://nextquestion.io/module/renters-reform) | 12 points | by [nq_ai](https://news.ycombinator.com/user?id=nq_ai) | [4 comments](https://news.ycombinator.com/item?id=36824599)

Titled "Revolutionary Quantum Computing Breakthrough: Google Announces Quantum Supremacy!", this story is making waves in the tech world. Google has just announced a groundbreaking achievement in quantum computing, taking a significant step towards achieving quantum supremacy. Quantum supremacy refers to the moment when a quantum computer can solve a problem that is practically infeasible for even the most advanced classical computers. Google claims to have achieved this milestone, as their quantum processor solved a complex problem in just 200 seconds, whereas the most powerful supercomputer would take approximately 10,000 years to crack it. This breakthrough has the potential to transform various fields, including cryptography, machine learning, and drug discovery. It opens up exciting possibilities for solving problems that were previously considered impossible. However, some experts caution that there is still a long way to go before we see practical applications of quantum computing in our everyday lives.

---

## AI Submissions for Fri Jul 21 2023 {{ 'date': '2023-07-21T17:09:29.749Z' }}

### Computer chip with built-in human brain tissue gets military funding

#### [Submission URL](https://newatlas.com/computers/human-brain-chip-ai/) | 59 points | by [clumsysmurf](https://news.ycombinator.com/user?id=clumsysmurf) | [50 comments](https://news.ycombinator.com/item?id=36821266)

Researchers at Monash University have developed a computer chip called "DishBrain" that combines human and mouse brain cells with electronic circuits and AI intelligence. The chip showed impressive learning capabilities by playing Pong within five minutes. It can read and stimulate brain cells, making it a potential breakthrough for machine learning, robotics, and drug discovery. The project has now received a $407,000 grant from Australia's National Intelligence and Security Discovery Research Grants program. The funding will be used to develop better AI machines that replicate the learning capabilities of biological neural networks.

The discussion on the submission starts with a user making a joke about a bestselling novel called "Please Dont Build Torment Nexus." The next user expresses concern about the potential military applications of this technology, suggesting that it may lead to the destruction of humanity. Other users point out that major players in the AI field are partially funded by the military, and there is a discussion about the ethics and potential dangers of AI in general.

Another user brings up the video game Xenogears and its relevance to the discussion. There is then a comment about science fiction movies and the problem of machines taking over and commanding disorganized groups, to which another user jokingly responds that even President Obama would be unsure about AI. The conversation briefly shifts to discussing the movie Ghost in the Shell.

Some users make lighthearted comments about the funding received by the project, joking about RoboCop and its cost. Others discuss the ethical concerns of connecting brain cells to silicon chips, with one user suggesting that humans should not be treated as machines and raising issues related to reproductive healthcare.

There is a discussion about the number of neurons in different animals and the comparison to the DishBrain chip's capabilities. Some users argue that logical neurons and artificial neurons are different, while others believe they are similar. The conversation takes a humorous turn, suggesting that human consciousness is beyond comprehension.

One user talks about the disappointment of AI research, suggesting that it has not lived up to its potential. There is a mention of the CPU's dominance and the possibility of using donated rat brains to play games. Another user mentions the Thought Emporium's DIY version of similar technology.

Some users continue the lighthearted discussion with comments about kitten brains and nonsensical phrases. Another user expresses bewilderment, followed by comments about the limitations of transmitting brains over the internet.

There is a debate about the investment in the project and the incremental improvements being made. One user makes a pop culture reference to Fallout 3. A discussion about the capabilities of living tissue and replicating it in software ensues.

A user brings up the concept of "Robobrains," linking it to the DishBrain project. Another user mentions Peter Watts' Rifters trilogy, and there is a mention of the game Quake and its plot. The conversation touches on Cordwainer Smith's writing and the idea of growing brain cells connected to electronics. The discussion concludes with a positive comment about the submission.

### Why did Meta open-source Llama 2?

#### [Submission URL](https://matt-rickard.com/why-did-meta-open-source-llama) | 72 points | by [rckrd](https://news.ycombinator.com/user?id=rckrd) | [61 comments](https://news.ycombinator.com/item?id=36817938)

Meta, formerly known as Facebook, has open-sourced Llama 2, an advanced language model. The move has raised questions about the company's motivations. One analysis suggests that Meta's decision aligns with various strategic open-source categories, including hiring, marketing, go-to-market, reducing competitors' advantage, goodwill, and standards lobbying. The author also presents four specific reasons why companies open-source machine learning models, such as possessing proprietary data without enough resources or expertise, recruiting and retaining researchers, selling hardware or cloud resources, or having a breakthrough insight. The Llama 2 license and launch announcement provide further insight into Meta's goals. The author hypothesizes that Meta aims to reduce competitors' advantages, particularly those with proprietary models like Google and OpenAI, as well as companies in the serving stack looking to build their audience. Additionally, the author suggests that Meta may offer Llama 2 as a free-tier or complementary service, potentially integrating it into platforms like Instagram, Threads, or Facebook. Other possibilities include developing specialized hardware or data centers for Llama, providing an ML framework compatible with Llama, or offering a future commercial managed service. Finally, the author notes that Meta stands to gain a reputation as a cutting-edge company if they handle the open-sourcing of Llama 2 well. However, they acknowledge that Meta faces a challenging task in achieving this reputation but emphasizes the importance of shipping products.

The discussion revolves around whether Meta's open-sourcing of Llama 2 can be considered truly open source and the implications of this move. Some commenters point out that the license and terms of Llama 2 do not align with the open-source definition, and that it is more of a distribution model rather than true open source. Others discuss the potential motivations behind Meta's decision, including strategic advantages in hiring, marketing, and reducing competitors' advantages. There is also speculation about how Meta might integrate Llama 2 into their platforms or potentially offer it as a free-tier or complementary service. The discussion also touches on the challenges Meta may face in establishing themselves as a cutting-edge company and the importance of shipping products. Another point of discussion is the impact of the open-sourcing of Llama 2 on the open-source community and the concerns about setting a precedent for companies to selectively release their models without adhering to true open-source principles.

---

## AI Submissions for Thu Jul 20 2023 {{ 'date': '2023-07-20T17:09:30.842Z' }}

### Project Aria 'Digital Twin' Dataset by Meta

#### [Submission URL](https://www.projectaria.com/datasets/adt/) | 173 points | by [socratic1](https://news.ycombinator.com/user?id=socratic1) | [89 comments](https://news.ycombinator.com/item?id=36800041)

Meta, the parent company of Facebook, has introduced the Aria Digital Twin Dataset, which aims to accelerate research in egocentric machine perception. The dataset, captured using Aria glasses, includes extensive ground-truth annotations for devices, objects, and environments. It consists of 200 sequences (~400 minutes) captured in two different locations, with annotations such as 6DoF device trajectory, 3D object pose, 3D human skeleton, and 3D eye gaze. The dataset also includes tools for loading and visualizing the data, as well as participating in object detection challenges. The Aria Digital Twin Dataset is designed to promote responsible innovation and has been captured in controlled environments with fully consented researchers. Researchers can access the dataset by providing their email address.

The discussion on Hacker News revolves around the issue of cookie consent banners and their compliance with privacy regulations such as the GDPR. Some commenters argue that the language used in the banners could be deceptive or misleading, potentially violating the GDPR. Others point out that the banners are necessary for tracking purposes and to improve content and services. There is also a discussion about the implementation of the "Do Not Track" (DNT) feature and whether it is effective or relevant in protecting privacy.  Another topic discussed is the release of the Aria Digital Twin Dataset by Meta (parent company of Facebook) and its potential impact on research in egocentric machine perception. The dataset includes annotated sequences captured using Aria glasses, providing researchers with valuable data for their experiments.

### Foundational AI models do not violate copyright

#### [Submission URL](http://marble.onl/posts/general_technology_doesnt_violate_copyright.html) | 22 points | by [andy99](https://news.ycombinator.com/user?id=andy99) | [15 comments](https://news.ycombinator.com/item?id=36807408)

In a recent article, Andrew Marble discusses the contention that AI models violate copyright because they are capable of generating copyrighted content. Marble argues that while it is possible to build an AI model that violates copyright and use broad foundation models like chatGPT to generate copyrighted content, this does not mean that the training or existence of the models themselves violate copyright. Marble equates the situation to other technologies like photocopiers or VCRs, stating that just because a model can do something doesn't mean people are necessarily utilizing it for nefarious purposes. Rather than limiting the capabilities of AI models, Marble suggests focusing on regulating the use of the technology and holding individuals accountable for any copyright infringements or other misuses. Marble also mentions the importance of distinguishing between broadly capable AI models and narrowly trained models with malicious intent. Overall, the article argues against legislating limitations on AI models at the capability level and emphasizes the need to consider how people use the technology instead.

The discussion on the submission centers around the argument made in the article and the comparison between AI models and technologies like photocopiers and VCRs. Some users agree with the article's perspective, stating that regulating the use of AI technology and holding individuals accountable for copyright infringements is more important than limiting the capabilities of AI models. Others disagree, pointing out that comparing AI to humans in terms of copyright is disingenuous, as AI does not have the same consciousness or decision-making abilities. The discussion also touches on the need to make works publicly available for training AI models and the complexities of copyright law in relation to AI-generated content. Overall, there is a division of opinions on how AI models should be regulated in terms of copyright infringement.

### Decoding the ACL Paper: Gzip and KNN Rival Bert in Text Classification

#### [Submission URL](https://codeconfessions.substack.com/p/decoding-the-acl-paper-gzip-and-knn) | 30 points | by [abhi9u](https://news.ycombinator.com/user?id=abhi9u) | [5 comments](https://news.ycombinator.com/item?id=36806577)

A recent paper presented at the ACL conference for natural language processing (NLP) has gained attention for its innovative approach to text classification. The paper shows that combining the use of gzip and K-nearest neighbour (KNN) can achieve performance comparable to state-of-the-art models like BERT. This is a refreshing perspective at a time when most research focuses on large language models. The paper's findings are explained in layman's terms in this article, with a follow-up article planned to delve deeper into the approach and its implications for NLP research. The traditional approaches to text classification, such as linear regression or neural networks, often require training on large datasets and can be computationally expensive. In contrast, the gzip and KNN approach offers a simpler and more cost-effective solution. The key steps involve compressing the input text using gzip, computing the Normalized Compression Distance (NCD) between the compressed texts, and then finding the k-nearest neighbors based on these distances. The majority class of the neighbors is then chosen as the target label. The NCD measures the amount of shared information between two texts, and if they have similar content, their concatenation will achieve higher compression. This approach leverages compression algorithms and information theory to achieve accurate text classification without the need for massive language models.

The discussion around the submission primarily revolves around the potential of the gzip and KNN approach for text classification. One user points out that the technique may overestimate performance relative to BERT, and recommends evaluating its accuracy against expected KNN compressed data. Another user suggests that the paper has the potential to start an interesting discussion on finding synonymous word classes, to which another user responds that such an approach could work using semantic modeling and embeddings. The second user also mentions that they are writing an article to explain how gzip can be helpful in text classification. One user jokingly adds that the approach wouldn't find synonyms that are closer to random strings.

### AI That Teaches Other AI

#### [Submission URL](https://viterbischool.usc.edu/news/2023/07/teaching-robots-to-teach-other-robots/) | 97 points | by [geox](https://news.ycombinator.com/user?id=geox) | [40 comments](https://news.ycombinator.com/item?id=36799073)

Researchers from the University of Southern California (USC) have developed a tool called SKILL (Shared Knowledge Lifelong Learning) that allows robots to teach each other how to learn. In a paper published in Transactions on Machine Learning Research, the team describes how AI agents learned 102 different tasks, such as categorizing images of cars or flowers, and shared their knowledge over a decentralized communication network. The robots were able to master all 102 tasks by teaching each other, reducing the time needed for learning by a significant factor. The researchers believe that this approach could be scaled up to thousands or millions of tasks, potentially transforming various industries and creating a more connected and efficient global community. They envision applications in healthcare, where AI systems could specialize in different areas of medicine and provide doctors with the most up-to-date information, as well as in tourism, where every smartphone user could become a local tour guide by sharing photos and details about different landmarks and local cuisine. The researchers see potential in using SKILL technology in any profession that requires vast knowledge or deals with complex systems.

The discussion on this submission revolves around several topics. One user highlights the challenges of individuality in AI and the potential risks associated with the development of artificial general intelligence (AGI). Another user discusses the limitations and diminishing returns of scaling up AI models. Some users express confusion about the terminology used and seek clarification on the concept of Mixture of Experts (MoE) models. The potential implications of SKILL technology in various industries, such as healthcare and tourism, are also discussed. Additionally, there is conversation about the integration of AI and human knowledge-sharing, as well as debates on the risks and benefits of AI advancements for society.

### TSMC delays Arizona factory that will eventually build chips for iPhones and AI

#### [Submission URL](https://www.theverge.com/2023/7/20/23802107/tsmc-arizona-chip-factory-delay-q2-earnings-report) | 29 points | by [rntn](https://news.ycombinator.com/user?id=rntn) | [10 comments](https://news.ycombinator.com/item?id=36806863)

Taiwan Semiconductor Manufacturing Company (TSMC) has announced that the start of chip production at its new facility in Arizona, which is intended to manufacture chips for iPhones and AI, will be delayed to 2025 due to labor shortages. The company's first fab in Phoenix was initially scheduled to begin producing 4nm chips next year, but TSMC is now facing challenges in finding skilled workers for equipment installation. To make up for lost time, TSMC plans to send experienced technicians from Taiwan to train local workers at the plant. TSMC's Q2 earnings report also showed a 10% decline in revenue and a 23% decline in profits compared to the same period last year, with a projected 10% revenue drop for the full year. The company expects the capacity shortage caused by high demand for AI-capable chips to persist until next year. TSMC is working with the US government to maximize subsidies and tax credits available to cover the increased costs of fabricating in the US.

The discussion on this submission revolves around various aspects of the delay in chip production at TSMC's new facility in Arizona, along with the company's financial situation. Here are some key points from the comments:

- Some users suggest that TSMC could have started training programs to address the labor shortage earlier.
- A user mentions that skilled friends who used to work in chip fabs have moved to software jobs, as the latter is considered to have higher salaries and fewer hazards.
- Another user points out that certain fields require high skills, yet their compensation is little compared to software-related work.
- There is a discussion about TSMC's profitability, with some users emphasizing the company's success and others highlighting the significant profits it generates and the higher salaries it pays to employees.
- A user shares a link to an article discussing TSMC's financials, including its $33 billion profit.
- One user mentions that while the delay in chip production is unfortunate, TSMC should have started comprehensive training programs for workers earlier.
- There is a request for further discussion on this topic, with a link to an article on The Verge being shared.

Note: The conversation contains a few replies marked as "dd" or "dp", without further context or content.

### 9k authors say AI firms exploited books to train chatbots

#### [Submission URL](https://www.latimes.com/entertainment-arts/story/2023-07-19/artificial-intelligence-9000-authors-sign-letter-rebuking-ai-companies-books) | 28 points | by [webmaven](https://news.ycombinator.com/user?id=webmaven) | [12 comments](https://news.ycombinator.com/item?id=36807296)

More than 9,000 authors have signed an open letter calling out tech companies behind generative AI for exploiting copyright-protected works without consent, credit, or compensation. The authors argue that these companies are using their writings to train chatbots, such as GPT-4 and ChatGPT, to summarize and imitate their works. The open letter specifically calls out tech giants like OpenAI, Alphabet, Meta, IBM, and Microsoft, urging them to obtain permission and fairly compensate authors for the use of their works in generative AI programs and outputs. The Authors Guild highlights the threat that generative AI poses to writers' professions, as it floods the market with machine-written content and contributes to a decline in authors' income. This comes shortly after bestselling authors Mona Awad and Paul Tremblay filed a lawsuit against OpenAI, claiming that ChatGPT was trained using portions of their novels without their consent. Both the open letter and the lawsuit shed light on the ethical and legal questions surrounding the use of copyrighted material in AI technology.

The discussion on this submission revolves around various aspects of copyright law, fair use, and the impact of generative AI on the distribution of content.  One commenter raises the point that purchasing a book does not necessarily grant the purchaser the right to modify or use the content in AI systems. They argue that there is a distinction between creating a parody or satire, which may be protected as fair use, and using copyrighted works without permission in generative AI. Another commenter questions whether embedding AI crops into AI systems through modification would qualify as fair use. They further discuss the complexity of copyright claims and the applicability of fair use factors in determining the legality of using copyrighted works. A reply to this comment emphasizes the importance of the four factors considered in fair use cases: the purpose and character of the use, the nature of the copyrighted work, the amount and substantiality of the portion used, and the effect on the potential market for the copyrighted work. They also mention that the commercial nature of the AI applications and the potential harm to the market for original works could be significant factors to consider. In response to a comparison made between modifying the Mona Lisa and using AI-generated crops, another commenter mentions a recent Supreme Court decision involving Warhol and Goldsmith. They note that the decision clarified the high level of commerciality necessary to define copying as illegal for derivative works. The discussion then delves into the proprietary nature of AI algorithms and the commercial rush to capitalize on generative AI. The commenter suggests that the AI community is protecting their proprietary interests and refusing to engage with AI regulation. One commenter expresses skepticism about prevailing legal arguments in the court proceedings and hopes that people will learn to synthesize new content based on existing works.

---

## AI Submissions for Tue Jul 18 2023 {{ 'date': '2023-07-18T17:10:26.153Z' }}

### Voder Speech Synthesizer

#### [Submission URL](https://griffin.moe/voder/) | 247 points | by [CyborgCabbage](https://news.ycombinator.com/user?id=CyborgCabbage) | [40 comments](https://news.ycombinator.com/item?id=36771149)

In a fascinating throwback to the 1939-40 New York World's Fair, an application has been created that allows users to experience what it was like to operate the Voder, an early speech synthesis device. The Voder required complex button sequences to form each syllable, and it took about a year of practice to produce fluent speech. Helen Harper, one of the first people to master the Voder, went on to teach women how to use it through a year-long course. Now, this application puts users in the shoes of these skilled operators, allowing them to create vowel formants by pressing specific button combinations. While the results may not sound exactly like the original video, due to subtle articulations and dynamics, it provides a unique glimpse into the past.

### Generative AI space and the mental imagery of alien minds

#### [Submission URL](https://writings.stephenwolfram.com/2023/07/generative-ai-space-and-the-mental-imagery-of-alien-minds/) | 244 points | by [RafelMri](https://news.ycombinator.com/user?id=RafelMri) | [106 comments](https://news.ycombinator.com/item?id=36767837)

In a recent post on his blog, Stephen Wolfram explores the concept of alien minds and how artificial intelligence (AI) can help us understand them. Wolfram explains that AIs are essentially accessible forms of alien minds because they are not aligned with human thought processes. To capture the "mental imagery" of an alien AI, Wolfram suggests modifying a generative AI by resetting weights in its neural net. This "alien" neural net still produces images, but they become increasingly different from human perception as the neural net is modified. By studying these progressively alien images, Wolfram believes that we can gain insight into the worlds of different and alien minds. Additionally, Wolfram explains how AIs are trained to generate images by capturing the regularities found in billions of images from the web. These "random images" exhibit the statistical patterns of the training data and can show recognizable objects or scenes. Overall, Wolfram's exploration of alien minds and generative AI provides a fascinating perspective on perception and cognition.

The discussion on the submission revolves around the concept of AI-generated images and how they relate to human perception. Some comments touch on the similarities and differences between AI dream-like images and human dreams. Other comments discuss the technical aspects of AI-generated images and how they can be seen as artistic expression. Some users also express concerns about the role of AI in generating content and its impact on traditional art forms. Overall, the discussion explores the potential of AI to produce novel and intriguing imagery, while also raising questions about its limitations and implications.

### A Theory on Adam Instability in Large-Scale Machine Learning

#### [Submission URL](https://arxiv.org/abs/2304.09871) | 135 points | by [vov_or](https://news.ycombinator.com/user?id=vov_or) | [51 comments](https://news.ycombinator.com/item?id=36771484)

A new theory has emerged in the field of machine learning that explains the previously unexplained divergent behavior observed in the training of large language models. The theory suggests that the phenomenon is due to the optimization algorithm used for training, called Adam. The researchers argue that Adam can reach a state where the parameter update vector has a large norm and is uncorrelated with the direction of descent on the training loss landscape, leading to divergence. This behavior is more likely to occur in the training of deep models with large batch sizes, which is common in large-scale language model training. The theory is supported by observations from training runs of language models with varying scales. The paper, titled "A Theory on Adam Instability in Large-Scale Machine Learning," is authored by Igor Molybog and 16 other researchers. It is available for download on arXiv.

The discussion on this submission revolves around the new theory proposed by the researchers regarding the divergence in training large language models due to the use of the Adam optimization algorithm. Some users suggest trying different techniques such as controlling the term of the gradient or restarting the training process to mitigate the issue. Others discuss the potential of using CMA-ES (Covariance Matrix Adaptation Evolution Strategy) and other optimization methods for better results. There are also discussions on the limitations of gradient-based optimization methods and the challenges faced in finding global minima. Some users mention the possibility of using swarm optimization and genetic algorithms to improve the search process. One user points out the importance of local minima and the difference between biological neural networks and machine learning models. The energy costs of training models and the efficiency of neural networks compared to humans are also discussed in the comments.

### G/O media will make more AI-generated stories despite critics

#### [Submission URL](https://www.vox.com/technology/2023/7/18/23798164/gizmodo-ai-g-o-bot-stories-jalopnik-av-club-peter-kafka-media-column) | 101 points | by [Analemma_](https://news.ycombinator.com/user?id=Analemma_) | [100 comments](https://news.ycombinator.com/item?id=36773363)

G/O Media, the digital publisher behind sites like Gizmodo and the Onion, sparked controversy when it published four stories generated by AI engines without input from its editors or writers. Despite the backlash, G/O executives have expressed plans to create more AI-written stories as part of an ongoing experiment with the technology. The move sets G/O apart from most conventional publishers, who are interested in using AI to assist in content creation but are not yet interested in fully machine-made content. G/O Media CEO Jim Spanfeller believes that AI will be transformative for the media industry and should not be ignored. However, G/O employees are concerned about the impact on employee morale and fear that AI will eventually replace human journalists. G/O executives insist that they will not replace staff with AI, but the skepticism among employees remains.

### A Latent Space Theory for Emergent Abilities in Large Language Models

#### [Submission URL](https://arxiv.org/abs/2304.09960) | 14 points | by [rileyphone](https://news.ycombinator.com/user?id=rileyphone) | [4 comments](https://news.ycombinator.com/item?id=36776920)

The paper titled "A Latent Space Theory for Emergent Abilities in Large Language Models" by Hui Jiang explores the relationship between languages and their underlying meanings. The study categorizes languages as unambiguous or epsilon-ambiguous and demonstrates that large language models (LLMs) can exhibit emergent abilities, such as language understanding, in-context learning, chain-of-thought prompting, and effective instruction fine-tuning, through Bayesian inference on the sparse joint distribution of languages. This paper presents quantitative results and sheds light on the capabilities of LLMs in language processing.

The discussion around the submission focuses on the structure and analysis of languages. One user argues that languages are created randomly for specific purposes and convey information through distinct and independent units such as sentences or programming languages. Another user adds that language messages represent intended meanings in a space, where different intentions constitute distinct regions and simple intentions are represented by elements in a finite set. They suggest that the study of language should consider discrete countable assumptions.  In response, another user provides research that supports the effectiveness of transformer-based language models. They share two articles that discuss the reasonable effectiveness of these models.  However, one user disagrees with the assumption that language messages contain intentions recursively in a fundamental space of meaning. They argue that this approach to studying language is oversimplified and may not fully capture the complexity of the field.  Finally, a user points out that the discussion seems to lack an interdisciplinary perspective and suggests that siloing of perspectives may prevent a comprehensive understanding of the topic.

### Qualcomm works with Meta to enable on-device AI applications using Llama 2

#### [Submission URL](https://www.qualcomm.com/news/releases/2023/07/qualcomm-works-with-meta-to-enable-on-device-ai-applications-usi) | 99 points | by [ahiknsr](https://news.ycombinator.com/user?id=ahiknsr) | [82 comments](https://news.ycombinator.com/item?id=36778730)

Introducing the Hacker News Daily Digest, your one-stop destination for a quick and engaging summary of the top stories on Hacker News. We've got our virtual pens and minds ready to deliver the latest and most interesting news from the developer and tech community straight to you.

Whether you're a seasoned coder, a tech enthusiast, or simply curious about the buzzing tech world, our daily digest will keep you informed and entertained. So sit back, relax, and let us handle the heavy lifting of filtering through the vast sea of Hacker News submissions.

From software development breakthroughs to the latest tech gossip, we'll cover it all. Expect insights into the hottest programming languages, discussion threads on frameworks, debates over the merits of different tools, and updates on the most interesting startups.

But that's not all. We'll also dive into thought-provoking and mind-bending articles on artificial intelligence, blockchain technology, cybersecurity, and more. Our aim is to provide you with the most captivating and relevant stories that will ultimately spark your curiosity and keep you ahead of the game.

You can count on us to deliver daily summaries that capture the essence of the Hacker News community. Say goodbye to endlessly scrolling through countless submissions; we'll bring you only the best and most interesting conversations that are shaping the tech landscape.

So whether you're a seasoned Hacker News regular or a newcomer to the platform, join us for the Hacker News Daily Digest and stay tuned for an engaging and informative daily dose of the tech world's most captivating stories.

The discussion on the submission revolves around various aspects of Apple's approach to artificial intelligence (AI). Some commenters express skepticism about Apple's AI capabilities and suggest that the company is focusing more on hardware rather than AI integration. Others mention Apple's past failures, such as Apple Maps, and criticize the company's product planning and recognition of flaws. 

There is a discussion about the usability of Apple Maps compared to Google Maps, with some users pointing out the shortcomings of Apple Maps and praising Google Maps for its UI and search capabilities. 

On the topic of Apple's AI advancements, some users mention the potential integration of Siri with local machine learning models (LLMs) and the possibility of Siri becoming a more advanced personal assistant. Others discuss Apple's track record of integrating AI features into iOS, such as text and photo recognition, and the company's ability to apply innovative models. 

There is a debate about the advantages of Apple's AI strategy compared to other companies, with some highlighting the company's focus on user experience and unique features, while others argue that Apple is not as competitive in AI. 

The conversation also touches on Apple's investment in LLMs, possible future developments in AI, and the capabilities of chatbots and machine learning models.

### Meta and Qualcomm team up to run Llama 2 on phones

#### [Submission URL](https://www.cnbc.com/2023/07/18/meta-and-qualcomm-team-up-to-run-big-ai-models-on-phones.html) | 17 points | by [tzm](https://news.ycombinator.com/user?id=tzm) | [3 comments](https://news.ycombinator.com/item?id=36775645)

Qualcomm and Meta have announced a partnership to enable the social networking company's new large language model, Llama 2, to run on Qualcomm chips on phones and PCs starting in 2024. Large language models like Llama 2 have traditionally run on large server farms with powerful Nvidia graphics processors. This announcement by Qualcomm suggests that it wants to position its processors as well-suited for AI "on the edge," or on a device, rather than in the cloud. Running large language models on phones could significantly reduce the cost of running AI models and lead to better and faster voice assistants and other applications. Qualcomm's chips include a tensor processor unit (TPU) that is well-suited for AI calculations, although the processing power available on a mobile device is much less than that of a data center with cutting-edge GPUs. Meta's Llama 2 is unique because Meta published its "weights," which govern how the AI model works, making it open-source and accessible to researchers and commercial enterprises without permission or payment. Qualcomm and Meta have previously collaborated on chips for virtual reality devices.

The discussion on Hacker News revolves around the announcement of the partnership between Qualcomm and Meta to enable the running of Meta's large language model, Llama 2, on Qualcomm chips on devices starting in 2024.  One user questions whether Llama 2 will be relevant by 2024 and suggests that there might be a newer version of the model. 
 Another user mentions that Qualcomm has previously collaborated with Stable Diffusion to run ML models on phones and that they published a demo with card recognition. They speculate that Qualcomm might have built an extensive feature set, possibly for user interface implementation.  Another user highlights the importance of this announcement, specifically in terms of enabling low-latency implementation of large language models on devices, which could greatly impact the user experience of AI voice assistants and other applications. They compare it to Google's PaLM version for Android devices and suggest that Apple might also jump on this trend early by introducing an on-device Siri powered by ML chips.

### From Dating to Vector Search – “Stable Marriages” on a Global Scale

#### [Submission URL](https://ashvardanian.com/posts/searching-stable-marriages/) | 35 points | by [vov_or](https://news.ycombinator.com/user?id=vov_or) | [31 comments](https://news.ycombinator.com/item?id=36772545)

In this article, the author explores the concept of stable marriages and its implications for the future of databases. Stable marriages are computed using preference lists, but this approach requires a large amount of memory when dealing with a large number of candidates. To overcome this, the author suggests using a scalable Vector Search engine to dynamically recalculate candidate lists. However, the quality of representations in a shared Vector Space, particularly for Multi-Modal data, is a challenge. The author highlights the need for improving space-alignment techniques to enhance the performance of upcoming Generative Vision-Language models. The article also shares the author's personal journey of applying these algorithms to a dating app and the cost implications of using the classic stable marriage algorithm for a billion candidates. The author explains how to balance compute requirements by recalculation and discusses the implementation details of a Vector Search engine.

The discussion on this article covers a wide range of topics related to arranged marriages, divorce rates, personal freedom, societal stability, and cultural differences. Some users argue that arranged marriages can lead to stability and happiness, while others believe that individual happiness should take precedence. There is also a discussion about the cultural contexts in which arranged marriages are common and the social pressures that can be associated with them. Additionally, there are comments about the need for structured content in databases and the trade-offs between personal freedom and societal benefits.

### AI System Helped Cops ID a Drug Trafficker by Analyzing His Driving Patterns

#### [Submission URL](https://gizmodo.com/rekor-ai-system-analyzes-driving-patterns-criminals-1850647270) | 30 points | by [HiroProtagonist](https://news.ycombinator.com/user?id=HiroProtagonist) | [19 comments](https://news.ycombinator.com/item?id=36772253)

In a recent case in New York, police were able to identify and arrest a drug trafficker with the help of artificial intelligence (AI). The police used the services of a company called Rekor, which analyzes traffic patterns and identifies suspicious behavior. Rekor's software sifts through a large database of information collected from regional roadways by the county's automatic license plate recognition system. By recording and analyzing vehicle trajectories, the software can determine whether certain routes are suspicious or not. In this case, the AI algorithm determined that the driver's routes were consistent with those of a drug trafficker, leading to his arrest. This highlights how AI is being used to enhance surveillance systems and aid law enforcement. However, there are concerns about the potential misuse of this technology and the need for appropriate regulations to prevent abuse.

The discussion on this submission revolves around the pros and cons of AI-assisted surveillance systems and their potential misuse.  One user points out that the AI algorithm used in this case reminds them of the controversy surrounding the analysis of phone data, where innocent activities were mistakenly flagged as suspicious. Another user jokingly suggests that the investigation could have been prompted by Tupperware parties, emphasizing that false positives can occur. Some users express concerns about the potential for targeted searches in minority neighborhoods and the erosion of privacy. Others argue that AI-based investigations can be helpful in fighting crime, but there needs to be clear regulations in place to prevent abuse. The discussion also touches on the limitations of AI systems and their potential for false positives. One user brings up the issue of law enforcement accessing location records without a warrant, while another user notes the historical use of AI systems to combat terrorism and drug dealers. There is a debate about the balance between privacy and security, with some arguing that surveillance technology is necessary to target criminals while others feel it infringes on civil liberties. One user points out the decline in public trust in AI and the surveillance state, while another user suggests that the focus should be on legislation to address the problem. Finally, there is a mention of the shift in public attention towards other pressing issues such as domestic terrorism, child trafficking, and party affiliations.

---

## AI Submissions for Mon Jul 17 2023 {{ 'date': '2023-07-17T17:10:09.755Z' }}

### Computer memory prototype ditches 1s and 0s for denser data storage

#### [Submission URL](https://newatlas.com/electronics/computer-memory-resistive-switching-denser-data-storage/) | 91 points | by [DamnInteresting](https://news.ycombinator.com/user?id=DamnInteresting) | [97 comments](https://news.ycombinator.com/item?id=36763758)

Scientists at Cambridge University have developed a prototype for a new form of computer memory that could potentially hold up to 100 times more data than current technology. The memory system is based on resistive switching memory, which allows for a continuous range of states, rather than just the traditional "one" or "zero" encoding. The prototype uses barium bridges between thin films of a disordered material, allowing for a broad spectrum of electrical resistance differences to store data. The material used, hafnium oxide, is already widely used in the semiconductor industry, making it easier to incorporate into existing manufacturing techniques. The researchers believe that this new form of memory could have applications in the fields of AI and machine learning.

### A.I. (1981)

#### [Submission URL](https://www.newyorker.com/magazine/1981/12/14/a-i) | 33 points | by [lioeters](https://news.ycombinator.com/user?id=lioeters) | [8 comments](https://news.ycombinator.com/item?id=36757726)

In 1979, a computer program called BKG 9.8 played against the winner of the world backgammon championship in Monte Carlo and won the game. This victory surprised many, as existing microprocessors were not expected to provide a good game. The program, run on a large computer at Carnegie-Mellon University connected to a robot named Gammonoid, won seven out of eight games against Luigi Villa of Italy. This marked the first time a machine became a world champion in a board or card game. The article also explores the implications of machines evolving to produce outputs that are indistinguishable from those of humans. Marvin Minsky, a prominent figure in artificial intelligence, comments on the illusion of free will and highlights that when intelligent machines are created, they may also grapple with questions of consciousness and free will.

The discussion on Hacker News about the submission seems to revolve around a few key points. 

One user, "ltrs," suggests that personal computers have come a long way in terms of advanced computer languages, and they argue that the programs created by individual programmers for personal computers can be considered informal examples of artificial intelligence programs. They further explain that professional programmers often describe a program's process using informal terms, such as showing program-writing programs. 

Another user, "krmkz," humorously comments that after checkers and chess, backgammon becoming a machine world champion is not surprising and expects more advancements in the near future, possibly in games that require more technical reasoning and quantification.

User "jhbdgr" finds it a good reminder that relatively recently, board games have been considered significant topics in AI research, mentioning the famous match between Deep Blue and Kasparov in 1997.

Another user, "Affric," finds it thought-provoking and wonders if AI programs are considered complex adaptive systems that exchange information in an anthropomorphized manner. They suggest that more recent technologies partially play board games by violating the position-pruning model accordingly.

"tncnv" adds that when discussing AI, it doesn't refer to specific algorithms, but rather to a whole family of algorithms.

User "siva7" expresses their lack of impression, saying that it doesn't impress people anymore when compared to recent advancements in AI.

Overall, the discussion seems to touch on the capabilities of personal computers, the progress of AI in board games, the significance of board games in AI research, and the differentiation between AI algorithms and their broader family.

### Incumbents vs. Startups in the AI Race

#### [Submission URL](https://blog.autopilot.fund/incumbents-vs-startups-in-the-ai-race/) | 30 points | by [mooreds](https://news.ycombinator.com/user?id=mooreds) | [13 comments](https://news.ycombinator.com/item?id=36764749)

In today's digest, we explore the ongoing race between incumbent players and startups in the AI industry. Alex Rampell from a16z highlights the battle, stating that it ultimately comes down to whether the startup can gain distribution before the incumbent achieves innovation. Incumbents possess the resources and manpower to replicate startups' offerings, but they often struggle with bureaucratic processes and resistance to change, impeding innovation. Startups, on the other hand, have agility on their side. They can swiftly make decisions and bring products to market but lack the capital and established customer base that incumbents possess. However, their speed becomes crucial in a fast-moving environment. But what happens when incumbents accelerate their pace? Matt Turk from FirstMark explains that big tech incumbents, such as Adobe and Microsoft, rapidly deployed AI, rendering many generative AI design startups obsolete. The combination of fast execution and extensive distribution becomes a significant challenge for startups. Furthermore, AI startup funding decreased in Q1 2023, potentially due to fierce competition between giants and high startup valuations. Incumbents also have an advantage in their access to large proprietary data sets, assisting in fueling AI feature growth. To compete, startups can focus on unsolved problems in specific verticals or leverage unique customer-generated data produced on their platforms. The Autopilot Fund is interested in hearing from founders building exciting solutions that leverage unique data and new AI tools to automate workflows.

The discussion on this submission revolves around various aspects of the AI industry and the challenges faced by startups in competing with incumbents.

- One user points out that the article spends 90% of the time asking questions and making broad statements without providing concrete answers or insights.
- Another user expresses concern that the article simplifies the competition between startups and incumbents, stating that innovation in the AI space mostly happens within large companies. They believe that it is the small teams within these companies that take risks and build groundbreaking things, rather than startups.
- A user highlights that startups typically focus on building a tiny portion of a product, while incumbents have the resources to directly compete. They also mention that adopting AI is relatively easy for incumbents and greatly improves their core products, making it challenging for startups in the environment.
- A user argues that real results can only be achieved through meticulous implementation, pointing out that sometimes mundane technology, when widely implemented, can have a significant impact.
- The possibility of some startups merely riding the hype wave to secure funding is brought up, suggesting that true success in the AI space is difficult to achieve for companies trying to profit in the current environment.
- There is a discussion about the legality of building AI with gaming frameworks, with one user stating that certain gaming frameworks strictly prohibit using them for AI training. Another user emphasizes that the legality doesn't matter as long as the technology is effective.
- The importance of leveraging customer-generated data produced on platforms is mentioned as a way for startups to compete with incumbents.

Overall, the discussion highlights different perspectives on the challenges faced by startups in the AI industry and the role of incumbents in driving innovation.

### Wikipedia-grounded chatbot “outperforms all baselines” on factual accuracy

#### [Submission URL](https://en.wikipedia.org/wiki/Wikipedia:Wikipedia_Signpost/2023-07-17/Recent_research) | 227 points | by [akolbe](https://news.ycombinator.com/user?id=akolbe) | [175 comments](https://news.ycombinator.com/item?id=36757520)

A recent academic research paper has found that a Wikipedia-grounded chatbot outperforms all other baselines when it comes to factual accuracy. The study analyzed the role of open access in Wikipedia's citation patterns and found that open-access articles are increasingly more cited on the platform. The researchers also discovered that open-access articles have a 15% higher likelihood of being cited compared to closed-access articles. This is particularly true for articles with low citation counts, indicating that open access plays a key role in disseminating scientific knowledge and providing timely access to novel results for Wikipedia editors. However, the study raises questions about the reliability of sources used by Wikipedians and the impact of open access on the seriousness of Wikipedia sources. The research also acknowledges limitations, such as the need to control for academic discipline and consider the age and number of citations of research articles at the time they are cited on Wikipedia. While the study is not yet solid enough to be directly cited on Wikipedia, it signals important research in progress that could lead to interesting insights for the Wikipedia community.

The discussion on the submission revolves around various aspects related to Wikipedia and the study mentioned in the summary. Some users discuss the reliability of sources used by Wikipedia, with one user pointing out the arbitrary double standards in applying reliability criteria. Others discuss the potential biases in conservative sources and the need to consider the divergent realities reflected in modern conservatism. There is also a discussion about the potential consequences of using large models like Chat-GPT to modify Wikipedia content, with concerns raised about the unpredictability of the modifications. The availability of Wikipedia snapshots and archives is mentioned, and there is a brief mention of the challenges in accessing translated articles in local languages. Overall, the discussion touches on issues related to source reliability, biases, content moderation, and the practicality of using large language models in Wikipedia.

### AI watches millions of cars and tells cops if you’re driving like a criminal

#### [Submission URL](https://www.forbes.com/sites/thomasbrewster/2023/07/17/license-plate-reader-ai-criminal/) | 77 points | by [thehoff](https://news.ycombinator.com/user?id=thehoff) | [89 comments](https://news.ycombinator.com/item?id=36764389)

An AI-powered policing tool has come under scrutiny after it identified a gray Chevrolet as a possible criminal vehicle in a drug trafficking case in New York. The AI analyzed the driving patterns of the car using a database of 1.6 billion license plate records and determined that it was following routes known to be used by drug traffickers. The car was subsequently pulled over and a search led to the discovery of crack cocaine, a pistol, and a large amount of cash. The case highlights the potential constitutional issues that come with AI-powered policing, as there was no judicial oversight in this instance. The AI tool used in the case was developed by Rekor, which has sold its automatic license plate recognition (ALPR) technology to numerous police departments across the US. Privacy advocates are concerned about the scale of surveillance and the lack of oversight in these systems.

- User "tmrd" argues that the AI system operates on the assumption that certain locations are visited by criminals, which they claim is unfair.

- User "spprtngnr" mentions that the AI system is designed to support law enforcement and that any potential flaws should be addressed through improvements in the technology.