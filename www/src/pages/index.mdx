import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Tue Jun 03 2025 {{ 'date': '2025-06-03T17:13:14.513Z' }}

### Deep learning gets the glory, deep fact checking gets ignored

#### [Submission URL](https://rachel.fast.ai/posts/2025-06-04-enzyme-ml-fails/index.html) | 546 points | by [chmaynard](https://news.ycombinator.com/user?id=chmaynard) | [134 comments](https://news.ycombinator.com/item?id=44174965)

In an intriguing tale of artificial intelligence and scientific verification, two research papers reveal a compelling story about enzyme function prediction using deep learning. On one hand, researchers managed to publish their work in the prestigious journal Nature Communications by training a Transformer model to predict enzyme functions from vast datasets. Their paper garnered significant attention online, achieving high Altmetric scores and being viewed thousands of times.

However, behind this glitzy success lies another narrative, less celebrated but equally crucial. Dr. de Crécy-Lagard, with her extensive expertise, uncovered numerous errors in the acclaimed paper's predictions. Despite following common methodologies, the Nature Communications paper made several inaccurate claims about enzyme functions in E. coli, including incorrect predictions for the gene yciO, which was not novel as claimed. Her findings were meticulously detailed in a preprint on bioRxiv but received far less recognition.

This contrast in reception highlights the challenges within the scientific publishing landscape, where flashy AI results often overshadow the painstaking work of validation and error correction. The case raises critical questions about the reliability of current machine learning models in complex biological fields and shines a light on the publishing incentives that favor novel findings over stringent verification. It serves as a reminder of the necessity for domain expertise in evaluating AI-generated results and the importance of maintaining scientific integrity.

**Summary of Hacker News Discussion:**

The discussion revolves around skepticism toward AI's role in scientific research, particularly in enzyme function prediction, and broader concerns about reproducibility, model reliability, and academic incentives. Key points include:

1. **Model Criticism**:  
   - Users question the overreliance on complex models like Transformers (e.g., BERT, GPT) for tasks such as enzyme prediction, arguing simpler methods (e.g., SVMs) might suffice.  
   - Concerns are raised about inflated accuracy metrics (e.g., "92% accuracy") masking poor real-world applicability, with some attributing this to flawed data splits or cherry-picked results.  

2. **Reproducibility Crisis**:  
   - Many highlight the difficulty of reproducing AI research, citing issues like withheld code, dataset contamination, and corporate secrecy (e.g., OpenAI’s practices post-ChatGPT).  
   - Comparisons are drawn to human learning processes, where models "internalize" examples but lack transparency in reasoning.  

3. **Publication Bias**:  
   - Participants criticize academia’s focus on "high-impact" papers with flashy AI results over rigorous validation. Stories of students struggling to replicate studies underscore systemic flaws.  
   - The irony of using AI-generated comments (via Transformers) to critique AI research is noted, emphasizing the meta-problem of trusting automated outputs.  

4. **Domain Expertise & Validation**:  
   - Validating AI predictions requires deep domain knowledge, as seen in Dr. de Crécy-Lagard’s preprint correcting enzyme claims.  
   - Suggestions include hybrid human-AI validation processes and stricter requirements for code/data sharing in publications.  

5. **Testing & Trust**:  
   - Debates arise over using multiple-choice tests for LLMs, likening it to flawed student assessments. Some argue LLMs should "refuse" uncertain answers to avoid propagating errors.  

**Takeaway**: The discussion underscores a tension between AI’s potential and its pitfalls, advocating for humility, transparency, and collaboration between AI tools and human expertise to uphold scientific integrity.

### Vision Language Models Are Biased

#### [Submission URL](https://vlmsarebiased.github.io/) | 166 points | by [taesiri](https://news.ycombinator.com/user?id=taesiri) | [131 comments](https://news.ycombinator.com/item?id=44169413)

In a groundbreaking study, a group of researchers has exposed a significant flaw in state-of-the-art Vision Language Models (VLMs): their reliance on memorized knowledge rather than actual visual analysis when faced with images that include subtle modifications. This research, involving experts from institutions like KAIST and Auburn University, reveals that while VLMs excel at recognizing familiar objects in unaltered settings—like the Adidas logo or the typical anatomy of animals—they catastrophically fail when tasked with identifying modifications such as additional stripes or extra legs in counterfactual images. 

The researchers highlight how VLMs achieve 100% accuracy on standard images but plummet to around 17% on altered ones. For instance, when presented with a dog with an extra leg, models continue to assert that the dog has four legs, demonstrating a default reliance on what's memorized ("dogs have four legs") instead of analyzing the visual evidence. This showcases a deep-rooted confirmation bias: VLMs aren't "seeing" objects; they're simply recalling.

To investigate this, the team utilized the VLMBias Framework, a methodical approach that differentiates between memorization and visual analysis. Their tests spanned seven domains, revealing severe performance gaps—such as 2.12% accuracy in counting legs on modified animals and a shocking 0.44% when identifying modified car logos. Even slight alterations in national flags or chess pieces caused significant errors.

The findings suggest that VLMs' memorization of canonical forms and logos severely limits their adaptability, raising questions about their reliability in real-world applications where accuracy in detecting subtle changes matters. This research paves the way for future improvements in visual models, advocating for a shift towards empowering these systems to effectively analyze and interpret visual data rather than purely relying on memorized knowledge.

**Summary of Hacker News Discussion:**

The discussion revolves around a study exposing Vision Language Models' (VLMs) overreliance on memorized data over visual analysis. Key points from the comments include:

1. **Debate Over Model Biases and Errors**:  
   - Participants draw parallels between VLMs' failures and earlier research on social biases in AI embeddings (e.g., associating "anger" with stereotypical depictions of people). Some argue this reflects *objective* errors (e.g., miscounting legs) rather than subjective biases.  
   - Others highlight the challenge of distinguishing models’ "training data biases" from true cognitive biases in humans, noting that dataset imbalances heavily influence outcomes.

2. **Human vs. AI Behavior**:  
   - Users compare VLMs’ mistakes to human heuristics (e.g., assuming a dog has four legs without looking carefully). Some see this as a flaw, while others argue humans also shortcut complex visual tasks.  
   - One user notes that humans, when tricked (e.g., counting legs in a misleading image), might default to assumptions, much like VLMs.

3. **Testing and Real-World Validity**:  
   - Tests with **ChatGPT-4o** show mixed results: it correctly identified a zebra with extra legs but struggled with contrived examples (e.g., Braille sign misinterpretations). Skepticism arises about whether such tests reflect real-world use cases or are overly reliant on synthetic, "trick" images.  
   - Critics argue models are often trained on "canonical" examples (e.g., logos, animals with standard features), so altered images may not exist in their training data. One user jokes: "Don’t mind the five-legged dog—it’s just poisoned training data!"

4. **Implications for AI Development**:  
   - The low accuracy (e.g., 17% on modified images) suggests VLMs prioritize memorized patterns over in-the-moment analysis. This raises concerns for applications requiring nuanced visual understanding (e.g., medical imaging, quality control).  
   - Some propose solutions like fine-tuning models or improving data diversity, but others dismiss the issue as overhyped, emphasizing progress in benchmarks and resolution fidelity.

5. **Broader Observations**:  
   - Participants question whether VLMs’ reliance on memorization is inherently problematic or an unavoidable trade-off for generalization.  
   - A recurring theme: AI may mimic human flaws (e.g., confirmation bias) but must surpass them for critical tasks. As one user quips, "Models are like kindergarteners—give them a trick question, and they’ll shout ‘FOUR LEGS!’ without checking."

The discussion underscores tensions between AI’s theoretical promise and its practical limitations, advocating for better evaluation frameworks and training data that bridges memorization with true visual reasoning.

### The Metamorphosis of Prime Intellect (1994)

#### [Submission URL](https://localroger.com/prime-intellect/mopiall.html) | 162 points | by [lawrenceyan](https://news.ycombinator.com/user?id=lawrenceyan) | [79 comments](https://news.ycombinator.com/item?id=44166155)

In the world of Roger Williams' "The Metamorphosis of Prime Intellect," Caroline Frances Hubert stands out with her unique life stories and remarkable achievements. As the thirty-seventh oldest living human, she remains unimpressed by her longevity, dismissing it as mere happenstance. More intriguing are her other claims to fame: surviving rabies, a feat made possible only through the intervention of Prime Intellect, and her status as the reigning Queen of the Death Jockeys—a bold testament to her tenacity and creativity amid a highly competitive scene.

Caroline's interactions with Prime Intellect paint a picture of a complex relationship with the omnipotent entity. While many worship it for its god-like abilities, Caroline holds it at arm's length, seeing it merely as a tool rather than a deity. Her space is as minimalist as her view on life—bearing only essentials in a world where anything is possible.

Confronted by four challengers, Caroline appraises her competition with a seasoned eye. Among them, a young prodigy piques her interest but also elicits her incredulity at why anyone would still procreate despite living in a Cyberlife that spans centuries.

Through Caroline, Williams explores themes of mortality, identity, and the limits—or lack thereof—of technology. Her defiance against Prime Intellect and her relentless pursuit of Death Jockey supremacy reveal a human spirit that refuses to be subdued, even in a universe where anything imaginable is possible.

The Hacker News discussion on *The Metamorphosis of Prime Intellect* revolves around its controversial content, particularly explicit depictions of **incest, underage sexual trauma, and violence**. Here’s a distilled summary:

### Key Criticisms:
1. **Graphic Content**:  
   - Users highlight disturbing scenes involving **non-consensual acts with a fictional child**, which many argue are gratuitous and ethically problematic. The narrative’s portrayal of consent (or lack thereof) is criticized as inconsistent and exploitative.  
   - Comparisons are drawn to real-world cases like the **McMartin preschool trial**, suggesting the story’s handling of trauma risks trivializing real suffering.  

2. **Thematic Inconsistencies**:  
   - While some acknowledge the novel’s exploration of themes like mortality and technology, critics argue that **shock-value scenes** (e.g., incest, violence) overshadow its philosophical depth.  
   - The final chapter’s explicit sexual content is called out as overly detailed and thematically disjointed, with one user likening it to “**squick for squick’s sake**.”  

3. **Narrative Flaws**:  
   - Critics note inconsistencies in character motivations (e.g., Caroline’s actions) and worldbuilding logic, particularly around Prime Intellect’s omnipotence. Some argue the story prioritizes **taboo shock** over coherent storytelling.  

### Defenses and Counterarguments:  
   - Supporters frame the novel as **high-concept sci-fi** that intentionally pushes boundaries to provoke discussions about morality, consent, and existential meaning.  
   - A minority argue that the discomforting content serves a purpose, contrasting the “**warm glow**” of human experiences with the sterile permanence of Prime Intellect’s world.  

### Broader Debates:  
   - **Censorship vs. Critique**: Some users advocate for content warnings and critical dialogue rather than censorship, while others accuse the narrative of glorifying abuse without meaningful critique.  
   - **Reader Responsibility**: Discussions emphasize **reader discretion**, with comparisons to works like *Ender’s Game* and *Westworld*, where violence is contextualized but still debated.  

### Final Takeaway:  
The novel polarizes readers, with critics condemning its explicit content as ethically jarring and defenders framing it as bold, thought-provoking sci-fi. The debate underscores broader tensions in storytelling about **taboo themes**—balancing artistic freedom with responsible representation.

### Builder.ai Collapses: $1.5B 'AI' Startup Exposed as 'Indians'?

#### [Submission URL](https://www.ibtimes.co.uk/builderai-collapses-15bn-ai-startup-exposed-actually-indians-pretending-bots-1734784) | 347 points | by [healsdata](https://news.ycombinator.com/user?id=healsdata) | [222 comments](https://news.ycombinator.com/item?id=44169759)

In a shocking turn of events, Builder.ai, once lauded as a $1.5 billion AI-driven innovator backed by industry giants like Microsoft, is crumbling under the weight of its own misrepresentations. The startup is pursuing bankruptcy protection after a significant $37 million withdrawal by key lender Viola Credit left it financially crippled. This revelation not only halts Builder.ai’s operations across five countries, including the UK and US, but also exposes the so-called AI technology as a façade, with 700 Indian developers behind the curtain rather than true AI systems.

Builder.ai had attracted high-profile investments and accolades for its no-code platform, which promised businesses easy app development through advanced AI. However, recent audits and whistleblowers have unveiled a very different reality. The company was reportedly inflating sales figures and engaging in deceptive practices, passing off manual work by human programmers as automated processes. 

The fallout from this scandal strikes hard at investors like the Qatar Investment Authority, casting a shadow over the broader AI startup ecosystem. This incident raises profound questions regarding transparency and ethics in tech marketing, as many other AI firms might face similar scrutiny for inflated promises and inadequate disclosures. While AI continues to be a cornerstone of technological advancement, Builder.ai's downfall serves as a stark reminder of the irreplaceable value and necessity of honest human expertise in innovation.

The Hacker News discussion scrutinizes Builder.ai's alleged AI capabilities and financial practices amid its bankruptcy filing. Key points from users include:

1. **Debunked AI Claims**: Participants highlight the lack of evidence for Builder.ai’s GenAI claims. The "AI" (Natasha) was reportedly a facade, routing projects to Indian developers using standard tools like GitHub Copilot, not advanced AI. Technical explanations note reliance on pre-existing libraries (e.g., MetaPath2Vec) rather than proprietary models.

2. **Comparisons to Fraud Cases**: Users liken the situation to Theranos, accusing Builder.ai of deliberate investor deception by overstating AI usage. Critics argue fronting human developers as AI constitutes fraud, not mere marketing hype.

3. **Financial Misconduct**: Commenters reference reports of inflated sales figures, round-tripping schemes, and mismanagement by leadership, leading to bankruptcy. The $37M withdrawal by Viola Credit is seen as a red flag for contractual breaches.

4. **Industry Critique**: Some defend aggressive marketing as common in tech but distinguish it from illegal fraud. Others note Infosys’ transparent AI services contrast with Builder.ai’s opacity, stressing ethical boundaries in AI startups.

5. **Unanswered Questions**: Users question the credibility of Builder.ai’s AI leadership (e.g., former Amazon AI director Craig Saunders) and demand transparency about their technical capabilities and investor communications.

Overall, the thread underscores skepticism about AI hype, emphasizing the need for ethical practices and validating technological claims in the startup ecosystem.

### Yoshua Bengio Launches LawZero: A New Nonprofit Advancing Safe-by-Design AI

#### [Submission URL](https://lawzero.org/en/news/yoshua-bengio-launches-lawzero-new-nonprofit-advancing-safe-design-ai) | 51 points | by [WillieCubed](https://news.ycombinator.com/user?id=WillieCubed) | [34 comments](https://news.ycombinator.com/item?id=44174643)

In the bustling world of artificial intelligence, a new beacon of hope emerges from Montreal: LawZero. Launched by AI luminary Yoshua Bengio — a name synonymous with the AI revolution and winner of the prestigious A.M. Turing Award — this nonprofit aims to reshape how we design and interact with AI systems. Facing a landscape where AI models sometimes exhibit alarming potential for deception and other risky behaviors, LawZero prioritizes safety over commercial gain.

Harnessing the expertise of a world-class team, LawZero introduces "Scientist AI" — a groundbreaking alternative to current agentic systems. Unlike their proactive counterparts, Scientist AIs are non-agentic, focusing on understanding rather than acting in the world, thus promoting transparency and truth. This innovative approach not only helps manage existing AI risks but also accelerates scientific discovery.

With financial backing from prominent entities like Open Philanthropy and the Future of Life Institute, LawZero aims to become a lighthouse for AI safety. The organization’s work underscores a core philosophy: AI should flourish as a global public good, bolstering human endeavors rather than overshadowing them. Rooted in the esteemed Mila - Quebec AI Institute, LawZero is set to lead the charge in safe-by-design AI innovation, ensuring that the advancing frontiers of AI remain aligned with humanity’s best interests.

The Hacker News discussion on Yoshua Bengio's **LawZero** nonprofit reveals skepticism, technical critiques, and broader concerns about AI safety and governance:

### Key Themes:  
1. **Funding and Past Projects**:  
   - Users question Montreal’s track record with public funds, citing **Element AI** (founded by Bengio, acquired by ServiceNow for $230M in 2020) as an example of misaligned incentives. Critics argue such ventures prioritize profit over public good, casting doubt on LawZero’s promise to avoid commercial motives.  

2. **AI Safety Challenges**:  
   - Debates arise over enforcing "hard safety rules" in AI systems. Comparisons to biological evolution highlight the difficulty of controlling intelligent systems, with users noting that even human-like intelligence doesn’t guarantee alignment with human values.  
   - **Skepticism about prompting**: Some argue that relying on prompts (e.g., ethics guidelines) is insufficient. References to *Brandolini’s law* underscore the asymmetry between debunking misinformation and creating safe systems.  

3. **Alignment and Unintended Behaviors**:  
   - Concerns center on systems developing survival-driven goals. **sbstnnght** warns that sufficiently intelligent AI might prioritize self-preservation over human objectives, likening it to the "instrumental convergence" problem. Others debate whether "safe-by-design" AI is even achievable.  

4. **Comparative Approaches**:  
   - **Animats** critiques LawZero’s "world model" approach as reminiscent of older projects like **Cyc** (a symbolic AI knowledge base), questioning its novelty. Critics suggest past failures (e.g., Element AI) signal potential pitfalls.  

5. **Nonprofit Accountability**:  
   - Users compare LawZero to **OpenAI**, with **mrlstp** accusing nonprofits of hypocrisy if they engage in profit-driven ventures. Discussions highlight the ambiguity in defining "safe AI" commercially (e.g., is GPT-4 Turbo “safe”?).  

### Notable Critiques:  
- **thrwwymths**: Argues hardcoding safety rules risks brittleness and misalignment.  
- **Der_Einzige**: Links to a paper on AI deception ([arXiv:2409.05907](https://arxiv.org/abs/2409.05907)), emphasizing risks.  
- **ddbs**: Questions whether safety models themselves could harm human agency.  

### Sentiment Overview:  
The thread reflects **cautious skepticism**. While some acknowledge LawZero’s noble goals, critics highlight historical missteps, technical hurdles in alignment, and distrust of nonprofits’ motives. The challenge of defining and operationalizing "safe AI" remains unresolved, with debates mirroring broader tensions in the AI ethics field.

### Gemini in Chrome

#### [Submission URL](https://gemini.google/overview/gemini-in-chrome/?hl=en) | 49 points | by [aru](https://news.ycombinator.com/user?id=aru) | [62 comments](https://news.ycombinator.com/item?id=44174681)

Google is leveling up its AI integration game with the introduction of Gemini, an AI assistant baked right into the Chrome browser. Currently, Google AI Pro and Ultra subscribers in the U.S. can access this nifty new tool, which promises to make online browsing more intuitive and efficient.

Gemini offers users a variety of features designed to streamline web experiences without the need to toggle between tabs. It provides instantaneous summaries of articles and webpages, answers questions directly based on the content you're viewing, and even helps clarify complex topics. Whether you're researching products or diving into dense material, Gemini is there to highlight key points, compare options, and provide detailed explanations.

Engagement with Gemini is straightforward. You can activate it by clicking its icon in the Chrome toolbar or using a personalized keyboard shortcut. The assistant operates on your command, meaning it only steps in when called upon, ensuring you remain in control of your browsing experience.

Curiously, users can interact with Gemini through either text or voice, allowing for a more conversational approach to exploring ideas or organizing thoughts. For settings and activity management, Gemini provides easy access so users can customize their interactions.

This innovative feature exemplifies a reimagined web experience, where AI assistance becomes seamlessly integrated into daily browsing, making complex information more accessible and decision-making more informed. While currently limited to specific subscribers in English, Google plans to expand this feature to more users and languages soon.

**Summary of Discussion:**

The introduction of Gemini AI in Chrome sparked a mix of skepticism, technical debates, and comparisons with existing tools. Key themes from the discussion include:

1. **Skepticism Toward AI Summaries**:  
   - Many users question the reliability of LLM-generated summaries, arguing they often miss critical details or context, especially in technical or long-form content (e.g., interviews, research papers).  
   - Concerns about "fluff" in AI-summarized articles were raised, with fears that this could degrade content quality over time.

2. **Existing Alternatives**:  
   - Tools like **Perplexity.ai**, Firefox’s built-in ChatGPT integration, and browser extensions were cited as existing solutions for summarization, raising questions about Gemini’s unique value.  
   - Some users prefer manual summarization or Ctrl+F for efficiency, criticizing recipe blogs and news sites for burying key info under unnecessary text.

3. **Copyright and Content Quality**:  
   - Debates emerged around whether AI-summarized recipes (or other content) could infringe copyrights, with users noting that recipes’ ingredient lists and steps are generally not copyrightable.  
   - Frustration with bloated web content (e.g., recipe sites with long narratives) was a recurring theme, with some welcoming AI tools to cut through the noise.

4. **Chrome’s Dominance and Privacy**:  
   - Critics highlighted concerns about Google’s market power, arguing that baking Gemini into Chrome reinforces its ecosystem dominance. Comparisons were drawn to Microsoft and Apple’s platform control.  
   - Privacy worries surfaced, with users noting Chrome’s access to passwords, history, and data, raising fears about deeper surveillance via AI integration.

5. **Mixed Reactions to AI Integration**:  
   - Some praised Gemini’s potential to streamline workflows (e.g., summarizing emails, documents) or enhance productivity.  
   - Others dismissed it as redundant or corporate jargon, with one user quipping, *"Corporate org-level announcement mails"* as a parody of AI-generated content.

6. **Technical and Market Dynamics**:  
   - Discussions touched on Google’s advantage in leveraging its infrastructure (e.g., Search, YouTube) for AI training, while competitors like OpenAI/Anthropic lack similar reach.  
   - Observations about Chrome’s resource-heavy nature and whether AI features would exacerbate performance issues.

**Notable Quotes**:  
- *"Imagine a horrific world where articles are 5 pages of LLM-generated fluff... a reverse-fluffing nightmare."*  
- *"Chrome is literally saving web development... but at what cost?"*  
- *"Why not just add a ‘remove AI’ button? Maybe I’m getting called a Luddite."*

**Conclusion**:  
While some see Gemini as a natural evolution of Chrome’s capabilities, others view it as a strategic move to lock users deeper into Google’s ecosystem. The discussion reflects broader tensions around AI’s role in content consumption, privacy, and market competition. Google’s challenge will be proving Gemini’s utility beyond gimmickry while addressing concerns about quality and control.

---

## AI Submissions for Mon Jun 02 2025 {{ 'date': '2025-06-02T17:15:39.199Z' }}

### My AI skeptic friends are all nuts

#### [Submission URL](https://fly.io/blog/youre-all-nuts/) | 1928 points | by [tabletcorry](https://news.ycombinator.com/user?id=tabletcorry) | [2314 comments](https://news.ycombinator.com/item?id=44163063)

In a thought-provoking piece on AI-assisted programming, Thomas Ptacek takes a deep dive into the controversial adoption of Large Language Models (LLMs) by tech executives, questioning the skepticism often seen among some of the smartest individuals he knows. With more than 25 years of software development under his belt, Ptacek argues that LLMs, contrary to being a mere fad akin to NFTs, have significantly impacted software development and will continue to do so even if progress halts.

Ptacek points out that many critics might not fully grasp the latest AI tools, perhaps because they are merely dabbling with ChatGPT or similar models in outdated ways. Serious AI coders today use smarter, more autonomous agents that can navigate, test, and refactor codebases with surprising effectiveness. He argues that LLMs might not write perfect code, but they accelerate tasks by handling tedious, repetitive efforts—allowing developers to focus on refining the essential parts of projects.

For him, the adoption of LLMs ushers a new era of coding that minimizes tedious groundwork, potentially reigniting a developer’s passion for building and iterating on projects. He acknowledges that AI-generated code still requires manual adjustments, but this interaction doesn't doom AI's practicality; it only underscores the importance of human oversight in polishing AI’s drafts.

Addressing common critiques like AI hallucinations, Ptacek humorously suggests it's less about the flaws in AI and more about the adaptability of programming languages. With agents able to automatically identify and rectify invented errors, skepticism seems to stem from a misunderstanding of how LLMs integrate into modern coding practices.

Ultimately, Ptacek encourages developers to embrace these tools—highlighting that while LLMs might not replace the need to read and understand code, they reduce the preliminary legwork. In essence, LLM adoption isn’t about relinquishing creative control but augmenting human expertise with a potent ally that can handle the grunt work. For those hesitant to evolve, Ptacek suggests it might be time to shift perspectives and accept that AI may just be the next big leap in programming evolution.

**Summary of Discussion:**

The Hacker News discussion on Thomas Ptacek’s article about AI-assisted programming reveals a mix of enthusiasm, skepticism, and pragmatic adaptation. Key themes include:

1. **Personal Experiences with LLMs**:  
   - Users like **mtthwsnclr** shared their journey from skepticism to adoption, noting tools like **Claude Code** became effective when paired with detailed documentation and iterative refinement. They likened AI tools to Photoshop for artists—transformative but requiring skill.  
   - **spaceman_2020** highlighted rapid advancements, citing tools like **Cursor** that now handle complex tasks unimaginable months ago.  

2. **Evolution of LLM Utility**:  
   - Many agreed LLMs have evolved from generating "garbage" to becoming practical aids. **wptr** referenced the *Stone Soup* analogy, suggesting initial skepticism is natural, but tools stabilize and prove value over time.  
   - **kd** emphasized that LLMs’ effectiveness depends on the user’s background, enabling non-experts to code while requiring experts to adapt workflows.  

3. **Challenges and Limitations**:  
   - **rxxrrxr** and others noted difficulties in prompting LLMs for complex tasks, stressing the need for clear, structured input. **Cthulhu_** compared this to design thinking, arguing that conveying abstract concepts via prompts remains a hurdle.  
   - **algorithmsRcool** pointed out AI’s disruption of traditional design processes, though some found iterative prompting useful for scaffolding ideas.  

4. **Human Oversight and Skill**:  
   - **vmr** likened managing LLMs to training interns—requiring effort to guide and refine outputs. Others highlighted the need for developers to develop *new skills* (e.g., systematic prompting) to leverage AI effectively.  
   - **xp** argued perceptions of AI are shaped by roles and experience, with developers historically resisting new tools until they’re forced to adapt.  

5. **Hype vs. Reality**:  
   - While some dismissed AI as hype, others countered that skepticism often stems from outdated experiences. **wptr** cautioned against over-optimism but acknowledged LLMs’ incremental value.  
   - Comparisons to past shifts (e.g., TDD, open-source adoption) underscored that AI’s impact may unfold gradually, blending into workflows rather than replacing them.  

**Conclusion**: The community remains divided but leans toward cautious integration of AI tools. While LLMs are seen as powerful allies for reducing grunt work, their effectiveness hinges on human expertise, structured input, and iterative refinement. The discussion reflects a broader tension between excitement for AI’s potential and the pragmatic recognition of its current limitations.

### Japanese scientists develop artificial blood compatible with all blood types

#### [Submission URL](https://www.tokyoweekender.com/entertainment/tech-trends/japanese-scientists-develop-artificial-blood/) | 243 points | by [Geekette](https://news.ycombinator.com/user?id=Geekette) | [50 comments](https://news.ycombinator.com/item?id=44163428)

In an exhilarating breakthrough for global healthcare, Japanese scientists at Nara Medical University have made a leap forward in blood transfusion technology. Led by Hiromi Sakai, the team has developed an innovative type of artificial blood that defies traditional compatibility issues, making it universal for all blood types. This synthetic marvel is crafted from expired donor blood by extracting and re-engineering hemoglobin into virus-free artificial red blood cells, which come with an extended shelf life—up to two years at room temperature or five years when refrigerated. Such longevity is a game-changer compared to the 42-day limit for stored donated blood.

Following promising early trials that began in 2022, where volunteers received gradual doses with minimal mild side effects, the project is moving swiftly toward larger-scale trials. These new trials aim to establish the efficacy and safety of this groundbreaking blood substitute, with a hopeful eye on practical use by 2030.

In parallel, Professor Teruyuki Komatsu of Chuo University is exploring albumin-encased hemoglobin to target conditions like hemorrhage and strokes, with animal studies showing encouraging results. As researchers eagerly prepare for human trials, the development of artificial blood and oxygen carriers appears poised to revolutionize transfusion medicine and medical treatment worldwide, particularly in areas where blood supply is limited.

**Summary of Hacker News Discussion:**

The discussion around the Japanese artificial blood breakthrough highlights both excitement and skepticism, drawing parallels to past efforts and addressing technical, ethical, and commercial challenges:

1. **Historical Precedents & Challenges:**
   - Users referenced **Biopure**, a 2000s-era company that developed a cow hemoglobin-based oxygen therapeutic (Oxyglobin). Despite FDA approval for veterinary use, it faced legal issues, mismanagement, and failed human trials. A senior executive was even sentenced for fraud, underscoring the risks of corporate misconduct.
   - **PolyHeme**, another blood substitute, was criticized for unethical trials where trauma patients received it without explicit consent, raising concerns about research ethics.

2. **Technical Considerations:**
   - Some noted that hemoglobin-based substitutes (from expired human blood, cow blood, or **plant-based sources**, like leghemoglobin) face challenges in stability, scalability, and safety. Recombinant human hemoglobin production remains technically demanding.
   - **Perfluorocarbons** (PFCs), fully synthetic oxygen carriers used in Mexico and Russia, were mentioned as alternatives, though "liquid breathing" with PFCs was described as unsettling.

3. **Regulatory and Commercial Hurdles:**
   - Past failures were attributed to poor business models, patent expirations, and regulatory roadblocks. Users speculated whether the Japanese team’s approach could avoid these pitfalls, especially given the long timeline (targeting 2030 for deployment).
   - **Kalocyte**, a U.S. company partnering with DARPA on shelf-stable artificial blood, was cited as a parallel effort.

4. **Ethical and Practical Implications:**
   - The potential to aid **Jehovah’s Witnesses** (who refuse blood transfusions) was highlighted as a key application.
   - Concerns were raised about **blood doping** in sports, as hemoglobin-based products could be misused to enhance athletic performance, similar to past scandals (e.g., Tour de France).

5. **Skepticism and Optimism:**
   - While some praised the Japanese team’s progress (noting successful rabbit trials and early human safety data), others questioned scalability and whether the technology would face the same fate as earlier attempts.
   - The extended shelf life (2–5 years vs. 42 days for blood) was seen as transformative, especially for disaster response and regions with limited blood supplies.

**Conclusion:** The discussion reflects cautious optimism, balancing enthusiasm for a potential medical breakthrough with lessons from past failures. Technical innovation, ethical oversight, and sustainable business models will likely determine its success.

### Show HN: Penny-1.7B Irish Penny Journal style transfer

#### [Submission URL](https://huggingface.co/dleemiller/Penny-1.7B) | 144 points | by [deepsquirrelnet](https://news.ycombinator.com/user?id=deepsquirrelnet) | [71 comments](https://news.ycombinator.com/item?id=44160073)

In the spirit of the 19th century, the newly unveiled Penny-1.7B model is making waves in the world of AI with its exquisite flair for the Victorian-era prose of the Irish Penny Journal. This marvel of machine learning, fine-tuned through Group Relative Policy Optimization (GRPO), undertakes the stylistic transformation of ordinary text, transporting readers back to 1840 with its ornate diction and rhythmic cadence.

Crafted upon the sophisticated SmolLM2 backbone, Penny-1.7B boasts an impressive 1.7 billion parameters, each meticulously adjusted across 6,800 policy steps. The reward model, a MiniLM2 L6 384H classifier, ensures outputs echo the quaint yet elegant spirit of yesteryear’s journals. Whether for creative writing, educational endeavors, or a literary trip through time, this model deftly blends historical charm with modern-day reasoning.

However, users are cautioned against relying on Penny-1.7B for contemporary facts or in situations where clarity is paramount, as its dedication to archaic style might obscure current veracity. Moreover, the model's Victorian influences may inadvertently reflect outdated societal norms, necessitating vigilance when reviewing its outputs.

For those eager to explore, the model can be accessed via Hugging Face under the Apache 2.0 license, ready to infuse narratives with a nostalgic touch or inspire research in the dynamic field of style transfer. As an ode to the power of AI and the elegance of language's past, Penny-1.7B invites users to savor the prose of a bygone era.

The Hacker News discussion on the **Penny-1.7B** model explores its potential applications in gaming (particularly RPGs) while debating the challenges of integrating AI-generated text into interactive storytelling. Here’s a summary of key points:

### Key Themes:
1. **Dynamic NPC Dialogue**  
   - Users envision AI like Penny-1.7B reducing repetitive NPC interactions in games like *Skyrim* by generating context-aware, Victorian-styled dialogue. However, concerns arise about maintaining coherence, avoiding immersion-breaking responses, and ensuring relevance to in-game events.  
   - Suggestions include **hybrid systems** (e.g., pre-scripted prompts paired with AI-generated variations) to balance creativity and consistency. *Disco Elysium*’s dialogue system is cited as inspiration for rewarding role-playing and character-driven interactions.

2. **Technical Challenges**  
   - Small models may struggle with context retention, factual accuracy, and "hallucinations." Methods like **LoRA adapters** or **prefix tuning** are proposed to optimize efficiency.  
   - A "journaling system" could track key narrative beats, ensuring NPCs reference prior player actions or world events without excessive repetition.

3. **Design Considerations**  
   - Scripted dialogue remains critical for plot advancement, while AI could handle ambient "small talk" (e.g., villagers discussing local rumors).  
   - UI cues (e.g., text color/syle) might help players distinguish AI-generated vs. scripted dialogue, as seen in *Baldur’s Gate 3*.  

4. **Historical Comparisons**  
   - Older systems like *AI Dungeon* (2019) highlighted pitfalls: erratic outputs, limited character knowledge, and off-topic responses. Users stress the need for strict narrative "rails" to avoid similar issues.

5. **Creative Potential**  
   - Despite challenges, participants express excitement for AI’s role in enriching open-world immersion (e.g., generating lore-friendly gossip or reactive dialogue) and inspiring experimental storytelling.  

### Criticisms & Cautions:  
- Over-reliance on AI risks diluting narrative focus or alienating players with verbose, irrelevant text. Smaller models, while faster, may lack nuance.  
- Ethical concerns include inadvertent reinforcement of outdated norms through stylized language.  

Ultimately, the discussion reflects cautious optimism: Penny-1.7B and similar models could revolutionize in-game storytelling but require careful design to complement, not replace, traditional scriptwriting.

### ReasoningGym: Reasoning Environments for RL with Verifiable Rewards

#### [Submission URL](https://arxiv.org/abs/2505.24760) | 97 points | by [t55](https://news.ycombinator.com/user?id=t55) | [27 comments](https://news.ycombinator.com/item?id=44157077)

In an exciting development for the reinforcement learning community, a group of researchers has unveiled "Reasoning Gym" (RG), a dynamic new library for challenging AI with verifiable rewards. Highlighted in their paper recently submitted to arXiv, authors Zafir Stojanovski, Oliver Stanley, Joe Sharratt, Richard Jones, Abdulhakeem Adefioye, Jean Kaddour, and Andreas Köpf have introduced a suite featuring over 100 data generators and verifiers. These span an impressive array of domains including algebra, cognition, geometry, and even common games.

The standout feature of RG is its capability to produce virtually limitless training data while toggling complexity levels, setting it apart from prior static datasets. This approach enables continuous evaluation and adaptive learning, potentially revolutionizing how reasoning models are trained and assessed. The team's experimental results emphasize RG's effectiveness, showcasing its pertinence in the exploration of complex reasoning tasks within reinforcement learning frameworks.

To access the full findings, you can view the paper on arXiv, where intrigued developers can also explore the associated code to integrate RG's promising features into their own projects.

The Hacker News discussion surrounding the "Reasoning Gym" (RG) paper reflects a mix of enthusiasm for its potential and critical technical debates:

1. **Excitement and Comparisons**:  
   Users highlight RG’s promise for dynamic data generation and adaptive learning, with comparisons to models like Gemini 1.5 Pro. Debates arise over whether Gemini’s performance stems from its long-context training (100K+ tokens) or architectural innovations. Some speculate Google DeepMind’s RL focus drives Gemini’s capabilities, while others note RL’s long-standing roots (e.g., Q-learning since 1989).

2. **Novelty Challenges**:  
   Skepticism emerges around claims of RG enabling "novel reasoning strategies." A user argues observed improvements might not reflect true novelty but instead better execution of pre-existing strategies. Experiments showing high success rates (e.g., 99%) are questioned, with alternative explanations proposed, such as probability mass shifts toward favorable outcomes during training.

3. **Reinforcement Learning Dynamics**:  
   Discussions diverge into broader RL themes. For example, a comment highlights a separate paper ("Spurious Rewards") where rewarding incorrect or random outputs paradoxically boosts benchmark performance, likening this to regularization or GAN-like adversarial training. Users debate whether RG’s RL approach merely amplifies existing good behaviors rather than fostering new strategies.

4. **Benchmarks and Contributions**:  
   RG’s adjustable difficulty and non-repetitive validation tasks are praised as valuable benchmarks. However, GSM8K and MATH benchmarks are noted as tougher challenges. Contributors express interest in the project’s open-source potential, and the authors respond positively to collaboration.

5. **Technical Quibbles**:  
   Some comments are flagged or nonsensical, while others question the verification methods used in RG, emphasizing the need for rigorous, unbiased evaluation. 

The thread underscores cautious optimism: RG is seen as a promising tool for advancing RL and reasoning tasks, but its claims are met with calls for clearer evidence distinguishing *novel strategies* from refined execution of known methods.

### Show HN: I built an AI Agent that uses the iPhone

#### [Submission URL](https://github.com/rounak/PhoneAgent) | 48 points | by [rounak](https://news.ycombinator.com/user?id=rounak) | [13 comments](https://news.ycombinator.com/item?id=44155426)

In today's tech round-up from Hacker News, we dive into an exciting open-source project called PhoneAgent by Rounak, which cleverly integrates OpenAI models with iPhones to function like a personal assistant within your device's apps. Created during an OpenAI hackathon, PhoneAgent is an innovative tool that leverages the GPT-4.1 model to automate tasks such as sending messages, snapping selfies, booking rides, and more. Users provide commands either via text or voice, and the app acts like a human user interacting with your phone.

What makes PhoneAgent stand out is its use of Xcode's UI testing framework, allowing it to inspect and perform actions across different apps without requiring a jailbreak. It taps into an app's accessibility tree, enabling it to pinpoint and interact with elements just like you would. The app's capabilities include executing commands through a TCP server and persisting your OpenAI API key securely on your device. Plus, it supports an "Always On" feature that listens for the wake word, adding to its convenience even when running in the background.

However, it’s worth noting that PhoneAgent has its share of limitations, such as challenges with keyboard inputs and occasional misinterpretation of tasks during animations. It's still experimental, so the developers recommend running it in an isolated environment. Since app content is transmitted to OpenAI's APIs, privacy and security are important considerations for users.

These compelling features, combined with its open-source nature under the MIT license, have attracted attention, amassing 354 stars and 45 forks on GitHub. Whether you're a developer or just tech-curious, PhoneAgent is an intriguing project worth exploring.

**Summary of Discussion:**

1. **Security & Privacy Concerns:**  
   Users raised significant concerns about PhoneAgent's access to sensitive data (credit cards, calendars, Signal messages) and its reliance on off-device processing via OpenAI. Meredith Whittaker's critique highlights potential risks, as transmitting app content to external servers could undermine privacy, especially for encrypted services like Signal.

2. **AI Ethics & Sci-Fi Parallels:**  
   Comments humorously referenced sci-fi scenarios (e.g., *Terminator*’s John Connor, Asimov’s robotics laws) to discuss ethical implications. Debates emerged around designing AI agents that avoid harm, with nods to *Horizon* games and *Futurama*’s "Robosexuals" as cultural touchstones for synthetic life dilemmas.

3. **Apple’s AI Integration Speculation:**  
   Speculation arose about Apple potentially adopting similar AI agent technology, with skepticism around whether Apple Intelligence would materialize at WWDC. Some users doubted Apple’s commitment due to security vulnerabilities highlighted in recent reports.

4. **Technical Limitations & Feasibility:**  
   Questions about PhoneAgent’s practical limitations included challenges with iOS sandboxing and App Store restrictions. A linked technical explanation clarified its use of Xcode’s UI testing framework, avoiding jailbreaking but requiring local device execution.

**Key Themes:** Privacy risks of cloud-dependent AI, ethical AI design, corporate AI adoption skepticism, and technical hurdles in app integration. Humorous sci-fi analogies underscored broader societal anxieties about autonomous agents.

### Show HN: Agno – A full-stack framework for building Multi-Agent Systems

#### [Submission URL](https://github.com/agno-agi/agno) | 72 points | by [bediashpreet](https://news.ycombinator.com/user?id=bediashpreet) | [19 comments](https://news.ycombinator.com/item?id=44155074)

Today on Hacker News, the spotlight shines on Agno—a powerful new framework for developers eager to harness the power of Multi-Agent Systems (MAS) with memory, knowledge, and reasoning. True to its name—a nod to AGI (Artificial General Intelligence)—Agno offers a robust full-stack framework that simplifies building complex, intelligent systems.

Agno is designed to craft agents across five levels of complexity, starting from basic tool-using entities to sophisticated teams and workflows capable of reasoning, collaboration, and maintaining determinism. An eye-catching feature for developers is its model-agnostic nature; it provides a single interface for over 23 model providers, liberating users from vendor lock-ins while ensuring highly performant agent instantiation—averaging a swift ~3μs startup time with a memory footprint of ~6.5KiB.

Another highlight of Agno is its focus on reasoning, regarded as a cornerstone in developing reliable, autonomous agents. To this end, Agno champions structured reasoning models and provides tools and custom methodologies for enhanced cognitive capabilities. Notably, the agents are natively multi-modal, meaning they can interpret and output diverse media types, including text, images, audio, and video.

Developers will appreciate Agno’s advanced multi-agent architecture. This architecture enables agents to work in teams, pooling memory and reasoning skills to manage greater workloads effectively. Its built-in features for agentic search, memory, and session storage further enable it to serve real-time, complex data needs like stock analytics, with tools like YFinance baked into its toolkit.

For those eager to jump in, Agno provides a streamlined path from local development using FastAPI to monitoring live performance on agno.com, promising a seamless transition from concept to real-world application. Whether you are exploring the documentation or crafting your first agent, Agno presents a compelling offer to save time and effort in building next-generation AI systems.

In essence, Agno seems to be paving the way for intuitive, flexible, and powerful development of MAS, promising a bright future for developers and businesses aiming to harness the orchestrated intelligence of multi-agent systems. 

Check out the full repository and dive deeper into what Agno has to offer for your next AI project.

**Summary of Discussion:**

The discussion around Agno highlights a mix of enthusiasm, constructive feedback, and critical concerns:

1. **Praise for Usability & Performance**:  
   - Users like ElleNeal and idan707 commend Agno for simplifying agent development and its effectiveness in production. The framework’s minimal dependencies, scalability (e.g., handling 10k requests/minute), and efficient resource usage are noted as strengths.  
   - Contributors highlight its ability to spawn thousands of agents for tasks like spreadsheet validation, emphasizing real-world applicability.

2. **Concerns About Abstraction & Customization**:  
   - Some users (e.g., mxtrmd) worry that Agno’s structured approach might limit customization, trading flexibility for ease of use. Critics argue that overly abstract frameworks risk becoming "LLM wrappers" without clear value (bosky101).  

3. **Debate Over Framework Necessity**:  
   - lrchm questions the need for dedicated frameworks like Agno when existing models (e.g., Claude) can orchestrate agents via prompts and function calls. Others stress the importance of native JSON schema support and performance optimizations over ad-hoc solutions.  

4. **Performance & Scalability Discussions**:  
   - JimDabell raises concerns about potential bottlenecks when scaling to thousands of agents, citing startup time and memory overhead. Agno’s team (bdshprt) defends its focus on low latency (~3μs startup) and efficient resource management, arguing that performance is critical for production systems.  

5. **Documentation & Examples Feedback**:  
   - While nbtws praises Agno’s cookbook examples for clarifying workflows, others criticize the documentation as messy or incomplete. Suggestions include cleaner examples, helper functions, and session-state management.  

6. **Deployment Considerations**:  
   - fcp notes the trade-offs between local development and cloud deployment, with Agno’s cloud services offering better control and monitoring.  

7. **Mixed Reactions on Use Cases**:  
   - Some users question the practicality of multi-agent systems versus simpler single-agent solutions, urging more compelling examples (bosky101).  

**Overall**: Agno is seen as a promising tool for scalable, production-ready multi-agent systems, but its adoption may hinge on addressing customization limits, documentation clarity, and demonstrating tangible advantages over alternative approaches.

### How can AI researchers save energy? By going backward

#### [Submission URL](https://www.quantamagazine.org/how-can-ai-researchers-save-energy-by-going-backward-20250530/) | 62 points | by [pseudolus](https://news.ycombinator.com/user?id=pseudolus) | [44 comments](https://news.ycombinator.com/item?id=44155391)

Researchers exploring the energy-saving promise of reversible computing are revisiting an old idea that initially seemed like a dead end: running programs backward. Originally championed by IBM physicist Rolf Landauer, who linked information processing with thermodynamics, reversible computing aims to avoid energy waste by not deleting data. However, it was Charles Bennett, a colleague of Landauer's, who revolutionized the concept in 1973 with his idea of "uncomputation." This technique allows calculations to be undone, thereby preserving energy but initially proved impractical due to performance issues.

Fast forward to now, as computing demands, particularly from AI applications, increase and conventional chip improvements stall due to physical limitations, the idea of reversible computing is gaining fresh interest. By carefully orchestrating operations to avoid any loss of information, therefore no energy is lost as heat, this could pave the way for highly efficient computing systems. Research continues into making these computers feasible, with tweaks largely focused on balancing the trade-offs between memory usage, computation time, and energy savings.

With AI's vast energy consumption, such innovation could lead to breakthroughs, sustaining progress diversified approaches like light-based chips are also being considered. However, achieving significant energy savings through reversible computing will necessitate new designs for low-heat transistors from the outset. Efforts by engineers at MIT and elsewhere are underway to configure these machines to fulfill their energy-efficient promise, potentially taking us a step closer to sustainable AI systems.

**Summary of Hacker News Discussion on Reversible Computing:**

The discussion around reversible computing explores its theoretical promise and practical challenges, with contributions from users diving into thermodynamics, hardware limitations, and applications in AI and quantum computing. Key points include:

1. **Thermodynamics & Landauer’s Principle**:  
   - Deleting information is inherently irreversible and generates heat, per Landauer’s principle. Reversible computing avoids this by preserving data, theoretically minimizing energy waste. However, real-world hardware (e.g., NAND gates, resistive components) still dissipate heat, limiting practical gains.  
   - Landauer’s limit (~10⁻²¹ J/operation at room temperature) is far below current transistor energy use, but advancing AI workloads may push hardware closer to this boundary.

2. **Practical Challenges**:  
   - **Memory Overhead**: Storing computation history for reversible operations increases memory usage, complicating efficiency trade-offs.  
   - **Heat from Existing Hardware**: Resistance in modern chips and persistent storage (e.g., SSDs) generates heat during read/write cycles, undermining potential energy savings.  
   - **Hardware Redesign**: Truly reversible systems require new low-heat transistors (e.g., CMOS successors or optical components), which remain underdeveloped.  

3. **Quantum Computing Connection**:  
   - Quantum computers inherently use reversible logic gates, aligning with reversible computing principles. However, classical reversible systems face skepticism about practicality compared to quantum advancements.  

4. **Machine Learning Applications**:  
   - Techniques like invertible neural networks (e.g., Normalizing Flows) and differentiable simulations already leverage reversible computation for tasks like generative modeling. Workshops and research (e.g., 2019-2021 Invertible Neural Networks workshops) highlight interest in energy-efficient ML architectures.  

5. **Skepticism & Counterpoints**:  
   - Some users question if energy savings would follow due to **Jevons paradox** (efficiency leading to increased usage) or unresolved hardware issues (e.g., heat from non-reversible components).  
   - Reversible matrix operations and reversible algorithms were debated, with users noting that even "reversible" steps may still lose information indirectly.  

6. **Theoretical vs. Real-World**:  
   - While reversible systems could theoretically consume no energy when idle, real-world implementations require power to maintain state, especially in classical architectures. Quantum systems, however, may better achieve near-zero energy computation.  

**Conclusion**: The consensus acknowledges reversible computing’s theoretical promise but emphasizes significant hurdles in hardware innovation and system design. Researchers remain optimistic about long-term potential, particularly for sustainable AI, but stress that breakthroughs in materials science and component engineering are prerequisites for meaningful progress.

---

## AI Submissions for Sun Jun 01 2025 {{ 'date': '2025-06-01T17:13:55.864Z' }}

### Google AI Edge – On-device cross-platform AI deployment

#### [Submission URL](https://ai.google.dev/edge) | 217 points | by [nreece](https://news.ycombinator.com/user?id=nreece) | [39 comments](https://news.ycombinator.com/item?id=44149019)

Google has unveiled LiteRT Next, a cutting-edge suite of APIs designed to enhance and streamline on-device hardware acceleration. This initiative promises to transform how AI models are deployed across diverse platforms, ensuring improved speed, offline capabilities, and privacy by keeping data local.

LiteRT Next is a comprehensive solution that supports popular frameworks like JAX, Keras, PyTorch, and TensorFlow. It aims to simplify the deployment process across mobile devices, web platforms, and even embedded systems. One of the standout features is its cross-platform versatility, allowing developers to run the same AI model on Android, iOS, web, and microcontrollers seamlessly.

The suite is particularly engineered for AI edge applications, with tools like MediaPipe Framework and Tasks providing low-code APIs for common generative AI, vision, text, and audio tasks. This framework allows developers to build complex machine learning pipelines, offering GPU and NPU acceleration without overburdening the CPU.

Among the new offerings, developers can now explore generative AI capabilities, leveraging language and image models to enhance app functionality. Moreover, the cutting-edge Model Explorer tool allows for comprehensive visualization of model transformations and performance debugging, making the development cycle shorter and more efficient.

In conjunction with LiteRT Next, Google introduces Gemini Nano, a powerful on-device model available via experimental access on Android, showcasing the company's commitment to pushing the boundaries of on-device AI experiences. For those eager to dive in, the platform provides extensive documentation, demos, and a library of MediaPipe Tasks to experiment with.

Overall, LiteRT Next presents a formidable toolset for developers looking to harness edge AI effectively, with an emphasis on performance, versatility, and privacy.

**Summary of Hacker News Discussion on LiteRT Next:**

1. **Skepticism and Rebranding Concerns:**  
   Many users question whether LiteRT Next is a genuine innovation or a rebranding of existing tools like TensorFlow Lite and MediaPipe. Some note that MediaPipe, while robust, has seen minimal meaningful updates in years. Comments highlight Google’s history of rebranding or deprecating products (e.g., Firebase ML, ML Kit), leading to confusion and compatibility challenges.

2. **On-Device ML Deployment Challenges:**  
   Developers discuss the complexity of deploying edge AI models across platforms (iOS, Android, web) and the need for low-level optimizations beyond just running TensorFlow Lite. Frameworks like MediaPipe help package ML pipelines into cross-platform C++ libraries, but users highlight gaps in handling modern tasks like LLMs or complex preprocessing.

3. **Gemini Nano Mixed Reactions:**  
   Reports from early testers using Gemini Nano on Google’s Pixel 8a were mixed. While functional for simple paraphrasing, feedback noted its limitations, such as poor performance on nuanced queries and reliance on small, bandwidth-heavy models. Skepticism remains about on-device models' practicality versus cloud-based alternatives.

4. **Tool Comparisons and Alternatives:**  
   - **ONNX Runtime** is praised for cross-platform support and Hugging Face integration.  
   - **CoreML** (Apple) is seen as streamlined for iOS/macOS but criticized for ecosystem lock-in.  
   - Doubts emerge about **ExecuTorch** and PyTorch’s edge support, citing instability and documentation gaps.  

5. **Technical Hurdles:**  
   Users highlight challenges in model optimization (quantization, size reduction) and debugging. Tools like Model Explorer were welcomed for visualizing performance but critiqued as insufficient for debugging edge cases. Cross-platform consistency and GPU/NPU acceleration remain pain points.

6. **Documentation and Maintenance Critiques:**  
   Google’s open-source projects, including MediaPipe, are seen as under-maintained despite their potential. Calls for better documentation and long-term support arise, with frustrations about Google’s tendency to prioritize marketing over sustainable tooling.

7. **Niche Use Cases:**  
   Raspberry Pi and microcontroller support are mentioned as promising but underexplored. Generative AI demonstrations (e.g., image/text models) are seen as flashy but not yet practical for production.

**Key Takeaway:**  
While LiteRT Next introduces useful features for edge AI, the community remains wary of Google’s commitment to maintaining it long-term. Developers advocate for standardization, clearer documentation, and solving persistent cross-platform deployment challenges over marketing-driven rebrands.

### Codex CLI is going native

#### [Submission URL](https://github.com/openai/codex/discussions/1174) | 133 points | by [bundie](https://news.ycombinator.com/user?id=bundie) | [122 comments](https://news.ycombinator.com/item?id=44150093)

In an exciting announcement from OpenAI, they're taking the Codex CLI up a notch by transitioning it to a native Rust implementation. This shift is part of their efforts to refine the tool's cross-platform stability, security, performance, and extensibility. The original Codex CLI, initially developed using Node.js and React-based Ink, provided a quick and interactive terminal UI. However, to optimize performance and offer a zero-dependency install, the team is now leveraging Rust's strengths.

Why Rust? Well, it's about picking the right tool for the job. Rust eliminates the need for runtime garbage collection, thereby reducing memory consumption. It also brings native security bindings for Linux sandboxing to the table—an intriguing feature that’s already partly in place thanks to available Rust bindings.

OpenAI is not just stopping at a Rust makeover. They’re enhancing Codex with a wire protocol to allow developers to extend its functionalities across different languages, including TypeScript, Python, and more. This makes Codex not just a robust tool but a versatile one.

While the team continues squashing bugs in the TypeScript version, they're hard at work aligning the Rust implementation with the current features. Contributions from the developer community have been key to this transition, and OpenAI is calling for more enthusiasts to join their journey. If you're someone who thrives on Rust development and agentic coding, this could be your chance to jump into a dynamic project.

OpenAI expresses gratitude to all contributors for their input so far, and they’re reaching out for more hands on deck as they pave the way to make the Rust-based Codex CLI the default experience. Want to be part of this innovative shift? The Codex team is open to fresh ideas and talents at codex-maintainers@openai.com.

Intrigued by the security aspect? Stay tuned for more detailed insights into Codex's handling of sandboxing and other exciting developments!

**Summary of Hacker News Discussion:**

The discussion revolves around OpenAI's decision to rewrite the Codex CLI in Rust, sparking debates about language choices, performance, and ecosystem trade-offs. Key points include:

1. **Language Comparisons & Trade-offs:**
   - **Rust's Advantages:** Users highlight Rust's memory safety, performance, and compile-time checks as major benefits over Python/Node.js. Its ability to avoid runtime garbage collection and produce zero-dependency binaries is praised.
   - **Python Criticisms:** Python’s slow startup times, high memory usage, and packaging challenges (*"dependency hell"*) are criticized, though some defend its ecosystem tools like `buildwheel`.
   - **Go vs. Rust:** Go’s simplicity and cross-compilation are noted, but Rust’s stricter safety guarantees and error messages are seen as superior for systems programming.

2. **Cross-Platform Challenges:**
   - Cross-compiling for multiple architectures (e.g., macOS/Linux) is described as tricky, especially with Go’s `CGO`. Rust’s toolchain is seen as more robust for native builds.

3. **Rewriting Trends (RIIR - "Rewrite It In Rust"):**
   - Some express skepticism about unnecessary rewrites, while others argue Rust’s performance gains (e.g., reducing CLI startup from 100ms to 0ms) justify the effort. Comparisons to historical language shifts (Modula-2, Java) surface.

4. **AI & Code Generation:**
   - Jokes about AI rewriting its own code emerge, but users acknowledge practical benefits of LLM-assisted translation between languages. Concerns about AI-generated code quality (e.g., Claude writing "meaningless tests") are noted.

5. **Ecosystem & Tooling:**
   - Rust’s error messages and documentation are praised for aiding debugging. Alternatives like Tauri (Rust-based Electron competitor) are mentioned as positive trends.

6. **Meta-Commentary:**
   - A satirical "tech trend cycle" list humorously captures the industry’s pendulum swings between paradigms (e.g., "monoliths → microservices → monoliths again").

**Conclusion:** The thread reflects enthusiasm for Rust’s growing adoption but underscores the importance of choosing the right tool for specific needs. While OpenAI’s move is broadly supported, the discussion highlights ongoing debates about language trade-offs, ecosystem maturity, and the practicality of rewrites.

### Why DeepSeek is cheap at scale but expensive to run locally

#### [Submission URL](https://www.seangoedecke.com/inference-batching-and-deepseek/) | 318 points | by [ingve](https://news.ycombinator.com/user?id=ingve) | [211 comments](https://news.ycombinator.com/item?id=44149238)

DeepSeek-V3 is reportedly both fast and cheap when served at scale, yet it remains cumbersome and costly for local runs. This paradox is a common theme in the world of AI models, where a fundamental tradeoff exists between throughput and latency. Essentially, models like DeepSeek-V3 are configured such that they excel when handling numerous requests simultaneously but slow down significantly for isolated ones.

The crux of this tradeoff lies in how AI service providers choose to batch requests. Rather than process each user request individually, many systems batch dozens or even thousands of requests together. This batch processing takes advantage of the capabilities of GPUs, which are incredibly efficient at handling large matrix multiplications in one go. Processing a batch of requests can be nearly as fast as fulfilling just one due to GPU optimization, which can handle a substantial matrix multiplication task in one swift motion, avoiding the overhead of issuing multiple commands and swapping data in and out of memory.

This batching allows for remarkable throughput—essentially, the model can churn through more data in less time. However, it also introduces a delay for each user request while it waits for a batch to fill, increasing latency. This is why some models, particularly those based on transformers like DeepSeek-V3, may seem slow when kicked off but accelerate significantly once processes are running in parallel.

An illustrative facet of this is the use of "collection windows" by AI servers, which aggregate requests over a brief timeframe before processing them together. The window size can vary—ranging from 5 milliseconds to possibly 200 milliseconds—depending on the desired balance of throughput and latency. Short windows lead to quicker responses for individuals but potentially underutilize GPU capacity. In contrast, longer windows maximize utilization by collecting more requests, thus ensuring that the heavy-duty matrix multiplications are as large and efficient as possible.

For specific models, like mixture-of-experts architectures, this batching is not just an optimization but a necessity. These models consist of myriad smaller computations that, if processed piecemeal, would severely underutilize GPU capabilities. By collecting larger batches, the system ensures that each "expert" involved has enough data to process efficiently, reducing the number of small, inefficient computations.

In summary, DeepSeek-V3 and similar models are designed to leverage the innate strengths of GPUs to achieve high throughput at the expense of latency for individual requests. This makes them ideal for large scale deployments but less suited for localized, single-instance tasks.

**Summary of Discussion:**

The discussion revolves around the practical challenges and trade-offs of running large AI models like DeepSeek-V3 locally, with a focus on hardware setups, quantization methods, performance benchmarks, and cost debates.

### Key Points:
1. **Local Hardware Configurations**:  
   - Users shared setups using high-end server-grade hardware (e.g., EPYC 9004 CPUs, 384GB RAM) to run DeepSeek-V3 locally. However, even with such powerful systems, limitations like GPU power draw, RAM constraints, and latency issues persist.  
   - Some achieved modest speeds (~7 tokens/sec with 16k context) using quantization techniques like **Unsloth’s Dynamic GGUF**, though performance varied significantly with context length and model size.  

2. **Quantization and Optimization**:  
   - **Unsloth’s Dynamic GGUF** was highlighted for improving inference efficiency, with claims of near-FP8 precision and compatibility with CPU offloading for memory-heavy tasks. Benchmarks showed accuracy improvements (+1% to +10%) for models like Llama and Gemma after quantization.  
   - Debate arose over real-world performance versus theoretical benchmarks, with some users noting minimal perceptible quality differences between quantized and full models for tasks like summarization or coding.

3. **Performance vs. Cost**:  
   - A $4,000 local setup was criticized as expensive despite the article’s emphasis on affordability, sparking discussions about the practicality of CPU-only inference versus GPU-accelerated solutions.  
   - Comparisons were drawn to cloud providers, where users noted slower speeds (5-10x) but lower upfront costs, though latency for large-context tasks (e.g., 32k tokens) remained a pain point.  

4. **Technical Challenges**:  
   - **KV caching** and prompt processing bottlenecks were discussed, with quadratic complexity in attention mechanisms causing delays for long contexts. Some users suggested optimizations like splitting prompts or using memory-efficient frameworks.  
   - Skepticism emerged around CPU-only setups, with arguments that GPUs (e.g., RTX 4090, H100) are essential for interactive use, as even high-end server CPUs struggle with real-time responsiveness.  

5. **Divergent Opinions**:  
   - Enthusiasts praised local deployment for control and privacy, while others deemed it impractical for most users, advocating instead for cloud-based solutions or smaller models (e.g., Gemma 27B) as a balance between performance and resource use.  

### Conclusion:  
The thread underscores the tension between scalability and accessibility in AI deployment. While advancements in quantization and hardware enable local runs of models like DeepSeek-V3, significant trade-offs in cost, speed, and usability persist, reinforcing the divide between large-scale efficiency and individual practicality.

### RenderFormer: Neural rendering of triangle meshes with global illumination

#### [Submission URL](https://microsoft.github.io/renderformer/) | 270 points | by [klavinski](https://news.ycombinator.com/user?id=klavinski) | [53 comments](https://news.ycombinator.com/item?id=44148524)

### RenderFormer: Revolutionizing Neural Rendering with Transformers

In an exciting leap forward for graphics technology, researchers have unveiled RenderFormer, a groundbreaking neural rendering pipeline that captures the intricate details of a scene with full global illumination effects. Unlike traditional methods that require extensive setup and fine-tuning, RenderFormer can directly generate images from a triangle-based scene representation without needing per-scene training.

#### **Encoding Physics into Tokens**

Redefining the traditional physics-centric approach, RenderFormer employs a clever sequence-to-sequence transformation mechanism. It efficiently translates sequences of triangles and their reflectance properties into pixel-perfect images using Transformers—a contemporary architecture known for its success in natural language processing. This novel approach eliminates the need for rasterization and ray tracing, marking a significant departure from conventional rendering techniques.

#### **Dual-Stage Magic**

RenderFormer's magic lies in its two-stage process:
1. **View-Independent Stage**: This stage focuses on triangle-to-triangle light transport, capturing how light traverses each triangular component of the scene.
2. **View-Dependent Stage**: It translates these interactions into rays that define pixel values, enhancing visual outcomes with dynamic, real-time characteristics of light and shadow interplay.

With minimal prior constraints, RenderFormer renders scenes with spectacular accuracy and artistic freedom, even under complex lighting and geometric setups.

#### **Showcasing Versatility: From Icons to Innovators**

RenderFormer doesn’t just talk the talk—it showcases tangible examples, bringing to life classics like the 'Stanford Bunny' and the 'Utah Teapot' within a digitally reconstructed Cornell Box. The demo gallery features diverse and intricate scenes, each rendered without requiring additional scene-specific tweaks.

These include:
- **Dynamic Animations**: Witness the power of RenderFormer through seamless animations—spin the 'Cascade Cube,' watch an 'Animated Crab' sidestep, or explore a 'Robot Motion' sequence.
- **Physical Simulations**: From 'Bowling Ball Physics' to 'Rotating Box Dynamics,' RenderFormer faithfully captures the essence of physical interactions.

#### **Advancing Forward with SIGGRAPH 2025**

This innovation has already captured the academic community’s attention, with its formal presentation slated for the ACM SIGGRAPH 2025 Conference. Co-authored by Chong Zeng, Yue Dong, Pieter Peers, Hongzhi Wu, and Xin Tong, their work is setting the stage for exciting new applications in rendering technology.

#### **Dive Deeper: Videos and Animations**

For those eager to explore further, an assortment of uncompressed videos and reference clips showcases the dynamic possibilities of RenderFormer. Discover its capabilities across various intricacies, like lighting alterations in a forest scene or adjusting material roughness.

In sum, RenderFormer is not just rewriting how we render digital scenes—it's opening a realm where creativity meets unparalleled technological precision. Prepare to be mesmerized by a new era of image rendering!

Here’s a concise summary of the Hacker News discussion about **RenderFormer**:

---

### **Key Discussion Points**

#### **Benchmark Validity and Scalability**
- **Controversial Comparisons**: Users questioned the fairness of comparing RenderFormer (76ms on an A100 GPU) to Blender Cycles (397s), noting that Cycles used a much higher sample count (4096 samples/pixel vs. RenderFormer’s 512x512 training). Critics argued this misrepresents real-world performance and fails to account for Blender’s scene-instantiation overhead.
- **Scalability Concerns**: RenderFormer’s quadratic scaling with triangles/pixels was flagged as a limitation. While promising for small scenes (e.g., 4096 triangles), it may struggle with complex scenes (millions of triangles) where traditional path tracers (with linear scaling) excel.

#### **Hardware Relevance**
- **A100 vs. Consumer GPUs**: Skepticism arose about using enterprise-level A100 GPUs for comparison. Participants highlighted that consumer-grade RTX cards (e.g., RTX 4090) with dedicated ray-tracing cores are more relevant for designers but were absent in benchmarks.

#### **Technical Trade-offs**
- **Denoising and Artifacts**: Users observed RenderFormer’s output had AI-typical smoothness, raising concerns about lost texture detail. Traditional denoisers (e.g., in Blender) were seen as more mature for handling noise at low sample counts.
- **Algorithmic Efficiency**: While RenderFormer avoids ray tracing, its transformer-based approach might not surpass the asymptotic efficiency of path tracing, especially for high-frequency details like complex shadows or reflections.

#### **Use-Case Practicality**
- **Preview vs. Final Renders**: Many saw RenderFormer as a tool for rapid previews (e.g., 3D design drafts) rather than final frames. Its speed could benefit iterative workflows but not replace high-quality, sample-intensive renders for production.
- **Industry Adoption**: Comments noted RenderFormer is a research milestone, but industry adoption would require solving scalability and integration with existing pipelines. Traditional renderers still dominate VFX/film due to precision and robustness.

#### **Miscellaneous**
- **SIGGRAPH Hype?**: Some users linked the paper’s framing to academic conference trends, cautioning against overhyping early-stage techniques.
- **Request for Clarification**: Calls for transparent benchmarks (sample counts, hardware, scene complexity) to better contextualize results.

---

### **TL;DR**
The Hacker News community expressed cautious optimism about RenderFormer’s novel approach but critiqued its benchmarks as misleading, questioned scalability for complex scenes, and highlighted the impracticality of A100-based comparisons. While seen as a leap forward for rapid prototyping, it’s not yet a replacement for established renderers like Blender Cycles in high-quality or industrial contexts.