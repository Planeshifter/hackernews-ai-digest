import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Thu Jan 04 2024 {{ 'date': '2024-01-04T17:10:00.504Z' }}

### AI and satellite imagery reveals expanding footprint of human activity at sea

#### [Submission URL](https://globalfishingwatch.org/press-release/new-research-harnesses-ai-and-satellite-imagery-to-reveal-the-expanding-footprint-of-human-activity-at-sea/) | 272 points | by [geox](https://news.ycombinator.com/user?id=geox) | [176 comments](https://news.ycombinator.com/item?id=38866256)

A new study from Global Fishing Watch has revealed that 75% of the world's industrial fishing vessels are hidden from public view. The groundbreaking study used machine learning and satellite imagery to create the first-ever global map of large vessel traffic and offshore infrastructure. It identified a significant amount of activity that was previously "dark" to public monitoring systems. This study sheds light on the extensive and intensifying human activity at sea, providing valuable insights for protecting and managing natural resources. It also highlights the potential of this technology to tackle climate change and improve ocean management and transparency.

The discussion on this submission revolves around various aspects of tracking fishing vessels and the implications of the study's findings. 
One commenter points out that different countries have different laws regarding broadcasting positions of fishing vessels, with Norway requiring it and the UK not. They argue that some systems wouldn't broadcast positions at all, and that machine learning-based systems could help analysts spot patterns in fishing spaces. However, concerns about privacy are also raised, as sharing vessel positions raises issues about enforcement and resources for fighting illegal fishing. 
Another commenter brings up the use of satellites and naval radar detectors to track ships, highlighting the military capabilities in place. However, they also note that military intelligence capabilities are most likely kept separate from regulatory agencies. 
The discussion also touches on the existing regulations for fishing vessels, with one commenter mentioning that EU regulations require vessels over 15 meters to have AIS, but enforcement and effectiveness vary across jurisdictions. The issue of privacy is brought up again, with concerns about the lack of coverage for smaller fishing vessels and the potential for abuse of tracking systems. 
There is a side discussion about the use of ADS-B (Automatic Dependent Surveillance-Broadcast) technology in aviation and the privacy concerns associated with it. Some commenters discuss the limitations and potential abuse of tracking technologies. 
Other comments suggest using ML-assisted tools for detection and analysis, highlight the commercial opportunities in the field, and discuss the options for tracking planes and drones. 

Overall, the discussion revolves around the trade-off between privacy and transparency, the challenges of enforcement and regulating fishing vessels, and the potential for technological solutions in monitoring and managing human activity at sea.

### GPUI 2 is now in production â€“ Zed

#### [Submission URL](https://zed.dev/blog/gpui-2-on-preview) | 48 points | by [DAlperin](https://news.ycombinator.com/user?id=DAlperin) | [16 comments](https://news.ycombinator.com/item?id=38871732)

Zed, a popular text editor, has announced the production release of GPUI 2, its new UI framework. This update brings significant improvements to Zed's user experience and speed. The team behind Zed has rewritten the UI framework from scratch, incorporating lessons learned over the past two years of working with UI in Rust. The new GPUI 2 offers better ergonomics for contributors, making it more enjoyable to work on and allowing for faster shipping. Additionally, this update lays a solid foundation for upcoming enhancements, including multi-platform support and animation. To upgrade to GPUI 2, the team cloned most of their crates and added a new version suffixed with 2, finally deleting the old version and crates. The revamped version of Zed has been extensively tested by the team for the past 2-3 weeks and is now available for users to try out as well. The majority of the improvements are internal, but one popular user request has been fulfilled: the ability to customize and scale the UI font. The team plans to release more preview versions as they work through remaining issues, and they do not anticipate any major stable releases until the preview is ready to be promoted. Upon reaching stable status, Zed will also be open sourced. The team encourages users to provide feedback and assistance in identifying any regressions. The upcoming open source release will allow for even deeper collaboration and contributions from the community. Exciting times lie ahead for Zed as it continues to evolve and grow. Interested users can download and try out Zed today on macOS.

The discussion on Hacker News revolves around Zed's new UI framework, GPUI 2, and its release for macOS. One commenter raises concerns about the exclusive focus on macOS, pointing out that it excludes 80% of computer users and limits the reach of the software. Another commenter counters this argument, stating that many landmark software applications like PowerPoint, Photoshop, and Excel were initially released for Mac and that it is a valid strategy to focus on one platform initially. 
Some users express their positive experiences with Zed, praising its helpful features and smooth collaboration capabilities. However, there are also some concerns raised, such as blurry fonts on Linux and Windows and the potential limitations of GPUI in terms of windowing and GPU usage. 
There is a discussion about the roadmap for Zed, with one commenter mentioning the plan to introduce paid features and multiplayer functionality, while another expresses frustration with a previous version of Zed that stopped working and hopes for a fix in the future. 
Additionally, there are suggestions from users for learning resources related to Rust and discussions around the need for support on other operating systems. The Zed team responds to these comments by mentioning that they are working on multi-platform support and that they have identified and addressed issues on GitHub.

### AI and satellite imagery used to create clearest map of human activity at sea

#### [Submission URL](https://www.theverge.com/2024/1/3/24018797/ocean-maps-ai-satellite-imagery-radar-fishing-vessels-offshore-energy-wind-oil) | 61 points | by [nathan_phoenix](https://news.ycombinator.com/user?id=nathan_phoenix) | [14 comments](https://news.ycombinator.com/item?id=38865449)

Researchers have used deep learning and satellite imagery to create the first global map of vessel traffic and offshore infrastructure, revealing previously unknown industrial activity at sea. The maps, published in the journal Nature, indicated that 75% of the world's industrial fishing vessels and up to 30% of transport and energy vessels are not publicly tracked. The researchers, led by Google-backed nonprofit Global Fishing Watch, stated that these blind spots could hinder global conservation efforts and called for a more accurate picture to protect the world's oceans and fisheries. The study used 2,000 terabytes of imagery from the European Space Agency's Sentinel-1 satellite constellation and three deep-learning models to classify vessels and estimate their size. The data revealed an explosion of offshore energy development, with wind turbines outnumbering oil structures by the end of 2020.

The discussion on this submission includes several comments discussing different aspects of the research and the technology used.

- One user shares a link to the article and mentions the significant percentages of industrial fishing and transport energy vessel activity that are missing from publicly tracked systems.
- Another user shares a link to a comment that provides additional information about how hunting vessels that do not use AIS (Automatic Identification System) can still be tracked.
- A user suggests exploring Sentinel-1 satellite images directly on certain websites.
- Several comments provide links to resources where users can access Sentinel-1 satellite imagery and explore it further. They also mention the availability of planetary computer platforms.
- One user flags the submission.
- A comment praises the advances in AI and satellite imagery in capturing human activity at sea, while another user raises concerns about privacy and the potential for misuse of such information.
- The topic of classifying and detecting human activity in satellite imagery is further discussed, mentioning legal systems, lawsuits, and the challenges of gathering information for the international community.
- A user mentions the trustworthiness of AI and its ability to classify images accurately.
- The discussion shifts toward the topic of machine learning models and their performance, with one user sharing the F1 score and accuracy achieved by their model in a classification task.
- A response to that comment mentions the trade-off between resolution and accuracy in sensing and tracking vessel activity.

Overall, the discussion covers a range of topics, including the technology used, the potential implications of the research, and the challenges of tracking vessel activity at sea.

---

## AI Submissions for Sun Dec 31 2023 {{ 'date': '2023-12-31T17:09:22.197Z' }}

### Counterfactual Regret Minimisation or How I won any money in Poker?

#### [Submission URL](https://rnikhil.com/2023/12/31/ai-cfr-solver-poker.html) | 69 points | by [whoami_nr](https://news.ycombinator.com/user?id=whoami_nr) | [17 comments](https://news.ycombinator.com/item?id=38823240)

Today's top story on Hacker News is about solvers in Poker and how they work. The author, who used to play Poker professionally, explains that solvers are tools used to analyze and optimize gameplay. Unlike games like Chess and Go, Poker involves imperfect information as players don't know their opponents' cards. Solving a Poker game means finding a Nash Equilibrium strategy, also known as a GTO (Game Theory Optimal) strategy, where neither player can improve their outcome by changing their strategy. However, while GTO ensures unexploitability, it may not maximize winnings. The best response strategy is one that maximally exploits an opponent's non-equilibrium play. Solvers determine the GTO play by using EV (Expected Value)-maximizing algorithms, where each agent represents a player and the goal is to maximize earnings. By iteratively playing against each other's strategies, the agents find the Nash equilibrium point. Regret is a measure of how well one could have done with an alternative action compared to their overall strategy. Minimizing regret is central to all GTO algorithms, with the most well-known being CFR (Counterfactual Regret Minimization). The author compares studying Poker to CFR, as they would play hands, review them with a coach, and aim to play more optimally in the future to reduce regret. The author also mentions the multi-armed bandit problem, which is a classic reinforcement learning problem and relates to Poker played in the partial information setting. The article concludes by discussing exploration vs. exploitation in decision making, using the example of picking travel destinations or restaurants.

The discussion on the submission includes various points and perspectives:

- One user mentions that CFR (Counterfactual Regret Minimization) solvers are slow and require a lot of RAM, but there is a smaller and abstract version of the game called Setback that doesn't need a perfect Nash equilibrium solution.
- Another user adds that commercially available solvers like CFR are not very strong and mentions Pluribus as an example of a precomputed solver that reduced state space and mapped hand combinations.
- A user shares their experience with using solvers like Monker Solver for multiple sports and states that they are fairly fast.
- The original poster mentions CFR in the context of transportation and logistics problems and gives other examples as well.
- A user comments that making money playing online poker by following computer-optimized strategies can be risky, but insights from optimized play are appreciated.
- The author responds, agreeing with the user's comment and adding that making money in online poker is more difficult now with increased scrutiny, and mentions GGPoker's detection system.
- Some users discuss the existence of detection systems for cheating in online poker and the challenges associated with implementing solvers.
- Another user comments on the theory of minimizing expected regret and its relevance to risk diversification and multiple perturbation groups in the modeling world.
- A user expresses surprise at the strength of open-source poker AIs and wonders if implementing CFR methods in poker is generally difficult.
- The author clarifies that there are CFR implementations and provides examples like Pluribus, but implementing them can be challenging due to replicating research. They also mention the cost of training and the possibility of bypassing KYC checks for multiple accounts.
- The user mentions training tools like GTO+ and PLOMastermind and a 2013 poker solver created by Oleg Ostroumov.
- Another user expresses surprise at the difference between the chess world and the poker world in terms of strong open-source engines and suggests that the attractiveness of making money might be a factor.
- The author explains that closed-source software in poker makes sense due to capital deployment and changing play patterns.
- A user adds that there have been recent examples in the chess world of closed-source commercial engines and open-source options.
  
Overall, the discussion covers various topics such as the limitations and challenges of poker solvers, commercial availability, cheating detection systems, and comparisons between the chess and poker worlds.

### Emacs-copilot: Large language model code completion for Emacs

#### [Submission URL](https://github.com/jart/emacs-copilot) | 361 points | by [yla92](https://news.ycombinator.com/user?id=yla92) | [147 comments](https://news.ycombinator.com/item?id=38822164)

Introducing Emacs Copilot, a large language model code completion tool for Emacs. This tool allows you to do pair programming with a locally running language model that generates code completions within Emacs buffers. With just ~100 lines of LISP, Emacs can generate code completions similar to what Github Copilot and VSCode are famous for, but with superior quality and freedom. You can interrupt the language model at any time by hitting C-g, and it remembers your local editing history on a file-by-file basis. The tool is language-agnostic and determines the programming language based on the file extension. Check out the repository for more details and instructions on how to get started!

The discussion on the submission revolves around various aspects of Emacs Copilot and the use of large language models (LLMs) in programming. 
- One comment mentions the increased productivity and quality of code achieved with LLMs compared to regular coding. They highlight the benefits of using LLMs for prototyping and debugging, and emphasize the importance of understanding and copying the generated code.
- Another comment praises the capabilities of Emacs in integrating LLMs, particularly mentioning the impressive Lisp support and the potential for further improvement.
- A user expresses their enthusiasm for the integration of LLMs in Emacs, comparing it to the beauty and longevity of the Duomo in Florence and suggesting that it will greatly enhance programming productivity.
- The topic of AI-based tools is raised, with one comment mentioning the potential acceleration of AI-based development tools in the future, and another user suggesting the usefulness of such tools for serious AI development.
- The efficiency of coding in modern front-end development frameworks like React is briefly discussed.
- A user reflects on GPT-generated commit messages and the challenges they present, such as difficulty in reviewing pull requests and the responsibility associated with using LLMs.
- There is a mention of the benefits of literate programming and its ability to provide a comprehensive and maintainable environment for coding.
- The discussion touches on specific technical aspects, including using Tramp for remote LLM integration, the ability to connect to LLMs over a LAN network, and the possibility of leveraging Llamafiles and the Llamafile API for more powerful AI development.
- A user shares their interest in self-hosted LLMs and expresses gratitude for the promotion of projects like Llamafiles. They also mention the distributed nature of StableDiffusion models and the challenges associated with loading and switching between different models.
- The value of local AI deployment and the security concerns associated with downloading executable files are discussed.

Overall, the discussion covers a range of topics, from the benefits and challenges of LLMs in programming to the technical aspects of integrating LLMs in Emacs and concerns about AI-based tools and their security implications.

---

## AI Submissions for Sat Dec 30 2023 {{ 'date': '2023-12-30T17:09:43.459Z' }}

### Midihum: An ML-Based MIDI Humanizing Tool

#### [Submission URL](https://www.erichgrunewald.com/posts/introducing-midihum-an-ml-based-midi-humanizing-tool/) | 40 points | by [erwald](https://news.ycombinator.com/user?id=erwald) | [14 comments](https://news.ycombinator.com/item?id=38814424)

midihum is a command-line tool that uses machine learning to humanize MIDI compositions. It takes MIDI compositions as input and produces new compositions with adjusted velocity values for each note, resulting in a more natural and expressive sound. The tool uses gradient boosted trees, trained on over 2.6K piano performances, to make the adjustments. The creator of midihum has been working on the tool for the past five years and is pleased with its performance, particularly for solo piano works from the Baroque, Classical, and Romantic periods. The tool accurately predicts and captures the dynamics of the music, identifying peaks and valleys in intensity. 

One interesting feature of midihum is that it tends to produce more extreme velocity values than those performed by humans, giving the compositions a unique character. The tool can be easily used through the command line by running a simple command. The midihum model was trained on performances from the International Piano-e-Competition for young pianists. The tool is distributed with a GPLv3 license, allowing users to freely copy, distribute, and modify the software. Overall, midihum is a powerful tool for musicians and composers looking to enhance the naturalness and expressiveness of their digital compositions.

The discussion on the submission revolves around various aspects of the midihum tool and MIDI humanization. One commenter, "DrSiemer," mentions having trouble installing and finding a stable MIDI file to test the tool. Another commenter, "brdgrs," suggests checking the Internet Archive for existing MIDI files. "cmiller1" provides guidance on how to use the midihum tool by cleaning the repository, navigating to the midihum directory, and installing the dependencies. "-db" suggests that using a machine learning approach can significantly improve the process of capturing dynamics. Another commenter, "CrypticShift," shares their experience using computer-generated music and mentions that MIDI keyboards often lack sufficient responsiveness, causing issues with generating humanized sounds. They also note that MIDI humanization in digital audio workstations (DAWs) is popularized using Ableton's Groove Pool.
"BriggyDwiggs42" mentions that they haven't been able to figure out how to draw MIDI data points.
In response, "lncslls" states that their composition tool, Hookpad, supports importing MIDI data into a DAW. "rwld" agrees with this statement.
"steve1977" adds that Logic Pro calls this feature Humanize, while "helpfulContrib" expresses agreement.
"cmmkbrwn" suggests that the issue might be related to properly controlling the dynamics and velocity of the keyboard.
"dzm" remarks that Ableton's Groove Pool is an amazing feature, and making slight adjustments to rhythm can bring new life to a composition.
A final commenter, "plmnl," provides a hint that MIDI humanization can be done using a MIDI to USB connection.

Overall, the discussion explores different aspects of MIDI humanization, shares personal experiences with composition tools and MIDI keyboards, and provides tips for using the midihum tool.

### The Heart of a Language Server

#### [Submission URL](https://rust-analyzer.github.io//blog/2023/12/26/the-heart-of-a-language-server.html) | 75 points | by [thesuperbigfrog](https://news.ycombinator.com/user?id=thesuperbigfrog) | [11 comments](https://news.ycombinator.com/item?id=38820454)

In a recent blog post, "The Heart of a Language Server," the author dives into the intricacies of building a language server. They discuss how language servers, like those used for Kotlin, C#, and Rust, rely on understanding the current position of the cursor to provide features like go to definition, code completion, and more. The post explains that the first step in the process is to determine the node in the syntax tree that covers the cursor's offset. Once this is established, semantic analysis is needed to gather additional information. The challenge with semantic analysis is that it often involves multiple layers of intermediate representations that are only indirectly related to the syntax tree.

To bridge this gap, the author proposes a solution using the concept of source spans. By attaching source span information to semantic elements, a language server can find the appropriate semantic element for a given cursor position by iterating over all semantic elements and finding the one with the smallest span that contains the cursor. However, this approach has drawbacks, as it can be slow and can erase information about the underlying syntax trees. To address these drawbacks, the author introduces an iterative recursive analysis technique. In this approach, each semantic element is assigned a source_syntax method that returns the original syntax node from which it originated. This allows the language server to map syntax nodes to corresponding semantic elements precisely and lazily compute the mapping.

The post provides examples of different approaches to implementing the source_syntax method, including storing a reference to a syntax node, computing the syntax on-demand, or using a side table for mapping. The author notes that all three approaches are used in Rust Analyzer, the language server for Rust. In conclusion, the post highlights the importance of mapping syntax nodes to semantic elements and the challenges involved in maintaining this mapping. It emphasizes the need for a precise and efficient solution to power language server features effectively.

Discussion:
User "l-mrk" comments that in Rust, finding the parent file that includes a semantic file declaration generally requires the entire file to be parsed. However, source browsers typically parse source files to a certain depth. Another user, "jen20," confirms that this is a path-based structure in C#.
User "brbl" points out that the way Rust, Kotlin, and C# languages start the package declaration immediately impacts the placement of semantic models. They mention that IDE-friendly languages are designed with flexible semantics in mind, making analysis easier. They note that languages like C++, Lisp, and Rust are difficult to analyze compared to languages like Java, Kotlin, and Dart. They further mention that Rust has good support from IDEs, such as Rust Analyzer, which has been helpful in their experience with the language.
User "mtkld" adds that managing the people, master huge efforts require a level of knowledge and expertise in tools proven to be successful. They mention that IntelliJ Rust, the dedicated Rust IDE, and Rust Analyzer are tools that work great because they have dedicated teams working specifically on analyzing Rust and its consequences. They also note that understanding the build IDE is crucial, and it requires a substantial amount of effort.
User "brbl" gives a brief history of the Rust Analyzer project. They mention that the project started in 2017 alongside the existing LSP implementation RLS, which provided IDE support for Rust. They highlight that initially, people completely rejected the idea that Rust Analyzer could succeed and that IDEs worked well with Rust. However, the success of Rust and the efforts of individuals working to improve the tooling led to surprising progress with relatively little effort. They also note that the Rust Analyzer project was initially a hobby project in 2018 and later sponsored by businesses in 2020.
User "glblr-tst" adds that their experience with Lisp shows that despite the IDE experience lacking compared to other languages, the language has great interactivity and good analysis capabilities, especially with tools like SLIME.
User "mrws" comments that semantic information generated from syntax is placed in a linking mechanism and doesn't need to be searched again. They also mention that finding symbols generated by macros can be challenging, and modifying the compiler could make plugins impossible but still necessary.
User "di4na" agrees and adds that analyzers have a problem of growing fast, as compilers extract information. Even regenerating a part of a file based on partial input is not as straightforward due to the correct syntax. They mention that linking is a persistent topic of conversation and the steps taken for transformations need to be well-defined. They also note that it is difficult to query data dependent on transformations. They mention that using the existing part of the Rust compiler and replacing the Rust Analyzer is not as straightforward and requires ongoing maintenance. They point to C# Roslyn and TypeScript as examples of languages with built-in mindsets for this.
User "cycm" shares their interest in analyzing and transforming trees, mentioning that they have been reading and increasing their belief in XSLT for searching, tree walking, and analyzing transformations. Another user, "hds," recommends functional XSLT as a practical approach.

The discussion covers various aspects of language server development, including the challenges of semantic analysis, IDE friendliness, tooling support, and the trade-offs involved in analyzing and transforming different programming languages.

### How is AI impacting science?

#### [Submission URL](https://michaelnotebook.com/mc2023/index.html) | 87 points | by [occamschainsaw](https://news.ycombinator.com/user?id=occamschainsaw) | [108 comments](https://news.ycombinator.com/item?id=38811704)

At the Metascience 2023 Conference, Michael Nielsen from the Astera Institute delivered a talk on the biggest success of AI in science so far: the AlphaFold 2 system. This deep learning system has made significant advancements in predicting the 3-dimensional structure of proteins from their amino acid sequences. AlphaFold's success has sparked a revolution in molecular biology and raises important questions about validation, understanding, and the impact of AI on the progress of science as a whole. Nielsen treats AlphaFold as a prototype for how AI can be used across various scientific fields, encouraging metascientists to engage with this groundbreaking technology. By understanding the background of proteins and the challenges in determining their structures, we can grasp the significance of AlphaFold's achievements and its implications for scientific discovery.

The discussion about the submission mainly revolves around the topic of AI in science and coding. Some commenters highlight the difficulty of coding complex AI systems and the importance of clear communication in research papers. Others discuss the limitations and challenges of current AI systems, emphasizing the need for human involvement in verification and validation processes. There is also a mention of AI in the context of space missions and the potential risks associated with fully trusting AI-driven systems. Another point raised is the importance of researchers learning proper coding techniques and the disparities in salaries between scientists and programmers. Finally, there is a brief mention of breaking code with chatGPT and its potential for aiding in learning.

### Driverless cars immune from traffic tickets in California under current laws

#### [Submission URL](https://www.nbcnews.com/business/business-news/can-driverless-cars-get-tickets-california-law-rcna131538) | 95 points | by [ceejayoz](https://news.ycombinator.com/user?id=ceejayoz) | [112 comments](https://news.ycombinator.com/item?id=38815531)

The rapid expansion of self-driving cars in California has raised concerns about the need for new laws and regulations to govern the technology. There are ongoing safety concerns and uncertainties about law enforcement's ability to cite autonomous vehicles when they violate traffic laws. While driverless cars have been involved in incidents such as running red lights and blocking emergency responders, there is currently little that law enforcement can do to penalize these violations. Traffic tickets can only be issued if there is an actual driver in the car. The lack of accountability has led some to question whether it's a level playing field and whether fairness is a priority. California's current legal framework does not encompass driverless vehicles and new laws are needed to govern the technology adequately. In contrast to California, Texas and Arizona have adapted their transportation laws to allow the ticketing of driverless cars. Despite the limitations, autonomous vehicle makers claim their technology is getting better and that their cars are already safer than human drivers. However, with the technology still in its early stages and much testing happening on city streets alongside human drivers and pedestrians, there are concerns about the unknown risks and the lack of consent for these experiments.

The discussion on this submission revolves around the need for regulations and liability in the context of self-driving cars. Some users point out that in Texas, autonomous vehicles can be ticketed for traffic violations regardless of whether there is a person present in the vehicle, while California's legal framework does not cover driverless vehicles. Others argue that individual owners of self-driving cars should be responsible for any violations, while some suggest that open-source software could bypass regulatory controls. There is also discussion about the potential market for self-driving cars and the challenges of regulatory compliance. Overall, the conversation showcases differing opinions on the responsibility and accountability for self-driving cars.

### AI can find your location in photos

#### [Submission URL](https://www.npr.org/2023/12/19/1219984002/artificial-intelligence-can-find-your-location-in-photos-worrying-privacy-expert) | 24 points | by [talonx](https://news.ycombinator.com/user?id=talonx) | [14 comments](https://news.ycombinator.com/item?id=38813150)

Researchers at Stanford University have developed an AI tool called PIGEON (Predicting Image Geolocations) that can accurately guess the location where a photo was taken. The system, which was trained using images from Google Street View, can identify the correct country 95% of the time and pinpoint the location within about 25 miles. While the technology has potential applications in fields such as biology and cultural heritage, privacy experts are concerned that it could be used for government surveillance, corporate tracking, or stalking. The PIGEON algorithm recently outperformed a human expert in a geolocation contest.

The discussion around the submission consists of several different topics:
1. Some users are surprised that the PIGEON tool is able to accurately geolocate photos using limited resources. One user mentioned that a YouTuber attempted to replicate the results and found them to be surprisingly accurate, even locating a random photo taken of a replanted tree outside a window.
2. Another user brings up Google's location statement AI and wonders if the technology could be applied to photos that are shared online.
3. A user mentions that changing camera settings on Android devices can enable location metadata to be included in photos.
4. Another user shares a link to an article about Google's dumbing down of geolocation in photos.
5. One user challenges a user named Bard who claimed to have given hints about their location, specifically in Switzerland. The challenge suggests that Bard is being cryptic without providing any concrete information.
6. There is a brief mention of Rainbolt, though it is not clear what this refers to.
7. A discussion ensues about using the iPhone Photos app to add location metadata to photos. One user mentions a feature in the app that adds location metadata automatically.
8. Some users discuss the need for a more sophisticated search method to find landmarks in photos and mention an AI model called Visual Lookup.
9. One user mentions that Stanford graduate students have written a paper about the PIGEON tool but have not made the full model publicly available, which raises concerns about its potential implications.