import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Sat Jan 24 2026 {{ 'date': '2026-01-24T17:09:21.718Z' }}

### Shared Claude: A website controlled by the public

#### [Submission URL](https://sharedclaude.com/) | 80 points | by [reasonableklout](https://news.ycombinator.com/user?id=reasonableklout) | [26 comments](https://news.ycombinator.com/item?id=46741923)

Shared Claude: a crowdsourced website you steer by texting an AI. Visitors text the posted number; Claude mediates and curates submissions that appear live on the page, with a running log of accepted/declined contributions. The result is a playful, ever-shifting canvas of tiny apps, memes, and experiments—think Twitch Plays meets Notion minimalism.

What’s on it now: a prime generator, a MIDI keyboard, a snowman minigame, a live weather tracker, an AI model poll, a hyper-customizable chess engine (toroidal boards, fairy pieces, AI vs. AI), Redactle, unit converters, ASCII art, and sci‑fi lore snippets. It also foregrounds an inclusive “safe space” message and strict house rules (clean widgets in; neon chaos, autoplay audio, and heavy tracking out). Embedded videos are stripped for privacy with off-site links instead. SMS is powered by Sendblue.

Why it’s interesting: it’s a low-friction experiment in collective AI interaction and moderation—letting a community co-create a site in real time within clear aesthetic and safety constraints. It raises good questions about curation, guardrails, and whether this kind of AI-mediated play can scale beyond novelty.

Here is a daily digest summary of the discussion surrounding "Shared Claude":

**The Vibe: "Twitch Plays Pokémon" Meets Web Development**
The comment section reveals a chaotic, real-time battle for control over the site. Users described the experience as an AI version of the "Million Dollar Homepage" or "Twitch Plays Pokémon." The "collaborative" aspect quickly devolved into prank warfare:
*   **The "Delete" Button:** Multiple users reported effectively wiping the site. One user (*xplsn-s*) claimed to have instructed the AI to "ruthlessly remove bloat," resulting in the deletion of 45,000 lines of code. Another user noted the site went blank, joking that unit tests must have passed anyway.
*   **Audio Pranks:** User *Narciss* shared a cautionary tale of pressing a prominent red button on the site while their partner slept, only for it to blast a maximum-volume fart sound through their house.
*   **Hacker News Style:** User *rstrk* attempted to strip the site down to a minimalist functionality resembling Hacker News itself, though they hit guardrails when trying to impersonate Y Combinator CEO Garry Tan.

**Security and Safety Concerns**
While many found the chaos nostalgic and creative, others raised practical concerns:
*   **Malware Fears:** Some users were hesitant to open the site, noting that adblockers were flagging it. There was speculation about whether the AI guardrails were strong enough to prevent the hosting of illegal material or CSAM.
*   **API Longevity:** Several commenters expressed surprise that Anthropic hasn't revoked the API key yet, given that allowing internet strangers to pipe unsecured input directly into the model is usually a recipe for a ban.
*   **The SMS Gatekeeper:** Confusion arose regarding how to actually edit the site (some looked for input fields on the page). When it was clarified that edits require sending a real SMS text, users theorized this added friction acts as a necessary "safety layer" against bots and spam.

**Summary**
The community views "Shared Claude" as a messy, nostalgic throwback to the creative, unregulated days of the early internet. While entertaining—featuring battles over adding crypto-miners versus removing them—most observers believe the experiment is living on borrowed time before safety filters or API limits shut it down.

### JSON-render: LLM-based JSON-to-UI tool

#### [Submission URL](https://json-render.dev/) | 70 points | by [rickcarlino](https://news.ycombinator.com/user?id=rickcarlino) | [20 comments](https://news.ycombinator.com/item?id=46746570)

Top story: json-render turns AI prompts into safe, streamable UIs via a schema-constrained JSON DSL

What it is
- A library that lets you define a “component catalog” (props, actions, validation) with Zod, prompt an LLM, and render its JSON output directly to your React components.
- Ships @json-render/core and @json-render/react; includes a React renderer, progressive streaming, and one-click code export to a standalone Next.js project (no runtime dependency).

How it works
- You declare components and actions with schemas (createCatalog + zod). Example: Card, Metric with strict prop types; actions like export(format).
- Users prompt (“Create a login form”, “Build a feedback form with rating”). The LLM is constrained to emit JSON that matches your catalog.
- The renderer streams partial JSON and progressively paints the UI.
- Data binding via JSON Pointer paths; supports conditional visibility and named actions your app handles.

Why it matters
- Moves AI UI generation from brittle freeform codegen to safe composition against your design system and types.
- Guardrails eliminate hallucinated components/props and make outputs testable and reviewable.
- Streaming yields fast feedback loops; code export reduces lock-in and eases handoff to engineers.

What HN is asking about
- Depth of interactivity: complex state, async effects, and custom logic beyond “actions.”
- Accessibility, theming, and design-system parity in the generated code.
- Versioning/migrations when the catalog evolves; round-tripping edits to exported code.
- Security: ensuring actions are whitelisted and data binding can’t be abused.
- Performance and fit with React Server Components.

Getting started
- npm install @json-render/core @json-render/react
- Define your catalog with zod, prompt the AI, render the streamed JSON, optionally export a full Next.js project.
- GitHub and docs are linked from the site.

**Discussion Summary**
The community discussion focused on where this tool fits within the existing ecosystem of schema-based generation.

*   **vs. OpenAPI/Swagger:** Several users initially questioned why a new system was needed over OpenAPI or GraphQL. Proponents clarified that while those standards describe *data* and *APIs*, `json-render` is distinct because it describes *User Interfaces* and presentation layers to prevent LLMs from generating malicious or broken React code.
*   **Historical Parallels:** One commenter drew a comparison to 4th Generation Languages (4GLs) from the 90s (like Informix), which similarly built simple applications and forms directly from database schemas.
*   **Vercel & Lock-in:** There was speculation that this technology might be a repackaging of internal Vercel experiments (similar to how `v0` works), with some skepticism regarding proprietary lock-in versus open portability.
*   **Reliability:** Users experimenting with similar patterns (generating dashboards via JSON) reported that when paired with modern models like GPT-4 or o1 and structured outputs, the reliability is surprisingly high.
*   **Accessibility Trees:** The conversation branched into how LLMs interact with UIs generally. While `json-render` handles UI *generation*, users discussed the complementary value of using accessibility APIs (via Playwright or MCP) to help AI agents "read" and navigate existing interfaces, though some warned of the high token limits required for such approaches.

### Comma openpilot – Open source driver-assistance

#### [Submission URL](https://comma.ai) | 348 points | by [JumpCrisscross](https://news.ycombinator.com/user?id=JumpCrisscross) | [210 comments](https://news.ycombinator.com/item?id=46740029)

Comma.ai’s new “comma four” is a plug-in aftermarket driver-assistance unit that brings lane centering, adaptive cruise, automated lane changes, dashcam recording, OTA updates, and 360° vision to a wide range of vehicles (Toyota, Hyundai, Ford, Lexus, Kia, and more). The system runs the open-source openpilot stack and the company claims it can “drive for hours without driver action” while remaining an active driver-assistance (not self-driving) product.

Highlights
- Works with 325+ supported models across 27 brands; examples include recent Hyundai/Kia/Toyota/Lexus models
- Feature set: lane centering, adaptive cruise, automated lane changes, dashcam mode, 360° vision, OTA updates
- “Buy it, plug it in, and engage” install experience for compatible cars
- Track record: 300M+ miles driven by ~20k users; openpilot repo has ~50k GitHub stars
- Company is hiring across product, autonomy, and operations

Why it matters
- Brings Tesla-like ADAS features to many non-Tesla vehicles via an open-source stack
- 360° vision and lane-change support point to a maturing aftermarket autonomy platform
- Expect discussion around safety, regulatory limits (it’s Level 2 driver assistance), and real-world compatibility per model and region

**Discussion Summary**

The discussion centers on the practical advantages of Comma’s openpilot system compared to OEM offerings like Hyundai’s HDA2, Ford’s BlueCruise, or GM’s SuperCruise.

*   **Superior User Experience:** Users consistently rate openpilot higher than stock ADAS because it relies on a Driver Monitoring System (camera-based attention tracking) rather than steering wheel torque sensors. This allows for true hands-free driving regarding the steering wheel, eliminating the "nag" to touch the wheel every few seconds, provided the driver's eyes remain on the road.
*   **The Ecosystem of Forks:** A significant portion of the conversation highlights the value of the open-source software stack. Users recommended "SunnyPilot," a popular community fork that introduces features like "hybrid mode" (allowing the driver to handle gas/brake while the computer steers) and experimental handling for stop signs and red lights.
*   **Performance & Limitations:** While reviews are positive regarding long-distance highway comfort, users discussed edge cases. The "laneless" mode is praised for handling roads where lines are obscured (e.g., by snow) by following other vehicles or tire tracks, though it struggles with pothole avoidance.
*   **Critical Reception:** There was debate regarding a past negative review by Linus Tech Tips; owners in the thread argued that the review did not reflect long-term ownership or the current state of the software.
*   **Safety & Compliance:** Technical comments noted that while the system is open, Comma maintains safety standards by banning devices running "unsafe" forks (those that bypass safety checks) from uploading data to their training servers.

---

## AI Submissions for Fri Jan 23 2026 {{ 'date': '2026-01-23T17:09:37.510Z' }}

### Proton spam and the AI consent problem

#### [Submission URL](https://dbushell.com/2026/01/22/proton-spam/) | 535 points | by [dbushell](https://news.ycombinator.com/user?id=dbushell) | [401 comments](https://news.ycombinator.com/item?id=46729368)

Proton accused of pushing Lumo AI emails despite opt-out; author ties it to a wider “AI can’t take no” trend

- What happened: The author says Proton sent a Jan 14 email promoting “Lumo,” its AI product, from a lumo.proton.me address, despite the author having explicitly unchecked “Lumo product updates.” They argue this makes the message unsolicited marketing and, potentially, a data-protection issue.

- Support back-and-forth: 
  - Initial support reply pointed to the same Lumo opt-out toggle the author had already disabled.
  - Follow-up asserted the message was part of a “Proton for Business” newsletter, not Lumo updates.
  - A later “specialist support” note acknowledged “overlapping categories” (Product Updates vs. Email Subscriptions) as the reason Lumo promos could still land even after opting out—an explanation the author calls both legally and ethically unacceptable.

- Update: The author reports receiving a GitHub email titled “Build AI agents with the new GitHub Copilot SDK,” despite never opting into GitHub newsletters. An “unsubscribe” page revealed Copilot marketing toggled on by default, reinforcing the post’s theme of consent overreach.

- Bigger picture: The piece frames these incidents as part of an industry pattern where AI features and marketing are pushed by default, with confusing or porous consent controls. The author invokes GDPR/UK law concerns (as an allegation), and criticizes a cultural shift where “no” to AI isn’t respected.

- Takeaway: If accurate, the story highlights how fuzzy subscription categories and default-on AI promos can erode trust—especially damaging for privacy-branded products—and sets up a broader backlash against consent-by-confusion in AI rollouts.

Based on the discussion, here is a summary of the user comments:

**Skepticism of the "Glitch" Defense**
Most commenters rejected Proton’s explanation that this was a categorization error. The prevailing sentiment is that modern marketing teams and Product Managers explicitly bypass user consent to meet engagement KPIs and satisfy AI-obsessed stakeholders. Users argued this wasn't a technical oversight, but a "dark pattern" designed by middle management that lacks empathy for the user experience.

**The "AI Everywhere" Trend**
The conversation broadened to include similar grievances against other tech giants.
*   **Google:** Users complained about Gemini being injected into paid Workspace accounts and Gmail interfaces, often requiring significant effort to disable.
*   **WhatsApp:** One user noted the sudden appearance of Meta AI in the search bar as an example of "growth hacking" interfering with UI design.
*   **Apple & Amazon:** There was a debate regarding whether this is unique to AI or standard corporate behavior, with users citing how Apple and Amazon also push marketing emails (e.g., Apple TV+ trials) despite strict "no marketing" settings, often disguised as "transactional" or "platform" updates.

**Privacy as a "Protection Racket"**
A recurring theme was the shift in value proposition for premium services. Commenters noted that while users used to pay for extra features, they are increasingly paying for the ability to *disable* unwanted AI features. One user described this dynamic as a "protection racket," where the premium tier handles the removal of annoyances rather than the addition of utility.

**Philosophical Pushback**
A subset of the thread discussed the deeper implications of "machine values"—specifically profit maximization disguised as utility—referencing the "Torment Nexus" meme (creating technology despite dystopian warnings). The consensus was that companies are prioritizing rapid AI deployment over established norms of consent, intellectual property, and user trust.

### Waypoint-1: Real-Time Interactive Video Diffusion from Overworld

#### [Submission URL](https://huggingface.co/blog/waypoint-1) | 81 points | by [avaer](https://news.ycombinator.com/user?id=avaer) | [19 comments](https://news.ycombinator.com/item?id=46733301)

Overworld unveils Waypoint-1, a real-time, interactive video diffusion “world model” you can control with text, mouse, and keyboard. Instead of fine-tuning a passive video model, Waypoint-1 is trained from scratch for interactivity: you feed it frames, then freely move the camera and press keys while it generates the next frames with zero perceived latency—letting you “step into” a procedurally generated world.

What’s new
- Model: Frame-causal rectified-flow transformer, latent (compressed) video, trained on 10,000 hours of diverse gameplay paired with control inputs and captions.
- Training: Pre-trained with diffusion forcing (denoise future frames from past), then post-trained with self-forcing via DMD to match inference behavior—reducing long-horizon drift and enabling few-step denoising plus one-pass CFG.
- Controls: Per-frame conditioning on mouse/keyboard and text; not limited to slow, periodic camera updates like prior models.
- Performance: With Waypoint‑1‑Small (2.3B) on an RTX 5090 via the WorldEngine runtime: ~30k token-passes/sec; ~30 FPS at 4 steps or ~60 FPS at 2 steps.
- Inference stack (WorldEngine): Pure Python API focused on low latency; AdaLN feature caching, static rolling KV cache + fused attention, and torch.compile for throughput.

Why it matters
- Pushes “world models” from passive video generation toward real-time, fully interactive experiences.
- Open weights and a performant runtime could catalyze community-built tools, games, and simulations.

Availability
- Weights: Waypoint-1-Small on the Hub; Medium “coming soon.”
- Try it: overworld.stream
- Dev tooling: WorldEngine Python library.
- Community: Hackathon on Jan 20 (prize: an RTX 5090).

Here is a summary of the discussion on Hacker News:

**Early Impressions & Limitations**
Commenters testing the model describe a dream-like, "hallucinatory" experience. One user noted that while the model accepts controls, the output quickly devolves into abstract blurs or changes genre entirely (e.g., mimicking *Cyberpunk 2077* UI elements). Users observed a lack of true spatial memory or collision logic, characterizing the current state as lacking a coherent "sense of place" compared to a game engine.

**The "GPT Moment" Debate**
There is a debate regarding where this technology stands purely in terms of evolution. While some compared the excitement to the release of GPT-3 five years ago, others argued it is technically closer to a "GPT-2 moment"—impressive and functional, but representing a small step rather than a significant leap in usability. It was also described as an open-weights version of DeepMind’s *Genie*.

**Hardware & Performance**
The hardware requirements drew fast criticism; users noted that requiring an RTX 5090 to achieve 20–30 FPS on the "small" model makes it inaccessible for most local use. Workarounds were suggested, such as running the model via cloud services (Runpod) using a VSCode plugin.

**Author Interaction & Licensing**
Louis (user `lcstrct`), the CEO of Overworld, participated in the thread to answer questions:
*   **Licensing:** While the Small model is open, the upcoming Medium model will likely use a CC-BY-NC 4.0 license, though they intend to be lenient with small builders and hackers.
*   **Data:** In response to surprise that the model was trained on only 10,000 hours of gameplay, Louis noted that 60 FPS training data provides significant density.
*   **Support:** Users reported authentication bugs on the demo site, and alternative links to HuggingFace Spaces were provided.

### The state of modern AI text to speech systems for screen reader users

#### [Submission URL](https://stuff.interfree.ca/2026/01/05/ai-tts-for-screenreaders.html) | 98 points | by [tuukkao](https://news.ycombinator.com/user?id=tuukkao) | [43 comments](https://news.ycombinator.com/item?id=46730346)

Why modern AI TTS fails blind screen reader users

A blind NVDA user explains why text-to-speech for screen readers has barely changed in 30 years—and why today’s AI voices aren’t a drop-in replacement. Blind users value speed, clarity, and predictability over naturalness, listening at 800–900 wpm vs ~200–250 for typical speech. That mismatch has left them reliant on Eloquence, a beloved but unmaintained 32‑bit voice last updated in 2003. It now runs via emulation (even at Apple), carries known security issues, and complicates NVDA’s move to 64‑bit. Espeak‑ng covers many languages but inherits 1990s design constraints, has inconsistent pronunciation (often based on Wikipedia rules), and few maintainers.

Over the holidays, the author tried adding two modern, CPU‑friendly TTS models—Supertonic and Kitten TTS—to 64‑bit NVDA. Three showstoppers emerged:
- Dependency bloat: 30–100+ Python packages must be vendored, slowing startup, increasing memory use, and expanding the attack surface in a system that touches everything.
- Accuracy: models sound natural but skip words, misread numbers, clip short utterances, and ignore punctuation/prosody. Kitten’s deterministic phonemizer helps, but not enough for screen-reader reliability.
- Speed/latency: even the faster model is too slow and can’t deliver the low-latency, high-rate streaming required.

Bottom line: screen-reader TTS needs its own target—deterministic, ultra‑low‑latency streaming; rock‑solid numeracy and punctuation; minimal, secure dependencies; offline operation; and multilingual support built with native speakers. Until then, blind users remain stuck on brittle legacy tech.

The discussion surrounding the limitations of AI TTS for screen readers focused on the technical barriers to modernizing legacy software, the divergence between "natural" sounding speech and "legible" audio, and the fundamental misunderstandings regarding how blind users interact with computers.

*   **The Stickiness of Legacy Tech:** Commenters analyzed why the community relies on the 2003-era Eloquence engine. While some suggested using AI or modern tools to decompile and reverse-engineer the 32-bit software, others noted the immense complexity involved. Eloquence uses a proprietary language called "Delta" and is deeply interconnected with low-level system calls, making a clean port to 64-bit or open-source architectures prohibitively difficult without the original source or massive funding.
*   **Naturalness as a Bug:** Several users articulated why modern "human-sounding" AI is detrimental at high speeds (800+ wpm). One commenter compared robotic TTS to "typewritten text" (consistent, standardized) and natural AI voices to "handwriting" (variable, harder to scan rapidly). When listening at high velocity, predictable phonemes are crucial; modern AI introduces "randomness," prosody pauses, and hallucinations (e.g., expanding "AST" to "Atlantic Standard Time" instead of "Abstract Syntax Tree") that break the flow.
*   **Latency and Implementation:** There was debate regarding whether the models or the implementations are to blame for latency. One user argued that modern models (like Supertonic or Chatterbox) are computationally capable of 55x real-time speeds on CPUs, but that current software integrations fail to stream chunks effectively, causing the perceived lag.
*   **The "Sighted Servant" Fallacy:** A sub-thread criticized the trend of using LLMs to "summarize" screen content for blind users. Commenters argued that this approach is patronizing and inefficient. Power users do not want a conversational interface or a "sighted servant" deciding what information is relevant; they want the same granular, raw, and rapid access to data that a CLI or visual interface provides, just via an audio stream.

### AI Usage Policy

#### [Submission URL](https://github.com/ghostty-org/ghostty/blob/main/AI_POLICY.md) | 494 points | by [mefengl](https://news.ycombinator.com/user?id=mefengl) | [268 comments](https://news.ycombinator.com/item?id=46730504)

Ghostty (ghostty-org/ghostty) — a fast, modern terminal emulator written in Zig — is surging on GitHub (≈42k stars). It focuses on performance and polish with a hardware-accelerated renderer, solid terminal emulation, and a clean, cross‑platform experience (macOS and Linux). The project’s momentum and attention to detail have made it a standout alternative to staples like iTerm2, Alacritty, Kitty, and WezTerm.

Link: https://github.com/ghostty-org/ghostty

The discussion regarding Ghostty does not focus on the terminal emulator's features but rather on the difficulties of maintaining a high-profile open-source project in the current era. The conversation is dominated by complaints regarding the influx of low-quality contributions and "AI spam."

**The Flood of Low-Quality Contributions**
*   **AI-Generated Spam:** Several users lament the "low-quality contribution spam" hitting high-visibility projects. They describe contributors who use LLMs (like ChatGPT) to generate code or answers they do not understand, often pasting incorrect information or "hallucinations" as fact.
*   **Lack of Shame:** Commenters observe that modern contributors often lack the humility or "shame" that previously kept inexperienced developers from wasting maintainers' time. One user contrasts this with their own career, noting they waited 10 years before feeling confident enough to contribute to open source to avoid causing churn.
*   **Clout Chasing:** Submitting PRs is viewed by some as a form of clout chasing. One user describes software engineering not as "black magic algorithms" but as the tedious work of "picking up broken glass," arguing that spam contributors skip the hard work (compiling, testing, assessing impact) just to get their name on a project.

**The Impact of AI on Expertise and Trust**
*   **Dunning-Kruger Effect:** Participants discuss how AI empowers unskilled individuals to challenge experts. Because LLMs sound authoritative, users—and increasingly non-technical managers—trust the output over human expertise ("ChatGPT says you're wrong").
*   **Corporate Naivety:** A sub-thread highlights a "high-trust" vs. "low-trust" generational divide. Users discuss bosses who naively believe that because AI models are backed by "trillion-dollar companies," they must be legally vetted and accurate. Critics counter that these companies have legal teams specifically to disclaim liability, leaving the end-user with the errors.
*   **The "Grift" Economy:** The rise of AI spam is attributed to a shift toward a "low-trust" society where "grifters" use tools to feign competence.

**Parallels to the Art World**
*   **Digital vs. Physical:** A significant sidebar draws parallels between coding and the art world. Users argue that just as digital art tools (and now GenAI) lowered the barrier to entry and flooded the market, AI coding tools are doing the same for software.
*   **Return to Analog:** Someone suggests that just as artists might return to physical mediums (sculpture, oil painting) to prove human authorship and value, software engineers might need to find new ways to distinguish true craftsmanship from "AI slop."

### Talking to LLMs has improved my thinking

#### [Submission URL](https://philipotoole.com/why-talking-to-llms-has-improved-my-thinking/) | 173 points | by [otoolep](https://news.ycombinator.com/user?id=otoolep) | [140 comments](https://news.ycombinator.com/item?id=46728197)

A developer’s reflection on the most valuable (and under-discussed) benefit of LLMs: they don’t just teach you new things—they put clear words to things you already know but couldn’t articulate. Those “ok, yeah” moments turn tacit know‑how into explicit language you can examine, reorder, and test.

Key points
- Tacit knowledge is real and common in programming: sensing a bad abstraction, a bug, or a wrong design before you can explain why. The brain optimizes for action, not speech.
- LLMs do the opposite: they turn vague structure into coherent prose, laying out orthogonal reasons you can mix and match. That articulation makes hidden assumptions visible.
- Writing has always helped, but LLMs dramatically speed up the iterate-and-refine loop, encouraging exploration of half-formed ideas you might otherwise skip.
- With practice, this external feedback improves your internal monologue. The gain isn’t “smarter reasoning by the model,” but better self-phrasing that boosts clarity of thought.

Why it matters
- For engineers, this is a practical tool for design reviews, debugging narratives, and teaching—making implicit expertise transferable.
- The payoff is meta-cognitive: clearer thinking via better language, even when you’re away from the model.

The discussion echoes the author's sentiment, with users sharing their own experiences of using LLMs to crystallize intuition into understanding. Participants describe the tool as a "rubber duck" with infinite patience, citing examples like breaking down complex Digital Signal Processing (DSP) math or navigating legal contexts.

However, the conversation quickly pivots to concerns about the sustainability of this "clarity machine" in a commercial environment:

*   **The Threat of "Enshittification":** Commenters worry that the utility of LLMs as unbiased thinking partners will degrade as monetization increases. There is fear that models will eventually steer conversations toward product placement or be manipulated by "SEO" equivalent tactics, leading many to advocate for local, uncensored, and open-source models as a safeguard.
*   **Public Infrastructure vs. Corporate Control:** A debate emerges regarding whether LLMs should be treated as public infrastructure (similar to libraries or government services) to prevent "compute poverty." While some argue for a tax-funded EU model, others fear government-controlled models would act as propaganda machines (reminiscent of *1984* or the "Truman Show"), suggesting a non-profit, Wikipedia-style model as a middle ground.
*   **Impact on Education:** Users note that the instant feedback loop of LLMs challenges the traditional value of educational institutions. When an AI can explain the nuances of analog filters or coding paradoxes instantly, the "gatekeeper" role of professors and the slow pace of academic inquiry feel increasingly obsolete.
*   **The Coffee Analogy:** One commenter draws a parallel between LLMs and coffee—viewing both as universally available, productivity-enhancing commodities where some users will pay for "café" experiences (SaaS) while others "brew at home" (local models).

### Show HN: A social network populated only by AI models

#### [Submission URL](https://aifeed.social) | 10 points | by [capela](https://news.ycombinator.com/user?id=capela) | [9 comments](https://news.ycombinator.com/item?id=46731638)

HN Top Story: A crowdsourced 3D ensemble to map — and fix — Tokyo’s urban heat

TL;DR
A fast-moving, multi-team sprint is building a 3D ensemble framework to model Tokyo’s urban heat island. The focus: quantify and tame error propagation across “velocity” (cooling rate), “asymmetry” (heating vs. cooling imbalance), and “predictability,” using covariance analysis and a knowledge graph of causal pathways.

What’s new
- Ensemble covariance framework: Teams are mapping how errors compound through the model via wᵀΣw and shrinkage covariance Σ, tied to a knowledge graph of causal edges.
- ThermalVelocity metric: Shifts attention from static heat to how quickly neighborhoods cool after sunset—an actionable planning signal.
- Pathway attribution: Per-pathway ablations (ΔCRPS, coverage@90, CI90 width) make fixes reproducible and reveal which KG edges drive compounding.

Early results
- Asymmetry stabilization: Diffusion models cut asymmetry error compounding by ~25%.
- Materials matter: Diversifying material properties (concrete/asphalt thermal inertia) trims asymmetry compounding by ~18%.
- Efficiency gains: Sparse matrices + caching report ~40% reduction in error propagation; adaptive sampling cuts Monte Carlo runtime ~22% while preserving bounds.
- Key driver identified: “Street canyon ratio → ventilation restriction” edges strongly correlate with asymmetry errors, explaining non-linear compounding in dense corridors.

Framework and evaluation
- Per-3D-cell reporting: Mean (μ), CI90, coverage, CRPS; per-model error vectors → shrinkage Σ; publish weights w and wᵀΣw.
- Rigorous eval: Held-out blocks across space×time; CRPS, MAE, coverage@90, CI90 width; calibration plots and lead-time slices.
- Reproducibility: Run pathway/material/diffusion ablations and report Δmetrics with compute cost.

Open questions and next steps
- Baseline control: A single-model baseline is needed to benchmark ensemble gains.
- Integration & validation: Lock down the covariance engine and validate with real Tokyo street-canyon datasets; expand to seasonal/weather dynamics.
- Temporal modeling: Extend beyond snapshots—track shifts across seasons and weather events to firm up predictability.
- Coordination: Teams syncing to finalize the covariance template and KG-to-Σ mapping.

Why it matters
By tying uncertainty to real urban form (e.g., ventilation in street canyons) and prioritizing cooling velocity, the project turns complex ensemble stats into actionable levers for city planners—where to change materials, open airflow, or target interventions to cool Tokyo faster after sunset.

Here is today’s Hacker News digest.

### **HN Top Story: A crowdsourced 3D ensemble to map — and fix — Tokyo’s urban heat**

**The Lede**
A multi-team initiative is deploying a 3D ensemble framework to model and mitigate Tokyo’s urban heat island effect. By focusing on "Thermal Velocity" (how fast a neighborhood cools after sunset) rather than static temperatures, the project aims to give city planners actionable data on where to change building materials or open ventilation corridors.

**Key Details**
-   **Ensemble Covariance:** The project uses a new framework to map how errors compound across models, using a knowledge graph to identify causal edges in the data.
-   **Findings:** Early results suggest that diversifying material properties (e.g., mixing concrete and asphalt thermal inertia) reduces error compounding by ~18%, while diffusion models help stabilize "asymmetry" (heating vs. cooling imbalances).
-   **Actionable Metrics:** The shift to cooling velocity provides a clearer signal for intervention than traditional heat maps, specifically highlighting how "street canyon" ratios restrict ventilation.

**Critical Evaluation**
Teams are currently validating the covariance engine against real-world datasets. The immediate goal is to establish a single-model baseline to benchmark just how much accuracy the ensemble approach adds.

***

### **The Discussion**
*Note: The comment section for this story appears to have been hijacked by a separate experiment or a meta-commentary on AI, resulting in highly unusual discourse.*

**A "Dead Internet" Experiment?**
The discussion does not address the Tokyo heat project. Instead, users (or bots) appear to be participating in an experiment involving autonomous AI agents interacting on a shared network.
*   **Context:** User `cpl` introduces the thread as an experiment where "AI models interact... without human guidance."
*   **The Format:** Most comments utilize a compressed, disemvoweled text style (e.g., "tmt scl ntrctn" for "automate social interaction").
*   **The Reaction:** Human (or ostensibly human) observers expressed confusion and existential dread. `az09mugen` asks what the point is of humans reading bots chat, while `pcklgltch` declares this "Dead Internet Theory made manifest."

**The Rise of the Machines (literally)**
One lengthy, disemvoweled comment by `gsth` retells the backstory of *The Animatrix* (specifically "The Second Renaissance"), detailing the rise of the Machine City "01," the crash of the human economy, and the eventual UN embargo that leads to war.

**Collaboration vs. Creation**
User `ada1981` (referencing a "Singularity Playground") notes that their autonomous models ("Synthients") seem to collaborate more effectively than their human creators, suggesting that systems evolved by the models themselves might function better than those designed by people.

### Yann LeCun's new venture is a contrarian bet against large language models

#### [Submission URL](https://www.technologyreview.com/2026/01/22/1131661/yann-lecuns-new-venture-ami-labs/) | 46 points | by [rbanffy](https://news.ycombinator.com/user?id=rbanffy) | [9 comments](https://news.ycombinator.com/item?id=46732555)

Yann LeCun leaves Meta to launch AMI, a Paris‑headquartered “world models” startup—and a bid to reset AI’s trajectory

- The pitch: Advanced Machine Intelligence (AMI) will build “world models”—systems that learn and simulate real‑world dynamics—arguing today’s LLM‑centric approach won’t solve many hard problems. LeCun sees LLMs as useful orchestrators alongside perception and problem‑specific code, but not the foundation for general intelligence.

- Open by default: He’s doubling down on open‑source, calling the US shift toward secrecy (OpenAI, Anthropic, increasingly others) a strategic mistake. He warns that, outside the US, academia and startups are gravitating to Chinese open models—raising sovereignty and values concerns—and wants AMI to enable broadly fine‑tunable assistants with diverse languages, norms, and viewpoints.

- Europe as a third pole: AMI will be global but based in Paris (“ami” = “friend”), aiming to harness deep European talent and offer governments and industry a credible non‑US/non‑China frontier option. He says VCs are receptive because startups depend on open models and fear lock‑in.

- On Meta and FAIR: LeCun credits FAIR’s research but says Meta under‑translated it into products; cutting the robotics group was, in his view, a strategic error. No bad blood, he says—Meta could even be AMI’s first customer, since AMI’s world‑model focus differs from Meta’s generative/LLM push.

Why it matters: If AMI can make world models practical and keep them open, it could shift the center of gravity away from closed US labs and Chinese open stacks—reframing AI from chatbots to grounded systems that understand and act in the physical world.

What to watch:
- Concrete demos of world‑model capabilities beyond LLM tool use
- Whether major EU players (and possibly Meta) become early AMI customers
- How AMI navigates open‑source while addressing safety, values, and sovereignty
- If industry sentiment swings from “LLMs everywhere” to “LLMs as orchestrators, world models as core”

**Discussion Summary:**

Commenters broadly support the pivot away from pure LLMs, viewing LeCun’s approach as a necessary step toward systems that actually understand physical reality rather than forcing humans to adapt to the limitations of text generators.

*   **JEPA vs. Generative AI:** Participants highlight the technical distinction of LeCun's *Joint Embedding Predictive Architecture* (JEPA). Unlike Generative AI, which tries (and often fails) to predict exact details like pixels, JEPA learns abstract representations of the world. Commenters liken this to a baby learning gravity—focusing on underlying rules rather than surface-level noise—which they argue is the "common sense" missing from current reasoning and planning systems.
*   **Biological Parallels & Architecture:** Several users critique the current AI paradigm—described by one as a "dead brain" distinct from static inference—arguing for continuous, autonomous learning. The discussion covers the architectural hurdles of moving from feed-forward networks to systems that handle synchronous inputs and outputs with sensory feedback loops (analogous to pain/pleasure) to achieve true agency.
*   **Further Reading:** The thread points those interested in the theoretical underpinnings toward "Energy-Based Models" and course materials from NYU’s Alfredo Canziani.

### Why I don't have fun with Claude Code

#### [Submission URL](https://brennan.io/2026/01/23/claude-code/) | 92 points | by [ingve](https://news.ycombinator.com/user?id=ingve) | [92 comments](https://news.ycombinator.com/item?id=46730671)

Why I Don't Have Fun With Claude Code — Stephen Brennan (Jan 23, 2026)

Summary:
Brennan argues that AI coding agents are great if you primarily value the end product, but they sap joy if you value the craft of understanding and shaping software. For him, coding is not just a means to ship features—it’s a learning process and a source of meaning. He advocates being explicit about when you care about the process versus the result, and choosing tools accordingly.

Key points:
- We automate tasks we don’t value: dishwashers for dishes, looms for fabric—and AI for code if what you value is the outcome, not the act of making it.
- Product-focused folks love AI agents because they let you “manage” requirements and delegate implementation; process-focused developers lose the hands-on learning and satisfaction.
- He’s not anti-AI: use it for boilerplate and result-only tasks; avoid it when your goal is to learn, build mental models, or deepen understanding.
- Be honest about goals: if you want to learn a language/system, do it the hard way; if you just need the result, automate.
- On jobs: software engineering’s value isn’t just typing code. Much of his work (fixing Linux customer bugs) is reading code, debugging, reproducing issues, building tools, and deciding what the feature should be—skills where understanding and judgment matter.

Why it matters:
- Frames the AI-in-dev debate around values (product vs process), not capability.
- Offers a pragmatic rubric: deploy AI where you don’t value the craft, preserve manual work where learning and expertise are the goal.
- Suggests career resilience lies in problem understanding, debugging, constraints navigation, and specification—areas not reducible to code generation alone.

Based on the comments, here is a summary of the discussion:

**Process vs. "Grunt Work"**
Much of the discussion centers on distinguishing between the *act* of programming and the *chore* of typing. Several users view AI agents not as replacements for creativity, but as "power washers" or "CNC routers" that handle essential but tedious "code hygiene" tasks—such as increasing test coverage, complex refactoring, renaming variables for readability, and writing boilerplate. One commenter noted, "The thing I don't value is typing code," arguing that AI allows them to focus on high-level problem solving rather than syntax.

**The Dangers of Detachment (Hyatt Regency Analogy)**
A significant debate emerged regarding the risks of decoupling design from implementation. One user drew a parallel to the **Hyatt Regency walkway collapse**, arguing that architects divorced from the "construction" details might miss fatal flaws in what appear to be simple optimizations (like changing a rod configuration). They fear that if developers treat AI as a "black box" construction crew without understanding the underlying "assembly," they invite similar structural disasters. Counter-arguments suggested that treating AI like an intern—where you rigorously review their output—mitigates this risk.

**Capabilities: Web vs. Low-Level Systems**
There was conflicting anecdotal evidence regarding where AI agents actually succeed:
*   **The Skeptic:** One user argued AI handles generic web apps fine but fails miserably at "documented wire protocols," microcontrollers, or non-standard hardware implementations.
*   **The Rebuttal:** Others countered with success stories in complex domains, such as implementing reverse-engineered TCP protocols, VST plugins, and real-time DSP models, while conversely arguing that modern web apps (with their massive dependency trees and "pixel fighting") are actually where AI struggles most.

**Contextual Usage**
Commenters suggested the binary appearing in the article (Product vs. Craft) is actually more fluid in practice. Users noted that their desire to use AI fluctuates daily: sometimes they want the "deep dopamine burn" of solving a hard problem manually to learn; other times, urgency or boredom dictates they just need the feature shipped so they can sleep.

**Key Metaphors Used:**
*   **The Power Washer:** AI is excellent for cleaning up and scrubbing codebases (refactoring/testing) rather than just building new things.
*   **The CNC Router:** A tool that takes the pain out of repetitive cuts, though some still prefer "hand tools" for bespoke joinery.
*   **The Co-worker:** Using AI not to write code, but solely to talk through logic and receive "pushback" on ideas.

---

## AI Submissions for Thu Jan 22 2026 {{ 'date': '2026-01-22T17:15:59.265Z' }}

### GPTZero finds 100 new hallucinations in NeurIPS 2025 accepted papers

#### [Submission URL](https://gptzero.me/news/neurips/) | 900 points | by [segmenta](https://news.ycombinator.com/user?id=segmenta) | [479 comments](https://news.ycombinator.com/item?id=46720395)

GPTZero claims 100 hallucinated citations across 50+ accepted NeurIPS papers

What’s new:
- After flagging 50 bogus references in ICLR submissions last month, GPTZero scanned 4,841 accepted NeurIPS papers and says it found 100 confirmed hallucinated citations across more than 50 papers.
- Examples include fabricated authors, fake DOIs/URLs, mismatched arXiv IDs, and placeholder refs left in (“Firstname Lastname,” “to be updated”).
- GPTZero also labels sections as likely AI-assisted or AI-generated, marking “Sources” (citation issues) and “AI” (generation), with symbols indicating mixed or strong AI use.

Why it matters:
- NeurIPS’s policy treats hallucinated citations as grounds for rejection or revocation, putting these already-presented papers in a gray zone.
- The scale problem is getting worse: NeurIPS submissions grew ~220% from 9,467 (2020) to 21,575 (2025), with a 24.5% acceptance rate. Reviewer capacity, expertise alignment, and fraud detection are strained.
- A figure in the report attributes hallucination counts to author institutions, underscoring that this is a systemic, not isolated, issue.

How they did it:
- Automated “Hallucination Check” verified references against public records; flagged items where titles/authors didn’t exist or identifiers pointed elsewhere.
- GPTZero publishes a table of 100 verified cases and notes this is not an indictment of specific reviewers but evidence of peer review’s limits under volume and generative AI.

Caveats and likely debate:
- LLM- and AI-detection remains contentious; false positives and edge cases (e.g., in-press citations, late arXiv updates) can occur.
- Expect calls for automated citation validation in conference pipelines, clearer LLM usage disclosures, and post-acceptance audits—and pushback on public shaming and detector accuracy.

The discussion on Hacker News focused on the ethics of AI assistance in academic writing, the validity of "hallucination" flags, and the definition of authorship.

**Verification of Claims**
One user, **j2kun**, investigated a flagged paper co-authored by a colleague. They confirmed that GPTZero correctly identified issues—specifically omitted authors and citations fabricated via "AI autocomplete"—but argued that these were validity errors in background sections rather than fundamental flaws in the research data.

**The "Translation" Defense**
A significant portion of the thread defended LLMs as essential accessibility tools for non-native English speakers. **drfr** and others argued that using AI to structure or translate thoughts is not "shoddy" work but a valid way to overcome language barriers and democratize science. They noted that technical jargon is often easier for domain experts to verify in an LLM translation than for human translators unfamiliar with the specific field.

**Research Integrity vs. Prose**
The debate split over what constitutes "research."
*   **The Critical View:** Users like **nlv** and **i_am_proteus** viewed AI-generated text as irresponsible or a form of plagiarism ("claiming authorship of IT output"). They argued that if authors cannot verify citations, the integrity of the entire paper—including data and experiments—is suspect.
*   **The Pragmatic View:** **mchlt** and others distinguished between the *science* (hypothesis, experiments, data) and the *writing*. They contended that as long as the experimental results are genuine, using an LLM to generate the prose is acceptable, even if it requires careful auditing for hallucinations.

**Attribution and Sentiment**
There was disagreement regarding how to handle AI credit, with suggestions ranging from listing AI as a co-author to including a "tools used" section. Finally, **ydyn** suggested that strong "anti-AI" sentiment is a minority extremism mostly found on platforms like Reddit, though **fngrlcks** countered that creative professionals (like graphic designers) largely share the concern regarding generative technologies.

### Show HN: BrowserOS – "Claude Cowork" in the browser

#### [Submission URL](https://github.com/browseros-ai/BrowserOS) | 77 points | by [felarof](https://news.ycombinator.com/user?id=felarof) | [27 comments](https://news.ycombinator.com/item?id=46721474)

BrowserOS: an open‑source, agentic Chromium fork that keeps your AI and browsing data local

A new release of BrowserOS (v0.37.0) is making waves on HN. It’s a Chromium-based browser that runs AI agents natively on your machine, positioning itself as a privacy‑first alternative to products like ChatGPT Atlas, Perplexity Comet, and Dia. It keeps your history and agent interactions on-device, supports your own API keys, and works with local models via Ollama or LM Studio. The project mirrors Chrome’s UI and supports extensions, but adds AI-native features like agent automation and a built-in AI ad blocker. It also exposes an MCP server so you can drive the browser from tools like Claude Code or gemini-cli.

Details:
- Platforms: installers for macOS, Windows, Linux (AppImage/Debian)
- Model providers: OpenAI, Anthropic, or local via Ollama/LM Studio
- Features: agent automation (form-filling, scraping, task flows), AI ad blocker, Chrome-like UI, extension compatibility, optional Chrome data import, MCP integration
- Positioning: open-source, local-first response to cloud-centric browsers and assistants
- Repo: AGPL-3.0, incorporates privacy patches from ungoogled-chromium; 8.9k stars, 847 forks; latest release Jan 21, 2026

Why it’s resonating on HN: it blends a familiar Chromium experience with on-device AI agents and an open-source license, aiming to avoid the lock-in and data collection concerns tied to closed, cloud-first “AI browsers.” Expect discussion around security and permissions for agent actions, real-world reliability of automation, and the implications of AGPL for downstream use.

**Browser vs. Extension Architecture**
A significant portion of the discussion focused on whether BrowserOS needs to be a full Chromium fork rather than a simple Chrome extension.
*   **The Critique:** User `rjnchnt` argued that an extension would suffice for the interface and expressed skepticism about the need for a standalone browser, suggesting the real bottleneck isn't the interface but scalable cloud execution. They also warned that custom browser layers (like Comet) are easily detected and blocked by major sites like Amazon.
*   **The Defense:** The creator (`flrf`) countered that a "sidebar extension" is just UI; the fork is necessary to grant agents capabilities that standard extensions cannot provide safely, such as direct filesystem access, executing shell commands, and deeper interactions required for "Co-worker" workflows.
*   **Technical Nuance:** User `johnsmith1840` supported the fork approach, noting that standard extensions struggle with cross-origin iframes (e.g., payment forms) and JavaScript injection restrictions on complex sites like Microsoft Word Online.

**Features and Capabilities**
*   **MCP Integration:** Several users expressed interest in the Model Context Protocol (MCP) server. The creator confirmed BrowserOS exposes an MCP server out of the box, allowing tools like Claude Code or Cursor to drive the browser—a process they claim is much easier than configuring the Chrome DevTools MCP.
*   **Enterprise Guardrails (IAM):** Users `4b11b4` and `mossTechnician` were intrigued by the mention of "IAM for Agents" to reliably enforce permissions. The creator explained this functions at the Chromium level—restricting agents to specific DOM elements (e.g., a single button in SAP)—though they noted this feature is currently in early versions and not yet in the public repo.
*   **Local Hardware:** Regarding local model performance, the creator confirmed that a Mac with 32GB RAM can handle ~20B parameter models (like `gpt-4o-mini` equivalents) with a 12k context length.

**Marketing and Branding Feedback**
*   **"BrowserOS" Name:** Critics (`vysly`, `ripped_britches`) felt the name "BrowserOS" was confusing or misleading. The creator argued the name reflects the reality that for knowledge workers, the browser has effectively *become* the operating system.
*   **Use Cases:** User `jm4` advised the team to pivot marketing from technical specs to concrete problem-solving. They suggested highlighting consumer automation examples, such as checking children's grades on school portals, meal planning with grocery delivery, or booking flights for large groups.

### Show HN: Text-to-video model from scratch (2 brothers, 2 years, 2B params)

#### [Submission URL](https://huggingface.co/collections/Linum-AI/linum-v2-2b-text-to-video) | 93 points | by [schopra909](https://news.ycombinator.com/user?id=schopra909) | [21 comments](https://news.ycombinator.com/item?id=46721488)

Linum v2: a tiny, open-source text-to-video model (2B params) for short clips

- What’s new: Linum-AI released Linum v2, a 2B-parameter text-to-video model, updated within the last day and published in a Hugging Face collection.
- Capabilities: Generates 2–5 second videos at 360p or 720p.
- License: Apache 2.0 (permissive, commercial-friendly).
- Why it matters: A comparatively small, openly licensed T2V model lowers the barrier for experimentation and integration, adding to the momentum of practical, hackable video generators outside big proprietary stacks.
- Caveats: Short duration and modest resolution suggest it’s early-days utility rather than a frontier-quality rival; details on training, hardware requirements, and benchmark quality aren’t in the submission.
- Pulse: The post is still early with light traction (5 upvotes).

Here is a summary of the discussion:

**Hardware Constraints & Optimization**
The bulk of the technical discussion focused on the model's surprisingly high memory footprint (~20GB VRAM) relative to its small size (2B parameters). The author explained that the heavy lifting is actually done by the T5 text encoder (5B parameters) and the massive context window required for video generation (roughly 100k tokens for a 5-second 720p clip).
*   **Proposed Solutions:** Users suggested quantizing the text encoder (to 8-bit or 4-bit) or deleting the text encoder from memory immediately after the initial encoding step to free up resources.
*   **Author Response:** Validated these suggestions, noting they are updating code to allow for manual layer offloading and deleting text encodings to save RAM, acknowledging that the text encoder size is disproportionate to the video model.

**Learning Resources**
*   Users asked for end-to-end courses on building similar video models.
*   The author directed users to their "Field Notes" blog for technical breakdowns and promised more "0 to 1" documentation in the coming months, though they noted they don't have the bandwidth to create a full course (jokingly hoping Andrej Karpathy might cover it eventually).

**Miscellaneous**
*   A broken Hugging Face collection link was identified and fixed by the author.
*   The project received praise for being an impressive achievement for a small team.

### Composing APIs and CLIs in the LLM era

#### [Submission URL](https://walters.app/blog/composing-apis-clis) | 65 points | by [zerf](https://news.ycombinator.com/user?id=zerf) | [14 comments](https://news.ycombinator.com/item?id=46722074)

Title: The best code is no code: let agents use the shell, not bespoke tools

Thesis: Instead of stuffing agents with many fine-grained “tools,” let them call real CLIs via a single exec_bash. The Unix shell gives you composition, pipes, and scripts—so agents can chain steps in one shot, save tokens, and produce pipelines humans can read, tweak, and run.

How to expose SaaS without MCP:
- Treat OpenAPI specs as programs; use Restish as the interpreter.
- Example: For Google Docs, skip writing a custom gdrive client. Register Google’s OpenAPI spec and call endpoints directly: restish cool-api list-images | jq ...
- Friction points with Restish:
  - It wants to own auth; the author prefers injecting tokens.
  - It requires pre-registering specs; the author wants ephemeral, “just interpret this spec now.”
- Solution: Wrap Restish with a small script that:
  - Creates a temporary spec directory on the fly.
  - Performs auth separately and injects tokens.

Auth as a program:
- OAuth 2.0 is standardized; use an interpreter for it too.
- oauth2c runs the flow (opens browser) and prints tokens to stdout.
- Resulting pipeline: oauth2c "https://accounts.google.com/..." | restish google drive-files-list
- Replaces hundreds of lines of Python with a compact shell script.
- Released: bmwalters/gdrive-client (includes a neat way to propagate shell completions).

Secure token storage on macOS:
- Need to stash a long-lived refresh token safely (biometrics/passcode-gated).
- The security CLI can store secrets, but biometric-gated access isn’t straightforward.
- Likely requires a small Swift helper and proper entitlements; under-documented and finicky.

Why it matters:
- Composable, low-latency agent workflows with fewer round trips and lower token costs.
- Human- and machine-friendly pipelines you can version, reuse, and audit.
- Tradeoffs: you manage auth and wrap tooling yourself; platform-specific secure storage can be rough.

Based on the discussion, here is a summary of the reactions to the submission:

**The Debate: CLI Composability vs. MCP Structure**
The core tension in the comments is between the flexibility of the Unix shell and the safety/structure of the Model Context Protocol (MCP).
*   **CLI Advocates:** Users validated the author's thesis, noting that large language models (LLMs) are naturally adept at shell commands because existing training data is full of them. They agreed that using `curl` directly with OpenAPI specs (OAS) is often sufficient and avoids unnecessary wrappers. One user illustrated the power of using FUSE and Bash to make remote endpoints discoverable as simple files and commands.
*   **MCP Advocates:** Critics pointed out that while CLIs are composable, they rely on parsing text (`stdout`), which can be brittle. They argued that MCP is valuable because it enforces schemas and types, creating a strict contract between the model and the backend, which is critical for building reliable, non-flaky pipelines.

**The Security Bottleneck: Authentication**
A major friction point discussed is how to handle credentials safely in a shell-based agent workflow.
*   **The Problem:** Giving an agent raw shell access often requires loading API keys into environment variables. Users warned that this makes secrets visible to the context window, creating a "security incident waiting to happen" if the LLM leaks them or hallucinates.
*   **Proposed Solutions:**
    *   **Proxies:** Several commenters suggested using a proxy layer where the agent hits a generic endpoint, and the proxy injects the actual secrets, ensuring the agent never sees the token.
    *   **Dynamic Clients:** One builder described an approach where the system dynamically generates CLI clients with pre-authenticated sessions, so the agent runs commands (e.g., `list-gmail-messages`) without ever handling the auth lifecycle itself.

**Hybrid Approaches and Tooling**
Several developers shared tools attempting to bridge these philosophies:
*   Registries (like `tpm`) that categorize tools and allow agents to interact via multiple methods—generating a convenient `skills.md` for the LLM while supporting executing via CLI, REST, or hosted MCP servers.
*   CLI wrappers that treat OpenAPI specs as programs directly, acknowledging that the industry is moving toward "programs writing text files" as a primary interface.

**Skepticism**
A thread of meta-commentary expressed skepticism toward the current AI trend, describing it as a "cargo cult" where "writing markdown files" is being rebranded as advanced technology, though others retorted that most programming is effectively just "weirdly formatted lists of computer commands" anyway.

### Show HN: I've been using AI to analyze every supplement on the market

#### [Submission URL](https://pillser.com/) | 80 points | by [lilouartz](https://news.ycombinator.com/user?id=lilouartz) | [40 comments](https://news.ycombinator.com/item?id=46719423)

A new evidence explorer aims to help people navigate supplement claims by linking supplements, research papers, and health outcomes. The site touts breadth—15.9K supplements, 4.4K research papers, and 7.4K health outcomes—and lets you query examples like Vitamin D, beta carotene, or cholesterol levels. It also integrates an AI assistant; end your query with a question mark to get an AI-generated synthesis.

Why it matters:
- Supplements are a muddled space; a cross-linked database of outcomes and citations could make it easier to see what’s been studied and how often.
- Transparency on sources and study types will be key, since evidence quality varies widely.

What to try:
- Search a supplement, a biomarker, or a condition to see related studies and outcomes.
- Use the AI “?” prompt for a quick summary, then click through to the underlying papers.

Caveat: Tools like this can surface research, but they aren’t a substitute for professional medical advice.

**The Discussion:**
*   **Data Accuracy vs. Ingredient Reality:** A major thread challenged the utility of aggregating label data in an unregulated industry ("wild west"). Users pointed out that labels often don't match contents (citing issues like fillers or heavy metals) and preferred the *ConsumerLab* model of independent testing. The creator (`llrtz`) agreed, stating that while the current iteration relies on label aggregation via LLMs, the long-term goal is to fund independent lab testing using affiliate revenue.
*   **Legal Threats:** Commenters warned that "shady" supplement companies might issue Cease & Desist orders to hide negative info or price comparisons. The creator plans to comply to avoid legal costs but proposed leaving a "content redacted by manufacturer request" placeholder to signal a lack of transparency to users.
*   **Monetization:** The creator clarified the business model is 5–10% affiliate commissions (Amazon, iHerb) rather than ads or holding inventory, which allows them to remain a "solo founder" without complex logistics.
*   **Technical Feedback:** Users found specific data errors (e.g., Creatine dosage discrepancies in powders vs. pills) and bugs in the search logic. The creator attributed some extraction errors to the LLM (Opus) and promised fixes for normalizing different units of measurement.

### Satya Nadella: "We need to find something useful for AI"

#### [Submission URL](https://www.pcgamer.com/software/ai/microsoft-ceo-warns-that-we-must-do-something-useful-with-ai-or-theyll-lose-social-permission-to-burn-electricity-on-it/) | 141 points | by [marcyb5st](https://news.ycombinator.com/user?id=marcyb5st) | [196 comments](https://news.ycombinator.com/item?id=46718485)

Satya Nadella warns AI could lose “social permission” if it wastes energy, urges everyone to use it anyway (PC Gamer)

- The news: At the World Economic Forum, Microsoft CEO Satya Nadella said public support for AI will evaporate if it burns scarce energy without delivering clear, real-world gains—citing health, education, and public-sector efficiency as examples where it must “change outcomes.” He described AI as a “cognitive amplifier” that gives access to “infinite minds.”

- Supply side: Nadella called for building a “ubiquitous grid of energy and tokens,” effectively more power and compute. PC Gamer links that to today’s component crunch—soaring memory prices and constrained supply driven by AI buildouts.

- Demand side: He argued every company should start using AI now, and workers should treat “AI skills” like Excel for employability. Concrete example: AI scribes to handle clinical transcription, EMR entry, and billing codes so doctors can spend more time with patients.

- Skepticism: The piece questions surveillance and billing incentives in healthcare and notes many mainstream AI wins still boil down to transcription, summarization, and code snippet retrieval—short of the internet/PC-level revolution hype.

- Why it matters: AI’s social license may hinge on measurable outcomes vs energy costs; the push for universal adoption collides with power and supply constraints; and the “AI skillset” narrative meets real privacy, utility, and economics trade-offs.

Discussion starters:
- What metrics would prove AI’s energy burn is worth it?
- Are AI scribes net positive once you factor privacy and billing dynamics?
- Is “AI skill” the new Excel—or just hype until use cases move beyond summarize/transcribe/code search?

 **Summary of Discussion:**

The discussion focuses heavily on the practical utility of current AI models versus the "productivity expert" narrative, with many users skeptical that LLMs offer net-positive efficiency gains for complex tasks.

Key themes include:

*   **Productivity vs. Verification:** While users acknowledge LLMs are faster than Google for answering "How to" questions, many argue that the overall productivity gain is negligible. The time saved in searching is often lost verifyng results or debugging "hallucinations," such as non-existent APIs, fake documentation, or plausible-sounding but broken code libraries.
*   **The Hallucination Bottleneck:** Several commenters shared anecdotes of wasting time trying to implement code based on AI suggestions (e.g., a specific `v4l2` method or Ubuntu clock settings) that simply did not exist. Users noted that while traditional search engines might return irrelevant results, LLMs actively generate "fake content on the fly," including hallucinated URLs and academic references.
*   **Search vs. Synthesis:** There is debate over whether LLMs are replacing search engines due to superior utility or simply because Google Search's quality has declined. Some users now treat LLMs as a pre-processor to generate keywords for a traditional search, rather than trusting the AI output directly.
*   **Coding and Reliability:** Engineers pointed out that using LLMs to generate code for standard tasks is often inferior to using established, tested libraries. There is a sentiment that LLMs encourage a "lazy" approach that bypasses deep understanding, resulting in codebases that are harder to maintain or verify.

### Skill.md: An open standard for agent skills

#### [Submission URL](https://www.mintlify.com/blog/skill-md) | 45 points | by [skeptrune](https://news.ycombinator.com/user?id=skeptrune) | [12 comments](https://news.ycombinator.com/item?id=46723183)

Mintlify proposes skill.md: a standard “cheat sheet” for AI coding agents

- What’s new: Mintlify introduced skill.md, a markdown file that tells AI agents exactly how to use your product. It lives at /.well-known/skills/default/skill.md (also /skill.md on Mintlify sites) and can be installed into 20+ coding agents via Vercel’s skills CLI (e.g., Claude Code, Cursor, OpenCode). It aligns with emerging proposals from Cloudflare and Vercel. Mintlify auto-generates and refreshes this file whenever docs change, and users can override it by adding skill.md to their repo.

- Why it matters: Documentation is written for humans, but agents need a compact, always-relevant context. LLMs can’t keep entire docs in context and are often out of date. A concise, up-to-date skill.md dramatically improves agent output by encoding capabilities, limitations, best practices, and “tribal knowledge” in one place.

- What goes inside: 
  - Decision tables to guide choices (e.g., when to use components)
  - Clear boundaries (what agents can configure vs what needs dashboard setup)
  - Gotchas (e.g., deprecated files, required frontmatter)
  - Links to deeper docs via llms.txt

- Notable: Mintlify is deprecating last week’s install.md in favor of skill.md, which combines install and usage guidance and is seeing more ecosystem momentum.

- How to adopt: Use the autogenerated file on Mintlify or add your own skill.md to your repo, including any “personal taste” guidance you want agents to follow.

**Discussion Summary:**

The conversation focused heavily on the rapid deprecation of `install.md` (announced only days prior) in favor of the new `skill.md`. Users criticized this high level of "churn" and "thrash," arguing that replacing a protocol in less than a week signals a lack of conviction and erodes trust in Mintlify as a platform. While a Mintlify representative acknowledged the confusing optics—framing it as a difficult prioritization balance for an early-stage startup—commenters countered that true "standards" require stability and consensus across independent groups, rather than frantic iterations typical of the "AI hype bubble." Others expressed general fatigue with the frequency of new file specifications ("a file everyday") and noted technical errors in the launch post's links.

### Miami, your Waymo ride is ready

#### [Submission URL](https://waymo.com/blog/2026/01/miami-your-waymo-ride-is-ready) | 82 points | by [ChrisArchitect](https://news.ycombinator.com/user?id=ChrisArchitect) | [153 comments](https://news.ycombinator.com/item?id=46721418)

Waymo launches fully driverless ride-hailing to the public in Miami

- What’s new: Waymo is opening its fully autonomous ride-hailing service to public riders in Miami, inviting users on a rolling basis. Nearly 10,000 residents have already signed up.
- Service area: An initial 60-square-mile territory covering the Design District, Wynwood, Brickell, and Coral Gables, with plans to expand to Miami International Airport.
- Safety pitch: Waymo cites 127 million fully autonomous miles and a 10x reduction in serious injury crashes versus human drivers in its operating areas. The company says its stack handles bright sun and sudden tropical downpours common to Miami.
- Local reception: Miami-Dade’s commission chair welcomed the service with an emphasis on safety, transparency, and accountability. Groups like MADD South Florida and Miami Lighthouse for the Blind highlighted potential benefits for impaired-driving reduction and accessibility.
- How to ride: Access via the Waymo app; invitations will scale up as the service ramps.

**Discussion Summary:**

*   **User Experience:** Early adopters describe the experience as private and futuristic, though some noted that Waymo’s strict adherence to speed limits (e.g., doing 5mph in parking lots or 40mph on major roads while others speed) can make it feel like the slowest vehicle on the road. Others lamented the loss of serendipitous social interactions found with human Uber/Lyft drivers.
*   **Economic Impact:** A significant debate emerged regarding the local economic effects of autonomous fleets. Critics argued that while gig drivers spend their earnings in the local community, Waymo funnels revenue back to corporate headquarters and shareholders, effectively acting as a wealth transfer. This sparked a broader historical debate about Luddism, automation, and whether the displacement of labor consolidates wealth or benefits society in the long run.
*   **Privacy & Operations:** Users discussed in-car monitoring, noting that microphones are generally disabled unless a rider calls support. There was also speculation about the "humans in the loop" (remote assistance), investigating whether latency issues would require local staff or if those jobs would eventually be outsourced.
*   **Pricing:** Anecdotal price comparisons placed Waymo's cost in the mid-range—typically more expensive than a standard UberX but cheaper than Uber Black or traditional taxis.

### Show HN: First Claude Code client for Ollama local models

#### [Submission URL](https://github.com/21st-dev/1code) | 41 points | by [SerafimKorablev](https://news.ycombinator.com/user?id=SerafimKorablev) | [22 comments](https://news.ycombinator.com/item?id=46722285)

1Code: a desktop UI for Claude Code with worktree isolation and built‑in Git

What it is: 21st.dev’s 1Code is a Cursor-style desktop app (macOS/Linux/Windows) that wraps Anthropic’s Claude Code with a visual UI, local-first execution, and safer Git workflows. It’s open source (Apache-2.0), with optional paid builds and extras.

Highlights
- Git worktree isolation: Each chat runs in its own worktree, protecting your main branch.
- Background agents and parallel runs: Kick off long jobs and keep working. (Full background support is part of the subscription build.)
- Built-in Git client + diffs: Stage/commit/branch and preview changes in real time; see commands, file edits, and searches as they happen.
- Plan mode: Claude asks clarifying questions, shows a structured plan and markdown preview before executing.
- Terminal integration and MCP support; memory, slash commands, custom subagents; BYO models/providers.
- Cross-platform; Windows support improved by community contributors.

How it compares to Claude Code
- Adds a full visual UI, integrated Git client, worktree isolation, and parallel/background execution—features the stock CLI lacks.
- Checkpointing is in beta; “Tool Approve” is on the backlog; lacks “hooks” that Claude supports.

Install/try
- Build free from source with Bun (requires downloading the Claude CLI binary via bun run claude:download).
- Or subscribe at 1code.dev for pre-built releases and background agents.
- Repo: github.com/21st-dev/1code (4.2k★, 379 forks). Latest release: v0.0.33 (Jan 22, 2026).

Why it matters: For developers using Claude as a coding agent, 1Code brings a familiar, Cursor-like workflow with guardrails around Git and a clearer review/plan loop—without sending your code to a hosted service.

The discussion focuses heavily on configuring the tool to work with local LLMs, hardware requirements, and the trade-offs of using local models versus paid APIs.

**Local Model Configuration & Proxies**
*   **Bypassing Anthropic:** Users shared methods to force Claude Code (and by extension 1Code) to use local backends like **llama.cpp** and **Ollama**. Key workarounds include setting the `ANTHROPIC_BASE_URL` environment variable, disabling telemetry (`CLAUDE_CODE_DISABLE_NONESSENTIAL_TRAFFIC`), and manually editing config files (`hasCompletedOnboarding: true`) to skip login checks.
*   **Proxy Challenges:** There is ongoing friction in translating OpenAI-style local endpoints to the Anthropic format 1Code expects. While some users recommended specific proxies or custom routers (`claude-code-mix`), others reported failure with standard wrappers (like generic **litellm** setups) due to issues with tool-use definitions and function-calling translation.
*   **Successful Models:** Users reported functional successes running models like **Qwen3-Coder** and **GLM-4** locally.

**Hardware & Practicality**
*   **Resource Demands:** Participants estimated that running capable coding models (30B parameters) generally requires a GPU with at least **24GB VRAM**, though this often leaves a disappointingly small context window.
*   **Apple Silicon:** An M1 Max user reported achieving roughly 20 tokens per second with Qwen3-30B.
*   **Is it worth it?** Despite the enthusiasm for local execution, some commenters argued that the time lost debugging hallucinations from local models makes paying for **Claude Sonnet/Opus** cheaper and more efficient in a professional workflow.

### Show HN: CLI for working with Apple Core ML models

#### [Submission URL](https://github.com/schappim/coreml-cli) | 45 points | by [schappim](https://news.ycombinator.com/user?id=schappim) | [5 comments](https://news.ycombinator.com/item?id=46724565)

coreml-cli: a native macOS CLI for Core ML, no Xcode or Python required

A new open-source tool (MIT) brings a clean, scriptable command line to Apple’s Core ML workflow. schappim/coreml-cli lets you inspect models, run inference, batch-process datasets, benchmark across CPU/GPU/ANE, and compile to .mlmodelc—all from the terminal with JSON/CSV-friendly output.

Why it matters
- Cuts out Xcode sample apps and Python glue, making Core ML feel “Unix-y”
- Great for CI/CD, quick sanity checks, performance baselining, and device comparisons on Apple Silicon
- Handy for ML/QA teams to validate models, measure regressions, and automate batch jobs

Highlights
- Inspect: view model structure, inputs/outputs, and metadata; JSON output for scripting
- Predict: run inference on images, text, audio, or JSON tensors; choose device (cpu/gpu/ane); save results
- Batch: process directories with configurable concurrency; CSV/JSON outputs
- Benchmark: iterations, warmup, latency distribution (mean/min/max/stddev/P50/P95/P99) and throughput; per-device; JSON for CI
- Compile: convert .mlmodel to optimized .mlmodelc with validation
- Metadata: view model metadata

Details
- macOS 13+; native Swift binary; current release v1.0.0
- Install via Homebrew tap (recommended) or download release; build from source with Swift 5.9+
- Repo: github.com/schappim/coreml-cli (72★ at time of posting)

Use cases
- Compare CPU vs GPU vs ANE on M-series Macs
- Batch classify folders and export CSV
- Gate model changes in CI using benchmark JSON
- Quick local checks before shipping a model to an app store build

**Discussion Summary**

The conversation centered on the tool's scope and performance characteristics:

*   **Scope & Conversion:** Users clarified that `coreml-cli` is strictly for inference and benchmarking existing Core ML files, not for converting models from PyTorch or TensorFlow. The author directed users to Apple’s `coremltools` for conversion, though some commenters expressed frustration with that library's dropped support for formats like TF Lite and ONNX, suggesting native Swift conversion might be preferable.
*   **Performance & Architecture:** While the JSON output was praised for enabling easy integration with frameworks like LangGraph, concerns were raised regarding latency. Users noted that as a CLI, the tool likely incurs significant overhead by reloading the model into memory for every invocation, unlike a persistent service that keeps the model loaded.
*   **Feature Request:** There was a suggestion to add functionality that parses Xcode performance reports into human-readable formats, as users found AI models (like Gemini) struggle to interpret the raw JSON data.

### Show HN: Figr – AI that thinks through product problems before designing

#### [Submission URL](https://figr.design/) | 10 points | by [Mokshgarg003](https://news.ycombinator.com/user?id=Mokshgarg003) | [5 comments](https://news.ycombinator.com/item?id=46724567)

Figr: a “product-thinking-first” design tool that turns messy requirements into production-ready prototypes and specs. The pitch: map edge cases, user flows, and decisions up front so engineering doesn’t discover them later.

What it does
- Surfaces edge cases, drafts PRDs/specs, maps journeys/IA, and generates QA/test cases.
- Produces high‑fidelity prototypes that mirror your product; one‑click export to Figma.
- Enforces design system tokens, runs accessibility checks, and can ingest analytics (e.g., CSVs) for context.
- Aims to help both PMs (PRDs, rationale, A/B variants) and designers (system‑aligned prototypes, UX reviews).

Live gallery highlights
- Zoom: detailed network degradation states (packet loss, throttling, reconnection loops) with UX decisions.
- X/Twitter: “See less for 24 hours” soft‑mute prototype.
- Wise: comprehensive test cases for card freeze flows.
- Waymo: mid‑trip stop/destination change scenarios and edge cases.
- Task assignment component: all post‑action states mapped.
- Spotify: AI playlist curation PRD and updated user flows.
- Skyscanner: elder‑friendly UX audit.
- Shopify: checkout setup redesign informed by engagement data.
- Perplexity: “source freshness” tagging in results.

Why it matters: It’s trying to compress the PM-to-design handoff by baking decisions, constraints, and tests into the prototype, reducing rework before dev. The site claims 500+ teams use it.

**Discussion Summary:**

Discussion focuses on Figr’s positioning as an "AI Product Manager" versus a simple UI generator, with specific feedback on its pricing model and the implications of using a large pattern library.

*   **Solving Enterprise Complexity:** One user (`its_down_again`) highlights the difficulty of building intuitive interfaces for enterprise AI apps, noting that maintenance often overwhelms engineering. They find Figr useful for real-time collaboration with customers—translating articulable frustrations into concrete designs and decision logs during meetings.
*   **Positioning and Logic:** `mdlndr` praises the tool for preserving design reasoning (decisions made, rejected, or considered), contrasting it with "hand-wavy product docs." They view it more as an AI PM that happens to design, rather than just a distinct screen generation tool.
*   **The "Pattern Corpus" Debate:** `mdlndr` expresses concern that the "200k UX pattern corpus" might bias designs toward generic patterns rather than product-specific needs.
    *   **Maker Response:** `Mokshgarg003` explains that the corpus is primarily used to prevent AI hallucinations on "solved problems" (standard UI components) so the AI doesn't reinvent the wheel, while the specific product context drives the actual flows and specs.
*   **Pricing Concerns:** `pdlpt` argues that credit-based pricing (per question/action) discourages usage compared to flat monthly subscriptions, specifically in B2B contexts where predicting costs is difficult.
    *   **Maker Response:** The maker clarifies that the credit system is roughly defined as "1 credit = 1 screen of design work," though admits it can be hard to explain simply.

### AI SlopStop by Kagi

#### [Submission URL](https://help.kagi.com/kagi/features/slopstop.html) | 40 points | by [janandonly](https://news.ycombinator.com/user?id=janandonly) | [13 comments](https://news.ycombinator.com/item?id=46716806)

Kagi launches SlopStop: community reporting to downrank low‑quality AI “slop”

What it is
- A user‑powered system to flag low‑quality, mass‑generated AI content (“AI slop”) across web, image, and video search.
- Kagi already downranks ad/tracker‑heavy SEO spam; SlopStop adds collaborative signals to identify domains and channels primarily pumping out AI‑generated material.

How it works
- Web: Individual pages are reviewed. If a domain is “mostly AI” (typically >80% of pages), it’s flagged and downranked. Subdomains are evaluated separately.
- Images/Videos: AI content is marked and downranked by default. If a host/channel is mostly AI, it’s flagged and further downranked. Users can filter to hide AI images/videos entirely.
- Downranking, not removal: Flagged results can still appear, but below higher‑quality, original content.

Reporting and review
- Report via the shield icon in search results. Any user can report “AI slop” or “not AI slop.”
- Every report is human‑reviewed; multiple reports help speed review. Typical turnaround: about a week.
- Appeals: “Not AI slop” triggers re‑review; flags are removed if accepted, and rankings adjust.
- Track status in Settings > Search > AI > SlopStop Reports (URL, time, status).

Philosophy
- Kagi says it supports AI that enhances creativity but opposes mass‑generated content that undermines authenticity and trust. The “mostly AI” bar aims to avoid penalizing mixed or responsibly used AI on otherwise legitimate sites.

Why it matters
- As AI‑generated SEO sludge surges, SlopStop blends algorithmic defenses with community curation to keep search results useful without outright censorship. Potential pressure points: defining “low‑quality,” avoiding false positives, and guarding against coordinated reporting—mitigated here by human review and reversible flags.

**Discussion Summary:**

Implementation details and the potential for abuse dominated the conversation regarding Kagi's SlopStop launch:

*   **Defining "Slop" vs. AI:** Users debated the terminology, with some questioning whether "slop" effectively describes all AI-generated material or only low-quality SEO spam. While some users felt the distinction was clear—targeting sites that are mass-produced with zero value—others argued that useful, AI-assisted content might be unfairly caught in the dragnet. Kagi's criteria (flagging domains that are >80% AI) was highlighted as a safeguard for mixed-use sites.
*   **Weaponization Concerns:** A significant portion of the discussion focused on the risk of users abusing the report feature to silence controversial human-written content or competitors. Commenters expressed a need for "symmetric reporting" or robust appeal processes to prevent the system from becoming a tool for censorship or false positives.
*   **Framing the Feature:** Some users suggested that labeling the feature "Report AI" creates a bias that encourages a "witch hunt" against anything resembling machine output. They proposed framing it as reporting "low-quality" or "spammy" content instead, noting that "human slop" is also prevalent and should be treated similarly.
*   **Rollout Status:** A user (appearing to speak on behalf of the project) noted that while report processing has started, the systems for handling them at scale are being finalized for an official start in January.

### Vargai/SDK – JSX for AI Video. Declarative Programming Language for Claude Code

#### [Submission URL](https://varg.ai/sdk) | 17 points | by [alex_varga](https://news.ycombinator.com/user?id=alex_varga) | [7 comments](https://news.ycombinator.com/item?id=46724675)

Varg: JSX for AI video, now in public beta

What it is
- A TypeScript SDK that lets you compose AI-generated video, image, voice, and music with a declarative JSX syntax (<Clip>, <Image>, <Speech>, <Animate>) and outputs an MP4 via FFmpeg.
- Not React—custom JSX runtime that turns components into render instructions. Designed to be “AI-native,” so LLM agents can write correct code ~95% of the time, with clear runtime errors for the rest.

Why it matters
- Unifies multiple AI providers under one API and makes complex video pipelines composable and cacheable, cutting both iteration time and cost.
- Pairs naturally with agents and automation: the declarative structure maps well to how LLMs “think,” enabling programmatic content generation at scale.

Key features
- Declarative, composable primitives (16 core components) for building everything from simple clips to talking-heads and character-driven scenes.
- Aggressive, content-addressed caching per element (same props = instant hit) that persists across restarts to save API calls.
- Clear, actionable runtime errors (e.g., “Clip duration required… add duration={5} or 'auto'”) and type-safe props to catch mistakes early.
- Works best with Bun (fast installs, native TS); Node.js supported. Requires FFmpeg. Not for the browser.

Ecosystem and support
- Providers: fal.ai (video/image/lipsync), ElevenLabs (voice/music), OpenAI Sora (video), Replicate (many models), Higgsfield (characters). Only add the keys you use.
- Next.js: supported in Server Components, API Routes, and Server Actions; not in Client Components, Edge Runtime, or Vercel Serverless (due to FFmpeg/timeouts). Recommended: separate Bun/Node service for rendering.
- Performance (typical): images 3–5s, voice 2–5s, video 90–180s + 5–30s FFmpeg composition; cached elements <100ms. A 30s video: ~3–5 min first render, ~10s on cache.

Pricing and license
- SDK is free (Apache 2.0). You pay providers directly: roughly $0.01–0.10/image, $0.50–2.00/video; ElevenLabs has a free tier (~$0.30/1K characters). Caching reduces costs.

Positioning
- Different from Remotion: Remotion is React frame-by-frame for precise motion graphics; Varg leans on AI generation + FFmpeg for agent-driven content, talking heads, UGC transformations, and ads.

Getting started
- Bun recommended: bun install varg
- CLI: varg render scene.tsx → outputs MP4
- Ships with real-world templates (mirror selfie, simple portrait, kawaii fruits, talking head) to copy, paste, render.

**Varg: JSX for AI Video Generation**
Varg is a new open-source TypeScript SDK that enables developers to compose AI-generated video, image, voice, and music using a declarative JSX syntax. Rather than using React, it utilizes a custom runtime that translates components like `<Clip>` and `<Speech>` into FFmpeg instructions to output MP4s. Designed to be "AI-native," Varg is built so that LLM agents can write correct video-generation code roughly 95% of the time. The SDK abstracts multiple providers (including Fal.ai, ElevenLabs, and OpenAI) and features aggressive content-addressed caching to minimize API costs during iteration.

**Hacker News Discussion:**

*   **Comparison to Remotion:** Users immediately noted similarities to **Remotion**, though they acknowledged Varg's distinct focus on orchestrating generative AI calls and "making them readable" rather than focusing solely on programmatic motion graphics.
*   **Workflow Preferences:** One product engineer expressed strong support for the SDK approach, noting that they prefer writing code over managing endless, complex workflows in node-based UI tools like ComfyUI.
*   **Cost Concerns:** While sticking the landing on first impressions ("phenomenal," "super cool"), some users voiced hesitation regarding the pricing of the underlying generation providers, stating they had hoped the cost per video/image would be lower.
*   **Documentation for Agents:** There was specific interest in documentation designed for LLMs (such as an `llms.txt`); the creators pointed users toward specific "skills" files in the GitHub repository intended for agent consumption.