import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Sun Nov 30 2025 {{ 'date': '2025-11-30T17:11:30.121Z' }}

### Writing a good Claude.md

#### [Submission URL](https://www.humanlayer.dev/blog/writing-a-good-claude-md) | 656 points | by [objcts](https://news.ycombinator.com/user?id=objcts) | [252 comments](https://news.ycombinator.com/item?id=46098838)

HN: How to write a useful CLAUDE.md (and why your agent keeps ignoring it)

- Core idea: LLMs are (mostly) stateless—your agent knows nothing about your codebase unless you put it in the prompt. In Claude Code–style setups, CLAUDE.md (or AGENTS.md) is the one file that’s injected into every session, so it’s your default onboarding doc.

- What CLAUDE.md should do: onboard the agent each session.
  - WHAT: map the tech stack, project structure, and monorepo layout (apps, shared packages, where things live).
  - WHY: explain the project’s purpose and the roles of each part.
  - HOW: explain how to work in the repo—tooling choices (e.g., bun vs node), how to run tests/typechecks/builds, and how to validate changes.

- Why Claude often ignores CLAUDE.md: the harness injects a reminder telling the model to use it only if “highly relevant.” If your file is stuffed with broad or situational instructions, the model is more likely to discard it. You can verify this by proxying the API via ANTHROPIC_BASE_URL and inspecting the injected system reminder.

- Less is more: keep instructions short, universal, and essential.
  - Models can only juggle a limited number of instructions reliably (~150–200 for large “thinking” models; much fewer for smaller/non-thinking models).
  - As instruction count increases, adherence drops across the board; smaller models degrade much faster (often exponentially).
  - The harness system prompt already burns ~50 instructions, shrinking your reliable budget.

- Placement matters: LLMs bias toward the edges of the prompt (the very beginning—system + CLAUDE.md—and the very end—latest user messages). Put truly universal rules up front; put task-specific guidance at the end of your prompt.

- Practical implications:
  - Don’t cram every command, style guide, or “hotfix” into CLAUDE.md.
  - Use CLAUDE.md for stable, evergreen orientation; provide task-specific commands/examples inline with the current request.
  - Prefer concrete, relevant context (examples, related files, tool outputs) over sprawling instruction lists.
  - For multi-step or complex plans, use larger “thinking” models; smaller models will struggle.

- Why it likely works this way: many teams use CLAUDE.md to patch behavior with ad hoc rules. Telling the model to ignore low-relevance instructions generally improves outcomes.

Bottom line: Treat CLAUDE.md as a tight, universal onboarding sheet that maps the repo and workflow. Keep it lean to preserve the model’s instruction budget and to make room for focused, task-specific context in each session.

Here is a summary of the discussion:

**The "Brown M&Ms" Compliance Test**
A significant portion of the discussion focused on how to verify if the model is actually respecting the `CLAUDE.md` file. One user shared an anecdote about instructing Claude to address them as "Mr. Tinkleberry"; if the model stops using the name, the user knows the context has been dropped or the file is being ignored.
- Commenters immediately drew a parallel between this technique and Van Halen’s famous "Brown M&Ms" contract rider, noting it is an effective canary for checking if the AI is paying attention to technical constraints.
- Other variations suggested included requiring specific start/end emojis or sign-offs (e.g., "Purple fish") to verify instruction adherence.

**Granularity: Monolith vs. Distributed Context**
Users debated the structure of onboarding files. While the article suggests a single file, several commenters argued for placing multiple `CLAUDE.md` files in specific subdirectories (e.g., `src/persistence/CLAUDE.md` or `tests/CLAUDE.md`).
- Proponents argued this allows the model to pull in highly specific context only when working in those directories, preventing the "one big file" from being ignored due to length.
- Critics felt this approach creates "directory clutter" and forces developers to manage multiple non-portable configuration files, arguing that standard `README.md` files should suffice if the AI were smarter.

**Tooling Comparisons (Cursor, Aider, Skills)**
The discussion compared Claude's implementation to other tools.
- **Cursor:** Some users noted that Cursor handles file/subdirectory context more naturally without needing a "giant blob" of instructions.
- **Aider:** Mentions were made of Aider’s "chat map" approach to context.
- **Claude Skills:** There was confusion and debate regarding the new "Skills" feature versus `CLAUDE.md`. Some users found that while Skills are good for dynamic actions (like converting files), `CLAUDE.md` is better for persistent, "evergreen" project orientation.

**Engineering vs. "Magic"**
A philosophical sub-thread emerged regarding the effort required to make LLMs effective. Skeptics addressed the irony of needing extensive configuration files to make "intelligent" tools work, questioning the promised productivity gains. Counter-arguments stated that "magic" is a marketing term; real productivity enhancement is an engineering discipline (likened to learning Vim or Emacs) that requires setup, process planning, and learning how to prompt the tool effectively.

### AI just proved Erdos Problem #124

#### [Submission URL](https://www.erdosproblems.com/forum/thread/124#post-1892) | 224 points | by [nl](https://news.ycombinator.com/user?id=nl) | [78 comments](https://news.ycombinator.com/item?id=46094037)

AI-and-Lean settle an Erdős problem (the “with 1s allowed” version); stronger variant still open

What’s the problem?
- For bases d1 < d2 < … < dr (each ≥ 3), let P(d, k) be the set of sums of distinct powers d^i with i ≥ k. A classical question asks: if sum_i 1/(d_i − 1) ≥ 1, can every sufficiently large integer be written as a 0/1-sum of elements from the union of these P(d_i, k)?
- There are two variants:
  - k = 0 (allow the 1 = d^0 term). This is how Erdős phrased it in 1997 (no gcd condition).
  - k ≥ 1 (exclude the 1’s place). Burr–Erdős–Graham–Li (1996) studied this version; here a gcd(d1,…,dr) = 1 condition is clearly necessary.

What’s new?
- “Aristotle” (an automated prover from Harmonic) found a simple, elementary solution to the k = 0 version under the Pomerance/Tao necessity condition sum_i 1/(d_i − 1) ≥ 1. Boris Alexeev then formalized and type-checked it in Lean.
- Stronger than asked: the Lean theorem shows every integer n (not just “sufficiently large”) can be expressed as a sum of at most r numbers, one per base, where each summand has only digits 0/1 in its respective base (i.e., lies in P(d_i, 0)).
- Timing: Aristotle needed ~6 hours to find the proof; Lean verified it in about a minute.

Why the nuance matters
- Literature mismatch: BEGL96 disallowed the 1’s place (k ≥ 1) and thus requires a gcd condition; Erdős’s 1997 formulation allowed 1’s and stated no gcd condition. The new proof resolves the Erdős-1997 version. The BEGL96-style version (k ≥ 1, gcd = 1) remains open in general (known for {3,4,7}).
- Necessity of sum_i 1/(d_i − 1) ≥ 1: Observed by Pomerance; Tao sketched a justification in the comments (think Kraft-type/density obstructions).
- Related: Melfi constructed infinite families showing you can get “completeness” with ∑ 1/(d_i − 1) arbitrarily small in a different, infinite-base setting.

State of play
- First (with-1s) version: now has a short, formally verified proof.
- Second (no-1s, gcd = 1) version: still open, aside from specific base sets.

Link: https://www.erdosproblems.com/124

Here is the summary of the discussion.

**The "Moving Goalposts" Debate**
A significant portion of the discussion focused on whether dismissing the achievement as an "easy" problem constitutes moving the goalposts for AI.
*   **The Pro-AI View:** Users argued that ten years ago, an AI solving an open Erdős problem—and formally verifying it—would have been considered science fiction. Minimizing the result because the math turned out to be "Olympic level" rather than "deep research level" is seen by some as a defense mechanism to downplay AI progress.
*   **The Skeptical View:** Critics countered that the skepticism isn't about moving goalposts, but addressing specific, potentially misleading hype from the company (Harmonic). They argue that the problem was less a "grand mystery" and more a "forgotten loophole" or typo in Erdős's papers that humans simply hadn't prioritized.

**"Low-Hanging Fruit" and Systematic Solutions**
Technically minded commenters (referencing Terence Tao and Boris Alexeev) clarified the nature of the solution:
*   **The "Typo" Theory:** The consensus is that the specific variant solved (the "with 1s" version) was likely left open due to a clerical oversight or phrasing mismatch in historical literature, making it "low-hanging fruit" rather than a deep mathematical blockade.
*   **The Value of the Bucket:** Despite the problem being "easy" in hindsight, users noted the value in having an AI capable of iterating through a "large bucket" of neglected or clearly solvable open problems. This demonstrated a strength in checking overlooked corners of mathematics, even if it doesn't yet demonstrate deep "understanding."

**VC Hype vs. Technical Progress**
There proved to be a strong undercurrent of cynicism regarding the commercial framing of the announcement.
*   Several users compared the announcement to the crypto boom, suggesting that VC-backed startups are incentivized to produce "breathless claims" to attract investment.
*   This creates a "boy who cried wolf" effect, where legitimate technical advances are viewed with suspicion because the marketing ("Aristotle," "solving Erdős problems") feels designed for viral engagement rather than scientific precision.

**Miscellaneous**
*   **Confusion:** A few users expressed temporary confusion, thinking the post meant the ancient philosopher Aristotle had solved the problem thousands of years ago.
*   **Future Utility:** Speculation arose that this type of AI—able to verify combinatorial complexity—will be more useful in fields like materials science and biology (finding patterns) than in abstract mathematics, which prioritizes understanding over raw solutions.

### Program-of-Thought Prompting Outperforms Chain-of-Thought by 15% (2022)

#### [Submission URL](https://arxiv.org/abs/2211.12588) | 128 points | by [mkagenius](https://news.ycombinator.com/user?id=mkagenius) | [33 comments](https://news.ycombinator.com/item?id=46099108)

- The idea: Instead of having a language model both “think” and compute in natural language (Chain-of-Thought), Program-of-Thoughts (PoT) has the model express its reasoning as short programs (mainly Python). An external interpreter executes the code to get the final answer. This cleanly separates planning from calculation.

- How it works: Provide few-shot examples where each problem is paired with a small program that solves it. The model (Codex in the paper) generates a program for a new problem; a sandbox runs it to produce the answer. With self-consistency, they sample multiple programs and aggregate the outputs.

- Results: Across five math word-problem datasets (GSM8K, AQuA, SVAMP, TabMWP, MultiArith) and three financial QA sets (FinQA, ConvFinQA, TAT-QA), PoT beats Chain-of-Thought by about 12% on average in both zero- and few-shot settings. With self-consistency, it achieves state-of-the-art on all the math sets and near-SOTA on the financial ones.

- Why it matters: Precise computation reduces arithmetic hallucinations, the generated code is auditable and debuggable, and the approach plugs neatly into the broader “LLMs + tools” pattern that’s powering more reliable agents.

- Caveats: Requires a code-capable model and a secure execution sandbox; brittle if the generated program is logically wrong or depends on unavailable libraries; not every reasoning task is easily expressible as code.

- Status: Published at TMLR 2023. Code and data are available on GitHub (linked from the paper).

Here is a summary of the discussion:

The discussion around "Program of Thoughts" (PoT) expanded beyond simple Python execution into a debate about the best intermediate representations for AI reasoning.

*   **"Chain of Spec" vs. Code:** User `rbt-wrnglr` argued that jumping from fuzzy natural language directly to concrete code skips necessary logic layers, potentially wasting tokens on implementation bugs rather than intent. They (and others) proposed a "Chain-of-Spec" approach—using semi-formal representations like Markdown bullet lists, TLA+, or Alloy—to verify logic *before* generating executable code.
*   **Prior Art and Tooling:** Commenters noted that this concept isn't entirely new, citing similarities to **PAL** (Program-Aided Language Models) and **DSPy**, which has supported similar "program of thought" workflows for some time. Others pointed out that modern implementations (like Claude’s artifacts or ChatGPTs Code Interpreter) effectively internalize this behaviors already.
*   **Alternative Languages:** While the paper focuses on Python, several users discussed the benefits of using logic programming languages like **Prolog** or logical specifications (TLA+) as the intermediate step. These languages force stricter reasoning and are easier to verify than imperative Python scripts.
*   **Skepticism and Security:** There was some pushback on the "natural language programming" paradigm, with one user calling it delusional. Others raised concerns about the security infrastructure required to run arbitrary generated code ("self-destructive prompting" if the sandbox is unsafe).
*   **Neuro-Symbolic Future:** The thread ultimately converged on the value of hybrid systems (LLMs + Symbolic Logic), suggesting that the industry has a vested interest in keeping these intermediate "thinking" languages obscure to maintain a competitive moat.

### AI rendering of Roman war scenes from Trajan's Column

#### [Submission URL](https://trajancolumn.com) | 25 points | by [unix-junkie](https://news.ycombinator.com/user?id=unix-junkie) | [3 comments](https://news.ycombinator.com/item?id=46099838)

Scroll to Rotate, Click to Zoom (Swipe/Tap on mobile) proposes a simple, consistent interaction model for 3D/product viewers: use the scroll wheel or a swipe to rotate the object, and a click or tap to enter/exit zoom. The goal is to avoid the common pitfalls of scroll-to-zoom (accidental zoom, hijacking page scroll) and make zoom a deliberate mode switch.

Why it matters
- Reduces accidental zooming and “scroll-jacking” that fights the page’s natural scroll.
- Gives desktop and mobile the same mental model: rotate as the default, zoom as an explicit action.
- Improves clarity: zoom becomes a state the user opts into, rather than a fragile continuous gesture.

Key ideas
- Default interaction rotates the object: scroll on desktop, swipe on mobile.
- Zoom is a discrete toggle: click/tap to zoom in/out or enter/exit a zoom mode.
- Provide clear affordances: on-hover/tooltips or subtle UI hints that say “Scroll/Swipe to rotate, Click/Tap to zoom.”
- Respect the page: don’t capture scroll outside the viewer bounds; release scroll when the cursor leaves.
- Accessibility: add keyboard shortcuts (e.g., arrows to rotate, Enter/Space to zoom) and maintain focus states.

Trade-offs and discussion points
- Discoverability vs. convention: many users expect click-and-drag to rotate or scroll-to-zoom; hints and gentle onboarding help.
- Trackpads and pinch: consider supporting two-finger pinch for zoom as a secondary gesture without making it the default.
- Precision: scroll-to-rotate can feel “steppy” on mice; add easing/inertia and sensible sensitivity.
- Nested scrolling/iframed embeds: ensure the viewer doesn’t trap scroll when not intended.

If you build 3D/product viewers, this pattern is a strong default: make rotation effortless and zoom intentional, keep it consistent across devices, and gently teach the controls.

**Scroll to Rotate, Click to Zoom (Swipe/Tap on mobile)**
This submission proposes a consistent interaction model for 3D product viewers to solve "scroll-jacking." The pattern uses the scroll wheel (or swipe) to rotate objects and requires a deliberate click (or tap) to enter a zoom mode, aiming to tackle accidental zooming while respecting the page's natural scroll flow.

**Discussion Summary**
The discussion was brief and primarily focused on the visual assets used in the demo rather than the interaction pattern itself.

*   **Implementation vs. Assets:** While `cnstnts` acknowledged that the technical implementation of the viewer deserved praise, they pointed out that the visual assets appeared to be low-effort AI generations or slight modifications of existing images.
*   **Visual Quality:** `alexalx666` criticized the quality of the imagery, comparing it negatively to early iPhone photos.
*   **AI Usage:** There was a brief mention regarding the use of AI in bringing the content to life, though the sentiment appeared mixed regarding the quality of the output.

---

## AI Submissions for Sat Nov 29 2025 {{ 'date': '2025-11-29T17:09:06.407Z' }}

### Leak confirms OpenAI is preparing ads on ChatGPT for public roll out

#### [Submission URL](https://www.bleepingcomputer.com/news/artificial-intelligence/leak-confirms-openai-is-preparing-ads-on-chatgpt-for-public-roll-out/) | 776 points | by [fleahunter](https://news.ycombinator.com/user?id=fleahunter) | [682 comments](https://news.ycombinator.com/item?id=46086771)

What’s new:
- Strings found in ChatGPT Android 1.2025.329 beta reference an “ads feature,” “bazaar content,” “search ad,” and a “search ads carousel,” suggesting OpenAI is preparing to show ads inside ChatGPT’s search experience. Spotted by Tibor on X; reported by BleepingComputer.

Why it matters:
- If ChatGPT starts inserting ads into answers or search-style results, it could shift ad spend from traditional web search to AI assistants.
- With an estimated 800M weekly users and roughly 2.5B prompts per day, OpenAI has the scale—and potentially richer conversational context—to deliver highly personalized ads.

Details and context:
- The leak points to ads limited to search initially, but scope could expand.
- Positioning would echo Google’s search ads, but inside an assistant interface.
- Mentions of “bazaar content” hint at a first-party marketplace approach rather than standard web ad networks.

Open questions:
- Will Plus/Team/Enterprise tiers be ad-free?
- How clearly will ads be labeled inside answers?
- What data (chat history, “memory”) will be used for targeting, and can users opt out?
- Will there be revenue sharing with content providers whose info anchors responses?

Status: Internal testing; no official rollout announced.

Here is a summary of the discussion:

**The Economics of "Enshittification"**
Commenters debate the inevitability of this move. Some argue that given the astronomical costs of compute and OpenAI's scale (roughly 1 billion users), an ad-supported model for free tiers was unavoidable. Users describe this as the beginning of "enshittification," fearing OpenAI will repeat Google’s trajectory where the drive for ad revenue eventually degrades the quality of the user experience.

**Threats to the Duopoly**
There is speculation that OpenAI represents an existential threat to Google and Meta. Users note that the "extremely personal data" and context windows in LLMs allow for higher-value, hyper-targeted advertising than traditional search or social feeds ever could. However, skeptics counter that Google’s defensive moat is vast and that OpenAI must tread carefully to avoid alienating users who switched to ChatGPT specifically to escape the "SEO-filled nonsense" of modern search engines.

**Product Placement vs. Answer Integrity**
A significant portion of the thread is dedicated to satirizing how "embedded" ads might look. Users mockingly script scenarios where, for example, a turkey recipe explicitly instructs the user to set their "GE Oven" to 350° or drink a "Coke Zero" while waiting.
*   **Legal Gray Areas:** This satire leads to a serious discussion about the legality of undisclosed ads. Users question whether weaving brand recommendations into "factual" answers constitutes deceptive marketing, drawing comparisons to product placement in film ("The Truman Show," "Seinfeld") versus strictly regulated media types.

**Adoption and Normalization**
One user suggests that while current tech-savvy users hate the idea, the "boiling frog" effect applies: if introduced slowly, the broader public—and specifically younger generations—will likely accept ads in AI interfaces as a normal part of the digital landscape, just as they did with the web.

### Student perceptions of AI coding assistants in learning

#### [Submission URL](https://arxiv.org/abs/2507.22900) | 93 points | by [victorbuilds](https://news.ycombinator.com/user?id=victorbuilds) | [115 comments](https://news.ycombinator.com/item?id=46089546)

New Kid in the Classroom: Exploring Student Perceptions of AI Coding Assistants (arXiv)
A small exploratory study (n=20) in an intro programming course finds AI coding assistants boost novices’ confidence and help them grasp code concepts during initial development—but many stumble when asked to extend solutions without AI. The two-part exam design (first with AI support, then without) surfaced signs of overreliance and weak knowledge transfer. The author argues for pedagogy that integrates AI while explicitly strengthening core programming skills, rather than letting tools “impersonate” them.

Why it matters: As AI helpers become standard in CS classrooms, instructors may need assignments with “AI-on” and “AI-off” phases, reflection prompts, and assessment that tests understanding beyond tool output.

Caveats: Perception-focused, small sample, single course; results are suggestive rather than definitive.

Paper: https://arxiv.org/abs/2507.22900 (shorter version accepted to CCC 2025)

**New Kid in the Classroom: Exploring Student Perceptions of AI Coding Assistants**
[https://arxiv.org/abs/2507.22900](https://arxiv.org/abs/2507.22900)

**Summary of the Discussion:**
The discussion on Hacker News wrestles with whether AI in education represents a "slide rule" evolution—a new layer of abstraction—or a detrimental shortcut that creates dependency.

*   **The Ironies of Automation:** Several commenters cited Lisanne Bainbridge’s "Ironies of Automation," arguing that while AI intends to help, it often de-skills the user. If students "outsource" the struggle of learning syntax and logic to an LLM, they may lack the fundamental mental model required to debug or extend that code when the AI inevitably fails.
*   **Analogy Wars:** There is a fierce debate over the correct analogy. Some view AI as a calculator or spellcheck (a necessary productivity booster for "knowledge work"). Others critique this, suggesting a better analogy is physical exercise: if a machine lifts the weights for you, you don't gain the muscle.
*   **Credentialism and Hiring:** A major concern is the devaluation of computer science degrees. If assignments can be solved effortlessly by AI, academic credentials may lose their signaling power, forcing employers to lengthen interview processes with rigorous in-person testing to verify actual competence.
*   **The Student Perspective:** Students in the thread expressed anxiety about their own learning paths. They questioned whether skipping rote memorization of syntax to focus on "broad strokes" logic via AI is efficient learning or an intellectual trap that leaves them unable to code "blank slate."
*   **Determinism vs. Stochasticism:** While some argued that relying on AI is just the next step after migrating from Assembly to Python, skeptics countered that high-level languages are deterministic and reliable, whereas LLMs are stochastic and prone to hallucination, making them a risky foundation for novices.

**Methodology Note:** While some dismissed the paper for its small sample size ($N=20$), others defended qualitative research as essential for understanding the *nature* of how students navigate new tools, rather than just measuring output statistics.

### Major AI conference flooded with peer reviews written by AI

#### [Submission URL](https://www.nature.com/articles/d41586-025-03506-6) | 207 points | by [_____k](https://news.ycombinator.com/user?id=_____k) | [131 comments](https://news.ycombinator.com/item?id=46088236)

ICLR 2026 rocked by AI-written peer reviews, organizers launch probe

- After Graham Neubig (CMU) suspected chatbot-written reviews for his ICLR 2026 submission, Pangram Labs scanned the entire conference corpus: 19,490 papers and 75,800 reviews.
- Findings: about 21% of peer reviews were flagged as fully AI-generated, and more than half showed signs of AI use. On the submissions side, 1% of manuscripts were deemed fully AI-generated; 9% had more than 50% AI-generated text; 61% were mostly human-written.
- Red flags authors reported included hallucinated citations, long and vague bullet-point feedback, incorrect numerical claims, and non-standard analysis requests.
- Pangram used its in-house detector, described in a preprint submitted to ICLR—whose own reviews included at least one that Pangram’s tool flagged as fully AI-generated.
- ICLR organizers say they will now use automated tools to assess possible policy breaches in submissions and peer reviews. Senior program chair Bharath Hariharan called it the first time the conference has faced the issue at scale.
- Researchers like Desmond Elliott (University of Copenhagen) say AI-written reviews are affecting outcomes; one flagged review gave his paper the lowest rating, leaving it borderline for acceptance.
- The episode underscores mounting pressure on peer review and raises questions about detection reliability, enforcement, and how much AI assistance—if any—should be acceptable.

Here is a summary of the discussion on Hacker News:

**Skepticism regarding AI Detectors:**
A significant portion of the discussion challenged the reliability of AI detection tools, with several users viewing the article as a PR stunt for Pangram Labs. Commenters argued that detectors suffer from high false-positive rates (citing instances where historical texts like the Declaration of Independence were flagged as AI) and that detecting LLM output is theoretically impossible as models improve.

**Pangram Co-founder Response:**
A user identifying as a Pangram co-founder ("mxspr") actively defended their technology in the comments. They distinguished their deep-learning approach from simpler "perplexity-based" methods used by competitors. They cited recent research claiming near-zero false positives on human documents and argued that skepticism is often based on outdated benchmarks, though other users continued to press for clean, public datasets to verify these claims.

**Systemic Issues in Academia:**
The conversation shifted from technical detection to the culture of peer review. Users argued that the prevalence of AI reviews signals a "freefall" in academic standards. Commenters described a "maximum extraction" mindset where researchers and reviewers utilize shortcuts to game the system, exacerbated by "reviewer rings" and a loss of professional duty. Some suggested that AI is merely automating a lack of care that already existed.

**The "Arms Race":**
Users noted the adversarial nature of the problem, predicting that better detection tools will simply encourage model providers (like OpenAI) to train models specifically to evade detection, ensuring that AI writing eventually becomes indistinguishable from human text.

### Show HN: Zero-power photonic language model–code

#### [Submission URL](https://zenodo.org/records/17764289) | 15 points | by [damir00](https://news.ycombinator.com/user?id=damir00) | [5 comments](https://news.ycombinator.com/item?id=46089764)

Entropica: an open photonic language model with “zero-power” optical inference

- What’s new: A 1024‑mode, 32‑layer unitary network whose entire forward pass can be realized as a passive linear‑optical interferometer (Reck MZI mesh). Tokens are sampled via a Born‑rule readout. Trained on TinyStories to produce coherent outputs in under 1.8 hours on a single laptop GPU.

- Why it matters: Inference can, in principle, run with no active electronics in the compute path (passive optics), offering near–speed-of-light latency and dramatically lower energy use compared to electronic hardware. The authors show a practical path to hardware with printed phase masks and even a $30 laser pointer.

- How it works: 
  - The model uses strictly unitary layers implementable by MZI meshes; sampling uses the Born rule over output intensities.
  - Training is conventional (Python/GPU); inference can be done optically by setting phase shifts and letting light propagate through the mesh.
  - All code, weights, and dataset generation scripts are public.

- Results: Learns TinyStories-style generation and demonstrates that a fully passive, linear-optical forward pass is viable for small generative models.

- Caveats: This is a TinyStories-scale demo, not a general LLM. “Zero-power” refers to the passive interferometer; a light source and detection still consume power. Scaling, noise, IO/memory, and broader benchmarks remain open questions.

- Links:
  - Repo: https://github.com/dwallener/EntropicaPublic
  - Tech note (PDF, CC BY 4.0): https://doi.org/10.5281/zenodo.17764289

**Discussion Summary:**

Commenters focused on the practical realities behind the "zero-power" claim, noting that while the inference path is passive, significant energy is still required for the light source and converting information between electronic and optical domains. Technical questions arose regarding how non-linearity—essential for neural networks—is achieved in a linear optical system; the explanation provided is that non-linearity occurs at the detection stage via intensity measurement ($E^2$). Skepticism remained regarding physical viability, with users warning that translating simulations to reliable optical hardware is notoriously difficult and questioning the architecture's ability to scale to useful models like GPT-2.

### Users brutually reject Microsoft's "Copilot for work" in Edge and Windows 11

#### [Submission URL](https://www.windowslatest.com/2025/11/28/you-heard-wrong-users-brutually-reject-microsofts-copilot-for-work-in-edge-and-windows-11/) | 85 points | by [robtherobber](https://news.ycombinator.com/user?id=robtherobber) | [28 comments](https://news.ycombinator.com/item?id=46087333)

Windows Latest highlights growing frustration with Microsoft’s AI-first direction in Edge and Windows 11. Copilot Mode—Microsoft’s agentic browsing experience akin to Perplexity’s Comet or ChatGPT’s Atlas—is now the default UX in Edge (you can turn it off in settings). Microsoft pitches it as “AI browsing that’s safe for work,” promising automated multi-step workflows and “multi-tab reasoning” that pulls from up to 30 tabs.

The reception on X has been harsh. Longtime Windows users and IT admins say no one asked for deeper Copilot integration and want ways to remove it. Microsoft’s social accounts are amplifying praise while largely ignoring critical replies, per the report. The piece also notes Microsoft plans to hide the “AI can make mistakes” disclaimer because some users found it distracting—despite ongoing accuracy concerns.

This follows earlier backlash to “agentic” Windows features, after which a Windows exec locked replies on social posts; Microsoft is now testing agent invocation from the taskbar. Meanwhile, Microsoft AI chief Mustafa Suleyman defended the AI push, arguing critics are underestimating progress, likening it to moving from Nokia Snake to today’s generative systems.

Why it matters: Default-on AI agents, muted disclaimers, and mixed accuracy claims risk eroding user trust—especially among power users and enterprises who didn’t ask for it. Source: Windows Latest

**The Exodus to Linux:** The overwhelming sentiment in the discussion is that Microsoft’s aggressive AI integration and "hostile" user experience changes are finally driving power users—particularly gamers—to switch to Linux. Commenters cite the "Year of the Linux Desktop" becoming a reality not through marketing, but because of Microsoft's alienation of its user base combined with Valve's heavy lifting on Linux gaming compatibility (Proton/SteamOS). Specific distributions mentioned as viable alternatives include CachyOS, Nobara, and Ubuntu.

**Enterprise vs. Consumer Bloat:** A sub-thread debated whether users should simply use Enterprise or LTSC (Long-Term Servicing Channel) versions of Windows to avoid "consumer" annoyances like Copilot and ads. However, others countered that acquiring these licenses as an individual is intentionally difficult, requiring resellers or minimum order quantities, validating the feeling that Microsoft does not offer a clean product to individual consumers.

**Echo Chambers vs. Utility:** While the majority expressed deep frustration with performance degradation and dark patterns (such as forced OneDrive syncing or Edge defaults), a contrarian voice argued that the backlash represents a power-user bubble; they posited that for the average corporate worker, Copilot is actually a useful productivity tool for summarizing and tedious tasks. Rebuttals focused on the lack of trust, arguing that even if the tool is useful, Microsoft’s history of overriding user preferences makes the integration unwelcome.

### I Know We're in an AI Bubble Because Nobody Wants Me

#### [Submission URL](https://petewarden.com/2025/11/29/i-know-were-in-an-ai-bubble-because-nobody-wants-me-%f0%9f%98%ad/) | 84 points | by [iparaskev](https://news.ycombinator.com/user?id=iparaskev) | [62 comments](https://news.ycombinator.com/item?id=46086410)

Pete Warden: The real AI bubble is in hardware, not software efficiency

- Warden (Jetpac cofounder, ex-Google/TensorFlow mobile lead) recounts how chasing efficiency—first to run AlexNet cheaply at scale, then to push inference onto phones—has always been where the biggest, most durable wins come from.
- He argues today’s AI spending is badly misallocated: hundreds of billions are flowing into GPUs, data centers, and power, while ML infrastructure/optimization teams struggle to raise money—even though GPU utilization is often under 50% (and much worse for interactive, small-batch, memory-bound workloads).
- There are well-known headroom and alternatives: hand-tuned kernels can beat vendor libraries; many inference loads can run far cheaper on CPUs; and efficiency brings real environmental benefits.
- So why the hardware arms race? Incentives and signaling. Buying GPUs is an easy-to-measure moat story, simpler to manage than deep software work, and it flatters investors’ narratives—“nobody ever got fired for buying IBM,” now transposed to OpenAI-scale capex.
- His company, Moonshine, has had to swim against that tide but expects to be cashflow-positive in Q1 2026; he believes the rational bet for any firm burning billions on GPUs is to invest hundreds of millions in software efficiency.

Why it matters
- If Warden is right, the next big AI gains won’t come from more H100s but from ruthless, full-stack optimization—model, runtime, kernel, OS, hardware choices—unlocking higher utilization, lower cost, and lower power. The market may be rewarding the wrong moat.

Based on the discussion, here is a summary of the comments:

**The Rise of "Assetocracy" vs. Local AI**
A significant portion of the discussion focuses on the shifting dynamics of power in tech. Commenters coined the term "assetocracy" to describe the current state where those with capital (access to expensive assets like H100s) control the market, replacing the traditional software "meritocracy." However, users noted that if Warden is right and efficiency creates viable "local AI," the business case for centralized cloud monopolies could evaporate, returning control to widespread, decentralized hardware.

**Infrastructure Plays and the "Bubble" Debate**
Users debated whether the current hardware spending is a bubble or a strategic necessity.
*   **The Long Game:** Some argued that buying infrastructure isn't just about today's needs, but about controlling the industry five years from now.
*   **Supply vs. Demand:** Users cited Microsoft CEO Satya Nadella’s statements that hyperscalers are "capacity constrained" (they have more demand than chips), suggesting the spending is justified.
*   **Skepticism:** Others remained skeptical of CEO narratives, debating whether big tech companies are signaling growth to shareholders rather than fulfilling genuine technical requirements, though some noted that lying to shareholders carries significant legal risk (SEC).

**Cultural Aversion to Optimization**
Commenters largely validated Warden’s observation that companies prefer buying hardware over optimizing software.
*   **The "Sun Microsystems" Parallel:** One user compared this to the dot-com boom, where startups burned VC money on expensive Sun servers rather than optimizing code.
*   **Management Ease:** Users noted that paying a cloud bill is easier for management than hiring and directing engineers to perform deep optimization work. It is often seen as "banging your head against a wall," and companies prefer releasing features over saving computing resources.

**Technical Viability and Market Signals**
There was debate regarding the technical feasibility of Warden's claims.
*   **CPU vs. GPU:** Some questioned the reality of running modern LLMs on CPUs, though others clarified that Warden’s focus is on specific inference tasks (speech recognition, translation) rather than massive model training.
*   **Plummeting Costs:** Counter to the idea that efficiency is being ignored, some users pointed out that token prices are dropping rapidly and models are becoming cheaper to run, suggesting that ruthless optimization is arguably already happening at the hyperscaler level to protect margins.

---

## AI Submissions for Fri Nov 28 2025 {{ 'date': '2025-11-28T17:09:37.563Z' }}

### So you wanna build a local RAG?

#### [Submission URL](https://blog.yakkomajuri.com/blog/local-rag) | 348 points | by [pedriquepacheco](https://news.ycombinator.com/user?id=pedriquepacheco) | [89 comments](https://news.ycombinator.com/item?id=46080364)

TL;DR: Skald shows you can stand up a fully local, open‑source RAG that’s fast to deploy, privacy‑preserving, and surprisingly competitive with cloud stacks—though model and DB choices still warrant benchmarking.

What they built (all-OSS, self-hostable):
- Vector DB: Postgres + pgvector (kept stack simple; fine up to “hundreds of thousands” of docs; they’ll benchmark vs Qdrant/Weaviate)
- Embeddings: Sentence Transformers (all‑MiniLM‑L6‑v2 default; also tested bge‑m3 for multilingual/larger)
- Reranker: ST cross‑encoder by default; also bge‑reranker‑v2‑m3 and mmarco‑mMiniLM variants (multilingual support)
- LLM: User‑provided; tested GPT‑OSS 20B via llama.cpp on an EC2 g5.2xlarge
- Document parsing: Docling via docling‑serve

Why it matters:
- For privacy‑sensitive orgs, “local RAG” avoids sending data to third parties without giving up modern LLM workflows.
- They favor OSS for every component, but note self‑hosting proprietary tools is also possible.

Deployment notes:
- Production instance (Skald + Postgres + embeddings + reranker + Docling) spun up in ~8 minutes; only the LLM was separate.
- Test corpus: full PostHog website; queries demanded aggregation across 15+ documents.
- Aggressive retrieval to favor accuracy: vector topK=100, rerank topK=50, distance threshold=0.8, no query rewriting, concise answers, references off.

Early results (not a formal benchmark):
- Cloud control (Voyage + Claude Sonnet 3.7): voyage‑3‑large embeddings + rerank‑2.5. LLM‑as‑a‑Judge score 9.45 average; answers correct with minor missing context in one case.
- Hybrid local (Voyage embeds/rerank + GPT‑OSS 20B via llama.cpp): qualitatively “did great,” comparable to the cloud LLM on their small eval. They don’t yet support LLM‑as‑a‑Judge on fully local runs, so no numeric score here. More benchmarks coming.

Takeaways/trade‑offs:
- A practical local RAG stack exists today with solid quality: Docling for parsing, ST/BGE for embeddings/reranking, pgvector for simplicity, and an OSS LLM served via llama.cpp.
- Postgres+pgvector is convenient but may be contentious for scale/perf; they plan head‑to‑head tests with specialized vector DBs.
- Model choices depend on language needs (ST defaults are English‑only; BGE adds multilingual).
- Fully local judging/eval isn’t ready yet in their setup; broader benchmarks are planned.

Who should care:
- Teams with strict privacy/compliance needs who want RAG without external APIs.
- Builders evaluating whether Postgres+pgvector is “good enough” vs Qdrant/Weaviate.
- Anyone comparing frontier cloud LLMs vs modern OSS models for retrieval‑augmented tasks.

The discussion focused heavily on the architectural trade-offs between simple lexical search (keywords) and the vector-based approach used in the submission, alongside debates on efficiency and complexity.

**Key themes included:**

*   **Vector DBs vs. "Agentic" Search:** Simon Willison argued that dedicated vector databases are often overkill; he suggested using standard Full Text Search (FTS) combined with an "agentic loop" where the LLM refines search queries multiple times to handle synonyms (e.g., "dog" vs. "canine"). Critics pushed back strongly, arguing that multiple LLM inference rounds are inefficient and slow compared to a single optimized vector lookup, and that vector search is essential for discovering concepts when the user lacks specific terminology (e.g., medical diagnoses).
*   **Precision vs. Recall:** Commenters characterized the split as a trade-off between **Precision** (Lexical/BM25) and **Recall** (Semantic/Embeddings). While semantic search handles typos and synonyms (e.g., "J Lo" mapping to "Jennifer Lopez") better, lexical search is superior for exact "Ctrl+F" style matches.
*   **The Hybrid Consensus:** Several users, referencing diverse benchmarks and Anthropic’s "Contextual Retrieval" research, suggested that neither pure semantic nor pure lexical is sufficient. The consensus skewed toward **Hybrid Search** (combining BM25 and Embeddings) followed by a generic Re-ranker step as the "gold standard" for accuracy.
*   **Chunking Strategy:** There was skepticism regarding how the submission handles documents. One user feared that embedding entire documents dilutes specific concepts; the OP clarified that the system aggregates specific context from multiple documents rather than just embedding whole files blindly.
*   **Engineering "Worth":** A minor debate emerged regarding whether the marginal improvement of semantic search is worth the engineering overhead compared to valid SQL/FTS setups, with some users favoring pre-packaged solutions (like Typesense) to bridge the gap.

### Show HN: An LLM-Powered Tool to Catch PCB Schematic Mistakes

#### [Submission URL](https://netlist.io/) | 48 points | by [wafflesfreak](https://news.ycombinator.com/user?id=wafflesfreak) | [26 comments](https://news.ycombinator.com/item?id=46080737)

Netlist.io is pitching an AI copilot for electrical engineers that catches schematic mistakes before you fab a board. You upload a netlist from KiCad or Altium plus the relevant component datasheets, and it runs “sanity checks” by cross-referencing connections against what the PDFs say—then you can poke it via a chat UI for follow-ups like “what’s R21’s value?” The tool emphasizes it doesn’t see your PCB layout; any layout suggestions are logic-only. There’s a free, no–credit card–required try-now flow, with visible per-chat usage/rate-limit meters and the usual “AI can make mistakes” disclaimer.

Why it’s interesting
- Aims to bridge the gap between built-in ERC and real-world datasheet constraints, potentially catching costly pin swaps, power/ground issues, missing pulls, voltage mismatches, and similar gotchas before a spin.
- Uses LLM-style reasoning over your actual datasheets, not generic models, which could reduce hallucinations on part specifics.
- Chat interface may speed up schematic review and quick queries during design.

Caveats and limits
- “AI can make mistakes”; treat results as advisory and verify critical paths.
- No layout visibility, so placement/return-path/EMI issues won’t be caught.
- Appears focused on KiCad and Altium; other EDA support isn’t mentioned.
- Token/usage limits per chat could throttle heavy sessions; privacy posture around uploaded datasheets/netlists will matter to teams.

Bottom line: A promising ERC-plus assistant that tries to read your datasheets so you don’t miss simple-but-expensive errors. Worth a trial on a non-critical design to see if it flags issues your current flow misses.

Here is a summary of the discussion:

The Hacker News community approached the tool with typical engineering skepticism, primarily focusing on the reliability of LLMs for hardware verification where precision is paramount.

**Accuracy and Benchmarking**
Users like **swtcdr** and **achr2** demanded performance data (false positive/negative rates), arguing that while catching a mistake saves money, chasing down AI hallucinations (false flags) wastes expensive engineering time. The creator (**wfflsfrk**) acknowledged that benchmarking is difficult but emphasized the tool is intended to complement, not replace, human judgment—functioning like a "pre-flight checklist" to catch silly errors.

**Real-World Testing**
Several users tested the tool on the spot with mixed-to-negative results:
*   **bArray** reported that the AI hallucinated features for an IC (confusing a simple chip for an MCU) even when provided the correct datasheet.
*   **scttptms**, a professional designing high-density boards, ran the tool against a design with known bugs. They reported the tool failed to identify critical issues like swapped gain resistors, incorrect voltage divider current draws, and power rating mismatches.

**Scope Limitations**
**ProllyInfamous** shared an anecdote about a costly physical connector error, asking if the tool could catch physical mismatches. The creator clarified that Netlist.io is strictly for schematic/logic analysis (e.g., swapped TX/RX) and cannot see physical layout or footprint data, meaning 3D interference or physical pin-ordering errors are out of scope.

**Implementation and Pricing**
There was debate regarding the workflow. **throwaway31131** noted that datasheets are notoriously inconsistent, making ingestion difficult. **vrptr** suggested that once the AI finds an error type, it should be converted into a deterministic code rule rather than relying on the LLM for future checks. Pricing was also a concern for hobbyists, though professional users noted that avoiding even one PCB respin would justify significant costs.

### Anti-patterns while working with LLMs

#### [Submission URL](https://instavm.io/blog/llm-anti-patterns) | 71 points | by [mkagenius](https://news.ycombinator.com/user?id=mkagenius) | [24 comments](https://news.ycombinator.com/item?id=46080597)

After 15 months building with LLMs, Manish outlines common failure modes that waste tokens, tank accuracy, or create security footguns—and offers practical fixes.

Key takeaways
- Stop resending the same context: Treat context like a scarce budget. Don’t stream near-identical screenshots every step; send only state-changing frames. He built an open-source tool (“click3”/clickclickclick) to dedupe screenshots.
- Ask models to do what they’re good at: If precision matters (counting letters, strict formatting, tool orchestration), have the model write code that does the job instead of doing it “in its head.” Code > brittle prompting. Cloudflare’s “code mode” is cited as validation.
- Long sessions degrade performance: As contexts approach or exceed 128k tokens, models forget or invent details. Externalize state, checkpoint critical facts, and periodically refresh concise summaries rather than trusting the model’s own compression.
- Obscure or post-cutoff topics underperform: Expect lower accuracy on niche or new material. Ground the model with up-to-date docs, RAG, or explicit examples. Even well-documented integrations (e.g., Stripe) can trip assistants without proper grounding.
- Don’t “vibe-code”: Actively review generated code and schemas. A real mishap: an invoices API response included a User object with password_hash—an unnecessary exposure that could aid attackers. Maintain code ownership and threat-model outputs.

Why it matters
- Better token hygiene, tool use via code, and human-in-the-loop review lead to cheaper, more reliable, and safer LLM systems.

References: Anthropic context management; instavm/clickclickclick and coderunner; Cloudflare’s code mode. The post ends with a waitlist for “secure AI agents.”

The discussion around the article was mixed; while some critics dismissed the post as vague or a veiled advertisement for the author’s tool (`click3`), others shared detailed anecdotes confirming the importance of context hygiene and human review.

**Key Themes and Debates:**

*   **Context Engineering Wins:** One user validated the need for "resourceful" context by sharing their experience with the complex SolidWorks SDK. After struggling with hallucinations, they manually crawled the documentation, translated it to Markdown, and optimized it for "grokability," which reduced their debugging loop from 15+ rounds to just one or two.
*   **The "Wrong Instance" Danger:** A harrowing anecdote highlighted the risks of "vibe-coding" without scrutiny. A developer described an incident where Claude Code hallucinated a configuration line, swapping a development database URL for a shared production instance. They nearly wiped the wrong database, sparking a sub-thread about the importance of separating configuration files and not connecting local AI tools to production environments.
*   **Reasoning vs. Correlation:** A debate emerged regarding whether expecting "logical reasoning" from LLMs is an anti-pattern in itself. While skeptics argued LLMs are only capable of data correlation and symbol manipulation, proponents countered that LLMs serve as effective "rubber ducks" that offer alternative perspectives, provided the user verifies the output.
*   **Framework Fatigue:** Several builders expressed frustration with current LLM frameworks/SDKs, describing them as over-engineered and hard to debug. These users preferred building their own lightweight tools that handle specific, small, and deterministic contexts.
*   **Practical Use Cases:** Beyond code generation, users reported success using LLMs to compare API implementations to find subtle bugs in sprawling codebases, analyze unfamiliar legacy code (e.g., Swift/Obj-C), and optimize file compression settings.

### Codex, Opus, Gemini Try to Build Counter Strike

#### [Submission URL](https://www.instantdb.com/essays/agents_building_counterstrike) | 35 points | by [stopachka](https://news.ycombinator.com/user?id=stopachka) | [6 comments](https://news.ycombinator.com/item?id=46080835)

Stepan Parunashvili pitted Gemini 3 Pro, Codex Max 5.1, and Claude Opus 4.5 against the same brief: build a basic Counter‑Strike‑style game in Three.js, then make it multiplayer with Instant presence. Each model got ~7 consecutive prompts, starting with core gameplay and visuals, then adding networking.

Quick verdict:
- Claude Opus 4.5 dominated the frontend: best maps, characters, guns, and overall polish.
- Gemini 3 Pro excelled on backend tasks: fewer errors wiring up multiplayer, presence, and persistence.
- Codex Max 5.1 landed in the middle: competent across both fronts but less standout.

Scorecard (high-level):
- Frontend: Claude > Gemini > Codex
- Backend: Gemini > Codex > Claude

Notable moments:
- Boxes/physics: Claude’s map looked the best; Codex fixed an import bug but shipped a darker, plainer scene.
- Characters: Claude produced the most “Minecraft‑like” humanoids; Codex’s single‑color palette hurt readability.
- POV gun: Claude and Codex nailed it; Gemini struggled until they discovered the gun was transparent.
- Sounds/“death” animations: all added chiptune shots; all misread “animate deaths” as visual death animations—Claude’s was the most fun.
- Multiplayer kickoff: models switched to Instant presence for shared movement in a single room before handling shots.

Why it matters: With minimal guidance, today’s models can scaffold playable 3D multiplayer prototypes, but strengths diverge—Claude for visual/design fidelity, Gemini for logical/system changes, Codex as a solid generalist.

Try the live demos:
- Codex Max 5.1: https://cscodex.vercel.app/
- Claude Opus 4.5: https://csclaude.vercel.app/
- Gemini 3 Pro: https://csgemini.vercel.app/

**Discussion Summary:**

Development and copyright concerns largely defined the discussion. **BearOso** analyzed the unminified source code and alleged that Gemini's sky shader appeared to be "regurgitated verbatim" from an existing algorithm (specifically citing "Preetham"), raising questions about whether the output would hold up under licensing scrutiny.

Other users focused on the nostalgia and future possibilities of the prototypes:
*   **vpShane** compared the "retro" aesthetics to Quake or Doom, noting they would happily play a lo-fi "demake" of Counter-Strike if the mechanical skill gaps were preserved.
*   **rrtrn** speculated on the potential power of an LLM trained specifically on Unreal Engine.
*   **wrmplld** expressed a personal preference for the Gemini build and felt inspired to transition from 2D to 3D game development.
*    Feedback was also provided on the article's layout, with **vndrb** requesting larger images or lightbox functionality.

### AI Adoption Rates Starting to Flatten Out

#### [Submission URL](https://www.apolloacademy.com/ai-adoption-rates-starting-to-flatten-out/) | 184 points | by [toomuchtodo](https://news.ycombinator.com/user?id=toomuchtodo) | [142 comments](https://news.ycombinator.com/item?id=46079987)

AI adoption may be hitting an early plateau. Apollo’s chief economist Torsten Sløk says bi‑weekly Census Bureau survey data and Ramp’s AI Index both show adoption rates flattening across small, mid, and large firms after a rapid run-up. The analysis uses a six-survey moving average; Ramp’s index is based on spend data from 40,000+ US businesses using its card/bill-pay platform.

Why it matters: a slowdown suggests the easy wins and pilot phase may be behind us, with the next leg requiring deeper integration, workflow changes, and clearer ROI—potentially tempering near‑term growth expectations for AI vendors.

Caveats: spend is a proxy for adoption, moving averages lag turning points, and sample bias (Ramp users) could skew results.

Based on the discussion, the feedback mirrors the submission's data, suggesting that individual developers are hitting their own "productivity plateaus" and reconsidering the role of AI in their workflows.

**The "Joy of Coding" and Disillusionment**
The thread was sparked by a user (`lmf4lol`) who drastically cut their AI usage after finding the output brittle and the correction process exhausting. They argued that relying on Large Language Models (LLMs) detached them from the "artistic aspect" of creation, leading to alienation from their work. Returning to manual programming (using Vim) reportedly restored their sense of joy and mental well-being without negatively impacting their actual productivity.

**Skill Degradation vs. Augmentation**
A major point of concern was "skill rot." Commenters reasoned that coding, like speaking a foreign language, degrades without practice.
*   **Atrophy:** Users warned that offloading too heavily to agents prevents developers from internalizing the logic necessary to review the AI's work effectively. One user suggested a "50/50" schedule (one week with AI, one week without) to maintain proficiency.
*   **The Dumb Intern:** Several described using AI not as a senior partner, but as a "dumb colleague" or intern that forgets context and produces broken code, often requiring more energy to manage than doing the work manually.

**The "Lazy" Trap and Quality Concerns**
Skeptics questioned the actual economic productivity of AI. `vnls` noted that if AI were truly a massive force multiplier, we would see a surge in software quality and distinct startups; instead, the market is seeing "uninspired wrappers" and questionable stability. Others argued that AI is currently useful primarily for "laziness"—saving mental energy on plumbing and boilerplate—rather than solving hard problems.

**Is this just the new IDE?**
The discussion drew parallels to historical shifts in tooling:
*   **Tool Evolution:** `lphnrd` and other supporters compared current AI skepticism to the early resistance against IDEs and StackOverflow. They argued that while "Agents" might be overhyped currently, AI features will eventually settle into standard workflows (documentation, linting, low-level support) just as syntax highlighting did.
*   **Specific Utility:** Most agreed AI remains useful for specific, low-stakes tasks—generating skeletons, writing "boring" extensions, or acting as a "calculator for words" (summarizing/translating)—but fails when tasked with creative or non-conventional logic.

### Beads – A memory upgrade for your coding agent

#### [Submission URL](https://github.com/steveyegge/beads) | 106 points | by [latchkey](https://news.ycombinator.com/user?id=latchkey) | [68 comments](https://news.ycombinator.com/item?id=46075616)

Beads is Steve Yegge’s new “memory upgrade” for coding agents: a git-backed, graph-based issue tracker that agents use instead of dumping plans into markdown. The pitch: treat your repo like a shared, queryable database so agents can plan long-horizon work, chain tasks with dependencies, and reliably pick up where they left off across sessions, branches, and machines.

What’s new in v0.20.1
- Hash-based issue IDs (bd-a1b2, bd-f14c…) replace sequential numbers to avoid merge conflicts, enabling reliable multi-agent/multi-branch workflows.
- Optional migration via bd migrate; old databases still work.
- Short hashes scale length (4/5/6 chars) to keep collisions vanishingly rare.

Why it’s interesting
- Agents, not humans, file and manage issues, with four dependency types (blocks, related, parent/child, discovered-from) and automatic “ready work” detection.
- Git as the transport: JSONL records versioned in git act like a distributed SQL-ish store shared by all agents on a repo.
- Agent-friendly CLI (bd) with JSON output, dependency tree views, full audit trail, and protected-branch support.
- “Agent Mail” optional fast path for real-time coordination (<100ms) with far less git traffic.
- Extensible via SQLite if you want custom tables; per-project isolation is automatic.

Status and setup
- Alpha; APIs may change before 1.0.
- MIT-licensed. Install with a one-liner; add instructions in your AGENTS.md so your coding agent uses bd for all new work.

TL;DR: Think “Jira for AIs, but git-native and lightweight,” aimed at fixing agents’ long-term planning and context limits while playing nicely with multi-branch, multi-agent dev workflows.

**Agent Experience (AX) vs. Anthropomorphism**
The thread opened with amusement over the submission’s claim that agents "report enjoying" the tool. While some dismissed this as marketing fluff or anthropomorphism (*skybrn*, *dude250711*), others argued that "Agent Experience" (AX) is a legitimate design discipline. *pbw* and *ththmbl* noted that since LLMs are stateless, building interfaces that cater to their specific limitations (context window, need for structure) is logical, separate from any notion of sentience.

**Structured State vs. Markdown Files**
*wwmt* questioned why a distributed system is better than agents simply updating a `PLAN.md` file. *smnw* (Simon Willison) explained that while LLMs treat files as a blank slate, Beads provides structured JSONL that mimics the issue-tracking systems LLMs were trained on, offering better context retention. User *qdt* reported that Beads is superior to manual Markdown files for long-horizon tasks because it handles dependencies and allows agents to "pick up" work across sessions without context loss.

**Git History and Branching Strategies**
A technical debate emerged regarding where the `beads` data directory should live. *smnw* expressed concern that high-frequency automated updates to JSONL files would pollute the main branch's git history, suggesting an orphan branch instead. *mnks* and *wwmt* countered that storing the plan alongside the code is a feature, ensuring the issue state represents a snapshot consistent with the code at that specific commit.

**Alternatives and Criticism**
Several users suggested existing tools might suffice:
*   *mbnrjplmr* suggested **Taskwarrior**, noting LLMs already know its syntax.
*   *ctktch* and *_joel* advocated for using the GitHub CLI (`gh`) to let agents manage standard GitHub issues/PRs.
*   *thmgt* critiqued the Beads repository itself, describing it as "slop" where the issue tracker state (`beads.jsonl`) seemed desynchronized from the actual codebase (e.g., referencing deleted architecture files), raising doubts about the tool's reliability.

### A Tale of Two AI Failures: Debugging a Simple Bug with LLMs

#### [Submission URL](https://bitmovin.com/blog/hackathon-debugging-ai-tools-llms/) | 9 points | by [slederer](https://news.ycombinator.com/user?id=slederer) | [12 comments](https://news.ycombinator.com/item?id=46083162)

A Bitmovin engineer spent two days integrating the FoxESSCloud API and discovered both Cursor and Claude failed on the same subtle requirement: building an HMAC signature string where certain newline characters had to exist as literal characters in the initial segment, not added via concatenation. Cursor fixated on hashing/encoding tweaks and never questioned the input string structure, returning “illegal signature” endlessly. Claude, meanwhile, confidently hallucinated a system clock issue (claiming the machine was a year ahead) before producing the same flawed concatenation. The fix was simply to embed the first newlines inside a literal string (e.g., "POST\n/api/v1/query\n" + token + "\n" + ts + ...), highlighting how LLMs struggle with byte-perfect protocols.

Why it matters:
- Shared blind spots: LLMs default to common patterns that can be wrong for proprietary signing schemes.
- Failure modes differ: one tool stalled quietly, the other misled with confident fiction.
- Practical lesson: for signatures and wire formats, print and compare exact bytes (or hex), build minimal test vectors, and verify against canonical examples—don’t trust eloquence over precision.

**Discussion Summary:**

Commenters were highly skeptical of the technical premise presented in the article, with several users arguing that the author's explanation of the bug makes little sense. Key points from the discussion include:

*   **Technical Confusion:** Multiple participants noted that in almost universally used programming languages, concatenating a string with a newline (`"POST" + "\n"`) results in the exact same bytes in memory as a literal string (`"POST\n"`). Consequently, they found the author's claim—that concatenation caused the signature failure—to be logically flawed.
*   **Alternative Theories:** Users suggested the actual bug was likely due to unescaped double quotes in the JSON body or "smart quotes" (Unicode U+201C) introduced via copy-pasting, rather than the newline concatenation issue identified by the author.
*   **Article Quality:** Some users speculated that the article itself might be AI-written or poorly communicated, as the code examples provided didn't clearly demonstrate the claimed "byte-level" issue.
*   **LLM & Debugging Strategy:** Regarding the AI failure, one user suggested that clearing the context window is essential when LLMs enter a logic loop. Others emphasized that for signature bugs, developers should rely on hexdumps to diff strings rather than trusting visual inspections.

### TPUv7: Google Takes a Swing at the King

#### [Submission URL](https://newsletter.semianalysis.com/p/tpuv7-google-takes-a-swing-at-the) | 29 points | by [pella](https://news.ycombinator.com/user?id=pella) | [3 comments](https://news.ycombinator.com/item?id=46079388)

- Big picture: SemiAnalysis argues Google’s TPU stack has matured into a credible alternative to Nvidia for frontier training and inference—and Google is now selling TPUs as hardware to external firms, not just via GCP.
- Evidence: Gemini 3 (trained entirely on TPUs) and Anthropic’s Claude 4.5 Opus (TPUs/Trainium) anchor the claim that top-tier models no longer depend on Nvidia GPUs. Anthropic is reportedly building >1GW of TPU capacity, with Meta, SSI, xAI, and even OpenAI in the target pipeline.
- Procurement leverage: “The more TPUs you buy, the more GPU capex you save.” Even without deploying TPUs, OpenAI allegedly cut lab‑wide Nvidia compute costs by ~30% thanks to competitive pressure—suggesting real price/TCO relief when credible non‑CUDA options exist.
- Nvidia on the defensive: A wave of wins for Google (TPU volume upsizing, marquee customers, SOTA results) triggered a reassessment of the TPU supply chain vs. Nvidia’s. Nvidia issued reassuring messaging and rebutted the “circular economy” critique that it props up demand via startup funding.
- Systems over cores: The piece reiterates that in AI software, infra choices dominate capex/opex and margins. Google’s edge stems from cohesive silicon, system, and software co‑design rather than raw microarchitecture alone.
- Software moat remains: To truly challenge CUDA, Google still needs to open source key parts of the TPU stack—XLA:TPU compiler/runtime and multi‑pod “MegaScaler”—despite recent ecosystem progress.
- History and strategy shift: Google started TPU development after a 2013 realization that AI at scale would otherwise double its datacenter footprint; TPUs entered production in 2016. Now, with TPUv7 in market and next‑gen TPUv8AX/8X (aka Sunfish/Zebrafish) coming, Google is moving from internal use to full commercial push.
- What’s paywalled: A deeper moat analysis for Nvidia and a comparison of Nvidia’s upcoming Vera Rubin platform versus TPUv8AX/8X.

Why it matters: For AI builders and buyers, viable TPU and Trainium alternatives weaken CUDA lock‑in, improve pricing, and broaden supply. If Google open-sources the critical TPU software layers, the competitive dynamics—and your TCO—could shift even more.

**TPUv7: Google takes a swing at Nvidia’s moat**
*   **The Story:** SemiAnalysis argues that Google’s TPU stack has matured into a credible threat to Nvidia’s dominance in frontier training and inference. With top-tier models like Gemini 3 and Claude 4.5 Opus relying on TPUs, and reports that Google is now selling hardware directly to external firms, the piece suggests this competition is already forcing Nvidia to adjust pricing.
*   **The Discussion:** Commenters focused on the specifics of Google's distribution strategy and the broader market implications.
    *   **Chips vs. Cloud:** User `londons_explore` sought clarification on the report's claim, asking if Google is truly shipping physical silicon to customers or simply selling reserved cloud instances (GCP) under a new model.
    *   **Breaking the Monopoly:** Highlighting frustration with Nvidia's market control, `bbjff` expressed hope that Google's push creates a dent in the ecosystem. They suggested that licensing the TPU architecture to other suppliers (similar to Broadcom) would be the ideal path to democratization.
    *   **Context:** `ChrisArchitect` linked to previous HN discussions regarding the long-term race between TPUs and GPUs and Google's "Ironwood" project.