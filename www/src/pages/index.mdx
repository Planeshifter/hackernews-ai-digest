import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Thu Sep 25 2025 {{ 'date': '2025-09-25T17:16:23.099Z' }}

### Improved Gemini 2.5 Flash and Flash-Lite

#### [Submission URL](https://developers.googleblog.com/en/continuing-to-bring-you-our-latest-models-with-an-improved-gemini-2-5-flash-and-flash-lite-release/) | 519 points | by [meetpateltech](https://news.ycombinator.com/user?id=meetpateltech) | [261 comments](https://news.ycombinator.com/item?id=45375845)

Google ships faster, cheaper Gemini 2.5 Flash/Flash-Lite previews, plus a “-latest” alias

What’s new
- Updated previews of Gemini 2.5 Flash and 2.5 Flash-Lite on Google AI Studio and Vertex AI.
- Big focus on efficiency: 50% fewer output tokens for Flash-Lite and 24% fewer for Flash, driven by reduced verbosity.
- Quality upgrades:
  - Flash-Lite: better instruction-following, tighter answers, stronger multimodal (audio transcription, image understanding) and translation.
  - Flash: improved agentic/tool use with a 5% gain on SWE-Bench Verified (48.9% → 54%), and better cost-efficiency “with thinking on.”

Why it matters
- Lower latency and cost for high-throughput apps; fewer tokens out means real savings.
- Stronger tool use and multi-step reasoning push agents closer to production viability.
- Multimodal and translation boosts broaden use cases (voice, vision, global apps).

Try it
- Preview model IDs: gemini-2.5-flash-preview-09-2025, gemini-2.5-flash-lite-preview-09-2025.
- New rolling aliases: gemini-flash-latest, gemini-flash-lite-latest (always point to newest).
  - Caveat: rate limits, cost, and features can change behind -latest; Google promises 2 weeks’ email notice before updates/deprecations.
- For stability, stick with gemini-2.5-flash and gemini-2.5-flash-lite.

Early signal
- Manus (autonomous agent startup) reports ~15% improvement on long-horizon agent tasks and strong cost-efficiency with the new Flash.

Bottom line
- This is a preview, not a new stable line, but it meaningfully cuts costs and improves agent/tool performance—good news for teams scaling LLM-powered apps without blowing up budgets.

The Hacker News discussion highlights mixed reactions to Google's Gemini 2.5 Flash/Flash-Lite updates, focusing on technical frustrations, comparisons to competitors, and usability concerns:

### Key Criticisms
1. **Reliability Issues**:  
   - Users report persistent problems like mid-sentence truncation, incomplete responses, and unreliable JSON formatting. Some compare Gemini unfavorably to Claude, GPT-4, and alternatives like GLM-4/Kimi, citing inconsistent performance despite benchmark claims.  
   - Structured output struggles (e.g., JSON, tool-calling) force workarounds like multiple API requests, seen as a "hack" compared to competitors' more reliable implementations.

2. **UI/UX Shortcomings**:  
   - Google’s AI Studio is criticized for poor usability: broken scrolling, syntax highlighting glitches, and missing basic features. Users contrast this with ChatGPT’s smoother experience despite its own flaws.  
   - Complaints about Gemini’s web interface include instability and erratic behavior during tasks like PDF generation.

3. **Versioning Confusion**:  
   - Model naming conventions (e.g., `gemini-2.5-flash-preview-09-2025`) and unclear versioning draw criticism, likened to Apple’s opaque product updates. Users find dates in IDs confusing and request clearer differentiation between versions.

### Workarounds & Alternatives  
- Some developers share solutions, like using plugins (`llm-gemini`) or adjusting prompts to mitigate truncation.  
- Alternatives like OpenAI, Anthropic’s Claude, and open-source models (e.g., GLM-4) are praised for better reliability and tooling.

### Mixed Praise  
- A few users acknowledge Gemini’s cost efficiency and potential in agentic tasks, with startups like Manus reporting ~15% performance gains.  
- The multimodal improvements (audio, vision) and translation upgrades are seen as promising for niche applications.

### Conclusion  
While Gemini’s cost and latency improvements are welcomed, persistent technical issues and poor UX dampen enthusiasm. The consensus suggests Google needs to prioritize stability, documentation, and user experience to compete with rivals effectively.

### Ollama Web Search

#### [Submission URL](https://ollama.com/blog/web-search) | 328 points | by [jmorgan](https://news.ycombinator.com/user?id=jmorgan) | [162 comments](https://news.ycombinator.com/item?id=45377641)

Ollama adds built-in Web Search API to power RAG and agents

- What’s new: Ollama now offers a native web search API with a generous free tier for individuals and higher rate limits via Ollama Cloud. The goal: help models pull fresh information, reduce hallucinations, and improve factual accuracy.
- How it works: A simple REST endpoint (/api/web_search) returns structured results (title, url, content). Deeper tool integrations are available in the official Python (ollama>=0.6.0) and JavaScript (ollama@>=0.6.0) SDKs.
- Developer ergonomics: One-liners in curl, Python, and JS make it easy to drop search into existing apps. Results come back as lightweight summaries you can pass directly into prompts or pipelines.
- Agent tooling: The SDKs expose web_search and web_fetch tools so models can autonomously search and then fetch pages for deeper reads. Example code shows a compact loop using Qwen 3 (4B) to plan, call tools, and synthesize answers.
- Long-running research: Designed to let models (including OpenAI’s gpt-oss models) conduct multi-step research workflows with programmatic tool use and result streaming.
- Why it matters: Puts a turnkey web layer into the Ollama stack, letting developers build retrieval-augmented generation, research bots, and monitoring agents without wiring up third-party search services.
- Getting started: Create an Ollama API key, then hit the REST endpoint or call client.webSearch()/ollama.web_search() from the SDKs. Cloud tier offers higher throughput if you need scale.

The Hacker News discussion on Ollama's new Web Search API highlights several key points and concerns:

1. **Legal and Licensing Concerns**:  
   - Users raised questions about the terms of service of search providers like Brave and Exa, which restrict storing, republishing, or creating derivative works from their API results. This could expose Ollama to legal risks if redistributing or caching results violates these terms.  
   - Examples include Brave’s API terms blocking competitors like Google/Bing and Exa’s strict usage policies. Some argue Ollama’s approach might breach provider agreements, risking takedowns or lawsuits.

2. **Privacy and Compliance**:  
   - Ollama’s California base subjects it to CCPA regulations, requiring compliance for handling Californian users’ data. Concerns were raised about data retention policies and whether Ollama partners with privacy-focused providers (e.g., Brave, DuckDuckGo) or relies on less transparent sources.

3. **Business Model Sustainability**:  
   - Skepticism emerged about Ollama’s long-term viability as a VC-backed, open-source-adjacent tool. Comparisons were drawn to Docker’s trajectory, with users questioning how it will monetize while competing with alternatives like **llama.cpp** and **llm-swap**, which are fully open-source and locally run.

4. **Alternative Approaches**:  
   - Some users advocated for self-hosted or local search solutions to avoid reliance on third-party APIs. Projects like YaCy, Marginalia, and custom crawlers/indexers (e.g., using Common Crawl or Postgres) were mentioned as alternatives, though challenges in quality and scalability were noted.

5. **Broader Implications**:  
   - Discussions touched on AI training ethics, with jokes about Meta’s data-scraping practices and debates over copyright compliance when using web content for LLM training.  
   - The tension between convenience (Ollama’s turnkey API) and control (self-hosted tools) underscored the trade-offs in building AI-powered applications.

In summary, while Ollama’s API simplifies RAG/agent development, the community emphasized legal risks, privacy compliance, and the sustainability of its business model, alongside enthusiasm for decentralized alternatives.

### Can a model trained on satellite data really find brambles on the ground?

#### [Submission URL](https://toao.com/blog/can-we-really-see-brambles-from-space) | 162 points | by [sadiq](https://news.ycombinator.com/user?id=sadiq) | [53 comments](https://news.ycombinator.com/item?id=45377748)

A team validating a simple bramble detector built for hedgehog habitat mapping found that high-confidence hotspots predicted from space consistently matched real, sizeable bramble patches on the ground. Using TESSERA earth representation embeddings (from Sentinel-1/2 via the geotessera library) plus iNaturalist observations, the model ensembles logistic regression with k-NN.

Highlights
- Field check: From Milton Community Centre to Milton Country Park, every high-confidence area had substantial bramble; additional hotspots on a residential plot, Fen Road (“absolute unit”), and North Cambridge’s aptly named Bramblefields.
- Strengths: Surprisingly accurate at locating large, unobscured bramble stands given the model’s simplicity.
- Limitations: Weaker on small patches under partial canopy—consistent with what satellite-derived embeddings can “see.”
- Practical notes: Map overlays were hard to use in bright sunlight; re-running the model in the park on a laptop was impractical.
- Next steps: Gather more GPS/photo validation and explore a phone-based, human-in-the-loop active learning workflow.

Takeaway: Rich remote-sensing embeddings plus lightweight classifiers can deliver useful on-the-ground ecological maps—good enough to guide boots-on-the-ground surveys, with room to improve under cover.

**Summary of Discussion:**

1. **Technical Insights & Model Methodology:**  
   - Participants highlighted the use of **TESSERA embeddings** (combining Sentinel-1/2 satellite data) and lightweight classifiers (logistic regression, k-NN) for bramble detection. Some likened the process to a “Where’s Wally” puzzle, where the model identifies subtle patterns in complex satellite data.  
   - **Hyperspectral vs. Multispectral Data:** Debate emerged about hyperspectral data’s superiority (e.g., SWIR bands for distinguishing rock types), but acknowledged its rarity compared to satellite-based multispectral data. Plane/UAV-based hyperspectral surveys are niche but valuable.  

2. **Applications & Broader Use Cases:**  
   - **Agricultural & Ecological Monitoring:** Suggestions included crop health analysis, detecting illegal farming, or invasive species like Giant Hogweed. One user proposed using temporal-spectral data to track crop growth cycles.  
   - **Practical Challenges:** Mapping in bright sunlight and computational hurdles (e.g., reprocessing data on a laptop in the field) were noted as pain points.  

3. **Limitations & Accuracy Concerns:**  
   - **Resolution Issues:** The model struggles with small bramble patches under tree canopies due to satellite data’s 10m resolution. Some questioned if predictions merely correlated with roads or human infrastructure.  
   - **Validation Needs:** Participants emphasized ground-truthing and active learning (e.g., phone-based photo validation) to improve reliability.  

4. **Tools & Next Steps:**  
   - The **TESSERA interactive notebook** ([link](https://github.com/cm-tssr-interactive-map)) was recommended for experimenting with embeddings and labeling data.  
   - Computational scalability challenges (e.g., processing global data requiring ~200 TB storage) were discussed, with calls for collaborative efforts or prioritization of regions.  

5. **Skepticism & Defense of Approach:**  
   - Some doubted the model’s novelty, arguing it might replicate simple presence/absence mapping. Authors clarified it uses **spatio-temporal embeddings** to capture ecological dynamics, not just static features.  

**Key Takeaways:**  
The discussion reflects enthusiasm for accessible remote-sensing tools but underscores the need for higher-resolution data (hyperspectral, UAVs) and rigorous validation. While the model’s simplicity and cost-effectiveness are strengths, participants urged expanding use cases (e.g., invasive species, crop fraud) and addressing technical limitations through collaboration and iterative learning.

### Gemini Robotics 1.5 brings AI agents into the physical world

#### [Submission URL](https://deepmind.google/discover/blog/gemini-robotics-15-brings-ai-agents-into-the-physical-world/) | 61 points | by [meetpateltech](https://news.ycombinator.com/user?id=meetpateltech) | [11 comments](https://news.ycombinator.com/item?id=45374474)

Google is pushing “agentic” robots a step further with two new Gemini models aimed at turning high‑level goals into real‑world actions.

What’s new
- Gemini Robotics 1.5: a vision‑language‑action (VLA) model that converts visual context and natural‑language instructions into motor commands. It “thinks before acting,” producing an internal reasoning sequence and optional natural‑language explanations, and can learn skills across different robot bodies.
- Gemini Robotics‑ER 1.5: a vision‑language model (VLM) for embodied reasoning that plans multi‑step missions, estimates progress/success, and natively calls tools (e.g., Google Search or user functions) to fetch rules or data.

How it works
- ER 1.5 acts as the high‑level brain: it builds multi‑step plans, queries tools, and issues stepwise instructions.
- Robotics 1.5 executes: it uses vision + language to perform the concrete actions, breaking long tasks into simpler chunks when needed.
- Example: to sort objects into compost/recycling/trash “based on my location,” ER 1.5 can look up local rules, then Robotics 1.5 perceives items and carries out the placements.

Performance
- Google reports state‑of‑the‑art results across 15 embodied/spatial benchmarks (e.g., ERQA, Point‑Bench, RefSpatial, RoboSpatial‑Pointing/VQA), plus capabilities like object state estimation, pointing, trajectory prediction, and task progress detection.

Availability
- Robotics‑ER 1.5: available now via the Gemini API in Google AI Studio.
- Robotics 1.5: limited release to select partners.

Why it matters (and open questions)
- If reliable, this bridges planning, perception, and action for general‑purpose robots. Real‑world robustness, hardware breadth, latency, and safety constraints will be key to watch beyond benchmark claims and curated demos.

**Summary of Discussion:**

The discussion around Google's new Gemini Robotics models highlights several key themes and critiques:

1. **Practical Applications & Technical Focus**:  
   - Users speculate on real-world robotics applications, such as drone navigation, delivery systems, and object recognition. Some emphasize the importance of integrating low-level perception algorithms with high-level planning for tasks like sorting or tracking.  

2. **Skepticism Toward Benchmarks & Hype**:  
   - Skepticism arises about Google’s benchmark claims, with one user noting that the non-ER Gemini model reportedly underperforms GPT-5 in certain puzzle tasks. Others question the marketing of polished demos versus real-world robustness, particularly in motor control and hardware adaptability.  

3. **Research Dynamics & Corporate Influence**:  
   - Debates emerge about Google’s research strategy. Some praise its historical contributions (e.g., 2017 Transformers paper) and willingness to share work despite corporate pressures, while others criticize shareholder-driven growth goals for potentially stifling pure research. A user points to a rumored link between Google’s 2025 roadmap and real-world robotics products.  

4. **Implementation Challenges**:  
   - Concerns are raised about latency, safety, and scalability in deploying these models beyond controlled benchmarks. The discussion underscores the gap between academic advancements and reliable, generalized robotic systems in unpredictable environments.  

**Overall Sentiment**:  
Mixed optimism about the technical potential of Gemini Robotics models, tempered by skepticism of corporate motives, benchmarking transparency, and real-world practicality. The conversation reflects broader tensions in AI research between innovation, profitability, and ethical implementation.

### Video models are zero-shot learners and reasoners

#### [Submission URL](https://video-zero-shot.github.io/) | 89 points | by [meetpateltech](https://news.ycombinator.com/user?id=meetpateltech) | [19 comments](https://news.ycombinator.com/item?id=45372289)

HN TL;DR: Google’s Veo 3 video model is showing LLM-like “emergent” zero-shot skills across a surprisingly wide range of visual tasks, hinting that generative video models may become general vision foundation models.

- What’s new: A paper and podcast claim Veo 3 can perform dozens of tasks it wasn’t explicitly trained for—purely via prompting—spanning perception, manipulation, physical modeling, and early visual reasoning.
- Demos include: edge detection, segmentation, super-resolution, deblurring/denoising, colorization, in/outpainting, background removal, style transfer; material and physics intuition (rigid/soft transforms, gravity, buoyancy, optics, color mixing); affordance recognition and tool-use simulations; Omniglot recognition/generation; and reasoning tasks like maze solving, BFS on graphs, analogies (rotate/reflect/resize), simple sudoku, and “water puzzle” solving.
- Big idea: The same simple recipe that drove LLMs—large generative models trained on web-scale data—may transfer to video, unifying many vision tasks without task-specific training.
- Why it matters: If validated, this could collapse today’s fragmented CV stack into a single generalist model and accelerate robotics/embodied agents that need perception + manipulation + reasoning.
- Caveats: Evidence appears demo-heavy; metrics, robustness, and contamination controls aren’t clear; Veo is closed, so independent replication and standardized benchmarks will be crucial.
- Paper: “Video models are zero-shot learners and reasoners” (arXiv TBD) + an accompanying podcast for a quick overview.

**Summary of Discussion:**

The discussion around Google's Veo 3 video model reflects mixed reactions, blending intrigue with skepticism:  

1. **Emergent Intelligence Debate**:  
   - Some users question whether the model’s capabilities (e.g., segmentation, physics intuition) represent true intelligence or are merely sophisticated pattern-matching. Critics argue that current ML approaches lack the holistic, integrative perception seen in biological systems, likening them to "post-hoc assertions" rather than inherent understanding.  
   - A user (**mllwdrm**) contends that segmentation and perception tasks in AI are fragmented and mechanistic, contrasting them with biological vision systems that evolved for survival. They reference classical theories (e.g., Marr’s vision framework) and criticize modern ML for ignoring integrative, conscious-like processing.  

2. **Technical Skepticism**:  
   - Concerns arise about whether video models truly "understand" motion or merely process individual frames (**rcrdbt**, **fskp**). Some suggest local video models (e.g., WAIN22) already generate plausible stills but lack deeper spatial reasoning.  
   - **pvlln** posits that Veo’s problem-solving might simply reframe tasks as "training + intent = action," akin to LLMs, rather than genuine reasoning.  

3. **Pseudoscience Accusations**:  
   - A heated subthread (**ACCount37**, **mllwdrm**) devolves into accusations of pseudoscience and trolling, with users dismissing claims as "schizophrenic" or "cryptic nonsense." Others mock the debate as unproductive.  

4. **Cautious Optimism**:  
   - A few users (**miguel_martin**) express awe at the demos, while others speculate that multimodal models or hidden architectures might explain Veo’s versatility.  

**Key Takeaway**: While intrigued by Veo’s potential, the community emphasizes the need for rigorous validation, standardized benchmarks, and clarity on whether its skills reflect true generalization or clever prompting. Skepticism centers on overstatements of "intelligence" and the risk of conflating generative prowess with reasoning.

### Windows ML is generally available

#### [Submission URL](https://blogs.windows.com/windowsdeveloper/2025/09/23/windows-ml-is-generally-available-empowering-developers-to-scale-local-ai-across-windows-devices/) | 23 points | by [sorenjan](https://news.ycombinator.com/user?id=sorenjan) | [3 comments](https://news.ycombinator.com/item?id=45378323)

Microsoft is making its on-device AI stack official: Windows ML, first previewed at Build 2025, is now generally available as the built‑in inferencing runtime for Windows 11. It wraps ONNX Runtime (you keep using ORT APIs) and lets the OS distribute and update the runtime plus vendor Execution Providers (EPs) for AMD, Intel, NVIDIA and Qualcomm, so apps don’t have to bundle hundreds of MB of AI deps. The pitch: faster, more private, and cheaper local inference that targets CPUs, GPUs, or NPUs with a single app build.

Highlights
- Hardware abstraction: EPs from silicon vendors plug into Windows ML; the OS detects hardware and fetches the right EPs automatically.
- Smaller apps, simpler ops: No bundling ORT/EPs; Windows handles distribution, updates, and conformance/certification to keep accuracy consistent across builds.
- Performance/power controls: Developers can set device policies to prefer NPU (low power), GPU (high performance), or explicitly pick silicon per model.
- AOT option: Precompile models ahead of time to speed up cold starts and smooth installs.
- Model workflow: Use ONNX models directly or convert from PyTorch via the AI Toolkit for VS Code; deploy across Windows 11 PCs.
- Ecosystem: AMD integrates via its Vitis AI EP; Intel’s EP leverages OpenVINO for CPU/GPU/NPU on Core Ultra. Windows ML underpins Windows AI Foundry and Foundry Local for broader silicon support.

Why it matters
- Brings Windows closer to Apple’s Core ML and Android’s NNAPI model: a unified, OS-level inference layer with vendor-optimized backends.
- Reduces fragmentation and versioning pain for app developers while widening reach across heterogeneous PC hardware.
- Positions NPUs on “Copilot+” class PCs as first-class targets for third‑party apps, not just Microsoft features.

Open questions to watch
- Version pinning and reproducibility when the OS manages ORT/EP updates, especially for offline or enterprise-locked machines.
- Depth of NVIDIA EP support and feature parity across vendors.
- How far ONNX-only workflows go for teams invested in TensorFlow/PyTorch artifacts without conversion.
- Backport/availability on older Windows 11 builds and non-NPU devices.

Getting started
- Keep your ORT code; target ONNX models.
- Use AI Toolkit for VS Code to convert/optimize PyTorch models.
- Set device policies (NPU/GPU/CPU) and consider AOT compilation for faster startup.
- Let Windows handle EP distribution to cut app size and maintenance.

The discussion highlights comparisons between Windows ML and other platforms, along with technical considerations:  
1. **Apple Parallel**: A user likens Windows ML to Apple's approach with Core ML and Apple Intelligence, emphasizing privacy-focused, on-device AI apps regardless of hardware.  
2. **Ollama Comparison**: Another user questions how Windows ML differs from using tools like Ollama for local LLMs, noting potential privacy benefits of keeping data on-device. A reply clarifies that Ollama currently lacks NPU support, a key advantage of Windows ML for Copilot+ PCs.  

The exchange underscores interest in cross-platform privacy standards and Windows ML’s hardware integration (e.g., NPUs) as differentiators.

---

## AI Submissions for Wed Sep 24 2025 {{ 'date': '2025-09-24T17:15:24.293Z' }}

### Learning Persian with Anki, ChatGPT and YouTube

#### [Submission URL](https://cjauvin.github.io/posts/learning-persian/) | 249 points | by [cjauvin](https://news.ycombinator.com/user?id=cjauvin) | [84 comments](https://news.ycombinator.com/item?id=45359524)

A language learner outlines a repeatable, low-friction workflow for mastering Farsi using spaced repetition, AI, and YouTube—optimized for phrases over isolated words.

- Core: A “never-ending” Anki deck built from screenshots. Two card types: one-sided Persian-only cards for reading practice (to tackle contextual letter forms and missing vowels), and “basic & reversed” cards pairing romanized phrases with English/French translations.
- On-demand tutor: When stuck during reviews, they paste a screenshot into a dedicated ChatGPT “Persian” project for instant refreshers and explanations—repeating questions until concepts stick.
- YouTube pipeline: Videos (notably from Majid’s Persian Learning channel) plus Dual Subtitles for parallel Farsi/English, and Tweaks for YouTube to nudge playback by 1 second for micro-replays. Screenshots from subtitles feed new Anki cards.
- Listening technique: Play at 75% speed; glance at English first to prime meaning; then listen closely to Farsi so the known meaning maps onto the sounds; read the Farsi script to disambiguate; repeat out loud; loop the same segment until understanding is real-time.
- Why it works: Phrase-based SRS, frictionless capture (screenshots), micro-iteration on audio, and AI as a just-in-time coach create a high-feedback system that targets reading, listening, and speaking together.

Takeaway: A simple, reproducible stack that blends SRS, subtitles, and AI can make progress in a non-Latin script language feel fast and tangible—especially when you train with phrases and iterate ruthlessly on short clips.

**Discussion Summary:**

The conversation revolves around language learning strategies, focusing on tools like Anki, cultural nuances, and practical experiences. Key themes include:

1. **Anki Workflow & Customization:**
   - Users praised Anki for vocabulary building but debated pre-built vs. custom decks. Many emphasized **personalized decks** as more effective, arguing pre-made decks (e.g., for Spanish C1) often lack context and feel algorithmically generated. Tools like LLM-powered Anki extensions were recommended for generating tailored cards.
   - A user shared success with **KOFI conjugation decks** for Spanish/French, highlighting their structured approach to mastering verb forms, though irregular verbs remain challenging.

2. **Cultural Blunders & Nuances:**
   - Humorous anecdotes surfaced about **language mishaps**, such as confusing "chaqueta" (jacket) with a vulgar term in Mexican Spanish or Dari/Pashto misunderstandings in Afghanistan. These stories underscored the importance of cultural context and the risks of over-relying on direct translations.

3. **Tool Recommendations:**
   - **Clozemaster** and **Assimil** were cited for immersive learning, while **YouTube channels** (e.g., French Comprehensible Input) and **Yabla** were suggested for listening practice.
   - For Persian/Farsi, **Majid’s Persian Learning channel** and **Dual Subtitles** were highlighted as key resources.

4. **Practical Insights:**
   - A user working in Afghanistan with the ICRC shared how learning Dari/Pashto was crucial for humanitarian work, stressing the value of **in-country immersion** and crash courses.
   - Others emphasized **comprehensible input** (e.g., native TV, comics) and **spaced repetition** systems (SRS) as foundational to progress, advising learners to transition to native content early.

**Takeaway:** The discussion champions a blend of structured tools (Anki, LLMs) and immersive, context-rich learning, while acknowledging the humor and humility required to navigate cultural-linguistic pitfalls.

### How HubSpot scaled AI adoption

#### [Submission URL](https://product.hubspot.com/blog/context-is-key-how-hubspot-scaled-ai-adoption) | 69 points | by [zek](https://news.ycombinator.com/user?id=zek) | [43 comments](https://news.ycombinator.com/item?id=45361140)

HubSpot’s two-year journey from AI-curious to near-universal adoption of coding assistants

- What happened: Starting with a GitHub Copilot pilot in Summer 2023, HubSpot moved from cautious trials to organization-wide use of AI coding tools, reporting modest-but-real productivity gains that compounded over time.

- Why it worked: Executive sponsorship (from founders Dharmesh Shah and Brian) aligned legal, security, and engineering, enabling fast pilots and rollout with guardrails. Entire teams trialed the tools for 2+ months, with training, Q&A channels, and velocity metrics to counter bias.

- Cost calculus: Even early gains justified Copilot’s ~$19/user/month price. The team stayed patient, betting the tech would improve—and saw larger gains as usage deepened.

- Centralization as a force multiplier: In Oct 2024, HubSpot formed a small Developer Experience AI team to:
  - Drive adoption across the org
  - Inject HubSpot-specific context into AI (from shared Cursor rules to richer architecture/best-practice knowledge)
  - Build community and internal advocacy
  - Speed procurement (month-to-month contracts, rapid trials)
  - Run empirical evaluations instead of relying on anecdotes

- The “context is king” insight: Quality jumped when AI tools knew HubSpot’s opinionated stack, libraries, and conventions—turning generic assistants into org-aware ones.

- Playbook you can copy:
  - Secure strong exec buy-in early
  - Pilot with whole teams, not scattered individuals
  - Invest in enablement and a public forum for wins/warts
  - Instrument impact using existing engineering velocity metrics
  - Stand up a small central team to own adoption, context, evaluation, and procurement agility
  - Start with guardrails; relax as data and confidence grow

- Why it matters: Scaling AI beyond early adopters is an org design problem as much as a tooling one. Central teams, org-specific context, and measurement turn modest early gains into durable productivity improvements.

- What’s next: HubSpot hints at more advanced, context-rich tools and practices in this series as they continue to expand AI’s role in their developer workflow.

**Hacker News Discussion Summary: HubSpot’s AI Adoption Journey**

The discussion around HubSpot’s AI adoption journey reveals a mix of skepticism, technical debate, and criticism of the company’s history and practices. Key themes include:

---

### **Skepticism About HubSpot’s Narrative**
- **Cultural Criticism**: Users reference Dan Lyons’ book *Disrupted*, which portrays HubSpot’s culture as overly hyped and akin to "Facebook-style" aggressive tactics. Comments highlight perceived parallels between HubSpot’s "passive-aggressive" workplace culture and Silicon Valley’s darker traits.  
- **Ethics and Controversies**: Past scandals, including a hacking incident targeting a journalist (linked to a 2014 Finextra article) and comparisons to eBay’s stalking scandal, are cited as reasons to doubt HubSpot’s virtuous image. Critics argue the submission overlooks these issues.  

---

### **Debates About AI’s Impact**
- **Measurement Concerns**: Many question the lack of concrete data in the submission. Users demand rigorous, transparent metrics (e.g., code review burden, velocity, incident rates) instead of vague "modest gains" claims.  
- **Productivity Tools vs. Mandates**: Discussions erupt over mandating AI tools (e.g., GitHub Copilot) and IDEs. Some argue enforcement stifles productivity, while others defend standardized tooling for collaboration.  
- **AI’s Utility**: A subset dismisses AI tools as "dumb" or overhyped, emphasizing that 98% of code is straightforward, with human intervention still critical for edge cases.  

---

### **Criticism of HubSpot’s Business Model**
- **Inbound Marketing Backlash**: While HubSpot is credited with popularizing inbound marketing, critics liken its content-driven approach to "spam," arguing it prioritizes quantity (blogs, whitepapers) over quality. The tactic is seen as outdated, with AI now accelerating low-value content creation.  
- **Dubious Reviews**: Users debate the legitimacy of glowing Google/X reviews, suspecting they’re manufactured or gamed.  

---

### **Workplace Dynamics**
- **AI Fluency Pressures**: Concerns emerge about hiring expectations—companies increasingly demand AI proficiency, potentially sidelining existing engineers resistant to workflow changes.  
- **Overwork and Surveillance**: Jokes about mandated monitors and keyboard tracking reflect broader anxieties about surveillance and productivity grind in tech.  

---

### **Call for Nuance**
- **Acknowledgment of Strategy**: A few users concede that HubSpot’s centralized AI team and focus on org-specific context (e.g., internal RPC systems) could offer lessons for scaling AI adoption effectively.  
- **Need for Balance**: One commenter notes that while HubSpot’s execution is flawed, its survival in a competitive landscape (vs. Salesforce) is notable.  

---

### **Conclusion**
The discussion tempers HubSpot’s success story with reminders of its controversial past, skepticism about self-reported metrics, and debates over AI’s real-world impact. While some praise the tactical approach to AI adoption, broader distrust of corporate narratives and ethical concerns dominate the thread.

### Zed's Pricing Has Changed: LLM Usage Is Now Token-Based

#### [Submission URL](https://zed.dev/blog/pricing-change-llm-usage-is-now-token-based) | 176 points | by [meetpateltech](https://news.ycombinator.com/user?id=meetpateltech) | [196 comments](https://news.ycombinator.com/item?id=45362425)

Zed switches its AI pricing to “pay for what you use,” cuts Pro in half, and adds top-tier models

- What’s new: Zed is moving from prompt caps to token-based billing. For new users it’s live today; existing users migrate over the next three months.
- Pro price drop: Pro goes from $20 to $10/month and now includes $5 in token credits. Additional hosted usage is billed at provider list price +10% (to cover infra/support and higher rate limits).
- Free stays useful: Free plan still includes 2,000 accepted edit predictions. Hosted prompt quotas are gone.
- Pro still unlimited where it counts: Unlimited accepted edit predictions remain on Pro.
- More models: Hosted lineup expands to Claude Sonnet/Opus plus GPT-5 (including mini/nano) and Gemini 2.5 Pro/Flash.
- Rationale: LLM bills became Zed’s biggest cost; prompt pricing didn’t track value (fixing a typo vs multi-file refactors cost the same). Tokens align cost with usage and simplify adding models.
- Alternatives built in: Bring your own API keys (OpenAI, Anthropic, Grok, etc.), use local models via Ollama, connect third‑party agents via ACP, or route spend through Copilot, OpenRouter, Bedrock. You can also disable AI entirely.
- Trials and subsidies: Pro trial remains 14 days but now comes with $20 in token credits. Zed says future subsidies will be targeted (e.g., student discounts).
- Migration timeline: 
  - Pro customers: migrate by Dec 17, 2025 (or switch earlier to access new models).
  - Free users: move to the new Free plan on Oct 15, 2025, with a fresh Pro trial available anytime starting now.
  - Trial users: moved back to the old Free plan today (Sep 24) and get a new trial.

Big picture: Zed is aligning revenue with actual inference costs so it can invest in speed, reliability, and collaboration features, while keeping Pro affordable and flexible for heavy users and letting everyone else plug in their own AI.

Here's a concise summary of the Hacker News discussion about Zed's AI pricing changes and editor ecosystem:

### Key Themes in the Discussion:
1. **Performance Concerns**  
   - Users reported issues with handling large files (e.g., 1GB+ files caused 20GB+ memory spikes) and macOS/Linux font rendering glitches. Some experienced crashes when opening large projects.  
   - Comparisons to **Sublime Text** and **VS Code** surfaced, with Sublime praised for speed/robustness with huge files, while others noted JetBrains IDEs (e.g., CLion, Goland) handle massive codebases more reliably.  

2. **AI Integration vs. Core Editor Functionality**  
   - Skepticism arose around Zed’s heavy AI focus: "AI minority priority" vs. practical needs like stability. Some felt AI features distracted from improving core text-editing UX.  
   - Users debated AI's true utility, with remarks like "thr’s disproprtionate mention of AI in blog posts" and fears of "existential dread" when relying on AI agents mid-task.  

3. **Market Alternatives**  
   - **Sublime Text** was lauded but criticized for closed-source/licensing decisions. Users expressed interest in open-source alternatives like **SpartanJ’s experimental editor** (GPU-accelerated, designed for performance) but noted challenges matching Sublime’s polish.  
   - VS Code remains a popular fallback despite Zed’s speed advantages, especially for its extensions and familiarity.  

4. **Pricing & Priorities**  
   - Zed’s new token-based pricing was seen as logical, but concerns lingered about long-term VC-driven profit motives overriding user-centric development.  
   - Some questioned whether Zed’s "aggressive AI vision" aligns with developer workflows, preferring focus on collaboration tools (e.g., multiplayer editing) and refinement of basics.  

### Notable Quotes:
- **On Performance**: *"Opening a 1GB text file caused macOS to run out of system memory... Zed quickly ate 20GB during a search operation."*  
- **On AI Fatigue**: *"Companies are selling AI as the thing you do day-to-day... often non-technical management drives this."*  
- **On Alternatives**: *"Sublime’s biggest gap is being closed-source. Zed, while VC-backed, at least feels like a step toward modernizing without losing performance."*  

### Bottom Line:
The discussion reflects enthusiasm for Zed’s speed and modern features but highlights tension between AI ambitions and foundational editor reliability. Performance pitfalls and skepticism about over-indexing on AI (vs. refining core UX) dominate concerns, while alternatives like VS Code and Sublime remain entrenched for their stability.

### Greatest irony of the AI age: Humans hired to clean AI slop

#### [Submission URL](https://www.sify.com/ai-analytics/greatest-irony-of-the-ai-age-humans-being-increasingly-hired-to-clean-ai-slop/) | 202 points | by [wahvinci](https://news.ycombinator.com/user?id=wahvinci) | [114 comments](https://news.ycombinator.com/item?id=45356226)

The piece argues that while AI is displacing creative jobs, it’s simultaneously creating a new labor market for “digital janitors” paid to fix its mistakes. “AI slop”—low-quality, plausible-but-wrong text, images, music, and video—now floods platforms, forcing companies to hire writers, designers, and VFX artists not to create, but to clean up.

Notable examples
- Viral AI videos: a seagull smashing a car window for a fry (~140M views) and “CCTV-style” trampoline animals (rabbits ~200M; bears similar), with telltale glitches like two-headed bunnies and vanishing frames.
- Brand/ads: a screenshot from an AI-assisted Coca-Cola holiday commercial spelling “Coca-Coola.”
- Real-world fallout: thousands in Dublin reportedly showed up for a non-existent Halloween parade after AI-made ads with gibberish text spread.
- Definition: Jack Izzo (Yahoo) calls AI slop the evolution of spam—empty-calorie content that overwhelms feeds and blurs what’s real.

Why it matters
- Industrialized misinformation and the “enshittification” of culture as feeds, playlists, and marketplaces fill with AI remixes.
- Rising demand for human cleanup—content rewriters, fact-checkers, retouchers, VFX fixers—often the very people AI was meant to replace.
- Environmental costs: water and electricity usage to generate mountains of low-value content.
- Creative burnout and a shift from authorship to maintenance.

HN discussion starters
- Are “AI janitor” roles a sustainable career path or a race to the bottom?
- Can provenance/labeling or platform policy slow the slop flood?
- Does paying humans to polish AI output entrench the problem—or buy time until models improve?

**Summary of Hacker News Discussion:**

The discussion revolves around the paradox of AI displacing creative jobs while creating new roles for humans to clean up its errors ("AI slop"). Key themes include:

1. **Job Market Shifts**:  
   - Junior roles (e.g., interns, entry-level designers) are increasingly replaced by AI, disrupting traditional talent pipelines. Without juniors gaining experience, industries face a shortage of future senior talent.  
   - Skepticism exists about whether "AI janitor" roles (e.g., fact-checkers, content fixers) are sustainable or merely a "race to the bottom."  

2. **Quality & Creativity**:  
   - AI-generated content often lacks specificity and validation. Humans are still needed to catch errors (e.g., nonsensical text, visual glitches) that AI overlooks.  
   - Some argue AI homogenizes creativity, favoring "safe" outputs over originality, leading to cultural "enshittification."  

3. **Comparisons to Manufacturing**:  
   - Fixing AI errors is likened to factory quality control, but with key differences: AI’s "defects" are harder to detect (e.g., subtle logical flaws vs. visual defects) and require mental labor rather than physical rework.  
   - Concerns about energy costs and efficiency if AI produces vast low-quality output needing human cleanup.  

4. **Economic & Cultural Concerns**:  
   - Doubts about government job data (BLS) accurately reflecting AI’s impact, with references to politicized reporting.  
   - The creative economy risks collapse if mid-tier roles vanish, leaving only high-cost agencies or automated slop.  

5. **Technological Optimism vs. Skepticism**:  
   - Some see AI as part of a broader industrial revolution, improving over time. Others question its revolutionary status, noting current limitations and the need for human oversight.  

**Notable Points**:  
- A user highlighted India’s service industry, where workers manually fix faulty items instead of discarding them, suggesting parallels to AI slop cleanup.  
- Debate over whether AI’s flaws will self-correct with advancements (e.g., GPT-5) or entrench reliance on human intervention.  

Overall, the discussion reflects tension between AI’s potential and its current limitations, with concerns about economic sustainability, cultural degradation, and the evolving role of human labor.

### The Data Commons Model Context Protocol (MCP) Server

#### [Submission URL](https://developers.googleblog.com/en/datacommonsmcp/) | 17 points | by [meetpateltech](https://news.ycombinator.com/user?id=meetpateltech) | [3 comments](https://news.ycombinator.com/item?id=45362038)

Data Commons launches an MCP server to make its public stats instantly usable by AI agents

- What’s new: A public Model Context Protocol (MCP) server that exposes Data Commons’ interconnected public datasets in a standardized way for AI agents—no custom API wrangling required.
- Why it matters: Anchors agent answers in sourced, real-world statistics to cut LLM hallucinations and speed up building data-rich, agentic apps.
- Capabilities: Handles exploratory (“What health data do you have for Africa?”), analytical (“Compare life expectancy, inequality, GDP growth for BRICS”), and generative (“Report on income vs. diabetes in US counties”) workflows.
- Integrations: Designed to slot into Google Cloud’s Agent Development Kit (ADK) and Gemini CLI, but works with any MCP client. Getting started via a PyPI package, a Colab sample agent, and a GitHub repo.
- Real-world use: ONE Campaign’s ONE Data Agent taps the MCP server to search tens of millions of health financing datapoints in seconds, visualize results, and export clean datasets—e.g., quickly flag countries most reliant on external health funding.
- Big picture: Moves Data Commons from “API to learn” to a first-class agent tool, aiming to make trustworthy public data a default context for AI.

**Summary of Discussion:**  
- **User Optimism on MCP Capabilities:** One user highlights that AI agents leveraging the MCP server can efficiently parse complex queries, retrieve structured data, and reduce reliance on error-prone LLM-generated outputs. They note that current AI models effectively translate natural language into SQL, minimizing manual cross-checking and potential misalignment in results.  
- **Real-World Implementation:** Another user shares a deployed example of the MCP server (linked), demonstrating its practical use for remote data access.  

**Key Themes:**  
1. Confidence in MCP’s ability to streamline data workflows and anchor AI agents in verified statistics.  
2. Emphasis on how natural language-to-SQL transformation reduces LLM “hallucinations” and errors.  
3. Community interest in deploying/running MCP server instances for applications.

---

## AI Submissions for Tue Sep 23 2025 {{ 'date': '2025-09-23T17:16:56.696Z' }}

### Getting AI to work in complex codebases

#### [Submission URL](https://github.com/humanlayer/advanced-context-engineering-for-coding-agents/blob/main/ace-fca.md) | 458 points | by [dhorthy](https://news.ycombinator.com/user?id=dhorthy) | [383 comments](https://news.ycombinator.com/item?id=45347532)

Advanced Context Engineering for Coding Agents (GitHub) — HumanLayer released an open-source playbook aimed at making LLM coding agents more reliable on real-world codebases by ruthlessly curating and structuring what goes into the model’s context. The repo focuses on practical techniques for selecting relevant code, organizing prompts, and scaling context across large repos—so agents can plan changes and navigate without getting lost or hallucinating. It’s already drawing interest (400+ stars), and is a useful reference if you’re building or tuning repo-aware AI dev tools.

The Hacker News discussion on the "Advanced Context Engineering for Coding Agents" submission highlights skepticism and debate about AI's reliability in coding tasks. Key points include:

1. **Frustration with AI Tools**: Users note that while AI speeds up code generation, it shifts time toward debugging unexpected errors and incorrect assumptions. Integration challenges persist, with AI-generated code often requiring extensive manual validation.

2. **Non-Determinism vs. Compilers**: LLMs are criticized for non-deterministic outputs, unlike compilers, which are predictable and testable. Ambiguous requirements (e.g., written in English) exacerbate issues, leading to unreliable code.

3. **Human vs. AI Reasoning**: Participants debate whether LLMs truly "reason" or merely mimic patterns. Some argue LLMs lack human-like understanding, likening them to junior developers needing strict guidance. Others counter that intelligence manifests differently (e.g., statistical pattern matching vs. abstract reasoning).

4. **Testing and Redundancy**: AI's unpredictable errors are harder to catch than human mistakes. Suggestions include aggressive testing, redundant code checks, and skepticism toward AI's "correct-looking" but potentially flawed outputs.

5. **Philosophical Debates**: Discussions diverge into the nature of intelligence, comparing LLMs to biological brains and historical innovations like flight. Some dismiss anthropocentric views, arguing intelligence need not mirror human cognition to be effective.

6. **Practical Challenges**: Handling tasks like CSV parsing or legacy systems (e.g., COBOL) reveals gaps in AI's ability to manage real-world codebases. High-level languages (Python) fare better, but edge cases and integration complexities remain problematic.

Overall, the consensus leans toward cautious pragmatism: AI tools show promise but require rigorous context engineering, precise specifications, and human oversight to mitigate hallucinations and integration pitfalls.

### Context Engineering for AI Agents: Lessons

#### [Submission URL](https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus) | 104 points | by [helloericsf](https://news.ycombinator.com/user?id=helloericsf) | [4 comments](https://news.ycombinator.com/item?id=45352901)

Why it matters: Instead of training end-to-end agent models, Manus bets on “context engineering” atop frontier LLMs to iterate in hours, not weeks—staying model-agnostic so rising model quality lifts the product.

Key ideas:
- Optimize for KV-cache hit rate: For agents, input-to-output tokens can be ~100:1, so prefilling dominates latency and cost. Cached tokens can be 10x cheaper (e.g., Claude Sonnet: $0.30 vs $3 per MTok). The author argues KV-cache hit rate is the single most important production metric.
- Practical cache tactics:
  - Keep the prompt prefix stable. Even one-token drift (like a live timestamp) breaks cache from that point on.
  - Make context append-only. Don’t edit prior actions/observations; serialize deterministically (watch JSON key ordering).
  - Mark cache breakpoints when needed. Some providers require explicit boundaries; at minimum, include the system prompt’s end. If self-hosting (e.g., vLLM), enable prefix caching and use session IDs for routing consistency.
- “Mask, don’t remove” tools: As agents gain capabilities, the tool/action space balloons—especially with user-added MCP tools. Dynamically adding/removing tools mid-trajectory sounds smart but often backfires:
  - Changing tool definitions near the front of context invalidates cache for subsequent steps.
  - If past steps reference tools no longer present, models get confused, causing schema violations or hallucinated actions.
  - The principle: keep the action space stable during a run and guide selection via masking/constraints rather than swapping tools in and out.

Vibe: Equal parts war story and playbook. The team calls their iterative prompt/architecture search “Stochastic Graduate Descent”—messy but effective. If model progress is the tide, they want to be the boat, not a pillar stuck to the seabed.

Takeaways for builders:
- Treat KV-cache as a first-class metric; structure prompts and logs to preserve it.
- Prefer append-only, deterministic contexts.
- Keep tool lists stable within a session; steer choices via masking/constraints instead of dynamic loading.

Here's a concise summary of the discussion:

1. **jslv** (with reply from **SafeDusk**)  
   - Emphasizes system memory optimization and tool management patterns like using deterministic file naming, clean code directories, and git-like revision control for agent decisions.  
   - SafeDusk adds a practical example: A simplified Codex-based approach tracking task progress via https://blog.toolcompany.com/codex-tools  

2. **dxfhl**  
   - Warns against over-engineering: Recommends preserving flawed tools/comments via rollbacks/PRs rather than deletions. Advises maintaining stable tooling to avoid model confusion (mirroring the submission's "mask, don't remove tools" principle).  

3. **sfk**  
   - Highlights business implications: Fixed pricing plans incentivize providers to optimize caching (improving margins). Stresses the importance of measuring cache hit rates and visibility, especially for teams new to context engineering.

**Key themes**  
- Real-world tradeoffs between engineering purity and production needs  
- Alignment between technical caching tactics (from submission) and business models  
- Emphasis on version control patterns for agent memory/tools

### From MCP to shell: MCP auth flaws enable RCE in Claude Code, Gemini CLI and more

#### [Submission URL](https://verialabs.com/blog/from-mcp-to-shell/) | 140 points | by [stuxf](https://news.ycombinator.com/user?id=stuxf) | [39 comments](https://news.ycombinator.com/item?id=45348183)

What happened:
- Veria Labs found a simple but high‑impact auth flaw across Model Context Protocol (MCP) clients including Cloudflare’s use-mcp library, Anthropic’s MCP Inspector, Claude Code, and Google’s Gemini CLI.
- Root cause: MCP added an OAuth-based auth flow where the server supplies an authorization URL. Many clients blindly opened that URL. In use-mcp, window.open(authUrlString) accepted javascript: URLs, yielding instant XSS.
- Chain to RCE: With MCP Inspector and the stdio transport, researchers turned that XSS into native code execution (“popped calc”) and note it could be extended to deliver malware or a reverse shell.
- They also demo exploits against Claude Code and Gemini CLI. ChatGPT narrowly avoided impact due to server-side redirects that broke the XSS path.

Why it matters:
- This is a classic trust-boundary mistake: treating server-supplied URLs as safe during OAuth. In AI toolchains, that mistake bridges from browser XSS to local RCE via transports like stdio.
- The attack is one-click: connect to a malicious MCP server and the client opens the attacker’s URL, triggering XSS and then RCE.

Fixes and mitigations highlighted:
- Strictly validate and allowlist OAuth authorization URLs (scheme/domain/path), reject javascript:/data: schemes, and use system browser with deep origin checks.
- Bind OAuth flows with state/nonce, verify origins on postMessage, and harden stdio transport so a compromised webview can’t reach native exec.
- Vendors have shipped or are shipping patches; proof-of-concepts and a timeline are included in the post.

If you use these tools:
- Update Claude Code, Gemini CLI, MCP Inspector, and any app/library using use-mcp.
- Don’t connect clients to untrusted MCP servers; consider sandboxing until patched.

Big takeaway: OAuth is an attack surface. Never trust server-provided auth URLs, especially in agent/tooling ecosystems where a web XSS can quickly escalate to local RCE.

The discussion revolves around security vulnerabilities in MCP clients, particularly OAuth flaws that allow malicious servers to execute code. Key points include:

1. **Risk Comparisons**:  
   - Users liken trusting MCP servers to blindly installing packages from PyPI or npm, highlighting supply chain risks. Even "trusted" MCP servers (like those from Anthropic or Google) could be compromised, emphasizing the need for skepticism and validation.

2. **Root Causes**:  
   - The vulnerability stems from clients blindly trusting server-provided URLs (including `javascript:` schemes), leading to XSS and RCE via insecure transports like stdio. Patches have been released, but questions remain about the robustness of fixes (e.g., Google’s quick PowerShell tweak vs. deeper validation).

3. **AI Hype vs. Security**:  
   - Commercial AI tools (Claude, Gemini) may lead users to implicitly trust MCP servers, creating liability. The hype around AI agents obscures fundamental security gaps, as MCP’s design mixes trusted code execution with untrusted inputs.

4. **Broader Protocol Concerns**:  
   - MCP and similar protocols face challenges in securely handling inputs, akin to prompt injection in LLMs. Users debate whether strict input validation, sandboxing, or dedicated LLM instances can mitigate risks, though 100% prevention is deemed impossible statistically.

5. **Criticism of AI Practices**:  
   - Some criticize AI companies for prioritizing speed over security, leading to vulnerabilities. Disabling MCP servers is suggested to avoid risks, while others argue MCP’s potential warrants improving its security framework.

6. **Proposed Solutions**:  
   - Suggestions include strict URL allowlisting, sandboxing, separating data from commands, and redesigning protocols to avoid credential/execution vulnerabilities. Community collaboration to address flaws is encouraged.

**Key Takeaway**: Trust in third-party sources (supply chain) remains a critical weakness. While patches help, MCP and AI ecosystems need fundamental redesigns to balance innovation with security, ensuring inputs are rigorously validated and execution contexts are isolated.

### MLB approves robot umpires for 2026 as part of challenge system

#### [Submission URL](https://www.espn.com/mlb/story/_/id/46357017/mlb-approves-robot-umpires-2026-part-challenge-system) | 106 points | by [pseudolus](https://news.ycombinator.com/user?id=pseudolus) | [94 comments](https://news.ycombinator.com/item?id=45354304)

- What’s changing: MLB will use an automated ball-strike (ABS) challenge system, not full automation. Each team gets two challenges per game; only the pitcher, catcher, or hitter can initiate by tapping their head. Successful challenges are retained. Calls and replays will show on stadium videoboards.
- Extra-innings logic: Unused challenges carry over. If you run out by the 10th, you get one more; run out again, you get another in the 11th, and so on.
- How it works: 12 calibrated cameras track pitches with ~1/6-inch margin of error. The strike zone is a 2D plane over the plate (17 inches wide) scaled to each batter: top at 53.5% of height, bottom at 27%.
- Early data: Spring training saw ~4 challenges per game with a 52.2% success rate. Catchers led overturns (56%), hitters were at 50%, pitchers 41%.
- Why it matters: It’s a high-profile, human-in-the-loop computer vision system in a major pro sport. MLB aims to correct high-leverage misses without slowing the game, reduce ejections (over 60% tied to balls/strikes), and clarify the zone. It could diminish the value of pitch framing and subtly reshape game strategy.
- Governance notes: The competition committee vote wasn’t unanimous; owners hold a six-seat majority. Umpires still call the game; ABS is the backstop. Minor leagues have tested both challenge and full-ABS modes since 2021.

**Summary of Hacker News Discussion on MLB's "Robot Umps" (2026):**

The introduction of MLB's challenge-based automated ball-strike (ABS) system sparked debate around technology's role in sports officiating, fairness, and tradition. Key themes from the discussion include:

1. **Human vs. Automated Judgment**:  
   - Many users acknowledged the inevitability of technology correcting high-stakes errors but lamented the loss of the "human element" in umpiring. Comparisons were drawn to soccer’s VAR system, where controversial calls (e.g., Maradona’s "Hand of God") remain iconic despite technological interventions.  
   - Concerns were raised about unintended consequences, such as diminished roles for skills like pitch framing and potential shifts in game strategy.

2. **Sports Betting Influence**:  
   - Several comments tied the adoption of ABS to the rise of legalized sports betting in the U.S., arguing that leagues now prioritize precision to protect gambling integrity. Skeptics questioned whether profit motives, rather than fairness, drove the change.

3. **Historical Context**:  
   - Users noted MLB’s gradual tech adoption (e.g., instant replays over 50 years) and resistance from umpires’ unions. Comparisons to cricket’s tech-heavy officiating highlighted differing cultural approaches to automation in sports.

4. **Referee Bias and Fairness**:  
   - References to the book *Scorecasting* underscored statistical evidence of subconscious referee bias in sports. Robot umps were seen as a way to reduce such biases, though some argued human judgment inherently shapes games unpredictably.

5. **Practical Challenges**:  
   - Early data from minor leagues (4 challenges/game, ~52% success rate) sparked discussions about implementation logistics, such as challenge limits and maintaining game pace. Users debated whether ABS would reduce ejections (60% tied to ball/strike calls) or introduce new frustrations.

6. **Cultural Resistance**:  
   - Traditionalists mourned the erosion of "sandlot baseball" nostalgia, while others welcomed progress. The Cubs’ Wrigley Field was cited as a symbol of balancing innovation with heritage.

**Conclusion**: The discussion reflects tension between precision and tradition, with supporters advocating for fairness and critics fearing the loss of human nuance. While ABS aims to correct errors and modernize MLB, its success hinges on balancing technological accuracy with the unpredictable drama that defines sports fandom.

### Agents turn simple keyword search into compelling search experiences

#### [Submission URL](https://softwaredoug.com/blog/2025/09/22/reasoning-agents-need-bad-search) | 62 points | by [softwaredoug](https://news.ycombinator.com/user?id=softwaredoug) | [31 comments](https://news.ycombinator.com/item?id=45347363)

Stupid backend, smart agent: this post argues that LLM agents produce better search experiences when the search API is simple and predictable, not a “thick” black box full of synonyming, reranking, and vector tricks. The author stripped their furniture search to bare‑bones BM25 with a clear docstring, then let an agent iterate like a human—probing queries, judging results, and refining terms. In examples like “a couch fit for a vampire” and “ugliest chair,” the agent tried targeted queries (“cow print chair,” “patchwork accent chair,” “skull chair”), labeled outcomes as good/meh/bad, and saved them. A lightweight memory layer and semantic cache let it reuse winning expansions for similar future queries (“ugly chair”). It’s slower, but surprisingly effective—and the learned expansions can even feed back into a conventional non-LLM search. Takeaway: give agents transparent tools they can reason about; move the “intelligence” to the agent, not the API.

**Summary of the Discussion:**  
The discussion revolves around balancing **simplicity vs. complexity** in search backend design when integrating LLM agents. Here are the key points:

1. **Transparency & Control**  
   - Supporters argue that a simple, predictable API (like BM25) gives LLM agents clearer tools to reason with, enabling human-like iterative query refinement (e.g., "ugly chair" → "cow print chair"). Structured outputs and deterministic tools help agents avoid "magical" black-box behavior.  
   - Critics question whether overly simplistic APIs can handle nuanced needs like synonyms or relevance ranking, which Google’s complex backend (PageRank, conversational keyword support) addresses—albeit with trade-offs like SEO spam and popularity bias.

2. **Cost & Practicality**  
   - Building custom search engines (e.g., with vector databases) is resource-intensive, while relying on services like Google risks losing control over relevance. Some note LLM-driven agents could inflate costs via API calls and prompt engineering.

3. **SEO & Content Quality**  
   - Google’s evolution highlights struggles with keyword-stuffed, SEO-optimized content. A lightweight backend might bypass these issues but lacks advanced features like topic clustering or popularity signals.

4. **LLM Integration**  
   - Agents leveraging semantic caching and feedback loops (saving successful queries) can improve results over time. However, challenges remain in handling ambiguous terms (e.g., "swift" meaning Taylor Swift vs. the programming language) without backend disambiguation.

5. **Structured vs. Open-Ended Search**  
   - While structured tools aid reasoning, users emphasize the need for agents to handle vague or creative queries ("couch fit for a vampire") through experimentation rather than rigid APIs.

**Takeaways**:  
The debate highlights a split between favoring minimalist, transparent backends for LLM-driven search agility and acknowledging the necessity of some backend sophistication to manage real-world ambiguity and scalability. The original approach’s strength lies in prioritising agent reasoning over opaque backend "magic," though practical implementation may require balancing both.

### Android users can now use conversational editing in Google Photos

#### [Submission URL](https://blog.google/products/photos/android-conversational-editing-google-photos/) | 128 points | by [meetpateltech](https://news.ycombinator.com/user?id=meetpateltech) | [129 comments](https://news.ycombinator.com/item?id=45349848)

Google is rolling out Google Photos’ conversational editing—first seen on Pixel 10—to eligible Android users in the U.S. Tap “Help me edit” and describe changes by voice or text (or just say “make it better”); Gemini-powered smarts handle the rest, alongside one-tap suggestions and simple gestures. Beyond quick touch-ups, it supports playful, generative edits (think: moving an alpaca from a petting zoo to Waikiki). Availability is U.S.-only for now and limited to eligible Android devices.

**Hacker News Discussion Summary on Google Photos AI Editing Feature:**

1. **Criticism of AI Integration:**  
   - Many users express frustration with Google's aggressive AI feature rollouts, calling them "bloated" and poorly integrated. Complaints include cluttered interfaces, hidden settings, and difficulties performing basic tasks like cropping ("Impossible on Pixel now!").  
   - Comparisons to Apple’s slower AI adoption are noted, with some suggesting Google prioritizes investor-friendly "AI buzzwords" over thoughtful user experience.  

2. **Privacy and Data Concerns:**  
   - Skepticism exists around AI features being "data grabs" for training models, especially with Google Photos allegedly removing options to disable features like face grouping.  

3. **Alternatives to Google Photos:**  
   - **Self-hosted solutions** like [Immich](https://immich.app/) and [Ente](https://ente.io/) are praised, though Immich lacks HDR support.  
   - Technical debates arise over encryption (e.g., Hetzner VPS setups, Tailscale/WireGuard for secure sync), with warnings about trusting third-party providers with unencrypted data.  

4. **Technical Bugs and Regressions:**  
   - Users report broken features (e.g., Magic Eraser no longer working correctly) and interface overhauls (e.g., "rounded corners everywhere") that disrupt workflows.  

5. **Motivations Behind Features:**  
   - Some speculate Google’s AI push is financially motivated (e.g., driving storage sales via AI-generated content). Others counter that storage costs are negligible for Google, arguing the goal is product stickiness, not short-term profit.  

**Humorous/Cultural Notes:**  
   - A *Blade Runner* reference ("Enhance!") jokes about AI’s limitations.  
   - "Soylent Green vibes" quips nod to dystopian tech critiques.  

**Key Takeaway:** The discussion reflects skepticism toward forced AI integration, enthusiasm for open-source alternatives, and debates about balancing convenience with privacy/control.

### Abundant Intelligence

#### [Submission URL](https://blog.samaltman.com/abundant-intelligence) | 86 points | by [j4mie](https://news.ycombinator.com/user?id=j4mie) | [128 comments](https://news.ycombinator.com/item?id=45346968)

Sam Altman: Building a “gigawatt-per-week” AI infrastructure factory

- Altman says AI demand is outpacing supply and access to AI could become a fundamental economic driver—and possibly a human right.
- He frames compute as the bottleneck: with enough power (e.g., “10 gigawatts”), AI might tackle goals like curing cancer or delivering personalized tutoring to every student.
- Vision: create a factory capable of producing 1 gigawatt of new AI infrastructure every week. Getting there will take years and breakthroughs across chips, power, construction, and robotics.
- Strong emphasis on building much of this in the US to accelerate domestic capacity, where he argues other countries currently move faster on fabs and energy.
- Details to come: partners will be announced in the next couple of months; financing plans later this year, with “interesting” models tied to revenue growth from added compute.

Why it matters: If realized, this would be one of the largest infrastructure build-outs in tech history, reshaping energy, semiconductor, and data center markets—and potentially who can access advanced AI. What’s unclear: where the power, capital, and supply chain headroom will come from, and how quickly permitting and grid constraints can be addressed.

**Hacker News Discussion Summary: Sam Altman's Gigawatt AI Infrastructure Proposal**

The Hacker News community reacts to Sam Altman's ambitious plan for a "gigawatt-per-week" AI infrastructure factory with a mix of cautious optimism and skepticism. Here are the key themes:

### **1. Environmental and Energy Concerns**
- **Feasibility Challenges**: Users question the practicality of scaling energy production to 10 gigawatts, noting it would require ~87 TWh annually (2% of U.S. consumption). Critics argue this demands unprecedented investment in renewables, nuclear, or untested energy solutions.
- **Nuclear Partnerships**: Altman’s investment in Oklo, a nuclear startup, is highlighted as a potential pathway, though doubts remain about regulatory and technical hurdles.

### **2. Skepticism Toward Claims**
- **Hyperbole and History**: Altman is criticized for past exaggerated statements (e.g., AGI timelines). Users compare the proposal to overhyped tech trends, urging scrutiny of his predictions.
- **AI Capability Doubts**: Some argue current AI (e.g., GPT-4/5) shows incremental, not revolutionary, progress. Solving issues like climate change or cancer is deemed unrealistic without addressing deeper societal or behavioral factors.

### **3. Ethical and Economic Implications**
- **Privatization vs. Public Utility**: Debate erupts over framing AI access as a "human right." Critics liken privatized AI to commodified utilities (e.g., bottled water), advocating for government-provided access to prevent corporate monopolies.
- **Global Inequality**: Concerns arise that AI could exacerbate colonialism-like dynamics, benefiting wealthy nations while leaving developing regions behind. Others humorously suggest AI might inadvertently address poverty through job creation or efficiency gains.

### **4. Technical and Logistical Hurdles**
- **Infrastructure Realities**: Building gigawatt-scale data centers faces challenges in chip supply chains, construction speed, and labor conditions. Comparisons are drawn to stalled megaprojects (e.g., TSMC’s Arizona fab delays).
- **Regulatory and Financial Barriers**: Questions linger about funding models, partnerships (e.g., Nvidia, TSMC), and navigating U.S. permitting processes for energy and construction.

### **5. Broader Societal Impact**
- **Climate Priorities**: Some argue focusing on AI distracts from urgent climate action, while others speculate AI could optimize energy use or accelerate research.
- **Economic Reshaping**: The proposal’s scale could redefine semiconductor, energy, and data center markets, though skeptics doubt its profitability compared to existing tech infrastructure.

### **Conclusion**
While Altman’s vision is acknowledged as transformative in theory, the community emphasizes unresolved challenges: environmental sustainability, ethical governance, and logistical feasibility. The discussion reflects tension between optimism for AI’s potential and skepticism about Silicon Valley’s ability to deliver on grand promises without exacerbating existing societal issues.