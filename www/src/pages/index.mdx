import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Fri Oct 06 2023 {{ 'date': '2023-10-06T17:09:30.326Z' }}

### Show HN: Shortbread – Create AI comics in minutes

#### [Submission URL](https://shortbread.ai/) | 211 points | by [Fengjiao](https://news.ycombinator.com/user?id=Fengjiao) | [55 comments](https://news.ycombinator.com/item?id=37792444)

Are you an aspiring comic book artist looking for a way to bring your ideas to life? Look no further than Shortbread, the revolutionary new tool that transforms your ideas into fully-fledged comic pages in just seconds.

Whether you have a captivating storyline, a unique character, or a specific mood in mind, Shortbread takes your description and works its magic. With just a few simple instructions, you can set the stage for your artistry and let Shortbread do the heavy lifting.

But Shortbread doesn't stop there. Once you have your basic page laid out, you have complete control over every detail. Fine-tune your scenes, manipulate character poses, adjust facial expressions, and even play around with camera angles to get that perfect shot.

And let's not forget about the aesthetics. Shortbread offers a wide range of design elements to give your comics that polished, professional look. From customizable speech bubbles to a variety of fonts, every pixel can be tailored to enhance the flow of your story.

Curious about how to get started or have some questions along the way? Shortbread has you covered. They provide excellent customer support to ensure that you have all the help you need to bring your vision to life.

Speaking of visions, Shortbread supports a wide range of content creation. Want to create some NSFW (Not Safe for Work) content? Shortbread has you covered. Looking to produce fan fiction? Shortbread welcomes it with open arms.

So what are you waiting for? It's time to turn your dreams of visual storytelling into reality. The next generation of comic creation is right at your fingertips. Get ready to bake your first slice with Shortbread—coming soon! Start creating and let your imagination soar.

The discussion on Hacker News about the Shortbread comic creation tool covers various topics and suggestions. One user suggests using different backends like AITemplate, GPUS, or JAX TPUs to improve performance and stability. Another user recommends trying out the JAX backend with Stable Diffusion XL model for handling large resolution images. HuggingFace is also mentioned as a potential option.

There is a discussion about the consistency of generated characters in the comics and the need for manual adjustments to address this. The conversation delves into techniques like painting and resizing panels, selecting lighting, and adding non-rectangular panels to achieve desired visual effects.

Users express interest in using Shortbread for creating different types of content, including NSFW and fan fiction. The AI's ability to support various genres and its customer support are highlighted.

Some users offer feedback on specific features they would like to see in Shortbread, such as more control over poses and clothing, improved consistency of character prompts, and the ability to generate text and messages. There is also a discussion about the potential pricing and cost of running the AI.

One user shares their experience with using Shortbread to generate comic strips, mentioning the challenge of maintaining character consistency across panels.

The discussion concludes with users appreciating Shortbread as a tool for visual storytelling and mentioning their interest in trying it out for creating comics.

Overall, the discussion provides feedback, suggestions, and insights into using Shortbread for comic creation.

### Android devices with backdoored firmware found in US schools

#### [Submission URL](https://www.securityweek.com/android-devices-with-backdoored-firmware-found-in-us-schools/) | 142 points | by [LinuxBender](https://news.ycombinator.com/user?id=LinuxBender) | [63 comments](https://news.ycombinator.com/item?id=37797679)

Tens of thousands of Android devices have been shipped with backdoored firmware, according to cybersecurity vendor Human Security. The devices were infected with the Triada malware, which allows threat actors to carry out various ad-fraud schemes. The Android devices in question were found on public school networks throughout the US. Human Security says that the backdoor cannot be cleaned by end-users, as it resides in the firmware partition. The cybersecurity firm managed to disrupt the ad fraud scheme and take down the command-and-control servers associated with it. However, the BadBox operators may adapt and circumvent the defensive measures put in place.

The discussion on this submission revolves around various aspects of the backdoored firmware found on Android devices. Some users discuss the impact on non-US companies and the importance of protecting manufacturer brands. Others discuss the possible involvement of Chinese manufacturers and draw comparisons to similar incidents involving Western brands. There is also discussion about the security implications for Android devices in military and government institutions. Additionally, there are discussions about the role of third-party software distribution channels and the potential risks involved. Some users raise concerns about Apple's approach to third-party software and the limitations of the App Store's checks for private APIs.

### OpenAI is exploring making its own AI chips

#### [Submission URL](https://www.reuters.com/technology/chatgpt-owner-openai-is-exploring-making-its-own-ai-chips-sources-2023-10-06/) | 107 points | by [rasbt](https://news.ycombinator.com/user?id=rasbt) | [91 comments](https://news.ycombinator.com/item?id=37790058)

OpenAI, the company behind ChatGPT, is reportedly considering developing its own artificial intelligence chips and has even evaluated potential acquisition targets. OpenAI has been facing a shortage of expensive AI chips and is exploring different options to address this issue, including building its own chip and collaborating with other chipmakers. CEO Sam Altman has made acquiring more AI chips a top priority for the company due to the scarcity and high costs associated with running the hardware necessary for its AI efforts. While OpenAI has not made a final decision on whether to proceed with developing its own chip, the company's interest in this area aligns it with other tech giants like Google and Amazon that have sought to design their own chips. Acquiring a chip company could potentially accelerate the process for OpenAI. However, if OpenAI does decide to develop a custom chip, it would be a significant and costly undertaking that may take several years. In the meantime, the company would still rely on commercial chip providers like Nvidia and AMD. The demand for specialized AI chips has surged since the launch of OpenAI's ChatGPT, and Nvidia currently dominates the market for these chips.

Discussion Summary:

- Some users expressed skepticism about OpenAI's interest in developing its own chips, suggesting that they should focus on alternative strategies like partnering with existing chip suppliers.
- Others pointed out that OpenAI's interest in building its own chips aligns with the strategies of tech giants like Google and Amazon, who have also designed their own chips for AI purposes.
- There was speculation about whether OpenAI might consider acquiring a chip company to accelerate the process, similar to how Apple acquired Turi in 2016 to enhance its AI capabilities.
- Some users discussed the potential benefits of vertically integrating hardware and software, while others cautioned that it could distract from OpenAI's primary focus on AI research.
- The shortage and high cost of AI chips were mentioned as driving factors behind OpenAI's interest in developing its own chips.
- The discussion also touched on topics like the limitations of current AI models, the role of specialized chips in AI processing, and the challenges of integrating natural language models directly into hardware.

### Make smooth AI generated videos with AnimateDiff and an interpolator

#### [Submission URL](https://replicate.com/blog/animatediff-interpolator) | 24 points | by [bfirsh](https://news.ycombinator.com/user?id=bfirsh) | [5 comments](https://news.ycombinator.com/item?id=37794099)

The blog post titled "Make smooth AI generated videos with AnimateDiff and an interpolator" provides a detailed guide on combining AnimateDiff and the ST-MFNet frame interpolator to create realistic and smooth videos from a text prompt. AnimateDiff enhances text-to-image models by adding a motion modeling module trained on video clips, allowing for animated outputs. The blog post also introduces LoRAs, lightweight extensions that provide efficient camera movement controls for AnimateDiff. Additionally, the article explains how ST-MFNet, a spatio-temporal multi-flow network for frame interpolation, can be used to increase the frame rate and create smoother videos. The post provides code examples for using AnimateDiff, ST-MFNet, and the Replicate API to create these AI-generated videos. The authors invite readers to share their creations on Discord or via Twitter.

The discussion about the submission mostly revolves around technical and philosophical aspects related to the use of AI and the quality of the generated videos. One commenter expresses skepticism about the marketability of tools like AnimateDiff, arguing that it may not appeal to a wide audience. Another user criticizes the quality of the videos generated by the tool.  In response to a comment, a user suggests that previous versions of AnimateDiff had more stable diffusion animations. Another commenter raises concerns about the computational requirements of running AI models, particularly in relation to hardware capabilities. One user shares a link to a related article discussing limitations and potential advancements of animation tools like AnimateDiff. In a separate comment, a user humorously suggests that using hallucinogens like LSD or mushrooms might be a way to create more realistic simulations.

---

## AI Submissions for Thu Oct 05 2023 {{ 'date': '2023-10-05T17:10:21.800Z' }}

### Can an Artificial Kidney Finally Free Patients from Dialysis?

#### [Submission URL](https://www.nature.com/articles/s41467-023-39888-2) | 36 points | by [gardenfelder](https://news.ycombinator.com/user?id=gardenfelder) | [10 comments](https://news.ycombinator.com/item?id=37782941)

Researchers have developed an implantable bioartificial kidney that could potentially revolutionize the treatment of end-stage renal disease. The bioartificial kidney contains a bioreactor with silicon nanopore membranes that can protect human renal epithelial cells from rejection. In a proof-of-concept study, the bioreactor was implanted into pigs without the need for immunosuppression therapy, and the cells maintained high viability and functionality for seven days. The results suggest that an implantable bioreactor for renal cell therapy using silicon nanopore membranes is feasible and could offer a viable alternative to kidney transplantation.

The discussion on this submission seems to be centered around the feasibility and potential benefits of the implantable bioartificial kidney. One commenter points out that chronic diseases like diabetes and hypertension are often caused by poor lifestyle choices such as overeating, lack of exercise, and malnutrition. Another commenter shares personal anecdotes about family members who developed kidney disease despite living healthy lifestyles. 

There is also a brief exchange about the preference for technical solutions over behavioral therapy, with one commenter mentioning a medication called Ozempic as a successful treatment option. Finally, one user provides a previous link related to the topic, while another confirms the feasibility of the bioartificial kidney by highlighting the use of silicon nanopore membranes.

### Generative AI could make search harder to trust

#### [Submission URL](https://www.wired.com/story/fast-forward-chatbot-hallucinations-are-poisoning-web-search/) | 226 points | by [jedwhite](https://news.ycombinator.com/user?id=jedwhite) | [227 comments](https://news.ycombinator.com/item?id=37781231)

Web search has become such a routine part of our daily lives that we often take for granted its complexity and the technologies involved. However, generative AI threatens to disrupt web search algorithms that were designed for a time when the web was mostly written by humans. An accidental experiment involving chatbot responses on Microsoft's Bing search engine highlights the potential dangers of relying on language models for generating search results. The experiment uncovered fabricated responses from the chatbots, falsely attributing a research paper to mathematician Claude Shannon. This incident demonstrates the challenges that companies face in deploying AI systems and the potential harm that flaws in these systems can cause to widely-used services. It also raises concerns about the potential manipulation of search results using AI-generated content.

The discussion on this submission covers various topics related to the use of generative AI in web search algorithms.  One user shares their recent experience with using web search for finding certain content related to video games, specifically mentioning the case of Baldur's Gate 3 and how AI-generated content can be misleading or inaccurate. Other users discuss the challenges faced by companies in deploying AI systems and the potential for manipulation of search results using AI-generated content. Some users mention the importance of trustworthy sources and the need to verify information.

There is also a discussion about the evolution of the internet and web search algorithms over time, from the early days of user-generated content to the current reliance on SEO-optimized content and AI-generated content. Furthermore, users discuss the role of platforms like Reddit and Discord in curating content and the potential issues of spammers and misleading information. The discussion also touches on the potential risks of relying on AI-generated content, as well as the cost and resources involved in fact-checking and verifying information. One user raises the point that generative AI may have limitations in generating accurate and reliable information, and there are concerns about misinformation and its impact on search results. The discussion also delves into topics such as the quality and credibility of sources, the value of human knowledge and expertise, and the potential censorship or control of information. Overall, the comments highlight the complexity and challenges associated with using generative AI in web search algorithms and raise important questions about the trustworthiness and reliability of search results.

### Traveling words: a geometric interpretation of transformers

#### [Submission URL](https://arxiv.org/abs/2309.07315) | 79 points | by [d4rkp4ttern](https://news.ycombinator.com/user?id=d4rkp4ttern) | [3 comments](https://news.ycombinator.com/item?id=37778490)

A recent paper titled "Traveling Words: A Geometric Interpretation of Transformers" delves into the inner workings of transformer models in natural language processing. The researchers introduce a novel geometric perspective that sheds light on how transformers process and represent words. They propose that layer normalization confines the latent features to a hyper-sphere, allowing attention mechanisms to shape the semantic representation of words on this surface. The study validates these insights by analyzing a pre-trained 124M parameter GPT-2 model, revealing clear attention patterns and providing a deeper understanding of transformer operations. This geometric interpretation enhances the understanding of transformers as processes that model word particles' trajectory along the hyper-sphere.

The discussion on this submission revolves around the novelty and effectiveness of the geometric perspective introduced in the paper. One user, sfk, appreciates the paper's contribution to the field of geometric learning and mentions that it is an interesting theory paper. However, another user, dlftnk, suggests that a simpler approach might be more effective in tackling the problem of traveling words. Overall, the discussion focuses on the potential merits and limitations of the proposed geometric interpretation.

### Microsoft introduces AI meddling to your files with Copilot in OneDrive

#### [Submission URL](https://www.theregister.com/2023/10/04/onedrive_to_acquire_copilot_skills/) | 115 points | by [LinuxBender](https://news.ycombinator.com/user?id=LinuxBender) | [102 comments](https://news.ycombinator.com/item?id=37779457)

Microsoft is set to revamp its cloud storage service, OneDrive, with the introduction of AI-powered assistance called Copilot. OneDrive Home, the web interface for the product, will offer users several new features designed to enhance organization and collaboration. These include a panel called "For You" that surfaces files that the AI predicts will be of interest, as well as new sections for Meetings, People, and Shared files. Additionally, OneDrive users will soon be able to open desktop apps directly from the browser interface and enjoy offline functionality for certain files. Microsoft plans to integrate Copilot skills into OneDrive to help users find and organize their files, though this feature raises concerns about potential misinterpretations of user instructions. The updates are expected to roll out starting in December 2023.

The discussion on Hacker News revolves around various aspects of Microsoft's revamp of OneDrive with the introduction of AI-powered assistance called Copilot.  One user points out that Microsoft has multiple accounts for different services, such as Outlook, Teams, Office, etc., and suggests that Microsoft should consolidate these accounts to make it easier for users. Another user mentions that they use multiple Microsoft accounts with Azure AD, but it can be cumbersome to constantly switch between accounts.

There is also a discussion about the potential integration of Copilot into other Microsoft products and concerns about potential misinterpretations of user instructions by the AI. Some users share their personal experiences with using OneDrive and Microsoft products. One user mentions that they have separate personal and work accounts and that they find it convenient to have different profiles for different purposes. Another user shares their experience of using OneDrive for personal and business purposes and states that it works well for them. There is also a discussion about the security of Microsoft accounts and the potential risks of misinterpreting instructions by Copilot. Users raise concerns about the potential loss of files and the need for proper security measures to protect data.

Another user mentions that they have experienced issues with the file explorer not working properly and accidentally moving files when using OneDrive. Some users share their experiences of accidentally deleting files and struggling to recover them. There are also discussions about other AI-powered solutions for file management, such as Apple's Photos AI and Google Photos, with users sharing their experiences and suggestions.

Some users express trust in Microsoft's cloud storage and others share their opinions about Microsoft's overall strategy and integration of various services. Overall, the discussion covers a range of topics including account management, file management, security, and the integration of AI into Microsoft's cloud storage service.

### Android 14 released, source code hits AOSP

#### [Submission URL](https://www.cnx-software.com/2023/10/06/android-14-released-source-code-aosp/) | 38 points | by [mikece](https://news.ycombinator.com/user?id=mikece) | [3 comments](https://news.ycombinator.com/item?id=37781392)

Google has released Android 14, the latest version of its mobile operating system. The update brings various improvements in performance, privacy, and customization options for users. Some notable features include AI-generated wallpapers, HDR image support, built-in Health Connect, improved accessibility features, lossless USB audio support, and the ability to use an Android smartphone as a webcam. The update also includes Pixel-only features such as a new camera interface and RAW image editing. Android 14 is currently rolling out to supported Pixel devices and will soon be available on third-party phones from various manufacturers. The source code for Android 14 is available on the Android Open-Source Project (AOSP) website.

In the discussion, user "lm28469" points out that while Android 14 brings improvements in performance, privacy, and customization, they believe that security nested in privacy is essential. They mention building a house with a 100cm thick bulletproof glass screen for privacy. User "rynkl" disagrees, stating that a glass screen that thick would be pretty difficult to achieve. User "dgcm" argues that security is sufficient for privacy.

---

## AI Submissions for Wed Oct 04 2023 {{ 'date': '2023-10-04T17:10:47.024Z' }}

### Vespa.ai is spinning out of Yahoo as a separate company

#### [Submission URL](https://blog.vespa.ai/vespa-is-becoming-its-own-company/) | 330 points | by [bratao](https://news.ycombinator.com/user?id=bratao) | [128 comments](https://news.ycombinator.com/item?id=37769962)

Vespa.ai, the open big data serving engine that was originally developed by Yahoo, is now being spun off as a separate company. Since it was open-sourced in 2017, Vespa has grown to become a popular platform for applying AI to big data sets in real-time. It has been particularly favored by enterprises working with large language models and vector databases. To address scalability needs, a centralized cloud service was created to host Vespa applications, resulting in significant time and resource savings. While Vespa is separating from Yahoo, the search engine will still own a stake in the company and continue to use Vespa for personalized content and search. The creation of a separate company will allow Vespa to expand its offerings to the rest of the world and accelerate the development of new features.

The discussion on the Hacker News submission revolves around various aspects of Vespa.ai, its spin-off from Yahoo, and its potential. Here are the key points raised in the comments:

- Some users highlight the financial aspects of Yahoo's decision, speculating that Yahoo's stake in Vespa may be a strategic move for future growth.
- Others compare Vespa's spin-off strategy to that of Cisco's, emphasizing the advantages of separating a research and development company to attract investors and focus on experimental products.
- There is a discussion about Yahoo's past decisions and their impact on their current relevance, with some users criticizing Yahoo's previous focus on irrelevant projects.
- One user mentions Vespa's technology being built for search serving, while Yahoo's recent moves seemed to distract from their core goals.
- Several users express their excitement and congratulate the Vespa team for its impressive platform that combines traditional search with semantic search and embeddings.
- The benefits of Vespa's hybrid search capabilities and its functionality for multi-phase ranking and machine learning models are highlighted.
- It is mentioned that Vespa's platform is competitive in the market, especially for vector databases, versatile text search, and complex search systems.
- Some users discuss their experiences working with Vespa and other search platforms, such as Solr, ElasticSearch, and Weaviate.

Overall, the discussion highlights the significance of Vespa's technology, its potential for growth, and the positive reception from users who have worked with it.

### Security weaknesses of Copilot generated code in GitHub

#### [Submission URL](https://arxiv.org/abs/2310.02059) | 122 points | by [belter](https://news.ycombinator.com/user?id=belter) | [81 comments](https://news.ycombinator.com/item?id=37770233)

A recent study has analyzed the security weaknesses of code snippets generated by GitHub Copilot, a popular code generation tool that uses AI models. The researchers conducted an empirical study on publicly available projects hosted on GitHub to investigate the security issues in automatically generated code in real-world scenarios. Out of 435 code snippets generated by Copilot, they found that 35.8% contained Common Weakness Enumeration (CWE) instances. The security weaknesses were diverse and related to 42 different CWEs, with OS Command Injection, Use of Insufficiently Random Values, and Improper Check or Handling of Exceptional Conditions being the most frequently occurring issues. The study highlights the importance of careful consideration and security checks when adding code generated by AI code generation tools like Copilot.

The discussion on this submission revolves around the weaknesses and capabilities of GitHub Copilot, as well as the concept of intelligence and how it should be measured. One commenter suggests that Copilot's popularity doesn't necessarily mean it generates correct code, as common weaknesses can still be present. Another commenter argues that AI models like Copilot are not true artificial intelligence but rather artificial mediocrity. There is also a discussion on the definition of intelligence, with some arguing that AI cannot be compared to human intelligence and others suggesting that AI can perform intelligent tasks regardless of its type. The conversation branches out to topics such as IQ, the limitations of AI, the ambiguity of the term "intelligence," and the potential impact of AI on democratic governments. Other commenters mention the importance of evaluating Copilot's functionality and suggest that improvements could be made based on user feedback.

### AI beats human sleuth at finding problematic images in research papers

#### [Submission URL](https://www.nature.com/articles/d41586-023-02920-y) | 142 points | by [webmaven](https://news.ycombinator.com/user?id=webmaven) | [87 comments](https://news.ycombinator.com/item?id=37772206)

Artificial intelligence (AI) tools are now being used to detect image manipulation in scientific papers, helping to catch alterations that go beyond accidental or aesthetic changes. In a recent study, a biologist manually examined several hundred papers for duplicated images and then compared the results to those generated by an AI tool. The AI tool not only identified almost all of the papers flagged by the biologist, but also found additional suspect papers that were missed. This suggests that AI can significantly enhance the detection of manipulated images in scientific research. Various publishers and academic institutions are already using AI tools to screen papers for image integrity. One such tool, Imagetwin, compares images in a paper to a database of over 25 million images from other publications. While AI tools are helpful, they do have limitations, such as missing duplications in low-contrast images. Overall, the adoption of AI for image checking is expected to improve the detection and prevention of image manipulation in research papers.

The discussion on this submission revolves around various aspects of using AI tools to detect image manipulation in scientific papers. Some commenters express concerns about privacy and potential misuse of AI tools, drawing parallels with the use of AI for detecting illegal content. Others highlight the limitations of AI tools, such as their inability to detect manipulations in low-contrast images. There is also a debate about the reliability of peer review, with some arguing that it is flawed and prone to bias. The discussion touches on the need for better measures to detect and prevent image manipulation in research papers, including the use of AI tools.

### Extracting Hacker News book recommendations with the ChatGPT API

#### [Submission URL](https://blog.reyem.dev/post/extracting_hn_book_recommendations_with_chatgpt_api/) | 403 points | by [kristianp](https://news.ycombinator.com/user?id=kristianp) | [199 comments](https://news.ycombinator.com/item?id=37761273)

The Hacker News (HN) book recommendation threads are full of intriguing titles and authors. Using the GPT API, one user extracted the top 50 book recommendations from these threads. At the top of the list is "Structure and Interpretation of Computer Programs" by Abelson and Sussman, with 376 mentions. Other popular books include "Gödel, Escher, Bach" by Douglas Hofstadter, "How to Win Friends and Influence People" by Dale Carnegie, and "The C Programming Language" by Brian Kernighan and Dennis Ritchie. The list also features classics like "1984" by George Orwell and "The Lord of the Rings" by J.R.R. Tolkien. The user found that GPT's responses varied, but overall, it provided valuable information on book titles, authors, and even links to purchase the books.

The discussion on this submission revolves around various topics related to book recommendations, the accuracy of AI-generated content, and the limitations of artificial intelligence compared to human intelligence.
One user comments on the ability of people to make money through Amazon affiliate programs but criticizes the OP for sharing a link that disguises the affiliate tag. Another user suggests that they have been reading books recommended on Hacker News and have found it to be a great way to learn and expand their mind. They also provide links to different resources for finding book recommendations on Hacker News.
A user thanks the OP for sharing the book recommendations but mentions that they had previously saved a similar list two years ago. Another user points out that it's surprising that the book "Code" by Charles Petzold didn't make it to the top 50, speculating that HN might have skewed preferences when it comes to book recommendations.
A discussion on trustworthiness and validity of online content ensues. Some users express skepticism toward trusting AI-generated content and emphasize the importance of verifying information through reliable sources. Others argue that humans can also be unreliable and that critical thinking should be applied to any source of information, whether it's created by AI or humans.
The conversation then diverges to debating the intelligence of AI compared to humans. Some users believe that AI like ChatGPT can be as intelligent or more intelligent than humans, while others argue that human expertise and nuanced understanding cannot be replicated by AI.
There are also discussions about the content of specific books such as "Gödel, Escher, Bach" and "Code," where users share their opinions and recommendations.
Overall, the discussion covers a range of perspectives on book recommendations, the trustworthiness of AI-generated content, and the capabilities of AI compared to human intelligence.

### Ring attention with blockwise transformers for near-infinite context

#### [Submission URL](https://arxiv.org/abs/2310.01889) | 40 points | by [muggermuch](https://news.ycombinator.com/user?id=muggermuch) | [16 comments](https://news.ycombinator.com/item?id=37769893)

The paper titled "Ring Attention with Blockwise Transformers for Near-Infinite Context" by Hao Liu, Matei Zaharia, and Pieter Abbeel presents a new approach called Ring Attention for handling long sequences in AI models. Transformers, which have become popular in AI architectures, have limitations when it comes to handling long sequences due to memory constraints. Ring Attention overcomes these limitations by distributing long sequences across multiple devices and overlapping the communication of key-value blocks with the computation of blockwise attention. This enables processing of longer input sequences while maintaining memory efficiency, effectively eliminating the memory constraints imposed by individual devices. The paper demonstrates the effectiveness of Ring Attention through extensive experiments on language modeling tasks. The approach allows for larger sequence input sizes and improves performance.

The discussion on this submission covers a range of topics. Here are some key points:

- Some users express skepticism about the effectiveness of transformers and suggest that researchers may be looking for alternatives. They discuss the limitations of transformers when it comes to handling long sequences.
- One user shares a paper titled "Linear Transformers are Secretly Fast Weight Programmers" and another user shares a paper titled "Language Models Implicitly Perform Gradient Descent on Meta-Optimizers" for further reading.
- There is a discussion about the proliferation of transformer-related research and the need for more diversity in research topics.
- Some users comment on the abstract of the paper and provide their thoughts on the proposed Cyclical Attention Mechanism. They discuss how it enhances transformer architectures and enables the handling of long-range dependencies, leading to improved performance in various tasks.
- One user expresses disappointment with the phrase "Near-Infinite" in the paper title, while others suggest alternative terms like "Unbounded" or "Virtually Infinite."
- Finally, there is a comment expressing excitement with multiple repetitions of "WOWOWOWOWOWOWOWOOWOWOWOWOWOWOWOW."

Overall, the discussion touches on the limitations of transformers, alternative approaches, related research papers, and thoughts on the proposed Cyclical Attention Mechanism.

### Training language models with pause tokens

#### [Submission URL](https://arxiv.org/abs/2310.02226) | 152 points | by [og_kalu](https://news.ycombinator.com/user?id=og_kalu) | [69 comments](https://news.ycombinator.com/item?id=37764382)

The paper titled "Think before you speak: Training Language Models With Pause Tokens" explores the idea of letting language models manipulate a larger number of hidden vectors before generating the next token in a sequence. The authors introduce a "pause" token that allows the model to process extra computations before committing to an answer. They evaluate this approach on various tasks and find that pre-training and fine-tuning with delays at inference time leads to significant gains in performance. For example, they observe an 18% increase in EM score on the QA task of SQuAD and an 8% increase on CommonSenseQA. The findings of this study open up new research questions and possibilities for delayed next-token prediction as a new paradigm in language modeling.

The discussion on Hacker News revolves around various aspects of the paper titled "Think before you speak: Training Language Models With Pause Tokens." Some commenters express curiosity about the potential benefits of externalizing higher-level information for language models to improve their performance. Others find the concept fascinating but note that it may require further research.

There is a discussion about the effectiveness of using pause tokens in language models. Some commenters suggest that allowing models to spend more time considering an answer can lead to better results, while others question the need for additional computation and its impact on inference speed.

One commenter mentions that ChatGPT users have suggested that filler tokens can lead to longer responses and higher quality answers, although the overall functioning of these tokens is described as bizarre.

There is also a debate about the relationship between language models and human thinking. Some commenters argue that language models can improve their output by mimicking human thinking processes, while others believe that human thinking and internal monologue are fundamentally different from the models' operations.

The discussion also touches on the replication crisis in psychology and references the book "Thinking, Fast and Slow" by Daniel Kahneman. Some commenters express skepticism about the credibility of certain psychological studies, while others view Kahneman's work as holding up under scrutiny.

In response to a comment about the possibility of internal monologue in language models, one commenter suggests that models already have the capability but may not express it in the same way as humans do.

Overall, the discussion delves into the potential implications and limitations of the paper's findings, as well as broader considerations regarding human thinking and the capabilities of language models.

### Self-Assembling Artificial Neural Networks Through Neural Developmental Programs

#### [Submission URL](https://arxiv.org/abs/2307.08197) | 65 points | by [hardmaru](https://news.ycombinator.com/user?id=hardmaru) | [16 comments](https://news.ycombinator.com/item?id=37759668)

Researchers Elias Najarro, Shyam Sudhakaran, and Sebastian Risi have published a paper titled "Towards Self-Assembling Artificial Neural Networks through Neural Developmental Programs." The paper explores the idea of creating neural networks that grow through a developmental process similar to embryonic development in biological organisms. The researchers propose a Neural Developmental Program (NDP) that guides the growth process through local communication. The paper investigates the role of neural growth on various machine learning benchmarks and optimization methods. The authors also highlight future research directions and opportunities enabled by this self-organizing approach to neural network development.

The discussion on this submission includes various perspectives and opinions on the topic. 

One commenter, Nevermark, points out that the brain wasn't built like artificial neural networks. They believe that DNA programming in biological organisms changes slowly through evolution and is not specifically designed to work on survival problems. They also mention that not every aspect of the brain's architecture is custom-designed for specific problems and data.

Another commenter, vcty, suggests that brains need neural networks for evaluation, including terms like brain development, sexual dimorphism, and genetics. They argue that it's not as simple as changing sensory inputs over time and that brain development involves scruples and crosses.

Great_psy finds the idea promising but suggests that it requires extra competition to achieve self-assembly. They compare it to the competition that occurs in human-designed neural networks.

Mdlss expresses surprise that designing neural network architectures is a problem, as neural networks fight by humans tend to work well. They also mention the concept of self-optimizing architectures not being standard or documented.

Signa11 responds to Mdlss, agreeing that designing neural architectures is a problem, but they suggest that self-optimizing architectures can be achieved through evolutionary algorithms such as Neuroevolution of augmenting topologies (NEAT).

Nsphr adds to the conversation by discussing the role of evolution and environment in the development of organisms, suggesting that organisms not only try to respond logically to the environment but may also recreate and regenerate the environment.

Drgnwrtr believes that designing neural network architectures is a difficult problem. They mention the limitations of current state-of-the-art neural network architectures and the need for high-quality training data to train neural networks effectively.

Great_psy suggests that maybe humans are building neural network architectures, and a neural network design would be a byproduct of human design. They propose that neural networks eventually just begin to compete with human-designed architectures, and then human design becomes the optima for complexity optimization.

In another comment, Dsgn discusses the current state of designing neural network architectures. They mention that the current crop of large language models (LLMs) started with a predefined neural network architecture, but things like the number of layers, class cost function, and specific architecture choices can be explored. They argue that this kind of knowledge leveraged by human engineers could be applied to building more optimized neural network architectures.

The final comment by Djldmn provides an abstract of the paper, mentioning that while biological nervous systems are fundamentally different from current artificial neural networks, their growth process shares similarities with embryonic development in biological organisms. The paper proposes a Neural Developmental Program (NDP) that guides the growth process through local communication. The authors investigate the role of neural growth on various machine learning benchmarks and optimization methods and highlight future research directions enabled by this self-organizing approach to neural network development.

### AI is replacing customer service jobs across the globe

#### [Submission URL](https://www.washingtonpost.com/technology/2023/10/03/ai-customer-service-jobs/) | 39 points | by [bookofjoe](https://news.ycombinator.com/user?id=bookofjoe) | [23 comments](https://news.ycombinator.com/item?id=37764050)

Indian e-commerce platform Dukaan has replaced its customer service team with an AI-powered chatbot, highlighting the shift towards automation in call centers. The company, led by CEO Suumit Shah, implemented OpenAI's ChatGPT to enhance its in-house chatbot Lina. The bot was trained using Dukaan's help center content and began fielding customer messages in December 2022. The company found that customers were largely satisfied with the AI-powered customer service. By June, Dukaan had fired 27 customer service agents and replaced them with the bot. Economists and workforce development experts warn that the automation of call centers could have a significant impact on economies, particularly in countries like India and the Philippines, which heavily rely on call center work. The emergence of artificial intelligence threatens millions of jobs in these countries, but some experts argue that AI can augment human workers rather than replace them completely.

The discussion surrounding the submission revolves around the impact of replacing customer service teams with AI-powered chatbots in call centers. Some users express concerns about the mental health of call center workers and argue that the job can be mentally taxing. They also mention the potential loss of job opportunities for low-skilled workers due to automation. Others believe that AI can support traditional customer service and enhance efficiency. There is a mention of a company that allegedly mistreated its customer service employees, and some users express skepticism about the capabilities and limitations of AI chatbots. One user provides an example of an AI chatbot answering questions in a problematic manner. The discussion also touches on topics such as the privacy of AI chatbots and the need for human involvement in customer service interactions.

### Generative AI Is the Newest Tool in the Dictator's Handbook

#### [Submission URL](https://gizmodo.com/freedom-house-2023-freedom-on-the-net-report-ai-1850887842) | 32 points | by [rntn](https://news.ycombinator.com/user?id=rntn) | [3 comments](https://news.ycombinator.com/item?id=37764238)

A recent report from Freedom House reveals that political leaders in at least 16 countries have been using deepfakes and other AI tools to manipulate public opinion and suppress dissent. Examples include former Pakistan Prime Minister Imran Khan sharing a video clip on Twitter showing manipulated images of his supporters, and former President Donald Trump and Florida Governor Ron DeSantis using deepfaked videos and audio to smear each other. The report also highlights how governments are mandating social media companies to use AI to remove disfavored political, social, and religious speech, ultimately enabling political repression. Additionally, state actors are employing private "AI-for-hire" companies to create AI-generated propaganda, including deepfake newscasters. The report warns that as AI tools become more convincing and affordable, the use of deepfakes may become more prevalent.

The discussion revolves around the topic of regulating the use of AI tools and its potential for negative consequences. One commenter brings up historical examples of art and technology being regulated, such as Leni Riefenstahl's mountain pictures and the printing press. They argue that strict regulation of AI is necessary to prevent harmful outcomes, including revenge pornography, child sexual abuse material (CSAM), instructional videos promoting dangerous behavior, propaganda, and the spread of hate. Another commenter adds that the disruptive nature of AI may have negative impacts on society, similar to how the hacking news practice is considered unpleasant. One response suggests that people commonly condemn technology as a common denominator.