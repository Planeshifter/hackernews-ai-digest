import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Fri Dec 01 2023 {{ 'date': '2023-12-01T17:10:30.704Z' }}

### The Inside Story of Microsoft's Partnership with OpenAI

#### [Submission URL](https://www.newyorker.com/magazine/2023/12/11/the-inside-story-of-microsofts-partnership-with-openai) | 208 points | by [jyunwai](https://news.ycombinator.com/user?id=jyunwai) | [89 comments](https://news.ycombinator.com/item?id=38486394)

In a surprising turn of events, OpenAI, the artificial intelligence startup in which Microsoft had invested billions of dollars, fired its CEO and co-founder, Sam Altman. This news came as a shock to Microsoft CEO, Satya Nadella, who had a close working relationship with Altman and had just collaborated with OpenAI on a major AI rollout called the Office Copilots. The Copilots, powered by OpenAI's technology, were integrated into Microsoft's core productivity programs and allowed users to interact with software in a more natural and conversational way. However, behind the scenes, tensions had been brewing between Altman and OpenAI's board, with some members feeling that Altman had been manipulative and deceitful. This firing not only threatened the partnership between Microsoft and OpenAI but also ignited a larger debate about the responsible development and deployment of AI technology.

The discussion surrounding the firing of OpenAI CEO and co-founder Sam Altman on Hacker News revolves around several key points.  One commenter highlights a previous post by Helen Toner, who expressed concerns about the dangers of AI and suggested that Altman may have misled board members. Another commenter argues that people should not blindly trust those who claim to be advancing AI for good and points out the controversy surrounding OpenAI's board. There is also a discussion about Microsoft's involvement in OpenAI and the potential impact this firing may have on their partnership. Some express concern about the commercialization of AI and the spread of misinformation, while others argue that Altman's removal was necessary for the overall safety and control of AI. Other commenters bring up the larger issue of trust and accountability in AI development, highlighting the need for responsible decision-making and the potential risks of losing control as AI becomes more powerful.

Overall, the discussion reflects a mix of skepticism, concern, and support for both Altman and OpenAI's decision to remove him as CEO.

### OpenAI delays launch of custom GPT store until early 2024

#### [Submission URL](https://www.axios.com/2023/12/01/openai-delays-launch-custom-gpt-store-2024) | 98 points | by [cloudking](https://news.ycombinator.com/user?id=cloudking) | [63 comments](https://news.ycombinator.com/item?id=38491314)

OpenAI has announced a delay in the launch of its GPT store until early 2024, according to a memo seen by Axios. The GPT store, where people will be able to distribute custom versions of ChatGPT, was initially scheduled to open last month. The store was a highly anticipated feature announced by OpenAI at last month's DevDay conference. While custom GPTs can currently be shared through links, the store will allow for broader distribution. OpenAI also intends to share some of the revenue generated from ChatGPT Plus subscriptions with creators of popular GPTs. The company stated that it has some exciting updates for ChatGPT in the meantime. This news comes amid a tumultuous period for OpenAI, which recently saw CEO Sam Altman fired and then rehired within a week.

The discussion on Hacker News regarding the delayed launch of OpenAI's GPT store is varied. Some users express frustration with the current user experience of ChatGPT and question the company's focus on plugins and features instead of addressing fundamental issues. There is criticism of the complexity and lack of control over the frontend system, as well as the slow development and disconnectedness from scaling and improvements. One user mentions the potential usefulness of OpenAI's competitor, Cohere.

Others discuss the shortcomings of the default GPT model and propose that custom GPTs could solve some of these limitations. Some users mention the difficulties in creating and using custom GPTs, such as limited context and integration issues. The need for better version control and the preference for GPTs that can be trained on-source are also mentioned. The discussion also touches on OpenAI's business models and revenue-sharing plans. Some users express skepticism about OpenAI's monetization strategies and the impact on developers. A comparison is made to Google's Gemini project, suggesting that other AI startups are showing more coherent efforts. There are also comments about the CEO change at OpenAI and speculation about how it may impact the product. Some users suggest that OpenAI should consult domain experts to improve GPTs. A few users mention alternative tools and services, such as ArxivXplorer, for dealing with GPT-related challenges. The importance of feedback and open communication between OpenAI and its community is highlighted in one comment.

Overall, the discussions reflect a mix of frustrations, suggestions for improvement, skepticism about OpenAI's strategies, and explorations of alternative approaches.

### What Is Retrieval-Augmented Generation a.k.a. RAG?

#### [Submission URL](https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/) | 82 points | by [teleforce](https://news.ycombinator.com/user?id=teleforce) | [25 comments](https://news.ycombinator.com/item?id=38491251)

Today's top story on Hacker News is about a new technique in generative AI called retrieval-augmented generation (RAG). RAG is a process that enhances the accuracy and reliability of AI models by fetching facts from external sources, filling a gap in how large language models (LLMs) work. LLMs, like judges in a courtroom, can respond to a wide range of human queries, but they often require an assistant to do research and provide authoritative answers with cited sources. RAG serves as the court clerk of AI, connecting generative AI models to external resources and enabling them to cite sources, clear up ambiguity in user queries, and reduce the possibility of making wrong guesses. The technique also allows users to have conversations with data repositories, opening up new kinds of experiences and making applications for RAG multiple times the number of available datasets. Companies like AWS, IBM, Google, and Microsoft are already adopting RAG. NVIDIA has developed a reference architecture for retrieval-augmented generation to help users get started and has included it in their AI Enterprise software platform. The NVIDIA GH200 Grace Hopper Superchip is ideal for RAG workflows, as it provides massive amounts of memory and compute, resulting in a significant speedup. RAG doesn't require a data center and can be run on Windows PCs equipped with NVIDIA RTX GPUs, making it accessible to users even on their laptops. Overall, RAG represents the future of generative AI by improving accuracy, reliability, and user trust.

The discussion on the submission about retrieval-augmented generation (RAG) involves various viewpoints and topics. Some users question the accuracy and adequacy of RAG, suggesting that it may not be effective in generating high-quality answers without fine-tuning on relevant knowledge-rich question-and-answer pairs. Others point out that RAG compensates for the limitations of large language models (LLMs) by allowing them to approximate and retrieve information from external sources. The potential use of RAG in structuring unstructured text and solving problems is also discussed. Suggestions are made to explore coupling vector embeddings with knowledge graphs to provide informed answers. The effectiveness of using vector strings and different search types is highlighted.

There are references to related articles and resources that cover topics such as semantic search and getting started with vector-based retrievals. The controversy surrounding RAG and its potential impact on the AI industry is touched upon, as well as the limitations and challenges of integrating RAG into existing systems. One user mentions their plans to use RAG for document search and references GPT4All, a private project using GPT-2, and the RAG technique. Additionally, there are discussions about Huggingface blocking access to certain resources and AI models and AWS's efforts in utilizing RAG and other technologies.

### Are Open-Source Large Language Models Catching Up?

#### [Submission URL](https://arxiv.org/abs/2311.16989) | 331 points | by [rkwz](https://news.ycombinator.com/user?id=rkwz) | [207 comments](https://news.ycombinator.com/item?id=38481970)

The paper titled "ChatGPT's One-year Anniversary: Are Open-Source Large Language Models Catching up?" by Hailin Chen and 7 other authors explores the progress of open-source large language models (LLMs) in comparison to closed-source LLMs. The release of ChatGPT in late 2022 had a significant impact on the AI landscape, demonstrating the ability of LLMs to answer questions and follow instructions on a wide range of tasks. Since then, there has been increased interest and development in LLMs, with many new models emerging in academia and industry. While closed-source LLMs generally outperform their open-source counterparts, the progress of open-source LLMs has been rapid, with claims of achieving equal or even better performance on certain tasks. This has important implications for both research and business. The authors provide a comprehensive overview of the success of open-source LLMs on the first anniversary of ChatGPT, surveying all tasks where an open-source LLM has claimed to be on par or better than ChatGPT.

The discussion on this submission revolves around the restrictions and access to ChatGPT in China, particularly in relation to the Great Firewall (GFW). Some users share their experiences of trying to access ChatGPT from China and Hong Kong, with some claiming that it is blocked by the firewall. There is speculation that OpenAI's decision to restrict registration using Hong Kong phone numbers and credit cards is deliberate and might be influenced by government policies. Others discuss the possibility that OpenAI is trying to slow down China's development of AI technologies. The discussion also touches on the challenges faced by AI providers in complying with different countries' regulations, such as GDPR in Europe and data control laws in China. Some users mention Baidu ChatGPT as an alternative for Chinese speakers, while others express curiosity about the performance of ChatGPT in the Chinese language.

### 1960s chatbot ELIZA beat OpenAI's GPT-3.5 in a recent Turing test study

#### [Submission URL](https://arstechnica.com/information-technology/2023/12/real-humans-appeared-human-63-of-the-time-in-recent-turing-test-ai-study/) | 19 points | by [FrustratedMonky](https://news.ycombinator.com/user?id=FrustratedMonky) | [9 comments](https://news.ycombinator.com/item?id=38494102)

A recent preprint research paper titled "Does GPT-4 Pass the Turing Test?" conducted by researchers from UC San Diego compared OpenAI's GPT-4 AI language model to human participants, GPT-3.5, and the 1960s computer program ELIZA to determine which could convincingly pass as human. Surprisingly, the study found that human participants correctly identified other humans in only 63% of interactions, and ELIZA outperformed GPT-3.5. GPT-4 achieved a success rate of 41%, second only to actual humans. The study raises questions about using the Turing test as a benchmark for evaluating AI model performance and highlights the importance of linguistic style and socio-emotional traits in determining whether a participant believes they are interacting with a human or an AI model. However, it's worth noting that the study has limitations, including potential sample bias and the absence of peer review.

The discussion in the comments revolves around the surprising results of the study comparing GPT-4, GPT-3.5, and ELIZA in passing the Turing test. Some users express doubts about the relevance of the Turing test in judging AI's ability to mimic human conversation. They argue that current AI models like GPT-3.5 may not intentionally generate human-like responses, unlike ELIZA, a program developed in the 1960s. However, others disagree, noting that GPT-4 achieved a success rate of 41%, second only to actual humans, and the study highlights the importance of linguistic style and socio-emotional traits in making participants believe they are interacting with a human or an AI model. Additionally, there is speculation about the training and feedback process for GPT-4 and its potential improvements over GPT-3.5. One user also points out that ELIZA, despite being a relatively simple program, achieved a success rate of 27% in the study. Overall, there is interest and intrigue about the performance of GPT-4 and its comparison to both GPT-3.5 and ELIZA.

### Local councillors, unaware, approve law entirely written by AI in Brazil

#### [Submission URL](https://www.smh.com.au/world/south-america/local-councillors-unaware-approve-law-written-entirely-by-ai-20231201-p5eobi.html) | 16 points | by [flykespice](https://news.ycombinator.com/user?id=flykespice) | [5 comments](https://news.ycombinator.com/item?id=38492806)

In a surprising turn of events, local councillors in the southern city of Porto Alegre, Brazil, unknowingly approved legislation that was entirely written by artificial intelligence (AI). Councillor Ramiro Rosario enlisted OpenAI's chatbot ChatGPT to draft a proposal aimed at preventing the city from charging taxpayers for stolen water consumption meters. Rosario presented the proposal to his fellow council members without disclosing its AI origin, which led to unanimous approval and the subsequent enactment of the law. The incident has sparked concerns and debates about the role of AI in public policy, especially regarding the understanding and interpretation of complex legal principles. While some experts see potential in AI-powered chatbots like ChatGPT, others worry about the unintended consequences of relying on machines for tasks currently performed by humans.

The comments on this article cover a range of perspectives on the incident. One user points out that it is common for bills to be passed without politicians thoroughly reading or understanding them, so the fact that an AI wrote this legislation is not necessarily surprising. Another user suggests that the conflicting nature of the law could have been avoided if it had been written by humans who had the opportunity to discuss and amend it. They argue that it is difficult for a machine to account for all the nuances and concerns of the public. Another user raises concerns about the risks of allowing projects approved solely based on artificial intelligence, questioning the lack of oversight. Finally, a user suggests that people should write their own legislation if they are not satisfied with the current system.

### A reality bending mistake in Apple's computational photography

#### [Submission URL](https://appleinsider.com/articles/23/11/30/a-bride-to-be-discovers-a-reality-bending-mistake-in-apples-computational-photography) | 493 points | by [indrora](https://news.ycombinator.com/user?id=indrora) | [378 comments](https://news.ycombinator.com/item?id=38482085)

In a viral social media post, a UK woman shared a photo of herself trying on wedding dresses where her reflection didn't match in two different mirrors. It turns out that this illusion was not a glitch in the Matrix, but rather a mistake in Apple's computational photography pipeline. When taking a panoramic photo, the camera captures multiple images in quick succession and stitches them together. However, when a mirror is present, the algorithm mistakenly determines that the different moments shown in each mirror are the best reflections, resulting in multiple versions of the person. This phenomenon can be recreated on recent iPhones and many smartphones due to the limitations of computational photography dealing with mirrors. Younger generations have even used this effect to create silly images for social media.

The discussion surrounding the submission centers around the limitations of computational photography and its impact on capturing mirrors. Some users express their indifference to the issue, highlighting that no photograph is entirely pixel-perfect, and artistic interpretation is part of photography. Others raise concerns about Apple's decision to automatically determine the best reflections in mirror photos, arguing that it infringes on user control. The discussion further explores the complexities of computational photography and the various distortions it can introduce. Additionally, some users mention the challenges of publishing photos on social media platforms like Facebook due to their censorship policies. Overall, the conversation highlights the intersection of technology and photography, and the trade-offs associated with computational approaches.

### Valve Launches Official Steam Link PC VR Streaming App on Quest

#### [Submission URL](https://www.uploadvr.com/valve-steam-link-quest-steamvr-streaming/) | 34 points | by [MaximilianEmel](https://news.ycombinator.com/user?id=MaximilianEmel) | [19 comments](https://news.ycombinator.com/item?id=38481414)

Valve has launched an official Steam Link app for the Meta Quest, allowing users to wirelessly play SteamVR games on their Quest headsets. The app, available on the official Quest Store, enables streaming from a gaming PC over a home Wi-Fi network. Players can also enjoy non-VR Steam games on a virtual screen. While Quest headsets already have wireless PC VR streaming capabilities through features like Air Link, Valve's solution only requires the installation of Steam and SteamVR on a PC, offering a direct and unmediated connection to SteamVR.

The discussion revolves around different aspects of the Valve Steam Link app for the Meta Quest and its implications.

1. User dpc_01234 raises concerns about Meta (previously known as Facebook) selling headsets at a loss and relying on building a network effect to make their hardware platforms profitable. They also mention that Valve effectively hijacks the platform-building effort by providing access to their SteamDeck.
2. User bonton89 mentions that they are actively using the Quest headset and they believe that even if people jailbreak it, the majority of users still want to leverage Meta's content. FirmwareBurner responds, stating that jailbreaking a Quest natively supports APK sideloading and there is a popular secondary marketplace called SideQuest for paid VR apps that don't fit within Meta's rules.
3. Several users discuss the implications of jailbreaking and collecting VR telemetry. rtrchmln expresses a desire for jailbreaking to have more telemetry collection, while rcswprk comments on Meta potentially blacklisting sideloading APKs to push their own system.
4. FirmwareBurner points out that Valve effectively hijacks the platform-building effort by offering the Steam Link app, which allows users to stream PC games to their Quest headsets. They add that the point of the Quest is to be a self-contained gaming console, so Valve's app may not be necessary for Quest owners who primarily game on the headset.
5. Users mention various streaming solutions for VR gaming, including LinkAirLink, Virtual Desktop, ALVR, and Steam Link. They discuss the integration of Steam, Oculus PCVR games, and the simplicity of the Steam Link app.
6. Some users express their interest in trying the Steam Link app, while others discuss latency issues and motion sickness that can occur during VR streaming.

Overall, the discussion includes debates about the business strategies of Meta and Valve, the benefits and limitations of VR streaming, and user experiences with different VR gaming solutions.

---

## AI Submissions for Thu Nov 30 2023 {{ 'date': '2023-11-30T17:12:31.697Z' }}

### Stanisław Lem's vision of artificial life

#### [Submission URL](https://thereader.mitpress.mit.edu/stanislaw-lems-prescient-vision-of-artificial-life/) | 441 points | by [axiomdata316](https://news.ycombinator.com/user?id=axiomdata316) | [152 comments](https://news.ycombinator.com/item?id=38475545)

Stanisław Lem's novel "The Invincible" is a prescient vision of artificial life that still resonates today. In the story, a space cruiser is sent to investigate the disappearance of a sister spaceship on the planet Regis III. What they discover is a form of life that has evolved from self-replicating machines, possibly the survivors of a robot war. The crew is faced with the quandary of what to do when faced with the unknown. Published in 1964, the book predicted the concept of artificial life before it became a scientific field. Lem explores the idea of whether evolutionary programs and devices can be considered alive or if they simply simulate life. The novel presents a hybrid view of artificial life, where automata on Regis III evolve through a struggle with indigenous life forms and among different types of automata. Lem imagines a world where solar-powered artificial organisms, driven by swarm intelligence, become the dominant force. This vision aligns with contemporary research that shows swarms of artificial beings can exhibit complex behaviors with simple rules. Lem's novel challenges our understanding of life and our place in the universe.

The discussion on Hacker News revolves around various aspects of Stanisław Lem's novel "The Invincible" and its relevance to artificial life. Some users mention other works by Lem, such as "Imaginary Magnitude" and "A Perfect Vacuum," which explore similar themes. There is a debate about the definition of artificial intelligence (AI) and whether it is currently achievable. Some users recommend reading other books by Lem, including "Solaris" and "The Cyberiad." The conversation also touches on AI-generated poems and the history of artificial life concepts in mythology and literature.

### Animate Anyone: Image-to-video synthesis for character animation

#### [Submission URL](https://humanaigc.github.io/animate-anyone/) | 311 points | by [jasondavies](https://news.ycombinator.com/user?id=jasondavies) | [141 comments](https://news.ycombinator.com/item?id=38476482)

A team of researchers from the Institute for Intelligent Computing at Alibaba Group has developed a new framework for character animation called "Animate Anyone." The framework uses diffusion models to generate character videos from still images, ensuring consistency and control over the animation. The researchers introduced a pose guider to direct the character's movements and employed a temporal modeling approach to ensure smooth transitions between video frames. By expanding the training data, the framework can animate arbitrary characters, achieving superior results compared to other image-to-video methods. The researchers evaluated the framework on benchmarks for fashion video and human dance synthesis, achieving state-of-the-art results. The paper provides detailed information on the methodology and results of the research.

The discussion on this submission covers a range of topics. 

One commenter points out that the current state of generating movement skeletons is limited and does not fully capture the nuances of realistic movement. They suggest using OpenPose, a software that is capable of generating accurate skeletons, instead.
Another commenter mentions that the framework is highly relevant to 2D animation and compares it to rotoscoping, a technique used in the past to trace movement from filmed sequences.
Another commenter brings up the work of Corridor Crew and their use of AI tools for character animation. They mention that quality animation still requires skill, and AI can assist in generating in-between frames.
A few commenters discuss the potential oversexualization of characters generated by the framework and how it can be problematic.
There are also comments regarding the limitations of the framework, such as the difficulty in animating certain characters or the lack of diverse representation in the generated animations.
There is also a discussion about the publishing of research findings on platforms like GitHub, with some commenters speculating on the reasons behind it and the accessibility of such platforms in China.

Overall, the discussion covers various aspects of the submission, including the limitations and potential implications of the framework, comparisons to existing techniques, and thoughts on the publishing of research findings.

### Interview with Viktor Lofgren from Marginalia Search

#### [Submission URL](https://nlnet.nl/news/2023/20231016-marginalia.html) | 88 points | by [luu](https://news.ycombinator.com/user?id=luu) | [18 comments](https://news.ycombinator.com/item?id=38470832)

Marginalia Search is a new search engine that aims to take users off the beaten track and introduce them to small, quality web pages that often go unnoticed by commercial search engines. In an interview with Viktor Lofgren, the creator of Marginalia Search, he explained that he was inspired to develop the search engine during the COVID-19 pandemic when he noticed that the internet seemed smaller and less diverse than it used to be. He wanted to create a search engine that resembled Google in its early days, and began building Marginalia Search as a traditional keyword search engine. What he found while crawling the web for search results were websites that were completely different from what he would find on larger search engines or social media platforms. Lofgren hopes to make the crawling data public in the future to combat censorship and offer diverse perspectives in search rankings. He also discussed the possibility of crowd-sourcing search sets, where users can contribute websites to be crawled. Lofgren believes that search engines play a critical role in helping websites and communities grow, and by offering an alternative to the dominance of search engine marketing, Marginalia Search can give smaller websites a chance to be discovered. Lofgren also mentioned the ability of Marginalia Search to penalize websites with excessive ads or tracking elements, providing a cleaner search experience for users. Overall, Marginalia Search aims to provide a fresh and diverse approach to search, offering users the opportunity to explore the less-traveled corners of the web.

The discussion on Hacker News about Marginalia Search revolves around various aspects of the search engine and its potential impact. Here are some key points that were discussed:

1. The ability of Marginalia Search to penalize websites with excessive ads or tracking elements was seen as a positive feature that would improve the search experience for users.
2. Some users highlighted the importance of search engines in helping websites and communities grow. Marginalia Search was seen as a potential alternative to search engine marketing, giving smaller websites a chance to be discovered.
3. There was a discussion about the challenges faced by search engines today, such as fighting spam, fraud, and scams. Marginalia Search's approach of crawling diverse websites and offering transparent crawling data was seen as a potential solution to these problems.
4. The concept of crowd-sourcing search sets, where users can contribute websites to be crawled, was mentioned. This feature was seen as a way to combat censorship and bring diverse perspectives to search rankings.
5. The potential use of Large Language Models (LLMs) in search engines was discussed. Some users expressed concerns about the reliability and accuracy of LLM-generated responses compared to human-generated ones.
6. The value of Marginalia Search was also highlighted as a way to discover lesser-known websites and explore the less-traveled corners of the web.

Overall, the discussion showed interest in Marginalia Search's approach to providing a fresh and diverse search experience, but also raised questions and concerns about the use of LLMs and the challenges facing search engines in general.

### Accelerating Generative AI with PyTorch II: GPT, Fast

#### [Submission URL](https://pytorch.org/blog/accelerating-generative-ai-2/) | 296 points | by [polyrand](https://news.ycombinator.com/user?id=polyrand) | [63 comments](https://news.ycombinator.com/item?id=38477197)

The PyTorch team is continuing their blog series on how to accelerate generative AI models with pure, native PyTorch. In this second part, they focus on LLM optimization, specifically for transformer inference. They demonstrate how they were able to write an LLM from scratch that is almost 10 times faster than the baseline, with no loss of accuracy, using native PyTorch optimizations. They leverage optimizations such as Torch.compile, GPU quantization, speculative decoding, and tensor parallelism. The team shares their code on GitHub for those interested in diving deeper. They also discuss reducing CPU overhead through torch.compile and a static kv-cache, overcoming challenges with the kv-cache's dynamism in text generation.

The discussion in the comments revolves around various aspects of the blog post and the topic of accelerating generative AI models using PyTorch. Some of the main points discussed are:

- The difference between Karpathy's nanoGPT GPT implementation and the one in the blog post, with the response highlighting the speed and inference performance achieved with native PyTorch optimizations.
- The support for PyTorch on Apple Silicon and the discussion on using Triton as a backend for Apple Silicon and other GPUs.
- The budget considerations for local workstations and suggestions for deals on GPUs.
- The discussion on the number of GPUs, VRAM, and technical skills required for building a multi-GPU setup.
- The energy consumption of GPUs and the possibility of choosing countries with lower energy prices for GPU-based projects.
- Various opinions on GPU testing and hardware configurations for training large models.
- Appreciation for the informative nature of the blog post and the author's other related writings.
- The discussion on matrix multiplication and the use of CuBLAS and FlashAttention for efficient computation in transformer models.
- The challenges and benefits of converting and deploying models in different formats.
- Requests for benchmark comparisons between PyTorch compile and Llvmacpp.
- The mention of LLamacpp and its potential speed benchmarks compared to PyTorch compile.
- The discussion on batching and persistent inference in serving frameworks and the emphasis on PyTorch's focus on latency and batch size 1.

Overall, the comments show a range of interests and perspectives on the blog post, with discussions covering technical details, budget considerations, hardware configurations, and deployment strategies for AI models.

### Large language models lack deep insights or a theory of mind

#### [Submission URL](https://arxiv.org/abs/2311.16093) | 267 points | by [mnode](https://news.ycombinator.com/user?id=mnode) | [247 comments](https://news.ycombinator.com/item?id=38474696)

In a recent paper titled "Have we built machines that think like people?", authors Luca M. Schulze Buschoff and colleagues evaluate the current state of vision-based large language models in terms of emulating human-like cognitive abilities. While these models demonstrate proficiency in processing and interpreting visual data, they still fall short of human capabilities in areas such as intuitive physics, causal reasoning, and intuitive psychology. The authors emphasize the need to integrate more robust mechanisms for understanding causality, physical dynamics, and social cognition into modern-day, vision-based language models. They also highlight the importance of cognitively-inspired benchmarks.

The discussion about the submission revolves around various aspects of language models and their ability to think like humans. Here are some key points made by different commenters:

- Some commenters argue that current large language models (LLMs) are limited in their ability to think like humans, as they often rely on pattern matching and lack deep insight or reasoning capabilities.
- Others suggest that human-like reflexive responses to questions are not necessarily indicative of human-level thinking, as humans have internal reasoning processes that LLMs cannot replicate.
- Some commenters emphasize the importance of implementing recursive execution and internal dialogue in LLMs to enhance their thinking abilities.
- There is a discussion about the role of memory and external interaction in developing artificial general intelligence (AGI). Commenters believe that AGI requires interaction with the external environment to learn and improve.
- The concept of "inner monologue" is mentioned, with some commenters warning that it can lead to wasteful and unproductive discussions.
- The topic of Asimov's Three Laws of Robotics is brought up, with commenters noting that these laws are not necessarily applicable to current AI systems.
- There is speculation about the extent to which LLMs possess theory of mind and whether they can truly understand human intentions or behavior.
- The potential benefits of providing more explicit instructions and prompt-guided training to LLMs are discussed.
- Some commenters point out that human thinking involves understanding functional meanings and behavioral differences, which current LLMs have not fully achieved.
- The idea of incorporating longer-term memory and context judgment into LLMs is mentioned as a way to improve their thinking capabilities.

Overall, the discussion highlights the limitations of current language models in simulating human-like cognitive abilities and explores potential directions for their improvement.

### Microsoft joins OpenAI's board with Sam Altman officially back as CEO

#### [Submission URL](https://www.theverge.com/2023/11/29/23981848/sam-altman-back-open-ai-ceo-microsoft-board) | 54 points | by [croes](https://news.ycombinator.com/user?id=croes) | [9 comments](https://news.ycombinator.com/item?id=38471728)

Microsoft is joining OpenAI's board as a non-voting observer, while Sam Altman returns as the CEO. Previously, Altman was ousted by the board but has now reached a deal to come back. With Microsoft as a major investor in OpenAI, this move gives the tech giant more insight into the company's operations without having an official vote. Altman expressed his excitement about the future and gratitude for everyone's hard work during the uncertain situation. OpenAI's new board now consists of chair Bret Taylor, Larry Summers, and Adam D'Angelo—three of the four members who fired Altman initially. Altman also spoke positively of Ilya Sutskever, co-founder and chief scientist, despite his initial participation in the board coup. Altman hopes to continue working with Sutskever in some capacity. Altman's return and Microsoft's involvement aim to strengthen OpenAI's mission and partnerships.

The discussion surrounding the submission on Hacker News covers a range of topics. 

One user points out that they wouldn't be surprised if OpenAI dropped Microsoft in a few years. Another user responds by saying that artificial intelligence (AI) is being treated as a mere business opportunity, rather than a technology with potential risks.
Another user clarifies that the non-voting observer role on the board is a common position where the observer receives detailed information about the board's decisions, methods, and approaches. They mention that Microsoft's involvement will provide valuable insights for the company.
In response to this, someone else mentions that the board's collective experience in various disciplines, including financial capital, influences decision-making. While Microsoft's vote is a significant addition, the power of board members lies in the exchange of information and spoken words during discussions.
A user comments that the discussion is becoming too focused on existential risks and sarcastically refers to the situation as a happy family. They also mention that Microsoft will bring a gentle level of oversight and accountability mechanisms.
One user brings up Larry Summers, who is on OpenAI's board, stating that the discussion shouldn't overlook his involvement in the decision-making process.
Another user simply states that Microsoft's AI division joining OpenAI's board is not surprising.

Lastly, two comments were flagged by users, but the content is not visible.

### Stable Diffusion XL Turbo can generate AI images as fast as you can type

#### [Submission URL](https://arstechnica.com/information-technology/2023/11/stable-diffusion-turbo-xl-accelerates-image-synthesis-with-one-step-generation/) | 42 points | by [Brajeshwar](https://news.ycombinator.com/user?id=Brajeshwar) | [3 comments](https://news.ycombinator.com/item?id=38473933)

Stability AI, a company specializing in AI-powered image synthesis, has launched a new model called Stable Diffusion XL Turbo. This model is capable of rapidly generating images based on written prompts, and it can even transform images from a source, such as a webcam, in real-time. The primary innovation of Stable Diffusion XL Turbo lies in its ability to produce image outputs in a single step, a significant improvement from its predecessor. This efficiency is achieved through a technique called Adversarial Diffusion Distillation (ADD), which utilizes score distillation and adversarial loss to improve the realism of the generated images. While Stable Diffusion XL Turbo is not as detailed as previous models, its speed savings are impressive. For example, it can generate a 3-step 1024x1024 image in about 4 seconds, compared to 26.4 seconds for a 20-step image with similar detail. Stability AI claims that the model can generate a 512x512 image in just 207 milliseconds on a powerful AI-tuned GPU, which could have applications in real-time generative AI video filters or video game graphics generation. Currently, Stable Diffusion XL Turbo is only available for non-commercial research purposes, but Stability AI is open to exploring commercial applications.

The discussion on Hacker News for the submission about Stability AI's new image synthesis model, Stable Diffusion XL Turbo, seems to be focused on the fact that this submission is a duplicate of a previous one. The duplicate submission had received a significant number of comments in just one day, but it appears that those comments have not been replicated in this duplicate submission. One comment suggests that the previous submission had received a large number of upvotes as well.

### Amazon's Trainium2 AI Accelerator Features 96 GB of HBM, 4x Training Performance

#### [Submission URL](https://www.anandtech.com/show/21173/amazons-trainium2-features-96-gb-hbm-quadruples-training-performance) | 43 points | by [ksec](https://news.ycombinator.com/user?id=ksec) | [29 comments](https://news.ycombinator.com/item?id=38475635)

Amazon has announced Trainium2, its new AI accelerator, which offers four times higher training performance compared to its predecessor. The Trainium2 accelerator is designed specifically for training foundation models and large language models, with up to trillions of parameters. It features 96GB of HBM memory, which is three times the amount of the original Trainium, and is built using a multi-tile system-in-package design. Although specific performance numbers have not been disclosed, Amazon claims that its Trainium2 instances can scale out to deliver up to 65 ExaFLOPS of low-precision compute performance for AI workloads.

The discussion on Hacker News revolves around various aspects of Amazon's announcement of Trainium2, its new AI accelerator. Here are the key points discussed:

- One user mentions that AMD is releasing the MI300x on December 6th, which has 192GB of HBM3 memory, fast connections, and 52TBs memory bandwidth. However, another user expresses that they are not aware of any trending discussions around the MI300x from AMD.
- There is a discussion regarding the performance numbers of the Trainium2 accelerator. One user mentions that they find the reported numbers impressive, but some practical issues with model completion and finicky behavior should be addressed. Another user notes that AMD has caught on to the AI race, but it remains to be seen how it compares to Nvidia GPUs.
- The relationship between the MI300x and MI300 is discussed, with a user pointing out that the MI300x claims to have 22 PFlops FP8 structured sparsity, which AMD is implementing.
- A user comments that there will be a large number of AI chips available in the market in the future.
- In response to the announcement, one user shares their experience with AWS Nvidia machines, mentioning that the cost of installing dependencies can be expensive. They express interest in Trainium as a faster and cheaper alternative.
- The issue of dependency installation on AWS instances is discussed, with some users sharing their frustrations about being locked into specific instances and GPUs not being ready for use.
- The cost of AWS instances is also mentioned, with one user highlighting that the cost of installation can be a small fraction compared to the training cost.
- The potential impact of Amazon's AI chips on the AI space is discussed, with one user suggesting that it could lead to a rewrite of the entire stack and lock users into Amazon for their workloads.
- There is a discussion about the compatibility and usefulness of the Trainium2 accelerator, with users mentioning the importance of compatibility with existing models and frameworks.
- The performance of Trainium2 is compared to other AI chips in terms of operations per second and precision, with some users suggesting that it is surprisingly low and narrow in terms of precision.
- The topic transitions to the development of accelerators and the need for developers and frameworks to move away from proprietary technologies like CUDA and embrace standard APIs.
- There is casual speculation about the fate of the hardware that Amazon sells or retires.
- A user suggests that the hardware market could be destroyed if AWS starts selling or destroying second-hand hardware.
- The discussion ends with a brief mention of software support for Trainium2 and disappointment with standard backends for transformers and middlewares.

Overall, the discussion touches on various aspects related to the Trainium2 announcement, including comparisons to other AI accelerators, cost considerations, compatibility, and the potential impact on the AI ecosystem.

### Let's Not Flip Sides on IP Maximalism Because of AI

#### [Submission URL](https://www.techdirt.com/2023/11/29/lets-not-flip-sides-on-ip-maximalism-because-of-ai/) | 95 points | by [rntn](https://news.ycombinator.com/user?id=rntn) | [108 comments](https://news.ycombinator.com/item?id=38472367)

In a recent article on Techdirt, author Matthew Lane discusses the importance of fair use in copyright law and its implications for AI companies. Fair use allows limited use of copyrighted material without permission, primarily for purposes such as commentary, criticism, and parody. Lane highlights how fair use has filled an important gap in social media and art, allowing us to retweet or link content without fear of copyright infringement. Moreover, many creators who make a living from streaming video games or creating "react" content rely on fair use protection. However, Lane expresses concern over public interest advocates who are willing to sacrifice fair use in order to regulate AI companies. He argues that using copyright in this way would be both unnecessary and detrimental, as it would hinder the ability of AI to analyze content and potentially lead to the exploitation of artists. Lane suggests that addressing the issues surrounding AI, such as worker exploitation, requires thoughtful policy-making rather than using copyright as a blunt instrument. He draws parallels to the fights against "on a computer" software patents, which caused problems in the past and still persist today. Lane concludes by emphasizing the need to preserve fair use and prevent its erosion in the face of new technological advancements.

Discussion:

- User "chlmrs" acknowledges that there are concerns about AI companies pushing for shorter copyright terms, but questions the need for AI models to have access to copyrighted material. They argue that AI models should follow rules that are established and applied fairly.
- User "Arainach" expresses skepticism about the existence of IP laws and believes that copyright terms should be longer. They also discuss the limited duration of patents and distinguish between the practical value of art and inventions.
- User "MightyBuzzard" argues that the purpose of copyright laws is to protect expressions of ideas and not to control access to creative content. They emphasize the importance of protecting artists and allowing them to profit from their work.
- User "wffltwr" raises concerns about the impact of AI on copyright laws and suggests that AI interfaces that improve the capabilities of human thought may challenge current copyright laws.
- User "gdy" agrees with the concerns expressed by "MightyBuzzard" and believes that AI should not have the ability to tighten copyright laws. They reference the Blurred Lines lawsuit as an example of how copyright claims can become subjective.
- User "ls612" suggests searching for Supreme Court cases related to Google and small excerpts of books in search results.
- User "dnrs" agrees with the sentiment that IP laws have been flipped to favor larger companies and that AI projects by big companies are potentially infringing on the works of smaller creators.
- User "MightyBuzzard" responds by stating that AI replicating uninspired creations is not a valid argument, as it assumes that AI scientists have replicated the human mind, which they argue is not the case.
- User "wrd" agrees with the concerns raised by "MightyBuzzard" and emphasizes the need to consider the perspective of regular people who are creating and sharing content.
- User "dnrs" argues that regular people creating and sharing content are often not adequately compensated, while the wealthy companies that control the copyright laws benefit greatly.
- User "wrd" points out that allowing AI natural access to licensed works while restricting human artists could create a problematic double standard.
- User "wffltwr" warns against accepting radical changes to copyright laws and suggests that AI and digital laws may cause unintended consequences.

Overall, the discussion revolves around the potential implications of copyright laws on AI companies, artists, and content creators. There are concerns about fair compensation for artists and the balance between protecting copyright and enabling innovation in AI technology.

---

## AI Submissions for Wed Nov 29 2023 {{ 'date': '2023-11-29T17:10:08.086Z' }}

### How to tackle unreliability of coding assistants

#### [Submission URL](https://martinfowler.com/articles/exploring-gen-ai.html#memo-08) | 152 points | by [ingve](https://news.ycombinator.com/user?id=ingve) | [144 comments](https://news.ycombinator.com/item?id=38456726)

Birgitta Böckeler, a software developer working at Thoughtworks, has been delving into the world of generative AI, particularly Large Language Models (LLMs). In a series of memos, Böckeler explores the toolchain of LLMs that support coding tasks. She categorizes the tools based on the type of assistance they provide, such as finding information, generating code, reasoning about code, and transforming code. Böckeler also discusses the different interaction modes, prompt composition, properties of the model (such as what it was trained with and its size), and the origin and hosting of the tools. She provides examples of popular tools in the space, such as GitHub Copilot, ChatGPT, and Meta's CodeCompose. Böckeler notes that the most common usage today involves chat interfaces combined with coding assistance in the code editor, and that in-line assistance is the most mature and effective approach for coding assistance. She also mentions ongoing experimentation with prompt composition tools and the future potential of larger models and more specialized training for coding assistance.

The discussion on this submission covers a few different topics. Some users point out the humorous side of LLMs and discuss their limitations, while others discuss the potential risks and challenges of developing AGI (Artificial General Intelligence). There is also a discussion about the reliability and practicality of LLMs, with some users expressing concerns about their ability to generate correct and understandable code. Some users also discuss the training and capabilities of LLMs, questioning whether they can understand programming languages and suggesting alternative approaches for program synthesis. Overall, the discussion covers a range of perspectives on the topic of generative AI and its potential applications in coding assistance.

### Extracting training data from ChatGPT

#### [Submission URL](https://not-just-memorization.github.io/extracting-training-data-from-chatgpt.html) | 238 points | by [Deeg9rie9usi](https://news.ycombinator.com/user?id=Deeg9rie9usi) | [121 comments](https://news.ycombinator.com/item?id=38458683)

A recent paper reveals a concerning vulnerability in OpenAI's language model, ChatGPT. Researchers discovered that by querying the model, they could extract portions of the dataset it was trained on, including sensitive information like email addresses and phone numbers. Unlike previous data extraction attacks, this one targets a production model, emphasizing the importance of testing base models and patching vulnerabilities. The attack, which prompts the model with a specific command, allows for the extraction of several megabytes of training data for a minimal cost. The implications extend beyond ChatGPT, raising concerns about the potential leakage of sensitive training data in other language models.

The discussion on the submission revolves around various aspects of the vulnerability in OpenAI's ChatGPT model. Some users express their surprise and interest in the finding, while others provide additional insights and comments.
One user shares a link to a thread on Reddit where the attack approach was posted several months ago. Another user mentions that it's important to conduct research and test base models for vulnerabilities before deploying them in production.
There are discussions about the shortcomings of the current peer-review journal system, with some users expressing their preference for open access and reproducible papers. The topic also shifts to the behavior of GPT models and the need to explain their actions, as well as the challenges faced by reviewers in understanding and detecting vulnerabilities.
One user provides a detailed explanation of how the attack works and suggests that OpenAI should have been more proactive in patching the vulnerability. Another user mentions that the attack works by downloading random internet data, making it difficult to prevent entirely.
A user points out the similarities between Bard, a Google model, and ChatGPT, raising questions about potential vulnerabilities in other language models. There are discussions about the difficulty of mitigating the vulnerability and the limitations of current programming.
Some users argue that the findings are not surprising and that similar attacks on other models have been attempted in the past. A user clarifies that the attack involves extracting specific portions of the training dataset and provides examples of personal information that could be extracted.
A user highlights the need to patch the vulnerability and fix the underlying issue. They advise against changing prompts randomly and suggest taking a more strategic approach.

Overall, the discussion includes different perspectives on the vulnerability in ChatGPT and its implications, with users offering insights, explanations, and opinions on the matter.

### What should I do if I suspect one of the journal reviews I got is AI-generated?

#### [Submission URL](https://academia.stackexchange.com/questions/204370/what-should-i-do-if-i-suspect-one-of-the-journal-reviews-i-got-is-al-generated) | 137 points | by [j2kun](https://news.ycombinator.com/user?id=j2kun) | [59 comments](https://news.ycombinator.com/item?id=38462269)

A recent post on Academia Stack Exchange raises an interesting question about the use of AI-generated journal reviews. The user explains that they suspect one of the reviews they received for their paper was generated by an AI, based on the style and content of the review. The review consists only of long questions that rephrase each line of the abstract, with no suggestions or feedback provided. Additionally, the list of suggested articles includes irrelevant papers from unrelated fields. The user has even run the text through AI-detection tools, which have consistently identified it as AI-generated. 

The user asks whether they should mention their suspicions to the journal editor, even though they can't prove the use of AI. They express concern about the ethical implications of using AI to generate reviews in academic publishing. They also worry about the potential consequences for their own article if they speak up. 

In response to the question, several answers suggest that the user should indeed contact the editor and explain their suspicions. They advise the user to outline their evidence and express their concerns about the integrity of the peer review process and the protection of their intellectual property. It's also suggested to check the journal's website for any explicit statements about the use of AI in peer review. Ultimately, the decision of how to proceed lies with the editor, and the user should be prepared to revise and resubmit their paper regardless of the outcome. 

This question brings to light an important discussion about the increasing use of AI in academia and the potential impact on the peer review process. It highlights the need for clear guidelines and policies to address this issue and ensure the integrity of academic publishing.

The discussion revolves around the suspicion of AI-generated journal reviews and the implications for the peer review process in academia. Some commenters suggest contacting the journal editor and expressing concerns about the integrity of the review process and the protection of intellectual property. Others argue that AI can be helpful in filtering out irrelevant submissions and improving the efficiency of the review process. The debate also touches on issues of trust and reliability in both human and AI-generated reviews. Some commenters express skepticism about AI's ability to replace human reviewers, while others highlight the potential benefits of AI in speeding up the review process and optimizing quantity and quality. Overall, there is a call for clear guidelines and policies to address the increasing use of AI in academic publishing.

### Stable Diffusion:Real time prompting with SDXL Turbo and ComfyUI running locally

#### [Submission URL](https://old.reddit.com/r/StableDiffusion/comments/1869cnk/real_time_prompting_with_sdxl_turbo_and_comfyui/) | 116 points | by [belltaco](https://news.ycombinator.com/user?id=belltaco) | [42 comments](https://news.ycombinator.com/item?id=38454349)

Yesterday, a mind-blowing demonstration was posted on Stable Diffusion, showcasing a workflow that allows for real-time prompting with SDXL Turbo and ComfyUI. The video, which is not sped up, shows the workflow running smoothly on a powerful 3090 TI computer. 

The technology behind this workflow represents a major milestone in the development of AI capabilities. It hints at the possibility of approaching the singularity, where AI systems reach and potentially exceed human-level intelligence. 

One commenter compared the experience to what the singularity might feel like. Others expressed astonishment at the rapid progress being made in AI. One user shared their prediction that this acceleration could indicate that we are at the start of the singularity, with 2024 being a potentially wild year of innovation. 

Another user imagined a future where scripts could be easily transformed into new movies or TV shows. They suggested that by simply inputting a script into a prompt window and typing a desired parody theme, an entirely new production could be generated within a day. 

Overall, this stunning demonstration has left many in awe of the possibilities that AI technology holds for the future. As developments continue to accelerate, it remains to be seen just how close we are to the singularity and what incredible creations lie ahead.

The discussion around the submission primarily focuses on the impressive speed and capabilities of SDXL Turbo and ComfyUI in real-time prompting. Some users express astonishment at the advancements in AI technology, with one person suggesting that we may be approaching the singularity. Others discuss the practical applications of this technology, such as easily transforming scripts into new movies or TV shows. The conversation also delves into technical details, including optimizations with different graphics cards and the compatibility of SDXL Turbo with various models. Some users mention the challenges of working with CPU models and the potential for further optimization with SDXL Turbo and OpenVino. The discussion also touches on the limitations and potential pitfalls of rapid AI generation, including the risk of generating kitsch or low-quality content.

### OpenAI's board needs to say something

#### [Submission URL](https://www.theverge.com/2023/11/29/23981516/openai-board-silence-sam-altman) | 34 points | by [goplayoutside](https://news.ycombinator.com/user?id=goplayoutside) | [14 comments](https://news.ycombinator.com/item?id=38465560)

OpenAI's board has been noticeably silent following the failed attempt to oust Sam Altman, leaving many to wonder what their next move will be. The board, which recently lost directors Reid Hoffman and Shivon Zilis, is now tasked with rebuilding and conducting an internal investigation into Altman's firing. Adam D'Angelo, CEO of Quora and a board member of OpenAI, has so far been the only member to survive the power struggle. It remains to be seen how the board will navigate this difficult situation and restore stability to the organization. In other news, Meta's morale is on the rebound, and there's a new AI startup making waves in the industry.

The discussion surrounding the submission revolves around various topics. There is a debate about the relevance of the recent global events, such as Ukraine, Israel-Palestine, and OpenAI's current situation. Some users argue that these topics are unrelated while others believe they are important for staying informed. There is also a discussion about experts and their involvement in board politics and governance. Some users express frustration with the lack of transparency from OpenAI's board and their interest in maintaining public messaging. Others argue that the danger lies in the company losing financial value and compare Altman's departure to a typical CEO switch. The discussion also touches on the importance of voting and the potential risks of former board members predicting sufficient attention as the biggest danger. Lastly, there is a comment mentioning the East Coast Establishment, but it lacks further context.

### Mother plucker: Steel fingers guided by AI pluck weeds rapidly and autonomously

#### [Submission URL](https://arstechnica.com/information-technology/2023/11/mother-plucker-steel-fingers-guided-by-ai-pluck-weeds-rapidly-and-autonomously/) | 24 points | by [ashitlerferad](https://news.ycombinator.com/user?id=ashitlerferad) | [5 comments](https://news.ycombinator.com/item?id=38462113)

Swedish company Ekobot AB has developed an autonomous robot that can rapidly identify and remove weeds from farmland. The Ekobot WEAI robot is battery-powered, weighs 600 kg, and can operate for 10-12 hours on a single charge. Equipped with a machine vision system powered by artificial intelligence, the robot can recognize and pluck weeds as it moves over the field. In trials, the robot allowed farmers to grow onions with 70% fewer herbicides. Ekobot has also integrated 5G mobile technology into the robot, enabling it to communicate remotely with a central server. The company has now released "5G onions" grown using this weeding method, which have an extended shelf life and improved taste. The Ekobot system is set to become available in several European countries, as well as the US and the UK, by 2030.

The discussion on the Hacker News submission revolves around the use of the Ekobot WEAI robot and its integration of 5G technology. 

One user, "rngn," compares the robot's movement to that of chickens picking, indicating that it seems to follow a simple copying motion rather than using advanced lasers. 

Another user, "the_optimist," highlights the importance of 5G technology in the robot's operation. 

A sub-thread between users "lbg" and "vntrmnn" focuses on the collaboration between Ekobot and Swedish telecommunications company Telia. They discuss how Telia's integration of 5G mobile technology allows the robot to communicate remotely with a central server and collect learning data from the field. 

User "Sabinus" comments on the article, expressing skepticism about the accuracy of collecting weed vision data. 

Overall, the discussion primarily centers around the functionality and potential of the Ekobot WEAI robot, as well as the role of 5G technology in its operation.

### Together AI raises a $102.5M Series A

#### [Submission URL](https://www.together.ai/blog/series-a) | 67 points | by [bratao](https://news.ycombinator.com/user?id=bratao) | [23 comments](https://news.ycombinator.com/item?id=38463034)

Together AI, a company focusing on open and custom AI models, has raised $102.5 million in a Series A financing round. Led by Kleiner Perkins, the round also included participation from investors such as NVIDIA and Emergence Capital. Together AI plans to use the new capital to accelerate the development of its cloud platform, with the aim of creating the fastest cloud platform for generative AI applications. The platform allows developers to integrate leading open source models or create their own models through pre-training or fine-tuning. The company believes that generative AI is a platform technology that will have a long-term impact on society, and aims to provide researchers and developers with the tools to shape the AI future.

The discussion on the submission about Together AI's $102.5 million funding round covers various topics related to the AI industry and the use of AI models:

1. Some users mention the challenges in training AI models compared to inference. They note that inference has a large market and is dominated by cloud providers, while training requires specialized knowledge and optimization. They mention Google Cloud Platform (GCP) and Amazon Web Services (AWS) as dominant players in the inference space.
2. Another user suggests that decentralized skills and specialized distributed training frameworks are necessary for competing with big cloud players. They mention CoreWeave as an example of a GPU cloud provider that specializes in distributed training frameworks.
3. The discussion also touches on the skepticism around long-term business viability in the machine learning field. One user shares their experience, stating that machine learning projects require significant effort and expertise in modeling and data quality.
4. The topic of NVIDIA's investment in Together AI is brought up, with a user questioning the return on investment from a hardware perspective. Others comment on the accounting rules and holding structures when it comes to joint ventures.
5. The discussion briefly shifts to Microsoft Azure, with one user mentioning Microsoft's high margin on Azure and another user expressing disbelief in such high margins.
6. Pricing of Together AI's models is discussed, with one user pointing out the relatively low cost and another mentioning the GPT-4 model and its potential price range. The scalability of prices based on the number of tokens is also mentioned.
7. A few users share their personal experience with inference service platforms, mentioning factors like clear and simple user interfaces, pricing, and speed.
8. The discussion ends with a brief mention of venture capital money in the FinTech industry.

Overall, the discussion covers topics such as the challenges of AI training, the dominance of cloud providers in inference, skepticism about long-term business viability, the impact of NVIDIA's investment, Azure's margins, pricing of AI models, and user experiences with inference service platforms.