import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Sat Dec 30 2023 {{ 'date': '2023-12-30T17:09:43.459Z' }}

### Midihum: An ML-Based MIDI Humanizing Tool

#### [Submission URL](https://www.erichgrunewald.com/posts/introducing-midihum-an-ml-based-midi-humanizing-tool/) | 40 points | by [erwald](https://news.ycombinator.com/user?id=erwald) | [14 comments](https://news.ycombinator.com/item?id=38814424)

midihum is a command-line tool that uses machine learning to humanize MIDI compositions. It takes MIDI compositions as input and produces new compositions with adjusted velocity values for each note, resulting in a more natural and expressive sound. The tool uses gradient boosted trees, trained on over 2.6K piano performances, to make the adjustments. The creator of midihum has been working on the tool for the past five years and is pleased with its performance, particularly for solo piano works from the Baroque, Classical, and Romantic periods. The tool accurately predicts and captures the dynamics of the music, identifying peaks and valleys in intensity. 

One interesting feature of midihum is that it tends to produce more extreme velocity values than those performed by humans, giving the compositions a unique character. The tool can be easily used through the command line by running a simple command. The midihum model was trained on performances from the International Piano-e-Competition for young pianists. The tool is distributed with a GPLv3 license, allowing users to freely copy, distribute, and modify the software. Overall, midihum is a powerful tool for musicians and composers looking to enhance the naturalness and expressiveness of their digital compositions.

The discussion on the submission revolves around various aspects of the midihum tool and MIDI humanization. One commenter, "DrSiemer," mentions having trouble installing and finding a stable MIDI file to test the tool. Another commenter, "brdgrs," suggests checking the Internet Archive for existing MIDI files. "cmiller1" provides guidance on how to use the midihum tool by cleaning the repository, navigating to the midihum directory, and installing the dependencies. "-db" suggests that using a machine learning approach can significantly improve the process of capturing dynamics. Another commenter, "CrypticShift," shares their experience using computer-generated music and mentions that MIDI keyboards often lack sufficient responsiveness, causing issues with generating humanized sounds. They also note that MIDI humanization in digital audio workstations (DAWs) is popularized using Ableton's Groove Pool.
"BriggyDwiggs42" mentions that they haven't been able to figure out how to draw MIDI data points.
In response, "lncslls" states that their composition tool, Hookpad, supports importing MIDI data into a DAW. "rwld" agrees with this statement.
"steve1977" adds that Logic Pro calls this feature Humanize, while "helpfulContrib" expresses agreement.
"cmmkbrwn" suggests that the issue might be related to properly controlling the dynamics and velocity of the keyboard.
"dzm" remarks that Ableton's Groove Pool is an amazing feature, and making slight adjustments to rhythm can bring new life to a composition.
A final commenter, "plmnl," provides a hint that MIDI humanization can be done using a MIDI to USB connection.

Overall, the discussion explores different aspects of MIDI humanization, shares personal experiences with composition tools and MIDI keyboards, and provides tips for using the midihum tool.

### The Heart of a Language Server

#### [Submission URL](https://rust-analyzer.github.io//blog/2023/12/26/the-heart-of-a-language-server.html) | 75 points | by [thesuperbigfrog](https://news.ycombinator.com/user?id=thesuperbigfrog) | [11 comments](https://news.ycombinator.com/item?id=38820454)

In a recent blog post, "The Heart of a Language Server," the author dives into the intricacies of building a language server. They discuss how language servers, like those used for Kotlin, C#, and Rust, rely on understanding the current position of the cursor to provide features like go to definition, code completion, and more. The post explains that the first step in the process is to determine the node in the syntax tree that covers the cursor's offset. Once this is established, semantic analysis is needed to gather additional information. The challenge with semantic analysis is that it often involves multiple layers of intermediate representations that are only indirectly related to the syntax tree.

To bridge this gap, the author proposes a solution using the concept of source spans. By attaching source span information to semantic elements, a language server can find the appropriate semantic element for a given cursor position by iterating over all semantic elements and finding the one with the smallest span that contains the cursor. However, this approach has drawbacks, as it can be slow and can erase information about the underlying syntax trees. To address these drawbacks, the author introduces an iterative recursive analysis technique. In this approach, each semantic element is assigned a source_syntax method that returns the original syntax node from which it originated. This allows the language server to map syntax nodes to corresponding semantic elements precisely and lazily compute the mapping.

The post provides examples of different approaches to implementing the source_syntax method, including storing a reference to a syntax node, computing the syntax on-demand, or using a side table for mapping. The author notes that all three approaches are used in Rust Analyzer, the language server for Rust. In conclusion, the post highlights the importance of mapping syntax nodes to semantic elements and the challenges involved in maintaining this mapping. It emphasizes the need for a precise and efficient solution to power language server features effectively.

Discussion:
User "l-mrk" comments that in Rust, finding the parent file that includes a semantic file declaration generally requires the entire file to be parsed. However, source browsers typically parse source files to a certain depth. Another user, "jen20," confirms that this is a path-based structure in C#.
User "brbl" points out that the way Rust, Kotlin, and C# languages start the package declaration immediately impacts the placement of semantic models. They mention that IDE-friendly languages are designed with flexible semantics in mind, making analysis easier. They note that languages like C++, Lisp, and Rust are difficult to analyze compared to languages like Java, Kotlin, and Dart. They further mention that Rust has good support from IDEs, such as Rust Analyzer, which has been helpful in their experience with the language.
User "mtkld" adds that managing the people, master huge efforts require a level of knowledge and expertise in tools proven to be successful. They mention that IntelliJ Rust, the dedicated Rust IDE, and Rust Analyzer are tools that work great because they have dedicated teams working specifically on analyzing Rust and its consequences. They also note that understanding the build IDE is crucial, and it requires a substantial amount of effort.
User "brbl" gives a brief history of the Rust Analyzer project. They mention that the project started in 2017 alongside the existing LSP implementation RLS, which provided IDE support for Rust. They highlight that initially, people completely rejected the idea that Rust Analyzer could succeed and that IDEs worked well with Rust. However, the success of Rust and the efforts of individuals working to improve the tooling led to surprising progress with relatively little effort. They also note that the Rust Analyzer project was initially a hobby project in 2018 and later sponsored by businesses in 2020.
User "glblr-tst" adds that their experience with Lisp shows that despite the IDE experience lacking compared to other languages, the language has great interactivity and good analysis capabilities, especially with tools like SLIME.
User "mrws" comments that semantic information generated from syntax is placed in a linking mechanism and doesn't need to be searched again. They also mention that finding symbols generated by macros can be challenging, and modifying the compiler could make plugins impossible but still necessary.
User "di4na" agrees and adds that analyzers have a problem of growing fast, as compilers extract information. Even regenerating a part of a file based on partial input is not as straightforward due to the correct syntax. They mention that linking is a persistent topic of conversation and the steps taken for transformations need to be well-defined. They also note that it is difficult to query data dependent on transformations. They mention that using the existing part of the Rust compiler and replacing the Rust Analyzer is not as straightforward and requires ongoing maintenance. They point to C# Roslyn and TypeScript as examples of languages with built-in mindsets for this.
User "cycm" shares their interest in analyzing and transforming trees, mentioning that they have been reading and increasing their belief in XSLT for searching, tree walking, and analyzing transformations. Another user, "hds," recommends functional XSLT as a practical approach.

The discussion covers various aspects of language server development, including the challenges of semantic analysis, IDE friendliness, tooling support, and the trade-offs involved in analyzing and transforming different programming languages.

### How is AI impacting science?

#### [Submission URL](https://michaelnotebook.com/mc2023/index.html) | 87 points | by [occamschainsaw](https://news.ycombinator.com/user?id=occamschainsaw) | [108 comments](https://news.ycombinator.com/item?id=38811704)

At the Metascience 2023 Conference, Michael Nielsen from the Astera Institute delivered a talk on the biggest success of AI in science so far: the AlphaFold 2 system. This deep learning system has made significant advancements in predicting the 3-dimensional structure of proteins from their amino acid sequences. AlphaFold's success has sparked a revolution in molecular biology and raises important questions about validation, understanding, and the impact of AI on the progress of science as a whole. Nielsen treats AlphaFold as a prototype for how AI can be used across various scientific fields, encouraging metascientists to engage with this groundbreaking technology. By understanding the background of proteins and the challenges in determining their structures, we can grasp the significance of AlphaFold's achievements and its implications for scientific discovery.

The discussion about the submission mainly revolves around the topic of AI in science and coding. Some commenters highlight the difficulty of coding complex AI systems and the importance of clear communication in research papers. Others discuss the limitations and challenges of current AI systems, emphasizing the need for human involvement in verification and validation processes. There is also a mention of AI in the context of space missions and the potential risks associated with fully trusting AI-driven systems. Another point raised is the importance of researchers learning proper coding techniques and the disparities in salaries between scientists and programmers. Finally, there is a brief mention of breaking code with chatGPT and its potential for aiding in learning.

### Driverless cars immune from traffic tickets in California under current laws

#### [Submission URL](https://www.nbcnews.com/business/business-news/can-driverless-cars-get-tickets-california-law-rcna131538) | 95 points | by [ceejayoz](https://news.ycombinator.com/user?id=ceejayoz) | [112 comments](https://news.ycombinator.com/item?id=38815531)

The rapid expansion of self-driving cars in California has raised concerns about the need for new laws and regulations to govern the technology. There are ongoing safety concerns and uncertainties about law enforcement's ability to cite autonomous vehicles when they violate traffic laws. While driverless cars have been involved in incidents such as running red lights and blocking emergency responders, there is currently little that law enforcement can do to penalize these violations. Traffic tickets can only be issued if there is an actual driver in the car. The lack of accountability has led some to question whether it's a level playing field and whether fairness is a priority. California's current legal framework does not encompass driverless vehicles and new laws are needed to govern the technology adequately. In contrast to California, Texas and Arizona have adapted their transportation laws to allow the ticketing of driverless cars. Despite the limitations, autonomous vehicle makers claim their technology is getting better and that their cars are already safer than human drivers. However, with the technology still in its early stages and much testing happening on city streets alongside human drivers and pedestrians, there are concerns about the unknown risks and the lack of consent for these experiments.

The discussion on this submission revolves around the need for regulations and liability in the context of self-driving cars. Some users point out that in Texas, autonomous vehicles can be ticketed for traffic violations regardless of whether there is a person present in the vehicle, while California's legal framework does not cover driverless vehicles. Others argue that individual owners of self-driving cars should be responsible for any violations, while some suggest that open-source software could bypass regulatory controls. There is also discussion about the potential market for self-driving cars and the challenges of regulatory compliance. Overall, the conversation showcases differing opinions on the responsibility and accountability for self-driving cars.

### AI can find your location in photos

#### [Submission URL](https://www.npr.org/2023/12/19/1219984002/artificial-intelligence-can-find-your-location-in-photos-worrying-privacy-expert) | 24 points | by [talonx](https://news.ycombinator.com/user?id=talonx) | [14 comments](https://news.ycombinator.com/item?id=38813150)

Researchers at Stanford University have developed an AI tool called PIGEON (Predicting Image Geolocations) that can accurately guess the location where a photo was taken. The system, which was trained using images from Google Street View, can identify the correct country 95% of the time and pinpoint the location within about 25 miles. While the technology has potential applications in fields such as biology and cultural heritage, privacy experts are concerned that it could be used for government surveillance, corporate tracking, or stalking. The PIGEON algorithm recently outperformed a human expert in a geolocation contest.

The discussion around the submission consists of several different topics:
1. Some users are surprised that the PIGEON tool is able to accurately geolocate photos using limited resources. One user mentioned that a YouTuber attempted to replicate the results and found them to be surprisingly accurate, even locating a random photo taken of a replanted tree outside a window.
2. Another user brings up Google's location statement AI and wonders if the technology could be applied to photos that are shared online.
3. A user mentions that changing camera settings on Android devices can enable location metadata to be included in photos.
4. Another user shares a link to an article about Google's dumbing down of geolocation in photos.
5. One user challenges a user named Bard who claimed to have given hints about their location, specifically in Switzerland. The challenge suggests that Bard is being cryptic without providing any concrete information.
6. There is a brief mention of Rainbolt, though it is not clear what this refers to.
7. A discussion ensues about using the iPhone Photos app to add location metadata to photos. One user mentions a feature in the app that adds location metadata automatically.
8. Some users discuss the need for a more sophisticated search method to find landmarks in photos and mention an AI model called Visual Lookup.
9. One user mentions that Stanford graduate students have written a paper about the PIGEON tool but have not made the full model publicly available, which raises concerns about its potential implications.

---

## AI Submissions for Thu Dec 28 2023 {{ 'date': '2023-12-28T17:11:49.520Z' }}

### Cold-blooded software

#### [Submission URL](https://dubroy.com/blog/cold-blooded-software/) | 500 points | by [arbesman](https://news.ycombinator.com/user?id=arbesman) | [198 comments](https://news.ycombinator.com/item?id=38793206)

In his blog post, "Cold-blooded software," Patrick Dubroy draws an interesting analogy between cold-blooded animals and software projects. He recounts a lecture where he learned about the remarkable ability of painted turtle hatchlings to survive being frozen. Just as cold-blooded animals adapt their metabolism to the temperature around them, Dubroy suggests that software projects should adopt a similar approach. He argues that warm-blooded projects, which thrive on constant motion and activity, are vulnerable to failure when frozen, akin to being put on hold for several months. In contrast, cold-blooded projects, like the baby painted turtle, can be frozen and easily picked back up without missing a beat. Dubroy emphasizes the importance of using "boring" technology in cold-blooded projects. By relying on vendored dependencies and avoiding external services that may change or disappear, he believes that software projects can remain resilient and functional for years to come. Dubroy shares the example of his own software project, a static site generator written in Python, which has remained intact and functional for twelve years. He concludes by saying that he fully expects it to continue working for another twelve years. The analogy and insights provided by Dubroy offer a unique perspective on the resilience and longevity of software projects.

The discussion on this submission involves various points regarding the longevity and stability of software projects. One commenter points out that Express, a popular JavaScript framework, has been running for 13 years and has served a large number of requests. They express contentment with using Express and state that it is good software overall. Another commenter adds that Express version 5 is expected to be released soon. A different commenter brings up Python and its ability to build resilient software. They mention that while Python 2 has reached its end of life, a Java 10-year-old code can still run in modern environments. This leads to a discussion about the differences between Python and Java and their approaches to backward compatibility. 

In response to this discussion, another commenter mentions that the Airflow project faced issues when renaming an operator, highlighting the complexities of maintaining software over time. They argue that Java is more cautious about making breaking changes compared to Python. The conversation shifts to the topic of build tools, with Maven being praised for its ability to handle large-scale dependencies. Others express their preference for Gradle or using Python's specific versioning for machine learning libraries. The discussion then touches on the challenges of managing dependencies in Python, with a suggestion to use tools like Poetry or conda to create virtual environments that work effectively. 

Lastly, there is a discussion about Python 3.11 and its release notes, which mention potential breaking changes and the removal of deprecated APIs. Some commenters argue that breaking changes in Python are limited and mainly involve removing deprecated APIs or semantics changes. Others mention specific examples of breaking changes they have encountered in different Python versions. 

Overall, the discussion delves into the considerations, challenges, and differences in maintaining and upgrading software projects.

### NY Times copyright suit wants OpenAI to delete all GPT instances

#### [Submission URL](https://arstechnica.com/tech-policy/2023/12/ny-times-sues-open-ai-microsoft-over-copyright-infringement/) | 505 points | by [justinc8687](https://news.ycombinator.com/user?id=justinc8687) | [849 comments](https://news.ycombinator.com/item?id=38790255)

The New York Times has filed a lawsuit against OpenAI and Microsoft, claiming that their AI models infringe on the newspaper's copyright and devalue its content. The suit alleges that OpenAI-powered software can bypass The New York Times' paywall and generate misinformation that is mistakenly attributed to the publication. The Times argues that this undermines its relationship with readers and reduces its revenue from subscriptions, licensing, advertising, and affiliates. The lawsuit also highlights examples of OpenAI's models reproducing large sections of Times articles and providing inaccurate information. The Times is seeking damages and an injunction to prevent further unauthorized use of its content.

The discussion surrounding the New York Times lawsuit against OpenAI and Microsoft is diverse. Some users argue that scraping and summarizing news articles is legal and a common practice. They point out that search engines also heavily rely on indexing and summarizing content without facing copyright lawsuits. Others argue that the Times' concern is justified because OpenAI's models can generate misinformation that is falsely attributed to the publication, undermining its relationship with readers and revenue streams.

There is also debate about the legality of AI models producing derivative work. Some users argue that OpenAI's models are transformative and don't directly copy the New York Times' content verbatim. They compare the situation to CliffNotes or Wikipedia, which provide summaries and impact the market for original works but are not considered copyright infringement. Others believe that these models still have a substantial negative impact on the market for the original content and should be subject to copyright laws.

The discussion also covers broader topics such as the nature of copyright in the digital age and the changing roles of lawyers in understanding and interpreting copyright laws. Some users argue that copyright laws need to be updated to reflect the realities of the digital world, while others emphasize the importance of protecting intellectual property rights.

Overall, the discussion showcases varying opinions on the legality and ethics of OpenAI's AI models and their impact on the New York Times' content.

### Dark Visitors – A list of known AI agents on the internet

#### [Submission URL](https://darkvisitors.com) | 127 points | by [johneth](https://news.ycombinator.com/user?id=johneth) | [63 comments](https://news.ycombinator.com/item?id=38797487)

Have you ever wondered about the hidden world of autonomous chatbots and data scrapers on the web? Well, now you can get a glimpse into this mysterious ecosystem with Dark Visitors. This platform provides insight into the AI agents crawling across the internet and allows you to protect your website from unwanted access. One of the featured AI agents is ChatGPT-User, dispatched by OpenAI's powerful ChatGPT. This intelligent assistant provides answers to user prompts and often includes summaries of website content. It's a handy tool for quickly grasping the essence of a page.

Cohere-ai is another intriguing AI agent, likely used by Cohere's AI chat products. This agent retrieves content from the internet based on user prompts, assisting in providing accurate information.
Anthropic-ai, an unconfirmed agent, may be employed by Anthropic to download training data for its Large Language Models (LLMs), including the notable Claude. It's fascinating to see how AI technologies rely on these behind-the-scenes processes.
CCBot, employed by Common Crawl, is an open-source web crawler. It maintains a repository of web crawl data accessible to everyone. This data has become instrumental in training a wide range of LLMs, including OpenAI's GPT-3.
FacebookBot is another web crawler, used by Meta. This bot downloads training data for Meta's AI speech recognition technology. It highlights the vast array of AI applications across different platforms.
Google-Extended, operated by Google, is yet another web crawler. It gathers AI training content for Google's AI products like Bard and Vertex AI generative APIs. This emphasizes the importance of quality data for training advanced AI models.
GPTBot, a familiar web crawler to OpenAI, is responsible for downloading training data for their LLMs. These models power various AI products, including the popular ChatGPT. It's remarkable to see how AI companies utilize such tools to continually improve their offerings.
Lastly, Omgilibot, operated by Webz.io, maintains a repository of crawl data that it sells to other businesses. This data is often employed for training AI models, enabling companies to enhance their AI capabilities.
If you want to explore further, Dark Visitors provides information on additional AI agents and allows you to stay updated on new ones. By keeping an eye on these agents and updating your website's robots.txt accordingly, you can ensure better control over your website's access.

The discussion surrounding the submission revolves around various topics related to AI agents and web crawling. Here are the key points:

- One user points out that Cohere's AI chat products likely use their AI agent, Cohere-ai, to retrieve content from the internet based on user prompts.
- Another user raises a question about the specificity of AI agents and suggests that the term "AI Agent" should refer to step-by-step procedures, multiple generation rounds, using tools to interact, and creating an environment for Large Language Models (LLMs).
- A user humorously notes that the term "Dark Visitors" is common in the English translation of books about hacking, such as in the context of hacking in China.
- One user argues that Common Crawl should not be blocked as they are a charity doing great work in producing open data sets.
- The legality and ethics of blocking web crawlers are discussed, with some arguing that it is within website owners' rights to control access, while others believe in the importance of maintaining an open internet.
- The potential risks and benefits of web scraping for training AI models are debated. Some argue that the availability of data benefits everyone, while others express concerns about potential copyright infringement and industrial espionage.
- There are suggestions for resources and tools to manage and block web crawlers, including a link to a robots.txt builder.

Overall, the discussion explores the nuances, challenges, and implications of AI agents and their impact on web browsing and data access.

### Knowledge Graph Reasoning Based on Attention GCN

#### [Submission URL](https://arxiv.org/abs/2312.10049) | 50 points | by [PaulHoule](https://news.ycombinator.com/user?id=PaulHoule) | [10 comments](https://news.ycombinator.com/item?id=38794757)

A new technique to enhance Knowledge Graph Reasoning has been proposed in a paper titled "Knowledge Graph Reasoning Based on Attention GCN," authored by Meera Gupta and three other researchers. The authors have combined Graph Convolution Neural Network (GCN) with the Attention Mechanism to develop a detailed feature representation for each entity in the knowledge graph.

The Attention Mechanism is used to examine the relationships between entities and their neighboring nodes, enabling the generation of comprehensive implicit feature vectors. By integrating entity attributes and interactions, this approach outperforms traditional neural network models in tasks such as entity classification and link prediction. The authors believe that this methodology can provide important support for applications like search engines, question-answering systems, recommendation systems, and data integration tasks.

The paper, available for download as a PDF, explains the methodology and presents experimental results to validate the effectiveness of the proposed approach. This research contributes to the field of Information Retrieval and offers insights for improving knowledge graph reasoning using attention-based graph convolutional networks.

The comments on this submission cover a range of topics related to knowledge graph reasoning and neural networks. Some users discuss the importance of explainability in artificial intelligence and the challenges of dealing with large knowledge graphs. One user mentions the limitations of current language models and the need to consider the context of knowledge graphs when answering questions. Others discuss the trade-offs and computational challenges of using attention mechanisms in neural networks. There is also a discussion about the performance and efficiency of graph neural networks for knowledge graph reasoning.

### SageBrush: AI Painter – the simplest way to create with AI

#### [Submission URL](https://www.apposite.ai/sagebrush-ai-painter-the-simplest-way-to-create-with-ai.html) | 19 points | by [gdubs](https://news.ycombinator.com/user?id=gdubs) | [15 comments](https://news.ycombinator.com/item?id=38788419)

Introducing SageBrush, the ultimate AI creativity tool that transforms your sketches into stunning artworks. Whether you're a seasoned artist or just starting out, SageBrush puts you in control of your composition with a human touch. With a prompt editor that allows you to specify subject and style details, you can guide the AI to create the perfect image. Want to let your imagination run wild? Crank up the prompt strength and watch the AI work its magic. SageBrush supports both left and right-handed drawing modes and accepts input from Apple Pencil or even your fingers. Plus, with light and dark modes, you can create anytime, anywhere. Get started with SageBrush and unleash your artistic potential today!

The discussion on Hacker News about the SageBrush AI creativity tool started with the creator, Gregory, announcing its release on the App Store. Users were impressed with the project and praised its capabilities. One user asked if the tool can generate detailed versions of the final image, to which the creator responded that there is no explicit setting for that but the canvas can be resized. Another user mentioned the confusion caused by some screenshots and suggested adding tooltips or explanatory tips to clarify the UI. The creator acknowledged the feedback and mentioned plans to include these features in the next version.

The discussion then shifted to the token-based pricing model of SageBrush. Some users expressed concern about the cost and suggested a local model to generate images instead. The creator explained that the local model is in the works but the initial version uses a token approach to cover costs and gather feedback. There were also comments about the prompt editor, with one user stating that it's not clear how to use tokens and that the prompts are too simplistic. The creator responded that they have received feedback on the prompt editor and are working on improving it. Other users expressed their desire for an Android or web-based version of SageBrush, as well as support for devices like foldable phones. There was also a request for more information about the pricing, to which another user shared that the token resolution is mentioned on the pricing page.

The discussion also touched on the hidden pricing, with a user pointing out that the dark pattern used makes it difficult to interact with. The creator responded that the dark pattern was chosen to indicate that the $1 token pack is a low-stakes option for people to try drawing. They clarified that in the first version, 1 token represents 1 brushstroke or prompt change, resulting in a maximum image resolution of 768px. The creator mentioned that they appreciate the feedback and are considering a broader and more serious bundle offering.

Overall, the discussion involved various feedback and requests for improvements, with the creator actively engaging with the comments and taking note of suggestions for future updates.

---

## AI Submissions for Wed Dec 27 2023 {{ 'date': '2023-12-27T17:09:32.668Z' }}

### Valetudo – Cloud replacement for vacuum robots enabling local-only operation

#### [Submission URL](https://valetudo.cloud/) | 399 points | by [philo23](https://news.ycombinator.com/user?id=philo23) | [111 comments](https://news.ycombinator.com/item?id=38788326)

Valetudo is an open-source software solution that allows users to free their vacuum robots from the cloud. Developed by Sören Beye, Valetudo offers a reliable and user-friendly way to gain ownership and control over these devices. With no telemetry built-in and no commercial interests, Valetudo prioritizes user privacy and control. While it's difficult to pinpoint the exact number of users, the project has gained traction with thousands of downloads and support group members. Valetudo is licensed under the Apache-2.0 open-source license, highlighting its commitment to transparency and user empowerment. The project offers comprehensive documentation, including a getting started guide and companion apps section. 

However, it's essential to understand that Valetudo is a privately-owned project. Beye compares it to a privately-owned public garden where users can access and enjoy the benefits for free. While visitors can take inspiration and make suggestions, they must remember that it is still someone else's property. The project exists as a gift to the public, and any changes or demands are not grounds for entitlement. If Valetudo doesn't meet a user's expectations, they are free to leave and build their own solution. The project's focus is on fostering user-controlled, local-only operation for vacuum robots, and it provides an alternative to cloud-based solutions.

The discussion on this submission revolves around various topics related to the Valetudo project and vacuum robots in general. 

- Some users discuss their experiences with different vacuum robot models, such as the Dreame Z10, Roborock S5, Neato Robotics Botvac Connected, and Roombas. They share their opinions on the compatibility, stability, and maintenance of these devices. Some highlight the benefits of using Valetudo with their vacuum robots.
- There is a discussion about soldering PCBs and the technical aspects of installing and using Valetudo. Some users provide links to resources and documentation for those interested in trying out the project. They also discuss the complexity of the process and the need for beginner-friendly instructions.
- Users point out the open-source nature of Valetudo and discuss the implications of its licensing under the Apache 2 license. They also discuss the importance of open-source software in providing transparency and user control.
- Some users express concerns about the compatibility of certain vacuum robots with Valetudo and the limitations of cloud-dependent functionality. They also discuss potential alternatives and modifications to overcome these limitations.
- Other topics of discussion include the integration of Valetudo with Home Assistant, the availability of replacement parts for vacuum robots, and the privacy and security implications of internet-connected devices.

Overall, the discussion reflects a mix of technical discussions, user experiences, and opinions on the Valetudo project and vacuum robot devices in general.

### Pushing ChatGPT's Structured Data Support to Its Limits

#### [Submission URL](https://minimaxir.com/2023/12/chatgpt-structured-data/) | 285 points | by [goranmoomin](https://news.ycombinator.com/user?id=goranmoomin) | [86 comments](https://news.ycombinator.com/item?id=38782678)

OpenAI has highlighted the untapped potential of its ChatGPT API, which allows users to have more control over the AI's output. By providing specific instructions, such as summarizing an article into bullet points or generating metadata for social media sharing, users can shape the AI's responses to their needs. The article also introduces the concepts of prompt engineering and system prompts, which can enhance the quality and accuracy of ChatGPT's output. By utilizing these techniques, users can maintain a structured JSON schema, access the output programmatically, and store it for indexing. The article provides examples of how to use system prompts and incentives to guide the AI's behavior and generate more desired responses. Additionally, it demonstrates how to seek algorithmically efficient solutions for coding problems through the API. Overall, the article offers valuable insights into maximizing the potential of ChatGPT with the API's capabilities and prompt engineering techniques.

The discussion revolves around various aspects of OpenAI's ChatGPT API and its capabilities. Some users share their experiences and suggestions for using the API effectively. There is a discussion about the use of JSON schema and grammar to guide the AI's responses and ensure accuracy. Some users also mention other models and libraries that can be used in conjunction with ChatGPT. The conversation touches on topics such as low latency, high-quality function calling, and the potential for generating structured data. There are also discussions about the limitations and challenges of using JSON schema and the potential for manipulating the AI's responses by varying the prompt temperature. Overall, users provide insights and share their experiments and findings with using the ChatGPT API.

### What comes after open source? Bruce Perens is working on it

#### [Submission URL](https://www.theregister.com/2023/12/27/bruce_perens_post_open/) | 127 points | by [gnufx](https://news.ycombinator.com/user?id=gnufx) | [156 comments](https://news.ycombinator.com/item?id=38783500)

Bruce Perens, one of the founders of the Open Source movement, believes it's time for the Post-Open Source movement. Perens states that current open source licenses are no longer effective, citing Red Hat Enterprise Linux (RHEL) as an example of GPL circumvention. He also criticizes IBM for its behavior towards the open source community. Perens argues that open source has failed to serve the common person and proposes the concept of Post-Open, which would involve a fair corporate relationship with developers and funding for user-friendly applications. However, Perens acknowledges that there are many challenges to overcome in implementing this new approach.

The discussion on Hacker News includes various perspectives on the concept of Post-Open Source proposed by Bruce Perens. One commenter argues that open source should not include restrictions and that people should have the freedom to create and use software without limitations. Others point out that while substantial open-source projects often rely on corporate sponsors and donations, the current landscape allows for smaller projects to flourish as well. The issue of incentives and rewards for open-source development is also raised, with some participants suggesting that a different entrepreneurial model may be required. There are discussions regarding the definition of open source and its relationship with companies like FAANG (Facebook, Amazon, Apple, Netflix, Google). Some argue that the current definitions of open source should be revised to include criteria that address corporate behavior and contributions. The conversation also touches on the Free Software vs. Open Source debate, with some commenters agreeing with Perens that the focus should shift back to Free Software principles. Others express concern that non-copyleft licenses and the release of source-available proprietary software pose a bigger threat to software freedom. The potential limitations and implications of language and semantics in defining software philosophies are also addressed. Overall, the discussion brings up a range of viewpoints on the state of open source and its future direction.

### Scientists discover new antibiotics using AI

#### [Submission URL](https://www.euronews.com/next/2023/12/20/scientists-discover-the-first-new-antibiotics-in-over-60-years-using-ai) | 172 points | by [taubek](https://news.ycombinator.com/user?id=taubek) | [93 comments](https://news.ycombinator.com/item?id=38782636)

Scientists at the Massachusetts Institute of Technology (MIT) have used artificial intelligence (AI) to discover a new class of antibiotics. The deep learning model was trained to predict the activity and toxicity of compounds, specifically focusing on methicillin-resistant Staphylococcus aureus (MRSA) bacteria. The researchers identified compounds from five different classes that showed potential in combating MRSA with minimal harm to the human body. In laboratory tests, two of these compounds reduced the MRSA population by a factor of 10. This discovery could be a turning point in the fight against antibiotic resistance, as it is the first new class of antibiotics to be discovered in 60 years.

The discussion on the submission revolves around several key points:
1. The viability of new antibiotics: Some commenters express skepticism about the viability of new antibiotics, noting that many inventions in the medical field are not commercially viable. They argue that if a new antibiotic is immediately classified as a reserved antibiotic, its use will be restricted and limited.
2. Conflict of interest in pharmaceutical companies: Some commenters argue that there is a fundamental conflict between the interests of pharmaceutical companies and the need for new antibiotics. They believe that the financial incentives for pharmaceutical companies to research antibiotics are significantly reduced.
3. The difficulty of synthesizing new chemicals: There is a discussion about the challenges in designing chemicals for synthesis and scaling up commercial manufacturing.
4. Government regulation and monopolies: There is a debate about the effectiveness of government regulation and the presence of trusted monopolies. Some commenters argue that effective government regulation can prevent the abuse of power, while others believe that government regulation can stifle innovation.
5. The role of capitalism: Commenters discuss whether capitalism can offer solutions or if other alternatives, such as government funding and research, are necessary to address the antibiotic resistance problem.
6. Preserving antibiotic effectiveness: Some commenters argue that preserving the effectiveness of antibiotics requires using combinations of multiple antibiotics to prevent resistance from developing. They note that resistance naturally occurs and that using multiple antibiotics disrupts bacteria in multiple ways.
7. Examples of antibiotic discoveries: Commenters provide examples of recent antibiotic discoveries, emphasizing that these discoveries are happening within the existing capitalist system and debunking claims that capitalist systems hinder scientific progress.

Overall, the discussion touches on various aspects of the challenges and potential solutions to combat antibiotic resistance, including commercial viability, government regulation, and the role of capitalism.

### Performance of Large Language Models on a Neurology Board–Style Examination

#### [Submission URL](https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2812620) | 48 points | by [geox](https://news.ycombinator.com/user?id=geox) | [22 comments](https://news.ycombinator.com/item?id=38777485)

Researchers have conducted a study to evaluate the performance of large language models (LLMs) on neurology examination questions. The study used OpenAI's ChatGPT and compared its performance to human users. The results showed that ChatGPT achieved a similar performance to human users on lower-order questions, but struggled with higher-order questions. The study also found that confidence in the language of the model was higher for correctly answered questions compared to incorrectly answered questions. The length of the question did not significantly differ between correctly and incorrectly answered questions. Overall, the study highlights the potential of LLMs for medical education and knowledge assessment, while also highlighting their limitations in handling complex medical questions.

The discussion surrounding the submission includes various perspectives on the study and the implications of large language models (LLMs) in the medical field.

- Some users point out that Microsoft researchers have previously experimented with querying LLMs multiple times and randomizing the ordering of questions to choose the most common answer.
- Others express caution about relying solely on LLMs for medical standards and emphasize the importance of human interpretation and comparisons with other models.
- The limitations of LLMs are discussed, including their difficulty in handling specific medical knowledge and the misinterpretation of certain medical terms and symptoms.
- The significance of transparency in training LLMs is highlighted, with one user mentioning a tweet from Andrej Karpathy about the importance of clear evaluation criteria.
- Users discuss the practicality and effectiveness of using LLMs in medical exams and the potential for them to supplement existing study materials.
- There is also a debate about the reliability and accuracy of LLMs in providing confident yet incorrect answers to questions.
- Some users question the accessibility of the study's question bank due to a paywall, while others suggest alternative platforms such as Reddit for discussing exam questions and learning.

### Microsoft is training an AI to help get nuclear reactors approved

#### [Submission URL](https://www.freethink.com/energy/nuclear-reactors-microsoft) | 13 points | by [Brajeshwar](https://news.ycombinator.com/user?id=Brajeshwar) | [10 comments](https://news.ycombinator.com/item?id=38783501)

Microsoft is collaborating with nonprofit Terra Praxis to train an AI to generate the necessary paperwork for the approval of next-generation nuclear reactors. These reactors would power Microsoft's data centers running generative AIs. OpenAI's ChatGPT alone requires a significant amount of computing power, and nuclear energy could help meet the increased electricity demand. Microsoft aims to leverage small modular reactors (SMRs), which generate electricity in any weather conditions and do not generate carbon emissions. The AI being trained will produce highly structured documents similar to previously approved applications, potentially reducing the human hours needed for approval by 90%. Microsoft hopes that its investment in generative AI will accelerate its nuclear energy goals.

The discussion on this submission revolves around the idea of using AI to assist in the approval process for nuclear reactors. Here are some key points from the discussion:
- Some users express concerns about the safety and potential risks associated with nuclear power.
- Others question the efficiency of using AI in generating complex regulatory paperwork, suggesting that internal scrutiny and human involvement are crucial.
- There is a mention of Bill Gates' interest in nuclear power and his advocating for its potential.
- One user brings up the case of Chernobyl and questions whether relying on AI for paperwork could lead to similar disasters.
- Another user shares an example of how AI has been used to streamline paperwork in another industry, reducing labor and supporting documentation.
- Lastly, there is a flagged comment that appears to be unrelated to the topic.

Overall, the discussion touches on the potential benefits and risks of using AI in the nuclear energy and regulatory approval process. Some express reservations about safety, while others highlight the efficiencies it could bring.