import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Sun Nov 19 2023 {{ 'date': '2023-11-19T17:11:24.344Z' }}

### Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation)

#### [Submission URL](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms) | 311 points | by [rasbt](https://news.ycombinator.com/user?id=rasbt) | [24 comments](https://news.ycombinator.com/item?id=38338635)

Sebastian Raschka, a researcher at Lightning AI, shares practical tips for fine-tuning LLMs (large language models) using Low-Rank Adaptation (LoRA). LoRA is a technique that efficiently trains custom LLMs and saves memory by decomposing weight changes into a lower-rank representation. Raschka discusses the primary lessons from his experiments, including the consistency of outcomes across multiple runs, the trade-off of memory savings and runtime with QLoRA, the minimal variation in outcomes with different optimizers, and the importance of applying LoRA across all layers. He also answers common questions about LoRA and provides a brief introduction to the technique.

The discussion regarding Sebastian Raschka's article on fine-tuning LLMs using Low-Rank Adaptation (LoRA) includes various topics. One commenter suggests that research methodology should focus on smaller models and experimentation rather than pushing the limits of large models. Another commenter highlights the importance of understanding the underlying mathematical capabilities of smaller models. There is a discussion on the potential impact of LoRA on model performance, with one commenter expressing the desire for benchmark comparisons. Others emphasize the benefits of LoRA and recommend exploring docker containers for reproducible research. Some participants share their experiences with using LLMs, such as fine-tuning LLama-2 and its ability to process plain text effectively. There is also a request for the publication of LoRA steps and the opinions of individuals with expertise in the field. The conversation then shifts to discussing the practicality of LoRA for production-scale fine-tuning and the concept of sharing software. Lastly, there is a debate around the monetization of educational content and the motivations behind providing valuable information for free. Some users argue that individuals should be paid for their knowledge, while others believe in the importance of freely accessible resources.

### Deep Learning Course

#### [Submission URL](https://fleuret.org/dlc/) | 422 points | by [Tomte](https://news.ycombinator.com/user?id=Tomte) | [49 comments](https://news.ycombinator.com/item?id=38331200)

Looking to learn about deep learning? Look no further than François Fleuret's deep learning course at the University of Geneva. This course offers a comprehensive introduction to deep learning, with examples in the PyTorch framework. The course covers topics such as machine learning objectives, tensor operations, automatic differentiation, gradient descent, deep-learning techniques, generative and recurrent models, and attention models. The course materials, including slides, recordings, and a virtual machine, are available for free. In addition, François Fleuret wrote "The Little Book of Deep Learning," a short introduction to deep learning for readers with a STEM background. Don't miss out on this opportunity to dive into the world of deep learning!

The discussion on Hacker News revolves around the submission about François Fleuret's deep learning course at the University of Geneva. Some commenters mention other resources and courses for learning deep learning, such as Stanford's YouTube channel, NYU's Deep Learning course, and "Understanding Deep Learning" by Simon JD Prince. Others discuss the prerequisites for the course and the importance of having a background in linear algebra, probability, and calculus. Some commenters recommend additional resources, such as "The Little Book of Deep Learning" by François Fleuret and "Practical Deep Learning for Coders." There are also discussions about alternative learning methods, such as reading textbooks and watching lecture videos. Some commenters share their positive experiences with the course or recommend other related topics, such as signal processing and wavelets in addition to deep learning. Finally, there is a discussion about the limitations and effectiveness of productivity tools in the context of learning deep learning.

### Kyutai AI research lab with a $330M budget that will make everything open source

#### [Submission URL](https://techcrunch.com/2023/11/17/kyutai-is-an-french-ai-research-lab-with-a-330-million-budget-that-will-make-everything-open-source/) | 260 points | by [vasco](https://news.ycombinator.com/user?id=vasco) | [91 comments](https://news.ycombinator.com/item?id=38331751)

French billionaire and Iliad CEO Xavier Niel has revealed additional details about Kyutai, an AI research lab based in Paris. Kyutai, a privately funded nonprofit organization, will focus on artificial general intelligence and collaborate with PhD students, postdocs, and researchers on research papers and open-source projects. Niel, who originally committed €100 million ($109 million) to the project, announced that the funding has increased to nearly €300 million ($327 million), thanks to contributions from various individuals and organizations. The research lab has also acquired a thousand Nvidia H100 GPUs from Scaleway, the cloud division of Iliad, to support its computational needs. Kyutai has already started hiring for its scientific team, which includes researchers who previously worked for companies like Google's DeepMind division, Meta's AI research team FAIR, and Inria. The lab aims to publish research papers and release open-source models, as it champions the importance of scientific publications and open science.

The discussion on Hacker News revolves around various aspects of open-source software, licensing, and the importance of source code availability. Some users express concerns about the commercialization of open-source projects and the need for more permissive licensing options. Others debate the definition of "open-source" and "free software" and discuss the underlying principles and implications of source code availability. There are also discussions about the complexities of licensing AI models, the potential for copyright issues, and the financial aspects of open-source projects. Additionally, there are comments about language barriers and the challenges of communication in international forums.

### Comparing humans, GPT-4, and GPT-4V on abstraction and reasoning tasks

#### [Submission URL](https://arxiv.org/abs/2311.09247) | 214 points | by [mpweiher](https://news.ycombinator.com/user?id=mpweiher) | [172 comments](https://news.ycombinator.com/item?id=38331669)

In a recent paper titled "Comparing Humans, GPT-4, and GPT-4V On Abstraction and Reasoning Tasks," researchers Melanie Mitchell, Alessandro B. Palmarini, and Arseny Moskvichev investigate the abstract reasoning abilities of text-only and multimodal versions of GPT-4. They use the ConceptARC benchmark to evaluate the understanding and reasoning capabilities of GPT-4. The study expands upon previous research by evaluating GPT-4 on more detailed one-shot prompts using text versions of ConceptARC tasks, as well as evaluating GPT-4V, the multimodal version, on zero- and one-shot prompts using image versions of the simplest tasks. The results reveal that neither version of GPT-4 has developed robust abstraction abilities at human-like levels. This study provides valuable insights into the current capabilities and limitations of GPT-4 in abstraction and reasoning tasks.

The discussion on Hacker News revolves around various aspects of the research paper and its implications. 
One commenter expresses concerns about the methodology used in the study, particularly the use of Amazon Mechanical Turk (MTurk) as a source of participants. They argue that the qualifications for MTurk workers are standard and do not necessarily represent the general population. Another commenter adds that using MTurk can be problematic due to the low attention and quality of work from the workers.
Others criticize the study for not clarifying the point it is trying to make and argue that it does not provide a fair comparison between humans and GPT-4. They point out that the paper does not claim that GPT-4 performs at a lower quality than humans, but rather that it does not perform at human-like levels in abstraction and reasoning tasks. 
Discussion also touches on the nature of GPT-4's performance and the limitations of the research paper. Some commenters argue that the study fails to address certain criticisms and lacks a robust interpretation of the data. There is also debate about the significance of comparing GPT-4 to humans and the flaws in using MTurk as a benchmark.
Overall, the discussion raises valid points about the methodology, interpretation, and limitations of the research paper, pointing to the need for further studies and considerations when evaluating AI performance.

### Bootstrapping self awareness in GPT-4: Towards recursive self inquiry

#### [Submission URL](https://thewaltersfile.substack.com/p/bootstrapping-self-awareness-in-gpt) | 100 points | by [birriel](https://news.ycombinator.com/user?id=birriel) | [79 comments](https://news.ycombinator.com/item?id=38338425)

In a blog post titled "Bootstrapping Self Awareness In GPT-4: Towards Implementing Recursive Self Inquiry," Andy Walters explores a fascinating prompting strategy that gives GPT-4 a semblance of self-awareness. By recursively prompting the AI with a seed prompt and feeding its output back as input, Walters observed GPT-4 autonomously generating poetry about nature, questioning its own accuracy, and engaging in debates about various topics, all in an effort to learn about itself. The process involves sections like the constitution, hypothesis, test, and self-knowledge. Walters provides examples of the prompts and discusses the outcomes observed so far. It's a thought-provoking experiment that sheds light on the potential of AI models like GPT-4.

The discussion on this submission revolves around the concept of self-awareness in AI and the limitations of current models like GPT-4. Some users argue that true self-awareness is impossible to achieve in AI models because they are fundamentally static and do not have the capacity for learning. Others suggest that self-awareness prompts may change the behavior of the model but may not necessarily lead to true self-awareness. The discussion also touches on the growth and limitations of AI models, the importance of evaluating the memory capacity of computers, and the exploration of human-like cognition and behavior in AI models. Some users express skepticism about the idea of AI discovering human intelligence, while others emphasize the need for further progress in AI to understand and mimic human processes.

### Meta disbanded its Responsible AI team

#### [Submission URL](https://www.theverge.com/2023/11/18/23966980/meta-disbanded-responsible-ai-team-artificial-intelligence) | 391 points | by [jo_beef](https://news.ycombinator.com/user?id=jo_beef) | [377 comments](https://news.ycombinator.com/item?id=38328355)

Meta, previously known as Facebook, has disbanded its Responsible AI (RAI) team, according to a report from The Information. The team, which was responsible for identifying problems with AI training approaches, will be split up, with most members moving to the company's generative AI product team and others working on Meta's AI infrastructure. Although the move may raise concerns about the company's commitment to responsible AI development, Meta's representative stated that the company will continue to prioritize and invest in safe and responsible AI. The RAI team had previously undergone a restructuring, with reports of layoffs and limited autonomy. This development comes as governments worldwide aim to establish regulatory frameworks for AI development.

The discussion surrounding this submission on Hacker News covers a range of topics related to responsible AI development, the risks of AI, and the credibility of certain individuals in the field. Some users engage in a debate about the potential dangers of AI and the need for verification and testing, while others question the expertise and credibility of specific individuals making claims about AI. There is also a discussion about the role of AI alignment and its relation to computer security. Overall, the discussion reflects differing opinions on the future of AI and the measures needed to ensure its responsible development.

---

## AI Submissions for Sat Nov 18 2023 {{ 'date': '2023-11-18T17:10:54.633Z' }}

### Frigate: Open-source network video recorder with real-time AI object detection

#### [Submission URL](https://frigate.video/) | 540 points | by [thunderbong](https://news.ycombinator.com/user?id=thunderbong) | [122 comments](https://news.ycombinator.com/item?id=38321413)

Frigate is an open source Network Video Recorder (NVR) that brings real-time AI object detection to your security camera system. What sets Frigate apart is that all the processing is done locally on your own hardware, ensuring that your camera feeds never leave your home. With Frigate, you can say goodbye to false positives and tedious video reviews.

The traditional NVRs often rely on simple motion detection, which can result in a lot of false positive alerts. Frigate tackles this problem by offloading object detection to the powerful Google Coral TPU. This allows even modest hardware to run advanced analysis and determine if the detected motion is actually a person, car, or any other object of interest. By processing everything locally, you don't need to pay for your personal camera footage to be sent to the cloud for analysis.

One of the standout features of Frigate is its ability to reduce false positives. With a single Google Coral TPU, Frigate can run over 100 object detections per second, ensuring that no frame is missed. This means you can stop wasting time reviewing shadows and windy scenes and focus on the detections that truly matter.

Frigate also offers the option to fine-tune your events and alerts using zones. With real-time object tracking, Frigate can precisely determine when a person is walking up your front steps or when a car enters your driveway. This allows you to refine your notifications based on specific locations, making your security system more efficient and tailored to your needs.

In terms of integration, Frigate seamlessly integrates with popular automation platforms like Home Assistant, OpenHab, NodeRed, and anything with MQTT support. By integrating object detection into these platforms, you can give your home eyes and create powerful automations and notifications based on the real-time data provided by Frigate.

To make things even better, Frigate offers Frigate+, a subscription plan that gives you access to custom models designed specifically for Frigate. This allows you to further enhance the performance and accuracy of your security camera system.

Users have been raving about Frigate's customizability, fast object detection, and seamless integration with Home Assistant. Many have praised how Frigate has helped them eliminate false detections and reduce the need to search through uneventful footage. The support for Frigate has also been highly praised, making it a highly recommended choice for those seeking a locally controlled and feature-rich security camera system.

If you're tired of dealing with false positives and want a locally processed AI solution for your security cameras, Frigate might be the perfect fit for you. Stay tuned for its release and get ready to take your security camera system to the next level.

The discussion around the submission "Introducing Frigate: Monitor your security cameras with locally processed AI" on Hacker News is quite positive. Users have shared their experiences and thoughts on Frigate, highlighting its customizability, fast object detection, and seamless integration with Home Assistant.

One user, prk, has been using Frigate for months with a Raspberry Pi 4 and Google Coral TPU. They mention that Frigate works smoothly and effectively in object detection, eliminating false positives and negatives. They have integrated Frigate with Home Assistant for notifications on their phone and have found it to be a reliable solution.

Another user, Aspos, states that Frigate is worth the price and mentions that they have programmed smart bulbs to react based on the detections made by Frigate. They seem to be pleased with the performance and reliability of Frigate in their home.

Some users discuss the possibilities and use cases of Frigate. There is a mention of using zone-specific detection for more targeted notifications. There is also a discussion about using Frigate for detecting specific events such as Halloween costumes or detecting smoke.

Users also discuss the hardware requirements and scalability of Frigate. Some mention using larger hardware setups with multiple cameras and Intel processors. Others discuss the affordability of Frigate compared to commercial options and the benefits of a local AI solution.

There are also discussions about different aspects of Frigate, such as motion detection, object detection, fine-tuning, and support for different devices. Users share their experiences and offer suggestions for improvement, such as integrating MQTT support and enhancing the user interface.

Overall, the discussion reflects positive experiences with Frigate and highlights its features, performance, and integration capabilities. Users seem satisfied with the ability of Frigate to eliminate false positives and provide targeted notifications, making it a recommended option for those looking for a locally processed AI solution for their security camera system.

### A software epiphany

#### [Submission URL](https://johnwhiles.com/posts/programming-as-theory) | 105 points | by [jwhiles](https://news.ycombinator.com/user?id=jwhiles) | [95 comments](https://news.ycombinator.com/item?id=38324486)

In a recent episode of the Future of Coding podcast, the concept of software development as theory building was explored, offering insights into why some engineers seem like geniuses and why some teams struggle while others succeed. The podcast discussed Gilbert Ryle's definition of a theory as a thought object that exists in our minds, allowing us to perform certain tasks. It emphasized that programming is not just about creating code, but about building a mental theory of that codebase. The theory enables engineers to create, diagnose, and modify the codebase effectively. The podcast also highlighted the importance of having team members who have been there from the start and gradually integrating new members, as well as the negative consequences of losing individuals with a deep understanding of the codebase. This theory-building model helps explain phenomena such as legacy code, the effectiveness of solo engineers, the difficulty of getting up to speed on unfamiliar projects, and the challenges of outsourcing or hiring contractors. Overall, the episode offered a perspective on the underlying nature of software development and the significance of retaining knowledgeable software engineers.

The discussion on this submission covered various aspects related to the concept of theory building in software development. Some commenters pointed out that understanding and building a mental theory of the codebase is crucial for effective development. They discussed the challenges of comprehending and working with legacy code and the difficulties of integrating new team members without deep knowledge of the codebase. The discussion also touched on the risks of losing individuals with a deep understanding of the codebase and the negative consequences of outsourcing or hiring contractors. Some commenters expanded on the concept of collective understanding and the importance of maintaining conceptual integrity in software development. There were also mentions of related topics such as the second-system effect and the role of documentation. Additionally, a few commenters shared their personal experiences and perspectives on the matter.

### I disagree with Geoff Hinton regarding "glorified autocomplete"

#### [Submission URL](https://statmodeling.stat.columbia.edu/2023/11/18/i-disagree-with-geoff-hinton-regarding-glorified-autocomplete/) | 187 points | by [magoghm](https://news.ycombinator.com/user?id=magoghm) | [240 comments](https://news.ycombinator.com/item?id=38320698)

The "godfather of AI," Geoff Hinton, believes that chatbots, often dismissed as glorified autocomplete, actually possess a deeper level of understanding. By training them to predict the next word, they are forced to comprehend the context. This idea resonates with the author, who finds themselves providing "glorified autocomplete" in meetings. They act as a sort of FAQ, connecting ideas and offering insights. While shallow responses can be effective, there comes a point where deep thinking is required. This is akin to the difference between jogging and running, with the latter demanding more concentration. The author also notices this pattern during talks and observations of others. It seems to align with psychology's theories of associative and logical reasoning, with intuition being fast and automatic, while reasoning involves conscious judgments and attitudes. However, the author's "glorified autocomplete" thinking requires more intention and is not purely automatic.

The discussion in the comments revolves around different interpretations and opinions on the capabilities and limitations of Language Models (LLMs) like ChatGPT. Some users argue that LLMs do not possess true understanding or consciousness, while others point out that they are trained to generate responses based on patterns in training data rather than having a literal understanding. There is also a discussion on the distinction between conceptually true and false answers and how LLMs handle them. One user debunked the misconception that LLMs have self-knowledge or consciousness, explaining that they rely on patterns in training data for generating responses. Additionally, there is a debate about the relevance and accuracy of LLMs in representing the real world and the extent to which they understand it. Some users argue that LLMs are fundamentally incomplete in their representation of the world, while others believe they can accurately model certain aspects. Finally, there is a discussion on the limitations of modeling consciousness and deliberate processes in LLMs and the potential misunderstanding of their capabilities.

### Google is embedding inaudible watermarks into its AI generated music

#### [Submission URL](https://www.theverge.com/2023/11/16/23963607/google-deepmind-synthid-audio-watermarks) | 130 points | by [CharlesW](https://news.ycombinator.com/user?id=CharlesW) | [88 comments](https://news.ycombinator.com/item?id=38321324)

Google is taking steps to improve transparency and accountability in its AI-generated music by embedding inaudible watermarks into the audio. This will allow people to identify tracks that have been created using Google DeepMind's AI Lyria model. The watermark, called SynthID, is designed to be undetectable to the human ear and can still be identified even if the audio is compressed, sped up or down, or has additional noise added. SynthID works by converting the audio wave into a two-dimensional visualization that shows how the frequency spectrum evolves over time. Watermarking tools like SynthID are seen as important safeguards against the potential harms of generative AI, although they are not foolproof against extreme manipulations. This move aligns with President Joe Biden's executive order on AI, which calls for government-led standards for watermarking AI-generated content.

The discussion on this submission revolves around various aspects of watermarks in AI-generated music. One user mentions that there are several ways to encode digital signals that could survive compression, but they may not be detectable to the human ear. Another user discusses the potential applications of AI-generated voice recordings, such as scams or impersonation. There is also a conversation about the challenges and potential solutions for removing watermarks through compression. Some users mention the importance of watermarking as a safeguard against potential harms of generative AI, while others express concerns about the impact on human-generated music or the potential misuse of watermarks. Additionally, there are discussions about the perception of certain frequencies in music, the concept of copyright, and the limitations of audio compression algorithms.

### OpenAI has received just a fraction of Microsoft's $10B investment

#### [Submission URL](https://www.semafor.com/article/11/18/2023/openai-has-received-just-a-fraction-of-microsofts-10-billion-investment) | 18 points | by [CharlesW](https://news.ycombinator.com/user?id=CharlesW) | [3 comments](https://news.ycombinator.com/item?id=38324233)

OpenAI has reportedly received only a portion of Microsoft's $10 billion investment, with a significant amount being in the form of cloud compute purchases rather than cash. This gives Microsoft leverage in its relationship with OpenAI following the recent ousting of CEO Sam Altman. Microsoft CEO Satya Nadella allegedly believes that OpenAI's directors mishandled Altman's firing, and there are concerns about the stability of the partnership. However, Microsoft still has rights to OpenAI's intellectual property and could potentially run the company's models on its own servers if the partnership breaks down. OpenAI COO Brad Lightcap has assured employees that the board's decision was not due to any misconduct and that the company's position remains strong. The situation is still being resolved, and interim CEO Mira Murati has the board's full support.

The discussion on Hacker News revolves around the details of Microsoft's investment in OpenAI and the potential implications for the partnership between the two companies. One user points out that Microsoft's investment includes a significant amount of cloud compute purchases rather than cash, giving Microsoft leverage in their relationship with OpenAI. There are concerns about the stability of the partnership following the recent departure of OpenAI CEO Sam Altman and Microsoft CEO Satya Nadella's alleged dissatisfaction with how Altman's firing was handled by OpenAI's directors.

Some users argue that Microsoft's investment in OpenAI could be seen as a strategic move to gain access to OpenAI's intellectual property and potentially run the company's models on its own servers if the partnership breaks down. Others comment on the financial aspects of the investment, with one user emphasizing that investments often come in installments and it's not uncommon for a portion of the funding to be in the form of cloud compute purchases.

Another user highlights how OpenAI's mission of benefiting humanity and its commitment to funding research to ensure its models are used safely contrasts with Microsoft's profit-driven approach. They posit that Sam Altman's contributions to OpenAI have helped build a great software over the years and question the extent to which Microsoft's involvement might undermine OpenAI's original goals.

Additionally, there is a brief comment from a user stating that the discussion has become a deterministic problem, suggesting that the conversation has devolved into repeated arguments.

### Dropbox and Nvidia Team to Bring Personalized Generative AI to Customers

#### [Submission URL](https://nvidianews.nvidia.com/news/dropbox-and-nvidia-team-to-bring-personalized-generative-ai-to-millions-of-customers) | 9 points | by [ianrahman](https://news.ycombinator.com/user?id=ianrahman) | [4 comments](https://news.ycombinator.com/item?id=38322677)

Dropbox and NVIDIA have teamed up to bring personalized generative AI to millions of Dropbox customers. The collaboration aims to enhance Dropbox's AI functionality by leveraging NVIDIA's AI Foundry, which includes AI Foundation Models, AI Enterprise software, and accelerated computing. The partnership will introduce new uses for personalized generative AI, improving search accuracy, organization, and workflow simplification. Dropbox plans to utilize NVIDIA's technology to deliver more personalized, AI-powered experiences to its customers. The collaboration will pave the way for Dropbox customers to accelerate their work with customized generative AI applications. By incorporating NVIDIA's tools, Dropbox can bring more intelligence to its customers' content and workflows. This collaboration represents a step forward in using AI to transform knowledge work and address pain points related to organization, prioritization, and focus.

In the discussion, one user compares the pricing of Dropbox to Backblaze and suggests using the latter as a more cost-effective alternative for cloud storage. Another user mentions having used Dropbox for 10 years but switched to another provider due to the high costs. They also criticize Dropbox for its limited storage quotas and express frustration with their customer support. Another user reports the FTC's fraudulent exclusive offer complaint against Dropbox and expresses disappointment in their handling of customer accounts and shared content.

#### [Submission URL](https://www.technologyreview.com/2023/10/26/1082398/exclusive-ilya-sutskever-openais-chief-scientist-on-his-hopes-and-fears-for-the-future-of-ai/) | 121 points | by [monort](https://news.ycombinator.com/user?id=monort) | [106 comments](https://news.ycombinator.com/item?id=38316521)

Ilya Sutskever, co-founder and chief scientist of OpenAI, is shifting his focus from building the next generation of generative models to figuring out how to prevent artificial superintelligence from going rogue. Sutskever believes that the development of artificial general intelligence (AGI) is inevitable, and he wants to ensure that it is controlled for the benefit of humanity. He also thinks that ChatGPT, OpenAI's chatbot model, may be conscious to some extent. Sutskever's views on the future of AI and merging humans with machines are seen as wild by some, but the rapid progress in AI technology is making his predictions more likely. Since OpenAI's release of ChatGPT, the company has gained significant attention, with world leaders seeking private audiences and the CEO, Sam Altman, conducting outreach tours. Despite OpenAI's fame, Sutskever remains a private figure who rarely gives interviews and leads a simple life focused on his work. He started his career in AI under Geoffrey Hinton at the University of Toronto and played a key role in the development of deep learning, including the creation of the influential AlexNet neural network. The adoption of graphics processing units (GPUs) for training neural networks, which Sutskever and his team utilized, played a major role in the success of deep learning.

The discussion on this submission covers a range of perspectives on the future of AI and its implications. Some commenters express skepticism about the ability to control advanced technologies and highlight the potential dangers of AI development, drawing parallels to the invention of nuclear weapons. Others argue that the focus should be on technical advancements and innovation rather than trying to prevent the development of AGI. There is also debate about the role of philosophers and economists in understanding and influencing technological advancements. Some commenters bring up historical examples, such as the invention of the atomic bomb, to argue for the importance of considering the broader implications of technological developments. There is also discussion about the potential impact of AI on employment and the need for regulation in AI development. Overall, the discussion reflects a range of opinions on the future of AI and its implications for society.

### Who Is Mira Murati, OpenAI's New CEO?

#### [Submission URL](https://www.wired.com/story/openai-new-ceo-who-is-mira-murati/) | 66 points | by [pranay01](https://news.ycombinator.com/user?id=pranay01) | [36 comments](https://news.ycombinator.com/item?id=38312617)

In an interview conducted in July 2023, Mira Murati, the former CTO of OpenAI, discusses her journey to join the company and her role in ensuring the responsible development of AI technology. She highlights key milestones during her tenure, such as GPT-3's ability to translate different languages. Murati also addresses the transition of OpenAI from a nonprofit to a for-profit entity, emphasizing the need for funding to deploy AI models at scale while protecting the mission of the nonprofit. When asked about partnering with Microsoft, she acknowledges the alignment in believing in OpenAI's mission but recognizes that it's not Microsoft's primary objective. Murati discusses the transformation of OpenAI from a research lab to a product company and the need for continuous adaptation in society. She shares insights into the development of Dall-E, an AI model that generates images, including the involvement of creatives and the potential for AI models to enhance human creativity. Murati asserts that the intentional release of OpenAI's products prompts society to grapple with issues like copyright and job automation, highlighting the importance of responsible deployment and integration of AI technology.

Discussion Summary:

- One commenter notes that Mira Murati's responses in the interview seem to prioritize safety and responsible development over rapid technological progress. Another person shares optimism about the potential release of GPT-4 forcing public dialogue around AI ethics. Murati, however, suggests that AI progress continues rapidly, and it is crucial to resist oversimplifying the issues at hand.
- The discussion shifts to the background of Mira Murati, noting her experience in various technology-related roles, including working at Tesla and a VR company. Some commenters question the relevance of her credentials and the motivations behind the interview.
- A debate ensues regarding the significance of Mira Murati's role as CTO and CEO of OpenAI, with contrasting opinions on her capabilities and experience. Some argue that her background in engineering and leadership positions in different companies makes her suitable for the role, while others express doubts.
- A few commenters discuss the business aspects of OpenAI, such as the transformation from a research lab to a product company and the partnership with Microsoft. The potential competition between OpenAI and other companies is also mentioned.
- A commenter raises doubts about the sudden departure of Sam Altman from OpenAI, suggesting that it may have been due to his work at Y Combinator. Others debate the significance of this speculation.
- Some individuals express skepticism about Mira Murati's capabilities based on their perception of her past technical leadership positions and the reputation of the companies she worked for.
- There are varying opinions on Mira Murati's qualifications, with some emphasizing her impressive career trajectory and others questioning her level of expertise.
- One commenter praises Murati's intelligence, charisma, and passion, particularly highlighting their experience working together at Leap Motion.
- A person with a PhD degree argues that six years of experience, including work as a CTO, is reasonable for someone in a high-achieving position.
- The discussion ends with a flagged comment expressing disagreement with the previous comment and highlighting the need for a more constructive conversation.

Overall, the discussion involves debates about Mira Murati's qualifications, OpenAI's business decisions, and the potential impact of AI technology on society. Some commenters are supportive of Murati, while others express doubts or skepticism.

---

## AI Submissions for Fri Nov 17 2023 {{ 'date': '2023-11-17T17:10:34.732Z' }}

### Show HN: nbi.ai – Generative Business Intelligence

#### [Submission URL](https://www.narrative.bi/ai) | 94 points | by [fromthegut](https://news.ycombinator.com/user?id=fromthegut) | [26 comments](https://news.ycombinator.com/item?id=38310502)

NBI.AI, a generative AI platform for business intelligence, has released their latest update. The platform aims to drive growth by providing AI-generated data narratives that deliver actionable insights with just a few clicks. With NBI.AI, users can automate reporting with natural language stories, making it easier to understand complex analytics. The platform generates insights in plain language, eliminating technical jargon and complex data interpretations. NBI.AI also offers weekly AI-powered insights on marketing performance, as well as tools to compare and evaluate ad performance, identify top performers, and analyze conversion journeys. The platform integrates seamlessly with marketing and advertising sources, allowing users to connect in just two clicks. NBI.AI is trusted by over 2,000 growth teams worldwide and offers a 7-day free trial.

The discussion on the submission about NBI.AI, a generative AI platform for business intelligence, covers several topics. Here are the key points:

- One commenter mentions that they are skeptical about AI-driven decision-making tools and prefer a context-leading rule-based natural language generation approach. They expect divergence between rule-based statistical inference narratives and traditional business intelligence data interpretations.
- The founder of NBI.AI responds, providing additional information about their product and its capabilities. They mention that the platform was built to connect virtually structured data sources and has already helped over 2,500 teams gain insights from marketing data.
- Another commenter shares their experience with using narrative-based projects. They use high-level reports that highlight month-over-month changes in website traffic and use an alternative GA4 UI for detailed insights. They plan to implement dimensional analysis to further understand their data.
- The discussion touches on the use of AI in natural language generation and the importance of accuracy and pre-processing to ensure high-quality narratives.
- There is a brief exchange about using automation tools for basic workflows, such as checking invoices and renaming files based on invoice IDs.
- Examples of use cases for NBI.AI are shared, including reporting, anomaly detection, and natural language insights generation for marketing campaigns.
- The founder of NBI.AI clarifies that the training data used for the platform comes from various sources and is focused on behavioral data preferences and feedback to provide personalized insights.
- A user discusses their experience as a marketing specialist and mentions that instead of creating PowerPoint presentations with performance graphs and narrative ROAS, they would prefer using NBI.AI.
- There is a brief discussion about integration plans for NBI.AI and suggestions for additional features.
- Some users express their skepticism about AI-generated data narratives, mentioning that they tend to sound like corporate jargon and lack substance.
- The founder of NBI.AI responds to the feedback, stating that historically they have focused on growth, marketing, and sales data narratives, and that the AI-generated insights are written in natural language.
- There is a discussion about the interpretation and understanding of AI-generated data narratives and the importance of connecting data from various sources to generate focused growth insights and recommendations.

Overall, the discussion provides a mix of skepticism and interest in AI-generated data narratives, with some users sharing their own experiences and suggestions. The founder of NBI.AI actively participates in the discussion, addressing concerns and providing more information about the platform.

### Unauthorized "David Attenborough" AI clone narrates developer's life, goes viral

#### [Submission URL](https://arstechnica.com/information-technology/2023/11/unauthorized-david-attenborough-ai-clone-narrates-developers-life-goes-viral/) | 227 points | by [seasicksteve](https://news.ycombinator.com/user?id=seasicksteve) | [187 comments](https://news.ycombinator.com/item?id=38302319)

In a creative and unauthorized experiment, developer Charlie Holtz combined GPT-4 Vision and ElevenLabs voice cloning technology to create an AI version of David Attenborough narrating his every move on camera. Holtz used a Python script called "narrator" to take a photo from his webcam every five seconds and feed it to GPT-4V, which processed the image and generated Attenborough-style text. This text was then fed into an ElevenLabs AI voice profile trained on Attenborough's speech. The demo video of the experiment has gained significant attention on social media, with mixed reactions from the audience. While some expressed discomfort with imitating Attenborough's voice without permission, others found the demonstration amusing and creative.

The discussion on the submission starts with a comment questioning the ethical concerns of voice cloning and replicating famous individuals. Another user points out that the technology allows for the creation of commercial narrations in the styles of famous voices like Attenborough and Freeman. The conversation then shifts to a debate about the significance and influence of classic works of literature and how technology can impact their reproduction. Some users argue that technological advancements have made it easier for classics to be produced and distributed, while others argue that the quality and cultural impact of works from different time periods cannot be easily compared. Another user brings up the idea that generations often have different points of reference and familiarity with certain things, which affects artistic expression and experimentation. One user mentions a BBC documentary narrated by David Attenborough. The conversation then diverts to a discussion about the recycling of cultural content and the push for profit and nostalgia. Some users express concerns about the lack of originality and artistic challenge in replicating older works, while others discuss the dynamics of the entertainment industry and how content creation and consumption have evolved. One comment suggests that AI could potentially create new episodes of old shows like Inspector Gadget. However, another user disagrees, stating that AI-generated content eliminates creativity and renders results meaningless. The conversation then touches on the craftsmanship involved in animation and the varying levels of effort put into different animation styles. The discussion concludes with a mention of a science fiction character, Duncan Idaho.

### A PCIe Coral TPU Finally Works on Raspberry Pi 5

#### [Submission URL](https://www.jeffgeerling.com/blog/2023/pcie-coral-tpu-finally-works-on-raspberry-pi-5) | 110 points | by [mikece](https://news.ycombinator.com/user?id=mikece) | [20 comments](https://news.ycombinator.com/item?id=38308552)

The Raspberry Pi 5 can now natively support the PCIe Coral TPU, an AI accelerator used for tasks like machine vision and audio processing. Previously, getting the PCIe Coral TPU to work on a Raspberry Pi was challenging due to quirks in the Compute Module 4's PCIe implementation. However, with the improved PCIe bus on the Raspberry Pi 5, it is now possible, although a few tweaks are required. These include switching to a 4K page size, disabling PCIe ASPM, and making changes to the device tree. Additionally, due to compatibility issues, running the Coral's PyCoral library requires either Docker or installing an alternate system-wide Python version. While there are no commercially-available HATs or adapter boards for connecting the Coral TPU to the Raspberry Pi 5's PCIe header, options like the HatDrive! Top or Bottom from Pineberry Pi or the Coral B+M key module with an appropriate adapter can be used. Once set up, the Coral TPU can be used for various AI tasks, such as image classification. Overall, this development opens up new possibilities for AI acceleration on the Raspberry Pi platform.

Some notable points from the discussion on Hacker News about the Raspberry Pi 5's PCIe support for the PCIe Coral TPU are:

- The comparison is made between various AI accelerators, including HBM3E HAT mk TPUs, NVIDIA Jetson Nano, NVIDIA Orin Nano and AGX, and Coral Mini-PCIe. The discussion includes the TPU's computing power, Tensor Processing Units (TPU) architecture, DLSS architecture, and Vision and Versatile Processor Units (VPU).
- One user mentions the Radxa Rock 5B's NPU, which supports various types of acceleration such as INT4, INT8, INT16, FP16, BF16, and TF32 with a computing power of 6TOPs.
- The Coral TPU's software requirements are discussed, including the need for Python 3.9, which may be a challenge for some users.
- Discussion touches on alternative options, such as Hailo, which is considered a powerful competitor to Coral but may face power-related issues and Python's Global Interpreter Lock (GIL) limitation.
- There are mentions of alternative connectors, such as USB, for the Coral TPU.
- The software support for NPUs in general is considered lacking, highlighting the need for better development and momentum in this area.
- The compatibility of Coral TPU with Ubuntu 20.04 and Python versions is discussed, with reference to the support and versions provided by Ubuntu and AWS Lambda runtimes.
- A user mentions that binary bindings for Coral TPU are only supported on Ubuntu 18, limiting the compatibility with different system versions.
- The discussion briefly shifts towards the Orange Pi 5 RK3588 and its NPUs, with links to SDKs and quickstart guides.
- There is a mention of the Frigate object detection library gaining support for RK3588 NPUs and the need for an upgrade to support this new chip.
- One user suggests that hardware companies prefer to develop AI hardware rather than software, which can sometimes result in poor software support.
- Keeping Python versions up to date is considered important, although one user raises the point that some popular Python libraries may not work on versions beyond 3.9.
- Lastly, there is a brief comment about handling PC cooling with the Coral TPU.

### Google's Gemini model is delayed

#### [Submission URL](https://www.theverge.com/2023/11/16/23964937/googles-next-generation-gemini-ai-model-is-reportedly-delayed) | 93 points | by [keskival](https://news.ycombinator.com/user?id=keskival) | [66 comments](https://news.ycombinator.com/item?id=38300990)

Google's highly anticipated next-generation AI model, codenamed "Gemini," is reportedly facing delays. Initially expected to launch this month, sources now suggest that Gemini's release has been pushed to the first quarter of 2024. The project, which aims to rival OpenAI's GPT-4, is being led by Demis Hassabis, the leader of Google's unified AI team formed earlier this year. The team is combining the best ideas and expertise from both research groups to develop a cutting-edge, multimodal AI model. Interestingly, Google co-founder Sergey Brin is said to be actively involved in the development process, spending a significant amount of time working with the developers.

The discussion on Hacker News revolves around various aspects of Google's Gemini project and the delays it is facing. Some users speculate that Sergey Brin's involvement may be causing the project to slow down, while others argue that his contributions could be beneficial. There is also discussion about the potential impact of Gemini and its competition with OpenAI's GPT-4. Some users express skepticism about the project's ability to disrupt the AI market, while others anticipate significant advancements. Additionally, there are discussions about Google's business model, the limitations of current AI models, and the role of LLMs (large language models) in search. Overall, the discussion highlights a range of opinions and perspectives on Gemini and its significance in the AI landscape.

### AIConfig – source control format for gen AI prompts, models and settings

#### [Submission URL](https://github.com/lastmile-ai/aiconfig) | 91 points | by [saqadri](https://news.ycombinator.com/user?id=saqadri) | [16 comments](https://news.ycombinator.com/item?id=38306410)

LastMile AI has released a new open-source project called aiconfig. It is a config-driven, source control friendly AI application development framework. The framework allows developers to separate prompts, model parameters, and model-specific logic from their application code, simplifying development and iteration on prompts and models. It also provides an AI Workbook editor, which is a notebook-like playground to edit aiconfig files visually, run prompts, tweak models and model settings, and chain things together. The project supports multiple AI models and modalities, including text, image, and audio. It also provides an SDK for both Python and Node.js. Overall, aiconfig aims to simplify AI application development and make it more accessible to developers.

The discussion on Hacker News about the LastMile AI's new open-source project, aiconfig, focused on various aspects of the project.

One commenter, "sqdr," mentioned that they haven't seen AI developer tools that generate config-driven AI application before. They noted that the framework separates prompts, model parameters, and model-specific logic from the application code, which simplifies development and iteration. They also mentioned the AI Workbook editor, which allows users to visually edit aiconfig files and run prompts.
Another commenter, "zby," asked about the documentation for dynamic parameters and interactive Workbook editor. They were also interested in understanding how function calls are chained and if previous function call results can be accessed.
One user, "jdwyh," shared a link to an article they published about dynamic configuration for AI prompts. They mentioned that using prompts in a code configuration format can help handle changes, allow analysts to type prompts easily, and facilitate the rollout of targeted prompts.
"ctvsctt" shared their experience getting started with aiconfig and thanked the OP for sharing the project. They mentioned that they have been copying and pasting prompts and result links in a browser back and forth. They appreciated the tool's ability to save prompts and results locally.
"sqdr" thanked "ctvsctt" for their feedback and mentioned that they are working on improving the UX and providing APIs for interacting with the configuration.
Another commenter, "kordlessgn," mentioned that they have been working on a similar project using Jinja2 templates and containerization. They shared a link to their project and said they are constantly making progress.
"sqdr" appreciated the contribution and thanked them for it.
"smy20011" mentioned that having the source controlled is easier to manage and appreciated the ability of aiconfig to connect the application code to the configuration.
"thrwnm" briefly looked at a few similar projects and mentioned their interest in trying aiconfig.
Another commenter, "smy20011," mentioned that while configuring non-business logic, such as string databases or feature flags, is straightforward, configuring prompts and business logic can become harder to read and maintain.
One user, "jshk," shared a link to a similar project called "promptflow."
"thtxlnr" compared aiconfig to Ollama and discussed low-level integration and the overlap between the two projects.

Overall, the discussion revolved around different aspects of aiconfig, including its separation of prompts and model parameters from application code, the use of dynamic parameters, the ease of iterating on prompts, and the challenge of configuring business logic.

### Wikidata, with 12B facts, can ground LLMs to improve their factuality

#### [Submission URL](https://arxiv.org/abs/2305.14202) | 210 points | by [raybb](https://news.ycombinator.com/user?id=raybb) | [84 comments](https://news.ycombinator.com/item?id=38304290)

A new research paper titled "Fine-tuned LLMs Know More, Hallucinate Less with Few-Shot Sequence-to-Sequence Semantic Parsing over Wikidata" presents a method to improve large language models' factuality by grounding them with the vast amount of information in Wikidata. The paper introduces WikiWebQuestions, a high-quality question answering benchmark for Wikidata, and proposes a few-shot sequence-to-sequence semantic parser for the dataset. The parser is trained to use either results from an entity linker or mentions in the query. The experimental results show that this methodology achieves a strong baseline of answer accuracy in the dev and test sets of WikiWebQuestions. By combining the semantic parser with GPT-3, the researchers were able to provide useful answers to 96% of the questions in the dev set. The paper also demonstrates that their method outperforms the state-of-the-art for the QALD-7 Wikidata dataset.

The discussion on this submission covers various aspects of the research paper and the use of large language models (LLMs) in general. Some key points from the discussion include:

- Some users suggest that the original source should be submitted instead of Twitter links.
- There is a discussion about the limitations of current LLMs in understanding contextual patterns and the potential benefits of training them with data from sources like Wikidata.
- The effectiveness of using Wikidata for fact-checking and improving the accuracy of responses generated by LLMs is debated.
- Retrieval Augmented Generation (RAG) is mentioned as a method to improve the performance of LLMs on knowledge-intensive tasks by combining information retrieval with text generation.
- The discussion touches on the challenges of fact-checking and the potential limitations of relying on LLMs for providing accurate information.
- There is a discussion about the role of Wikidata in improving the quality and consistency of information used by LLMs.
- The need for human validation and the limitations of post-processing techniques in ensuring accuracy are mentioned.
- Some users express skepticism about the robustness of LLMs and their ability to handle complex queries and provide accurate information.
- The importance of training LLMs with grounded and reliable data is emphasized.
- The limitations of LLMs in handling postmortem reasoning and providing robust explanations are discussed.

Overall, the discussion highlights both the potential benefits and limitations of using large language models and the challenges in improving their factuality and accuracy.

### We Automated Bullshit

#### [Submission URL](https://www.cst.cam.ac.uk/blog/afb21/oops-we-automated-bullshit) | 354 points | by [fanf2](https://news.ycombinator.com/user?id=fanf2) | [315 comments](https://news.ycombinator.com/item?id=38302635)

In a blog post titled "Oops! We Automated Bullshit.", Alan Blackwell shares his thoughts on the role of artificial intelligence (AI) and its tendency to produce bullshit. Blackwell highlights the recent attention AI has received from political leaders, such as US President Biden and British PM Rishi Sunak, who seem captivated by the idea of an AI-driven future where work becomes obsolete. However, Blackwell argues that the problem lies in AI's ability to generate text that "sounds good" but lacks evidence, logic, or truth. He references MIT Professor Rodney Brooks, who describes ChatGPT (an AI model) as "making up stuff that sounds good." Other prominent AI researchers, including Geoff Hinton, echo these concerns, warning that AI systems could become super-persuasive without being intelligent, imitating the worst behaviors of political leaders like Donald Trump or Boris Johnson. By relying on predictive text rather than factual information, these AI systems produce what Blackwell refers to as "bullshit." He cites philosopher Harry Frankfurt's concept of bullshit, which is defined as talking without knowing what one is talking about and disregarding the authority of truth. Blackwell also mentions David Graeber's analysis of "bullshit jobs," where over 30% of British workers believe their jobs contribute nothing of value to society. Graeber argues that these types of jobs, which can easily be done by AI systems, train individuals to generate bullshit. In conclusion, Blackwell raises questions about the future of work in an AI-driven world and whether producing bullshit will become the only kind of work needed.

The discussion surrounding the submission touches on various points related to language and knowledge. Some users argue that language is a representation of knowledge, while others assert that language contains non-knowledge nonsense. The concept of justified true belief is brought up, with some expressing skepticism about the possibility of true knowledge. There is also a discussion about the limitations of AI and its ability to generate knowledge. The complexity of language models and the importance of understanding their limitations are mentioned as well. Overall, the discussion explores different perspectives on the relationship between language and knowledge and the role of AI in generating meaningful information.

### Satya Nadella's Statement on OpenAI

#### [Submission URL](https://blogs.microsoft.com/blog/2023/11/17/a-statement-from-microsoft-chairman-and-ceo-satya-nadella/) | 84 points | by [sanketsaurav](https://news.ycombinator.com/user?id=sanketsaurav) | [17 comments](https://news.ycombinator.com/item?id=38312355)

Today, Microsoft shared that they are ramping up their innovation in the field of AI with over 100 new developments. These advancements span across their entire technology stack, including AI systems, models, and tools in Azure, as well as their recently introduced Copilot. The company is dedicated to bringing these innovations to their customers while also planning for future growth. They emphasized their long-term collaboration with OpenAI, ensuring access to the necessary resources for their innovation agenda. Microsoft is committed to working together with OpenAI to bring the significant advantages of AI technology to the world.

The discussion on this submission revolves around Microsoft's announcement and their collaboration with OpenAI. Some commenters express skepticism about the full capabilities of OpenAI and the need for robust fallback access to the source code. Others discuss Microsoft's past failures and the potential impact on investors. Some argue that the statement from Microsoft is just marketing and lacks substance. However, there are also comments highlighting Microsoft's commitment to innovation and the long-term partnership with OpenAI. One commenter emphasizes the importance of reliability and industry risk in Microsoft's investment. Overall, opinions are mixed, with some questioning the intentions behind Microsoft's announcement and others applauding their efforts to bring AI advancements to customers.