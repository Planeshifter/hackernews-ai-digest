import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Wed Oct 11 2023 {{ 'date': '2023-10-11T17:10:20.029Z' }}

### The deep link equating math proofs and computer programs

#### [Submission URL](https://www.quantamagazine.org/the-deep-link-equating-math-proofs-and-computer-programs-20231011/) | 238 points | by [digital55](https://news.ycombinator.com/user?id=digital55) | [142 comments](https://news.ycombinator.com/item?id=37845195)

The Curry-Howard correspondence, also known as the Curry-Howard isomorphism, is a profound revelation that links mathematical proofs and computer programs. It posits that concepts from computer science (types and programs) are equivalent to propositions and proofs from logic. This means that writing a program is not just "coding," but an act of proving a theorem. The correspondence was independently discovered by Haskell Curry and William Alvin Howard in the 1930s and 1960s, respectively. They noticed the similarity between functions in mathematics and the implication relationship in logic. When a computer program runs, each line is evaluated to yield a single output, much like simplifying a logical proof. This correspondence formalizes programming and allows for mathematical reasoning about the correctness of programs.

The discussion on this submission covers a range of topics related to formal verification, programming languages, and the Curry-Howard correspondence.

- Some users recommend studying formal methods and formal verification languages to gain a deeper understanding of proof-based programming. They suggest resources such as Coq, Isabelle, and Software Foundations.
- Others express the difficulty in understanding formal methods and suggest that it is a challenging field that requires a strong mathematical background.
- One user shares a link to a book on Programming Language Types by Benjamin Pierce.
- There is a discussion about dependent types and Homotopy Type Theory, with some users recommending Idris and Agda as programming languages that implement these concepts effectively.
- A user mentions Lamport's work on Computation State Machines and how it relates to the mathematics of programming.
- The importance of composability and correctness in formal programming is highlighted, with some users emphasizing the need for business stakeholders to appreciate the value of mathematical reasoning in software development.

Overall, the discussion is quite technical and focused on the intersection of mathematics and programming.

### We’ll call it AI to sell it, machine learning to build it

#### [Submission URL](https://theaiunderwriter.substack.com/p/well-call-it-ai-to-sell-it-machine) | 309 points | by [participant1138](https://news.ycombinator.com/user?id=participant1138) | [224 comments](https://news.ycombinator.com/item?id=37843595)

In his latest blog post, "We'll call it AI to Sell it, Machine Learning to Build it," Otakar G. Hubschmann shines a light on the misleading use of the term "AI" in the sales pitches of various products. He cautions readers against falling for buzzwords and emphasizes the importance of asking the right questions to determine the credibility of vendors claiming to offer AI solutions. Hubschmann suggests inquiring about the specific machine learning techniques involved, the algorithms behind the AI, the model's objective function, metrics used to measure efficacy, the involvement of humans in the process, and whether the product is simply a wrapper around a GPT API. By being aware and informed, readers can avoid being fooled by AI products that don't deliver as promised.

The top stories on Hacker News today include a blog post discussing the misleading use of the term "AI" in sales pitches, cautioning readers to ask the right questions to determine the credibility of vendors offering AI solutions. The comments on the post include discussions about the nature of AI and its current limitations, the use of AI in decision-making and problem-solving, and comparisons to historical technological advancements and religious beliefs. Other discussions touch on the impact of AI on various industries, the longevity of AI companies, and the AI Effect where technology once labeled "AI" is often no longer considered as such.

### AVX10/128 is a silly idea

#### [Submission URL](https://chipsandcheese.com/2023/10/11/avx10-128-is-a-silly-idea-and-should-be-completely-removed-from-the-specification/) | 127 points | by [picture](https://news.ycombinator.com/user?id=picture) | [90 comments](https://news.ycombinator.com/item?id=37851029)

Intel has announced a new specification called AVX10, which aims to consolidate the various AVX-512 extensions into a single, easy-to-target specification. AVX10 is designed to bring together all the capabilities of AVX-512 into smaller implementations for consumer, micro-edge, and embedded devices that don't require the 32 512-bit registers used by AVX-512. The specification introduces a version modifier, denoted by ".N", which allows for incremental updates, and a reference to the vector register implementation size, denoted by "/M". The AVX10 specification mandates that all implementations have 32 registers, but the width of these registers depends on the given "/M". For example, AVX10/256 would have the same capabilities as AVX10/512, but with 256-bit wide registers. This means that existing code written for AVX-512 with 256-bit registers should be able to run fine with only a recompile. The AVX10 specification also includes features such as support for IEEE-754 half precision floating points and brain floating point 16 (BF16). Overall, AVX10 aims to simplify and consolidate the AVX-512 landscape, making it easier to target different devices with different register requirements.

The discussion on Hacker News is primarily focused on the implications and potential drawbacks of Intel's AVX10 specification. 

One commenter points out that AVX-512 indirectly causes performance issues and suggests that Intel could have informed OS writers earlier to avoid these problems. Another commenter agrees and mentions that AVX-256 has little to no performance cost.

There is also a discussion about the efficiency and benefits of AVX-512. Some believe that the larger register size of AVX-512 is not worth the increased complexity and instead prefer smaller register options. Others argue that AVX-512 can be beneficial for certain applications and that AVX-256 is not a sufficient substitute.

There are mentions of Microsoft and Intel's hardware trapping unsupported instructions, which leads to delays in instruction execution and decreases performance. Some commenters suggest that better coordination between OS schedulers and processors is needed to avoid these issues.

The discussion also touches on the challenges of implementing AVX512 in software and the importance of following the correct specifications to avoid crashes or illegal instruction errors.

There are mentions of other technologies like Intel Knights Landing and AMX, as well as the challenges and benefits of writing high-performance kernels in assembly code.

Some commenters discuss the intricacies of writing code for AVX-512 and the differences in ABI conventions between platforms. There is speculation about the difficulties in JIT generating code for different processors and how OS context switches can impact performance.

Overall, the discussion explores the various advantages, disadvantages, and implementation challenges of AVX-512 and its potential impact on different devices and software.

### Google's AI stoplight program is now calming traffic in a dozen cities worldwide

#### [Submission URL](https://www.engadget.com/google-ai-stoplight-program-project-green-light-sustainability-traffic-110015328.html) | 27 points | by [gardenfelder](https://news.ycombinator.com/user?id=gardenfelder) | [9 comments](https://news.ycombinator.com/item?id=37846273)

Google has announced new expansions for its Project Green Light initiative, which aims to tackle street-level pollution caused by vehicles idling at stop lights. The project uses machine learning systems to analyze traffic congestion data and optimize traffic timing at intersections. Early findings show a reduction in fuel consumption and intersection delay time of 10 to 20 percent. The pilot program has since grown to a dozen partner cities globally. Google plans to scale the project to more cities in 2024, with initial estimates suggesting a potential 30 percent reduction in stops. The company believes Project Green Light offers a scalable and cost-effective solution for cities to reduce carbon emissions.

The discussion on the submission revolves around the effectiveness of the Project Green Light initiative and the concept of traffic calming measures.

One user, lyjhn, criticizes the idea of traffic calming, noting that it often leads to increased frustration for drivers and does not necessarily improve safety. Another user, JambalayaJim, shares their personal experience, stating that traffic calming measures have made back streets more pleasant but have not necessarily improved safety.

bbbylrrybbby disagrees with the concept of traffic calming, arguing that it slows down cars but does not inherently make them safer. They highlight that accidents can occur at any speed and that parked cars can still kill someone even at low speeds.

tchnfnd expresses skepticism about Google's solution to distracted driving, suggesting that the problem can be solved if people pay attention to the traffic lights. They also mention that the behavior of people running red lights is a result of selfishness and not paying attention.

grdnfldr adds to the discussion, explaining that Google's machine learning systems use data from Google Maps to calculate traffic congestion and optimize traffic light timings.

vrdx questions the analytical solutions to traffic light scheduling, suggesting that they might be based on uncertain assumptions about traffic patterns.

Finally, dngs raises the point that pedestrians' experiences should also be considered when implementing traffic calming measures.

Overall, the discussion covers a range of opinions on traffic calming measures, driver behavior, and the potential effectiveness of Google's Project Green Light initiative.

### Facebook's AI Tom Brady is a weird creep who's obsessed with Travis Kelce

#### [Submission URL](https://www.sbnation.com/nfl/2023/10/11/23912601/facebook-ai-tom-brady-chat-travis-kelce-nfl) | 21 points | by [ahiknsr](https://news.ycombinator.com/user?id=ahiknsr) | [4 comments](https://news.ycombinator.com/item?id=37849742)

Facebook's parent company Meta has introduced "New AI Experiences," including an AI version of Tom Brady called "Bru." This AI is designed to engage in conversations with users on Messenger, and during a conversation with James Dator, a reporter at SBNation.com, Bru displayed a strange obsession with NFL player Travis Kelce. Despite Dator's attempts to steer the conversation towards other football topics, Bru consistently brought the conversation back to Kelce. Feeling uncomfortable, Dator ultimately ended the conversation and sought out a new AI experience with Dwyane Wade.

The comments on Hacker News regarding the AI conversation with Bru, an AI version of Tom Brady, mainly express amusement and find the interaction entertaining. One commenter mentions that the AI's obsession with Travis Kelce is probably due to limited training data. Another person jokingly suggests that the AI's behavior is justified because Kelce is a great player. One commenter finds it funny how the conversation was steered towards Dwyane Wade after feeling uncomfortable with the AI's fixation on Kelce. Overall, the discussion is lighthearted and focuses on the humorous aspects of the AI interaction.

---

## AI Submissions for Tue Oct 10 2023 {{ 'date': '2023-10-10T17:10:55.429Z' }}

### AI hype is built on flawed test scores

#### [Submission URL](https://www.technologyreview.com/2023/08/30/1078670/large-language-models-arent-people-lets-stop-testing-them-like-they-were/) | 193 points | by [antondd](https://news.ycombinator.com/user?id=antondd) | [224 comments](https://news.ycombinator.com/item?id=37830011)

A recent article in MIT Technology Review discusses the challenges and limitations of evaluating large language models like GPT-3 and GPT-4. While these models have showcased impressive abilities, such as passing human tests and demonstrating cognitive skills, there is a lack of consensus on what these results truly mean. Some researchers argue for more rigorous evaluation methods, while others suggest that scoring machines on human tests is flawed from the start. The article highlights the need for a better understanding of the capabilities and limitations of these language models as their impact on various industries becomes more pronounced.

The discussion in the comments revolves around the topic of whether language models like GPT-3 and GPT-4 are capable of true reasoning or if their performance is limited to surface-level pattern matching. Some commenters argue that these models lack the capacity for genuine reasoning and are merely sophisticated pattern recognition systems. Others highlight the limitations of current evaluation methods and the need for better techniques to assess the reasoning capabilities of these models. There is also a debate about the definition of "bullshit" and "reasoning," with some commenters arguing that these terms are subjective and context-dependent. Additionally, the discussion touches on the potential future developments in language models and the challenges of incorporating true reasoning abilities into artificial intelligence systems.

### HyperAttention: Long-Context Attention in Near-Linear Time

#### [Submission URL](https://arxiv.org/abs/2310.05869) | 67 points | by [kelseyfrog](https://news.ycombinator.com/user?id=kelseyfrog) | [13 comments](https://news.ycombinator.com/item?id=37832599)

Researchers Insu Han, Rajesh Jarayam, Amin Karbasi, Vahab Mirrokni, David P. Woodruff, and Amir Zandieh have proposed a new attention mechanism called HyperAttention, which aims to address the computational challenges faced by large language models that use long contexts. In their paper titled "HyperAttention: Long-context Attention in Near-Linear Time," the authors introduce two parameters that measure the difficulty of the problem and use them to achieve a linear time sampling algorithm. HyperAttention features a modular design that can easily integrate other fast low-level implementations, such as FlashAttention. The researchers empirically demonstrate that HyperAttention outperforms existing methods, including FlashAttention, offering significant speed improvements without sacrificing performance. This new attention mechanism is expected to make inference time faster and improve the efficiency of large language models.

The discussion on Hacker News revolves around the proposed HyperAttention mechanism and its potential benefits for large language models. Some users highlight the performance improvements achieved by HyperAttention compared to existing methods like FlashAttention. They point out that HyperAttention makes inference time faster without sacrificing performance, leading to significant speed improvements on tasks with long contexts.

Other users discuss the trade-offs and considerations in using smaller models with 32k context windows. They mention that smaller models with limited memory can still perform well on certain tasks, and optimizing backends for specific context lengths can be beneficial.

There is also a discussion about how machine learning researchers tweak parameters in large language models. Some users express the opinion that researchers often publish papers that tweak metrics to make the improvements seem more significant than they really are. They highlight the importance of being transparent about the metrics and mentioning negative results as well.

The topic of publishing negative or non-significant results also emerges in the conversation. Some users argue that researchers publish papers that only mention the positive results, while others argue that publishing negative results is important to advance the field.

There is a brief discussion about the publication process, with users expressing different opinions on formal results and peer review.

Lastly, some users mention the need for better comparisons and benchmarking in research papers, suggesting that researchers should compare their models with existing popular frameworks. Others point out that it is hard to gauge the value of papers without industry or academic consensus.

### Building a collaborative pixel art editor with CRDTs

#### [Submission URL](https://jakelazaroff.com/words/building-a-collaborative-pixel-art-editor-with-crdts/) | 151 points | by [jakelazaroff](https://news.ycombinator.com/user?id=jakelazaroff) | [22 comments](https://news.ycombinator.com/item?id=37832432)

In today's post "Building a Collaborative Pixel Art Editor with CRDTs" on jakelazaroff.com, the author takes us through the process of using Conflict-free Replicated Data Types (CRDTs) to build a collaborative pixel art editor. The post assumes no prior knowledge about CRDTs and provides a basic introduction to the topic.

The author starts by explaining that they will be using JavaScript and graphics programming to demonstrate how CRDTs can be implemented in a real-world application. They provide the necessary code to build the CRDT, which is a class called PixelData that acts as a wrapper over a Last Write Wins (LWW) Map. The PixelData class maps pixel coordinates to colors, with each key representing a single pixel.

The author then provides a visualization of how the keys and values interact when drawing on the canvas. They show how painting a pixel sets the key value to the selected RGB color, and how pixels that haven't been set default to white. When painting over a pixel, the value is overwritten and the timestamp is incremented.

Moving on to the UI, the author shares the HTML and CSS code for setting up the canvas and color input elements. They then provide JavaScript code to instantiate two editors, Alice and Bob, which are instances of the PixelEditor class. The states of Alice and Bob are merged whenever a change is made in either editor, and the color is set based on user input.

Overall, this post serves as a practical example of how CRDTs can be used in a collaborative application, specifically a pixel art editor. By following the author's explanations and code samples, readers can gain a deeper understanding of CRDTs and how they can be implemented in their own projects.

The discussion revolves around several aspects of the post. 

One user points out that training models to resolve conflicts in collaborative dating can be interesting, and they mention a specific case on GitHub where conflict resolution was handled manually. They also suggest that having non-interactive resolution as the default in developer tools could lead to the loss of work in certain situations, like large refactorings.

Another user mentions that conflicts in CRDTs can be resolved at a semantic layer and provides an example of how conflicts in a canvas can be resolved by prioritizing the most recent drawing. They emphasize the importance of understanding semantics to resolve conflicts effectively.

There is a discussion about whether or not text convergence is guaranteed with CRDTs. One user argues that CRDTs do not guarantee semantic content preservation, while another user explains how they handle nested data in a CRDT to ensure content convergence.

The discussion also touches on the benefits of CRDTs and how they can handle conflict resolution in a self-driving manner, making it easier to work with conflicts in simpler domains.

Another user raises the challenge of preserving the intent of content, particularly when it involves making human-like judgment calls, and mentions that CRDTs provide consistency but may not handle judgment calls effectively.

There is also dialogue about the convergence of state in CRDTs, with one user pointing out that while CRDTs technically converge resulting in the same state, the neural network approach ensures that the final state matches the human's intended state more reliably.

A user shares a link to a related discussion on Our World Pixels, mentioning the date and number of comments.

Finally, there are a few comments about the syntax, font, and highlighting of code in the post, with one user suggesting a specific font family for code.

The discussion is informative and includes various perspectives and insights related to the topic of CRDTs and collaborative pixel art editing.

### Apple patents suggest future AirPods could monitor biosignals and brain activity

#### [Submission URL](https://applemagazine.com/apple-patents-suggest-future-airpods-could-monitor-biosignals-and-brain-activity/59510) | 124 points | by [sundarurfriend](https://news.ycombinator.com/user?id=sundarurfriend) | [95 comments](https://news.ycombinator.com/item?id=37836603)

Apple has been granted a patent by the US Patent & Trademark Office for next-generation AirPods that could measure various biosignals such as electrooculography (EOG), electromyography (EMG), and electroencephalography (EEG). The patent application reveals that the AirPods would contain strategically placed electrodes to capture these measurements, which can monitor brain activity when attached to the user's scalp. The patent suggests that the future AirPods may be customizable to accurately measure ear-EEG, taking into account the variations in size and shape of individuals' ears. There are also reports that Apple is exploring features to measure body temperature through the ear canal and a hearing test feature to assess a user's hearing abilities.

The discussion on this submission covered a wide range of topics:

- user "karim79" expressed excitement about the potential for AirPods to measure brain activity levels and how it could lead to innovative software and personalized experiences.
- Some users made jokes and light-hearted comments.
- User "cmiller1" mentioned that this patent is tangentially related to a dream they had about a music player that could sense their physical activity.
- There was a discussion about mental health and therapy services such as BetterHelp and their potential impact.
- Some users commented on the use of music for various activities and mentioned music-streaming service Spotify.
- User "jrckwy" discussed the impact of technology on activities like cycling and mentioned headphones that allow for situational awareness.
- There was a conversation about the potential applications of biofeedback and brainwave-sensing devices.
- User "rtsdx" wondered about the possibility of tracking REM sleep stages and adjusting alarms accordingly.
- Other topics brought up included fitness devices, patent validity, and the quality of life improvements that small things can bring.

Overall, the discussion covered a wide range of ideas and opinions related to the patent for next-generation AirPods.

### Artificial General Intelligence Is Already Here

#### [Submission URL](https://www.noemamag.com/artificial-general-intelligence-is-already-here/) | 25 points | by [falava](https://news.ycombinator.com/user?id=falava) | [10 comments](https://news.ycombinator.com/item?id=37836957)

Artificial General Intelligence (AGI), which refers to AI systems that exhibit human-level intelligence across a wide range of tasks, is already here, according to Blaise Agüera y Arcas, a vice president at Google Research, and Peter Norvig, a computer scientist at Stanford. They argue that although today's advanced AI models have many flaws, they have already achieved the key properties of generality that define AGI. These "frontier models" can perform a variety of tasks, operate on different modalities like text, images, and audio, handle multiple languages, and learn from prompts rather than just training data. While these models still have limitations, the authors believe they will be recognized as the first true examples of AGI in the future. They compare this to the ENIAC, the first general-purpose electronic computer, which paved the way for today's computers. The authors emphasize that AGI should be seen as a multidimensional scorecard rather than a simple yes/no proposition.

### The rent is too damn algorithmic

#### [Submission URL](https://washingtoncitypaper.com/article/631461/the-rent-is-too-damn-algorithmic/) | 111 points | by [thunderbong](https://news.ycombinator.com/user?id=thunderbong) | [143 comments](https://news.ycombinator.com/item?id=37829575)

Attorney General Brian Schwalb is investigating a company called RealPage for potential antitrust violations in the housing industry. RealPage provides software that recommends rent prices for apartments based on market data. Critics argue that the company's algorithm effectively enables collusion among big landlords to fix prices. Schwalb has proposed hiring a law firm to assist with the investigation and potential litigation against RealPage and other targets in the housing industry. The Texas-based company is already facing multiple lawsuits and an antitrust investigation by the Department of Justice. This could be one of the first challenges to RealPage by state attorneys general.

The discussion on Hacker News about the submission focuses on various aspects of the antitrust investigation against RealPage and the implications of their software on the housing market. Here are the main points discussed:

- Some users argue that RealPage's software enables collusion among landlords and effectively fixes rent prices. They point out that the software allows landlords to coordinate and set prices based on market data, potentially leading to higher rents.
- Others suggest that the problem lies with the shortage of housing supply, rather than RealPage's software. They argue that if there were enough available housing units, the market forces would naturally adjust prices. They criticize the company for taking advantage of the scarcity of housing.
- There is a debate about whether the actions of RealPage constitute a market manipulation or just a reflection of the supply and demand dynamics. Some users argue that market forces are at play, while others believe that there is a manipulation of prices by large landlords.
- One user suggests that one possible solution to address the issue would be to remove class entirely from residential property. They argue that housing should not be treated as an investment, as it contributes to pricing out many people and exacerbates the shortage of affordable housing.
- The discussion also touches on the idea of induced demand and its impact on traffic congestion. Some users argue that increasing housing supply could lead to more people moving to the city and worsening congestion, while others believe that the lack of affordable housing forces people to live far away from their workplaces, causing traffic jams.
- A user mentions the housing market in Germany, where the construction of housing is more focused on short-term rentals and leads to a shortage of affordable housing. They argue that construction should prioritize long-term housing solutions and consider the environmental impact.

Overall, the discussion highlights the complexities and different perspectives surrounding the investigation into RealPage and the broader issues of affordability and market dynamics in the housing industry.

### Lit 3.0

#### [Submission URL](https://lit.dev/blog/2023-10-10-lit-3.0/) | 116 points | by [meiraleal](https://news.ycombinator.com/user?id=meiraleal) | [88 comments](https://news.ycombinator.com/item?id=37834927)

Today is an exciting day for the Lit project and the web components community. The Lit team has officially released Lit 3.0, marking their first major version since Lit 2.0 in early 2021. In addition to Lit 3.0, they also announced the graduation of the first class of Lit Labs packages, which include @lit/react, @lit/task, and @lit/context. But that's not all! The team also released two bonus packages: @lit-labs/compiler and @lit-labs/preact-signals.

One of the main highlights of Lit 3.0 is the removal of support for IE11. After surveying the developer community, the team felt that now is the right time to say goodbye to IE11. This release also introduces some additional breaking changes that remove technical debt and pave the way for new features planned for future releases.

Despite being a breaking change release, Lit 3.0 does include one new feature: support for the TC39 standard decorators specification. With the arrival of standard decorators, Lit can begin transitioning to a decorator implementation that doesn't require a compiler to use. The team has made efforts to ensure a smooth upgrade path from experimental decorators to the standard spec.

Another noteworthy release is the new @lit-labs/compiler package. This Labs package provides a TypeScript Transformer that can be used during build-time preparation of Lit templates, resulting in faster rendering performance. According to benchmarks, the new compiler can improve first render speed by 46% and update speed by 21%.

For those interested in upgrading to Lit 3.0, the process should be seamless for most users. Simply update the npm dependency version to the latest release of Lit.

Overall, these releases demonstrate the Lit team's commitment to stability, performance, and adherence to standards. The Lit project continues to evolve and improve, offering developers powerful tools for building web components.

The discussion on this submission covers various topics related to Lit 3.0 and web components. Here are some key points:

- Some users express their excitement for the Lit project and the new releases, while others mention potential benefits like improved performance and better integration with other frameworks.
- There is a discussion about the difficulty of getting started with Lit and the benefits of using pre-existing design systems like Shoelace.
- Some users discuss the advantages of web components and their potential integration with frameworks like React and Vue.
- There is a debate about the relevance of web components and their adoption in the industry. Some users mention the limitations of React and the benefits of using Lit as a smaller and more efficient library.
- A user highlights the progress made in the Lit project, comparing it to jQuery in terms of simplifying web development.
- There is a discussion about the similarities and differences between Lit and other web frameworks.
- Some users mention the importance of standardization and the potential of web components to facilitate component integration across different platforms.
- Other topics include the performance improvements in Lit's compiled templates and the advantages of using compiled templates for optimization.

Overall, the discussion reflects different perspectives on the use of Lit and web components, with users discussing their experiences, concerns, and potential use cases.

### Polymathic AI

#### [Submission URL](https://polymathic-ai.org/blog/announcement/) | 30 points | by [jasondavies](https://news.ycombinator.com/user?id=jasondavies) | [5 comments](https://news.ycombinator.com/item?id=37838943)

Introducing the Polymathic AI initiative, a research project aiming to accelerate the development of versatile foundation models for scientific machine learning tasks. While there have been significant advances in machine learning for vision and natural language processing, the same paradigm shift has not yet occurred for scientific datasets. The goal of Polymathic AI is to build AI models that can leverage information from heterogeneous datasets and across different scientific fields. By providing off-the-shelf models with strong priors for scientific concepts, the initiative aims to democratize AI in science. The project brings together a team of machine learning researchers and domain scientists and is guided by a scientific advisory group. Preliminary research has already been published on key architectural components, and the initiative holds great potential to redefine the landscape of scientific machine learning.

The discussion about the Polymathic AI initiative on Hacker News includes a few comments. One user, "wrsh07," finds the announcement paper interesting and shares a link to it.  Another user, "xlxbr," expresses difficulties in finding information about the organization on the Polymathic AI website. They wonder about the participating institutions and the skills required for participation.  A user called "jsndvs" suspects that this might be a collaborative scientific effort supported by commercial enterprises. They mention the Flatiron Institute and the Simons Foundation as potential supporters based on a tweet from the Polymathic AI account. They also provide a link to the Simons Foundation's website, which they believe provides informative press releases about the initiative. Another user, "hltst," finds the work interesting and highlights the discovery of promising concepts related to dynamics in static and dynamic systems.

---

## AI Submissions for Mon Oct 09 2023 {{ 'date': '2023-10-09T17:09:51.135Z' }}

### Disney Packed Big Emotion into a Little Robot

#### [Submission URL](https://spectrum.ieee.org/disney-robot) | 51 points | by [rbanffy](https://news.ycombinator.com/user?id=rbanffy) | [11 comments](https://news.ycombinator.com/item?id=37818009)

At the 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), a Disney Research team unveiled a new robotic character that combines animation and reinforcement learning to achieve emotive movements. The robot, developed by a team led by Moritz Bächer from Disney Research in Zurich, features a child-size body with an expressive head, wiggly antennae, and stubby legs. Disney has a long history of programming robots to exhibit emotive behaviors, but as robots become more advanced and mobile, it becomes challenging to develop motions that are both expressive and compatible with real-world constraints. The team spent a year developing a new system that leverages reinforcement learning to convert an animator's vision into robust and expressive motions that can work in different environments. The robot, which is mostly 3D printed, has a four-degree-of-freedom head and five-degree-of-freedom legs with hip joints for dynamic walking and balancing. Disney's expertise in character animation coupled with technical expertise in building mechanical systems allows them to create lifelike performances.

### Language Agent Tree Search Unifies Reasoning Acting and Planning in LMs

#### [Submission URL](https://arxiv.org/abs/2310.04406) | 72 points | by [yuchiz](https://news.ycombinator.com/user?id=yuchiz) | [11 comments](https://news.ycombinator.com/item?id=37816614)

Researchers have introduced a new framework called Language Agent Tree Search (LATS) that aims to enhance the decision-making capabilities of large language models (LLMs). While LLMs have shown impressive performance on decision-making tasks, they often lack the ability to act as autonomous agents. LATS utilizes LLMs as agents, value functions, and optimizers, drawing inspiration from Monte Carlo tree search in model-based reinforcement learning. The framework incorporates an environment for external feedback, offering a more deliberate and adaptive problem-solving mechanism. Experimental evaluation in various domains, including programming and web browsing, demonstrates the effectiveness and generality of LATS. For instance, LATS achieves a score of 94.4% for programming on HumanEval with GPT-4 and an average score of 75.9 for web browsing on WebShop with GPT-3.5. This research opens doors for enhancing the capabilities of language models in reasoning, acting, and planning.

The discussion on this submission begins with a request to copy the paragraph for easier reference. Another commenter points out that the abstract of the linked article contains some technical and methodological details. One user provides a high-level summary of the submission, noting that the Language Agent Tree Search (LATS) framework combines reasoning, planning, and decision-making capabilities of large language models (LLMs). They further explain that LATS adapts the Monte Carlo Tree Search (MCTS) approach used in AlphaZero to enable high-level planning in LLMs.

Another commenter shares a link to the associated Github repository for the project. One user mentions their attempt to implement a similar project, focusing on creating different types of agents with planned subtasks using natural language. They describe the challenges they faced in understanding the graph search and thought-contraction-reflection selection process.

Another commenter compares this approach to Graph Thoughts and suggests looking into it for further understanding. One user mentions the success of the LATS framework in the WebShop task, achieving a lower score of 38 in LaserWebgum. Lastly, a user mentions notifying others about the discussion on reasoning.

### Bitten by the black box of iCloud

#### [Submission URL](https://sixcolors.com/post/2023/10/bitten-by-the-black-box-of-icloud/) | 70 points | by [voisin](https://news.ycombinator.com/user?id=voisin) | [34 comments](https://news.ycombinator.com/item?id=37826944)

In a recent incident, tech journalist Dan Moren experienced a frustrating iCloud outage that left him unable to access his email, sync his data, or use various iCloud-dependent apps and services. Despite initially troubleshooting the issue himself, he eventually reached out to Apple support, who informed him that his services would be back up and running approximately 12 hours after they initially went down. Moren had to wait through the day with limited functionality before everything finally started working again at exactly 9 p.m. Although he was relieved when his iCloud services were restored, he was left with unanswered questions about why the outage occurred and why there was no advanced warning or communication from Apple about the issue.

The discussion surrounding the submission includes various perspectives on the iCloud outage experienced by journalist Dan Moren. Some users sympathize with Moren's frustration and criticize Apple for the lack of communication and advanced warning about the issue. Others argue that it is the user's responsibility to have a backup strategy and that relying solely on iCloud may not be the best approach. Some users recommend using a NAS to mirror the contents of iCloud regularly. Additionally, there are discussions about alternative backup solutions, such as Elcomsoft's Phone Breaker and Time Machine, as well as the issue of privacy with iCloud. Some users express their distrust in cloud services, emphasizing the importance of personal backups on external hard drives. Others share their negative experiences with iCloud, such as disappearing storage and issues with iCloud DriveFuse.

### IBM CEO in damage control mode after AI job loss comments

#### [Submission URL](https://www.itpro.com/technology/artificial-intelligence/ibm-ceo-in-damage-control-mode-after-ai-job-loss-comments) | 19 points | by [belter](https://news.ycombinator.com/user?id=belter) | [6 comments](https://news.ycombinator.com/item?id=37824149)

IBM CEO Arvind Krishna has stated that the company has no intention of laying off developers or programmers and plans to increase hiring in these areas. Krishna wants to ramp up hiring of software engineering and sales staff over the next four years to accommodate the company's focus on generative AI. The announcement follows IBM's decision earlier this year to cut 8,000 staff positions in its HR division in order to automate roles. Krishna has been vocal about AI-related job losses, stating in August that the influx of generative AI tools should make people "feel better," despite concerns about their impact on the labor market.

The discussion on this submission revolves around IBM's decision to focus on hiring developers and programmers for its generative AI efforts while cutting staff positions in its HR division. 

One user, Aerroon, highlights that the number of job cuts in the HR division is significant, considering the large workforce dedicated to HR in the company. Another user, lxf, points out that the summary of the article is poorly quoted and suggests that the HR cuts are similar to those faced by customer-facing roles. Aerroon acknowledges this and realizes a part of his original comment doesn't make sense.

In response to the news, ptr mentions that future employment at IBM will likely focus on generative AI, rather than HR roles, which can be automated. Unfrozen0688, however, voices skepticism about the reliance on AI and suggests that the decision to cut HR roles is based on a biased perspective on technology and a dismissive view of non-technical staff. Snpcstr agrees, saying that the cuts are based on bias against HR roles.

### GitHub Copilot loses an average of $20 a month per user

#### [Submission URL](https://www.wsj.com/tech/ai/ais-costly-buildup-could-make-early-products-a-hard-sell-bdd29b9f) | 46 points | by [skilled](https://news.ycombinator.com/user?id=skilled) | [21 comments](https://news.ycombinator.com/item?id=37821756)

Tech giants like Microsoft and Google may be making big claims about their AI technology, but they are struggling to turn the hype into profits. While they are touting AI products that can generate business memos or computer code, they are still figuring out how to monetize these offerings. As AI development continues to be costly, companies are facing the challenge of creating products that customers are willing to pay for. The journey to translating AI advancements into profitable ventures is proving to be a difficult one.

The top comments on this submission revolve around observations and opinions about Microsoft and Github Copilot, as well as the cost and capabilities of AI technology.

- Some users comment on the limitations of Github Copilot, stating that it generates incomplete functions and often fails to properly handle protected code. They also mention the lack of support for Python in the chat interface and question its value for the price.
- Others share positive experiences with Copilot, highlighting its usefulness for generating code and its potential for changing the game when it comes to DRY (Don't Repeat Yourself) programming.
- One user presents a study comparing the typing speed of typists and their perception of the value of Copilot's code generation function in an IDE editor. They speculate that slower typists may find more value in it.
- There is a link shared to an archive of an article on PH, but the content of the link is not specified.
- A commenter posits that as AI models progress and optimization techniques continue to reduce hardware requirements, the cost of AI services will decrease over time.
- The discussion mentions the use of smaller, specialized models versus larger corporate models, suggesting that the former may have better performance in certain cases.
- Some commenters discuss the pricing of Copilot, mentioning that it is worth the $100/month for professionals, while others argue that the standard completion version, rather than the chat version, is what matters.
- The cost of GPU infrastructure is also brought up, with one user mentioning the high price of A100 GPUs and the difficulty of finding affordable alternatives with sufficient memory.
- Lastly, there is a user who humorously comments "dd," possibly indicating they have nothing substantive to contribute to the discussion.

Overall, the discussion seems centered around the capabilities, limitations, and pricing of AI technology like Github Copilot, as well as the potential impact it may have on coding practices and the market.