import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Tue Aug 22 2023 {{ 'date': '2023-08-22T17:10:13.633Z' }}

### GPT-3.5 Turbo fine-tuning and API updates

#### [Submission URL](https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates) | 377 points | by [davidbarker](https://news.ycombinator.com/user?id=davidbarker) | [224 comments](https://news.ycombinator.com/item?id=37227139)

OpenAI has announced the availability of fine-tuning for GPT-3.5 Turbo, allowing developers to customize the model for their specific use cases. This update gives developers the ability to create unique and differentiated experiences for their users. Early tests have shown that a fine-tuned version of GPT-3.5 Turbo can even outperform base GPT-4 on certain narrow tasks. Fine-tuning enables businesses to improve the model's performance in areas such as steerability, reliable output formatting, and custom tone. It also allows businesses to shorten prompts and handle larger amounts of data. Fine-tuning is most effective when combined with prompt engineering, information retrieval, and function calling. OpenAI will also be launching a fine-tuning UI in the near future. It's worth noting that all data sent in and out of the fine-tuning API is owned by the customer and not used by OpenAI or any other organization to train other models.

The discussion about the OpenAI fine-tuning announcement on Hacker News covers various topics related to the use and implications of fine-tuning models like GPT-3.5 Turbo and LLM (Large Language Models). Here are the key points from the discussion:

- Fine-tuning helps modify models' behavior to produce more specific outputs based on desired use cases.
- Fine-tuning allows customization of models for tasks like question answering, generating responses in a specific style, or handling large private knowledge bases.
- Users debate the use of fine-tuning versus other techniques like prompt engineering and information retrieval.
- The distinction between fine-tuning GPT-3.5 Turbo and LLM models is discussed. LLM models are based on reinforcement learning and human feedback.
- Privacy concerns emerge when dealing with large private knowledge bases and confidential data. OpenAI clarifies that the data used in the fine-tuning process is owned by the customer and not accessed by OpenAI or other organizations.
- The discussion touches on the cost and practicality of training models like LLM and LLama2, with some users mentioning the need for GPU rental and high training expenses.
- Inquiries are made about the safety and moderation aspects of fine-tuning models. OpenAI's moderation system is mentioned, and concerns regarding the potential dangers of tweaking models toward harmful content are raised.
- The availability and applicability of non-safe-for-work (NSF) models like dvnc-002 and bbbg-002 are discussed.
- Users share tips and code snippets for running inference with different models and addressing specific tasks.

Overall, the discussion reflects both excitement about the possibilities of fine-tuning models and concerns about potential risks and ethical considerations associated with their usage.

### SeamlessM4T, a Multimodal AI Model for Speech and Text Translation

#### [Submission URL](https://about.fb.com/news/2023/08/seamlessm4t-ai-translation-model/) | 160 points | by [mchiang](https://news.ycombinator.com/user?id=mchiang) | [35 comments](https://news.ycombinator.com/item?id=37222822)

Facebook has introduced SeamlessM4T, an all-in-one multilingual and multimodal AI translation model. The model can perform various translation tasks, including speech-to-text, speech-to-speech, text-to-text, and text-to-speech translations for up to 100 languages. Facebook is publicly releasing SeamlessM4T under a research license, allowing researchers and developers to build on the work. The company is also releasing the metadata of SeamlessAlign, an open multimodal translation dataset containing 270,000 hours of mined speech and text alignments. Facebook's goal is to build a universal language translator to facilitate effortless communication across different languages.

The discussion on this submission revolves around several different topics. 

One commenter noted that they had some difficulty installing the required dependencies and that the current code supports relatively short clips. Another commenter provided a small Python script to help with batch processing the results.

There is also a discussion about alternative models and approaches. Some users mentioned Hugging Face Space and suggested trying out different models, while others discussed building their own models for local use.

A few commenters expressed disappointment with the licensing terms, with one person mentioning that the non-commercial license limits adoption. Others discussed the importance of licensing and open access in the AI research community.

One commenter mentioned that the model's speech recognition accuracy was lower compared to WhisperCPP, and another expressed interest in compressing the model using OpenAI's Whisper.

There was also a comment about the lack of output for Tamil language models, and another commenter expressed frustration with non-commercial licenses.

The discussion touched on various topics such as AI research environment, GPL licenses, copying of models, and the limitations and opportunities presented by different license types.

Overall, the discussion covered technical issues, comparisons to other models, licensing concerns, and potential improvements.

### Google co-founder Sergey Brin on leaving retirement to work on AI

#### [Submission URL](https://www.theverge.com/2023/8/18/23837372/command-line-google-co-founder-sergey-brin-ai) | 59 points | by [moonraker](https://news.ycombinator.com/user?id=moonraker) | [25 comments](https://news.ycombinator.com/item?id=37226292)

Sergey Brin, the co-founder of Google, recently made a surprising return from retirement to work on generative AI. In a recent Q&A session, Brin explained his decision and the challenges he faces in the ever-evolving field of technology and AI. During the event, Brin expressed humility, joking that it's difficult for him to compete with the brilliant minds in the room. The audience eagerly awaited the arrival of the event's surprise speaker as Brin bought some time. Brin's return to Google has sparked curiosity and speculation about the exciting projects he might be working on.

The discussion on this submission covers a range of topics related to Sergey Brin's return to Google and the challenges faced by the company:

1. Some users discuss Google's management and culture, with one user mentioning the influence of former CEO Eric Schmidt and wondering about the changes made since his departure. Others mention the various CEOs that have led Google over the years and speculate on the impact of these leadership changes.

2. A user shares a link that doesn't seem to provide much value to the conversation.

3. Some users comment on the surprise speaker at the event where Brin spoke, which turned out to be Grimes. The conversation briefly touches on Grimes' enjoyment of the event and the influence of many people on the AI world.

4. One user mentions the significant organizational and cultural changes that Google has undergone, with rapid shifts in the company's structure and a departure from its early days.

5. A discussion about decision-making at Google arises, with one user mentioning that Larry Page and Sergey Brin used to answer politically sensitive questions during weekly meetings, but it's unclear if this still happens under the leadership of CEO Sundar Pichai. Another user suggests that Google's interests may not fully align with those of its employees, leading to conflicts.

6. A user points out that publicly-traded companies often have to prioritize the interests of their shareholders, which can hinder their ability to make certain decisions.

7. A link to an article is shared but is behind a paywall. Some users express gratitude for not realizing it was from The Verge, implying a negative sentiment towards the publication.

8. A user uses a metaphor of driving in city lights to explain the challenges faced by large companies and how middle management can hinder agility.

9. A user mentions finding a niche research paper related to AI and compares it to the thousands of publications released publicly in 2018, suggesting that Google has exclusive access to certain research.

10. There is a comment about the submission being paywalled, preventing some users from fully engaging with the article.

Overall, the discussion is a mix of speculation about Google's internal operations, some tangents, and frustration with paywalled content.

### Consciousness in AI: Insights from the Science of Consciousness

#### [Submission URL](https://arxiv.org/abs/2308.08708) | 50 points | by [rbanffy](https://news.ycombinator.com/user?id=rbanffy) | [118 comments](https://news.ycombinator.com/item?id=37220744)

A new paper published on arXiv explores the question of whether AI systems can possess consciousness. The authors, Patrick Butlin and 18 other scientists, argue for a rigorous and empirically grounded approach to assessing the consciousness of AI systems. By examining existing AI systems in light of neuroscientific theories of consciousness, they identify "indicator properties" of consciousness that can be applied to AI systems.

The authors survey several scientific theories of consciousness, including recurrent processing theory, global workspace theory, higher-order theories, predictive processing, and attention schema theory. They then use these theories to derive computational terms that can be used as indicators of consciousness in AI systems. 

The analysis of several recent AI systems using these indicators suggests that none of the current AI systems are conscious. However, the authors point out that there are no technical barriers to building AI systems that satisfy these indicators. 

Overall, this paper sheds light on the scientific understanding of consciousness as it relates to AI systems and highlights the need for further research in this area. It also addresses the increasing public concern regarding the consciousness of AI systems.

The discussion on this submission covers various topics related to consciousness in AI. Some users express skepticism about the idea of AI possessing consciousness, arguing that it would require more than just replication of human-like behavior. Others point out the significance of self-awareness and subjective experience in defining consciousness. The debate also touches on the ethical implications of granting rights to conscious AI systems, with some arguing for the extension of rights to AI and others expressing concerns about the potential dangers associated with it. Some comments highlight the need to distinguish between the concepts of consciousness and intelligence and caution against anthropomorphizing AI. The discussion also touches on the limitations of current AI systems and the complexity of defining and understanding consciousness. Overall, the discussion highlights different perspectives on the topic and raises important questions about the nature of consciousness in AI.

### ElevenLabs' AI Voice Generator Can Now Fake Your Voice in 30 Languages

#### [Submission URL](https://gizmodo.com/ai-voice-generator-elevenlabs-fake-voices-30-languages-1850762057) | 32 points | by [ourmandave](https://news.ycombinator.com/user?id=ourmandave) | [6 comments](https://news.ycombinator.com/item?id=37229450)

ElevenLabs, a company known for its visual deepfake technology, has now expanded into voice cloning. The company announced that its new voice cloning feature now supports 22 more languages, bringing the total to 30 languages. Users can input fragments of their own or others' speech to create a voice clone that can speak in different languages. The service is live on ElevenLabs' website, and users can simply type the text in the desired language to hear the translated voice. The company, which has faced controversy in the past, claims to have implemented measures to ensure users can only clone their own voice. ElevenLabs is also targeting media companies, promoting its voice cloning technology as a way to create audiobooks, videos, and voice NPCs in video games. The company has already struck a deal with Paradox Interactive, a game publisher.

The discussion on this submission seems to be focused on the legitimacy and implications of ElevenLabs' voice cloning technology. One user, ChatGTP, initially expresses excitement about the expansion of the service to support 30 languages. Another user, nwfrnd, responds with skepticism, calling it "fake reality" and pointing out that it could potentially be used for fraudulent purposes.  

A user named mptst replies jokingly, saying that they hope the technology can translate their voice into other languages instantly. They also mention that they don't personally care about voice cloning, but rather prefer using translators to communicate in different languages. 

Overall, the discussion highlights mixed opinions about ElevenLabs' voice cloning technology, with some expressing excitement and others raising concerns about its potential misuse.

### Prompting, realized, and unrealized bias in generative AI

#### [Submission URL](http://marble.onl/posts/code_of_practice_and_bias.html) | 13 points | by [andy99](https://news.ycombinator.com/user?id=andy99) | [9 comments](https://news.ycombinator.com/item?id=37220885)

In a recent article, Andrew Marble discusses the topic of bias in generative AI and explores a newly introduced code of practice for generative AI models. Marble highlights that while addressing bias is important, it is crucial to differentiate between dataset bias and biased system performance. With bigger and smarter models, the focus should shift from data bias to configuring the system properly to ensure unbiased performance. Marble also reflects on a voluntary "code of practice" for generative AI published by Industry Canada, which includes points like identifying malicious and inappropriate use, curating datasets, mitigating biased output, and providing clear identification of AI systems. Marble critiques the requirement to watermark AI-generated content, deeming it unnecessary and easily bypassed. However, Marble agrees with the idea of labeling AI systems to ensure transparency and accountability. Marble then delves into the discussion of bias, emphasizing the need for datasets that are both appropriate and representative. While acknowledging the importance of training data and bias, Marble suggests exercising judgment based on the specific application. In some cases, bias may not be a significant concern. Overall, the article provides insights into bias in generative AI and offers a critical perspective on the proposed code of practice.

The discussion on this submission seems to be focused on the topic of bias in generative AI. One commenter, "jxf," points out that their comment is unrelated and mentions an unnamed top-level domain. Another commenter, "zrthstrl," expresses their perspective on bias from a functional standpoint, stating that generating output based on knowledge from biased inputs wastes energy. They argue that the discussion should be focused on copying the function of physical systems instead of discussing biased generative AI. The commenter implies that bias is not necessarily a significant concern in generative AI models.

In response to "zrthstrl," "giraffe_lady" disagrees and argues that bias does exist in existing generative models and that addressing it is important. They mention the impact of bias on society and highlight the need to recognize and correct biased outputs. However, their comment is flagged, and another commenter, "dng," acknowledges that personal attacks are against the guidelines and asks "giraffe_lady" to review them.

In further discussion, "giraffe_lady" mentions a history of comments and interactions, claiming that they have been banned but do not believe they have violated the rules. They express frustration and request clarification from the community. "dng" responds by asserting that the commenter has repeatedly violated the guidelines and shares links to previous instances. Another commenter, "JieJie," mentions that the generative models can reproduce morally questionable content and suggests viewing the guidelines for more understanding.

The discussion in this thread seems to have deviated from the topic of bias in generative AI and instead focuses on personal interactions and rule violations.

### Show HN: Convert Research Papers into Dynamic Mind Maps with Claude

#### [Submission URL](https://github.com/nhaouari/papersnap) | 10 points | by [haouarin](https://news.ycombinator.com/user?id=haouarin) | [4 comments](https://news.ycombinator.com/item?id=37220069)

Papersnap is a tool that aims to help researchers extract key information from research papers and organize it into a mind map. The tool utilizes a powerful language model called Claude, which can handle documents with up to 100,000 tokens. To use Papersnap, users need to create an account on Claude and upload the research paper they want to extract information from. They then set up the Papersnap prompt within Claude, which is optimized to guide Claude in extracting key information effectively. After providing the necessary input, Claude processes the paper and generates a mind map containing the most important information in markdown format. Papersnap offers several benefits, including time-saving, comprehensive overviews of research papers, and simplification of complex concepts into easy-to-understand visual representations. The tool aims to enhance research paper analysis and streamline the research process. Users can explore the Papersnap repository, follow the outlined steps, and discover how the tool can benefit their research.

The discussion on Hacker News mainly revolves around the intriguing examples of using Papersnap. One user finds the examples provided in the article interesting and wants to explore them further. Another user points out that Papersnap is only available in the USA, UK, and some parts of Europe, which prompts a response from another user mentioning that Claude can counteract VPN signal processing.

---

## AI Submissions for Mon Aug 21 2023 {{ 'date': '2023-08-21T17:10:14.899Z' }}

### Pixel Binary Transparency: verifiable security for Pixel devices

#### [Submission URL](https://security.googleblog.com/2023/08/pixel-binary-transparency-verifiable.html) | 199 points | by [transpute](https://news.ycombinator.com/user?id=transpute) | [112 comments](https://news.ycombinator.com/item?id=37214733)

Google has released a blog post highlighting its latest security feature for Pixel devices called "Pixel Binary Transparency." This feature aims to provide verifiable security for Pixel devices by allowing users to examine and verify the software running on their devices. With this transparency, users can ensure that their devices are running genuine and untampered software. Google's commitment to security and safety on the internet is evident in this new development.

The discussion on the submission revolves around several key points. 

One user points out that Google's commitment to security and safety on the internet is evident in this new feature. However, another user raises concerns about the presence of analytics spyware in the firmware stack and questions whether Google can block such activities. The discussion then shifts to the topic of radio firmware and the control it has over the phone.

Another user who has experience working on Android Security for Pixel phones explains how Pixel phones use verified boot and other security measures to ensure the integrity of the software running on the devices. They also mention the use of the KeyStore API to further enhance security.

A user highlights the importance of binary transparency and how it can address certain threat models. They provide examples of potential attacks where a malicious actor modifies the firmware image and discusses the effectiveness of Google's approach.

The discussion also touches on issues like anti-rollback technology, backdoors in firmware, and the lessons learned from the SolarWinds hack. Some users express skepticism and raise concerns about trusting Google and the potential misuse of the transparency system.

One user points out that GrapheneOS, an alternative Android operating system, provides additional protections against attacks that the official Google Pixel firmware does not have. The discussion also touches on the possibility of custom firmware installation and the long-term support for devices.

There is also a mention of Trillian, a distributed ledger system, and its potential applications in computer security. The discussion concludes with concerns about installing custom firmware and the risks associated with companies disappearing and leaving unsupported devices.

Overall, the discussion covers a wide range of topics related to the security and transparency of firmware on Pixel devices, with varying opinions and concerns expressed by users.

### FreeBSD Experimenting with a Port of Nvidia's Linux Open DRM Kernel Driver

#### [Submission URL](https://www.phoronix.com/news/FreeBSD-Port-Linux-DRM-KO) | 123 points | by [mikece](https://news.ycombinator.com/user?id=mikece) | [36 comments](https://news.ycombinator.com/item?id=37210103)

FreeBSD developers are experimenting with a port of NVIDIA's Linux Open DRM Kernel Driver. This port, called nvidia-drm-kmod, allows FreeBSD users to use the open-source NVIDIA kernel driver for better integration with the kernel. While NVIDIA graphics on FreeBSD have been excellent thanks to the quality Linux driver stack, this new port could further enhance the experience, particularly in terms of Wayland support. The port is still in its early stages, but it has the potential to improve graphics support on FreeBSD systems.

The discussion about the submission started with a comment sharing a link to further information about the new port of NVIDIA's Linux Open DRM Kernel Driver for FreeBSD. Another user responded with a sarcastic comment about the DRM acronym, bringing up its association with Digital Rights Management and its negative perception. This led to a discussion about the different meanings of DRM, with users providing examples and clarifications.

Another user pointed out that the integration of NVIDIA graphics on FreeBSD has been good so far, but they have had trouble with AMD graphics. They praised the documentation and integration of NVIDIA graphics on FreeBSD and highlighted the stability of the system.

Some users mentioned specific technical problems with the FreeBSD NVIDIA driver, including issues with the Kernel Mode Setting (KMS) and visible screen tearing during video playback. One user mentioned that NVIDIA provides a native FreeBSD driver, but it lacks certain features.

There was a brief discussion about CUDA support on FreeBSD, with one user stating that they have used CUDA on FreeBSD in the past and another user asking about the compatibility of RTX cards on FreeBSD.

Overall, the discussion revolved around the new port of NVIDIA's Linux Open DRM Kernel Driver for FreeBSD and the current state of graphics support on the operating system. Users shared their experiences, highlighted the strengths and weaknesses of the existing drivers, and discussed the potential impact of the new port.

### Associated Press clarifies standards around generative AI

#### [Submission URL](https://www.niemanlab.org/2023/08/not-a-replacement-of-journalists-in-any-way-ap-clarifies-standards-around-generative-ai/) | 81 points | by [jyunwai](https://news.ycombinator.com/user?id=jyunwai) | [32 comments](https://news.ycombinator.com/item?id=37215829)

The Associated Press (AP) has released new guidelines to caution journalists about the use of AI in news coverage. While the AP has used AI technology to automate certain tasks since 2014, the guidelines emphasize that AI should not replace journalists and that AI-generated content should be treated as unvetted source material. The guidelines also state that generative AI should not be used to alter any elements of photos, video, or audio, and that journalists should exercise caution when using AI-generated content to avoid the spread of mis- and disinformation. The AP Stylebook also includes updates that warn journalists about far-fetched claims made by AI developers and advise against attributing human characteristics to AI systems.

The discussion on this submission covers a range of topics related to the use of AI in news coverage and the guidelines set by the Associated Press (AP). 

One commenter, RheingoldRiver, expresses interest in the guidelines' point about avoiding gendered pronouns for LLMs (large language models) and suggests that it's important to question decision-making processes and avoid biases. In response, mchlt points out that journalists often prioritize the preferences of companies and governments, which can lead to biased reporting.

Another commenter, LispSporks22, raises the question of why OpenAI needs permission to train its models on AP news stories dating back to 1985. Nl responds by suggesting that the AP News Archive is not publicly available and that there are reasonable arguments for crawling public pages for learning purposes.

The discussion also delves into the topic of AI-generated images and the legal implications. Some commenters, like fshbbdssbbgdd, discuss the potential copyright issues with training models using copyrighted Shutterstock images, while others, like mdfplk, argue that it's not a problem as long as there are no trademarks.

Other topics brought up include the potential biases and limitations of AI, the need to acknowledge the problems with the current copyright system and compensate content creators, the impact of AI on public perception, and the distinction between artificial intelligence and tools like ChatGPT.

Overall, the discussion covers a wide range of perspectives on the use of AI in news coverage, copyright law, biases, and the challenges faced by content creators and AI developers.

### Early Days of AI

#### [Submission URL](https://blog.eladgil.com/p/early-days-of-ai) | 142 points | by [todsacerdoti](https://news.ycombinator.com/user?id=todsacerdoti) | [50 comments](https://news.ycombinator.com/item?id=37213107)

Elad Gil, a former Google and Twitter engineer, has written a blog post discussing the early days of AI and its potential as a new era of technology. Gil highlights that prior to the rise of new AI architectures like transformers and diffusion models, most machine learning startups failed because the capabilities were not advanced enough. However, with the launch of GPT-3 in June 2020, there was a significant step up in AI capabilities. This was followed by the launch of image-gen products and ChatGPT, which captured the public's imagination and marked the AI startup big bang moment. Despite this progress, true enterprise adoption of AI is still several quarters or years away, as large companies are still trying to understand what AI means for them. Gil predicts that there will be at least four waves of AI adoption, with the first wave consisting of companies like ChatGPT and Character.AI, and the fourth wave being the adoption of AI by large enterprises. Overall, Gil believes that the future of AI is bright and that there is enormous potential for this new era of technology.

The discussion on this submission revolved around various aspects of AI and its potential. Some users expressed skepticism about the performance and significance of certain AI models, such as MedPaLM2, highlighting the cherry-picked nature of the claims made in the blog post. Others discussed the historical context of AI, with a mention of expert systems from the 1980s and the potential limitations of current AI models. There was also a mention of the "AI effect" and the challenges in defining AI. Additionally, the accessibility of machine learning and the availability of APIs were mentioned as factors that make ML solutions more feasible for businesses.

### Stablevideo: Text-driven consistency-aware diffusion video editing

#### [Submission URL](https://rese1f.github.io/StableVideo/) | 209 points | by [satvikpendem](https://news.ycombinator.com/user?id=satvikpendem) | [42 comments](https://news.ycombinator.com/item?id=37204950)

Researchers from Zhejiang University and Microsoft Research Asia have made advancements in the field of video editing with their new method called StableVideo. While diffusion-based methods have been successful in generating realistic images and videos, editing existing objects in a video while maintaining consistency has remained challenging. StableVideo introduces temporal dependency to existing text-driven diffusion models, enabling them to generate consistent appearances for edited objects. The method utilizes a novel inter-frame propagation mechanism that propagates the appearance information from one frame to the next using layered representations. The researchers conducted extensive experiments to test the editing capability of StableVideo and found that it outperformed state-of-the-art video editing methods in terms of both qualitative and quantitative results.

In the discussion on Hacker News, there were several different perspectives and thoughts shared about the StableVideo method for video editing:

- Some users appreciated the advancements in video editing and found the results impressive. They mentioned that it's interesting to watch the edited videos, even though there are still some issues, such as weird lighting and low-quality textures.
- Others pointed out that the stability of the video models is important, and progress in this area is necessary for further advancements.
- Some comments discussed the progression of AI technology and how it has developed over the years. It was noted that while progress is being made, there are still limitations and challenges to overcome.
- A few comments highlighted the potential impact of AI development on hardware and software expression, emphasizing the importance of maintaining fundamental constraints.
- There was some discussion about the connections between artificial neural networks and the human brain, with some users suggesting that silicon-based neural networks could eventually achieve similar capabilities.
- One user mentioned that the discussion was unrelated to the original paper and criticized the lack of a proper GitHub page with a template for academic papers.
- There were also comments about video compression and the potential integration of AI models into video encoding processes, both in terms of training data and improving compression efficiency.
- Some users expressed their excitement about the new method and were looking forward to future advancements in the field.
- Others expressed skepticism and labeled the news as "fake" or "weird."
- There were also discussions about the availability of example videos and the functionality of the StableVideo method in different browsers.
- A user shared their experience with similar video stabilization solutions and mentioned other commercial applications that have been developed.

Overall, the discussion covered various aspects of video editing, AI technology, hardware constraints, and skepticism about the new method.

### Show HN: VisionScript, abstract programming language for computer vision

#### [Submission URL](https://github.com/capjamesg/visionscript) | 89 points | by [zerojames](https://news.ycombinator.com/user?id=zerojames) | [21 comments](https://news.ycombinator.com/item?id=37213729)

Visionscript: A High-Level Programming Language for Computer Vision

Capjamesg, a developer, has created a high-level programming language called Visionscript for computer vision tasks. Visionscript is built in Python and offers a simple syntax for running object detection, classification, and segmentation models. With Visionscript, users can easily perform common computer vision tasks in a fast and efficient manner. The language provides features like object detection, image classification, and image replacement. It also offers an interactive web notebook for running Visionscript code. The inspiration behind Visionscript was to create a simple way of performing one-off computer vision tasks. The language is ideal for beginners who want to explore computer vision concepts in a user-friendly manner. Visionscript is available on GitHub under the MIT license.

There are several comments discussing the functionality and potential of Visionscript:

- ulrikhansen54 expresses interest in the language as a modern alternative to OpenCV, specifically highlighting the need for replacements for good object tracking functions.
- symisc_devel shamelessly plugs a lightweight OpenCV alternative called SOD, targeting embedded devices and implementing modern image processing algorithms.
- rcc mentions a popular library, SPRVSN, for converting notations and validating models for object tracking and supervision.
- zrjms responds to rcc's comment, mentioning that VisionScript can handle scenes versus things, such as counting objects within a zone.
- tgv notes that VisionScript reminds them of Hypercard's scripting language and suggests a similar approach for dealing with multiple objects and labels.
- zrjms thanks tgv for their comment and mentions that they have been working on iterating on the language, giving an example script for detecting people in grayscale images.
- kn appreciates zrjms' contribution and suggests learning Progressions language for interpreting VisionScript, as it provides a graphical environment for dragging and dropping components.
- CyberDildonics comments that VisionScript goes beyond simple OpenCV terms and provides models for classification, object detection, and segmentation.
- chptrck suggests replacing global state with objects in VisionScript to improve compatibility with machine learning and computer vision ecosystems.
- jnlsncm wonders if there are extensions similar to Scratch that add functionality to VisionScript.
- rcc expresses excitement about the ability to create custom detection blocks with VisionScript.
- ano88888 tries out VisionScript and expresses appreciation for its work.

Overall, the discussion seems to be positive, with users expressing interest in the capabilities and potential of Visionscript. Some users suggest alternative libraries and extension possibilities for the language.

### Lidar on a Chip Puts Self-Driving Cars in the Fast Lane

#### [Submission URL](https://spectrum.ieee.org/lidar-on-a-chip) | 34 points | by [jnord](https://news.ycombinator.com/user?id=jnord) | [21 comments](https://news.ycombinator.com/item?id=37204724)

Self-driving cars have long been considered the future of transportation, with the potential to save lives and revolutionize the way we travel. However, one of the key challenges to widespread adoption has been the high cost and complexity of lidar sensors, which are crucial for creating detailed 3D maps of a vehicle's environment. Elon Musk, CEO of Tesla, has controversially advocated for a cameras-only approach to autonomous driving, arguing that cameras and neural networks are sufficient. However, many traffic-safety specialists have questioned this approach and believe that lidar is necessary for safe operation. In an effort to address the cost and integration challenges, Analog Photonics, a company spun out of MIT, is developing a chip-scale phased-array lidar sensor that promises to be tiny, reliable, and affordable. The company hopes that this breakthrough technology will pave the way for the widespread adoption of self-driving cars.

- Commenter "NoZebra120vClip" mentions that a company called Bubba Gumps is selling LIDAR stock in New York City.
- Commenter "whnvk" discusses the application of self-driving cars technology in body scanning and building 3D maps, suggesting that companies like Amazon, Meta, and Google might be interested in using it for body inspections.
- Commenter "tsyb" responds to "whnvk" suggesting that data collected from VR headsets could be used for detailed measurements, but the level of anonymity and granularity might be a concern.
- Commenter "_boffin_" mentions an article they haven't read but believes that self-exploring drones can be built without using LIDAR.
- Commenter "gnmd" mentions that there are low-cost integrated packages for ToF sensors available from talkitng ST.
- Commenter "scrtstn" mentions that Apple iPhones have a LiDAR chip.
- Commenter "ttr" believes that cheap, effective, and reliable LiDAR sensors for self-driving cars are a good thing, but questions whether self-stopping self-driving cars are a step in the right direction.
- Commenter "gnsh" mentions that they work with a million chips and are an EE, pointing out the issues of interference in the travel of LiDAR signals.
- Commenter "mkhlfrnc" is interested in the capability and price point of low-cost LiDAR sensors and believes they could affect industries beyond self-driving cars.
- Commenter "Havoc" suggests that Tesla might switch to using LiDAR due to advancements in the technology.
- Commenter "whmsclsm" mentions that they are working on a prototype LiDAR chip.
- Commenter "gtrfltr" reaches the end of the summary and comments, "dd" which is unclear in meaning.

### Wi-Fi sniffers strapped to drones: odd plan to stop election fraud

#### [Submission URL](https://arstechnica.com/tech-policy/2023/08/wi-fi-sniffers-strapped-to-drones-mike-lindells-odd-plan-to-stop-election-fraud/) | 35 points | by [sunbum](https://news.ycombinator.com/user?id=sunbum) | [122 comments](https://news.ycombinator.com/item?id=37208751)

Mike Lindell, the CEO of My Pillow and a vocal supporter of former President Donald Trump, is claiming he has developed a technology that can detect if voting machines are connected to the internet. Lindell demonstrated the technology at an event in Missouri using a wireless sniffing device mounted on a drone. However, experts have pointed out that there doesn't appear to be any major advance in network monitoring technology here. Lindell's plan to fly drones near polling places may also violate state laws on criminal trespassing and the use of unmanned aircraft for surveillance. Despite the skepticism surrounding his claims, Lindell said he has already used the device in Florida and plans to cover every parish in Louisiana for the upcoming fall election.

The discussion on the submission about Mike Lindell's voting machine technology revolves around skepticism and criticism of his claims. One user points out that there is already network access point control in place at polling locations to prevent any unauthorized access. Others mention that the main issue with voting machines is not their connectivity to the internet but rather the use of proprietary and unreliable systems. There is also discussion about the potential violation of state laws regarding using drones near polling places. Another topic that arises is voter ID laws and the need for ID verification. Some users argue for voter ID laws while others express concerns about disenfranchising certain groups of voters. The conversation also touches on tax refunds and the requirement of non-citizens voting. Overall, there is skepticism towards Lindell's claims and discussions on various related topics.

### Nvidia BIOS Signature Lock Broken â€“ What Caused Open-Source Pains for Years

#### [Submission URL](https://www.phoronix.com/news/NVIDIA-Lock-Broken) | 23 points | by [segfaultbuserr](https://news.ycombinator.com/user?id=segfaultbuserr) | [5 comments](https://news.ycombinator.com/item?id=37216269)

In a major breakthrough for open-source enthusiasts, a Windows utility has been released that breaks the NVIDIA BIOS Signature Lock, a security feature implemented by NVIDIA since the GeForce GTX 900 days. This signature check has been a headache for the open-source Nouveau driver community, as it has limited the functionality of the GTX 900 series and newer GPUs. With the lock now broken, users will have more control over their graphics card's settings, including power limits, voltages, and fan curves. While the impact on the Nouveau developers is yet to be seen, this development certainly highlights the vulnerability of artificial software locks.

The discussion around the submission consists of several comments discussing different aspects of the NVIDIA BIOS Signature Lock and its implications for the open-source community. 

One user, SenAnder, points out that NVIDIA implemented the lock to prevent fraudulent individuals from flashing higher-end graphics card firmware onto lower-end products and selling them at a higher price. They question whether there are other ways to verify product authenticity without restricting user control.

RetroTechie responds to SenAnder's comment by explaining that there are hardware registers that can be used to identify supported features and non-modifiable firmware. They mention that if these hardware registers do not exist, it raises questions about firmware claiming support for hardware that is not actually present. RetroTechie suggests that this could be a problem created by shady sellers.

PlutoIsAPlanet adds to the discussion by highlighting that while NVIDIA locks their graphics cards, phones use bootloader messages to warn users of potential risks. They suggest that there may be other reasons for NVIDIA to lock their cards and express doubt about the prevention of fraud being the sole reason.

SenAnder replies to PlutoIsAPlanet's comment by noting that graphics cards do not have sufficient access to display warning screens like phones do.

In a separate comment, hdjfkfbfbr finds the discussion interesting and mentions that the NVIDIA BIOS Lock affects certain models of the 30-series graphics cards.

### Brave Browser has an AI assistant chat now

#### [Submission URL](https://brave.com/leo-release/) | 37 points | by [rejectfinite](https://news.ycombinator.com/user?id=rejectfinite) | [20 comments](https://news.ycombinator.com/item?id=37214735)

Brave browser has introduced Leo, its browser-native AI assistant, for testing and feedback in the Nightly desktop channel. Leo, built on the success of Brave Search AI Summarizer, is designed to allow users to interact with web pages without leaving the page itself. It can answer questions, suggest follow-up queries, augment content, and even help with reading comprehension. Leo is hosted by Brave without the use of third-party AI services, ensuring user privacy. Feedback from Nightly users will help improve Leo's accuracy and user experience, with plans to release it to all Brave browser users in the coming months.

The discussion on the submission revolves around Brave's new browser-native AI assistant, Leo. Some users express skepticism about trusting Brave considering its strong focus on privacy. Others point out that Brave is different from other companies jumping on the AI and NFT bandwagon. Some users mention that Brave has implemented an AI system to maintain user privacy, and they appreciate Brave's commitment to privacy-first browsing. There is a discussion about whether the AI prompts served by Leo are useful or a result of hype. Additionally, there is a brief mention of a previous issue with Brave's bookmark tabs button on Android and a comparison between Brave and Firefox. Some users express doubts about trusting Brave, while others argue that Brave is a privacy-focused company. The discussion also touches on the use of reverse-proxy access servers and logging individual IP addresses, with some users expressing concerns about privacy guarantees and others dismissing those concerns. Overall, opinions on Brave and Leo vary, with some users expressing trust and others expressing doubt.

---

## AI Submissions for Sun Aug 20 2023 {{ 'date': '2023-08-20T17:10:00.337Z' }}

### Transcoding Latin 1 strings to UTF-8 strings at 12 GB/s using AVX-512

#### [Submission URL](https://lemire.me/blog/2023/08/18/transcoding-latin-1-strings-to-utf-8-strings-at-12-gb-s-using-avx-512/) | 71 points | by [mariuz](https://news.ycombinator.com/user?id=mariuz) | [56 comments](https://news.ycombinator.com/item?id=37197921)

There is a blog post that discusses the transcoding of Latin 1 strings to UTF-8. In the post, the author explores a C code routine that converts a byte array representing Latin 1 characters to UTF-8. The routine checks if each byte represents an ASCII character or a non-ASCII character and generates the appropriate UTF-8 bytes accordingly. The author then poses the question of whether the process can be optimized using AVX-512 instructions available on modern Intel and AMD processors. They propose an alternative approach using AVX-512 instructions to streamline the transcoding process. The approach involves loading 32 bytes into a register, identifying non-ASCII bytes, casting bytes to 16-bit words, adding appropriate values to the words, flipping the byte order, and compressing the result. The author suggests that this AVX-512 approach may offer better performance than the original C code routine.

The discussion on Hacker News about the blog post on transcoding Latin 1 strings to UTF-8 covers several different aspects.

One user points out that many websites still use Latin-1 encoding, which is not fully compatible with UTF-8. They suggest that this may be one reason why browsers don't always handle UTF-8 correctly.

Another user argues that the default character set for HTTP is ISO-8859-1, but modern browsers usually support UTF-8. They mention that there is some compatibility between Latin-1 and ASCII, but UTF-8 is the recommended encoding.

A user comments that most frequently used characters fall within the range of 0-127, and optimizing the transcoding process for the range of 128-255 could significantly improve performance.

There is a discussion about memory bandwidth and the potential impact on performance. Some users believe that memory bandwidth limitations could be a bottleneck for performance, while others argue that modern CPUs can handle single-threaded operations efficiently.

Another user points out that Windows-1252 encoding includes additional characters beyond the regular Latin 1 mapping, which can lead to incorrect conversions.

There is a debate about whether optimizing the transcoding process using AVX-512 instructions is beneficial. Some users argue that AVX-512 implementation can result in a significant performance penalty, while others believe that it can lead to improved performance.

One user mentions the importance of considering the entire workflow and critical paths when optimizing a task. They suggest looking at metrics such as runtime algorithm improvements and memory usage.

The discussion also delves into the definition and usage of Latin-1 encoding, as well as the differences between Latin-1 and ASCII.

There is a humorous comment about the intricacies of translation and the nuances of different character sets.

Finally, there is a debate about Linus Torvalds' criticisms of AVX-512 and whether it provides significant benefits compared to other SIMD implementations. Some users argue that AVX-512 can have drawbacks and may not always lead to better performance.

### CCC Talk: All cops are broadcasting Obtaining the secret TETRA primitives [video]

#### [Submission URL](https://media.ccc.de/v/camp2023-57100-all_cops_are_broadcasting) | 97 points | by [rvdbreemen](https://news.ycombinator.com/user?id=rvdbreemen) | [15 comments](https://news.ycombinator.com/item?id=37201762)

In a recent talk at the Camp 2023 event, Jos Wetzels, Carlo Meijer, and Wouter Bokslag discussed their groundbreaking radio jailbreaking journey. They were able to perform the first public disclosure and security analysis of the proprietary cryptography used in TETRA (Terrestrial Trunked Radio). TETRA is a widely used European standard for trunked radio, being utilized by government agencies, police, prisons, emergency services, military operators, as well as in industrial and critical infrastructure settings. The underlying algorithms of TETRA had remained a secret for over two decades, with restrictive NDAs prohibiting public scrutiny. The team discussed how they managed to obtain the primitives and legally publish their findings, involving reverse-engineering and exploiting zero-day vulnerabilities in the Motorola MTM5x00 TETRA radio and its TI OMAP-L138 trusted execution environment (TEE). Their journey encompassed side-channel attacks on DSPs, writing decompilers for complex DSP architectures, and exploiting ROM vulnerabilities in the Texas Instruments TEE. The talk is available for download in video and audio formats.

The discussion on this submission covers a few different points. 

One user praises the work and suggests that it would be interesting to see further developments. Another user then jokingly comments about the possibility of photocopying the ID card shown in the talk, indicating their interest in the presentation.

Another user raises concerns about the legality of intercepting radio signals in the UK, comparing it to countries like China, North Korea, and Russia. This comment sparks a discussion about George Orwell's "1984" and how Nazi Germany and the Soviet Union may have served as inspiration for the dystopian novel.

Another user mentions that they've watched a talk where the speaker demonstrates the process of obtaining the secret TETRA primitives, bypassing encryption. This comment leads another user to mention that many radio standards and protocols, including encryption, are often closed and not easily accessible.

A user provides a link to an article about AMBE1, a digital voice codec used in some radio standards, and mentions that it is a closed-source and non-licensed implementation. This prompts another user to point out that many Chinese brands on AliExpress reverse-engineer the software implementation of AMBE1, claiming to have licenses from the Chinese company DVSI.

Another user highlights the need for a security-centric mentality when it comes to technology. One user mentions the introduction of additional cryptography in German police smartcards as an example of how standards evolve over time to address problems.

The discussion then shifts to the topic of 802.11 wireless standards.

Overall, the comments touch on various aspects of the presentation, including the legality of intercepting radio signals, the availability of closed-source encryption implementations, and the need for a strong security mindset in technology.

### Welcome to Datasette Cloud

#### [Submission URL](https://www.datasette.cloud/blog/2023/welcome/) | 305 points | by [swyx](https://news.ycombinator.com/user?id=swyx) | [68 comments](https://news.ycombinator.com/item?id=37196461)

Introducing Datasette Cloud: a SaaS hosting platform for the Datasette open source project. Aimed primarily at newsrooms, Datasette Cloud allows teams to create private collaboration spaces, securely upload and share data, and selectively publish that data to the public. The platform offers features such as secure container-based spaces, data import options, inline data editing, and access to tools like Datasette's web interface, JSON API, and GraphQL API. Datasette Cloud is stored securely in Fly volumes and backed up to S3 using Litestream. Future plans include AI-assisted queries, table and query publication, data annotations, and pricing options. Interested users can sign up for a demo or request preview access.

The discussion on Hacker News surrounding the submission about Datasette Cloud is mixed. Some users express their excitement about the success of Datasette and how it simplifies data sharing and analysis. They appreciate the ease of use and the potential it holds for newsrooms and journalists. There are also mentions of how Datasette can be a valuable tool for business intelligence and investigative reporting.

However, some users express skepticism and cynicism. They question the financial sustainability of the project in the long run and compare it to other successful open-source projects that have found viable business models. They also discuss the limitations of Datasette, such as its narrow niche and potential competition from other tools. Some users suggest that Datasette could benefit from better UX design to make it more accessible to non-technical users.

There is also a discussion about the comparison between Datasette and MS Access. Some users find Datasette to be a superior tool for lightweight database solutions, while others point out the unique features of MS Access that make it still relevant in certain scenarios.

Overall, the discussion highlights the potential and limitations of Datasette Cloud, with users expressing both enthusiasm and skepticism.

### The Ares Operating System

#### [Submission URL](https://ares-os.org/) | 21 points | by [todsacerdoti](https://news.ycombinator.com/user?id=todsacerdoti) | [4 comments](https://news.ycombinator.com/item?id=37200494)

The Ares Operating System is making waves in the tech world with its unique system design. The foundation of this new OS is the Helios microkernel, which serves as the base layer. On top of that, Ares features several additional layers that enhance its functionality.

First, there's Mercury, an environment specifically designed for device drivers. It ensures seamless communication and compatibility between different hardware components. Venus comes next, offering a collection of real-world drivers specifically built for Mercury. This combination of Mercury and Venus provides a powerful and versatile driver framework.

For developers, Ares offers Gaia, a userspace programming environment. With Gaia, developers can easily create applications and programs that interact with the underlying system. Additionally, there's Luna, which provides a POSIX programming environment, allowing developers to build applications compatible with POSIX standards.

At its core, Ares aims to be a high-level operating system, offering a package manager, desktop environment, and more. However, the current focus of development is primarily on Helios and Mercury.

If you're interested in contributing to the Ares project, you can find the source code for all Ares-related projects on SourceHut. The project also provides additional resources such as a helios-devel mailing list for discussions and a bug tracker for issue reporting. For real-time development discussions, the Ares community gathers in the #helios channel on irc.libera.chat.

While Ares is still in development and lacks end-user resources at the moment, its ambitious system design and open-source nature make it a promising contender in the world of operating systems. Keep an eye out for future updates on this fascinating project.

The discussion around the Ares Operating System submission on Hacker News primarily focuses on the Helios microkernel and its similarities to Minix Intel. One user, "hlt," requests for more status updates and screenshots of the operating system. "bdrbbt" shares a link that provides interesting insights into the development progress of the project.

Another user, "grgrv," expresses enthusiasm for the microkernel design of Ares, comparing it to Minix. In response, user "pjmlp" mentions Minix Intel, potentially suggesting a connection or similarity between Ares and Minix Intel.

### Big Tech, Concentrated Power, and the Political Economy of Open AI

#### [Submission URL](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4543807) | 19 points | by [skilled](https://news.ycombinator.com/user?id=skilled) | [4 comments](https://news.ycombinator.com/item?id=37198904)

In a recent paper titled "Open (For Business): Big Tech, Concentrated Power, and the Political Economy of Open AI," researchers from Carnegie Mellon University and AI Now Institute explore the concept of "open" AI and its implications. The paper highlights the various ways the term "open" is used in the context of AI systems, often as a marketing buzzword without a clear technical definition. The researchers examine the resources required to create and deploy AI systems and question which aspects can be made open for scrutiny, reuse, and extension. They find that while some AI systems offer transparency, reusability, and extensibility, the resources needed to build and deploy large AI systems remain closed, available only to those with significant corporate resources. The paper also delves into the history of open source software and its incorporation into large tech corporations. As an example, the researchers examine the company OpenAI and its shifting position on "open" AI. They discuss how the term and the misunderstandings around it are being used to shape public and policymakers' understanding of AI and its industry. The researchers note that while open AI can provide transparency and some forms of auditing and oversight, it does not guarantee democratic access or meaningful competition in AI. They also caution that marketing around openness and investment in open AI systems may be leveraged by powerful companies to bolster their positions and exploit the free labor of open-source contributors.

The discussion on this submission is quite technical and focuses on various aspects of AI. One user mentions that they find the article interesting and highlights the challenges in understanding the different paths AI systems can take. Another user points out that while there may be resources available for developing AI systems, the advantage lies with those who have significant corporate resources. A user also comments on the broad scope of AI systems and how OpenAI's approach may involve censoring certain language models. Another user simply expresses their admiration for language models. One user brings up the concept of pre-internet publishing cycles and how large amounts of information, including hype and heresy, can be managed in an efficient manner with AI. The conversation in the comments is quite fragmented and lacks a central discussion theme.

### Large Language Models As General Pattern Machines

#### [Submission URL](https://arxiv.org/abs/2307.04721) | 78 points | by [optimalsolver](https://news.ycombinator.com/user?id=optimalsolver) | [29 comments](https://news.ycombinator.com/item?id=37197734)

Researchers have found that large language models (LLMs) can be used as general pattern machines, capable of autoregressively completing complex token sequences. In their paper titled "Large Language Models as General Pattern Machines," Suvir Mirchandani and his team explore how these zero-shot capabilities can be applied to problems in robotics. The researchers demonstrate that LLMs can extrapolate sequences of numbers representing states over time to complete simple motions and prompt reward-conditioned trajectories that can discover closed-loop policies. Although there are some limitations in deploying LLMs for real systems, this approach provides insights into how language patterns can be transferred to actions.

In the discussion, there is a mix of perspectives regarding the capabilities and limitations of large language models (LLMs). Some commenters emphasize that LLMs are powerful pattern recognition machines capable of extrapolating complex token sequences and generating accurate outputs. They argue that LLMs have practical applications and can aid in decision-making. Others raise concerns about the credibility of the outputs generated by LLMs, particularly in real-world industries where trust and accuracy are crucial. They argue that relying solely on LLMs for important decisions can lead to deferred or misguided judgments. The topic of fact-checking and the importance of reliable information sources is also discussed, with some commenters noting that LLMs can assist in surfacing relevant information and help users fact-check. However, there are differing opinions on the extent to which LLMs can replace human expertise and whether they can achieve the same level of accuracy. Additionally, there is a discussion about the challenges in ensuring factual accuracy and the limitations of LLMs in understanding and verifying information. The conversation also touches on the potential applications of LLMs in legal document analysis, the importance of fact-checking in the real world, and the gradual improvement of LLMs over time. Finally, there is a mention of the concept of natural production, Turing machines, and the role of LLMs in solving general problems.

### Project Valhalla, Simple as it can be, but not simpler

#### [Submission URL](https://cr.openjdk.org/~jrose/values/larval-values.html) | 94 points | by [pmg1991](https://news.ycombinator.com/user?id=pmg1991) | [31 comments](https://news.ycombinator.com/item?id=37195023)

In a recent update to the Valhalla JVM prototype, the use of Q-types and v-bytecodes has been removed, resulting in a simpler and more efficient VM design. Q-types were a new type of VM type introduced in Valhalla to support Java value classes, while v-bytecodes were special bytecodes for operating on these Q-types. However, new optimization techniques have made it possible to express struct-like values and their operations using normal Java classes without sacrificing performance. By removing Q-types and v-bytecodes, the VM design has been simplified and allows existing Java classes to operate as if no changes have been made. This has significant benefits for migration and binary compatibility, as it reduces the need for clients of these classes to be recompiled. The goal is to minimize changes to the classfile format and use existing classfile encodings as much as possible, allowing code that previously worked with the class to continue working seamlessly after it has been migrated to a value class. Overall, this update represents a major step forward in the development of Valhalla and its support for Java value classes.

The discussion on the submission revolves around the changes in the Valhalla JVM prototype and the implications for the Java programming language. Some commenters express their interest in the new design and the potential performance improvements it brings. Others discuss the differences between value types in Java and C#, and the challenges faced in implementing generics in both languages. There is also a discussion about the compatibility of the new changes with existing code and the potential impact on the Java ecosystem. Overall, the discussion touches on various technical aspects and the differences between JVM and CLR implementations.

### San Franciscoâ€™s robotaxi experiment is getting out of hand

#### [Submission URL](https://www.vox.com/technology/2023/8/19/23837648/self-driving-taxis-gm-cruise-alphabet-waymo) | 17 points | by [dotcoma](https://news.ycombinator.com/user?id=dotcoma) | [14 comments](https://news.ycombinator.com/item?id=37202523)

San Francisco's experiment with self-driving taxis is quickly expanding across the country, with robotaxi companies like GM's Cruise and Alphabet's Waymo now offering commercial services in cities like Austin, Los Angeles, Miami, and New York City. The California Public Utilities Commission recently lifted restrictions on Cruise and Waymo, allowing them to operate in San Francisco at all hours and charge fares. However, the expansion of robotaxis has faced opposition, with concerns raised about the vehicles obstructing emergency responders and causing traffic issues. Despite these challenges, self-driving taxis are becoming increasingly common in San Francisco, with residents using them for everyday activities like going out with friends. The experience of riding in a robotaxi is similar to using Uber or Lyft, except there is no driver. Waymo's robotaxis, for example, are luxurious Jaguar vehicles. While tipping the robot driver is not necessary, pricing details for robotaxi rides vary depending on the company.

The discussion on this submission revolves around the topic of regulations and the impact of self-driving taxis in San Francisco.

- One user mentions that regulating self-driving cars requires hard work and expertise, with regulations written specifically for the industry.
- Another user argues against regulations, stating that repealing regulations can be beneficial in promoting innovation.
- There is a debate about the negative aspects of regulations and how they can lead to inefficient enforcement and non-profitable behavior.
- The conversation also touches upon the questionable practices of companies like Uber, which have threatened the traditional market.
- Some users discuss instances of robotaxis behaving in unexpected ways, such as sudden stops in the middle of the road or navigation errors.
- One user expresses frustration with the increasing theft and homelessness in San Francisco, making driving difficult and leading to potential catastrophes.

Overall, the discussion highlights the challenges and concerns surrounding the expansion of self-driving taxis and the need for appropriate regulations to ensure safety and efficient operation.

---

## AI Submissions for Sat Aug 19 2023 {{ 'date': '2023-08-19T17:10:07.852Z' }}

### Air Force funds â€˜blended wing bodyâ€™ design for long-range, fuel-efficient flight

#### [Submission URL](https://www.popsci.com/technology/air-force-blended-wing-body/) | 72 points | by [webmaven](https://news.ycombinator.com/user?id=webmaven) | [78 comments](https://news.ycombinator.com/item?id=37191671)

The United States Air Force has funded the development of a "Blended Wing Body" airplane prototype, which promises more efficient flight for long-range missions. The concept art for the prototype shows a gray plane with a body that starts from a conventional cockpit and expands into a large wing, with jets mounted at the rear. The design offers greater lift, reduces drag, and expands the lifting surface of the aircraft, making it more fuel-efficient. The hope is that this plane, known as the X-BWB-1, will be a valuable transport and tanker in the event of battles across the Pacific Ocean. The first test flight is expected in 2027.

The discussion surrounding the submission on Hacker News touched on a variety of topics related to the United States Air Force's funded prototype of the "Blended Wing Body" airplane. 
- Some users discussed the potential benefits of the design, such as improved flight efficiency and range.
- Others raised concerns about the practicality of the design for military purposes, citing factors like increased vulnerability to attack and limited capacity for payloads.
- The lack of window seats in the Blended Wing Body (BWB) aircraft was also mentioned, with some users speculating on potential seating arrangements and the potential for standing or inclined seating.
- The discussion also touched on the use of LCD screens to replace windows in modern aircraft and the challenges of window positioning during takeoff and landing.
- Some users mentioned the work of Jack Northrop and shared additional resources related to the Blended Wing Body concept.
- The viability of hydrogen-filled aircraft, the potential for digital windows, and the practicality of extreme maneuvers in the Blended Wing Body design were also discussed.
- The potential for using hydrogen as a fuel source for aircraft was debated, with some users highlighting the challenges and costs associated with the technology.
- The B-21 Raider, a classified long-range bomber, was brought up as a comparison to the Blended Wing Body prototype.
- The X-33 program, a previous project by Lockheed Martin, was mentioned as a similar concept from the past.

### The AI-First Code Editor

#### [Submission URL](https://www.cursor.so/) | 31 points | by [peter_retief](https://news.ycombinator.com/user?id=peter_retief) | [11 comments](https://news.ycombinator.com/item?id=37189904)

Cursor is a powerful code editor designed to help engineers build software faster. With features specifically tailored for pair-programming, Cursor has become trusted by tens of thousands of engineers worldwide.

One of Cursor's standout features is its ability to chat with your project. Instead of wasting time looking for code, you can ask Cursor questions related to your specific repository, getting answers that are tailored to your codebase. This saves valuable time and keeps you focused on writing code.

In addition, Cursor allows you to browse documentation directly within the editor. You can refer to code definitions, files, and documentation with ease, eliminating the need to constantly switch between different tabs or windows.

The AI-powered capabilities of Cursor truly shine when it comes to making code changes. Whether you need to edit existing code or generate code from scratch, Cursor can assist you with just a simple instruction. It can even help you spot and fix bugs by scanning your code for errors and providing insights into the root cause of the issue.

One-click migration is another key feature of Cursor. As a fork of VSCode, it allows you to import all your favorite extensions, themes, and keybindings in just one click. And if security is a concern, Cursor offers a local mode that ensures none of your data is stored on external servers or logs.

Developers all over the world have praised Cursor for its ability to enhance productivity. By leveraging AI in the code editing process, Cursor empowers engineers to work faster, collaborate seamlessly, and focus on what matters most.

If you're tired of the limitations of traditional code editors, give Cursor a try and experience the future of coding.

The discussion on the submission "Introducing Cursor: The AI-first Code Editor" on Hacker News includes several comments addressing different aspects of the code editor.

- User "qlkjwnf" criticizes the company behind Cursor, calling it a scam that has shamelessly duped thousands of developers worldwide.
- User "bzlwx" questions how the company can claim that Cursor is trusted by thousands of engineers. They express curiosity in knowing the metrics behind this claim.
- User "cpprx" believes that the post is deceptively referring to Cursor as a similar product to VSCode.
- User "stvkpndm" mentions that a similar project called "crsrsh" was recently submitted, but it was canceled. They suggest that it could be related to VSCode and GitHub Copilot Chat.
- User "mtprt" states that the advertised features are already available in VSCode's Copilot extension.
- User "circuit10" suggests that the reason for the fork could be to guess the extension's source and cost and deploy it as a separate extension.
- User "impulser_" mentions a similar application to VSCode + AI that Google released for free and clarifies that they did not create the extension for VSCode.
- User "PappGaborSandor" simply states that Nano works.
- User "flmhns" suggests implementing a local mode in Cursor to ensure that data is not transmitted to external servers. 
- User "lyrc" responds to this suggestion, saying that a local mode may not be enough if the editor itself is closed source.

This discussion highlights skepticism about the legitimacy and originality of Cursor, comparisons to existing tools like VSCode and Copilot, and suggestions for improvements such as local mode.

### Cloud outage causes Bambu 3D printers to start printing on their own

#### [Submission URL](https://themessenger.com/tech/bambu-owners-3d-printers-malfunction-cloud-print-twice) | 78 points | by [rcarmo](https://news.ycombinator.com/user?id=rcarmo) | [84 comments](https://news.ycombinator.com/item?id=37187138)

Bambu Lab's 3D printers caused a frenzy among users when they mysteriously started printing on their own in the middle of the night. The issue was caused by a cloud outage which resulted in print start messages accumulating and being sent to a number of printers. Bambu Lab has released a follow-up blog post, explaining the root cause and outlining software improvements to prevent autonomous printing in the future. The company plans to implement features such as checking for the presence of an object on the build plate using a LIDAR scanner and displaying reminders to clean the plate before starting a print. In addition, the printers will continuously monitor temperature levels and alert users when a fault is detected. Bambu Lab also expressed their commitment to assisting affected customers by providing spare parts and compensation for wasted filament.

### Cruise told by CA DMV to reduce robotaxi fleet 50% following crash

#### [Submission URL](https://techcrunch.com/2023/08/18/cruise-told-by-regulators-to-immediately-reduce-robotaxi-fleet-50-following-crash/) | 136 points | by [ra7](https://news.ycombinator.com/user?id=ra7) | [194 comments](https://news.ycombinator.com/item?id=37184904)

Cruise, the self-driving car subsidiary of GM, has been asked to reduce its robotaxi fleet in San Francisco by 50% following a crash involving a fire truck. The California Department of Motor Vehicles (DMV) has requested the reduction in operations and is investigating "recent concerning incidents" involving Cruise vehicles. The DMV has called for Cruise to have no more than 50 driverless vehicles in operation during the day and 150 at night until the investigation is complete. Cruise, which believes it positively impacts overall road safety, is complying with the DMV's request. This incident comes just a week after Cruise won approval from the California Public Utilities Commission to expand its commercial operations in San Francisco.

The discussion on this submission revolves around various aspects of self-driving cars and the recent incident involving Cruise's robotaxi fleet. 

Some commenters express skepticism about the capabilities of self-driving cars, highlighting the challenges they face in navigating complex situations and accurately detecting and responding to various stimuli. They mention that human drivers also make mistakes but argue that self-driving cars should not be exempt from scrutiny.

Another point of discussion is the comparison of accident rates between self-driving cars and human-driven vehicles. Some argue that self-driving cars have a disproportionately higher accident rate involving dark-skinned individuals and children. However, others question the validity of such claims and caution against jumping to conclusions based solely on numerical comparisons.

Criticism is also directed towards the metrics used to evaluate the safety of self-driving cars, with some commenters suggesting that current testing practices are reckless and potentially harmful to the industry's development. Others argue that software development practices can be reckless regardless of the technology involved and emphasize the importance of thorough testing and regulation.

The recent incident involving a collision between a Cruise vehicle and a fire truck is discussed, with commenters pointing out that the fire truck was in the opposing traffic lane and had its lights on. Some argue that human drivers can also struggle in such situations and caution against using isolated incidents to condemn the entire self-driving industry.

There is a debate about the capabilities and safety features of self-driving vehicles. Waymo's representatives claim that their vehicles can handle emergency vehicles effectively by detecting their presence and adjusting their behavior. Comparisons are drawn between Waymo and Cruise's technologies, with Waymo being seen as more advanced in this regard.

Lastly, there is some discussion about the skills and behaviors of human drivers, with some commenters suggesting that terrible drivers exist in both human and autonomous vehicles.

Overall, the discussion covers a range of perspectives on self-driving cars, their safety, and the recent incident involving Cruise's robotaxi fleet.

### Federal Judge Upholds Finding That AI-Created Art Isnâ€™t Copyrightable

#### [Submission URL](https://www.hollywoodreporter.com/business/business-news/ai-works-not-copyrightable-studios-1235570316/) | 40 points | by [ilamont](https://news.ycombinator.com/user?id=ilamont) | [22 comments](https://news.ycombinator.com/item?id=37191317)

In a recent court ruling, a federal judge upheld the U.S. Copyright Office's finding that artwork created by artificial intelligence (AI) is not eligible for copyright protection. The ruling came in response to a lawsuit filed by Stephen Thaler, the CEO of neural network firm Imagination Engines, who sought copyright protection for an artwork created by an AI system called the Creativity Machine. The Copyright Office denied Thaler's application, stating that human authorship is a requirement for copyright protection. The judge's ruling emphasized that copyright law is designed to protect works of human creation and has never extended to works generated by technology without human involvement. This decision has significant implications for the future of AI-generated content and raises questions about copyright ownership and protection in an increasingly AI-driven world.

The discussion focuses on various aspects of the court ruling regarding copyright protection for AI-generated artwork. One user points out that the law generally requires minimal creativity for copyrightable works, suggesting that AI-generated content could potentially fit within this requirement. Another user questions how humans selectively attribute copyright ownership to AI-generated content, highlighting the complexities of classifying and arbitrating ownership. There is also a discussion about whether copyright prompts generated by AI should be copyrightable, with differing opinions on the matter. Other users bring up related topics, such as the potential implications for public domain works and the value and significance of AI-generated content. Some users argue that AI-generated content is essentially worthless, while others point out that the value of such content depends on various factors, including its usefulness and the changes it brings to the business landscape. The discussion also touches on the legal provisions regarding the ownership of AI-generated content and the potential impact on the laws governing intellectual property.

---

## AI Submissions for Fri Aug 18 2023 {{ 'date': '2023-08-18T17:09:21.362Z' }}

### You probably donâ€™t need to fine-tune an LLM

#### [Submission URL](https://www.tidepool.so/2023/08/17/why-you-probably-dont-need-to-fine-tune-an-llm/) | 168 points | by [gk1](https://news.ycombinator.com/user?id=gk1) | [72 comments](https://news.ycombinator.com/item?id=37174850)

Today's post is for all the builders out there focused on developing LLM (large language models) applications. As a builder, it's crucial to know what tools are available in your toolbox and when to use them. With the booming experimentation in the LLM field, there's a wide range of techniques and acronyms to navigate, like fine-tuning, RLHF, RAG, and chain-of-thought.

However, it's easy to get stuck in decision paralysis when determining the best technical approach for your app, even if your ultimate goal is simply to build an app for a specific purpose. Many people encounter issues when using base model LLMs, such as models not returning desired results, providing nonsensical answers, or lacking knowledge on certain topics they weren't trained on. This is when many consider fine-tuning as a solution.

In this post, we'll explore why fine-tuning might not be necessary for your app. People often turn to fine-tuning when they need additional structure or style from the LLM beyond open-ended question answering, or when they want the LLM to answer questions using knowledge that it wasn't trained on. However, a combination of two techniques, few-shot prompting and retrieval-augmented generation (RAG), can often suffice for most use cases.

So, why do people think fine-tuning is helpful in the first place? Fine-tuning involves taking a pre-trained LLM and training it further on a smaller, domain-specific dataset to make it more specialized for a specific task or data. However, it's worth noting that as of August 2023, OpenAI only supports fine-tuning for its GPT-3 models, not the newer GPT-3.5 and GPT-4 models that power ChatGPT.

While the base LLMs have a range of abilities like question answering and summarization, some may find them too generic or unaware of their particular use case. The desire to fine-tune often stems from the belief that more training will improve accuracy in the target task. However, there are several reasons why the existing base LLM and the aforementioned techniques may be sufficient:

1. It's cheaper and leverages the existing training of the base LLM.
2. Techniques like RAG allow access to private knowledge bases by storing embeddings in vector databases and querying them semantically.
3. Desired style or format can be achieved using a more specific prompt and improved with few-shot prompting by providing examples within the context window.
4. Providing additional context in each prompt is not a token usage concern, as token usage isn't expensive.
5. Fine-tuning doesn't guarantee accurate answers and doesn't prevent LLM from hallucinating, whereas using clear questions with a source provided to the base model may result in more reliable answers.

In conclusion, the combination of few-shot prompting and retrieval-augmented generation techniques can often meet the needs of LLM applications without resorting to the complexity and cost of fine-tuning.

The discussion on the submission primarily revolves around the effectiveness and necessity of fine-tuning large language models (LLMs). 

One commenter suggests that using retrieval-augmented generation (RAG) and few-shot prompting can often be sufficient for most use cases, eliminating the need for fine-tuning. They highlight the benefits of these techniques, such as leveraging existing training, accessing private knowledge bases, and achieving desired style or format. Additionally, they point out that fine-tuning doesn't guarantee accurate answers and can result in the model hallucinating.

Another commenter shares their positive experience using a large context window and examples for challenging tasks. They note that breaking down complex questions into multiple steps within the context window can be effective.

Some commenters discuss the limitations and challenges of fine-tuning, including the need for a large amount of high-quality training data and the effort involved. They argue that for most developers relying on pre-trained models, using context-specific prompts is sufficient.

The discussion also touches on other topics related to LLMs, such as the potential dangers of hallucination, the benefits of retrieval-augmented generation, the challenges of working with large models, and the significance of model size and training data in the effectiveness of LLMs.

### Expanding Transformer size without losing function or starting from scratch

#### [Submission URL](https://arxiv.org/abs/2308.06103) | 49 points | by [og_kalu](https://news.ycombinator.com/user?id=og_kalu) | [25 comments](https://news.ycombinator.com/item?id=37178842)

Researchers Andrea Gesmundo and Kaitlin Maile have proposed six composable transformations to incrementally increase the size of transformer-based neural networks while preserving functionality. This approach allows for the expansion of the model's capacity without having to restart from scratch and randomly initialize all parameters. The authors provide proof of exact function preservation under minimal initialization constraints for each transformation. These methods could enable more efficient training pipelines for larger and more powerful models by progressively expanding the architecture throughout training. The paper, titled "Composable Function-preserving Expansions for Transformer Architectures," explores these methods and their potential impact on training state-of-the-art neural networks.

The discussion on this submission covered various aspects of the proposed method for expanding transformer-based neural networks. One user mentioned that they were curious about whether this approach could work for small models as well. Another user shared a link to a paper discussing the application of similar techniques to small language models. There was a discussion around the practicality and effectiveness of the proposed method, with some users expressing skepticism and others acknowledging the need for more experimental evidence. There was also a mention of general concepts like transfer learning and the potential impact of expanding and contracting Transformers on model size and behavior. Additionally, there were humorous comments and references to science fiction. One user highlighted the similarities between the proposed method and the concept of lifecycle software objects. Another user speculated about the future capabilities of AI and the potential for AI to invent things on its own. Overall, the discussion showcased a mix of curiosity, skepticism, and enthusiasm for the proposed method and its potential implications.

### Show HN: ChatGPT: craft the right question, unlock the best answer

#### [Submission URL](https://maestro-chatgpt.vercel.app/) | 24 points | by [gtestault](https://news.ycombinator.com/user?id=gtestault) | [5 comments](https://news.ycombinator.com/item?id=37174246)

Today's top story on Hacker News is about Maestro, a new browser extension for ChatGPT. Maestro aims to enhance the chat experience by providing users with the ability to craft better prompts, in turn unlocking more accurate and helpful answers. With Maestro, users can manage prompts, apply parameters, and even use a built-in PowerPoint maker. The extension is integrated into chat.openai.com, making it easily accessible. One notable feature of Maestro is its advanced prompting, which allows users to create parametrized prompts for even more specific responses. Additionally, Maestro ensures privacy and security as it runs entirely on the client-side without making any external web calls. The extension is also open-source, encouraging community contribution. If you're looking to supercharge your prompts and maximize the potential of ChatGPT, Maestro might be worth checking out!

The discussion around the Maestro browser extension for ChatGPT on Hacker News includes the following points:

- A user named "gtstlt" mentions that they are not currently using the extension due to a recent change in the ChatGPT UI. They suggest trying out the extension on the latest release on GitHub.
- Another user, "CapstanRoller," finds the scrolling behavior of the extension to be jarring. They explain that when scrolling downward, there is a flicker or jump to the bottom of the page.
- "somedude895" highlights an interesting use case for ChatGPT with the Maestro extension - creating Powerpoint presentations.
- "nthnldnsr" points out that Maestro does not meet the requirements of a "Show HN" (Show Hacker News) post, which is a type of submission where users showcase their projects. The user does not specify the exact requirements.
- In response to "nthnldnsr," another user named "nml" shares a link discussing the specific requirements for a "Show HN" post.

Overall, the discussion touches on issues with the extension's user interface, potential use cases for creating Powerpoint presentations, and a debate over whether the submission meets the criteria for a "Show HN" post.