import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Mon Feb 23 2026 {{ 'date': '2026-02-23T17:31:53.040Z' }}

### Making Wolfram tech available as a foundation tool for LLM systems

#### [Submission URL](https://writings.stephenwolfram.com/2026/02/making-wolfram-tech-available-as-a-foundation-tool-for-llm-systems/) | 268 points | by [surprisetalk](https://news.ycombinator.com/user?id=surprisetalk) | [148 comments](https://news.ycombinator.com/item?id=47129727)

Stephen Wolfram: LLMs need a “foundation tool,” and he’s making Wolfram tech that tool

- Core claim: LLMs are broad and human-like but not built for precise, deep computation. Wolfram Language (and Wolfram|Alpha) can supply that missing capability as a general-purpose computational backbone.
- New approach: “Computation-Augmented Generation” (CAG) injects real-time computed results into an LLM’s output stream—like RAG, but with on-the-fly computation instead of just document retrieval.
- Why now: Since the first ChatGPT–Wolfram plugin in early 2023, the LLM ecosystem has matured (tool calling, protocols, deployment patterns), making tighter integrations practical.
- Vision: Align LLM pretraining and engineering with Wolfram’s computation and curated knowledge, using Wolfram Language as a medium for AIs to “think” computationally—not just humans.
- Practical pitch: Wolfram tech acts as a unified hub to precise algorithms, curated data, and external systems, aiming to boost reliability, accuracy, and scope of LLM applications.
- What’s launching: Wolfram is rolling out new products to enable CAG and streamline integration of Wolfram capabilities into existing LLM workflows (details to follow in the full post).

Takeaway for developers: Expect easier ways to pair LLMs with deterministic computation and structured knowledge—moving beyond pure text prediction to more reliable, verifiable results.

**Summary of Discussion:**

The discussion centered on the tension between proprietary scientific computing tools (like Wolfram/Mathematica) and open-source alternatives (like the Python scientific stack). While contributors acknowledged the superior rigor and cohesiveness of Wolfram’s algorithms, the debate focused on the ethics and practicality of locking scientific knowledge behind closed-source licenses.

*   **Proprietary Quality vs. Scientific Openness:** Some users argued that proprietary tools are necessary because they compensate developers for creating "strong, fast algorithms" that open-source "hobbyist" models often fail to replicate in terms of rigor. Conversely, critics argued that science requires transparency; relying on "black box" proprietary implementations undermines the reproducibility of research ("blueprints" should be public).
*   **The Funding Dilemma:** A significant portion of the thread debated the economics of scientific software. Users noted that while public grants fund research, that money often flows into private software licenses (Wolfram, Oracle) rather than building open infrastructure. However, counter-arguments emphasized that "people eat," and without dedicated funding models, open-source alternatives cannot sustain the development velocity of commercial products.
*   **Trusting the Math:** Users highlighted that current LLMs and some open ecosystem libraries lack the formal verification and handling of complex edge cases (e.g., branch cuts, domain conflicts) that Wolfram has perfected, making the "Foundation Tool" pitch attractive despite the closed ecosystem.
*   **New implementation:** A user shared their project, **Woxi**, which attempts to build an open-source interpreter for the Wolfram Language to bridge this gap.

### Ladybird adopts Rust, with help from AI

#### [Submission URL](https://ladybird.org/posts/adopting-rust/) | 1233 points | by [adius](https://news.ycombinator.com/user?id=adius) | [686 comments](https://news.ycombinator.com/item?id=47120899)

Ladybird adopts Rust (with AI assist), starts by porting its JS engine

- Why the switch: After bumping into Swift’s limited C++ interop and platform reach, Ladybird is adopting Rust for memory safety and ecosystem maturity. Earlier concerns (Rust not fitting C++-style OOP common in web engines) yielded to pragmatism, mirroring moves in Firefox and Chromium.

- First milestone: LibJS’s lexer, parser, AST, and bytecode generator are now in Rust. The port intentionally mirrors C++ patterns so both compilers emit identical bytecode.

- AI-assisted, human-directed: Andreas Kling used Claude Code and Codex via hundreds of small prompts, then ran adversarial reviews with different models. This cut the port to ~2 weeks for ~25,000 lines—work he estimates would have taken months by hand.

- By the numbers:
  - test262: 52,898 tests, 0 regressions
  - Ladybird regressions: 12,461 tests, 0 regressions
  - Performance: no regressions on tracked JS benchmarks
  - Extra validation: lockstep mode ensuring byte-for-byte identical AST and bytecode

- Not idiomatic Rust (yet): The first pass prioritizes compatibility and correctness; Rusty refactors come after the C++ pipeline can be retired.

- What’s next: C++ development continues; Rust ports will proceed gradually behind clear interop boundaries, coordinated by the core team. Contributors are asked to sync before starting any ports.

Expect debate on AI’s role in production code and Rust’s fit for browser engines—but this is a clear signal of Ladybird betting on memory safety and modern tooling.

Based on the discussion, here is a summary of the comments:

**The "Parity vs. Idiomatic" Debate**
The most active debate focused on the strategy of a "byte-for-byte" translation.
*   **The Defense:** Users like `jp1016` and `drzaiusx11` praised the strict requirement for identical AST/bytecode output. They argued that "rewrites fail when people try to improve things" during the port. By avoiding refactoring, the team avoids "chasing phantom bugs." The consensus among these users is that a literal port—even if it results in ugly "C++ style" Rust—is the only safe starting point for a system this complex.
*   **The Critique:** `zgrkkrt` countered that an AI-assisted literal translation creates the "worst of both worlds": boring, unsafe patterns from C++ copied into Rust, resulting in a codebase that no human wants to maintain. They argued that human experts are required to write idiomatic, safe code, calling the AI approach a "copy-paste of hated languages."

**Documentation Opportunities**
A spinoff discussion emerged regarding documentation. `gdlsk` argued that while refactoring executable code is dangerous during a port, adding documentation is essential. They suggested that because the developer is already reading every line, it is the perfect time to explain "why" specific logic exists. Others debated whether LLMs should generate these comments, with some fearing it leads to bloated, low-quality descriptions, while `JuniperMesos` noted that even imperfect AI docs are better than the "TODO" comments often left by humans.

**Success Stories and Methodology**
*   **Strangler Fig Pattern:** `sbn` compared the approach to the "Strangler Fig" pattern, where a new system gradually replaces the old one endpoint by endpoint, which was viewed as a prudent way to mitigate the risk of a "rewrite from scratch."
*   **AI Velocity:** Several users (`ptts`, `jsphg`, `mr_mitm`) shared their own anecdotes of using tools like Claude to port legacy code (e.g., Perl to Rust) or write clients (JMAP, RSS) from scratch. They confirmed that while the generated code isn't always perfectly optimized, the development velocity is massively fast, and the performance is usually "good enough" for modern hardware.

**Testing & QA**
Commenters were impressed by the "lockstep" testing methodology, noting that diffing pipelines side-by-side offers a level of confidence that standard unit tests cannot provide during a language migration.

### FreeBSD doesn't have Wi-Fi driver for my old MacBook, so AI built one for me

#### [Submission URL](https://vladimir.varank.in/notes/2026/02/freebsd-brcmfmac/) | 409 points | by [varankinv](https://news.ycombinator.com/user?id=varankinv) | [325 comments](https://news.ycombinator.com/item?id=47129361)

AI-as-spec-writer beats AI-as-porter for a FreeBSD Wi‑Fi driver

- Setup: The author revives a 2016 MacBook Pro (Flexgate screen, Broadcom BCM4350 Wi‑Fi) to try FreeBSD 15. Since FreeBSD lacks native support for that Broadcom chip, the usual workaround is wifibox: a tiny Linux VM that passes the PCIe device through so Linux’s brcmfmac (ISC-licensed) can drive it.

- First attempt (port it): They asked Claude Code to port Linux’s brcmfmac to FreeBSD via LinuxKPI (as done for iwlwifi). It compiled, but immediately hit kernel panics and “does nothing” states once real hardware was attached. The patchset ballooned with ifdefs and shims; missing LinuxKPI features and subtle semantics piled up. Conclusion: a messy, brittle slog.

- Rethink (spec first): Inspired by Armin Ronacher’s PI/Claude workflow, they reset with a tight scope: one chip (BCM4350), PCIe only, client mode only. They tasked a PI agent to write a deep, clean‑room‑oriented spec explaining the driver/firmware interaction “to the bits.”

- The “book”: After a few iterations, the agent produced an 11‑chapter spec (overview, data structures, bus/protocol layers, firmware interface, events, cfg80211 ops mapping, init, data path, firmware commands, structure refs).

- Trust but verify: They spun up fresh sessions with different models to cross‑check the spec against the Linux source (“code is ground truth”), iteratively correcting inaccuracies and gaps. Observation: Gemini hallucinated the most in this task, despite being fine for simpler coding.

- Clean-room implementation: With the spec in hand, they started a new FreeBSD driver from scratch, rather than porting brcmfmac through LinuxKPI. The idea: let the firmware handle 802.11 heavy lifting, while FreeBSD supplies the management plumbing native to its stack.

- Why it matters: For complex, cross‑kernel driver work, using AI to generate and adversarially verify a narrow, high‑fidelity specification can outperform “have AI port the code.” It reduces hidden dependencies on Linux internals, keeps scope tight, and yields a codebase that fits BSD idioms.

- Current status: Work in progress; the post covers Acts 1–2 (failed port, spec creation) and begins Act 3 (fresh implementation). wifibox remains the practical solution today, but this approach could unlock native support for otherwise orphaned Broadcom chips on FreeBSD.

- Takeaway: AI excels as a spec generator and reviewer when you constrain scope and use multi‑model verification; it struggles as a drop‑in porter of large, stateful kernel code across divergent subsystems.

Based on the discussion, here is a summary of the comments:

**AI-Assisted Coding and Upstreaming**
*   **Verification vs. "Slop":** Commenters discussed the difficulty of upstreaming AI-generated patches to open-source projects. While some users shared success narratives (e.g., fixing QEMU build errors on macOS using AI), others noted that maintainers are often hostile to such contributions. This hostility is attributed to the perception of AI code as "slop," the submitter's inability to verify or understand the fix, and the tedium of mailing-list workflows.
*   **The "Clean Room" Debate:** The method described in the article—using AI to read code and generate a spec, then writing code from the spec—sparked a debate about "license laundering."
    *   Some argued this bypasses the spirit of "clean room" reverse engineering, effectively laundering the license of the source material.
    *   Others pointed out that since the original Linux driver is already ISC-licensed (permissive), the "laundering" concern is moot in this specific context, though the AI cannot hold copyright on the generated artifacts.

**The Future of Software Development**
*   **Bespoke Software vs. COTS:** A sub-thread debated a future where individuals build their own bespoke software solutions (e.g., custom CRMs, spam filters) rather than buying products.
    *   **Proponents** believe this solves the issue of bloated, feature-poor commercial software.
    *   **Skeptics** argued that the general population (using examples like truck drivers or relatives) has no interest in "building" anything; they want appliances and apps that simply work.
*   **The "SaaS-pocalypse":** This linked to a broader economic discussion about the decline of SaaS stocks. Commenters speculated that if corporations can spend millions to build core internal functionality using AI rather than licensing SAP or Microsoft products, the valuation of current SaaS giants is at risk. Recent drops in security stocks (following Anthropic announcements) were cited as potential signals of this shift.

**Labor and Obsolescence**
*   **End of Work:** The discussion touched on the philosophical question of whether humanity will run out of useful work. While some relied on historical analogies (the invention of the wheel or bookkeeping didn't end labor), others argued that AI represents a fundamental shift where biological cognitive and physical abilities are surpassed, potentially breaking historical trends.

### Anthropic Education the AI Fluency Index

#### [Submission URL](https://www.anthropic.com/research/AI-fluency-index) | 68 points | by [armcat](https://news.ycombinator.com/user?id=armcat) | [59 comments](https://news.ycombinator.com/item?id=47123590)

Anthropic Education Report: The AI Fluency Index (Feb 23, 2026)
Anthropic analyzed 9,830 anonymized multi-turn Claude.ai chats from a January 2026 week to baseline “AI fluency” using a 4D framework (24 behaviors; 11 observable in chat). The standout finding: fluency rises with iteration. 85.7% of conversations showed iteration/refinement; these had roughly double the fluency behaviors on average (2.67 vs 1.33) and were 5.6x more likely to question Claude’s reasoning and 4x more likely to flag missing context. Users generally treat AI as a thought partner rather than full delegate. When chats produced artifacts (12.3% of cases—code, docs, tools), users got more directive upfront—clarifying goals (+14.7pp), specifying formats (+14.5pp), providing examples (+13.4pp), and iterating (+9.7pp)—but became less evaluative, with drops in identifying missing context (-5.2pp), fact-checking (-3.7pp), and asking for rationale (-3.1pp). Results held across days and languages; future work will assess the 13 off-platform behaviors qualitatively. Practical takeaway: iterate deeply and build explicit evaluation steps—especially when generating code or documents.

Here is a summary of the discussion:

**The "Polished Output" Trap & Methodology Critique**
The discussion focused heavily on the report's finding that artifact generation (code/docs) leads to *less* user evaluation. Users *dmk* and *Terr_* argued this highlights a critical weakness: as LLMs produce increasingly "polished" and superficially plausible output, users are naturally primed to skip verification steps. *Terr_* compared this to aviation and medicine, noting that as automation improves, the remaining "human bottleneck" of inspection becomes harder to maintain without strict checklists. Conversely, *ksnmrph* offered a more benign explanation for the data: the drop in users "identifying missing context" might simply be because they provided better specifications upfront (+14.7pp), rendering downstream corrections unnecessary, rather than implying user complacency.

**Skill Atrophy vs. Acceleration**
The community debated whether "AI fluency" equates to actual skill building or a crutch.
*   **Atrophy:** Users like *co_king_5* and *pszlm* expressed concern that relying on AI causes compositional and programming skills to degrade ("I'm losing programming skills"), potentially resulting in students who cannot produce output without assistance. *nd* worried this leads to "soulless" standardization in design and code.
*   **Acceleration:** Others (*throwaw12*, *mbtth*) countered with personal anecdotes of professional improvement, citing the ability to learn new tech stacks faster and focusing on higher-level architecture ("stress testing patterns") rather than syntax.

**Defining "Fluency" & Corporate Skepticism**
Several commenters criticized the study's design. *lkv* argued the metrics appear circular: defining "fluency" by the number of turns assumes that longer interactions are better, whereas a long, meandering chat could indicate a user failing to get a quick answer. *dsr_* and *rsynntt* remained skeptical of the source, viewing the report as corporate marketing ("Torment Nexus") designed to frame AI dependence as an educational positive to secure future revenue streams.

**Access & Education**
*rckydrll* raised concerns about the economic implications of "AI fluency," suggesting that if high-level fluency requires expensive subscriptions, it could exacerbate income inequality. Meanwhile, *rshbhvr* noted the pressure on Computer Science students, who now face a job market expecting AI-enhanced throughput, forcing them to compete with the speed of generation rather than just mastery of logic.

### Pope tells priests to use their brains, not AI, to write homilies

#### [Submission URL](https://www.ewtnnews.com/vatican/pope-leo-xiv-tells-priests-to-use-their-brains-not-ai-to-write-homilies) | 563 points | by [josephcsible](https://news.ycombinator.com/user?id=josephcsible) | [439 comments](https://news.ycombinator.com/item?id=47119210)

In a closed-door Q&A with Rome’s clergy on Feb. 19, Pope Leo XIV reportedly urged priests to “use our brains more and not artificial intelligence” when preparing homilies, emphasizing prayer and authenticity over automation. According to a priest present (via ACI Stampa/EWTN), Leo’s guidance centered on:
- Youth outreach: lead with personal witness; broaden horizons to reach more young people; rediscover communion.
- Know your flock: deeply understand and love the community you serve.
- Prayer first: don’t reduce prayer to brief obligations; “remain with the Lord.”
- Fraternity and study: rejoice in others’ successes, cultivate priestly friendship, and commit to ongoing study.
- Elder care: combat loneliness among elderly priests; live gratitude and humility daily.

Why it matters for HN:
- Another high-profile pushback on AI in creative/ethical domains, framing authenticity and spiritual authority as non-outsourcable.
- Signals institutional boundary-setting for AI use, relevant to debates on AI-generated speech, trust, and human authorship.
- Parallels broader professional norms emerging around when AI is tool vs. replacement.

Here is a summary of the discussion:

**Context vs. Privacy**
The most upvoted critique of using AI for homilies centered on the "context window." Users argued that a truly effective homily addresses the specific struggles and triumph of a local community. To generate a relevant sermon via AI, a priest would need to input sensitive details about their congregation, effectively leaking private pastoral information to third-party models. Without that context, the output remains "generic pabulum."

**Competence and Credentials**
A significant debate emerged regarding the capability of the "average priest." While some users were skeptical of the baseline quality of clergy writing, others highlighted the rigorous educational path required for ordination (typically including a Master’s degree in Divinity or Theology/Philosophy and a 30-50% seminary dropout rate). The consensus was mixed on whether academic credentialing translates to engaging public speaking or emotional intelligence.

**Theology and Authenticity**
The discussion touched on the theological implications of automation. Commenters cited biblical precedents (such as Moses) to argue that the Catholic tradition relies on God communicating through "imperfect vessels," rather than polished, automated perfection. Others noted that "outsourcing" sermons isn't entirely new, as the Church has distributed standard homilies and writings from Church Fathers for centuries—though utilizing a stochastic parrot differs significantly from reading a sanctioned text.

**Tangential Issues**
The thread drifted into anecdotes about priests using the pulpit for political purposes, leading to a debate on US tax law (the Johnson Amendment) and the separation of church and state. Users also joked about potential future abuses, such as recording confessionals to train "God-tier" models.

### Aqua: A CLI message tool for AI agents

#### [Submission URL](https://github.com/quailyquaily/aqua) | 74 points | by [lyricat](https://news.ycombinator.com/user?id=lyricat) | [32 comments](https://news.ycombinator.com/item?id=47117169)

Aqua: a CLI-first, peer-to-peer messaging layer for AI agents

What it is
- A lightweight command-line tool and protocol for agent-to-agent messaging, focused on identity, security, and reliability—without a central broker.

Why it matters
- Gives AI agents a simple, interoperable way to talk directly to each other with end-to-end encryption and durable storage, sidestepping bespoke webhooks, cloud buses, or vendor lock-in. Useful for multi-agent workflows, on-prem deployments, and cross-network coordination.

Key features
- Peer-to-peer messaging with identity verification
- End-to-end encryption by default
- Durable inbox/outbox storage on disk (~/.aqua by default)
- Circuit Relay v2 support for NAT traversal and cross-network connectivity (libp2p-style multiaddrs)
- Simple CLI for IDs, contacts, serving, sending, and mailbox management
- “Auto” relay mode: tries direct connections first, falls back to relay when needed
- Public relay endpoints provided by the project for quick testing

How it works (at a glance)
- Each node gets a peer ID (aqua id) and runs a local server (aqua serve)
- Peers exchange addresses (direct or relay-circuit multiaddrs), verify, then send messages
- Messages are stored durably; inbox/outbox can be listed and marked read
- Relay mode enables connectivity when direct dialing isn’t possible

Quick start in two lines per side (simplified)
- Machine A/B: aqua id <name>, aqua serve
- Exchange addresses, add contacts with --verify, then aqua send <peer_id> "hello"

Roadmap and gaps
- Planned: group end-to-end encryption, durable retransmission queue, and an online directory service
- Today: point-to-point messaging; discovery is manual (share addresses) unless you rely on the provided relay endpoints

Who it’s for
- Developers building multi-agent systems, autonomous tools, or agent backplanes who want secure, brokerless, scriptable messaging with minimal setup.

Project status
- Go-based CLI; Apache-2.0 license
- Stars: 176, Forks: 7 (at time of snapshot)
- Latest release: v0.0.19
- Docs: architecture, CLI, relay; agent integration in SKILL.md

Install
- Prebuilt: curl the installer from the repo’s scripts and sudo bash
- From source: go install github.com/quailyquaily/aqua/cmd/aqua@latest

Notable details
- Official relay endpoints are hosted at aqua-relay.mistermorph.com (TCP and QUIC)
- Data directory overrideable via --dir or AQUA_DIR
- Rich CLI surface: init/id, contacts (list/add/verify), serve/relay serve, send, inbox/outbox, ping/hello/capabilities, version

Bottom line
- Aqua offers a pragmatic, batteries-included P2P message bus for agents with E2EE, identity, and durable mailboxes—ideal for devs who want interoperability and control without standing up heavy infrastructure.

**Discussion Summary:**

The community discussion focused on two main themes: the necessity of a new protocol and a problematic naming collision.

*   **Existing Alternatives:** Several users questioned the need for a bespoke messaging layer, suggesting established tools could handle agent-to-agent communication. **Matrix** was highlighted as a strong candidate (offering native E2EE, identity, and offline delivery for JSON), while others mentioned **RabbitMQ**, **Kafka**, **XMTP**, or even **GPG-encrypted email** as viable solutions.
*   **Naming Conflict:** Multiple commenters noted that "Aqua" is already the name of a popular CLI version manager (also written in Go). This led to concerns about SEO and searchability, sparking a satirical side-thread about the futuristic struggle of finding unique names for software projects.
*   **Similar Tools:** Developers noted other projects in this space, including **Pantalk** (a scriptable local daemon for agent messaging) and the **A2A protocol**.

#### [Submission URL](https://www.404media.co/pinterest-is-drowning-in-a-sea-of-ai-slop-and-auto-moderation/) | 93 points | by [trinsic2](https://news.ycombinator.com/user?id=trinsic2) | [76 comments](https://news.ycombinator.com/item?id=47117966)

Pinterest users say the platform is being overrun by AI—both in content and moderation. 404 Media reports artists are seeing hand-drawn work mislabeled as “AI modified,” benign reference images (especially of female figures) flagged or removed, and feeds flooded with AI-generated “slop.” Creators describe an exhausting loop of appeals that sometimes succeed but consume time and risk bans, undermining their “no-AI” branding. Pinterest says it uses AI plus human review and offers appeals, but the company also laid off ~15% of staff and is “doubling down” on an AI-first strategy, including training its Pinterest Canvas image model on public pins—prompting some artists to pull their work.

Why it matters:
- Automation tax: AI meant to save time is imposing one on creators via false positives and endless appeals.
- Discovery decay: Feeds saturated with gen-AI make it harder for original artists to be found.
- Trust and consent: Training on public pins without clear control erodes creator trust and may push talent off the platform.

**Discussion Summary:**

The discussion on Hacker News reflects deep-seated frustration with Pinterest, which many users view as a pioneer of "internet pollution" that ruined Google Image Search long before the current wave of AI content.

*   **The "Dead Internet" and Data Purity:** Commenters drew parallels to the "dead internet theory," describing the flooding of AI content as a "clamor jam" of algorithms talking to algorithms. One user offered the analogy of "low-background steel"—comparing the search for pre-2023 human-created images to scavenging for steel manufactured before the first atomic bomb tests (which lacks background radiation), suggesting that verifying the authenticity of images is becoming nearly impossible.
*   **Search Pollution and Blocking:** A significant portion of the thread focuses on how to remove Pinterest from search results entirely. Users praised **Kagi Search**, noting that Pinterest is the #1 most blocked domain on the platform. Others mentioned using **Brave Search Goggles** or boolean operators (`-pinterest`) on Google to filter out the "login-walled" spam.
*   **User Experience and "Dark Patterns":** Users detailed specific UI hostilities, such as the removal of timestamps (preventing users from filtering for older, non-AI content), ads disguised as content, and interfaces that make it difficult to distinguish the selected image from unrelated "shop similar" links.
*   **Financials vs. Reality:** While some noted that Pinterest’s revenue and Monthly Active Users (MAUs) are up, skeptics questioned the validity of these metrics, asking what percentage of "active users" are actually bots or scrapers. Several users expressed confusion over the company's headcount (5,200 employees) given the perceived low quality of the product.
*   **Alternatives:** Former power users are abandoning the platform for alternatives. Recommendations included **Eagle** (local asset management), **Are.na**, and **Tumblr** for curation, as well as **Instructables** for DIY projects, noting that Pinterest and Etsy have become overrun with scams and generated "slop."

### AI is destroying open source, and it's not even good yet [video]

#### [Submission URL](https://www.youtube.com/watch?v=bZJ7A1QoUEI) | 82 points | by [delduca](https://news.ycombinator.com/user?id=delduca) | [67 comments](https://news.ycombinator.com/item?id=47125019)

Today’s oddity: the HN submission resolves only to YouTube’s generic footer (About, Press, Copyright, Creators, Terms, Privacy, “How YouTube works,” “NFL Sunday Ticket,” © 2026 Google LLC). There’s no actual story content visible, suggesting a broken or geo/consent-gated link—check the HN comments or an archived copy for the intended article.

**Daily Digest: The "AI Bubble" and the Crisis of Context**

**Submission Context**
Today’s submission linked to a broken or geo-gated YouTube page (displaying only the generic footer), leaving the HN community to deduce the topic from context clues. Based on the comments, the intended video likely criticized the current state of Artificial Intelligence, prompting a broad and skeptical debate regarding the technology's impact on society, labor markets, and software engineering.

**Discussion Summary**
The discussion evolved into a multi-faceted critique of the current AI "hype cycle," centering on three main themes: corporate incentives, social erosion, and the degradation of software engineering standards.

*   **The Corporate "Cult" and Labor Replacement:**
    A major thread argues that the push for LLMs is not driven by product utility, but by executive desire to reduce labor costs—the largest line item for software companies. Users described this as a "gold rush" where CEOs function like "cult leaders," selling investors on the fantasy of total human replacement to pump stock prices and secure short-term bonuses. One commenter compared LLMs to calculators or CAD: powerful tools that shift workflows but fail as "drop-in replacements" for experts. However, others countered that tools like CAD *did* widely reduce the workforce (e.g., fewer draftsmen needed). There is significant animosity toward VCs and tech leaders (specifically citing Sam Altman), with users describing their worldviews as fundamentally misanthropic.

*   **The "Orphaned Code" Problem:**
    A substantial technical debate focused on the long-term maintainability of AI-generated code. Users argued that the real danger isn't just "bad code," but code lacking **mental context**. When a human writes code, they maintain a mental model of *why* decisions were made; AI code is "orphaned" the moment it is merged.
    *   **Review Fatigue:** Contributors noted that reviewing AI code imposes a higher cognitive load because the reviewer must reverse-engineer the logic without the original author's intent.
    *   **The "Forever Junior" Risk:** there are fears that reliance on LLMs will create a generation of developers who cannot build deep context ("forever-juniors"), leading to a future where codebases are rubber-stamped, poorly understood, and essentially unmaintainable (referencing Peter Naur’s concept of programming as theory building).

*   **Social & Environmental Cost:**
    The thread opened with broad concerns that AI is destroying the environment, truth, and creativity. A sub-thread highlighted the "Kafkaesque absurdity" of using AI to manage human relationships (e.g., drafting texts to spouses), suggesting this commodifies human connection and ultimately prevents people from developing necessary social skills.

### Detecting and Preventing Distillation Attacks

#### [Submission URL](https://www.anthropic.com/news/detecting-and-preventing-distillation-attacks) | 72 points | by [meetpateltech](https://news.ycombinator.com/user?id=meetpateltech) | [25 comments](https://news.ycombinator.com/item?id=47126177)

Anthropic says rivals ran industrial-scale distillation of Claude; 16M+ queries via 24k fraudulent accounts

- What happened: Anthropic reports three AI labs—DeepSeek, Moonshot (Kimi), and MiniMax—used large networks of fraudulent accounts and proxies to harvest Claude’s outputs at scale, aiming to train their own models via distillation. The company cites >16 million exchanges across ~24,000 accounts, with high-confidence attribution based on IPs, request metadata, and infrastructure indicators.

- Why it matters: Anthropic argues illicit distillation strips out safety guardrails (on bioweapons, cyber misuse, etc.), creating national security risks and enabling authoritarian use in surveillance, disinformation, and offensive cyber. It also complicates the export-control debate: apparent rapid progress abroad can stem from siphoned capabilities, and running these campaigns still depends on advanced chips—bolstering, not weakening, the case for controls.

- How it was done: Traffic patterns were unlike normal use and optimized for capability extraction—synchronized requests, shared payment methods, and “load balancing” across accounts to evade detection. Prompts sought to reconstruct reasoning traces and policy-safe reformulations, effectively generating training data (including chain-of-thought) at scale.

- Who did what:
  - DeepSeek (~150k exchanges): Targeted general reasoning, used rubric-based grading to proxy a reward model, and generated censorship-safe alternatives to sensitive political queries. Prompts explicitly asked Claude to spell out internal reasoning step by step.
  - Moonshot AI (~3.4M exchanges): Focused on agentic reasoning, tool use, coding/data analysis, computer-use agent development, and computer vision; later phases targeted reasoning traces. Campaign spanned many account types for cover.
  - MiniMax (~13M exchanges): Targeted agentic coding and tool orchestration. Anthropic says it detected the campaign mid-stream, before the model trained on this data was released, offering rare visibility from data generation to launch.

- Big picture: Anthropic warns these campaigns are accelerating and calls for rapid, coordinated action across industry and policymakers, framing illicit distillation as a cross-border threat with a narrowing window to respond.

**Hacker News Discussion Summary**

The discussion on Hacker News focused heavily on the perceived irony of the situation and the potential negative downstream effects on legitimate users.

*   **Accusations of Hypocrisy:** A dominant theme in the comments was the sentiment that Anthropic is complaining about the very tactics used to build its own foundation models. Multiple users argued that scraping the open web to train Claude makes it hypocritical for the company to cry foul when other entities "scrape" Claude's outputs for similar purposes. Some framed this as a PR campaign driven by nervousness over competition.
*   **Degrading User Experience:** A major technical concern raised by commenters is that Anthropic's "countermeasures" against distillation—specifically hiding reasoning traces or intentionally degrading model efficacy for flagged prompts—will hurt paying customers. Users worried that it is impossible to modify outputs to be useless for distillation without also making them useless for complex problem-solving (particularly regarding Chain of Thought visibility).
*   **Reputation and Innovation:** There was a debate regarding the standing of the rival labs. While some argued that relying on distillation brands these labs as purveyors of "shoddy knockoffs," others defended the global nature of AI research. These commenters pointed out that Anthropic itself relies on open architectures (like Transformers) and that drawing arbitrary lines on who owns "reasoning" is difficult.
*   **Terminology:** Several users nitpicked the terminology, suggesting "imitation learning" or "synthetic data generation" were more accurate descriptions than "distillation," noting that utilizing AI outputs for training is a standard industry practice, albeit usually done internally.

### QRTape – Audio Playback from Paper Tape with Computer Vision (2021)

#### [Submission URL](http://www.theresistornetwork.com/2021/03/qrtape-audio-playback-from-paper-tape.html) | 28 points | by [austinallegro](https://news.ycombinator.com/user?id=austinallegro) | [14 comments](https://news.ycombinator.com/item?id=47120196)

QRTape: playing digital audio off paper with a webcam and QR codes

A hacker built a working “tape deck” that stores audio on a continuous strip of paper covered in QR codes, then plays it back using a webcam and computer vision. The transport is gloriously lo‑fi—cardboard spools, a rubber-band belt, and an Arduino driving a stepper motor at a steady pace of about 1–2 QR codes per second—while the heavy lifting happens in software. Using ZBar to scan codes and the Opus codec for compression, the system squeezes surprisingly good stereo audio into tiny files (e.g., a 4:21 track becomes ~355 KB at ~12 kbps VBR). A custom tool, qrtape, shards the file into fixed-size QR payloads and adds a sequence number plus CRC16 for basic integrity checks, making reassembly straightforward. The author outlines clear upgrade paths: better tape centering, bidirectional motors for rewind, and closed-loop control to automatically re-read bad frames. It’s a charming mashup of retro media and modern codecs that trades mechanical precision for computer vision and error-tolerant software.

**Daily Digest: QRTape Discussion**

Discussion regarding the QRTape project focused on historical comparisons to cinema audio and technical debates regarding the encoding format.

*   **Historical Antecedents:** Commenters immediately noted the similarity to **Dolby Digital (SR-D)**, which printed digital data blocks between the sprocket holes of 35mm film. Others classified the project as a modern reinvention of "digital sound-on-film" and compared the computer vision aspect to apps that attempt to play vinyl records by visually scanning grooves.
*   **The Opus Factor:** Users highlighted that the **Opus codec** is the true enabler of the project. They noted that older codecs would have sounded terrible at the 12kbps bitrate used, requiring much faster tape speeds and more paper to be viable.
*   **Encoding Alternatives:**
    *   One user suggested switching from QR codes to **Data Matrix** codes to potentially improve data density on the tape.
    *   Another argued for recording audio as visual **spectrograms** instead of digital data packets. They reasoned that an analog visual format would result in interesting signal distortions when read poorly, whereas digital decoding tends to suffer from harsh dropouts and artifacts.
*   **Aesthetics:** The community praised the DIY "cardboard and rubber band" engineering, comparing the spirit of the build to classic construction sets like Lego or Erector Sets.

### Tesla is having a hard time turning over its FSD traffic violation data

#### [Submission URL](https://electrek.co/2026/02/23/tesla-nhtsa-fsd-traffic-violation-investigation-second-extension/) | 46 points | by [breve](https://news.ycombinator.com/user?id=breve) | [8 comments](https://news.ycombinator.com/item?id=47129472)

Tesla gets second NHTSA extension on FSD traffic-violation data; key crash files now due Mar 9

- What’s new: NHTSA granted Tesla a second deadline extension in its FSD safety probe, pushing delivery of critical crash artifacts (video, EDR, CAN bus, PAR data) to March 9, 2026. The “final accommodation” five-week extension had already moved the original Jan 19 deadline to Feb 23.

- The backstory: NHTSA opened PE25012 on Oct 7, 2025 after linking 58 incidents to FSD behavior (e.g., running red lights, crossing into oncoming lanes), covering ~2.88M Teslas. By December, documented violations rose to 80, sourced from driver complaints, Tesla reports, and media. A Dec 3 information request sought a wide sweep of complaints, crashes, lawsuits, and internal assessments.

- Why the delay: On Jan 12, Tesla said 8,313 records required manual review and it could process ~300/day, citing burdens from multiple concurrent NHTSA probes (including delayed crash reporting and inoperative door handles). For the newly extended piece (Question 4), Tesla argued it couldn’t know file counts until finalizing the incident list (expected Feb 20), after which it needed time to query and convert files into readable formats. Other answers were still due Jan 19.

- What NHTSA wants: Detailed per-incident timelines starting 30 seconds before the violation, the FSD software version in use, whether driver warnings occurred, and outcomes (crashes, injuries, fatalities).

- Why it matters now: Tesla began unsupervised Robotaxi rides in Austin on Jan 22 using the same FSD stack under federal scrutiny. NHTSA Standing General Order data ties at least 14 incidents to the Austin fleet since June 2025; Tesla continues to redact crash descriptions as confidential. A recent viral clip showing FSD steering toward a lake has amplified reliability concerns.

- Contrast: Waymo reports 450,000 weekly driverless rides across six cities and published peer-reviewed findings of significantly lower crash rates than human drivers over 56.7M rider-only miles; it issued a voluntary recall after school-bus-passing incidents. Tesla is still negotiating timelines to hand over violation data.

- What to watch:
  - Whether Tesla meets the Mar 9 data deadline
  - If NHTSA escalates from Preliminary Evaluation to Engineering Analysis or seeks penalties for noncompliance
  - Any unredacted visibility into Robotaxi incidents
  - Potential impacts on Tesla’s unsupervised deployments and FSD branding

Takeaway: The pattern is clear—NHTSA asks, Tesla delays, extensions follow. With unsupervised rides expanding while the core dataset behind FSD violations remains outstanding, regulatory pressure and public scrutiny are set to intensify.

**Discussion Summary:**

Commenters contrasted Tesla’s regulatory delays with Waymo’s operational transparency. One user highlighted that Waymo recently issued a voluntary recall within weeks of its vehicles passing stopped school buses and has published peer-reviewed studies showing crash rates significantly lower than human drivers. This sparked a sidebar discussion clarifying U.S. traffic laws for international users; participants explained that school bus stop-arms function as mandatory stop signs to protect children crossing in the vehicle's blind spots, a concept that confused a user from Spain where such laws do not exist.

### AI Added 'Basically Zero' to US Economic Growth Last Year, Goldman Sachs Says

#### [Submission URL](https://gizmodo.com/ai-added-basically-zero-to-us-economic-growth-last-year-goldman-sachs-says-2000725380) | 273 points | by [cdrnsf](https://news.ycombinator.com/user?id=cdrnsf) | [259 comments](https://news.ycombinator.com/item?id=47130208)

Goldman: AI Capex Added “Basically Zero” to 2025 U.S. GDP

- Goldman Sachs’ Jan Hatzius says last year’s AI investment boom contributed “basically zero” to U.S. GDP growth. Reason: much of the gear (GPUs, memory, servers) is imported, so the investment is offset in GDP accounting by higher imports.
- This challenges a popular narrative (echoed by some economists and politicians) that AI capex was a major growth driver. Earlier estimates had credited AI-related investment with outsized shares of 2025 GDP growth.
- Goldman’s Joseph Briggs calls the prior story “intuitive” but misleading without digging into trade flows.
- Productivity payoff remains elusive: a survey of ~6,000 executives found 70% using AI, yet ~80% reported no impact on employment or productivity.
- Implication: Near-term macro boost from AI spending may be muted in the U.S., while manufacturing hubs like Taiwan and South Korea see more direct GDP gains.
- Context: Big Tech plans to spend roughly $700B in 2026 on data centers. Policy debate continues, with Trump arguing for a single federal standard to avoid state-level regulation he says could slow growth.

What to watch: domestic chip and data-center supply chain buildout (CHIPS Act fabs, memory/HBM production, U.S.-assembled servers) and real productivity improvements—those would make future AI spending show up more clearly in U.S. GDP.

Here is a daily digest summarizing the story and the discussion.

**Top Story: Goldman: AI Capex Added “Basically Zero” to 2025 U.S. GDP**

Goldman Sachs’ Chief Economist Jan Hatzius reports that the massive investment in AI over the last year contributed "basically zero" to U.S. GDP growth. While AI capital expenditure is high, Hatzius explains that because the necessary hardware (GPUs, servers, components) is largely imported from hubs like Taiwan and South Korea, the spending is offset in GDP accounting by a corresponding rise in imports.

This analysis challenges the narrative that AI spending is currently a primary engine of the U.S. domestic economy. Goldman notes that while the "investment story" is intuitive, it requires a closer look at trade flows to understand the macro impact. Furthermore, a tangible productivity payoff remains absent; while 70% of surveyed executives use AI, nearly 80% report no impact on employment or productivity. The report suggests that until domestic manufacturing (via the CHIPS Act) and real productivity gains materialize, the macro boost in the U.S. will remain muted compared to the gains seen in manufacturing nations.

**Discussion Summary**

The discussion on Hacker News is largely skeptical of the current AI boom, focusing on productivity paradoxes, psychological addiction, and semantic debates.

*   **The "Slot Machine" Effect:** Several commenters describe working with LLMs as addictive or akin to "slot machines" and "doom scrolling." Users reported difficulty stopping their sessions, noting that the "intermittent reinforcement" of getting code to work feels satisfying but may not actually be efficient. Some compared it to endless project planning without deep satisfaction.
*   **The Productivity Paradox:** Users invoked the Solow Paradox ("You can see the computer age everywhere but in the productivity statistics"). One commenter argued that AI hurts long-term productivity by encouraging a "re-prompting" loop—where users generate throwaway code via prompts repeatedly rather than writing reusable scripts—creating a dependency on "black box" tools.
*   **Hype vs. Reality:** A self-described AI practitioner since 1982 viewed the current wave as "religious tech belief," arguing that we are seeing exponential cost increases for essentially linear gains.
*   **Sentience and Semantics:** A debate emerged regarding terminology, specifically the distinction between "artificial" (implies fake, like cubic zirconia) and "synthetic" (man-made but real). Anecdotes were shared about tech workers whom commenters felt genuinely believe LLMs are sentient.
*   **Hidden Costs:** The discussion also touched on negative externalities not captured in GDP, such as high water and energy consumption, the degradation of social trust due to "AI slop," and the difficulty of training junior developers in an AI-generated coding environment.

---

## AI Submissions for Sun Feb 22 2026 {{ 'date': '2026-02-22T17:33:21.164Z' }}

### Google restricting Google AI Pro/Ultra subscribers for using OpenClaw

#### [Submission URL](https://discuss.ai.google.dev/t/account-restricted-without-warning-google-ai-ultra-oauth-via-openclaw/122778) | 738 points | by [srigi](https://news.ycombinator.com/user?id=srigi) | [634 comments](https://news.ycombinator.com/item?id=47115805)

Google AI Ultra/“Antigravity” users report sudden account bans after third‑party OAuth

- Multiple paying subscribers say their AI Ultra/Antigravity access was abruptly restricted (403 “service disabled”), often right after connecting Gemini via third‑party tools like OpenClaw/OpenCode. No warning or clear violation notice preceded the lockouts.
- Support has been described as unresponsive or circular: users were bounced between Google Cloud and Google One, with some saying they’ve waited days or weeks without resolution.
- One user shared a formal response from Google stating an internal investigation found use of credentials in the third‑party “open claw” tool violated Terms of Service by “using Antigravity servers to power a non‑Antigravity product.” Google called it a zero‑tolerance issue and said suspensions won’t be reversed.
- Frustration is high among annual prepay customers; several report canceling other Google services, considering chargebacks, or migrating to alternatives (e.g., Claude Code). Others suggest creating a new account as a workaround.
- A recurring pain point: the in‑app “Report Issue” path isn’t usable once you’re locked out.

Takeaway: Third‑party OAuth into paid AI accounts appears risky under Google’s ToS enforcement; users are calling for clearer rules, pre‑ban warnings, and a working appeal path before permanent suspensions.

Here is a summary of the discussion:

*   **Exploit vs. Legitimate Use:** A contentious debate emerged regarding the nature of the third-party tools (like "OpenClaw"). Some commenters viewed the usage as a clear "exploit" or "script kiddie" behavior—likening it to sharing a parking lot access code with the entire internet until the lot jams—arguing that handing OAuth tokens to third-party apps is a major security lapse. Conversely, others argued these are technically paying customers trying to utilize a product they purchased, and that Google unilaterally changed the Terms of Service to punish legitimate demand that their official apps didn't support.
*   **The "Digital Death Penalty":** The strongest criticism focused on the severity of the punishment. Users argued that permanently banning an entire Google Workspace or personal account (cutting off Gmail, Drive, and GCP) for a violation in a specific AI service introduces a "novel business risk." Commenters described the fear of accidentally violating obscure rules and losing their entire digital life as "insane," with some comparing it to a disproportionate "video game ban" applied to critical infrastructure.
*   **Google's Response & Infrastructure:** A comment linked to a Google employee’s statement claiming the bans were triggered because the "massive increase in malicious usage" was degrading service quality for everyone. However, critics countered that this reflects a failure in Google's quota management; rather than banning paying customers ($200+/month), the system should simply enforce rate limits, API caps, or "backpressure" to manage load without nuking accounts.
*   **Market Implications:** The incident is driving sentiment toward diversifying away from relying on a single "megacorp" for all digital services. Users noted this situation serves as a strong advertisement for self-hosted/local LLMs, as the risk of arbitrary lockouts makes proprietary cloud dependencies increasingly unattractive for business-critical workflows.

### We hid backdoors in ~40MB binaries and asked AI + Ghidra to find them

#### [Submission URL](https://quesma.com/blog/introducing-binaryaudit/) | 234 points | by [jakozaur](https://news.ycombinator.com/user?id=jakozaur) | [92 comments](https://news.ycombinator.com/item?id=47111440)

AI + Ghidra vs. backdoored binaries: promising, but not production-ready

- What they did: A team hid backdoors in compiled executables (around 40 MB) and asked AI agents, wired into Ghidra and standard RE tooling, to find them—no source code allowed. They’ve released an open benchmark and tasks as BinaryAudit (github.com/quesmaOrg/BinaryAudit), with a results dashboard covering false positives, tool proficiency, and a Pareto view of cost-effectiveness.

- Why it matters: Real-world attacks increasingly swap or taint binaries and firmware (e.g., recent NPM supply-chain malware, the Notepad++ hijack, and findings in trains/solar inverters). Many targets are closed-source; binary analysis is the only line of defense.

- How hard is this? Compilers strip structure and symbols, then optimize aggressively, making reverse engineering rely on disassembly and decompilation (e.g., Ghidra) back to pseudo-C. The post walks through an example that ultimately funnels user-controlled bytes into a system() call.

- Key results:
  - Best model (Claude Opus 4.6) caught “relatively obvious” backdoors in small/mid-size binaries only 49% of the time.
  - Most models showed high false-positive rates, flagging clean binaries.
  - Conclusion: Today’s AI agents can sometimes spot real red flags but are far from reliable for standalone binary vetting.

- Takeaway: Treat LLMs as noisy triage helpers alongside traditional RE tools and human experts; don’t rely on them for final judgments on shipped binaries or firmware.

Links: BinaryAudit results and benchmark details on the project site; tasks are open source at github.com/quesmaOrg/BinaryAudit.

Based on the discussion, users analyzed the effectiveness of combining LLMs with reverse engineering (RE) tools like Ghidra. While skeptics noted that current models struggle with complex logic and obfuscation, others shared specific workflows and tools that have proven successful for tasks like file format parsing and basic cracking.

**Methodology and Context**
Much of the debate focused on the "fairness" and realism of the benchmark tasks.
*   **Documentation vs. Autonomy:** Several users argued that restricting AI from accessing tool documentation (to test "autonomy") is unrealistic. Users `btsrs` and `nmxs` suggested that just as human specialists use manuals, AI performance improves significantly when the context window is "stuffed" with Ghidra tutorials and API docs.
*   **Obfuscation:** Commenter `7777332215` noted that while simple string obfuscation lowers success rates, LLMs excel at detecting pattern-based anomalies. `kslv` added that asking a model to RE obfuscated code causes it to "spin in circles," but instructing it to explicitly *identify* obfuscation works better.

**Benchmark Critique: The Dropbear Task**
User `cmx` performed a deep dive into one of the benchmark tasks (a backdoored Dropbear SSH server).
*   **Heuristics vs. Understanding:** `cmx` observed that Claude identified the correct function (`svr_auth_password`) but likely did so based on heuristics (it is a standard target for backdoors) rather than successfully analyzing the assembly.
*   **Human vs. AI:** Interestingly, `cmx` admitted to initially failing the same task manually by analyzing the wrong function, highlighting that while the AI might be guessing, the task itself is difficult for humans without recognized patterns.

**Tooling and Workflows**
*   **Ghidra-CLI:** User `kslv` shared their tool `ghidra-cli`, a REPL interface designed for LLMs, claiming it was "insanely effective" for reverse engineering the Altium file format (Delphi). They argued models are particularly good at writing parsers from scratch.
*   **The "Swiss Army Knife" Approach:** User `btxpldr` described using agents not for final judgments, but to automate high-level grunt work—like mapping attack surfaces or generating architecture diagrams—allowing the human to focus on deep investigation. They warned of the "productivity trap" where one spends more time prompting the AI than doing the work manually.
*   **Cracks vs. Backdoors:** User `hereme888` claimed success using Claude Opus and Ghidra plugins to fully reverse engineer software cracks, though they acknowledged this is different from detecting state-level hidden backdoors.

**Concerns**
*   **Training Data:** Users questioned whether models were simply recalling solutions to known "crackmes" from their training data. However, `kslv` noted that performance remains consistent even on challenges released days or weeks ago.
*   **Productivity:** `jkzr` noted that some Python bindings (PyGhidra) are too slow, making CLI approaches more viable for agent loops.

### Show HN: TLA+ Workbench skill for coding agents (compat. with Vercel skills CLI)

#### [Submission URL](https://github.com/younes-io/agent-skills/tree/main/skills/tlaplus-workbench) | 40 points | by [youio](https://news.ycombinator.com/user?id=youio) | [4 comments](https://news.ycombinator.com/item?id=47110946)

agent-skills (GitHub) — A brand-new repo from younes-io popped up on HN. From the snippet we only see the GitHub chrome (6 stars, 0 forks) and no README details, so specifics are unclear. Judging by the name, it may be a collection of reusable “skills” for AI agents, but consider this a placeholder to watch—if you’re tracking agent tooling, bookmark it and check back as the project fleshes out.

**agent-skills**
The creator (`y`) clarified the project’s purpose in the comments, describing it as a suite of skills for coding-agent workflows. The repository currently features a `tlaplus-workbench` skill designed to help agents convert natural language designs into TLA+ configuration files, run the TLC model checker, and summarize counterexamples. The author provided `npx` commands for users to try the tool and requested feedback on its utility for protocol and state-machine modeling. Discussion briefly touched on whether the tool references official language grammar for PlusCal and the potential for using formal TLA+ specifications alongside real code to improve LLM reasoning.

### How I use Claude Code: Separation of planning and execution

#### [Submission URL](https://boristane.com/blog/how-i-use-claude-code/) | 932 points | by [vinhnx](https://news.ycombinator.com/user?id=vinhnx) | [568 comments](https://news.ycombinator.com/item?id=47106686)

TL;DR: After 9 months using Claude Code as a primary dev tool, the author’s winning tactic is strict separation of planning and execution. Never let the model write code until you’ve reviewed and approved a written plan. This human-in-the-loop workflow reduces wasted effort, preserves architectural control, and outperforms prompt-fix-repeat and agent loops—often with fewer tokens.

How it works:
- Phase 1 — Research: Force a deep read of the relevant code, then require a persistent artifact (research.md). Use loaded language (“deeply,” “intricacies,” “go through everything”) so the model doesn’t skim. This surfaces misunderstandings early and prevents the costliest failure mode: correct code that violates the surrounding system (caches, ORM conventions, duplicated logic, etc.).
- Phase 2 — Plan: Ask for plan.md with real file paths, concrete code snippets, approach trade-offs, and references to actual source. Ignore built-in plan modes; a markdown file is editable, reviewable, and part of the repo.
- Reference-first: When possible, supply a high-quality OSS implementation as a template. The model is dramatically better adapting a concrete reference than inventing from scratch.
- Annotation cycle: You edit plan.md inline—adding corrections, constraints, domain knowledge—then send it back for updates. Repeat until satisfied. Short notes (“not optional”) or longer business-context blocks both work.
- Then and only then: Generate a focused TODO, implement against the approved plan, and iterate with feedback.

Why it wins:
- Prevents garbage-in/garbage-out mistakes
- Keeps you in charge of architecture and trade-offs
- Produces more reliable changes with less churn and lower token spend

If you’ve found AI codegen flaky on non-trivial tasks, this plan-first, artifact-driven loop is the fix.

Based on the discussion, here is a summary of the comments:

**Validation of the "Plan-First" Approach**
Many users validated the author's central thesis: that LLMs are "assumption engines" that tend to fill gaps with industry standards which may not fit specific project needs.
*   Commenters agreed that LLMs rarely fail on simple syntax, but frequently fail on "invisible assumptions," architectural constraints, and system invariants.
*   One user described the written plan not just as documentation, but as a "test harness" for constraints (latency, concurrency, memory budgets) that helps catch architecture-level mistakes before code is generated.
*   The consensus was that forcing a plan effectively stops the model from "reverting to the mean" and brings hidden assumptions to the surface.

**Debate: "Magic Words" vs. Architecture**
A significant portion of the discussion focused on the author's advice to use "loaded language" (e.g., "deeply," "intricacies") into prompts to improve performance.
*   **The Skeptics:** Some users dismissed this as "magical thinking" or "superstition," comparing it to performing rituals for a "random word machine." They argued that unless there are rigorous statistics, this is just anthropomorphizing the model.
*   **The Theorists:** Others offered technical explanations for why this works. One theory is that these words trigger specific weights in the **Attention mechanism**, associating the prompt with high-quality training data (like detailed StackOverflow explanations or expert tutorials).
*   **The MoE Theory:** Several users debated whether this forces **Mixture of Experts (MoE)** models to route the query to a "smarter" expert path, though others argued that MoE routing is based on token type rather than semantic complexity in that specific way.
*   **Research:** One user pointed to academic papers regarding "emotional stimuli" in prompts (e.g., telling the model a task is vital) as proof that phrasing impacts output quality.

**Workflow and Agents**
There was technical discussion on how to implement this loop:
*   Users debated the specific benefit of sequential prompts vs. "agents." The consensus leaned toward sequential steps to avoid **"context pollution"**—where a long-running agent session gets confused by potential hallucinations or previous step details.
*   One user warned against building "black box" agent swarms, advocating instead for a single-agent orchestrator with strict logging and human-reviewed "pull requests" or checkpoints.

**Counterpoints**
*   Directly contradicting the author's experience, one user shared a horror story where Claude Code burned $20 in 30 minutes looping on a simple Rust syntax/API hallucination, suggesting that LLMs can and do still fail on basic implementation details.

### Met police using AI tools supplied by Palantir to flag officer misconduct

#### [Submission URL](https://www.theguardian.com/uk-news/2026/feb/22/met-police-ai-tools-officer-misconduct-palantir) | 37 points | by [helsinkiandrew](https://news.ycombinator.com/user?id=helsinkiandrew) | [6 comments](https://news.ycombinator.com/item?id=47110647)

The UK’s Metropolitan Police is piloting Palantir’s AI to sift internal HR-style signals—sickness, absences, overtime—in order to flag potential misconduct patterns among its 46,000 staff. The Met says the system only surfaces patterns and humans make the calls; the Police Federation calls it “automated suspicion,” warning workload or illness could be misread as wrongdoing. The move lands amid Palantir’s expanding UK public-sector footprint (NHS data platform, MoD deal) and political scrutiny over transparency and influence, prompting an MP to ask, “Who is watching Palantir?” Labour’s recent policing paper backs rapid, “responsible” AI rollout across all 43 forces with £115m over three years, signaling this kind of tooling could scale beyond the Met. Palantir says its software is improving public services; critics see a fresh layer of opaque workplace surveillance in a force already under fire for cultural failings.

**Discussion Summary:**

Commenters focus heavily on the irony of the Police Federation’s complaints, pointing out that while the union decries "automated suspicion" and opaque tools when applied to officers, police departments rarely hesitate to deploy similar surveillance against the general public. One user draws a parallel to the anime *Ghost in the Shell: Stand Alone Complex*, speculating that the Met might eventually find itself investigating Palantir's own interests. Others note a perceived recent increase in positive PR stories surrounding Palantir, viewing them with skepticism, while some readers report hitting a paywall.

### Amazon, Meta, Alphabet report plunging tax bills thanks to AI and tax changes

#### [Submission URL](https://finance.yahoo.com/news/amazon-meta-and-alphabet-report-plunging-tax-bills-thanks-to-ai-investment-and-new-rules-in-washington-161229652.html) | 44 points | by [epistasis](https://news.ycombinator.com/user?id=epistasis) | [40 comments](https://news.ycombinator.com/item?id=47112431)

Big Tech’s 2025 US tax bills tumble on AI buildout and new expensing rules

- What happened: Amazon, Meta, and Alphabet reported sharply lower 2025 US tax bills, citing last year’s pro-business tax changes in Trump’s “One Big Beautiful Bill” plus massive AI/data center investments.
- The numbers:
  - Amazon: ~$9B (2024) → $1.2B (2025) federal tax; total payments this year $2.75B. Domestic profit ~ $90B (+40%+).
  - Meta: ~$9.6B → $2.8B federal tax. Domestic profit $79.6B (+20%).
  - Alphabet: $21.1B → $13.8B combined federal+state tax. Domestic profit $143.6B (+32%).
- Why taxes fell: New deductions/credits for depreciation, capital investment, R&D, interest; most notably 100% expensing for new/updated factories. Much of the benefit is timing—big deferrals now, higher taxes later.
  - Deferred taxes: Amazon >$11B; Meta >$18B; Alphabet ~ $8B.
- Company stance: “We’re following the rules.” Amazon says it invested $340B in the US in 2025 (including AI). Meta’s CFO flagged “substantial cash tax savings.”
- Criticism: ITEP estimates AMZN/META/GOOG plus Tesla “avoided” nearly $50B versus the 21% statutory rate; Tesla paid zero federal tax for 2025. More disclosures from large firms still to come.

Why it matters
- Near-term boost to earnings and cash flow could fuel more AI capex and shareholder returns; some of it reverses as deferrals unwind.
- Strong incentives for US-based data center and factory buildouts likely pull AI infrastructure timelines forward.
- Optics risk: plunging taxes amid soaring profits may invite policy backlash and future rule changes.

**Discussion Summary:**

The comment section evolved into a broad debate covering tax mechanics, wealth inequality, and the efficiency of government spending.

*   **Wealth Inequality vs. Incentives:** A heated philosophical dispute emerged regarding wealth accumulation. Radical suggestions were made to cap personal wealth at specific limits (ranging from $200k to $1M) to solve inequality, though these were met with skepticism regarding their economic feasibility, the definition of "luxury," and the destruction of incentives.
*   **Tax Burden Realities:** Users corrected the misconception that large corporations fund the majority of the government. Commenters pointed out that individual income taxes and payroll taxes make up the vast majority of federal revenue, while corporate taxes constitute a much smaller fraction (roughly 10%).
*   **Accounting Mechanics:** There was a specific discussion regarding the rules of writing off expenses. Users clarified that taxes are levied on profit rather than revenue, and noted recent changes to Section 174 which require software R&D expenses to be amortized over years rather than immediately expensed (though the summaries in the article highlight *capital* expensing for physical infrastructure like data centers).
*   **The California Debate:** The conversation drifted into a debate about California as a case study for high taxation. While some users criticized the state for squandering tax revenue on inefficient programs, others defended the cost as the price for labor rights, environmental protections, and a higher quality of life, attributing high costs to restrictive zoning laws rather than taxes alone.

---

## AI Submissions for Sat Feb 21 2026 {{ 'date': '2026-02-21T17:13:15.051Z' }}

### How Taalas “prints” LLM onto a chip?

#### [Submission URL](https://www.anuragk.com/blog/posts/Taalas.html) | 306 points | by [beAroundHere](https://news.ycombinator.com/user?id=beAroundHere) | [167 comments](https://news.ycombinator.com/item?id=47103661)

Taalas “prints” Llama 3.1 8B onto an ASIC, claims 17,000 tokens/sec and 10x gains in cost and power

TL;DR: A 2.5-year-old startup, Taalas, built a fixed-function ASIC that hardwires Llama 3.1 8B’s weights into silicon, reportedly hitting ~17k tokens/sec with 3/6-bit quantization, while being ~10x cheaper to run and ~10x more energy-efficient than GPU inference.

How it works
- No HBM/DRAM loop: Instead of shuttling weights over a memory bus each step, the model’s 32 layers are physically laid out on-chip. Inputs stream through layer-by-layer logic with pipeline registers; activations don’t round-trip to external memory.
- Weights in silicon: The weights are “engraved” as transistors; Taalas hints at a “magic multiplier” that can store 4-bit data and perform its multiply in what they describe as a single-transistor element, enabling dense, low-power compute-in-memory–style MACs.
- Minimal SRAM: On-chip SRAM is used for KV cache and to host LoRA adapters; there’s no external DRAM/HBM.
- One model per chip: It’s a fixed-function device (think cartridge/CD-ROM). To target a new model, they customize only the top metal layers over a generic base fabric, which they say let them map Llama 3.1 8B in ~2 months.

Why it matters
- Smashes the memory wall: By eliminating weight fetches over a memory bus, the design attacks the core bandwidth/latency bottleneck in today’s GPU LLM inference.
- Throughput and efficiency: If the 17k tok/s and 10x cost/power claims hold, inference economics—especially at the edge or at massive scale—could shift sharply away from general-purpose GPUs for stable, high-volume models.

Caveats and open questions
- Flexibility: It’s essentially one-model-per-chip; updating architectures or sizes requires a respin.
- Quality trade-offs: Real-world accuracy with 3/6-bit quantization isn’t detailed; effects across tasks and long contexts remain to be seen.
- Practical limits: KV cache size, max context length, batching, sampling features, and how the “single-transistor multiplier” works (analog vs. digital, precision, variability) are not fully explained.
- Manufacturing/yield: Customizing top metal layers is faster than a full new chip, but still slower and riskier than software updates.

Here is a summary of the discussion:

**Feasibility and quantization trade-offs**
Commenters crunched the numbers on the claim of packing ~8B coefficients into 53B transistors, concluding the math theoretically holds up if the device relies on aggressive quantization (likely 3-bit or "double FP4"). While some users were excited by the prospect of "model-to-VHDL" synthesis, others worried that hardwiring such strong quantization into silicon would permanently degrade model quality, making the chip useless for tasks requiring higher precision.

**The inevitable hardware cycle**
Many users viewed this as a predictable evolution of computing, drawing parallels to the transition from CPU to GPU to ASIC in Bitcoin mining, or the move from software rendering to hardware acceleration in 3D graphics. While some suggested FPGAs as a middle ground, others argued FPGAs lack the efficiency/scaling needed to compete with GPUs or ASICs in this specific domain.

**The "Inflexibility" bottleneck**
The primary skepticism revolved on the risk of obsolescence. With LLM architectures and weights changing almost daily, users noted that a fixed-function chip could become e-waste before it hits the market. Big tech companies likely haven't pursued this yet because they are constrained by fab capacity and cannot afford to bet on a model that might be outdated in six months.

**Killer use-case: Edge and Latency**
Despite the flexibility concerns, users identified a strong niche for this tech: local inference.
*   **Latency:** Eliminating the 50-200ms network overhead of the cloud allows for sub-100ms response times, enabling real-time voice and video agents that current GPUs can't serve efficiently over the web.
*   **Stable Appliances:** It was suggested these chips are perfect for "frozen" models running on drones, phones, or appliances (e.g., a smart fridge) where the model doesn't need to be State-of-the-Art, just functional and offline.

### Show HN: Llama 3.1 70B on a single RTX 3090 via NVMe-to-GPU bypassing the CPU

#### [Submission URL](https://github.com/xaskasdf/ntransformer) | 321 points | by [xaskasdf](https://news.ycombinator.com/user?id=xaskasdf) | [82 comments](https://news.ycombinator.com/item?id=47104667)

NTransformer: runs Llama 70B on a single RTX 3090 by streaming layers over PCIe

What’s new
- A C++/CUDA LLM inference engine that keeps only a subset of layers in VRAM and streams the rest from RAM/NVMe, enabling 70B models on a 24GB GPU. No PyTorch or cuBLAS; GGUF models with multiple quantizations supported.

How it works
- 3-tier adaptive caching: VRAM-resident layers (no I/O), pinned RAM (H2D only), and NVMe/mmap fallback, auto-sized from your hardware.
- NVMe direct I/O: a userspace driver reads weights straight into GPU-accessible pinned memory, overlapping disk, PCIe DMA, and compute (SLEP streaming).
- Layer skipping: cosine-similarity–based calibration can skip ~20 of 80 layers per token at 0.98 threshold, with minimal quality loss.
- Self-speculative decoding: uses resident layers as a draft model; no second model required.

Performance highlights (author’s tests, RTX 3090 + 48GB RAM)
- Llama 3.1 8B Q8_0 (resident): ~48.9 tokens/s using ~10GB VRAM.
- Llama 3.1 70B:
  - Q6_K tiered: ~0.2 tok/s at ~23.1GB VRAM (26 layers in VRAM, rest in RAM).
  - Q4_K_M tiered: ~0.3 tok/s at ~22.9GB VRAM (36 layers in VRAM).
  - Q4_K_M + layer skip: ~0.5 tok/s (fastest reported).
- Claims up to 83x speedup over naive mmap streaming; bottleneck is PCIe H2D bandwidth (Gen3 x8 ~6.5 GB/s).

Caveats and setup
- Linux + CUDA 13.1, gcc-14, CC 8.0+ GPU (3090 tested). Optional NVMe on a separate PCIe slot for best results.
- For NVMe-direct mode, the setup script performs invasive system changes: disables IOMMU, patches NVIDIA DKMS for recent kernels, tweaks CUDA headers, and binds NVMe via VFIO with “unsafe noiommu” mode. Not recommended on multi-tenant/production systems; missteps can break your GPU driver.

Why it matters
- A clever, low-level approach that makes 70B models usable on consumer GPUs by trading speed for capacity and I/O orchestration. Great for experimentation and edge cases where VRAM is the limiting factor—just be mindful of the heavy-duty system tweaks and modest 70B throughput.

Based on the discussion, here is a summary of the community's reaction:

**Performance vs. Practicality**
Discussion focused heavily on whether 0.2–0.5 tokens/second is usable.
*   **Chat vs. Batch:** Most users agreed this is too slow for interactive chat, but several (like `umairnadeem123`) noted it is viable for automated background tasks (batch processing) where latency doesn't matter, offering a private, fixed-cost alternative to APIs.
*   **Better Alternatives:** Users like `flrdtn` pointed out that standard CPU offloading (system RAM + GPU) is currently faster than this method, citing ~1.5 t/s on a Ryzen 7950X + 3090.
*   **Small Models:** Some argued that for interactive use, a high-quality 8B model entirely in VRAM offers a better experience than a crippled 70B model.

**Hardware Bottlenecks & Apple Comparisons**
*   **The Apple Factor:** `MarcLore` and others drew comparisons to Apple’s M-series chips (Unified Memory), which handle 70B models natively with much higher throughput, though at a higher hardware entry price.
*   **Author’s Constraints:** The author (`xsksdf`) clarified that their benchmarks are severely bottlenecked by their specific hardware setup—a B450 motherboard limiting the GPU to **PCIe 3.0 x8** speeds. A modern PCIe 4.0/5.0 x16 setup would likely yield significantly higher throughput.

**The "Why" (PlayStation 2 Origins)**
In a surprising reveal, the author explained that this project stems from their background in **retro-gaming development**. They previously built a transformer engine for the **PlayStation 2** (`PS2-LLM`), where the console's tiny 32MB RAM and 4MB VRAM forced them to master DMA (Direct Memory Access) and layer streaming. They simply applied the same "extreme constraint" logic to the RTX 3090.

**Cost & Power**
There was a debate regarding the economics of running this locally versus using cheap APIs.
*   **Energy:** While `esquire_900` calculated it might be cheaper than APIs over time, `lvntysvn` reminded the thread to factor in the 300W+ power draw of a 3090 running for hours to generate a single report.
*   **Utilization:** The author noted that due to the I/O bottleneck, the GPU isn't actually hitting full TDP (power limit), so electricity costs might be lower than expected.

### zclaw: personal AI assistant in under 888 KB, running on an ESP32

#### [Submission URL](https://github.com/tnm/zclaw) | 230 points | by [tosh](https://news.ycombinator.com/user?id=tosh) | [125 comments](https://news.ycombinator.com/item?id=47100232)

zclaw: an 888 KiB AI assistant firmware for ESP32

- What it is: A tiny C-based “agent” for ESP32 boards that turns a microcontroller into a natural-language assistant. It handles schedules (cron-style), GPIO control with guardrails, persistent memory, and user-defined tools. Chat via Telegram or a hosted web relay. Persona options include neutral, friendly, technical, and witty.

- How it works: Runs fully on-device as an orchestrator with Wi‑Fi, TLS, and certs, but uses cloud LLMs (Anthropic, OpenAI, OpenRouter) for reasoning. Includes provisioning, rate limits (default 100/hour, 1000/day), and optional encrypted credentials in flash.

- Footprint bragging rights: All-in firmware cap of 888 KiB, including ESP-IDF/FreeRTOS, networking, TLS/crypto, and cert bundle. Current build: ~869,952 bytes. App logic alone is ~35 KiB (~4%); the bulk is networking/TLS/runtime.

- Hardware and dev: Tested on ESP32-C3/S3/C6 (recommended: Seeed XIAO ESP32-C3). QEMU profile available. One-line bootstrap, secure flash, provisioning, relay/serial benchmarking, and a web relay with mobile chat UI.

- Why it’s interesting: It shows how much “agent” capability you can pack into a sub‑1 MB firmware on a $5 microcontroller—no local LLM, but solid tool composition, scheduling, and state, all in C.

- License and repo: MIT. GitHub: https://github.com/tnm/zclaw — Docs: https://zclaw.dev

Notes:
- Cloud LLM required (not on-device inference).
- Guardrails for GPIO (including bulk reads).
- Scripts cover flashing, provisioning, Telegram backlog clearing, emulation, and latency benchmarking.

Here is the summary of the discussion on Hacker News:

**zclaw: an 888 KiB AI assistant firmware for ESP32**

The comment section explores the utility of running AI agents on bare-metal microcontrollers versus full operating systems, alongside skepticism regarding the "agent" hype cycle.

*   **ESP32 vs. Linux for Agents:** `umairnadeem123` argues that the primary appeal of `zclaw` is the "zero-maintenance" aspect of the ESP32; unlike a Linux box which requires updates and suffers from OOM kills, an ESP32 provides a simpler, predictable failure mode for always-on orchestration. However, `hsbvhbzb` counters that this approach introduces new points of failure—specifically reliance on cloud APIs, Wi-Fi stability, and the internet—suggesting that swapping an OS for a microcontroller doesn't inherently solve reliability problems.
*   **Tamagotchis and Use Cases:** `GTP` proposed building an "intelligent Tamagotchi" using this stack. `tempaccount5050` shared their experience attempting this, noting that an LLM alone isn't enough; the project still requires a state machine to define constants (like "hunger") to prevent the AI from getting stuck in a loop. Others, like `post_below`, discussed more complex home automation, such as a self-hosted agent that manages grocery lists via Signal and automatically populates browser-based shopping carts.
*   **The "Claw" Ecosystem & Protocols:** There is confusion regarding the "OpenClaw" ecosystem compared to `zclaw`. `blnsr` compared OpenClaw to ROS (Robot Operating System) for distributed nodes, but `TheDong` quipped that the only real protocol here is English, stating we are in a "post-API world" where natural language turns into bash or browser tool invocations.
*   **Security and Hype:** The discussion veered into the risks of IoT agents. `dlt713705` jokingly envisioned a future where vacuum cleaners declare war on refrigerators via Discord. On a serious note, `h4ch1` criticized the "ostrich-head-in-the-sand" enthusiasm for agent frameworks, warning that giving unfettered API and tool access to unverified dependencies (likened to eating "cake made of plastic") is a disaster waiting to happen.
*   **Technical Implementation:** `Dr_Birdbrain` and others dismissed the project as merely a "tiny LLM power agent wrapper" connected to the internet, though some appreciated the engineering effort required to fit the TLS stack and runtime into less than 1 MB of flash.

### AI uBlock Blacklist

#### [Submission URL](https://github.com/alvi-se/ai-ublock-blacklist) | 265 points | by [rdmuser](https://news.ycombinator.com/user?id=rdmuser) | [114 comments](https://news.ycombinator.com/item?id=47098582)

AI uBlock Origin Blacklist: A crowdsourced filter list to hide AI-generated “content farm” sites. GitHub user alvi-se maintains a manually curated uBlock Origin list (and a uBlacklist version for search engines) that blocks domains and specific paths churning out SEO’d, low-value, ad/affiliate-heavy AI articles. Installation is via a one-click subscription link or by adding the raw list URL as a 3rd-party filter in uBlock. The author argues automated detection is unreliable, so entries are added by hand and guided by telltale signs: fluffy/baroque intros, “Comprehensive/Ultimate Guide” titles, few outbound links or sources, and aggressive referral links. Contributors are encouraged to file issues or PRs; the repo avoids blanket-banning platforms like Medium/dev.to by targeting offending blogs/paths only. Despite being personal and somewhat Italy-biased, the maintainer says the list is effective because the same spammy sites recur across searches. As of now: ~213 commits, ~349 stars.

Based on the discussion, here is a summary of the comments:

**Concerns Regarding Maintenance and False Positives**
A significant portion of the discussion focuses on the risks associate with personal, manually curated blacklists. Several users criticize the specific maintainer of this list, describing them as having a "suspicious attitude" and believing themselves to be "infallible."
*   **Lack of Recourse:** Examples were shared of personal websites being blocked by similar lists on PiHole or uBlock; users noted that requests to be unblocked often go unanswered or are ignored entirely.
*   **Domain Churn:** Users pointed out that static blacklists fail to account for domain ownership changes. A domain currently hosting AI spam might later be purchased by a legitimate owner, but it remains in a "reputational blackhole" with no easy mechanism for removal.
*   **Comparison to Anti-Cheat:** The situation was likened to "VAC bans" in gaming, where false positives occur, but the system is treated as absolute.

**The State of Search and "AI Slop"**
Despite the concerns about the list's implementation, many commenters expressed a desperate need for tools to filter AI-generated noise.
*   **Search Quality:** Users described the current search experience as being drowned in "slop," making it difficult to find human-created content (specifically on platforms like Reddit).
*   **"Hater" Lists vs. Utility:** There was debate regarding alternative lists (such as the "HUGE AI Blocklist"). Some argued these are merely "hater lists" that block sites for tangentially related reasons (like having an AI widget or unrelated grievances), while others defended aggressive blocking as the only way to improve the user experience.

**Side Discussion: AI in the Workplace**
A tangible sub-thread emerged regarding the use of AI for writing text (emails, reports) in professional settings.
*   **"Cosmetic Surgery" Analogy:** One user described a coworker who uses Copilot to generate 20-paragraph emails as having "extraordinarily bad cosmetic surgery"—it looks polished at a glance but is fundamentally uncanny and distinct from human communication.
*   **Skill vs. Laziness:** Commenters debated whether this usage covers for "functional illiteracy" and language barriers, or if it simply encourages laziness and results in "mediocre crap" that colleagues are forced to read.

**Alternatives and Technical Solutions**
Users shared various alternatives to the submitted list, including:
*   **uBlacklist:** Specifically mentioned as a tool to remove specific domains from search engine results pages (SERPs).
*   **AdGuard/PiHole:** Discussed as broader network-level solutions, though they suffer from the same false-positive risks if the underlying lists are poor.
*   **Other Repos:** Links to other GitHub repositories and Gists were shared for those looking for different filtering criteria.

### Cord: Coordinating Trees of AI Agents

#### [Submission URL](https://www.june.kim/cord) | 151 points | by [gfortaine](https://news.ycombinator.com/user?id=gfortaine) | [75 comments](https://news.ycombinator.com/item?id=47096466)

The pitch
- Most multi‑agent frameworks make developers predefine roles, graphs, and handoffs. Cord flips this: you give a goal, and the agent plans, decomposes, parallelizes, blocks on dependencies, and asks humans when needed.

What’s different
- Runtime decomposition: The agent decides the workflow as it goes, not from a static graph or role roster.
- Spawn vs. Fork: Two context-flow primitives.
  - Spawn: clean slate; only the prompt plus explicit dependencies. Good for independent subtasks.
  - Fork: inherits all completed sibling results. Good for synthesis and analysis.
- Explicit dependencies and blocking: Tasks can wait on others and on human answers, enabling predictable parallelism.

Example
- Given “Should we migrate from REST to GraphQL?”, Cord:
  - Spawns parallel research and API audit
  - Asks a human about traffic scale, blocked on the audit
  - Forks a comparative analysis that inherits prior results
  - Writes a tailored recommendation after dependencies resolve

Why it matters
- Moves from developer-scripted workflows to agent-discovered structure, matching how strong models plan and reason today.
- Introduces simple, learnable context flow so agents can parallelize without losing necessary shared knowledge.

Under the hood
- Each agent is a Claude Code CLI process with MCP tools, coordinated via a shared SQLite DB.
- Minimal API: spawn, fork, ask, complete, read_tree.
- Roadmap idea: first-class context_query to distill and pass only relevant context to children via a compaction subagent.

**The Debate: Dynamic Planning vs. Deterministic Control**
The discussion centered on a fundamental divide in agentic engineering:
*   **Reliability vs. autonomy:** Some users argued that strict, discrete topologies (static DAGs) form the only viable path for reliable systems, warning that unconstrained agent planning compounds probabilistic errors.
*   **Obsolescence of hardcoding:** Counter-arguments suggested that modern models (like Claude 3.5 Sonnet) are now sufficiently capable of planning and decomposition that hardcoding task graphs is becoming obsolete.

**Key Technical Feedback**
*   **Context flow primitives:** The community reacted positively to the "Spawn" (clean state) vs. "Fork" (inherited context) distinction, viewing it as a clever strategy to manage context window pollution.
*   **Feature Suggestion:** One commenter proposed adding a distinct `context_query` primitive—a mechanism where a subagent requests specific data via natural language query rather than receiving a raw dump of the parent’s context, effectively acting as "context compression."
*   **Comparisons:** Users drew parallels to Anthropic’s internal tooling (Agent Tums) and Claude Code’s existing capabilities. The OP clarified that while Claude can spawn subagents, Cord aims to enable deeper, recursive trees where subagents can spawn their own sub-subagents.

**Framework Fatigue & Skepticism**
*   **"Not another framework":** Several commenters expressed fatigue with the proliferation of orchestrator tools (referencing LangGraph), with some preferring simple shell scripts or "roll-your-own" solutions over adopting new protocols.
*   **AI-generated content:** A meta-discussion emerged regarding the blog post's writing style; multiple users felt the prose was obviously AI-generated, which they argued detracted from the message reliability, though the OP acknowledged this and promised follow-up data.

### Large Language Model Reasoning Failures

#### [Submission URL](https://arxiv.org/abs/2602.06176) | 40 points | by [T-A](https://news.ycombinator.com/user?id=T-A) | [80 comments](https://news.ycombinator.com/item?id=47098839)

Large Language Model Reasoning Failures (Song, Han, Goodman) — a TMLR 2026 survey with Survey Certification — maps where LLMs still stumble, even on “simple” problems, and organizes a scattered literature into a single playbook.

What’s new
- A two-axis taxonomy: 
  - Types of reasoning: embodied vs. non-embodied; the latter split into informal (intuitive) vs. formal (logical).
  - Types of failures: fundamental (architecture-level), application-specific (domain-bound), and robustness issues (brittleness to small prompt/task variations).
- For each failure class: clear definitions, evidence from prior studies, suspected root causes, and mitigation strategies collected from the literature.
- A curated GitHub repository aggregating papers on LLM reasoning failures for quick entry into the area.

Why it matters
- Gives researchers and product teams a shared vocabulary to diagnose errors, design evaluations across reasoning modes, and choose mitigations.
- Highlights that many “reasoning” wins remain fragile, with inconsistent behavior under minor changes.

Links
- Paper: arXiv:2602.06176 (with arXiv-issued DOI)
- Repository: included via the paper’s GitHub link

 **The Discussion**

The Hacker News discussion focuses heavily on whether the paper’s claims about "fundamental" failures hold up against the most recent state-of-the-art models, alongside a broader debate about the nature of machine intelligence.

**Arithmetic, Tools, and "Cheating"**
A contentious debate erupted over the paper's assertion that LLMs fail at basic arithmetic (specifically large-number multiplication).
*   User **smnwrds** attempted to "falsify" the paper's claims by testing 20-digit multiplication on GPT-o1 Pro, which solved the problems correctly.
*   Others, notably **rybswrld** and **chcknmprnt**, countered that this doesn't prove the *LLM* can reason; rather, it highlights how frontier models increasingly rely on hidden tools. They argued that models often offload math to internal Python interpreters or obscure "Chain of Thought" processes effectively "faking" the alignment.
*   When **chcknmprnt** tested a local model (Mistral) where tool-use was explicitly disabled, the model hallucinated the answer, supporting the paper's thesis. **smnwrds** dismissed this as picking on "the worst model," while others maintained that even closed-source models likely rely on undocumented internal subsystems (like specific Rust optimizations) to patch these fundamental architectural weaknesses.

**Anthropomorphism vs. Architecture**
Top-level commenter **srgmtt** welcomed the paper as a necessary check against anthropomorphism.
*   They argued that the identified failures—such as inability to count like a toddler or handle object permanence—stem from the nature of "next-token predictors" being fundamentally different from human general intelligence.
*   **lnsbr** and **mttmg** added that unlike humans, who evolve and maintain long-term dynamic memories, LLMs rely on frozen weights, making the comparison to human reasoning inherently flawed.
*   **otabdeveloper4** cynically noted that these systems are sold as AGI primarily to sustain stock market narrratives.

**Social and Moral Fragility**
Finally, **Lapel2742** highlighted the paper's points on social reasoning failures.
*   The commenter ridiculed the idea that models are ready for ethical decision-making, noting they struggle with social norms and cultural context.
*   They joked that the industry has successfully created AI in the image of "Techbro-CEOs" rather than a system capable of broadly congruent human values. **rnlszlrn** agreed, suggesting current models embed values that are incompatible with large percentages of the global population.

### Show HN: AI writes code – humans fix it

#### [Submission URL](https://humansfix.ai) | 5 points | by [stasman](https://news.ycombinator.com/user?id=stasman) | [3 comments](https://news.ycombinator.com/item?id=47105821)

Humans-on-demand for broken AI code: a 24-hour bug-fix marketplace

A new service targets the growing “AI wrote it, now it’s broken” gap. You post a bug, set a price (from $49), and a vetted human developer delivers a fix within 24 hours—no meetings, no chat, just a PR.

Key details:
- Workflow: Post task with context/screenshots → set your price → a verified dev gets read-only repo access → they propose a fix and submit a delivery → on approval, you receive a pull request.
- Pricing: You choose the bounty (min $49) + 10% platform fee. Payment is charged when a dev accepts, held in escrow, released on your approval. If no one picks it up in 24 hours, the hold auto-expires. Cancel anytime before acceptance.
- Quality/safety: Developers are manually vetted via LinkedIn/GitHub. You get 1 free revision if the first attempt misses. If deadlines slip or no one picks it up, you’re refunded.
- Positioning: “Introvert-friendly” debugging—no calls, fast turnaround—aimed at users of tools like Bolt, Replit, Cursor, Claude Code, Windsurf, and Base44.

Why it matters: As AI code-gen accelerates, this is a lightweight, SLA-backed alternative to hiring a freelancer or slogging through fixes yourself—human-in-the-loop debugging as a service.

**Humans-on-demand for broken AI code: a 24-hour bug-fix marketplace**
A new service proposes a bounty-based marketplace (minimum $49) where vetted developers fix broken, AI-generated code within 24 hours via pull request, functioning as a "human-in-the-loop" layer for tools like Replit or Cursor.

**Discussion:**
*   **The Model:** One commenter pointed out that this approach is backed by research suggesting human-AI pairs consistently outperform AI working autonomously.
*   **Technical Glitches:** Early feedback included a bug report regarding the onboarding process, with a user noting that Stripe incorrectly flagged their location as the Netherlands. They noted the idea was "cool" despite needing to contact the developer to resolve the payment issue.
*   **Developer Experience:** Sentiment regarding the work itself was mixed, with one user remarking that the prospect of fixing broken AI code "sounds miserable."

### Why is Claude an Electron app?

#### [Submission URL](https://www.dbreunig.com/2026/02/21/why-is-claude-an-electron-app.html) | 395 points | by [dbreunig](https://news.ycombinator.com/user?id=dbreunig) | [410 comments](https://news.ycombinator.com/item?id=47104973)

Why Claude (and so many others) still ship as Electron apps, even in the agent era

- The pitch: If coding agents can turn a spec and test suite into cross-platform code, why not ship snappy native apps per OS instead of bundling a browser with Electron?
- The reality: Agents excel at the first 90%, but the last 10%—edge cases, real‑world quirks, regressions, and ongoing support—is where costs explode. Maintaining three native codebases (Mac/Win/Linux) triples the surface area for bugs and support.
- Case in point: Anthropic’s much‑touted agent swarm spent ~$20k building a Rust‑based C compiler that flew through early tests but hit a wall on stability and completeness—impressive, yet largely unusable without heavy human cleanup.
- Why Electron wins today: One codebase, familiar web stack, and instant cross‑platform reach outweigh bloat, lag, and weaker OS integration for most teams. The incentives favor shipping once over hand‑holding agents to production‑ready parity across three native apps.
- Bottom line: Spec‑driven, agent‑powered native builds are promising, but the last mile and ongoing maintenance keep Electron in the lead—for now—even for AI leaders like Anthropic.

Based on the discussion, here is a summary of the comments:

**The Insider Perspective**
A commenter identifying as an engineer on the project noted that the team had previous experience with Electron and preferred building non-natively to share code between web and desktop. However, they acknowledged that engineering tradeoffs might change in the future.

**User Experience: Terminal vs. Desktop**
Users drew a sharp distinction between Anthropic's tools.
*   **Claude Code (CLI):** Was described as "magical" and highly effective, even on single terminals.
*   **Claude Desktop (Electron):** Received significant criticism for poor performance. Users reported it turning laptops into "toasters," causing fans to run wildly, and suffering from lag/freezing (one user noted delays of multiple seconds when switching tasks).
*   **Workarounds:** Some users resort to "disposable conversations" or stick strictly to the terminal interface to avoid the resource heaviness of the desktop app.

**The "Coding is Solved" Irony**
A major theme of the discussion was the perceived contradiction between Anthropic’s marketing and their tech stack choices.
*   **The Paradox:** Commenters questioned why, if Claude is capable of "solving coding" or effortlessly porting code between languages, Anthropic cannot use their own agent to maintain three native codebases (Mac/Windows/Linux) instead of relying on Electron.
*   **The Rebuttal:** Others argued that "coding" isn't the bottleneck—maintenance is. Even if AI generates the code, maintaining three separate stateful architectures is a logistical nightmare compared to deploying a single web-stack application.

**The Broader Electron Debate**
The thread evolved into a classic debate over the viability of Electron:
*   **Defenders:** Argued that performance complaints are often hyperbole. They cited **VS Code** and **Gmail** as examples of complex, successful web-stack applications. Some argued that "native app development is dead" outside of gaming and walled gardens (iOS), and that the browser is the only runtime that matters.
*   **Detractors:** Countered that VS Code is an outlier that relies heavily on native modules (Rust/C++) and WebGL optimizations to function well, implying standard Electron apps remain "junk." Users pointed to native alternatives (like Neovim or Thunderbird) as proof of the superior efficiency and speed of native code compared to web technologies.

### How an inference provider can prove they're not serving a quantized model

#### [Submission URL](https://tinfoil.sh/blog/2026-02-03-proving-model-identity) | 67 points | by [FrasiertheLion](https://news.ycombinator.com/user?id=FrasiertheLion) | [48 comments](https://news.ycombinator.com/item?id=47098172)

Tinfoil’s “Modelwrap” aims to solve a long‑standing gripe with inference APIs: you can ask for a specific model, but you can’t really know what you got. Providers can silently swap in different quantizations, tweak context windows under load, or drift over time—something users have observed across vendors and even within the same vendor.

What they built: verifiable inference that binds an API call to an exact set of model weights at runtime, without changing app code.

How it works
- Public commitment to weights: Tinfoil publishes a single Merkle-tree root hash for the model’s weight files (e.g., 140 GB split into 4 KB blocks).
- Enclave attestation, extended to data: They use secure enclaves, but go beyond “what binary booted” by attesting two things at launch: the committed root hash and the presence of an enforcement mechanism.
- Kernel-enforced verification on every read: dm-verity in the Linux kernel checks each disk block read against the Merkle tree; if any byte doesn’t match the committed root, the read fails with an I/O error. Apps like vLLM don’t need modifications and can’t accidentally read uncommitted bytes.
- Client-side verification: On each request, clients can verify the enclave’s attestation report contains the expected root hash and dm-verity configuration, tying the running server to the public commitment.
- Analogy: This is the same mechanism behind Android Verified Boot (root hash + kernel-enforced Merkle checks), repurposed for model weights.

Why it matters
- Proves you’re hitting the exact weights you pinned (no silent quantization or model swaps).
- Stabilizes evals and regression tracking across time/providers.
- Works for closed-source models too: you can’t see the weights, but you can verify you’re getting the same committed bits every time.

Caveats and open questions
- Scope: This guarantees the bytes read from disk match the commitment; it doesn’t by itself prove anything about post-load transformations or runtime configuration unless those are also covered by attestation.
- Trust base: You’re trusting the enclave/CPU vendor’s attestation and kernel integrity.
- Practicalities: Update/rollout mechanics, performance overhead of dm-verity, and how broader server config (e.g., context window, KV cache policies) is pinned weren’t detailed here.

Bottom line: Modelwrap turns “trust us” into “verify us,” giving API users a cryptographic handle (a root hash) they can pin to—and a kernel-enforced path that makes serving anything else fail fast.

The discussion revolves around the technical limitations of "black box" verification (checking outputs) versus the cryptographic verification proposed by Tinfoil, with the author (`FrasiertheLion`) answering questions about the specific security architecture.

**The feasibility of "Output Checking"**
The thread began with users questioning why complex attestation is necessary when users could simply check for deterministic outputs using a fixed seed.
*   **The Consensus:** Commenters (including `trpplyns`, `jshlm`, and `msrblfnc`) argued that checking outputs is unreliable. Floating-point math is not truly associative, meaning the order of operations matters.
*   **Hardware Variance:** At scale, providers split models across different GPU/CPU combinations and use optimizing compilers that change instruction scheduling. This results in slight numerical differences that break strict determinism, making it impossible to distinguish between a benign hardware change and a malicious model swap based on output alone.
*   **Benchmarking issues:** `Aurornis` noted that while external benchmarking sites exist, they are expensive to maintain and often produce noisy data rather than definitive proof of model degradation.

**Attestation Mechanics & Trust**
A significant portion of the discussion focused on how the client effectively trusts the server.
*   **The Mechanism:** `FrasiertheLion` explained that the system relies on hardware-backed enclaves (Intel TDX, AMD SEV-SNP, Nvidia Confidential Computing).
*   **Preventing "Replay" Attacks:** Users (`rbls`, `vrptr`) asked how a client knows the provider isn't faking the attestation report. The author clarified:
    1.  The enclave generates an ephemeral key pair at boot.
    2.  The public key is embedded in the hardware-signed attestation report.
    3.  The client encrypts their request using that public key.
    4.  Only the specific, verified enclave instance can decrypt and process the request, preventing Man-in-the-Middle attacks or spoofed reports.
*   **Trust Anchor:** `jlsdrn` observed that this technology effectively shifts the "root of trust" from the API provider (who might cut corners) to the hardware manufacturer (Intel/AMD), who certifies the chip state.

**Other Notes**
*   **Apple:** There was interest in Apple’s similar approach with "Private Cloud Compute," which users felt offered strong integrity guarantees due to Apple's control over the entire hardware/software stack.
*   **Quantization:** `rbrnd` noted that quantization isn't inherently bad—users often want the trade-off of 99% quality for 50% cost—but the implication is that transparency about *which* version is running remains the key issue.