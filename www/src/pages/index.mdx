import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Sat Oct 18 2025 {{ 'date': '2025-10-18T17:12:11.454Z' }}

### Most users cannot identify AI bias, even in training data

#### [Submission URL](https://www.psu.edu/news/bellisario-college-communications/story/most-users-cannot-identify-ai-bias-even-training-data) | 104 points | by [giuliomagnifico](https://news.ycombinator.com/user?id=giuliomagnifico) | [68 comments](https://news.ycombinator.com/item?id=45629299)

Researchers at Penn State and Oregon State found that laypeople largely fail to notice when training data is racially skewed, even in obvious setups. In experiments with 769 participants using a prototype facial emotion detector, the training data confounded race with emotion (e.g., happy faces mostly white, sad faces mostly Black). Despite seeing the data, most participants said the AI treated groups equally—unless they experienced biased outputs themselves.

What they did
- Built 12 versions of a facial-expression classifier and ran three experiments with images of Black and white individuals.
- Manipulated training data in two ways: confounding race with emotion (e.g., happy=white, sad=Black) and under-representing a race entirely.
- A final experiment mixed biased and counterexample conditions (happy Black/sad white; happy white/sad Black; all white; all Black; no racial confound).

Key findings
- Most participants did not detect bias in the training data across scenarios.
- People noticed bias mainly after seeing biased performance (e.g., misclassifying Black faces).
- Black participants were more likely to flag bias, particularly when their group was overrepresented in negative emotion categories.
- Quote from senior author S. Shyam Sundar: People “trust AI to be neutral, even when it isn’t,” and failed to see the race–emotion confound “even when it was staring them in the face.”

Why it matters
- Transparency alone (showing datasets) may not help typical users spot harmful confounds.
- Bias perception is driven by outcomes, not inputs—raising the bar for pre-deployment audits, fairness testing, and guardrails that prevent spurious correlations from being learned.
- The study underscores that “AI that works for everyone” requires design-time safeguards, not just user oversight.

Published in Media Psychology; authors: Cheng “Chris” Chen (Oregon State), S. Shyam Sundar and Eunchae Jang (Penn State). Date: Oct 16, 2025.

**Summary of Hacker News Discussion:**

1. **Critiques of Study Methodology**:  
   - Users debated the experimental design, arguing that the study used extreme, hyper-focused distributions (e.g., "happy=white, sad=Black") that might not reflect real-world scenarios.  
   - Some questioned whether participants truly understood statistical nuances or if the setup exaggerated biases.  
   - A recurring point: Bias detection often hinges on observing skewed outputs, not just examining training data.  

2. **Bias in Training Data vs. Outcomes**:  
   - Commenters highlighted that AI models trained on biased data inherently reproduce those biases, especially in underrepresented languages or frameworks (e.g., Svelte vs. React code generation).  
   - Concerns were raised about commercial models embedding biases through configuration files or defaults, leading to outputs that subtly disadvantage marginalized groups (e.g., HR tools misrepresenting demographics).  

3. **Identity and Terminology Debates**:  
   - A heated thread debated capitalization ("Black" vs. "black") and the cultural/political implications of racial labels. Critics argued that "Black" as an identity in the U.S. stems from shared historical trauma (e.g., slavery), while others dismissed "White culture" as a flawed concept.  
   - Pushback emerged against American-centric views of race, with some noting global diversity in racial and cultural identities.  

4. **User Awareness and Critical Thinking**:  
   - Many agreed that lay users struggle to detect bias without explicit examples of flawed performance.  
   - Some emphasized that addressing bias requires proactive critical thinking and self-awareness, which typical users (and even developers) often lack.  

5. **Societal and Political Implications**:  
   - Users compared AI bias to media bias, noting how people’s perceptions of neutrality are shaped by their own beliefs (e.g., conservatives vs. liberals accusing AI of opposing biases).  
   - Confirmation bias was cited as a key challenge, with users favoring outputs that align with their preexisting views.  

**Key Takeaways**:  
- Technical debates centered on study validity and AI’s reflection of training data.  
- Cultural discussions underscored the complexity of racial identity in AI representation.  
- Broad consensus: Detecting bias requires more than transparency—it demands rigorous auditing, diverse training data, and user education to mitigate harm.

---

## AI Submissions for Fri Oct 17 2025 {{ 'date': '2025-10-17T17:12:44.799Z' }}

### Andrej Karpathy – It will take a decade to work through the issues with agents

#### [Submission URL](https://www.dwarkesh.com/p/andrej-karpathy) | 990 points | by [ctoth](https://news.ycombinator.com/user?id=ctoth) | [875 comments](https://news.ycombinator.com/item?id=45619329)

Andrej Karpathy on Dwarkesh: “AGI is still a decade away” and why this is the decade of agents, not the year

Key takeaways
- Decade of agents: Karpathy pushes back on hype that “this is the year of agents.” To be genuinely useful as intern/employee-level assistants, agents still need major upgrades: reliable computer/tool use, strong multimodality, persistent memory, and continual learning.
- Timelines: “The problems are tractable, but they’re still difficult.” Based on ~15 years in AI, he pegs the path to capable agents and AGI at roughly a decade—not 1 year, not 50.
- Cognitive gaps in LLMs: Today’s models are impressive but lack stable memory, on-the-fly learning, and robust reasoning—shortfalls that limit real-world autonomy.
- RL is “terrible” (but best available): Reinforcement learning remains sample-inefficient and finicky, yet alternatives are even less practical for aligning complex behaviors.
- Model collapse: Heavy training on model-generated data can degrade learning dynamics, a key reason LLMs won’t simply scale into human-like learners without fresh, high-signal data.
- Macro impact: He expects AGI’s economic effects to blend into the historical ~2% GDP growth trend rather than trigger a sharp step change overnight.
- Self-driving lessons: The slog to autonomy underscores how edge cases, reliability, and real-world integration dominate late-stage timelines.
- Education’s future: Anticipates agent-tutors and personalized curricula once memory, adaptation, and tool use mature.

Why it matters
- A sober, experience-based forecast from a leading practitioner: rapid progress, but many non-glamorous engineering and data hurdles remain before agents can truly “work.”
- Signals near-term priorities for labs and startups: memory, continual learning, tool use, multimodality, and high-quality data curation.

Listen/watch: YouTube, Apple Podcasts, Spotify (episode posted Oct 17, 2025).

**Summary of Discussion:**

The discussion delves into philosophical and technical debates sparked by Karpathy’s claims about AGI timelines and agent limitations. Key themes include:

1. **LLMs vs. World Models**:  
   - Participants debate whether LLMs truly “understand” the world or merely compress text data. Some reference Rich Sutton and Yann LeCun’s arguments that LLMs lack **grounded world models** (e.g., simulating physics, causality), unlike biological systems (e.g., squirrel brains) that evolve through direct environmental interaction.  
   - Skepticism arises about equating LLMs with AGI, as they lack **persistent memory**, continual learning, and embodied experiences.

2. **Intelligence Metrics**:  
   - Critics argue Sutton’s “squirrel” analogy for AGI is semantically vague and lacks quantitative benchmarks. Others counter that LLMs’ text compression still reflects a form of intelligence, albeit narrow compared to humans’ multimodal, feedback-driven abstraction.

3. **Consciousness and Reductionism**:  
   - A subthread explores whether AI can achieve consciousness. Some analogize human cognition to emergent properties of cells (e.g., photoreceptors processing light), while others stress that LLMs lack **self-preservation, adaptation, or sensory grounding** — key traits of living systems.  
   - Debates touch on reductionism vs. emergent phenomena, with references to Kantian philosophy and the “map vs. territory” problem.

4. **Aesthetics and Training Data**:  
   - The discussion whimsically pivots to why humans find the night sky beautiful. Some attribute this to evolutionary training (e.g., associating clear skies with survival), while others argue beauty transcends utility. Critics note LLMs might mimic such associations but lack genuine subjective experience.

5. **Technical Hurdles**:  
   - Participants align with Karpathy’s emphasis on **memory, tool use, and multimodal integration** as critical gaps. Reinforcement learning’s inefficiency and the risk of “model collapse” from synthetic data are cited as barriers to agent autonomy.

**Why It Matters**:  
The conversation underscores the interdisciplinary challenges of AGI, blending technical critiques (e.g., world modeling, data quality) with philosophical questions about intelligence and consciousness. It reinforces Karpathy’s argument that agents require foundational advances beyond scaling LLMs, while highlighting divergent views on how to define and measure progress toward human-like AI.

### Claude Skills are awesome, maybe a bigger deal than MCP

#### [Submission URL](https://simonwillison.net/2025/Oct/16/claude-skills/) | 633 points | by [weinzierl](https://news.ycombinator.com/user?id=weinzierl) | [330 comments](https://news.ycombinator.com/item?id=45619537)

Headline: Claude Skills: simple, load-on-demand “packages” that turn Claude into a practical general agent

- What’s new: Anthropic introduced Claude Skills—folders containing a Markdown “how-to” plus optional scripts and resources. Claude scans each skill’s YAML frontmatter at session start (just a few dozen tokens per skill) and only loads full details when relevant. Anthropic published an official anthropics/skills repo; their document-creation features (.pdf, .docx, .xlsx, .pptx) are implemented as skills.

- Why it’s interesting: The model gains specialized abilities (e.g., Excel work, brand guidelines) without bloating prompts. The pattern is conceptually simple and easy to iterate on—more like shareable “playbooks” than complex plugins.

- Real-world test: Willison tried the slack-gif-creator skill with Sonnet 4.5. The Python script imports the skill’s core library, generates an animated GIF, and validates Slack constraints (e.g., under 2MB). The first result was ugly, but the workflow shows how quickly skills can be refined.

- Big caveat: Skills depend on a sandboxed coding environment with filesystem and command execution. That’s powerful—and raises familiar safety concerns (prompt injection, isolation). Still, it unlocks a lot compared to MCP/plugins.

- Takeaway: Skills feel like a pragmatic path to “agents” (tools-in-a-loop). Claude Code looks less like just a coding tool and more like general computer automation. The simplicity is the point—and could make Skills a bigger deal than MCP.

The discussion revolves around the challenges and evolving role of documentation in software development, particularly with AI tools like Claude Skills. Key points include:

1. **Context & Maintenance Challenges**:  
   - Documentation often becomes outdated quickly due to changing contexts ("context window" problem).  
   - Writing docs in advance risks wasted effort if assumptions prove wrong, emphasizing the need for *testable*, *current-context* documentation.  
   - Stable systems (e.g., libraries, dependencies) are exceptions where long-term docs add value.

2. **AI’s Role**:  
   - **Potential benefits**: AI can reduce documentation costs, auto-generate explanations, and verify consistency (e.g., flagging outdated docs in PRs).  
   - **Risks**: Over-reliance on AI may erode human understanding, create misaligned incentives (e.g., docs for AI vs. humans), and introduce errors if unchecked.  
   - **Debate**: Some argue AI-generated docs (verified by humans) are more reliable than traditional ones, while others stress the irreplaceable value of human context and reasoning.

3. **Human Factors**:  
   - **Principal-agent problem**: Developers may write docs for AI consumption rather than human clarity, risking disconnects in team discipline.  
   - **Job security**: Poor documentation can make developers replaceable, incentivizing "knowledge hoarding."  
   - **Organizational trust**: High-friction, low-trust environments hinder documentation quality, whereas small teams with shared missions prioritize it.

4. **Practical Solutions**:  
   - Integrate docs into code review processes.  
   - Use tests as living documentation.  
   - Focus on self-explanatory code with minimal, high-value comments explaining *why* (not just *what*).  

**Takeaway**: Documentation’s future lies in balancing AI efficiency with human context, ensuring clarity, testability, and alignment with real-world use cases.

### Asking AI to build scrapers should be easy right?

#### [Submission URL](https://www.skyvern.com/blog/asking-ai-to-build-scrapers-should-be-easy-right/) | 125 points | by [suchintan](https://news.ycombinator.com/user?id=suchintan) | [66 comments](https://news.ycombinator.com/item?id=45620653)

Skyvern teaches itself to code: 2.7x cheaper, 2.3x faster browser automations

- What’s new: Skyvern can now generate and maintain its own Playwright code from prompts, using reasoning models to turn one “explore” run into a deterministic, reusable script. It falls back to the agent only when something novel happens.
- Why it matters: Removes an LLM from the hot path on every run, making automations faster, cheaper, and more reliable—especially on messy, change-prone sites.
- How it works:
  - Explore mode: the agent navigates a flow, learns coupled interactions (e.g., linked radio buttons), records a trajectory, and captures intent metadata (the “why,” not just the “what”).
  - Replay mode: compiles those learnings into Playwright for deterministic execution; uses targeted fallbacks to handle outages, DOM shifts, or unexpected branches.
- Real-world example: Registering for an EIN via Delaware/IRS forms. Naive scripts break on linked choices and nighttime outages; Skyvern infers the branching logic at runtime, encodes it, and recovers gracefully when the portal changes or is down.
- Why reasoning models matter: They boost accuracy to production levels and let the agent write engineer-like scripts that reflect its exploration and intent.
- Availability: Open source and Cloud options.

Bottom line: This is a record→compile→repair loop for web automation—self-healing scrapers with auditable Playwright code and LLMs only where they add value.

The Hacker News discussion on Skyvern's browser automation tool highlights several key themes:

1. **Efficiency & Practicality**:  
   - Skyvern's ability to generate/maintain Playwright scripts via exploration and deterministic replay is praised for reducing LLM dependency, lowering costs, and improving reliability. Users note its potential for handling dynamic, real-world sites (e.g., IRS forms) gracefully.

2. **Debates on LLM Limitations**:  
   - Skepticism arises about LLMs’ ability to handle formal logic and deterministic programming. Critics argue LLMs rely on pattern recognition rather than true reasoning, struggling with abstraction and context shifts. Some suggest pairing LLMs with symbolic logic systems for robustness.

3. **Comparisons to Human Learning**:  
   - A philosophical debate emerges: while some liken Skyvern’s exploration to a child learning through trial/error, others emphasize LLMs lack embodied experience or intentionality, limiting their “learning” to statistical text manipulation.

4. **Technical Challenges**:  
   - Users discuss hurdles like JavaScript-heavy sites (e.g., Cloudflare-protected pages) and advocate for reverse-engineering APIs instead of browser automation. Concerns about brittle scrapers persist despite self-healing claims.

5. **Legal & Ethical Concerns**:  
   - Automating sensitive tasks (e.g., IRS forms) raises red flags about legality and risk, though proponents argue public web forms are fair game.

6. **Skepticism vs. Optimism**:  
   - While some herald Skyvern as a breakthrough for reducing maintenance, others remain wary of fully replacing human expertise, stressing the need for hybrid systems combining LLMs with traditional code.

Overall, the discussion reflects cautious optimism about Skyvern’s approach but underscores broader challenges in AI-driven automation, particularly around reliability, adaptability, and the inherent limitations of LLMs.

### AI has a cargo cult problem

#### [Submission URL](https://www.ft.com/content/f2025ac7-a71f-464f-a3a6-1e39c98612c7) | 172 points | by [cs702](https://news.ycombinator.com/user?id=cs702) | [128 comments](https://news.ycombinator.com/item?id=45618350)

The FT piece argues that much of today’s AI hype mimics the outward trappings of progress—flashy demos, leaderboard wins, and ritualized “best practices”—without reliably delivering durable value. Think cargo cult science: copying the form of intelligence (prompts, agents, benchmark badges) instead of verifying substance.

What the article is getting at
- Surface over substance: Teams replicate viral techniques (chain-of-thought, agents, RAG “sprinkles”) and cherry-picked demos that don’t survive real workloads, edge cases, or cost/latency constraints.
- Benchmark theater: Synthetic leaderboards and narrow tasks are optimized to a fault, while real-world reliability, safety, and maintenance are under-measured.
- Misaligned incentives: Marketing-driven releases, demo-first roadmaps, and “GPU burn” as status symbols encourage ritual rather than rigor.
- Enterprise AI theater: Pilots abound, but few ship robustly; success criteria are vague, and post-deployment telemetry is thin.

Why it matters
- Wasted spend and trust erosion: Organizations burn time and money on brittle solutions, souring stakeholders on genuinely useful AI.
- Policy and investment misfires: Decisions based on demos and vanity metrics can skew capital allocation and regulation.
- Slower real progress: Overfitting to leaderboards crowds out hard engineering on data quality, evaluation, and reliability.

What to do instead
- Define success up front: Tie tasks to ground truth and business metrics (quality, latency, cost, reliability). Compare against strong non-AI baselines.
- Make evals adversarial and reproducible: Pre-register test sets, include edge cases and distribution shifts, run ablations, report error bars.
- Measure total cost of ownership: Track token spend, infra, human oversight, drift management, and failure recovery.
- Prefer the simplest thing that works: Smaller/cheaper models, retrieval and data fixes before bigger models, explicit guards and fallbacks.
- Operate it like a system: Observability, feedback loops, red-teaming, human-in-the-loop where stakes are high.

A fair counterpoint
- Not all progress is theater: There are real gains in coding assistance, summarization, and search—especially when teams do the unglamorous work on data, UX, and ops.
- Demos can be useful probes—provided they’re followed by rigorous evaluation and iteration.

Bottom line
The piece is a call to swap ritual for rigor: value real-world metrics over demo vibes, and build AI systems that stand up to adversarial tests, costs, and time—not just conference stages.

**Summary of Discussion:**

The discussion revolves around skepticism toward AI's current hype cycle, mirroring the FT article's concerns about prioritizing form over substance. Key points include:

1. **Skepticism & Overhyped Claims**:  
   - Many users express fatigue with AI/LLM hype, noting inflated promises versus real-world utility. Critics argue that while AI tools (e.g., ChatGPT) offer productivity gains, they often fail in reliability, accuracy, and ethical alignment. Examples include hallucinations in web searches and nonsensical code generation.  
   - Concerns are raised about companies prioritizing investor-driven metrics (e.g., user growth, token spend) over genuine problem-solving, likening AI adoption to a "cargo cult" of superficial practices.

2. **Data Privacy & Exploitation**:  
   - Debates emerge over whether AI companies exploit user data, with accusations of unethical scraping (e.g., training on personal data without consent). Some counter that enterprise contracts often include data clauses, though skepticism remains about compliance and transparency.

3. **Mixed Practical Experiences**:  
   - Developers share positive anecdotes, such as using AI to automate scripting tasks (e.g., AWS CLI/SDK integration), saving significant time despite occasional errors. Others highlight frustrations with AI-generated inaccuracies requiring manual correction, questioning overall efficiency gains.  
   - A recurring theme is the difficulty in quantifying AI's value: while some users report saving hours on research or coding, others argue these benefits are overstated or context-dependent.

4. **Comparisons to Academic & Industry Failures**:  
   - Users draw parallels between AI's reliability issues and broader systemic problems, such as the replication crisis in academia. Critics note that AI’s error rates (e.g., 40% inaccuracies) might be no better—or worse—than human errors in fields like medical or legal research.

5. **Market Dynamics & Ethics**:  
   - Discussions touch on the concentration of power among major AI players (OpenAI, Anthropic) and their subsidized models, raising concerns about long-term sustainability and monopolistic practices. Some predict a "bubble" akin to the dot-com crash if hype outpaces real utility.  

**Conclusion**:  
The thread reflects a tension between acknowledging AI's potential (e.g., coding assistance, summarization) and critiquing its overhyped, often unproven applications. Calls for rigorous evaluation, transparency, and ethical practices align with the FT article’s push for substance over spectacle. However, personal experiences vary widely, underscoring the technology’s uneven adoption and impact.

### OpenAI Needs $400B In The Next 12 Months

#### [Submission URL](https://www.wheresyoured.at/openai400bn/) | 254 points | by [chilipepperhott](https://news.ycombinator.com/user?id=chilipepperhott) | [235 comments](https://news.ycombinator.com/item?id=45619544)

The take: Zitron argues OpenAI’s newly signaled buildout—spanning Broadcom custom chips, AMD MI450s, NVIDIA’s next-gen “Vera Rubin,” and multiple “Stargate” data centers—implies roughly 33 GW of capacity and would require capital on the order of hundreds of billions, with ~$400B needed in the next year to make promised 2026–2029 timelines remotely plausible.

Key points
- Cost math escalates: He updates to ~$50B per gigawatt of data center capacity (including GPUs, networking, buildings, power infrastructure), up from earlier ~$32.5B estimates. Back-of-the-envelope: ~333k Blackwell GPUs per GW at ~$60k each is ~$20B before networking/power/buildings.
- Time and supply-chain constraints: GW-scale sites typically take ~2–2.5 years; even if money existed, transformers, electrical-grade steel, grid interconnects, and specialized labor are bottlenecks.
- 2026 milestones he deems unrealistic:
  - Broadcom/OpenAI inference chip taped out and enough units manufactured to fill a 1 GW site in H2’26.
  - 1 GW of AMD Instinct MI450s deployed in H2’26.
  - 1 GW of NVIDIA Vera Rubin systems deployed in H2’26.
  For any of this, he says, construction and power procurement should already be well underway.
- Site reality checks: Some locations remain unnamed; a Lordstown, OH facility cited in local reporting is “not a full-blown data center,” undermining the scale implied by announcements.
- Skepticism on financing/optics: He frames vendor and partner statements as market-pleasing optics rather than executable plans, pointing to NVIDIA’s rising accounts receivable as a potential red flag about customer financing.

Why it matters
- If these roadmaps are overstated, the repercussions could hit chipmakers, data-center builders, utilities, and investors banking on uninterrupted AI capex hypergrowth.
- The piece challenges mainstream coverage for treating multi-GW pledges as straightforwardly achievable.

Caveats
- Highly opinionated and combative in tone; many figures rest on author assumptions (e.g., $50B/GW) and partial public disclosures.

What to watch
- Concrete site announcements, grid interconnect agreements, and transformer orders.
- Actual tapeouts/volume production for Broadcom’s OpenAI chip, AMD MI450, and NVIDIA Vera Rubin.
- Capital raises, JV structures, or vendor financing that could bridge the funding gap—or any walk-backs and delays.

**Summary of Hacker News Discussion on OpenAI's $400B Funding Needs and Growth Claims:**

The discussion revolves around skepticism and debate over OpenAI's reported growth metrics, infrastructure challenges, and the broader utility of AI tools like ChatGPT. Key points include:

1. **Productivity vs. Usage**:  
   - Users question whether high adoption (e.g., 100M monthly users) equates to **real productivity gains**. Critics argue frequent usage (e.g., 26 messages/day) doesn’t inherently prove value, comparing it to social media or "sugar"—habitual but not necessarily transformative.  
   - Others counter that sustained user engagement (e.g., weekly usage) signals practical utility, especially in professional contexts like coding or research.  

2. **Growth Metrics Scrutiny**:  
   - OpenAI’s reported **700M-800M weekly active users** are met with doubt. Comparisons are drawn to social media platforms (Instagram, TikTok) that boast large user bases but varied utility.  
   - Questions arise about monetization: Can free users convert to paid ($20/month) at scale? Some note even a 5% conversion rate would be surprisingly high.  

3. **Infrastructure and Competition**:  
   - Concerns about the **energy and financial costs** of scaling AI infrastructure (e.g., GPUs, data centers) are highlighted, with parallels drawn to crypto/NFT bubbles.  
   - Competition from alternatives like **Grok, Mistral, and Gemini** is noted, with users citing tool-switching for cost or quality reasons.  

4. **Societal Impact and Priorities**:  
   - Critics contrast AI’s “luxury” focus with **unmet basic needs** (clean water, housing), calling tech investment myopic. Others defend AI’s potential to democratize access to advanced tools globally.  

5. **Financial Realities**:  
   - Skepticism about **investor enthusiasm** driving unrealistic valuations, with comparisons to past tech hype cycles. Questions linger about return on investment (ROI) for AI’s massive capex.  

6. **Tone and Sentiment**:  
   - The debate is polarized: Some dismiss growth claims as marketing hype, while others defend AI’s transformative potential. A recurring theme is the difficulty of measuring “usefulness” objectively.  

**Key Takeaway**: The discussion underscores deep divides over AI’s practical value, financial sustainability, and prioritization in a world with pressing systemic challenges. While critics demand proof of ROI and societal benefit, proponents emphasize AI’s evolving utility and long-term potential.

---

## AI Submissions for Thu Oct 16 2025 {{ 'date': '2025-10-16T17:16:52.000Z' }}

### DoorDash and Waymo launch autonomous delivery service in Phoenix

#### [Submission URL](https://about.doordash.com/en-us/news/waymo) | 288 points | by [ChrisArchitect](https://news.ycombinator.com/user?id=ChrisArchitect) | [650 comments](https://news.ycombinator.com/item?id=45605501)

DoorDash x Waymo: autonomous delivery pilot in Phoenix + $10 Waymo ride perk for DashPass

- What’s new: DoorDash is testing fully autonomous deliveries with Waymo in Metro Phoenix now, aiming for broader commercial ops later this year. Early rollout starts with DashMart orders; some customers may be matched with a driverless Waymo vehicle via DoorDash’s Autonomous Delivery Platform (which orchestrates Dashers, robots, drones, and AVs).

- Member promo: DashPass members in LA, SF, and Phoenix get $10 off one Waymo ride per month through Dec 31, 2025. A new promo code is issued at the start of each month. Valid on weekday rides booked between 2 a.m. and 2 p.m.; terms apply.

- Why it matters: 
  - Signals DoorDash’s push toward a multimodal, automated last mile (and follows its Oct 9 partnership to bring Serve Robotics’ delivery robots onto the platform).
  - Phoenix remains a key AV testbed; this ties robo‑taxis directly into mainstream delivery commerce.
  - If it scales, it could change delivery unit economics and labor mix; for now it’s limited in geography, merchants (DashMart), and scope.

- Fine print: The AV delivery is a test; timelines and expansion are subject to change (forward‑looking statements). Promo is time‑windowed and limited to one discounted ride per month.

The discussion revolves around the challenges faced by small restaurants in cities with high minimum wages, such as Seattle and Denver, and broader economic implications:

1. **Impact of High Minimum Wages**:  
   - Critics argue that elevated minimum wages strain small restaurants, forcing price hikes and reducing customer traffic. Some claim this favors large chains (e.g., McDonald’s) with better labor efficiency, squeezing out independent eateries.  
   - Counterarguments assert businesses unable to pay living wages “shouldn’t exist,” emphasizing ethical labor practices over profitability.  

2. **Commercial Rent and Urban Costs**:  
   - Many highlight **skyrocketing commercial rents** and zoning restrictions as critical issues, arguing these costs outweigh wage pressures. Corporate landlords and real estate speculation are blamed for displacing small businesses.  
   - Suggestions include a **Land Value Tax** to deter rent-seeking and zoning reforms to increase housing density, lowering operational costs.  

3. **Drone Delivery and Automation**:  
   - A tangent proposes drone delivery as a cost-saving model (à la Ryanair), but skeptics note logistical hurdles (e.g., air traffic management for millions of packages).  

4. **Systemic Solutions**:  
   - Ideas like **Universal Basic Income (UBI)** and worker-owned cooperatives emerge as alternatives to wage mandates, aiming to reduce reliance on low-wage labor.  
   - Others blame urban desirability and immigration for inflating housing/rental markets, exacerbating small-business struggles.  

5. **Geographic Examples**:  
   - Seattle’s housing shortage and construction costs are dissected, with mixed views on whether zoning reforms (e.g., allowing multi-family units) have helped.  
   - Australia is cited as a counterpoint, where high wages coexist with thriving small restaurants, suggesting other factors (e.g., rent control) might be at play.  

**Key Tensions**: The debate reflects ideological divides—pro-labor vs. pro-business perspectives, with systemic critiques of capitalism (e.g., corporate consolidation, rentier economies) underpinning many arguments. The DoorDash-Waymo pilot, while not directly addressed, symbolizes the automation trend that could further disrupt labor dynamics in delivery services.

### Codex Is Live in Zed

#### [Submission URL](https://zed.dev/blog/codex-is-live-in-zed) | 258 points | by [meetpateltech](https://news.ycombinator.com/user?id=meetpateltech) | [56 comments](https://news.ycombinator.com/item?id=45606698)

Zed adds OpenAI Codex via ACP, open-sources adapter

- The Zed IDE now supports OpenAI’s Codex out of the box through the Agent Client Protocol (ACP), joining existing integrations like Claude Code and Google’s Gemini CLI. You can pick Codex from the New Thread menu.
- Privacy and billing: Zed doesn’t proxy requests or charge for external agents—your prompts/code go directly to OpenAI, and you pay OpenAI directly.
- The codex-acp adapter is open-sourced, so Codex via ACP can be used outside Zed as well.
- Implementation notes: Codex runs terminal commands inside the agent and streams output to the client, unlike other agents that ask the client to run commands. This surfaces tradeoffs:
  - PTY mode (client-run): interactive, colorful output, but can deadlock agents (e.g., git rebase --continue opening an editor).
  - Non-PTY (agent-run): fewer colors/less interactivity, but fewer “stuck” states.
- ACP momentum: Originally built with the Gemini CLI team, ACP is now being adopted across editors (Neovim, Emacs, JetBrains). Zed plans to focus on evolving the protocol with the community rather than building more adapters.

Why it matters: ACP is quickly becoming a cross-editor standard for AI coding agents, and Zed’s privacy-first, open-source adapter approach makes it easier for developers to choose their agent without switching tools.

**Summary of Hacker News Discussion on Zed's OpenAI Codex Integration via ACP:**

1. **Performance and Feature Comparisons**  
   - Users noted Zed's speed but highlighted **pain points with Python completions** and file navigation compared to JetBrains IDEs (e.g., PyCharm).  
   - **C# support via OmniSharp** was criticized as slow for larger projects, prompting discussions about alternatives like Rider or VS Code.  
   - Missing **Jupyter notebook support** was raised as a barrier to adoption, though some suggested plugin possibilities.  

2. **AI Integration and Quality**  
   - **Codex via CLI** was perceived as slower than Claude and Gemini, with mixed feedback on its utility.  
   - **In-line AI suggestions** faced criticism:  
     - Users compared Zed unfavorably to **Cursor** and JetBrains AI, citing weak renaming/module refactoring support.  
     - Some argued that AI should complement—not replace—traditional LSP-driven features like semantic search.  
   - Skepticism emerged about relying on AI for deterministic tasks (e.g., code renaming), with calls to prioritize accuracy over novelty.  

3. **Pricing and Privacy**  
   - Confusion arose over Zed’s **$10/month subscription vs. $5 AI credits model**. Clarifications highlighted separate billing for AI providers (OpenAI, Claude).  
   - Privacy practices (direct API calls to OpenAI, no proxying) were praised, though debates surfaced about AI-generated comment detection.  

4. **Community and ACP Adoption**  
   - Enthusiasm for **ACP becoming a cross-editor standard** (Neovim, Emacs, JetBrains) but calls for Zed to focus on **improving core IDE features** rather than building more adapters.  
   - GitButler comparisons sparked interest in collaborative workflows, though users questioned its required workflow changes.  

5. **User Workflow Preferences**  
   - Some advocated disabling AI entirely, favoring **traditional snippets, regex, and multi-cursor edits** for reliability.  
   - Others expressed frustration with Zed’s **learning curve for keyboard shortcuts** and context-aware features.  

6. **Criticisms and Requests**  
   - Requests for **Windows optimizations** and clearer release cycles (LTS vs. rapid updates).  
   - Mixed reactions to Zed’s UI/UX: praise for its minimalist design but complaints about **"logger-like" file exploration** and diff-view limitations.  

**Conclusion**: The discussion reflects **cautious optimism** for ACP’s potential and Zed’s privacy-first approach, tempered by critiques of its current AI implementation and niche IDE shortcomings. Users emphasized balancing innovation with refining core functionality (e.g., LSP performance, language support) to compete with established tools.

### Gemini 3.0 spotted in the wild through A/B testing

#### [Submission URL](https://ricklamers.io/posts/gemini-3-spotted-in-the-wild/) | 401 points | by [ricklamers](https://news.ycombinator.com/user?id=ricklamers) | [255 comments](https://news.ycombinator.com/item?id=45607758)

Rumored Gemini 3.0 surfaces in Google AI Studio A/B test, excels at SVG generation

- What happened: A user reports catching an A/B test in Google AI Studio that appears to expose “Gemini 3.0.” Using a simple prompt to “Create an SVG image of an Xbox 360 controller,” the model produced a notably high‑fidelity SVG—better than current frontier models in their experience.

- Why it matters: SVG/structured drawing has emerged as a surprisingly strong proxy for overall model capability (popularized by Simon Willison’s “pelican riding a bicycle” test). Strong SVG suggests better spatial reasoning, precision, and code/format adherence—skills that often correlate with coding performance, a key expectation for Gemini 3.0.

- Details:
  - Prompt: “Create an SVG image of an Xbox 360 controller. Output it in a Markdown multi-line code block.”
  - Reported model ID: ecpt50a2y6mpgkcn (not clearly indicative of version).
  - Performance deltas vs Gemini 2.5 Pro: ~+24s time-to-first-token; ~40% longer output (including apparent reasoning tokens).
  - Author speculates the A/B was likely Gemini 3.0 Pro vs 2.5 Pro; a 3.0 Flash vs 2.5 Pro matchup seems less likely.

- Caveats:
  - N=1 anecdote; A/B access appears sporadic.
  - Model ID isn’t definitive; Google hasn’t announced details.
  - Longer latency/output doesn’t necessarily imply heavy test-time compute—just different generation behavior.

Bottom line: If accurate, early signs point to Gemini 3.0 making a visible leap in structured/code-like generation, bolstering hopes for improved coding performance ahead of any official release.

**Summary of Discussion:**

The Hacker News discussion revolves around the rumored Gemini 3.0's SVG generation capabilities and expands into broader debates about AI model performance, creativity, and practical applications. Key points include:

1. **Model Comparisons and Strengths**:  
   - Users compare Gemini 2.5 Pro, Claude Opus, GPT-5, and DeepSeek in creative writing and reasoning tasks.  
   - Gemini is praised for technical tasks (e.g., summarizing papers, HTML/CSS) but criticized for weaker creative writing and poetry compared to Claude or GPT-5.  
   - Some note Gemini’s ability to handle large token contexts, making it useful for technical documentation and structured outputs like SVG generation.

2. **Creativity vs. Determinism**:  
   - Adjusting parameters (temperature, top_p, top_k) significantly impacts creativity. Lower settings yield predictable outputs, while higher values produce more poetic or nonsensical text.  
   - Users joke about AI-generated "slackborn" poetry, referencing historical examples like Racter (1980s procedural poetry) and literary figures like Paul Celan.  

3. **Practical Applications**:  
   - AI’s role in creative workflows is debated. Some use models for brainstorming RPG campaigns or drafting content, though outputs often lack polish.  
   - For collaborative storytelling (e.g., D&D), AI-generated characters/worlds are seen as useful starting points but require human refinement.  

4. **Technical Limitations**:  
   - Concerns arise about AI’s tendency to "spiral" into incoherence, especially in creative tasks.  
   - Users critique Gemini’s latency and occasional inconsistency in code generation compared to competitors.  

5. **Philosophical Reflections**:  
   - Debates emerge about whether "good writing" hinges on human intent or reader perception, with some arguing AI’s role is to augment, not replace, creativity.  

**Bottom Line**: While excitement exists for Gemini 3.0’s potential in structured tasks like SVG/code generation, the discussion underscores ongoing challenges in balancing AI creativity, reliability, and practical utility. Historical parallels and technical parameter debates highlight the community’s nuanced views on progress in AI capabilities.

### Claude Skills

#### [Submission URL](https://www.anthropic.com/news/skills) | 743 points | by [meetpateltech](https://news.ycombinator.com/user?id=meetpateltech) | [394 comments](https://news.ycombinator.com/item?id=45607117)

Anthropic launches Agent Skills: on-demand “skill packs” that make Claude better at specific jobs across apps, Claude Code, and the API.

- What it is: Skills are folders with instructions, scripts, and resources (SKILL.md + assets). Claude scans available skills and auto-loads only what’s needed, stacking multiple skills when relevant.
- Why it matters: Moves beyond ad‑hoc prompting to reusable, governed workflows—more predictable outputs on tasks like Excel, PowerPoint, Word, and fillable PDFs. Think “packaged expertise” you can share across teams.
- Under the hood: Skills can run executable code inside the Code Execution Tool beta sandbox. They’re composable, portable, and efficient (minimal loading). You can see which skills were invoked as Claude works.
- For users: Available in Claude apps for Pro, Max, Team, and Enterprise. A “skill-creator” guides you through building skills—no manual file editing. Admins must enable Skills org‑wide for Team/Enterprise.
- For developers: Add skills to Messages API requests; manage versions via the new /v1/skills endpoint and the Claude Console. Install via the anthropics/skills marketplace or manually at ~/.claude/skills. Supported in the Claude Agent SDK.
- Early adopters: Box, Notion, and Canva highlight faster, more consistent outputs; finance teams report multi-spreadsheet reviews and reporting dropping from a day to about an hour.
- What’s next: Simplified creation and enterprise-wide deployment. Caveat: Skills execute code—use trusted sources and be mindful of data security.

Docs, console, and example skills are available to get started.

The discussion around Anthropic's Claude Skills reveals several key themes and debates:

**1. Comparisons to Model Control Protocol (MCP):**  
Many commenters contrast Claude Skills with the open-source MCP framework. Skills are seen as more proprietary and client-focused, while MCP emphasizes a server-client architecture with discoverable prompts. Some view Skills as a streamlined alternative to MCP's complexity, praising their folder-based structure (`SKILL.md` + assets) over JSON configurations. However, critics argue Skills lack MCP's standardized tooling ecosystem.

**2. Technical Implementation:**  
- Skills leverage a **directory pattern** similar to `AGENTSmd`/VSCode's agent system, allowing dynamic context-aware loading.  
- The **code execution sandbox** (via Code Interpreter) enables practical workflows like PDF processing or spreadsheet automation.  
- Users highlight efficient context management, with Skills loading only relevant instructions to avoid token bloat.  

**3. Enterprise Adoption & Use Cases:**  
- Early adopters like finance teams report **90% time savings** (e.g., multi-spreadsheet analysis reduced from a day to an hour).  
- Enterprise admins appreciate governance features—skills can be centrally managed via API endpoints and require org-wide enablement.  

**4. Security Concerns:**  
Warnings emerge about **code execution risks**, echoing Anthropic's caveat. Users stress the need to vet skill sources, as malicious skills could exploit sandbox access.

**5. Community Reactions:**  
- **Positives**: Praise for simplified prompt engineering, reusable workflows, and Claude Console integration. Examples like [PDF processing skills](https://github.com/anthropic/skills/tree/main/pdf-document-skill) demonstrate tangible utility.  
- **Criticisms**: Some see Skills as "hyped framework appendices" that merely inject text prompts, questioning if they meaningfully differ from existing prompt-chaining techniques.  

**6. Ecosystem Integration:**  
- Skills complement tools like **Cursor Rules** and **Linear** for task-specific context retrieval.  
- Developers note parallels with ChatGPT's "Projects" but highlight Claude's tighter focus on task-specific skill stacking.  

**7. Future Directions:**  
Debates arise about whether Skills will evolve toward **sub-agent orchestration** or remain focused on prompt templating. Simon Willison's [analysis](https://simonwillison.net/2025/Oct/16/claude-skills/) suggests Skills could mature into a standardized "package manager" for LLM capabilities.  

In summary, Claude Skills are viewed as a pragmatic step toward modular, enterprise-safe AI workflows, albeit with lingering questions about differentiation from existing paradigms like MCP and the long-term vision for AI agent ecosystems.

### New coding models and integrations

#### [Submission URL](https://ollama.com/blog/coding-models) | 215 points | by [meetpateltech](https://news.ycombinator.com/user?id=meetpateltech) | [65 comments](https://news.ycombinator.com/item?id=45601834)

Ollama adds big-code models and one-click IDE integrations

- New models: GLM-4.6 and Qwen3-Coder-480B are now on Ollama Cloud; Qwen3-Coder-30B is updated for faster, more reliable tool-calling in Ollama’s new engine.
- Local if you dare: Qwen3-Coder-480B can run on your machine if you’ve got more than 300GB of VRAM; otherwise use the cloud variants.
- Plug into your editor: Native hooks for VS Code (Copilot Chat → Provider: Ollama), Zed (now on Windows; LLM providers → Ollama, host http://localhost:11434), and Droid (factory.ai CLI with simple model switching). Docs also cover Cline, Roo Code, and more.
- Quickstart: ollama run glm-4.6:cloud or ollama run qwen3-coder:480b-cloud; or ollama pull those models to make them selectable inside your IDE.
- Cloud API: Create an API key and hit ollama.com/api/chat with Bearer auth to use cloud models directly from your apps.
- Why it matters: You get access to giant, code-specialized LLMs without managing GPUs, plus smoother agent/tool use in popular editors.

**Summary of Discussion:**

1. **Model Performance & Preferences:**
   - Users report positive experiences with **GLM-4.6** for complex reasoning and coding tasks, though some note limitations compared to **Claude** and **Codex**. 
   - **Qwen3-Coder-30B** is praised for backend code generation but criticized for impractical local hardware requirements (300GB+ VRAM). 
   - Mixed opinions on **Claude**: While powerful, users express frustration with usage limits (daily/weekly caps) and pricing tiers ($20-$100/month). Some have switched to GLM-4.6 or ChatGPT due to cost.

2. **Cost Concerns:**
   - Debates over subscription models: $6/month for Claude is deemed reasonable, but higher tiers ($100+) face backlash. 
   - Alternatives like **OpenRouter** (cheaper credits) and **GLM-4.6 via cloud** are explored to bypass Claude’s restrictions.

3. **Hardware Challenges:**
   - Running large models locally (e.g., Qwen3-Coder-480B) requires extreme hardware (e.g., NVIDIA GH200 GPUs, Mac Studio with 512GB RAM), sparking skepticism about accessibility. 
   - Local inference on laptops is deemed impractical due to slow performance; cloud solutions are preferred for practicality.

4. **Ollama’s Direction & Sustainability:**
   - Criticism of Ollama’s shift toward cloud reliance and opaque model support. Users advocate for broader local compatibility (e.g., via KoboldCPP).
   - Concerns about Ollama’s business model: Reliance on VC funding, potential acquisitions, or subscription fatigue. Some suggest open-source sustainability or community funding.
   - Defenders highlight Ollama’s convenience as a wrapper for `llama.cpp` and willingness to pay for bandwidth/development.

5. **Political & Ethical Tangents:**
   - A subthread critiques Chinese AI integration with military research, countered by examples of U.S. tech companies (Meta, Microsoft) implicated in conflicts (Myanmar, Israel-Palestine).

**Key Takeaways:**  
Users value Ollama’s new models and IDE integrations but question hardware feasibility and long-term viability. While GLM-4.6 gains traction, Claude’s limits and cost drive exploration of alternatives. Debates highlight tensions between local/cloud workflows and sustainability of AI tooling ecosystems.

### SWE-Grep and SWE-Grep-Mini: RL for Fast Multi-Turn Context Retrieval

#### [Submission URL](https://cognition.ai/blog/swe-grep) | 93 points | by [meetpateltech](https://news.ycombinator.com/user?id=meetpateltech) | [27 comments](https://news.ycombinator.com/item?id=45607822)

Cognition unveils SWE-grep and SWE-grep-mini: RL-trained, parallel “Fast Context” retrievers for codebases, now rolling into Windsurf

- The problem: In tools like Windsurf and Devin, more than 60% of the first turn often goes to context retrieval. Traditional approaches either rely on embedding/RAG (fast but often imprecise for multi-hop code tracing) or “agentic” CLI exploration (flexible but slow, chatty, and prone to context pollution).

- The pitch: SWE-grep and SWE-grep-mini are small, agentic retrieval models trained via RL to match the retrieval quality of frontier coding models while running an order of magnitude faster. They power a new Windsurf subagent called Fast Context.

- How it works:
  - Trained to run limited serial turns (about 4), each with highly parallel tool calls (e.g., 8-way grep/glob/read), minimizing latency while exploring multiple code paths at once.
  - Co-designed with a tight toolset and fast execution (indexing, multi-threading, restricted commands) plus fast inference.
  - Retrieval is verifiable: instead of summarizing, the subagent returns file paths and line ranges, enabling a clean, deterministic reward signal for RL and avoiding misleading summaries.

- Why a subagent: It conserves the main agent’s context budget and reduces “context poisoning,” handing over only the relevant lines/files so the smarter model can focus on reasoning and edits.

- Where to try:
  - Rolling out progressively in Windsurf; it triggers automatically when code search is needed (or force it with Cmd+Enter in Cascade).
  - Public demo playground with side-by-side runs against stock Claude Code and Cursor CLI: https://playground.cognition.ai/
  - Caveat: The comparison is a demo, not a rigorous benchmark; all agents are hosted in identical Modal containers with stdin/stdout piping to resemble local use. Authors recommend trying in your own setup.

- Why it matters: If retrieval is the latency bottleneck for coding agents, a fast, parallel, RL-tuned retriever that returns precise code spans could make “smart” agents feel much snappier on large codebases—without drowning them in irrelevant tokens.

Here's a concise summary of the Hacker News discussion about Cognition's SWE-grep tools and Windsurf integration:

**Key Reactions & Discussions:**
1. **Positive Feedback**:  
   - Users praised the engineering effort ("highly impressive") and real-world utility, noting Windsurf's responsiveness compared to traditional LLM coding tools.  
   - SWE-grep's parallel processing (8-way grep/glob) and deterministic context retrieval were highlighted as innovative solutions to LLM latency issues.

2. **Technical Questions**:  
   - Clarification sought on how SWE-grep balances speed/precision vs embeddings/RAG.  
   - Interest in how "Fast Context" subagents avoid "context poisoning" by returning clean file/line references instead of summaries.  

3. **Performance Observations**:  
   - Users shared speed comparisons: Claude Code (0.1s) vs Cursor CLI (19s) in demo tests.  
   - Discussion emphasized that retrieval speed ("60% of first-turn latency") critically impacts coding agent usability.  

4. **Skepticism & Caveats**:  
   - Some questioned demo validity, urging independent benchmarks due to concerns about cached results or artificial constraints.  
   - Noted the tradeoff between "agentic search" flexibility and SWE-grep's restricted-but-faster command set.  

5. **Broader Implications**:  
   - Debate about whether dedicated retrieval subagents (like Fast Context) represent a paradigm shift vs incremental optimization.  
   - Recognition that efficient context engineering ("Read Files" capability) is becoming as crucial as generation quality ("Generate Diffs") for coding agents.  

**Miscellaneous**:  
- Public demo availability drew interest, though some reported temporary access issues.  
- Users requested deeper technical details about RL training and verifiable reward signals.  
- Comparisons made to IDE tools (RubyMine) and speculation about future "workflow memory" optimizations.

### Nvidia DGX Spark and Apple Mac Studio = 4x Faster LLM Inference with EXO 1.0

#### [Submission URL](https://blog.exolabs.net/nvidia-dgx-spark/) | 57 points | by [edelsohn](https://news.ycombinator.com/user?id=edelsohn) | [19 comments](https://news.ycombinator.com/item?id=45611912)

- The pitch: EXO 1.0 pairs two very different boxes for one request—DGX Spark handles the compute‑heavy prefill (time‑to‑first‑token), while an Apple Mac Studio M3 Ultra handles the memory‑bound decode (tokens/sec). Result: up to 4x faster inference in the right conditions.

- Why this works:
  - Prefill is compute‑bound. DGX Spark (~100 TFLOPs FP16, 128 GB coherent CPU‑GPU memory at 273 GB/s) excels here.
  - Decode is memory‑bandwidth‑bound. M3 Ultra (512 GB unified memory at 819 GB/s, ~26 TFLOPs FP16 GPU) excels here.
  - EXO streams each layer’s KV cache over the network as it’s produced, overlapping communication with compute to hide transfer costs.

- Key technical idea: layer‑by‑layer KV streaming over 10 GbE. If per‑layer compute time exceeds KV transfer time, the network cost is hidden. Rule of thumb from their derivation:
  - Need s > (P/B)·q / K, where s is prompt length, P/B is compute-to-bandwidth ratio, q is KV quantization bits, and K depends on attention type.
  - With P/B ≈ 10,000 (100 TFLOPs over 10 Gbps), 8‑bit KV:
    - K=16 (GQA, e.g., Llama‑3 70B): s > ~5k tokens
    - K=8 (Llama‑3 8B): s > ~10k tokens
    - K=2 (MHA, Llama‑2 7B): s > ~40k tokens
  - Takeaway: bigger contexts and GQA models benefit sooner; faster links would lower thresholds.

- Benchmark (Llama‑3.1 8B, FP16, 8,192‑token prompt, 32‑token output):
  - Mac Studio only: 6.42 s (baseline)
  - DGX Spark only: 4.34 s (1.9× faster)
  - DGX Spark + Mac Studio (EXO): 2.32 s (2.8× faster)
  - Interpreting the split: DGX slashes TTFT (prefill), Mac Studio maximizes TPS (decode).

- Why it matters: A pragmatic, heterogenous inference design that exploits what each device does best—compute vs memory bandwidth—without requiring giant GPUs. It also shows how model architecture (GQA vs MHA), KV precision, context length, and network speed jointly determine whether hybrid inference pays off.

**Summary of Discussion:**

1. **Prefill vs. Decode Importance:**  
   - Users debate the practical balance between prefill (compute-heavy) and decode (memory-bound). Some argue that prefill dominates in scenarios with short outputs (e.g., simple queries), while decode matters more for tasks requiring long token generation.  
   - Medium-sized prompts (~1k tokens) on smaller models (8B-20B) reportedly suffer slowdowns on Apple Silicon (e.g., M1), emphasizing prefill’s role in latency.

2. **Hardware Limitations & Model Compatibility:**  
   - Concerns arise about DGX Spark’s 128GB memory cap limiting larger models. A reply notes upcoming EXO 1.0 support for models like DeepSeek-R1 (up to 128GB) and plans to open-source the framework.  
   - Questions about MoE (Mixture of Experts) model compatibility and layer distribution efficiency remain unresolved.

3. **Networking & Cost Concerns:**  
   - USB-C/Thunderbolt networking between DGX Spark and Mac Studio is discussed, with users confirming USB4/Thunderbolt 3 compatibility for high bandwidth.  
   - The $30k+ cost of DGX hardware sparks jokes about affordability, though some suggest alternatives like RTX Pro 6000 GPUs for cost-conscious setups.

4. **Project Availability & Use Cases:**  
   - Some express disappointment that EXO is currently private, though the team hints at future open-sourcing.  
   - Potential applications beyond LLMs (e.g., Stable Diffusion) are noted, with DGX Spark’s compute benefits highlighted for other AI workloads.

**Key Takeaways:**  
The discussion reflects enthusiasm for hybrid inference architectures but highlights practical hurdles like hardware costs, model compatibility, and real-world prompt dynamics. Community interest in open-sourcing EXO and leveraging Apple’s upcoming M5 hardware (claimed 35x TTFT gains) suggests ongoing optimization trends in edge AI.

### TaxCalcBench: Evaluating Frontier Models on the Tax Calculation Task

#### [Submission URL](https://arxiv.org/abs/2507.16126) | 68 points | by [handfuloflight](https://news.ycombinator.com/user?id=handfuloflight) | [23 comments](https://news.ycombinator.com/item?id=45601230)

TaxCalcBench: Can AI file your taxes? Not yet. A new benchmark tests frontier LLMs on calculating US personal income tax returns when given all the needed facts. Even on a simplified set, state-of-the-art models correctly compute fewer than one-third of federal returns. Common failure modes: misusing tax tables, arithmetic mistakes, and misjudging eligibility. The authors conclude that LLMs need additional tooling/infrastructure to be reliable for tax prep—plain language modeling isn’t enough for rule-heavy, tabular, multi-step calculations. (arXiv:2507.16126)

**Summary of Hacker News Discussion:**

The discussion revolves around the challenges and limitations of using LLMs (like Claude, GPT, etc.) for tax preparation, as highlighted in the TaxCalcBench study. Key points include:

1. **Mixed Practical Experiences**:  
   - Some users shared attempts to automate personal finance/tax tasks with LLMs (e.g., Claude Code), achieving partial success in exploratory data analysis but facing issues like misclassified transactions. Manual verification was still required, underscoring the need for better tooling.  
   - Skepticism persisted about LLMs’ reliability for direct financial decisions without rigorous testing or safeguards.

2. **Common Failure Modes**:  
   - Errors in arithmetic, tax table usage, and eligibility judgments were noted as persistent issues. Even advanced models like ChatGPT produced wildly incorrect tax numbers despite claiming confidence.  
   - Participants emphasized that tax calculation involves narrow legal definitions and multi-step logic, which LLMs struggle to navigate without structured data (e.g., CSV/TSV) or domain-specific scaffolding.

3. **Potential Solutions**:  
   - Suggestions included integrating LLMs with calculators, task-specific guardrails, or retrieval-augmented generation (RAG) to access tax code references.  
   - The authors clarified that their benchmark tested models’ ability to interpret tax forms (e.g., IRS Form 1040) and code, linking to open-source tax tools like Iris for context-aware solutions.

4. **Liability & Trust Concerns**:  
   - Users questioned liability if AI-made errors occurred, highlighting risks in real-world deployment. Many argued humans already struggle with tax complexity, and AI’s current limitations make it unsuitable for unsupervised use.  

5. **Community Reactions**:  
   - Surprise was expressed at the IRS’s GitHub repository of tax questions in XML, hinting at potential structured data sources for future LLM training.  
   - A leaderboard for model performance on TaxCalcBench was shared, with speculation about restrictions in benchmark simplicity (e.g., scores might improve with more constrained scenarios).  

**Conclusion**: While LLMs show promise in automating tax tasks, their current limitations in accuracy, legal nuance, and calculation reliability necessitate hybrid approaches combining AI with traditional software tools and human oversight. The discussion reflects cautious optimism tempered by practical and ethical concerns.

### Who's Submitting AI-Tainted Filings in Court?

#### [Submission URL](https://cyberlaw.stanford.edu/whos-submitting-ai-tainted-filings-in-court/) | 79 points | by [cratermoon](https://news.ycombinator.com/user?id=cratermoon) | [59 comments](https://news.ycombinator.com/item?id=45600263)

Who’s submitting AI-tainted court filings? A Stanford researcher dug into a global database of “AI hallucination” incidents and analyzed 114 U.S. cases from June 2023 (post–Mata v. Avianca) through Oct 7, 2025. Key takeaways:

- It’s mostly small shops: 90% of implicated firms were solo practitioners or small firms.
- Plaintiffs more than defendants: 56% of cases were attributed to plaintiff’s counsel, 31% to defense, 13% were “other” (e.g., bankruptcy, family, probate, tax, agency, habeas, disciplinary).
- ChatGPT shows up a lot: Among matters where a tool was specified, about half cited some version of ChatGPT.
- Government lawyers were rare in the sample; there were no U.S. prosecutor cases in the time window.
- The problem persists despite court standing orders on AI, ethics opinions, and CLEs warning about AI use in legal practice.

Method notes and caveats:
- Source: Damien Charlotin’s AI Hallucination Cases database; author restricted to U.S., excluded pro se matters, and verified parties and firm affiliations via orders and dockets.
- Firm-size bands followed NALP conventions with solos split out; some classifications required best guesses from websites and directories.
- The dataset is moving fast; the author notes it was outdated within days as new matters were added. Treat results as indicative, not comprehensive.

Why it matters:
- The skew toward solo and small firms suggests resource and workflow gaps—less access to vetted research tools, fewer review layers, and greater temptation to lean on general-purpose LLMs.
- Courts are increasingly scrutinizing filings, and lawyers face reputational and potential sanctions risk if they don’t verify citations.
- For tool builders: audit trails, authoritative citation verification, and guardrails against fabricated authorities remain must-haves for legal workflows.

**Summary of Hacker News Discussion:**

1. **Surprise at Lawyers’ Oversight**:  
   Commenters expressed disbelief that legal professionals often fail to rigorously verify AI-generated content, relying on unreliable sources like Google, blogs, or general LLMs (e.g., ChatGPT) instead of authoritative databases like Westlaw. This aligns with the study’s finding that hallucinations stem from workflow gaps in smaller firms.

2. **Small Firms & Resource Constraints**:  
   Many agreed that solo/small firms lack resources (e.g., vetting tools, peer review) to catch errors, increasing hallucination risks. However, critiques emerged about the study’s methodology:  
   - Without comparing against the broader lawyer population (e.g., small firms dominate the legal market), claims of overrepresentation might be misleading.  
   - Statistical context was noted: In some jurisdictions, 50%+ of legal work is handled by firms with ≤50 lawyers, complicating causality between firm size and error rates.

3. **AI Limitations & Verification Challenges**:  
   - LLMs like ChatGPT are seen as “early-stage” tools requiring heavy human oversight. Skepticism exists about their ability to collate data accurately without guardrails (e.g., citation verification).  
   - Technical debates arose about AI’s role in solving NP-hard problems (e.g., verifying legal citations), though these were tangential to the core issue.

4. **Professional Standards & Accountability**:  
   - Concerns were raised about lower-performing law graduates or overworked lawyers cutting corners. Some argued the bar exam and professional certifications inadequately enforce rigor.  
   - Anecdotes highlighted real-world consequences: One user described a lawyer using Claude (an AI) who faced judicial criticism for flawed filings.

5. **Critique of Government Sources**:  
   Even government-published legal guidance was noted to sometimes contain subtle errors, exacerbating reliance on flawed sources. This underscores the need for authoritative, up-to-date materials in legal workflows.

**Key Takeaways**:  
While the study’s findings resonated (small firms, ChatGPT prevalence), commenters emphasized methodological caveats and broader systemic issues (e.g., resource disparities, professional accountability). Calls for AI tools with audit trails, better citation checks, and ethical guardrails were echoed, alongside skepticism about current AI reliability in high-stakes legal contexts.