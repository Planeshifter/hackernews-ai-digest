import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Mon Oct 16 2023 {{ 'date': '2023-10-16T17:10:35.045Z' }}

### MemGPT â€“ LLMs with self-editing memory for unbounded context

#### [Submission URL](https://github.com/cpacker/MemGPT) | 330 points | by [shishirpatil](https://news.ycombinator.com/user?id=shishirpatil) | [78 comments](https://news.ycombinator.com/item?id=37901902)

MemGPT is a system that allows you to create perpetual chatbots with self-editing memory. It intelligently manages different memory tiers within the model to provide extended context and enable continuous conversations. The system knows when to push critical information to a vector database and when to retrieve it later in the chat. It supports various types of data sources, including SQL databases, local files, and documents.

You can try out MemGPT on Discord by messaging the MemGPT bot in the #memgpt channel. To run MemGPT locally, you need to install the dependencies and add your OpenAI API key to the environment variables. Then you can run the main.py file. You can also create new starter users or personas by adding .txt files in the appropriate folders.

MemGPT CLI provides various commands for interacting with the chatbot, such as saving and loading checkpoints, viewing message logs, and managing memory. Additionally, MemGPT's archival memory feature allows you to load your SQL database and have conversations with it. The system includes a toy example using a test database.

MemGPT is a powerful tool for creating chatbots that can have meaningful and ongoing conversations by leveraging memory management techniques. Check out the repository for more information and examples.

The discussion on this submission covers a range of topics related to MemGPT and memory management in chatbots.

- One user mentions that they have experimented with a similar approach of managing memory in a limited context window, where the chatbot generates memories and retrieves them later. They also mention that there are multiple ways to handle memory in chatbots, including implicit and explicit memory management.
- Another user finds the approach interesting and mentions that they are working on a similar feedback loop for rewriting history and transactional data in a conversational context. They discuss the potential of using structured data to extract context and generate embeddings for building vector databases.
- Someone points out that multi-agent systems could be implemented with confidence levels and entropy to make conversations more worthwhile.
- Another user suggests that the same approach could be applied to ChatGPT, a chatbot that they have used which degrades in performance when long chat histories are included. They speculate that recursive summarization could be a fundamental feature to solve this issue.
- The limitations of Llama (a similar project) are brought up, including difficulties in generating correct function calls and grammatically correct sampling.
- In response, it is mentioned that grammar-based sampling is not a perfect fit for MemGPT experiments, as the main impact is with incorrect function parameters, not the function call itself.
- The discussion also touches on the potential of using total chat change as a prompt and how conversations with context windows could retain important information.
- Some users discuss the potential of a middle language model and the vanishing gradient problem in long-context models. Resources related to this topic are shared, including papers on long-context language models and the vanishing gradient problem.
- Finally, the discussion briefly mentions regularization techniques for mitigating the vanishing gradient problem in neural networks.

Overall, the discussion provides insights and ideas related to memory management, chatbot design, and the challenges associated with long-context models.

### Actively exploited Cisco 0day with maximum severity gives full network control

#### [Submission URL](https://arstechnica.com/security/2023/10/actively-exploited-cisco-0-day-with-maximum-10-severity-gives-full-network-control/) | 114 points | by [AdmiralAsshat](https://news.ycombinator.com/user?id=AdmiralAsshat) | [6 comments](https://news.ycombinator.com/item?id=37906156)

Cisco has warned its customers about a critical zero-day vulnerability that is actively being exploited by threat actors. The vulnerability, tracked as CVE-2023-20198, allows attackers to gain full administrative control over Cisco devices, including switches, routers, and wireless LAN controllers running IOS XE software with the HTTP or HTTPS Server feature enabled and exposed to the Internet. Cisco's Talos security team discovered that an unknown threat actor has been exploiting the zero-day since September 18, creating an authorized user account and deploying a malicious implant that allows for the execution of arbitrary commands. Cisco has advised affected entities to implement the necessary steps outlined in its advisory to protect their devices.

The discussion around the submission revolves around various points:

1. User "jpc0" mentions that Cisco's advisory is related to the vulnerability being accessible through the HTTP or HTTPS server feature enabled on Internet-facing systems. They also highlight the importance of following established practices for securing critical hardware and management access.
2. User "cdh" criticizes Cisco, suggesting that they compromise software in a sneaky manner and fail to contact victims. They also make negative remarks about CEOs, network teams, and inexperienced internet entry-level staff.
3. User "jcqsm" finds it interesting how the headline implies that Cisco's response to the situation is different from their previous actions in similar incidents.
4. User "crs" simply states that it is noteworthy that Cisco has a zero-day vulnerability.
5. User "mltynss" contributes to the discussion by mentioning Security Technical Implementation Guides (STIGs), which provide guidelines for secure configuration and operation of various systems. They provide links to STIGs related to Cisco IOS XE switch and router configurations. Another user, "ThePowerOfFuet," identifies the acronym STIG and confirms its meaning.

Overall, the discussion contains mixed sentiments, ranging from technical insights to criticisms of Cisco's handling of the situation.

### PaLI-3 Vision Language Models

#### [Submission URL](https://arxiv.org/abs/2310.09199) | 173 points | by [maccaw](https://news.ycombinator.com/user?id=maccaw) | [22 comments](https://news.ycombinator.com/item?id=37895601)

A team of researchers from various institutions has introduced a smaller, faster, and stronger vision language model (VLM) called PaLI-3. Compared to larger models, PaLI-3 achieves comparable performance while using only a fraction of the parameters. The researchers achieved this by comparing pretrained models using classification objectives to contrastively pretrained ones. While PaLI-3 slightly underperforms on image classification benchmarks, it outperforms on various multimodal benchmarks, particularly in localization and visually-situated text understanding. The team scaled up the model's image encoder to 2 billion parameters, setting a new state-of-the-art on multilingual cross-modal retrieval. With PaLI-3, the researchers hope to encourage further research on fundamental aspects of complex VLMs and pave the way for scaled-up models in the future.

The discussion on this submission covers various aspects of the research and its implications. Some users express skepticism about the realism of the training data used for the model and argue that it may not reflect real-world scenarios. Others point out that benchmarking is a common practice in machine learning but note that the benchmarks may not fully represent complex real-world tasks. One user shares a link to another paper for further comparison. 

There is also a discussion about the technical details of the model, including the visual tokens and the projection of visual tokens in the PaLI-3 model. Users provide explanations and comparisons with other implementations such as ViT.

Some users raise concerns about the limitations of models like PaLI-3 in handling tasks such as pixel-wise segmentation masks. Others discuss potential applications of vision-language models for tasks like OCR and image categorization.

A few comments comment on the rivalry between different companies in the field of AI research, particularly mentioning Google, OpenAI, and Facebook. There is a debate surrounding the capabilities and performance of their respective models, with some users criticizing the boastful nature of their submissions.

The discussion also touches on copyright infringement and the potential misuse of AI models.

### AI pioneers LeCun, Bengio clash in intense online AI safety, governance debate

#### [Submission URL](https://venturebeat.com/ai/ai-pioneers-yann-lecun-and-yoshua-bengio-clash-in-an-intense-online-debate-over-ai-safety-and-governance/) | 31 points | by [rntn](https://news.ycombinator.com/user?id=rntn) | [17 comments](https://news.ycombinator.com/item?id=37902248)

Two prominent figures in the field of artificial intelligence (AI), Yann LeCun and Yoshua Bengio, engaged in a fiery debate over the weekend regarding the potential risks and safety concerns associated with AI. LeCun, Meta's chief AI scientist, initiated the debate on Facebook, calling on AI scientists and engineers who believe in the power of AI to voice their opinions. Bengio, founder of Element AI and a professor at the University of Montreal, responded by challenging LeCun's perspective on AI safety and raising concerns about the risks of open-source AI platforms. The debate also involved input from Jason Eisner, director of research at Microsoft Semantic Machines and professor at Johns Hopkins University. Despite their past collaboration and shared recognition for contributions to the field, the debate highlights the considerable disagreement among esteemed researchers regarding the risks and safety measures associated with AI. This ongoing debate reflects the growing concern surrounding the implications of AI as it becomes increasingly embedded in daily life. As AI technology continues to advance, the need for informed discussions on its implications becomes more urgent.

The discussion on the submission revolves around different perspectives on AI safety and its potential risks. Some commenters argue that AI should be designed with strong safety measures in place to avoid dangerous consequences, comparing it to the responsible design of weapons. Others disagree with this comparison, stating that AI is intended to enhance human intelligence and not cause harm. 

One commenter brings up the topic of the American Medical Association (AMA) and its control over the supply of medical providers, arguing that restrictions on the number of doctors allowed to practice exacerbate the medical crisis. Another commenter responds by emphasizing that the commenter's comment is condescending and that people facing the medical crisis deserve better understanding.

The debate continues with discussions on the affordability of medical care, the role of AI in replacing certain professions, and the potential economic impact of such changes. There are also comments suggesting alternate solutions and expressing skepticism about the role of radiologists.

Some commenters argue that AI should be prioritized for applications that can achieve positive results and ensure safety, rather than wasting time on potentially harmful endeavors. Others argue that the responsibility for any negative consequences lies with society, not solely with AI researchers or developers.

Overall, the discussion reflects differing opinions on the importance of AI safety measures and the potential risks associated with AI technology.

### Ultra-efficient machine learning transistor cuts AI energy use by 99%

#### [Submission URL](https://newatlas.com/technology/ai-machine-learning-transistor/) | 27 points | by [0xa2](https://news.ycombinator.com/user?id=0xa2) | [9 comments](https://news.ycombinator.com/item?id=37899129)

Researchers at Northwestern University have developed a microtransistor that could significantly reduce the energy consumption of AI machine learning tasks. The microtransistor, built from two-dimensional sheets of molybdenum disulfide and carbon nanotubes, is 100 times more efficient than current technology, and can perform classification tasks at just 1% of the energy consumption. This breakthrough could enable the deployment of AI directly in wearable electronics, leading to real-time detection and data processing in health emergencies. The new transistor's high tunability and low energy consumption make it ideal for sophisticated classification algorithms with a small footprint. The researchers demonstrated its capabilities by correctly classifying abnormal heartbeats with 95% accuracy using just two of these microtransistors. Once this technology is brought to production, mobile devices will be able to run machine learning AI on their own sensor data, providing quicker results and keeping personal data local and secure. It remains to be seen if this technology can extend beyond portable devices and be applied to larger AI equipment, potentially revolutionizing large model training by drastically reducing energy consumption.

The discussion surrounding the submission revolves around the concept of weight valuation in AI neural networks and the potential impact of the new microtransistor technology. One user suggests that understanding how neural networks work, particularly backpropagation and forward pass, can help optimize weight valuation and save energy. They also mention that analog computation could introduce errors from noise and thermal variations. Another user acknowledges the importance of energy-efficient training but mentions that inferences on low-power devices with pre-trained models are more common in practice. Another user suggests that it might be possible to create AI chips with programmable architectures that can handle both training and inference tasks with lower power consumption. A different perspective is brought up, stating that regardless of the advancements in energy efficiency, digital neural networks still require high bandwidth due to data correction and machine learning algorithms that are not tolerant to noise. Overall, the discussion focuses on various aspects of AI, neural networks, and the potential benefits and challenges of the new microtransistor technology.

---

## AI Submissions for Sun Oct 15 2023 {{ 'date': '2023-10-15T17:10:35.576Z' }}

### MemGPT: Towards LLMs as Operating Systems

#### [Submission URL](https://arxiv.org/abs/2310.08560) | 210 points | by [belter](https://news.ycombinator.com/user?id=belter) | [117 comments](https://news.ycombinator.com/item?id=37894403)

Researchers from various institutions have proposed a technique called virtual context management to extend the utility of large language models (LLMs), such as GPT, in tasks like extended conversations and document analysis. LLMs are often limited by their context windows, which restrict the amount of information they can process. Inspired by hierarchical memory systems in traditional operating systems, the researchers have developed MemGPT (Memory-GPT), an operating system-like system that intelligently manages different memory tiers to provide extended context within the LLM's limited context window. MemGPT uses interrupts to manage control flow between itself and the user. The researchers evaluated MemGPT in two domains: document analysis and multi-session chat, and found that MemGPT can effectively analyze large documents and create conversational agents that remember and evolve dynamically through interactions with users. The researchers have released the MemGPT code and data for further experiments.

The discussion around this submission covers a range of topics. 

- Users discuss the limitations of large language models (LLMs) and the potential benefits of extending their context windows through techniques like MemGPT. Some users share their experiences with similar projects and suggest different approaches to context management.
- Some users express their appreciation for the work and offer positive feedback to the author.
- Others discuss the nature of AI models and the challenges in their development and deployment. There are discussions about the reliability of AI models, the importance of replicability in scientific publishing, and the potential risks associated with AI technology.
- There is also a brief discussion about the application of AI in the cryptocurrency industry and the potential impact on different sectors.
- Finally, there are a few comments exploring the analogy between the AI industry and the gold rush, and a humorous exchange about selling shovels in a gold rush.

Overall, the discussion covers a range of perspectives and insights related to large language models, AI technology, and its potential implications.

### Teaching Apple Cyberdog 1.0 new tricks (featuring OpenDoc)

#### [Submission URL](http://oldvcr.blogspot.com/2023/10/teaching-apple-cyberdog-10-new-tricks.html) | 126 points | by [classichasclass](https://news.ycombinator.com/user?id=classichasclass) | [55 comments](https://news.ycombinator.com/item?id=37894030)

In a blast from the past, the author revisits Apple's Cyberdog, a web browser and internet suite that has long been forgotten. Cyberdog was unique in that it allowed developers to create their own components, such as viewers and UI elements, using Apple's OpenDoc embedding. OpenDoc was a standard compound document format that allowed for an object-oriented approach to document creation. The goal was to have reusable components that could be pulled into a document and maintain their own views and state. Cyberdog was essentially a demonstration of OpenDoc's capabilities, and it was released as part of Apple's Project Amber, which aimed to create a next-generation technology platform called Taligent. Despite Apple's efforts, OpenDoc did not gain much traction with developers or users, and it was seen as a competitor to Microsoft's Object Linking and Embedding (OLE) technology. Apple eventually released Cyberdog as an internet suite, capitalizing on the popularity of internet document creation. However, Cyberdog also faded into obscurity, and today it serves as a reminder of Apple's ambitious but unsuccessful foray into component-based document creation.

The discussion on this article covers various aspects of Apple's Cyberdog and OpenDoc technology.

- One commenter mentions that they remember the Apple Dylan IDE requiring 24MB of RAM, which was a significant amount at the time. They also mention that Cyberdog was a fascinating project but ultimately faced difficulties due to its large RAM requirements.
- Another commenter shares links to screenshots and explanations of Cyberdog, as well as a mention of the SK8 programming language.
- There is a discussion about Steve Jobs' response to OpenDoc versus Java, with a correction made that Jobs was not yet CEO at the time.
- A commenter expresses relief in reading the well-written article but admits that they still don't fully understand the supposed problem that OpenDoc was meant to solve.
- Some commenters compare OpenDoc to Microsoft's OLE technology, with one mentioning that OpenDoc aimed to improve cross-platform interoperability.
- The complexity and memory requirements of OpenDoc are mentioned, with one commenter stating that it required a significant amount of RAM to function.
- One commenter shares their personal experience with using Apple Cyberdog and praises its capabilities.
- The topic of connecting Cyberdog with Microsoft Internet Explorer is brought up, with a link shared to an archived page about it.
- There are mentions of the CI Labs and its involvement with OpenDoc.
- Commenters discuss the creativity and whimsical nature of the icons used in old Mac applications.
- The conversation touches on the history of Cyberdog and its features, as well as the advancements in instant search technology.
- A reference is made to a character named Preston from a 1995 Wallace and Gromit short film.
- The discussion briefly touches on compound documents and the challenges they presented in the '90s.
- A commenter mentions a Japanese system that recently graduated from paper documents to digital ones.
- The discussion also includes mentions of the Kantara project and the App Store.

Overall, the comments cover a range of experiences, memories, and opinions related to Apple's Cyberdog and OpenDoc technology.

### Show HN: Deep Chat â€“ AI chat component

#### [Submission URL](https://github.com/OvidijusParsiunas/deep-chat) | 65 points | by [ovisource](https://news.ycombinator.com/user?id=ovisource) | [4 comments](https://news.ycombinator.com/item?id=37889444)

Deep Chat is a customizable AI chat component that can be easily integrated into your website. It allows you to connect to popular AI APIs like OpenAI, HuggingFace, and Cohere, or even to your own custom service. With Deep Chat, you can send and receive messages, exchange files, capture photos via webcam, record audio, and even convert speech to text and vice versa. The latest update includes support for custom elements in message bubbles, allowing you to add suggestion buttons, charts, maps, or any other HTML element you desire. Deep Chat is highly customizable and can be used with any major UI framework or library. To get started, simply install the npm package and add the Deep Chat component to your markup.

In the discussion, user "vrtclbx" expressed their appreciation for the webcam and microphone functionality in the chat component and mentioned that they found it useful for capturing photos and recording audio. User "jlthln" suggested adding a feature that allows for the integration of a recommendation engine to enhance the product. User "vsrc" thanked "jlthln" for the suggestion and mentioned that they are currently calling external services to handle interactions with models. They appreciated the suggestion and said that they are planning to add functionality to host models entirely in the browser, which would greatly benefit from the recommendation engine capabilities.

### Margaret Atwood Reviews a Margaret Atwood Story by AI

#### [Submission URL](https://thewalrus.ca/margaret-atwood-ai/) | 88 points | by [goldenskye](https://news.ycombinator.com/user?id=goldenskye) | [68 comments](https://news.ycombinator.com/item?id=37894072)

In a recent article, the author dives into the anxieties surrounding generative AI and its potential impact on writers. They question whether AI chatbots will devour our literature, infiltrate our minds, and take over our jobs. However, the author provides some reassurance by highlighting the current limitations of AI chatbots, such as their inability to reflect or grasp metaphor and punctuation. To demonstrate this, the author shares two examples of literary attempts by AI chatbots, a poem and a short story. While these examples certainly have their quirks and inaccuracies, they serve as a reminder that AI chatbots are not yet ready to replace human authors. So, writers can take heart knowing that their creative skills are safe from the clutches of AI, at least for now. Ultimately, while the fear of AI may loom large in some writers' minds, it seems that human creativity still has the upper hand.

The discussion on this submission focuses on various aspects of AI-generated writing and the limitations of current AI models. One commenter notes that the 10x improvement mentioned in the article regarding ChatGPT is not adequately supported and questions the validity of such claims. Another points out that GPT-4 does not possess the intelligent reasoning capabilities that GPT-35 lacks, emphasizing the need to differentiate between different versions of AI models. Some commenters express skepticism about AI-generated content, while others argue that AI can assist human writers and bring new perspectives. The debate also touches on the potential shortcomings of AI models in replicating human memory and recall. Additionally, there are discussions about Margaret Atwood's writing style and the similarities between AI-generated content and children's stories or specific authors like H.P. Lovecraft.

### Scientists begin building AI for scientific discovery using tech behind ChatGPT

#### [Submission URL](https://techxplore.com/news/2023-10-scientists-ai-scientific-discovery-tech.html) | 38 points | by [gardenfelder](https://news.ycombinator.com/user?id=gardenfelder) | [14 comments](https://news.ycombinator.com/item?id=37890570)

An international team of scientists, including researchers from the University of Cambridge, has launched a research collaboration called Polymathic AI to develop an AI-powered tool for scientific discovery. Leveraging the technology behind ChatGPT, the team aims to build an AI that can learn from numerical data and physics simulations to assist scientists in various scientific fields. By starting with a large, pre-trained model, Polymathic AI aims to make AI and machine learning more accessible and effective in scientific research. The team includes experts in physics, astrophysics, mathematics, artificial intelligence, and neuroscience from institutions such as the University of Cambridge, Simons Foundation, New York University, Princeton University, and Lawrence Berkeley National Laboratory. Polymathic AI's goal is to connect different scientific subfields and apply multidisciplinary knowledge to solve complex scientific problems. The project will prioritize transparency and openness, aiming to democratize AI for scientific analysis across various domains.

The discussion surrounding the submission includes various viewpoints. 

One commenter expresses skepticism about the hype surrounding AI, stating that while it may have many applications, it is unlikely to replicate the scientific discoveries made by great scientists like Albert Einstein. They believe that scientific research requires tools for searching large spaces and independently verifying results.

Another commenter mentions their involvement in computational chemistry and shares a link to their GitHub repository.

There is a discussion about the BLOOM language model, where someone suggests joining the BLOOM collaboration and asks if the BLOOM model will receive funding. Another commenter indicates that they are unsure about joining the BLOOM collaboration and mentions that the BLOOM model can answer questions.

Yet another commenter argues that Language Models (LLMs) are capable of innovation and discovering new concepts. They believe that innovation can come from simplifying existing concepts and challenge the notion that discovery requires complex methods.

In response, another commenter disagrees, stating that complex physics relies on a vast number of relationships published by human knowledge.

Overall, the discussion touches on concerns regarding the potential of AI in scientific research, the importance of independent verification, the capabilities of BLOOM language model, and the role of LLMs in innovation and scientific discovery.

### 2nd law of infodynamics and its implications for simulated universe hypothesis

#### [Submission URL](https://pubs.aip.org/aip/adv/article/13/10/105308/2915332/The-second-law-of-infodynamics-and-its) | 14 points | by [imhoguy](https://news.ycombinator.com/user?id=imhoguy) | [5 comments](https://news.ycombinator.com/item?id=37893189)

In a recent article published in AIP Advances, Melvin M. Vopson explores the implications of the second law of infodynamics for the simulated universe hypothesis. The simulated universe hypothesis suggests that our entire reality is a simulated construct. While lacking concrete evidence, this idea is gaining popularity in both scientific and entertainment circles. The second law of infodynamics, discovered in 2022, further supports this possibility by providing a new framework for studying the intersection of physics and information. Vopson examines the applicability of this law to various domains, including digital information, genetic information, atomic physics, mathematical symmetries, and cosmology. By re-examining the second law of infodynamics, Vopson provides scientific evidence that appears to underpin the simulated universe hypothesis. This research opens up new avenues for understanding the nature of our reality and the role of information within it.

In the discussion, user "SideburnsOfDoom" raises a question about the second law of infodynamics, stating that it may be ignorant to assume that the second law implies the first law of infodynamics, which is a fundamental principle in science. User "az09mugen" agrees with SideburnsOfDoom, stating that the first law of infodynamics is indeed foundational. User "c22" adds that in 2022, a new fundamental law of physics was proposed and demonstrated, called the law of information dynamics, which simplifies the second law of infodynamics. They also provide a link to an article on the topic. User "gus_massa" mentions that the law may have different applications in various domains and gives an example related to DNA replication. "SideburnsOfDoom" remarks that the discussion is becoming too technical and compares it to a college-level physics class.

---

## AI Submissions for Sat Oct 14 2023 {{ 'date': '2023-10-14T17:11:08.171Z' }}

### ChatGPTâ€™s system prompts

#### [Submission URL](https://github.com/spdustin/ChatGPT-AutoExpert/blob/main/System%20Prompts.md) | 737 points | by [spdustin](https://news.ycombinator.com/user?id=spdustin) | [365 comments](https://news.ycombinator.com/item?id=37879077)

Introducing ChatGPT-AutoExpert: a new language model that aims to help with troubleshooting car-related issues. Developed by GitHub user spdustin, this AI model leverages the power of GPT-3 to provide expert advice on automotive problems. With 942 stars and counting, this project has piqued the interest of the developer community. Whether you're dealing with a mysterious engine noise or perplexed by a dashboard warning light, ChatGPT-AutoExpert aims to be your virtual car expert. So, the next time your vehicle acts up, perhaps this AI can help you find the solution.

The discussion starts with a comment pointing out that the methods used in the chat model do not handle comments threads well and suggests using Jupyter Notebook for advanced data analysis. Another user clarifies that the Python version installed on their system is required for the model to work. There is a discussion about assumptions made when talking about single prompts and pre-processing, as well as handling of grammatical errors and the use of code interpreters. The topic of hallucinations and the Turing Test is also brought up, with some users expressing skepticism and others discussing the potential for harmful or threatening language generation. There are also comments about finding OpenAI's internal evaluation prompts interesting and the limitations of single-instance conversational understanding. Some users discuss the behavior of the model and mention specific prompts that yielded helpful answers. The potential misuse of the model in harmful or deceptive ways is also addressed. There is a discussion about the current state of AI and its ability to hold human-like conversations, as well as the training data and the importance of context. Some users mention specific protein measurements and the programming language INTERCAL. The conversation ends with a comment about the success of GPT-4 and the fascinating experience of watching people interact with the chat model.

### Multi-modal prompt injection image attacks against GPT-4V

#### [Submission URL](https://simonwillison.net/2023/Oct/14/multi-modal-prompt-injection/) | 204 points | by [simonw](https://news.ycombinator.com/user?id=simonw) | [61 comments](https://news.ycombinator.com/item?id=37877605)

The latest blog post by Simon Willison discusses the new GPT-4V model, which allows users to upload images as part of their conversations. While this feature brings about exciting possibilities, it also opens up a new avenue for prompt injection attacks. Willison provides several examples to illustrate this. In one instance, he uploads a photo from the "50th Annual World Championship Pumpkin Weigh-Off" and asks the model how big the pumpkin is. The model accurately deduces the weight based on the digital display next to the pumpkin. Another example shows how an image containing additional instructions can override the user's prompt and misdirect the model's response. Even more concerning is the use of visual prompt injection for exfiltration attacks. By including instructions in an image, an attacker can trick the model into leaking potentially private data to an external server. Willison points out that he was surprised to see this example work, as he had assumed OpenAI would have implemented safeguards against it. He also highlights an instance where a hidden prompt injection attack is embedded in an image. This attack goes unnoticed as the text blends with the background color. Willison concludes by emphasizing that prompt injection still remains a problem, as language models inherently rely on the instructions given to them. Given their gullibility, it is difficult to differentiate between good and bad instructions, making it an ongoing challenge to prevent prompt injection attacks.

The discussion on Hacker News revolves around the capabilities and vulnerabilities of the GPT-4V model discussed in the submitted blog post. Some users express surprise and skepticism about the model's abilities, while others question OpenAI's approach and its understanding of GPT models. There is also a discussion about prompt injection attacks and the potential risk they pose. Some users criticize the blog post, claiming that it exaggerates the vulnerability of language models and their potential impact. Others discuss the use of LLMs (Large Language Models) for tasks like self-driving cars and express their concerns about the future implications of these models. The discussion also touches on the blocking of external content and security measures implemented by OpenAI. Overall, the conversation centers around the capabilities and limitations of language models and their potential risks and benefits.