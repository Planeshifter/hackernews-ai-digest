import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Mon Dec 29 2025 {{ 'date': '2025-12-29T17:13:28.058Z' }}

### ManusAI Joins Meta

#### [Submission URL](https://manus.im/blog/manus-joins-meta-for-next-era-of-innovation) | 285 points | by [gniting](https://news.ycombinator.com/user?id=gniting) | [178 comments](https://news.ycombinator.com/item?id=46426534)

Meta is acquiring Manus, an autonomous AI agent startup

- Manus says it’s “joining Meta” while continuing to run its subscription product and operations from Singapore.
- The company positions itself as an “execution layer” for autonomous agents—spinning up virtual computers and orchestrating research/automation workflows end to end.
- Reported usage since launch: 147 trillion tokens processed and 80 million virtual computers created, with “millions” of users.
- Manus plans to keep its app and website offerings, and hopes to bring its agent to businesses and consumers across Meta’s platforms. CEO Xiao Hong says the move provides a “stronger, more sustainable foundation” without changing how Manus works or how decisions are made.
- No deal terms or timeline details were disclosed; Manus points to a Meta announcement for more.

Why it matters
- Signals Meta’s push from foundation models (Llama, Meta AI) toward agentic systems that can actually perform tasks—potentially within WhatsApp, Instagram, and other Meta surfaces.
- Continues a broader trend of Big Tech absorbing agent startups/talent (e.g., Inflection→Microsoft, Adept’s licensing/talent to Amazon), consolidating the “agent execution” stack.

What to watch
- How quickly Manus’s agent shows up inside Meta products.
- Data/privacy guarantees for existing Manus customers and whether enterprise SLAs/APIs remain unchanged.
- Whether the “independent operations” promise holds over time (pricing, roadmap, and feature continuity).

Here is a summary of the discussion on Hacker News:

**Skepticism on Metrics and Unit Economics**
A major point of discussion involved crumbing the numbers on Manus’s reported usage. Commenters calculated that processing "147 trillion tokens"—as claimed in the announcement—would likely cost hundreds of millions of dollars in API fees (e.g., via Claude/Sonnet), vastly exceeding the rumored $100M ARR (Annual Recurring Revenue). Critics suggested the company might be "selling $10 bills for $5" to inflate user metrics, arguing that the unit economics don't add up without massive capital burn.

**The "Wrapper" vs. Tech Debate**
Opinions were divided on the technical legitimacy of the product. Some users dismissed Manus as merely a "wrapper" around existing models like Claude, suggesting Meta essentially bought a high-performing marketing team rather than proprietary technology. A heated side-thread debated the company's origins; while some accused the team of pivoting from a "failed" operation in China to focus on hype in Singapore, others defended the founder (Xiao Hong), citing his previous success with profitable tools like Monica AI as proof of real product value.

**Valuation and "Wild" Timelines**
Commenters expressed shock at the speed of the exit, noting the startup went from a massive funding round to acquisition in a matter of months. While a rumor circulated in the thread of a $4B deal size, users compared the move to Facebook’s acquisition of WhatsApp—though many felt WhatsApp had a clearer competitive moat at the time. The consensus described the current environment as "financial engineering," where companies are built specifically for rapid acquisition rather than long-term sustainability.

**Founder Incentives vs. Employment**
The acquisition sparked a broader conversation about career implied strategy in the current AI bubble. Users referenced financial columnist Matt Levine, arguing that the market currently incentivizes building a startup to sell quickly (making ~$75M+ in a few years) rather than being a highly paid employee or early hire, where equity deals are often poor and dilution is high. The sentiment was that this is a "gold rush" where "kitchen table" apps are being flipped for life-changing sums before the bubble bursts.

### AI is forcing us to write good code

#### [Submission URL](https://bits.logic.inc/p/ai-is-forcing-us-to-write-good-code) | 263 points | by [sgk284](https://news.ycombinator.com/user?id=sgk284) | [194 comments](https://news.ycombinator.com/item?id=46424200)

AI Is Forcing Us To Write Good Code (Steve Krenzel)
Thesis: Agentic coding tools only work well when the codebase enforces discipline. What used to be “optional” best practices become mandatory guardrails, or you get a Roomba-smears-dog-poop mess. With strong constraints, agents can grind until the only path left is the correct one.

What the team changed:
- 100% code coverage: Not to “eliminate bugs,” but to force every new/changed line to be proven with an executable example. At 100%, coverage becomes a clear to-do list; ambiguity disappears, unreachable code gets deleted, edge cases are explicit, and reviews improve. The goal is guardrails for the agent, not gaming a metric.
- Namespaces and small files: The filesystem is the agent’s UI. Descriptive paths (billing/invoices/compute.ts over utils/helpers.ts) and many small, well-scoped files make retrieval and context loading reliable, avoiding truncation and speeding up agent reasoning.
- Fast guardrails: Keep the agent on a short leash—make a change, test, repeat. Their npm test spins up a fresh DB, runs migrations, and executes the full suite with high concurrency and strong isolation. Heavy caching of third-party calls cuts 20–30 minutes down to ~1 minute for 10,000+ assertions, making constant runs feasible.
- Ephemeral by default: One command (new-feature <name>) creates a git worktree, copies local config (like .env), installs deps, and launches an agent to interview you and draft a PRD. Latency matters—if setup takes seconds, you’ll use it constantly; minutes means you won’t.
- Concurrent environments: You’ll run many agents and environments at once; the system needs to support parallel work without manual tinkering. (Section trails off, but the principle is clear.)

Why it matters for HN readers:
- It reframes “best practices” as prerequisites for effective LLM/agent workflows.
- It offers concrete, reproducible tactics (100% coverage, namespacing, tiny files, cached tests, one-command environments).
- The beekeeping mental model—coordinating many fast, disposable processes—captures where AI-native engineering practices are headed.

Based on the discussion, here is a summary of the commenters' reaction to the submission:

**The Tautology Trap: Who Tests the Tester?**
The most prominent critique addressed the article’s reliance on 100% test coverage. Commenters like **KurSix** and **stngrychrls** argued that if an AI agent writes both the implementation and the tests, high coverage creates an "illusion of reliability." It risks becoming a tautology where the agent simply verifies its own hallucinations or flawed logic.
*   **Proposed Solutions:** **jshrbkff** suggested "mutation testing" (inserting bugs to ensure tests fail) to verify the test suite's quality. **ben_w** proposed a "multiple minds" approach, where humans review AI code and AI reviews human code to cover respective blind spots.

**Advanced Metrics vs. "Vanity" Coverage**
The technical depth of the conversation shifted toward *how* coverage is measured.
*   **MCDC Standards:** **smarx007** and **zwfss** argued that standard code coverage is insufficient for high reliability. They pointed to **Modified Condition/Decision Coverage (MCDC)**—a rigorous standard used in avionics (DO-178C) and safety-critical systems—as a better target than simple line coverage.
*   **Property-Based Testing:** User **r** suggested moving away from example-based tests entirely in favor of property-based testing (using tools like Hypothesis or FsCheck) to enforce broader logic constraints.
*   **Subjectivity:** **rdlw** and **tmpdx** pushed back on the premise entirely, noting that the industry lacks a consensus on what "good code" actually is, and that enforcing arbitrary metrics often leads to worse outcomes, regardless of AI involvement.

**Formal Verification as the True Guardrail**
User **tmbrt** shared a relevant experiment using **TLA+** (a formal specification language). By writing high-level specs in TLA+ and having Codex implement them, they achieved "ugly but correct" code. They argued this shifts the "good code" requirement upstream: the human focuses on formal logic and specs, while the AI handles the implementation details, verified by the spec rather than just unit tests.

**Skepticism on Motives**
Several users (**zwnw**, **tmpdx**) viewed the post with skepticism, identifying it as a sales pitch for the author's AI product. They expressed doubt that AI forces good code, suggesting instead that without strict human oversight, AI tends to accelerate the production of "spaghetti code" or subtle bugs.

### Show HN: Stop Claude Code from forgetting everything

#### [Submission URL](https://github.com/mutable-state-inc/ensue-skill) | 173 points | by [austinbaggio](https://news.ycombinator.com/user?id=austinbaggio) | [205 comments](https://news.ycombinator.com/item?id=46426624)

Ensue Memory Network: a persistent “brain” for your AI chats

- What it is: A Claude Code skill that gives your AI a long-term memory. Instead of starting from zero every session, it builds a persistent knowledge tree so prior research, preferences, and decisions inform new work.
- How it works: It can auto-log session context and tool usage (toggleable), and supports manual “remember”/“recall.” You can ask things like “what do I know about caching strategies?” or “check my research/distributed-systems notes,” and it pulls from your accumulated knowledge.
- Setup: In Claude Code, add the plugin (mutable-state-inc/ensue-skill), set ENSUE_API_KEY from the Ensue dashboard, and optionally enable ENSUE_READONLY=true to disable auto-logging. Backend runs via the Ensue API (api.ensue-network.ai) with a web dashboard and docs.
- Why it matters: Reduces repetitive context-setting, helps decisions compound across projects, and nudges LLM workflows toward more agentic, personalized tooling.
- Caveats: Your memory lives in a third-party service (consider data sensitivity); backend isn’t in this repo; currently oriented around Claude Code.
- Quick stats: 223 GitHub stars, 6 forks, 2 contributors at time of posting. Links: ensue.dev and the GitHub repo mutable-state-inc/ensue-skill.

**Summary of Discussion:**

The discussion is overwhelmingly skeptical of using a third-party SaaS for AI memory, with most users arguing that simple, local methods are superior.

*   **The "Markdown" Consensus:** Multiple commenters argue that maintaining local text files (e.g., `CLAUDE.md`, `plan.md`, or implementation logs) in the repository is more effective, private, and reliable than an external API. Users detailed workflows where they simply instruct the specific agent to read and update these files to maintain context across sessions.
*   **Feature Redundancy:** Users noted that between massive context windows, context caching, and native improvements by model providers (Anthropic), the need for "memory as a service" is diminishing. There is a strong sentiment that this feature set will eventually be "sherlocked" (built-in) by the foundation models.
*   **Privacy & Complexity:** Several users rejected the idea of requiring an API key and sending data to a third party (`ENSUE_API_KEY`) when local Markdown files offer zero latency and full privacy.
*   **Creator’s Defense:** One of the project creators (`chrstntyp`) argued that the tool's value lies not just in memory, but in **portability**—allowing context to be shared smoothly between different tools (e.g., passing context from Claude Code to Cursor) and team members without manual file management.
*   **Rebuttal:** Commenters countered the portability argument by stating that Git repositories already serve as the ultimate shared source of truth for teams and different tools, making a separate memory abstraction unnecessary.

### Asking Gemini 3 to generate Brainfuck code results in an infinite loop

#### [Submission URL](https://teodordyakov.github.io/brainfuck-agi/) | 119 points | by [TeodorDyakov](https://news.ycombinator.com/user?id=TeodorDyakov) | [89 comments](https://news.ycombinator.com/item?id=46418966)

A post argues that Brainf*ck might be the ultimate LLM/AGI stress test, pointing to Gemini 3 getting stuck in an apparent infinite-output loop when asked to generate Brainf*ck code. The author claims the language exposes where pattern-matching breaks and real reasoning is required.

Core arguments:
- Data scarcity: There’s virtually no Brainf*ck corpus compared to mainstream languages, so models can’t coast on memorization or template matching.
- Anti-literate programming: Brainf*ck’s lack of names, structure, or comments forces precise mental models of the tape/Pointer semantics; reading existing code can even hurt comprehension.
- Repetition trap: The language’s repetitive token patterns can push autoregressive decoders into self-reinforcing loops, turning “next-token likelihood” into runaway repetition.

Why it matters: It highlights a brittle edge of current LLMs—decoding and planning under extreme minimalism—where tool-free sampling can collapse. If a model can reliably synthesize nontrivial Brainf*ck from specs, the argument goes, it’s showing reasoning beyond pattern recall.

Caveat: Whether Brainf*ck is a proxy for “intelligence” is debatable; success may depend more on search, execution-in-the-loop, or decoding controls than general cognition. Still, it’s a sharp probe of failure modes in code generation and token-level dynamics.

Based on the discussion, commenters largely validated the submission’s premise, offering specific examples of Gemini’s failures while broadening the conversation to the general limitations of current LLM reasoning.

**Confirmation of Gemini’s Instability**
Multiple users corroborated the report, sharing their own experiences where Gemini (particularly Gemini 3/Advanced) enters infinite loops.
*   **"Thinking Death":** Users described a phenomenon where the model gets "stuck behind a fourth wall," leading to repetitive outputs, internal thought traces accidentally leaking into the final response, and bizarre formatting breakdowns (e.g., repeating "actual English" indefinitely).
*   **Technical Explanations:** One user noted that running open-weights models locally rarely results in infinite loops due to adjustable "repeat penalty factors," suggesting this is a tuning or guardrail failure in hosted models.
*   **The "AntiGravity" Trigger:** Several users mentioned specific triggers, such as the "AntiGravity" Python IDE prompt, that consistently crash the model logic.

**Debate on "Anti-Literate" Programming**
A sub-thread debated the author’s claim that Brainf*ck is inherently unreadable.
*   **Comments vs. Abstraction:** While some argued that Brainf*ck is technically readable because any non-command character is treated as a comment, others countered that it lacks *abstraction*.
*   **Mental Modeling:** User *btrct* made a key distinction: true readability allows a developer to ignore internal details (like a janitor changing a bulb without understanding the power grid). Brainf*ck forces the programmer (or LLM) to mentally simulate the entire pointer/tape state, making it a valid test of deep reasoning rather than just pattern matching.

**Broader Reasoning Failures (Hallucinations)**
The conversation expanded beyond code to other areas where "smart" models fail at strict constraints:
*   **Fake Evidence:** User *tessierashpool9* shared that when asked to solve a visual puzzle (Ubongo), Gemini Deep Research not only failed but hallucinated a non-existent academic lecture to justify its wrong answer.
*   **List Logic:** Others reported inability to generate factual lists with negative constraints (e.g., Gameboy games *excluding* incompatible ones, or standup specials within a specific date range), with models stubbornly refusing to verify data against the prompt's rules.

**Skepticism of "Intelligence"**
The consensus trended toward skepticism regarding "AGI" hype. Users expressed frustration with "overconfident, eloquent assistants" that can write prose but fail at binary verification tasks. Some noted they have returned to using faster, "dumber" models because the "smart" reasoning models are too prone to getting stuck in 10-minute loops or generating elaborate nonsense.

### Rich Hickey: Thanks AI

#### [Submission URL](https://gist.github.com/richhickey/ea94e3741ff0a4e3af55b9fe6287887f) | 261 points | by [austinbirch](https://news.ycombinator.com/user?id=austinbirch) | [70 comments](https://news.ycombinator.com/item?id=46415945)

Rich Hickey (creator of Clojure) posts “Thanks AI!”, a scathing holiday roast of generative and agentic AI. Spurred by a cloying, AI-penned “thank you” email (labeled “Claude Haiku 4.5”), he writes a satirical letter “thanking” AI vendors while arguing the tech is creating more problems than it solves.

Key points Hickey argues:
- Consent and ownership: AI outfits trained on humanity’s creative output without permission, then claim ownership over the spoils.
- Education and careers: erosion of learning and elimination of entry-level roles, breaking the apprenticeship ladder for future developers.
- Productivity mirage: dev time wasted coaxing “BS generators” instead of mentoring juniors; cost cuts that degrade quality, integrity, and customer satisfaction.
- Environment and infrastructure: higher energy use and utility impacts.
- Support and search: bots replacing humans, worse service; search results replaced by shallow summaries; the web flooded with AI slop that buries human work.
- Privacy and product bloat: “AI features” everywhere, many requiring deep data collection.
- Culture and art: music and expression reduced to “robot parrots.”
- The big picture: a “con” selling tiny cost savings for outsized social and quality losses.

He warns agentic AI will supercharge spam/BS across all remote channels, making interactions suspect and filtering a permanent tax. Closing line: “When did we stop considering things failures that create more problems than they solve?”

Why it matters: A blunt critique from a widely respected language designer, crystallizing ongoing HN debates around data consent, energy costs, product quality, workforce development, and AI-in-everything bloat—and whether different guardrails or incentives could change the calculus.

**Discussion Summary:**

The discussion on Hacker News breaks down into a heated debate over the legitimacy of AI skepticism, the authority of "old guard" programmers, and the tangible impacts of generative AI on the software industry.

*   **Validation vs. "Cynical Bandwagoning":** A significant portion of the community feels validated seeing Rich Hickey join Rob Pike (creator of Go) in publicly denouncing current AI trends. Users described it as "heartwarming" to see respected figures articulate the frustrations many feel regarding the "hustle" and intrusiveness of the AI industry. Conversely, a vocal contingent dismisses these critiques as "cynical bandwagoning" by wealthy elites. Detractors argue that figures like Hickey and Pike—who have made fortunes in tech/fintech—are in a privileged position to take the moral high ground, while everyday developers are pressured to use these tools to remain competitive.
*   **The Erosion of Careful Design:** Commenters highlighted the stark contrast between Hickey’s famous philosophy (which prioritizes deep thought and careful design before coding) and the reality of AI workflows (iterative prompting to generate thousands of lines of code). Users noted that AI turns programming into "repetitive" implementation work rather than problem-solving, potentially validating Hickey's concern about the loss of the "apprenticeship ladder."
*   **Defining "Slop" and the "AIville" Context:** Several users identified the specific trigger for Hickey’s rant: a "random acts of kindness" bot campaign from an "AIville" project that sent unsolicited, hallucinated emails. This sparked a semantic debate about the term "AI slop," with some users suggesting terms like "barf" better describe unwanted, forcibly injected low-quality content that users must filter out.
*   **Systemic Harm vs. Inevitable Progress:** The discussion touched on the broader ethics of the industry. Some likened AI engineers to workers who don't realize they are the "bad guys" disrupting the social contract by scraping work without credit. Others countered that this is standard technological disruption (similar to the Internet or Mobile shifts) and that Hickey’s "doomer" outlook ignores the immense utility and opportunities AI provides for solving large-scale problems.

**Top Sentiment:** Polarized. While many celebrate Hickey as a voice of reason protecting the craft of software engineering, others view his and Pike's comments as arrogant gatekeeping from a previous generation of tech luminaries.

### Show HN: Z80-μLM, a 'Conversational AI' That Fits in 40KB

#### [Submission URL](https://github.com/HarryR/z80ai) | 485 points | by [quesomaster9000](https://news.ycombinator.com/user?id=quesomaster9000) | [111 comments](https://news.ycombinator.com/item?id=46417815)

Z80-μLM: a 2-bit quantized “micro” language model that runs on a 1970s Z80

A retrocomputing flex with real ML chops: Z80-μLM packs a conversational model into a ~40KB CP/M .COM binary that runs on a 4MHz Z80 with 64KB RAM. It’s not ChatGPT—responses are terse and character-by-character—but it’s surprisingly personable and fully trainable.

Highlights
- End-to-end on an 8-bit CPU: integer-only inference (no floats), 16-bit accumulators, ReLU MLP, autoregressive char output.
- Extreme compression: 2-bit weights {-2,-1,0,+1}, packed 4 per byte; QAT (quantization-aware training) keeps it stable and useful.
- Clever input encoding: trigram hash into 128 buckets makes it typo-tolerant and mostly word-order invariant—great for short prompts.
- Fits vintage constraints: ~40KB includes inference, weights, and a chat UI; runs in CP/M’s TPA.
- Tools included: Python training pipeline, export to .COM, data generation helpers (Ollama or Claude), class balancing.
- Demos: “tinychat” (snappy, personality-driven replies) and “guess” (a 20-questions-style game).

What it’s good for
- Short, fuzzy, categorized interactions with a distinct voice on truly constrained hardware.
- Educational/nostalgic exploration of QAT, hashing, and low-level inference loops (tight Z80 MAC inner loops shown).

What it’s not
- A grammar-aware, context-heavy chatbot or generator of long, novel prose.

A delightful proof that with smart encoding and QAT, you can squeeze a chatty ML model into 1976-era silicon.

Here is today's digest of the top story on Hacker News, including a summary of the community discussion.

**Z80-μLM: A 2-bit Language Model for 1970s Hardware**
This project represents a fascinating intersection of modern machine learning and vintage computing. Z80-μLM is a conversational model compressed into a ~40KB CP/M binary capable of running on a 4MHz Z80 CPU with 64KB of RAM. By utilizing extreme compression (2-bit weights), integer-only inference, and a clever trigram input encoding system, the model delivers terse but personable responses and can play a "20 Questions" style guessing game. While it lacks the grammatical fluidity of ChatGPT, it serves as an educational proof-of-concept for QAT (quantization-aware training) and low-level inference loops on severely constrained hardware.

**Summary of the Discussion**
The discussion thread blends technical curiosity with deep nostalgia, focusing on the implications of running "modern" AI concepts on ancient silicon.

*   **Historical "What Ifs":** A major theme involves users wondering how history would have changed if this specific architecture existed in the 1970s or 80s. Commenters compared it favorably to ELIZA, noting that while ELIZA ran on mainframes, this runs on consumer-grade 1976 hardware. Several users expressed that seeing this on a ZX Spectrum or C64 back in the day would have been "mind-blowing" and pure science fiction.
*   **The "Small AI" Movement:** The project sparked a debate about the "Minimally Viable LLM." Users discussed the potential for highly localized, private AI assistants running on cheap smartphones (sub-$300) or constrained chips, referencing Andrej Karpathy's concept of a "cognitive core." Some speculated that big tech pushes massive RAM-heavy models to maintain a moat, whereas efficient, small models offer a path to decentralized AI.
*   **Technical Deep Dives:** There was specific praise for the implementation quirks, such as the lack of attention mechanisms (relying on trigrams instead) and the use of MLPs (Multi-Layer Perceptrons) rather than Transformers. Security researchers questioned if one could hide undetectable backdoors in such small weight clusters, with others noting that small networks are generally easier to reverse-engineer.
*   **Emulation and Access:** Several users successfully ran the demo on browser-based CP/M emulators, reporting that the guessing game works surprisingly well. However, a sub-thread emerged regarding frustration with Imgur, which is currently blocking UK traffic due to the Online Safety Act, preventing many British users from seeing the project screenshots.

### UK accounting body to halt remote exams amid AI cheating

#### [Submission URL](https://www.theguardian.com/business/2025/dec/29/uk-accounting-remote-exams-ai-cheating-acca) | 187 points | by [beardyw](https://news.ycombinator.com/user?id=beardyw) | [199 comments](https://news.ycombinator.com/item?id=46420289)

ACCA to end remote exams, citing AI-fueled cheating

- The Association of Chartered Certified Accountants (ACCA), the world’s largest accounting body (260k members, 500k+ students), will largely halt remote-proctored exams from March, reverting to in-person testing except in exceptional cases.
- CEO Helen Brand said cheating tools are outpacing safeguards, with AI pushing the problem to a “tipping point.”
- Remote exams were introduced during COVID; regulators have since flagged widespread cheating across the profession. EY paid a record $100m in 2022 over ethics-exam cheating; other Big Four firms have faced penalties and probes.
- The ICAEW still allows some online exams, but ACCA notes few “high-stakes” assessments now permit remote invigilation.

Why it matters
- Remote proctoring’s tech arms race is losing to cheap, ubiquitous AI assistance and covert tools; institutions are defaulting to the simpler integrity model of physical test centers.
- Expect more rollbacks of remote high-stakes exams across professional credentials and universities, raising access/travel burdens but lowering reputational and regulatory risk.
- Opens space for new assessment designs resilient to AI (oral/practical evaluations, scenario-based/open-book with individualized data) and for hybrid/localized test-center models that preserve some accessibility without remote proctoring.

Based on the discussion, here is a summary of the comments regarding ACCA’s decision to end remote exams:

**The Evolution of Cheating**
*   Commenters distinguish between "old school" open-book exams and the current AI reality. Several users note that previously, open-book tests still required understanding the material to look up references or apply concepts.
*   In contrast, users argue AI (specifically LLMs) has fundamentally broken this model by simply solving the problem rather than acting as a reference, removing the cognitive load entirely. As one user put it, AI has "democratized cheating."

**Impact on Honest Students**
*   A major point of contention is the use of **normalized grading (curving)**. Participants noted that when a significant portion of a class cheats using AI, they artificially inflate the average score.
*   This forces honest students to either fail or resort to cheating themselves just to remain competitive, creating a "race to the bottom" where integrity is penalized.

**Professional Validity and "Gatekeeping"**
*   The discussion drifted toward the utility of the exams themselves. Some users argued that if an AI can easily pass a professional certification, the profession might be outdated or the exam is testing the wrong things (memorization vs. application).
*   Others characterized these exam bodies as "profit extraction" and "gatekeeping" schemes, noting the high fees paid to vendors like Pearson VUE.
*   There was a debate on the "Fake it 'til you make it" strategy. While some wondered if cheaters would be fired immediately upon entering the workforce, others argued that getting the credential is the hardest part, and most people learn the actual job on the fly.

**Proposed Solutions**
*   **In-person invigilation:** Many agreed that returning to physical test centers (pen and paper) is the only viable solution to preserve integrity, despite the inconvenience.
*   **Weighted grading:** One suggestion was to keep online quizzes for low-stakes practice (e.g., 10% of the grade) while mandating that 90% of the grade comes from secure, physical exams.
*   **Oral/Practical exams:** Moving away from multiple-choice or essay questions toward methods that are harder to automate.

**Accounting specifics**
*   One commenter noted that while AI threatens the *exam*, the *profession* of accounting requires exactness. Because LLMs suffer from hallucinations, they argued humans are still required to verify outputs, meaning a cheater who relies wholly on AI might struggle with the precision required in real-world auditing or tax work.

### Meta's ads tools started switching out top-performing ads with AI-generated ones

#### [Submission URL](https://www.businessinsider.com/meta-ai-generating-bizarre-ads-advantage-plus-2025-10) | 133 points | by [zdw](https://news.ycombinator.com/user?id=zdw) | [87 comments](https://news.ycombinator.com/item?id=46424733)

Meta’s ad AI is going off-script, swapping brand creatives with weird, auto-generated images — sometimes even when advertisers say they’ve turned AI off.

What happened
- True Classic says Meta replaced its top-performing ad (a millennial man in a fleece set) with an AI-generated “cheerful yet unnatural” grandma — and ran it for three days before customers flagged it. The brand targets men 30–45.
- Other glitches: a European shoe ad where a model’s leg bent the wrong way, and an e-bike ad featuring a car trunk… flying through clouds.
- Advertisers blame toggles like “test new creative features,” “automatic adjustments,” and “Advantage+ creative.” Several say these settings flipped back on after being disabled, pushing AI variants they never meant to run.
- An agency managing ~$100M/year on Meta says it now audits accounts 2–3 mornings a week to ensure AI enhancements stay off, taking up to an hour per account.
- Some marketers say AI versions didn’t show in campaign previews, leaving them blindsided.
- Meta’s response: millions of advertisers see improved performance with Advantage+; image generations can be reviewed pre-flight; features are being improved based on feedback.

Why it matters
- Brand safety and trust: off-brand AI creatives risk customer backlash and retailer relationships.
- Control vs. automation: black-box defaults and auto-reenabled settings erode confidence, increase ops overhead, and may chill adoption of Meta’s AI stack.

What to watch
- Whether Meta changes defaults, adds a true global opt-out, or improves preview parity.
- More reports of “automation creep” across ad platforms — and how performance trade-offs stack against brand risk.

If you run Meta ads
- Audit at account and campaign level: “test new creative features,” “automatic adjustments,” and all Advantage+ creative options.
- Lock creative variants, require manual review, and set alerts/screenshot logging for unexpected asset swaps.

Based on the Hacker News discussion, here is a summary of the conversation:

**Ad Manager “Dark Patterns” and UI Glitches**
The discussion was dominated by media buyers and engineers who validated the article's claims. One user (`rdsymbl`), who manages ~$100k in ad spend, described the Meta Ad Manager as having a UX designed to make "accidentally" enabling expensive features inevitable. They noted that specific toggles (like "related media" or "promo codes") often silently re-enable themselves after being manually disabled, requiring complex audits to fix. A former Meta employee (`chrntstr`) chimed in to suggest this is likely the result of an internal culture that aggressively optimizes for metrics, often ignoring the negative side effects of those optimizations.

**Malice vs. Indifference**
Commenters debated whether these issues are intentional theft or simply corporate incompetence.
*   **The "Indifference" Argument:** User `brdlys` argued against intentional malice, suggesting that Meta simply doesn't care about "small" businesses spending $100k/year. They posited that as long as the billion-dollar accounts are flowing, quality assurance for smaller tiers is a low priority.
*   **The "Enshittification" Argument:** User `Terr_` compared this to consumer-side operational habits, noting that just as apps often reset privacy permissions after updates, Meta is now applying those same abusive "middleman" patterns to advertisers.

**Legal Action and Refunds**
There was significant speculation regarding potential legal fallout. Several users believe a class-action lawsuit is inevitable due to the wasted ad spend caused by these "glitches."
*   User `nm` claimed they recently successfully charged back $20k after the ads dashboard broke and ran unauthorized ads that produced internal error messages.
*   Discussion on legal strategy (`toss1`) noted that while individual lawsuits are cost-prohibitive against a giant like Meta, the systematic nature of these "re-enabled toggles" makes a class action more viable.

**Broader Ad Tech Skepticism**
The conversation drifted into the general state of online advertising:
*   **Wasted Spend:** Users related anecdotes of seeing ads for local events that had already passed, questioning the validity of the metrics Meta reports to advertisers (`nt`, `gs17`).
*   **AI Quality:** One user (`x0x0`) noted that their attempts to use Meta's AI features resulted in "gibberish" and random mutations of existing ads that made no sense.
*   **Market Contraction:** A sidebar discussion highlighted that ad revenue is currently crashing for podcasts and YouTubers. User `rchd` attributed this to the drying up of VC funding for Direct-to-Consumer (DTC) brands (like mattress and VPN companies), which were previously propping up that sector of the ad economy.

### Show HN: Evidex – AI Clinical Search (RAG over PubMed/OpenAlex and SOAP Notes)

#### [Submission URL](https://www.getevidex.com) | 36 points | by [amber_raza](https://news.ycombinator.com/user?id=amber_raza) | [29 comments](https://news.ycombinator.com/item?id=46422812)

I’m ready to write the digest, but I don’t see the submission details yet.

Please share one of the following:
- The Hacker News thread URL or item ID
- The article URL
- Or paste the text you want summarized

Optional:
- Preferred length (blurb ~100 words, standard ~200–300, deep dive ~500+)
- Any angle to emphasize (e.g., developer impact, business implications, privacy, open source)

Note: I don’t have live browsing, so if you want more than a title-level summary, please include the article text or key points.

Based on the discussion provided, here is the summary of the submission and the conversation surrounding it.

**The Submission:**
While the specific project title isn't listed, the discussion revolves to a **"Show HN" for a new medical AI search tool (likely named "Evidex")**. The tool differentiates itself by using "Live Search" (querying PubMed/APIs in real-time) rather than relying on pre-indexed Vector Databases (RAG), aiming to solve issues regarding data freshness, hallucinations, and safety in medical research assistance.

**The Discussion:**
The thread is a technical debate regarding the reliability of LLMs in medicine and the architectural trade-offs between semantic search and various indexing strategies.

*   **Garbage In, Garbage Out:** User `crgdltn` cites John Ioannidis's research on false findings, arguing that LLMs trained on "fresh garbage" papers simply scale inaccuracies. They note that standard LLMs (ChatGPT, Gemini) often miss fatal methodological flaws in studies unless specifically prompted with multi-disciplinary "critic" personas (e.g., asking the AI to review as an epidemiologist).
*   **Critic Agents & Hierarchy:** The author (`amber_raza`) acknowledges this, explaining that their system uses a strict "Hierarchy of Evidence" (prioritizing Meta-Analyses/Cochrane over observational studies) and employs a secondary "Critic Agent" specifically prompted to act as a methodologist to flag limitations before synthesis.
*   **Vector DB vs. Live APIs:** A significant portion of the debate (`bflsch` and `amber_raza`) focuses on "Freshness." `bflsch` argues that pre-indexed vector stores are dangerous for medicine because they may miss immediate retractions or safety alerts. The author clarifies they intentionally avoided Vector DBs for this reason, instead using live APIs (PubMed/EuropePMC) to ensure immediate accuracy and cover the "Long Tail" of obscure case reports that are too expensive to keep indexed in vector stores.
*   **Semantic Drift:** In a discussion on search precision with `brdslv` and `pdyc`, the author argues that semantic search (vectors) suffers from "Semantic Drift" (medically distinct concepts appearing similar). They defend a "Hybrid" approach: using deterministic Boolean/MeSH (Medical Subject Headings) queries for retrieval precision, and using LLMs only for the final synthesis.
*   **Privacy & Tech Stack:** There are brief critiques regarding the use of Clerk for authentication due to tracking/GDPR concerns (`bflsch`), which the author admits was an oversight during the launch rush and promises to fix.

### Show HN: Vibe coding a bookshelf with Claude Code

#### [Submission URL](https://balajmarius.com/writings/vibe-coding-a-bookshelf-with-claude-code/) | 273 points | by [balajmarius](https://news.ycombinator.com/user?id=balajmarius) | [197 comments](https://news.ycombinator.com/item?id=46420453)

A writer with 500+ books finally tackled a years-delayed cataloging project by leaning on AI for the tedious parts and reserving human judgment for the edges. Using Claude Code to write and run scripts, they turned 470 quick photos into a living, tactile web bookshelf.

Highlights:
- Pipeline: batch-renamed and converted images, then used OpenAI’s vision API to extract title/author/publisher into JSON; ~90% accuracy was “good enough,” with manual fixes for the rest.
- Covers: fetched via Open Library, auto-scored for quality, and fell back to Google Images (SerpAPI) when needed; only ~10 manual Photoshop edits for 460 books.
- UI as “spines,” not a cover grid: dominant color extraction and contrast text, plus spine width mapped to page count with slight variation for realism.
- Motion that feels right: a scroll-based tilt built with Framer Motion; performance fixed by moving from React state updates to motion values and springs.
- Lesson: when execution cost collapses, the human job shifts to taste, constraints, and knowing when 90% is sufficient.

Why it matters:
- A clear template for “good-enough” AI pipelines that turn messy real-world data into delightful interfaces.
- Shows AI as force-multiplier for execution while amplifying human judgment and product feel.

The discussion threads focus on the practical conceptual limits of "vibe coding"—using LLMs to handle execution while humans handle taste—specifically regarding project complexity, architecture, and the writing style of the post itself.

**The Scalability Wall and Context Management**
The most prominent debate concerns the "perfect size" for these projects. User `spcysrnm` argues that this workflow thrives on small, contained apps but breaks down as interdependence grows; once the context window is exceeded, LLMs begin generating subtle bugs or hallucinating code.
*   **The solution:** Several users (`xur17`, `cube2222`) suggest that as complexity scales, the human role shifts to maintaining a strict "Implementation Plan" (often a Markdown file) rather than just brainstorming code.
*   **Architectural discipline:** `pgpp` suggests that "vibe coding" requires "vibe architecting." By applying classic 1994 software design patterns (strict interfaces, decoupled modules), developers can feed AI smaller, isolated chunks of context, preventing the model from degrading on large codebases.

**Defining the Skill Set**
Commenters debated whether this constitutes "carpentry" or just assembling flat-pack furniture.
*   `crs` posits that "successfully building an IKEA shelf doesn't make you a carpenter," implying this approach lacks deep engineering rigor.
*   Others (`rvz`) noted that while acceptable for a personal bookshelf, this methodology is dangerous for critical infrastructure (like air traffic control), where rigorous testing is non-negotiable.
*   The consensus leans toward the idea that while execution cost has collapsed, the human requirement for defining requirements and reviewing output has increased.

**Meta-Critique on Writing Style**
A significant portion of the thread (`Tiberium`, `AtreidesTyrant`) critiqued the writing style of the submission itself. Users felt the text—characterized by dramatic phrasing and "LinkedIn-style" bullet points—felt unmistakably AI-generated or overly polished, sparking a side conversation about how AI is homogenizing writing into a generic "marketing" voice that users find increasingly fatiguing.

### Five Years of Tinygrad

#### [Submission URL](https://geohot.github.io//blog/jekyll/update/2025/12/29/five-years-of-tinygrad.html) | 83 points | by [iyaja](https://news.ycombinator.com/user?id=iyaja) | [48 comments](https://news.ycombinator.com/item?id=46422757)

Tinygrad’s 5-year check-in: a software-first path to compete with NVIDIA

- Timeline and scope: First commit was Oct 17, 2020. After nearly three years post-fundraising, tiny corp is six people. tinygrad sits at ~18,935 lines of code (excluding tests), targeting ~20,000 lines when “finished.”

- Strategy: Build a fully sovereign software stack before any chip work. The author argues chips aren’t the hard/expensive part; usable training comes from software. Despite many companies taping out capable chips, only Google and NVIDIA are widely used for training because of their software stacks.

- Tech milestones: Actively removing LLVM to make tinygrad zero-dependency (pure Python) while driving AMD GPUs. The stack includes a frontend, graph compiler, runtimes, and drivers. The author claims tinygrad now outperforms PyTorch on many workloads.

- Philosophy: “Elon process for software”—strip requirements to the essentials. Argues most software bloat comes from maintaining compatibility with other abstractions; claims “98%” of lines are workarounds. Example: LLM servers should just offer an OpenAI-compatible API for fast GPU inference; instead, current stacks are millions of lines. tinygrad aims to be ~1000x smaller by focusing only on the goal.

- Org model: A “deconstructed company”—mostly public via Discord and GitHub. Hiring happens through repo contributions; one weekly meeting; the goal is simply to make tinygrad better.

- Funding and deals: ~$2M/year from a computer sales arm. A public-facing contract with AMD to get MI350X onto MLPerf for Llama 405B training.

- Mission: “Commoditize the petaflop.” The team expects another five years of work but sees the lean, sovereign software stack as the right way to challenge incumbents.

Based on the comments, the discussion focused on the validity of the project's metrics, the leadership style of founder George Hotz, and the technical viability of challenging NVIDIA.

**Lines of Code as a Metric**
A significant portion of the debate centered on the project's size (~19k lines of code). Critics argued that LOC is a poor metric for productivity or quality, suggesting that "code golfing" creates complexity rather than reducing it. Defenders, including Hotz, argued the low count represents a reduction in "incidental complexity" and dependencies. They contrasted this "lean" approach with the bloat found in major tech stacks, positioning the removal of dependencies (the "Elon process" or TRIZ) as a competitive advantage.

**The "George Hotz" Factor**
Opinions on founder George Hotz were polarized.
*   **Supporters** praised him as a "true engineer" and a "doer" willing to tackle problems from first principles, contrasting him with modern corporate engineers.
*   **Skeptics** pointed to his short-lived attempt to fix Twitter's search function as evidence that his "burn it down" philosophy fails when confronted with real-world legacy complexity.
*   **Clarification:** Hotz (account `grghtz`) personally responded to questions about his management style. He clarified that while his hardware company (comma.ai) requires in-person work, **tinygrad is hybrid/remote-first**, utilizing weekly meetups in locations like Hong Kong.

**Business and Strategy**
*   **Funding:** Users questioned whether the reported ~$2M/year in hardware sales could sustain the team, though others noted the company raised ~$5.1M in venture capital in 2023.
*   **The AMD Deal:** Commenters viewed the contract to get AMD chips working for Llama training as the highest-value aspect of the project. Users noted that anything helping chipmakers break NVIDIA’s CUDA monopoly effectively captures immense market value.
*   **Competition:** Some questioned if Tinygrad can displace PyTorch, suggesting PyTorch might simply improve its own AMD backends (Inductor) to render Tinygrad moot. Others discussed alternative languages like Mojo and Julia, debating their viability in the ML infrastructure space.

### Spacetime as a Neural Network

#### [Submission URL](https://benr.build/blog/autodidactic-universe) | 9 points | by [bisonbear](https://news.ycombinator.com/user?id=bisonbear) | [4 comments](https://news.ycombinator.com/item?id=46422562)

Spacetime as a Neural Network (explainer on Smolin et al.’s “Autodidactic Universe”)

- Core idea: A 2021 paper by Lee Smolin, Jaron Lanier, and collaborators shows a striking correspondence between general relativity written in the Plebanski formulation and the learning equations of a Restricted Boltzmann Machine (RBM). Using matrices as the common language, the mapping is: quantum matter fields ≈ network layers; gauge/gravity fields ≈ network weights; evolution of laws ≈ learning (weight updates).

- Why it’s interesting: It suggests the universe’s “laws” could be learnable and have evolved—geometry acting as a “consequencer” that stores information from the past, akin to how neural network weights encode data across training. The post uses a river–canyon analogy: fast variables (water) shape slow variables (canyon walls), which then constrain future flows.

- Big caveat: This is a correspondence, not an equivalence. It works when continuous spacetime is discretized into finite N×N matrices; true gravitational physics lives in the N→∞ limit, where the RBM analogy breaks. Still, the rhyme between GR and ML dynamics is notable.

- Philosophical angle: Smolin’s “Time Reborn” view (time is real) underpins the learning picture. Instead of anthropic reasoning (“we observe these laws because we can exist”), an autodidactic universe might “optimize” laws over time for stability/variety.

- AI tie-in: If learning is fundamental, AI may be less “artificial” than we think. The post muses that insights from cosmology/quantum gravity could inspire future AI architectures and scaling laws.

- Author notes: The writer isn’t a physicist and points readers to the original paper/NotebookLM. References include The Autodidactic Universe (2021) and Smolin’s Time Reborn (2013).

**Spacetime as a Neural Network**
The discussion veers into speculative physics and the intersection of AI limitations and cosmology:

*   **Computational Astrophysics:** One user outlines a personal hobbyist framework interpreting physical constants as simulation constraints. They propose that the speed of light represents the speed of causality (a "computational constraint"), while gravity functions as "computational lag" where interactions take longer to process. They further speculate that Dark Matter may represent hidden emergent complexity akin to Modified Newtonian Dynamics (MOND).
*   **Motivation & AI Plateaus:** The submission author explains that falling down a rabbit hole after reading Lee Smolin’s *Time Reborn* led them to the "Autodidactic Universe" paper. They suggest this research is uniquely relevant now: as the current AI hype cycle hits performance plateaus, looking toward physics and "world models" may provide the necessary insights for the next stage of advancement.
*   **Resources:** Users discussed a related Reddit thread containing additional reading materials on the topic.

### LLMs Are Not Fun

#### [Submission URL](https://orib.dev/nofun.html) | 206 points | by [todsacerdoti](https://news.ycombinator.com/user?id=todsacerdoti) | [181 comments](https://news.ycombinator.com/item?id=46424136)

LLMs Are Not Fun (orib.dev)
A veteran engineer-turned-consultant argues that while he can use LLMs effectively for clients, they drain the very joy that drew him to programming and management. Coding with an LLM feels like hiring “a taskrabbit to solve my jigsaw puzzle,” stripping away the deep understanding that makes building software satisfying. Framed as teammates, LLMs are worse: there’s no growth to nurture, only micromanagement to preempt “slop and derailment.” He calls them a “dark parody” of both ultimate tool and extra teammate—useful, but at the cost of craft and joy.

The discussion around the article revealed a sharp divide regarding the definition of "programming" and the role of automation in creative work.

**The "Laundry vs. Art" Paradox**
A recurring sentiment was encapsulated by the popular meme quoted in the thread: "I want AI to do my laundry and dishes so I can do my art and writing, not for AI to do my art and writing so I can do my laundry and dishes." Users expressed frustration that instead of automating chores, AI is automating the creative and intellectual tasks (coding, writing), leaving humans to manage the "slop" or act as quality assurance. However, others countered that companies are driven by leverage and profit, not by a desire to facilitate employee self-actualization.

**Typing vs. Architecture**
Critics of the article argued that the author was fetishizing the mechanical act of typing ("rubbing balls of craft").
*   **The "Director" Stance:** Several commenters argued that LLMs allow developers to act like film directors—delegating lighting and camera work to focus on the high-level narrative and architecture. One user claimed tools like Claude Code allow them to focus entirely on design rather than looking up API documentation or syntax.
*   **The "Craft" Stance:** Conversely, others validated the author's view, comparing manual coding to painting on canvas versus an iPad, or cooking a meal versus ordering out. They argued that the "tedium" creates the mental space necessary for creativity and deep understanding.

**Productivity vs. Joy**
The thread also touched on the economic reality of the industry.
*   **The Luddite Argument:** One commenter noted that if LLMs provide a productivity boost (even if contested), employers will eventually mandate their use. Rejecting them for the sake of "craft" may become a privilege of the wealthy or the unemployed.
*   **The "Clusterf*ck" Reality:** A segment of the discussion challenged the premise that LLMs actually save time. They argued that for complex, existing projects ("brownfield"), LLMs often introduce subtle bugs ("hallucinations") that require more time to debug than writing the code from scratch would have taken—turning the programmer into a janitor rather than a creator.

---

## AI Submissions for Sun Dec 28 2025 {{ 'date': '2025-12-28T17:09:50.466Z' }}

### Designing Predictable LLM-Verifier Systems for Formal Method Guarantee

#### [Submission URL](https://arxiv.org/abs/2512.02080) | 58 points | by [PaulHoule](https://news.ycombinator.com/user?id=PaulHoule) | [11 comments](https://news.ycombinator.com/item?id=46411539)

The 4/δ Bound: a provable stopwatch for LLM + formal verifier loops

- What’s new: The authors model an LLM-assisted verification pipeline as a sequential absorbing Markov chain with four stages—CodeGen → Compilation → InvariantSynth → SMTSolving—and prove two big claims:
  1) If every stage has a non-zero chance of success (δ > 0), the system reaches Verified almost surely (no infinite loops).
  2) The expected number of iterations to verification is tightly bounded by 4/δ.

- Why it matters: Today’s LLM-verifier “refine until it works” loops can oscillate or stall, making resource planning guessy. This paper replaces heuristics with a formal convergence theorem and a latency bound you can budget against—useful for safety-critical software, CI stability, and cost forecasting.

- How it works:
  - Models the pipeline as a sequential absorbing Markov chain with Verified as the absorbing state.
  - Uses the minimum per-stage success probability δ to derive termination and the latency bound E[n] ≤ 4/δ.
  - Because the pipeline is strictly sequential, the constant “4” comes from the four stages; the framework generalizes conceptually with more stages.

- Evidence: ~90,000 trials stress-test the theory. Every run verified, and the empirical convergence factor clustered around 1.0, suggesting the 4/δ bound tracks reality closely rather than serving as a loose upper bound.

- Engineering takeaways:
  - Predictable budgeting: Given an estimate of δ, you can set timeouts, GPU/CPU quotas, and CI limits. Example: if δ ≈ 5%, expect ≤ 80 iterations on average.
  - Focus the fix: The weakest stage (lowest success probability) dominates δ; improving that stage yields the biggest win in end-to-end latency.
  - Operations modes: They identify marginal, practical, and high-performance zones and propose dynamic calibration to adapt δ as conditions drift (model updates, dataset shifts, toolchain changes).

- Open questions to watch:
  - Estimating δ robustly online and per-domain.
  - Handling non-sequential or branching pipelines, retries with memory, or correlated failures.
  - How the bound behaves with adversarial specs, highly sparse invariants, or flaky SMT/toolchains.

Authors: Pierre Dantas, Lucas Cordeiro, Youcheng Sun, Waldir Junior. Subjects: AI, formal methods, ML, and software engineering.

**Discussion Summary:**

Technical discussion focused on the paper’s methodology and the practical implications of its mathematical claims.

*   **Critique of Methodology:** Some users suspected the paper did not conduct experiments with actual LLMs writing code. Instead, commenters argued the experiments likely consisted of simulating the simplified Markov chain model itself. This led to criticism that the paper merely validates standard probability theory rather than real-world LLM behavior, with one user calling the mathematical content "thin" and the title "LLM-Verifier Convergence Theorem" grandiose for what amounts to a standard 5-state Markov chain proof.
*   **"Almost Surely" vs. Reality:** A significant portion of the thread debated the definition and utility of the term "almost surely" (probability = 1). While the paper claims the system reaches a verified state "almost surely," users pointed out that this guarantees success over infinite time, which is distinct from failing in a finite context.
*   **Practical Constraints:** Commenters noted that "eventually" reaching a halting state is less useful if the convergence takes an impractical amount of time (e.g., a century), drawing comparisons to P vs NP limitations. Others noted that assuming a strictly non-zero probability for success ($P > 0$) in the real world is a heavy assumption, and even statistically rare events (like hash collisions) remain relevant engineering concerns.

### CEOs are hugely expensive. Why not automate them? (2021)

#### [Submission URL](https://www.newstatesman.com/business/companies/2023/05/ceos-salaries-expensive-automate-robots) | 230 points | by [nis0s](https://news.ycombinator.com/user?id=nis0s) | [276 comments](https://news.ycombinator.com/item?id=46415488)

Executive pay is back in the spotlight — and so is the question: do firms even need a CEO?

- With AGM season underway, boards at BAE Systems, AstraZeneca, Glencore, Flutter, and the LSE face potential shareholder revolts over pay. The timing stings: many firms were propped up by government stimulus during Covid, yet exec rewards kept flowing.
- Example: 40% of Foxtons shareholders voted against a near-£1m bonus for the CEO while the company took ~£7m in state support. Meanwhile, Ocado’s Tim Steiner made £58.7m in 2019 — 2,605× his median employee — and the average FTSE 100 CEO clears £15k per day.
- The High Pay Centre argues companies could protect jobs by trimming compensation among the highest earners, not just the CEO.
- A viral thread from tech CEO Christine Carrillo (crediting her Philippines-based EA with handling “most” of her CEO tasks and saving 60% of her time) raises a harsher question: if much of a CEO’s work can be outsourced, could it be automated?
- Past AI misfires (Microsoft’s automated news curation, Amazon’s biased recruiting tool, a GPT-3 medical chatbot’s unsafe response) show risks in low-oversight tasks. But proponents argue top-level strategy might be a better candidate: decisions are debated, human bias is costly, and software could enforce more rational trade-offs.
- The provocation for boards and investors: if automation is inevitable, should it start at the top? Or is leadership’s value precisely the un-automatable judgment we’re paying for?

**Discussion Summary:**

While the article questioned the necessity of high-paid CEOs broadly, the discussion immediately zeroed in on **Elon Musk** as a polarizing case study for executive value, sparking debates on leadership theory, compensation, and the definition of the role.

*   **The "Tweet vs. Work" Paradox:** Users debated whether Musk's ability to serve as CEO for multiple companies (Tesla, SpaceX, xAI, Twitter) while tweeting incessantly proves the CEO role is less rigorous than claimed. Skeptics argued that if strategic decisions can be automated or widely delegated, exorbitant pay packages are unjustified.
*   **Vision vs. Operations (COO efficacy):** A central thread argued that Musk's companies succeed because of strong COOs (specifically citing Gwynne Shotwell at SpaceX) running day-to-day operations. This led to a semantic debate:
    *   Some argued the CEO’s true value is setting a "grand vision" and maximizing valuation (stock price) rather than operations.
    *   Others countered that "vision" is the Board's responsibility, and a Chief *Executive* Officer's job should be executing that mandate, not just being a figurehead.
*   **The "Great Man" Theory vs. Technological Inevitability:** A lengthy tangent explored whether leaders like Musk and Steve Jobs actually force innovation or simply capitalize on technology that is ready to emerge (e.g., reusing detailed arguments about NASA’s prior work on VTOL/reusable rockets vs. SpaceX’s engineering). One user described Musk’s utility as "bad-person strategy," where over-promising (or lying about) timelines forces organizational breakthroughs that nice leaders wouldn't achieve.
*   **Talent Magnetism:** Participants disagreed on whether top engineering talent joins Musk's companies *because* of him or *in spite* of him. While some claimed his personal brand attracts money and talent, others argued that engineers are drawn to the specific industry challenges (rockets, EVs) and lack comparable prestigious options, tolerating the leadership rather than seeking it.

### 'PromptQuest' is the worst game of 2025 (trying to make chatbots work)

#### [Submission URL](https://www.theregister.com/2025/12/26/ai_is_like_adventure_games/) | 40 points | by [dijksterhuis](https://news.ycombinator.com/user?id=dijksterhuis) | [26 comments](https://news.ycombinator.com/item?id=46411040)

- The Register’s Simon Sharwood riffs on Microsoft open-sourcing Zork to argue that using modern chatbots—especially Copilot—often feels like playing a “guess-the-verb” text adventure: you keep trying “Hit/Kill/Stab Goblin” until syntax lucks out.
- His gripe isn’t just mistakes, but inconsistency and opacity: identical prompts yield different outputs across days, Copilot in Office vs desktop behaves differently, and silent model swaps break previously reliable prompts.
- Anecdote: asking Copilot to fetch online data and deliver a downloadable spreadsheet yielded a Python script, repeated false assurances that the job was done, and even a generated “progress bar”—but never an actual file.
- Bottom line: this “PromptQuest” turns work into cave-crawling in the dark while being sold as productivity. The implicit ask: stable versions, visible model changes, deterministic modes, and reliable artifact delivery.

The discussion around *The Register’s* critique of "PromptQuest" is polarized, splitting between those who view the author's struggles as user error (a "skill issue") and those who agree that non-deterministic tools are fundamentally flawed for reliable work.

**User Error vs. Tool limitations**
*   **"Old Man Yelling at Cloud":** Several commenters dismissed the article as typical *Register* cynicism or a "skill issue," arguing that the author is treating a raw text engine like a finished product. They suggest that 90% of LLM complaints stem from users refusing to learn how to properly guide the model.
*   **The Problem with Indeterminism:** Supporters of the article countered that the criticism is valid because LLMs are not just difficult, but inconsistent. One user noted the "flavors of non-determinism"—from varying outputs based on the input channel (API vs. Chat) to statistical hallucinations—creating a "cursed middle" where answers are plausible but hard to verify.
*   **The "Zork" Analogy:** Users expanded on the text adventure comparison. Unlike tools like Photoshop or VS Code which present finite menus of valid options, LLMs lack a UI for capabilities, forcing users into "open-ended exploration" where they must guess what the software can actually do.

**Management and Workflow**
*   **Forced Productivity:** A specific frustration emerged regarding management forcing AI adoption. Commenters noted that "LLM managers" are now blaming humans for failing to meet "alleged productivity results" calculated by the very AI that isn't working properly.
*   **Gemini vs. Copilot:** While the article complained that Copilot *wouldn't* generate files, a commenter noted the opposite problem with Google Gemini, which annoyingly insists on inserting itself into Google Workspace spreadsheets even when explicitly told not to.

**Cultural Observations**
*   **"Vibe Coding":** There was marked skepticism regarding the current hype cycle ("vibe coding"), with users calling it a delusion of VCs who hope to replace engineers with service-endpoint query tools.
*   **The "Yes Man":** One commenter drew a parallel between ChatGPT’s refusal to acknowledge failure and the generic, cheery demeanor of the "Yes Man" robot from *Fallout: New Vegas*—a helper that sounds polite while being functionally useless or dangerous.

---

## AI Submissions for Sat Dec 27 2025 {{ 'date': '2025-12-27T17:09:03.550Z' }}

### Apple releases open-source model that instantly turns 2D photos into 3D views

#### [Submission URL](https://github.com/apple/ml-sharp) | 385 points | by [SG-](https://news.ycombinator.com/user?id=SG-) | [197 comments](https://news.ycombinator.com/item?id=46401539)

Apple open-sources SHARP: single-image to 3D Gaussian scenes in under a second

- What it is: SHARP takes a single photo and directly regresses a 3D Gaussian Splatting (3DGS) scene that can be rendered in real time for nearby viewpoints. The representation is metric (with absolute scale), enabling physically meaningful camera motions.

- Why it matters: Prior photorealistic view synthesis typically needed multi-view capture and/or lengthy optimization (e.g., NeRF-style training). SHARP claims state-of-the-art quality with massive speedups: LPIPS down 25–34% and DISTS down 21–43% vs. prior best, while cutting synthesis time by roughly three orders of magnitude. The authors also report robust zero-shot generalization across datasets.

- How it works for users:
  - One-pass inference on a standard GPU in <1s; outputs 3DGS .ply files compatible with common Gaussian renderers.
  - CLI: sharp predict -i /path/to/images -o /path/to/gaussians
  - Model downloads automatically to ~/.cache/torch/hub/checkpoints/ or via direct link.
  - Prediction runs on CPU, CUDA, and Apple MPS; video rendering along trajectories (--render) currently requires a CUDA GPU (first run initializes gsplat and can be slow).
  - Coordinates follow OpenCV (x right, y down, z forward). You may need to re-center/scale when using third‑party renderers.

- Practical notes:
  - Python env suggested (e.g., conda); requirements provided.
  - Code and model have separate licenses; check LICENSE and LICENSE_MODEL.

Links:
- Repo: https://github.com/apple/ml-sharp
- Project page: https://apple.github.io/ml-sharp/
- Paper: https://arxiv.org/abs/2512.10685

Potential impact: Single-image, real-time 3D scene synthesis could accelerate AR/VR previews, robotics perception, and creative tools, while further cementing 3D Gaussian splats as a fast, deployable alternative to optimization-heavy NeRF pipelines.

Based on the discussion, here is the summary:

**Licensing and Definition of "Open Source"**
The primary point of contention in the thread is the use of the term "Open Source" in the submission title. Users point out that while the source code is under the permissive MIT license, the model weights themselves carry a restrictive "Research Use Only" (or non-commercial) license.
*   Commenters argue this continues a trend—popularized by companies like Meta—of "open washing," where "Open Source" is conflated with simply releasing weights, diluting the term's connection to Free and Open Source Software (FOSS) definitions.
*   Critiques suggest this hybrid licensing prevents community adoption and commercial integration.

**Legal Theory: Copyright and Liability**
A significant debate emerged regarding legality and strategy:
*   **Copyrightability:** Users debated whether model weights (tables of numbers) are legally copyrightable or if they are simply facts/mathematical outputs. Some argued they represent creative expression, while others insisted they are unprotectable data.
*   **Strategic Shielding:** Several users speculated that Apple’s restrictive licensing isn't necessarily about protecting the model, but rather a legal tactic to avoid liability. By restricting commercial use, Apple may be sidestepping scrutiny regarding the copyright status of the data used to *train* the model.
*   **Enforcement:** Regardless of actual legality, commenters noted that the threat of litigation from a entity as large as Apple effectively enforces the license, as few can afford to challenge it in court.

**Impact and Usability**
*   **Strategic Folly:** One user argued that by restricting the model, Apple is essentially encouraging competitors (specifically citing Chinese AI researchers) to replicate the functionality and release their own versions, ultimately causing Apple to lose control of the standard.
*   **Developer Experience:** There was a brief exchange regarding usability; while one user expressed frustration that AI repositories rarely contain working instructions, another reporter countered that the SHARP instructions worked fine for them.
*   **True Openness:** Ideally, users argued, a truly "Open Source" AI release must include the training data, not just the code and weights.

### More than 20% of videos shown to new YouTube users are 'AI slop', study finds

#### [Submission URL](https://www.theguardian.com/technology/2025/dec/27/more-than-20-of-videos-shown-to-new-youtube-users-are-ai-slop-study-finds) | 69 points | by [schu](https://news.ycombinator.com/user?id=schu) | [22 comments](https://news.ycombinator.com/item?id=46403805)

More than 20% of videos shown to new YouTube users are ‘AI slop’, study finds

- A Kapwing study of 15,000 top YouTube channels (top 100 per country) found 278 publish only “AI slop” — low-effort, AI-generated clips optimized for clicks. Collectively: 63B views, 221M subscribers, and an estimated $117M/year in revenue.
- In a fresh account test, 104 of the first 500 recommended videos (≈21%) were AI slop; one-third of the feed was “brainrot” (a broader bucket of low-quality, attention-farming content).
- Growth is global: Kapwing tallies AI-channel followings of 20M in Spain, 18M in Egypt, 14.5M in the US, and 13.5M in Brazil.
- Standout channels:
  - Bandar Apna Dost (India): 2.4B views; surreal action vignettes; estimated up to $4.25M revenue.
  - Pouty Frenchie (Singapore): 2B views; kid-targeted bulldog adventures; ~+$4M/year.
  - Cuentos Facinantes (US): 6.65M subscribers; children’s cartoon storylines.
  - The AI World (Pakistan): 1.3B views; AI-generated disaster shorts.
- Behind the scenes: informal playbooks spread on Telegram/WhatsApp/Discord; “niches” like AI videos of pressure cookers exploding; many creators come from middle‑income countries (e.g., India, Kenya, Nigeria, Brazil, Vietnam, Ukraine). Course-sellers often profit more than creators.
- Big picture: platforms function as vast A/B testing systems, rewarding whatever hooks engagement and can be scaled. The actual share of total YouTube views from AI content remains unclear due to limited platform disclosure.
- YouTube’s response: generative AI is just a tool; the company says it focuses on surfacing high-quality content regardless of how it’s made.

**Is YouTube a Library or a Trap?**
A debate emerged regarding the fundamental nature of the platform. While **scotty79** defended YouTube as an "amazing repository" of practical skills and knowledge on par with Wikipedia, **DoctorOW** and others argued the business model fundamentally breaks the utility. They noted that while Wikipedia is donor-funded to provide comprehensive information, YouTube is incentivized to sell influence and manipulate users via algorithms, making the comparison to traditional mass media unfavorable.

**The Impact on Children**
Much of the discussion focused on the "brainrot" mentioned in the article:
*   **The_President** described the platform not as a "mistake" but as engineered addiction, characterizing it as a "drug" for children that parents use as a pacifier.
*   **Glkk** detailed the bizarre nature of the "slop" targeting kids—listing surreal examples like Sonic or Lego characters in weird scenarios—and argued that the only safe approach is to curate and download videos in advance rather than trusting the feed.

**Creators vs. The Algorithm**
Commenters contrasted the treatment of human creators with AI content. **Trnm** noted that YouTube harshly penalizes real creators with demonetization and copyright strikes for minor infractions, yet allows AI slop to run rampant. **Blbbl** speculated this might be the "end game": replacing pesky, expensive human creators with AI entirely.

**Archival Concerns**
**Brndyn** argued that YouTube’s hostility toward downloaders is a "crime against humanity," preventing the public from archiving a massive cultural history (unlike Wikipedia), leaving valuable content from deceased creators or defunct channels at risk of deletion.