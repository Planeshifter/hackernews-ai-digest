import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Mon Jun 03 2024 {{ 'date': '2024-06-03T17:12:23.610Z' }}

### Hacking millions of modems and investigating who hacked my modem

#### [Submission URL](https://samcurry.net/hacking-millions-of-modems) | 112 points | by [albinowax_](https://news.ycombinator.com/user?id=albinowax_) | [168 comments](https://news.ycombinator.com/item?id=40560010)

Today's top story on Hacker News unravels a cybersecurity mystery involving a user who discovered that their modem had been hacked. The user noticed unusual activity in their network logs, revealing that someone was intercepting and replaying their HTTP traffic across all their devices. Further investigation led them to an unknown IP address linked to DigitalOcean, which had a history of hosting phishing websites targeting a cybersecurity company. The user disconnected the compromised device, a Cox Panoramic Wifi gateway, and attempted to hand it over to their ISP for further analysis. The story highlights the intricate world of cyber threats and the complexities of securing personal networks against sophisticated attackers.

The discussion on Hacker News regarding the cybersecurity mystery of a hacked modem involves a deep dive into the technical aspects of network security and potential solutions to mitigate the risks associated with compromised devices. Users share experiences and advice related to dealing with security threats, examining the response of ISPs such as Cox and discussing the implications of responsible disclosure programs. There are also debates on the ethics of vulnerability disclosure and the motivations behind security research, touching on issues such as financial incentives and professional integrity. Additionally, the conversation delves into broader societal concerns around cybersecurity, poverty, and the prioritization of resources. Overall, the discussion reflects a mix of technical expertise, ethical considerations, and social awareness in addressing cybersecurity challenges.

### Grokfast: Accelerated Grokking by Amplifying Slow Gradients

#### [Submission URL](https://arxiv.org/abs/2405.20233) | 106 points | by [johnsutor](https://news.ycombinator.com/user?id=johnsutor) | [32 comments](https://news.ycombinator.com/item?id=40567165)

The latest submission on Hacker News is a paper titled "Grokfast: Accelerated Grokking by Amplifying Slow Gradients" by Jaerin Lee and three other authors. The paper delves into the phenomenon of grokking in machine learning, where delayed generalization occurs after near-perfect overfitting to training data. The authors aim to accelerate the generalization process of models experiencing grokking by analyzing and amplifying the slow-varying components of gradients. Their algorithm claims to accelerate grokking by more than 50 times with just a few lines of code. The experiments showcased the effectiveness of the approach across various tasks involving images, languages, and graphs. For those interested, the code is available for practical use.

The discussion on the submission regarding the paper "Grokfast: Accelerated Grokking by Amplifying Slow Gradients" covers various aspects of grokking in machine learning and its practical implications. 

- **utensil4778**: Makes a comment about AI creating new vocabulary and not requiring disambiguation pages.
- **svr**: Recalls seeing grokking demonstrated in MNIST and synthetic datasets and expresses interest in its practical applications.
- **fwlr**: Discusses the distinct aspects of grokking and its significance in resource allocation.
- **Legend2440**: Points out that practical applications should not necessarily expect cutting-edge research.
- **whmsclsm**: Highlights the role of grokking in modern ML and the importance of regularization.
- **sfk**: Reflects on the beginnings of the grokking phenomena and its implications in learning and research.
- **curious_cat_163**: Mentions signal processing in relation to the discussion.
- **bldbt**: Talks about MNIST Graph CNN and scaling models with OpenWebText dataset.
- **Imnimo**: Discusses grokking in critical zones of training data and its implications.
- **gssh**: Comments on the competitive effort to reproduce results on complex datasets.
- **QuadmasterXLII**: Mentions optical phenomena in the context of the discussion.
- **HarHarVeryFunny**: Refers to a recent paper on a deep model related to grokking.

These comments show a diverse range of perspectives on grokking, its applications, implications, and challenges in the field of machine learning.

### Mamba-2 â€“ State Space Duality

#### [Submission URL](https://tridao.me/blog/2024/mamba2-part1-model/) | 143 points | by [bratao](https://news.ycombinator.com/user?id=bratao) | [28 comments](https://news.ycombinator.com/item?id=40564067)

The release of Mamba-2 brings a new structured state space model (SSM) variant, addressing key questions regarding the conceptual connections between state space models and attention, as well as the efficiency in training. The SSD model, part of Mamba-2, introduces structured state space duality, which includes a layer that can be integrated into neural networks efficiently. The SSD algorithm ensures more efficient computation of SSD layers compared to previous SSMs.

The SSD model, at its core, involves a structured state space model with a scalar times identity structure for improved performance. Additionally, multihead SSMs can handle multiple channels of data independently, enhancing the model's versatility. Stay tuned for more insights in the upcoming parts of the blog post series!

The discussion on the submission about Mamba-2 and its structured state space model (SSM) variant includes various insights and debates. Users discuss the efficiency and improvements in training of the SSD model introduced in Mamba-2, highlighting its benefits such as efficient computation of SSD layers compared to previous SSMs. 

There is a comparison made between Transformers and humans in recalling tasks, noting that humans underperform Transformers in certain recall tasks. Additionally, the discussion delves into the capabilities and limitations of different models in tasks like translation and memory recall. 

Some users talk about Quadratic Transformers outperforming other models in attention recall tasks, while others emphasize the importance of linear SSMs and the advancements made in Mamba-2. Debate arises over the practical benefits of different models like RNN and Mamba in terms of layers, transformations, and efficiency in training.

Furthermore, there are discussions on the scalability of attention mechanisms, the differences between Transformers and traditional models like LSTMs, and the training processes of Mamba-2. An explanation of Mamba-2's improvements over Mamba-1 from both a training and inference perspective is also given. The conversation includes technical insights, comparisons between models, and practical implications for various tasks in the field of natural language processing.

### The simdjson library

#### [Submission URL](https://simdjson.org/) | 57 points | by [fanf2](https://news.ycombinator.com/user?id=fanf2) | [23 comments](https://news.ycombinator.com/item?id=40560233)

A new library called simdjson is revolutionizing the way servers parse JSON, making the process faster and more efficient than ever before. This library utilizes SIMD instructions and microparallel algorithms to achieve remarkable speeds, outperforming other popular JSON parsers by a significant margin.

Some key highlights of the simdjson library include its incredible speed - over 4x faster than RapidJSON and 25x faster than JSON for Modern C++, its user-friendly APIs, strict JSON and UTF-8 validation, automatic CPU-tailored parser selection, and robust design focused on reliability and performance.

Widely used by industry giants like Microsoft, Google, and Intel, simdjson is also integrated into various technologies such as ClickHouse, Shopify, and Node.js runtime. Additionally, the library offers support for multiple languages and platforms, making it accessible and versatile for developers across different ecosystems.

With features like On Demand API for blazing speeds, minification capabilities, standalone UTF8 validation, multithreaded processing for massive JSON files, support for JSON Pointer, and runtime dispatch for optimized performance, simdjson stands out as a game-changer in the world of JSON parsing. Whether you're working on a Python, Rust, Go, or C# project, simdjson has got you covered with its wide range of ports and bindings.

The discussion around the simdjson library on Hacker News delves into various aspects of JSON parsing, binary formats, and hardware optimizations. Some users express their opinions on using binary formats like Protocol Buffers for improved performance, while others highlight the importance of human-readable properties in JSON compared to binary formats. The conversation touches on topics like file formats such as Amiga's IFF and different perspectives on binary format vs. human-readable format discussions. Additionally, comments discuss the use of SIMD instructions like AVX-256 for faster JSON parsing and how hardware acceleration can optimize parsing performance. There is also a conversation about ARM instructions like FJCVTZS and their application in JavaScript for floating-point conversions.

### Scientists should use AI as a tool, not an oracle

#### [Submission URL](https://www.aisnakeoil.com/p/scientists-should-use-ai-as-a-tool) | 113 points | by [randomwalker](https://news.ycombinator.com/user?id=randomwalker) | [103 comments](https://news.ycombinator.com/item?id=40568026)

The latest article on Hacker News delves into the importance of viewing AI as a tool rather than an oracle in scientific research. The piece discusses how the hype surrounding AI can lead to flawed research, perpetuating a cycle of misinformation and misguided expectations. The authors emphasize the significance of maintaining a skeptical mindset in the face of AI's capabilities, highlighting the need for thorough validation and reproducibility in ML-based science. By acknowledging the potential pitfalls of overreliance on AI and promoting a culture of critical inquiry, researchers can work towards improving the quality and integrity of scientific discoveries in the field.

1. **nklnkl**: The commenter expresses concern about the susceptibility of scientific fields to fall for AI hype, resulting in flawed research and inaccurate predictions. They mention the danger of self-fulfilling prophecies and the importance of distinguishing between experts and machines in evaluating predictions.
2. **gdlsk**: Responding to nklnkl's comment, gdlsk points out the issue with blindly trusting machines, citing the example of AI safety proponents critiquing the credentials of AI Safety proponents.
3. **starship006**: starship006 engages in a discussion about the flaws in the argument structure broadly criticizing the credentials of AI safety proponents.
4. **lcksr**: lcksr contributes by cautioning against trusting machines and emphasizes the importance of human judgment over blindly relying on AI technologies, criticizing the arguments based on Harry Potter fandom.
5. **SrslyJosh**: SrslyJosh mentions the necessity for AI safety proponents to understand the technology objectively, warning against creating potential dangers inadvertently.
6. **mmbs**: mmbs introduces the idea of corporations being responsible for creating significant threats to humanity, mentioning Skynet, capitalism, and the Internet of Things.
7. **thrwnm**: Discussing the potential misuse of GPT and the implications of people following devices without critical thinking, thrwnm emphasizes the importance of distinguishing between genuine information and manipulated content.
8. **grdsj**: grdsj critiques the accuracy and specificity of the submitted article, calling it "Garbage" and pointing out various errors.
9. **cptnkrtk**: cptnkrtk discusses the challenges in professional work related to language models like GPT and cautions against blindly trusting AI-generated content due to potential mistakes and biases.
10. **tmbrt & ntnvs**: These two users engage in a detailed discussion about the complexities of AI models, the implications of wrong responses, and the challenges of ensuring correct outputs based on the input provided.

Overall, the discussion highlights the importance of maintaining a critical mindset towards AI technologies, questioning their reliability, and considering the ethics and consequences of their applications in various fields.

---

## AI Submissions for Sun Jun 02 2024 {{ 'date': '2024-06-02T17:10:23.884Z' }}

### What We Learned from a Year of Building with LLMs (Part II)

#### [Submission URL](https://www.oreilly.com/radar/what-we-learned-from-a-year-of-building-with-llms-part-ii/) | 23 points | by [tim_sw](https://news.ycombinator.com/user?id=tim_sw) | [4 comments](https://news.ycombinator.com/item?id=40558044)

In the second part of the series on building with Large Language Models (LLMs), the authors delve into the operational aspects of creating LLM applications, bridging the gap between strategy and tactics. They address key questions related to data, models, product, and people.

1. **Data**: Quality input data is crucial for LLM performance. Monitoring LLM inputs and outputs regularly is essential to understand data distribution, detect skew, and ensure alignment between development and production data. Skew can be structural (formatting discrepancies) or content-based (differences in meaning or context). Strategies to mitigate skew include tracking metrics like input/output length, conducting qualitative assessments of outputs ("vibe checks"), and incorporating non-determinism into skew checks.

2. **Models**: Integrating LLMs into the tech stack and managing versioning are key considerations. Thinking about how to version models, migrate between versions, and maintain compatibility is crucial. Balancing conflicting requirements, involving design early in the development process, and prioritizing user experiences are vital aspects of product development.

3. **Product**: Designing user experiences with human-in-the-loop feedback, calibrating product risk, and prioritizing requirements are essential for successful LLM applications. Cultivating a culture of experimentation, hiring the right talent, and leveraging emerging LLM applications to build your own are important for team success.

4. **People**: Hiring the right team members, fostering a culture of experimentation, and navigating the balance between process and tooling are key considerations for building successful LLM applications.

By focusing on these operational aspects, organizations can effectively develop and manage LLM applications and the teams that build them, ensuring the optimal performance and impact of their machine learning systems.

The discussion on the submission includes a comment expressing skepticism about the quality of the AI-generated introduction, suggesting that AI language models lack the ability to provide a genuinely engaging and tailored introduction. Other users agreed, with one pointing out that building large language models (LLMs) involves more than just writing text.

### AI Is a False God

#### [Submission URL](https://thewalrus.ca/ai-hype/) | 50 points | by [pseudolus](https://news.ycombinator.com/user?id=pseudolus) | [25 comments](https://news.ycombinator.com/item?id=40553571)

In the latest issue of The Walrus, the author Navneet Alang dives into the world of artificial intelligence in a thought-provoking piece titled "AI Is a False God." The article explores the hype and concerns surrounding AI, drawing parallels to historical technological advancements like the World Wide Web.

Alang discusses how AI technology, particularly large language models (LLMs) such as ChatGPT, has garnered immense interest and investment from tech giants like Microsoft, Meta, and Alphabet. While some see AI as a revolutionary force that will usher in a new era of innovation, others, like philosopher Nick Bostrom, warn of the existential risks associated with super-intelligent machines.

The narrative touches on the potential societal impacts of AI, from concerns about job displacement to fears of losing control over AI systems. By reflecting on past technological advancements and their unintended consequences, Alang urges readers to approach AI development with caution and critical scrutiny.

Through engaging storytelling and compelling artwork by designer Marian Bantjes, "AI Is a False God" prompts readers to ponder the implications of our growing reliance on artificial intelligence and the need for thoughtful regulation and ethical considerations in this rapidly evolving field.

The discussion on Hacker News regarding the article "AI Is a False God" in The Walrus covers a wide range of viewpoints and criticisms. Here are some key points from the conversation:

- Some users feel that the article is not worth reading, describing it as shallow and dismissive, with obscure philosophical references that make it challenging to grasp the central arguments. They suggest that the article fails to delve deeply into the subject matter and is overly hyped.
- Another user, with a background in philosophy, appreciates the mention of Derrida and Deleuze in the article, highlighting the connection to the high-dimensional spaces that large language models (LLMs) like transformers operate in. They point out that the references to these philosophers help support the argument about the transformative power of LLMs.
- There is a debate about the value of AI and whether it is overhyped. Some users express skepticism about the productivity gains from AI tools like CoPilot, while others argue that AI advancements have the potential to significantly impact various industries and improve productivity.
- One user questions the perspective presented in the article, suggesting that the comparison of AI to a false god may overlook the potential benefits and advances that AI technology can bring. They encourage a broader examination of the risks and rewards associated with AI development.

Overall, the discussion showcases a mix of skepticism, philosophical analysis, and differing opinions on the implications of AI technology and its portrayal in the article.

---

## AI Submissions for Sat Jun 01 2024 {{ 'date': '2024-06-01T17:14:11.197Z' }}

### Lisp: Icing or Cake?

#### [Submission URL](https://dthompson.us/posts/lisp-icing-or-cake.html) | 190 points | by [psj](https://news.ycombinator.com/user?id=psj) | [36 comments](https://news.ycombinator.com/item?id=40549250)

In the latest update from Hacker News, David Thompson discusses the insights he gained from the recent Spring Lisp Game Jam 2024, where a record-breaking 48 games were submitted. The breakdown of submissions by language reveals Guile as the most popular choice, with more games leveraging the Scheme-to-WebAssembly compiler 'Hoot'. Fennel, a Lisp that compiles to Lua, was another favored language for game development. Thompson identifies two predominant approaches in building games with Lisp: the 'Lisp as icing' and 'Lisp as cake' paradigms. The former involves using Lisp as a scripting language atop a base code in static languages like C or Rust, while the latter integrates Lisp throughout the entire software stack, minimizing external dependencies. The discussion touches on specific implementations like Guile for extending C programs and Common Lisp for full-stack Lisp development.

Case studies from the game jam illustrate these patterns, with Fennel paired with love2d exemplifying 'Lisp as icing' - where Fennel acts as a scripting layer on top of the C++ game engine. On the other hand, the S7 and raylib stack represents a 'Lisp as cake' approach, where Lisp plays a significant role in the project architecture.

Thompson's analysis provides valuable insights into the diverse ways Lisp is being utilized in game development, showcasing the flexibility and versatility of this language in creating engaging and innovative projects.

The discussion revolves around the recent insights shared by David Thompson on the Spring Lisp Game Jam 2024 and the use of Lisp in game development. Here are some highlights from the comments:

- **nctdncn**: Discusses the experience of using Scheme in Max MSP and the comparison between Guile, Clojure, and Common Lisp.
- **rnlszlrn**: Expresses surprise at the number of games using Janet and mentions encountering gaming surprises.
- **hzn**: Encourages readers to check out the special thread on Lisp.
- **rcrm**: Mentions using Guile and discusses WASM development.
- **rtctn**: Shares a 3D bass fight prototype using Clojure.
- **bmch**: Mentions a joint project and critiquing a vintage article.
- **swatson741**: Shares an interest in creating games using Lisp.
- **skssn**: References a game called Dunnet and its history.
- **nthk**: Discusses experiences with the ZMachine interpreter for interactive fiction.
- **ngcc_hk**: Engages in a conversation about the evolution of Lisp and CPUs.

Overall, the discussion delves into various aspects of game development using Lisp and shares insights and experiences related to utilizing different Lisp dialects for programming projects.

### How to Distribute Postgres Geographically

#### [Submission URL](https://xata.io/blog/geo-distributed-postgres) | 94 points | by [mebcitto](https://news.ycombinator.com/user?id=mebcitto) | [11 comments](https://news.ycombinator.com/item?id=40542940)

In the blog post titled "Geographically distributed Postgres for multi-tenant applications" by Tudor Golubenco, a pattern for global distribution of Postgres databases for multi-tenant applications is detailed. The approach involves separating per-tenant data tables from control plane tables, placing tenant data in the region closest to users, and creating a global view using Postgres Foreign Data Wrappers (FDW) and partitioning.

The pattern is useful for applications with a global customer base due to reasons such as reducing latencies, complying with data residency laws, running apps on the edge, and enabling multi-region or multi-cloud setups. It works when data can be segmented by a key (e.g., customer id) into tenants and when each tenant has a natural affinity to a particular region.

The control plane data, including authentication and user tables, is kept in a centralized region, while tenant data is distributed based on affinity regions. This ensures that foreign keys point inside the tenant, allowing access to all tenant data by connecting to a single region. Authentication is done centrally, with subsequent requests directed to the region storing the relevant tenant data.

The blog post uses a Notion clone as an example to illustrate how pages data could be distributed based on tenant and region. By following this pattern, multi-tenant SaaS applications can efficiently manage data across multiple regions while maintaining centralized control and authentication.

The discussion on the blog post about geographically distributed Postgres for multi-tenant applications covered various aspects related to implementing and managing such systems.

1. **jtl and tdrg** discussed experimenting with sharded tenant-clustered tables using Postgres Foreign Data Wrappers (FDW) and the challenges of routing queries efficiently in a multi-region setup. They also mentioned the complexity and risks involved in altering the schema across multiple Postgres instances.

2. **Ozzie_osman** shared familiarity with AWS in the context of preparing sharded Postgres databases and questioned the approach detailed in the blog post.

3. **sgrlnd** raised concerns about the performance limitations of Aurora Serverless and the high costs associated with serverless approaches.

4. **nrpl** discussed the complexity of managing sharded Postgres databases, emphasizing the challenges in handling schema changes, balancing data, and preventing production outages. They highlighted the benefits of using smaller databases for better performance and operational efficiency.

5. **hzskll** provided a critical perspective on the manageability of shared data in Postgres using FDW, emphasizing the importance of consistent backups and mechanisms to handle schema changes across multiple regions. They also questioned the necessity of a distributed system and the transparency of its design for reliability.

6. **tdrg and hzskll** further discussed the challenges and considerations of designing systems for distributed data sharing, emphasizing the security, scalability, and performance implications of such architectures.

Overall, the discussion delved into the technical complexities, performance considerations, and operational challenges associated with implementing geographically distributed Postgres databases for multi-tenant applications.

### The quest to craft the perfect artificial eye through the ages

#### [Submission URL](https://www.popsci.com/health/artificial-eye-history/) | 32 points | by [chat](https://news.ycombinator.com/user?id=chat) | [3 comments](https://news.ycombinator.com/item?id=40548399)

The art of prosthetic eyes has a rich history dating back nearly 5,000 years, involving materials from tar to gold wires to PMMA. Glassblowing techniques from 16th-century Venetian artisans laid the foundation, with a shift to acrylic like PMMA during World War II due to a lack of German glass exports. Ocularists, the skilled artisans who craft these intricate eyes, pass down their expertise through generations, with a typical apprenticeship taking around five years.

The quest for realistic prosthetic eyes mirrors the importance of eye contact in human communication. Recent studies have shown that eye contact triggers important brain activities and plays a role in romantic attraction and social interactions. Motility, or the ability of the prosthetic eye to move in sync with the natural eye, is crucial for a natural appearance.

The evolution of prosthetic eyes took a significant step forward in the late 19th century with the introduction of orbital implants, allowing for more natural movement and function. Today, advancements in technology, such as 3D printing, continue to improve prosthetic eyes, enhancing their appearance, comfort, and functionality.

The discussion revolves around personal experiences with prosthetic eyes and advancements in the field. The original commenter, "Daub," shares their story of experiencing an odd feeling and adjustments after a workshop accident that resulted in their prosthetic eye. They mention how the artificial eye behaved differently from the natural eye, with slightly slower movements and limited vision. Despite the artificial eye being perfectly matched to the natural one, they still faced challenges in various situations, such as navigating traffic.

Another user, "acheong08," mentions hearing about scientific progress in connecting a camera directly to the brain. "Daub" then brings up the topic of artificial vision in terms of success for blind people but notes that it may still be a challenge for some individuals.

Overall, the discussion provides personal anecdotes and touches on the technological advancements in artificial vision systems.

### Re-Evaluating GPT-4's Bar Exam Performance

#### [Submission URL](https://link.springer.com/article/10.1007/s10506-024-09396-9) | 27 points | by [rogerkeays](https://news.ycombinator.com/user?id=rogerkeays) | [7 comments](https://news.ycombinator.com/item?id=40543535)

A recent paper has raised doubts about the reported 90th percentile performance of GPT-4 on the Uniform Bar Exam. The study highlights methodological challenges and presents findings that suggest OpenAI's estimates of GPT-4's performance may be overinflated. Data from different exam administrations indicate varying percentiles for GPT-4, with estimates ranging from below the 69th percentile to as low as the 15th percentile on essays for licensed or license-pending attorneys. The paper also questions the validity of GPT-4's reported essay score and explores the impact of hyperparameter combinations on its performance. These findings raise important considerations for the use of AI in legal tasks and underscore the need for rigorous evaluations to ensure the reliability of AI models like GPT-4.

The discussion on the submission revolves around skepticism regarding the reported performance of GPT-4 on the Uniform Bar Exam. Some users argue that the tests designed to discriminate between humans and AI may not accurately measure the abilities needed for legal practice, with one user emphasizing the importance of subjective interpretation in legal analysis. Another user points out discrepancies in GPT-4's reported 92nd percentile score, with findings indicating scores as low as the 15th percentile in specific sections. There is also a mention of the competitive nature of legal practice and the significance of passing the bar exam. Additionally, users question the credibility of GPT-4's essay scores and its ability to perform legal tasks effectively. One user expresses surprise at the lack of replication of the research and doubts the validity of the reported 90th percentile performance of GPT-4.

### Instagram will use users' content to train it's AI

#### [Submission URL](https://in.mashable.com/tech/76239/meta-is-using-your-posts-to-train-ai-its-not-easy-to-opt-out) | 8 points | by [nehagup](https://news.ycombinator.com/user?id=nehagup) | [5 comments](https://news.ycombinator.com/item?id=40546888)

Meta, the tech giant behind Facebook and Instagram, is using your posts to train its AI models, but opting out is not as easy as it seems. The company's data sharing practices have drawn attention, especially in Europe where users were recently notified about changes to privacy policies related to new generative AI features. This move has sparked concerns about privacy and data usage.

Meta's generative AI privacy policy reveals that it uses information shared on its platforms, such as posts and photos, to train its AI models. While private messages are not used for training, public content is fair game. Users in the UK and EU have the option to object to this data sharing, but the process is cumbersome and intentionally challenging to navigate, as noted by some users.

To limit what you share with Meta's AI models, one option is to delete your accounts, although that's a drastic measure. Users can also submit requests to access, delete, or file complaints about personal information used for AI development. However, the process has limitations and may not guarantee full opt-out.

Another avenue to safeguard your data is through the "activity off Meta" settings, where you can manage sites and apps sharing information with Meta and control future data sharing. Disconnecting and managing future activity can help prevent further data sharing with third parties.

Despite these measures, Meta's privacy settings primarily address sharing with third parties, leaving questions about internal data usage for training AI models. While the options may not offer a foolproof solution, they are steps towards regaining some privacy control. Clarifications from Meta on data usage internally are awaited to provide a clearer understanding of the situation.

The discussion around Meta using posts to train its AI models touches on various points. One user pointed out the challenge of opting out of this data sharing practice with Meta's AI, while another mentioned the difficulty in working around internet safety caveats when attempting to limit data sharing while still allowing AI training. The conversation also delved into the legal implications of scraping data and the relevance of Zuckerberg's comments today in light of the consequences it may bring. Additionally, there was a brief mention of corporate intellectual property, indicating the complexity of the situation regarding data usage and privacy with Meta's AI models.