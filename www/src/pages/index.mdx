import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Fri Mar 01 2024 {{ 'date': '2024-03-01T17:14:19.319Z' }}

### Where is Noether's principle in machine learning?

#### [Submission URL](https://cgad.ski/blog/where-is-noethers-principle-in-machine-learning.html) | 275 points | by [cgadski](https://news.ycombinator.com/user?id=cgadski) | [68 comments](https://news.ycombinator.com/item?id=39560862)

The post discusses the application of Noether's Principle to Machine Learning and its comparison with its usage in Physics. Noether's Principle in Physics relates continuous invariances of the action to conservation laws of the system. For instance, in the two-body problem, certain transformations are invariants for the action, leading to conserved quantities like momentum. In machine learning, processes involve choosing control parameters to minimize a quantity, such as in a deep residual network. The trajectory of values in machine learning can be viewed as a discrete-time process analogous to physical trajectories, albeit with differences in time and constraints. The post raises questions about applying Noether's theorem to these processes and finding meaningful conserved quantities, highlighting similarities and distinctions between physics and machine learning.

The discussion on Hacker News around the submission discussing the application of Noether's Principle to Machine Learning and comparing it with its usage in Physics covered various viewpoints. Here are some key points:
1. **Connection Between Physics and Machine Learning**: Users discussed the similarities between Noether's Theorem in Physics and its potential application in machine learning. They highlighted parallels between the conservation laws in physics and the training processes in machine learning, raising questions about finding meaningful conserved quantities in these processes.
2. **Noether's Theorem in Neural Networks**: There was a detailed comparison drawn between Noether's Theorem and its potential role in neural networks, particularly in understanding symmetry breaking in neural networks and the conservation principles analogous to those in physics.
3. **Conserved Quantities and Energy Conservation**: The conversation delved into the concept of conserved quantities in both physics and machine learning, with a particular emphasis on the conservation of energy and momentum in physical systems and their potential analogs in machine learning processes.
4. **Understanding Noether's Theorem**: Users shared insights on Noether's Theorem, emphasizing its significance in physics and potential implications in machine learning. They discussed the importance of invariance and symmetry in both disciplines and how Noether's Theorem plays a role in establishing conservation laws.
5. **Conservation Laws and Symmetry**: There was a discussion on the relationship between conservation laws and symmetry, particularly in the context of time invariance and conservation of energy, momentum, and angular momentum, highlighting the fundamental principles that govern physical systems and potentially extend to machine learning algorithms.

Overall, the discussion showcased a deep dive into the application of fundamental physical principles such as Noether's Theorem to the realm of machine learning, exploring the potential connections and implications for understanding the underlying principles governing both disciplines.

### WhatsApp forces Pegasus spyware maker to share its secret code

#### [Submission URL](https://arstechnica.com/tech-policy/2024/03/whatsapp-finally-forces-pegasus-spyware-maker-to-share-its-secret-code/) | 404 points | by [Tomte](https://news.ycombinator.com/user?id=Tomte) | [131 comments](https://news.ycombinator.com/item?id=39566766)

WhatsApp has scored a significant legal victory in its ongoing battle against the NSO Group, a developer of sophisticated spyware known as Pegasus. A US district judge has ruled in favor of WhatsApp, granting the messaging app access to explore the "full functionality" of Pegasus. This decision marks a crucial milestone in WhatsApp's efforts to protect users from unlawful surveillance activities. While the NSO has lost some battles in court, it still retains the secrecy of its clients, leaving countries like Poland, Saudi Arabia, and India potentially shielded from public scrutiny for their use of the controversial spyware. The case is set to proceed to trial in 2025, shedding light on the complex legal and ethical issues surrounding surveillance technology and its impact on civil society.

The discussion on the WhatsApp legal victory against NSO Group and the Pegasus spyware on Hacker News covers various aspects. Users discuss the challenge of maintaining national security while dealing with surveillance technology and legal repercussions. The conversation delves into the complexities of intelligence agencies seeking ways to avoid legal consequences, the role of the FISA court system in overseeing governmental scrutiny, and the implications of Executive Orders in intelligence operations. Additionally, there are debates about compound words, linguistics, and language usage. Some users emphasize the importance of correct language usage, while others argue that language evolves based on common usage. The discussion also touches on contronyms, syntactic correctness, and the cultural differences in language interpretation.

### A story of a large loop with a long instruction dependency chain

#### [Submission URL](https://johnnysswlab.com/a-story-of-a-very-large-loop-with-a-long-instruction-dependency-chain/) | 30 points | by [signa11](https://news.ycombinator.com/user?id=signa11) | [4 comments](https://news.ycombinator.com/item?id=39562194)

Johnny's Software Lab LLC delves into the intricate world of software performance. In a recent case study, they tackled a thorny performance issue involving a lengthy loop. Despite the loop being optimized with vector intrinsics, a high cycles per instruction (CPI) number hinted at underlying inefficiencies. Upon closer inspection, they uncovered a chain of instruction dependencies within the loop, dampening CPU performance.

To experiment with this scenario, a loop with a prolonged dependency chain but no loop-carried dependencies was constructed. By analyzing the impact of nested cosine calculations on CPI, they revealed a diminishing CPI trend as the dependency chain lengthened. Introducing interleaving as a solution to boost instruction level parallelism (ILP) proved effective but complex, raising concerns of register usage and code complexity.

Exploring an alternative approach, they employed loop fission to split the original loop into smaller segments, each handling a cosine calculation. This method, although not as efficient as interleaving, showed improvements in performance compared to the initial loop setup. By breaking down the loop into manageable chunks with temporary storage for intermediate results, the fissioned loop exhibited better performance outcomes.

In the realm of software performance optimization, Johnny's Software Lab LLC delves deep into the intricacies of code structure to enhance efficiency and execution speed.

- User "rbnhstn" shared a link to the web archive of Johnny's Software Lab LLC's report on software performance optimization.
- User "tmnd" commented on the post mentioning concerns about bandwidth consumption.
- User "jrt" discussed coding with AVX2 and the challenges of handling multiple data product dependencies, mentioning the performance differences seen in MKL. They experimented with Linked L1 cache but did not see improvements.
- User "pltcn" flagged the post, stating that the comment by "mrkbrns" seemed like punch line and not relevant to the discussion. They pointed out that the comment was intended to provide feedback on the study tackled by Johnny's Software Lab LLC.
- 
### CACM Is Now Open Access

#### [Submission URL](https://cacm.acm.org/news/cacm-is-now-open-access-2/) | 349 points | by [transpute](https://news.ycombinator.com/user?id=transpute) | [49 comments](https://news.ycombinator.com/item?id=39559411)

The latest news on Hacker News is about the exciting announcement that Communications of the ACM (CACM) is now fully Open Access. This means that over six decades of CACM's research articles, technical reports, and more are now accessible to everyone, not just ACM members or Digital Library subscribers. This change comes as part of ACM's plan to transition to a fully Open Access publisher by 2026. The move aims to increase engagement with the broader computer science community and benefit CACM authors by expanding their readership. Members are encouraged to support ACM's efforts to keep this transition sustainable. The ACM Digital Library has also been opened up, with plans to make the entire archive of over 600,000 articles accessible in the future. This shift aligns with ACM's goal to shape the future of computing by involving its members.

The discussion on the Hacker News submission about Communications of the ACM going fully Open Access includes various perspectives and additional information. Some users express their delight at the move, highlighting the importance of making research accessible to all. There is a mention of the significance of the change in reaching a wider audience and aiding in clarification on complex subjects. Some users also reference other related publications and the availability of content such as Programming Pearls and New Turing Omnibus. The conversation expands to discuss licensing issues and the complexities related to Open Access publications, with points raised about licensing models, the distribution of content, and the implications for readers and researchers. Overall, the discussion reflects a mix of reactions, ranging from appreciation for the move towards Open Access to considerations about the practicalities and implications of such a transition.

### Open-Source AI at FOSDEM

#### [Submission URL](https://lwn.net/Articles/961868/) | 74 points | by [kristianpaul](https://news.ycombinator.com/user?id=kristianpaul) | [4 comments](https://news.ycombinator.com/item?id=39567960)

The latest buzz at FOSDEM 2024 was all about open-source AI models, particularly large language models (LLMs) that can generate human-like text. Even though companies developing these models are hesitant to open-source them due to the hefty investment required, there's a growing trend of imposing ethical restrictions on AI models through licensing. Niharika Singhal from the Free Software Foundation Europe highlighted various restrictions like the Hippocratic License and the Llama 2 v2 use policy that limit the use of AI models for certain activities. She emphasized the importance of ensuring that the licenses of AI models are interoperable with free-software licenses to maintain openness in AI.

Stefano Maffulli from the Open Source Initiative discussed efforts to define open-source AI, stating that for an AI system to be categorized as open-source, it should grant users the freedoms to use, study, modify, and share it. The OSI plans to release a new draft of the open-source AI definition monthly, aiming for a 1.0 release by the end of October 2024. Maffulli emphasized that there can't be a middle ground – an AI system is either open source or it isn't. Misuse of the term "open source" was pointed out, such as Meta's Llama 2 model which, despite being labeled as open source, has restrictions on commercial use that conflict with the Open Source Definition.

The discussion also touched on the significance of open data sets, particularly for non-English languages, in training AI models. Overall, FOSDEM shed light on the evolving landscape of open-source AI and the need for transparency, ethical considerations, and community involvement in shaping its future.

- **mjns:** Foundation mentions the transparency index at Stanford.
- **TaylorAlexander:** OSI plans to release a draft of the open-source AI definition monthly through virtual public town halls, aiming for version 1.0 by October 2024. Maffulli welcomes participation in discussions regarding the drafts on the OSI's public forum.
- **vrvrd:** Comment on GPT-4 being a closed model and the challenges of training models due to issues like data set weights. There is an understanding of metrics but mentions a potential bias in the output.
- **sylwr:** Mentions that AI requires high hardware and training data access, suggesting a need for more robust hardware and training data sources.

### Groq's ultrafast LPU could well be the first LLM-native processor

#### [Submission URL](https://www.techradar.com/pro/feels-like-magic-groqs-ultrafast-lpu-could-well-be-the-first-llm-native-processor-and-its-latest-demo-may-well-convince-nvidia-and-amd-to-get-out-their-checkbooks) | 21 points | by [IronWolve](https://news.ycombinator.com/user?id=IronWolve) | [9 comments](https://news.ycombinator.com/item?id=39566649)

Groq, led by ex-Google engineer and CEO Jonathan Ross, has made a groundbreaking claim by creating the first-ever Language Processing Unit (LPU) that promises to revolutionize AI applications with its lightning-fast speeds. The Tensor Stream Processor (TSP) by Groq is designed like an assembly line, optimizing data processing tasks unlike traditional GPUs, which operate as static workstations. The efficiency and scalability of Groq's chip design have been demonstrated through impressive demos, showcasing the potential for significant advancements in AI technology. The latest public demo revealed Groq's AI Answers Engine's remarkable speed in generating factual, cited answers in less than a second. This achievement has positioned Groq as a key player in the AI industry, challenging existing technologies like Chat-GPT. If you're curious to experience the speed of Groq for yourself, you can explore it on a chat page with various available models. Groq's innovative approach to AI processing has set a new standard for performance and efficiency in the field.

- User "3abiton" questions the distinction between Groq's Language Processing Unit (LPU) and Google's Tensor Processing Unit (TPU) in terms of their native language processing capabilities.
- User "seungwoolee518" discusses the impact of the pre-cryptocurrency mining boom on the market, highlighting that many people traded proprietary GPUs for FPGA and ASIC accelerated devices.
- User "jsnjmcgh" expresses skepticism about Groq's claims, mentioning the need for concrete proof of their technology's capabilities. User "Archit3ch" references a benchmark test where silicon problem bits worth $12 million were thrown away, indicating a possible critique of Groq's approach.
- User "dk" expresses concern about the security of Groq's intellectual property, comparing it to Fort Knox and suggesting that China might attempt to access it.
- User "LorenDB" praises Groq for being a game-changer that emphasizes self-hosting within AI applications, but user "zchb" counters this by noting that Groq's systems rely on a memory system of on-chip SRAM rather than larger systems with local DRAM or HBM.

### Show HN: OfflineLLM – a Vision Pro app running TinyLlama on device

#### [Submission URL](https://apps.apple.com/us/app/offlinellm/id6478590762) | 120 points | by [codepixel](https://news.ycombinator.com/user?id=codepixel) | [60 comments](https://news.ycombinator.com/item?id=39557098)

The top story on Hacker News today is about a new app called VisionLLM that offers unlimited, private, offline access to an AI chat-bot. Users can augment their daily activities with the help of this powerful tool, which can be downloaded easily in just a few seconds. The app allows users to start new chats, send messages via voice input or typing, and delete chats as desired. Developer Konrad Gnat ensures user privacy by not collecting any data from the app. The app is available for $6.99 and is compatible with visionOS 1.0 and later. With VisionLLM, users can enhance their lives using the power of AI at their fingertips.

The discussion on the top story about the new app VisionLLM on Hacker News covers various aspects such as system requirements, alternative models, user experience, and potential applications. 

1. Some users discuss the high RAM usage of apps like Vision Pro and limitations on iOS/iPadOS systems.
2. There is a comparison with MLX optimizations, and users share their experiences with different models like Stable LM and Gemma.
3. Feedback on the presentation of the app, requesting better screenshots and expressing interest in potential Venture Capital opportunities.
4. Discussions about 3D vector assistants, AI-human relations, and the potential of combining technologies like SillyTavern, Whisper TTS, and Silero.
5. Feedback on the choice of LLM models and their implications on device privacy and efficiency.
6. Further discussions on the performance of different models, the development process, and the pricing strategy.

Overall, the comments highlight a mix of technical assessments, user experiences, and suggestions for improvement in various aspects of the app and its underlying technologies.

### Measuring GitHub Copilot's impact on productivity

#### [Submission URL](https://cacm.acm.org/research/measuring-github-copilots-impact-on-productivity/) | 82 points | by [explosion-s](https://news.ycombinator.com/user?id=explosion-s) | [73 comments](https://news.ycombinator.com/item?id=39564965)

The latest study on AI pair-programming tools like GitHub Copilot sheds light on how these tools significantly boost developer productivity across all skill levels. While the correctness of suggestions is important, the real driver of productivity gains is the utility of the suggestions as a starting point for further development.

The study focused on analyzing 2,631 survey responses from developers using GitHub Copilot to understand how developer interactions with the tool correlate with perceived productivity. Results show that the acceptance rate of suggestions is a better predictor of perceived productivity than other detailed contribution measures. The study also delves into the variations in acceptance rate among developers and over time.

Using acceptance rate as a coarse-grained monitoring tool for neural code synthesis systems can provide valuable insights into developer productivity. However, fine-grained investigation methods are still necessary due to the complexity of human factors involved in the coding process.

The study highlights the challenges of evaluating code completion systems, especially in generating accurate measures of productivity gains. By focusing on perceived productivity and acceptance rates, researchers aim to provide a more holistic understanding of the impact of AI tools on developer workflows.

The discussion on the submission regarding the study on AI pair-programming tools like GitHub Copilot covers various perspectives. 

Users like "nmlk" share their experiences using Copilot and how it has challenged their thinking processes, leading to improved outcomes. They emphasize the importance of simplicity and MVP solutions over complex ones for practical usage. However, others like "dgcm" caution that while refined software is valuable, overly refined tools can degrade quality.

There is also a debate on the perceived productivity of Copilot, with some users expressing skepticism and others highlighting its potential impact. The conversation shifts to the importance of self-improvement in coding and the challenges in balancing using tools like Copilot with enhancing personal skills.

Furthermore, the discussion delves into the efficiency of automated tool design, with some users pointing out the impact on developer costs and productivity. The debate extends to the significance of self-development in software engineering and the role of AI tools like Copilot in the coding process.

Overall, the discussion reflects a diverse range of opinions on the benefits and drawbacks of AI pair-programming tools like GitHub Copilot and their impact on developer productivity and skill development.

### Generative AI and the big buzz about small language models

#### [Submission URL](https://the-decoder.com/stripedhyena-a-new-architecture-for-next-generation-generative-ai/) | 12 points | by [milliondreams](https://news.ycombinator.com/user?id=milliondreams) | [4 comments](https://news.ycombinator.com/item?id=39567770)

In the latest development in AI research, Together AI has unveiled the cutting-edge StripedHyena, a revolutionary architecture challenging the dominance of the transformer models like GPT-4. This new family of language models, including the base model StripedHyena-Hessian-7B (SH 7B) and the chat model StripedHyena-Nous-7B (SH-N 7B), boasts 7 billion parameters and can process incredibly long contexts of up to 128,000 tokens.

What sets StripedHyena apart is its utilization of a state-space model (SSM) layer, which enhances training and inference efficiency, outperforming traditional transformers in processing sequences of 32,000 to 128,000 tokens with impressive speed gains reaching over 100%. This innovation aims to push the boundaries of AI architecture design and promises further advancements such as larger models, multimodal support, and improved performance optimizations in the future.

With researchers from various institutions collaborating on this project, StripedHyena represents an exciting leap forward in the quest for next-generation generative AI. AI enthusiasts and developers worldwide now have a promising alternative to explore in their pursuit of enhanced AI capabilities.

1. "mllndrms" mentioned systems involving blending specialist small language models using the MoE framework in the industry. This could be a reference to the potential impact of specialized models within the context of the AI industry.
2. "cmprssdgs" provided a brief summary and shared a link to a detailed source linked in the article mentioned in the submission about StripedHyena-7B. This indicates a desire to explore further details on the topic.
3. "swmwththbt" discussed the Mamba architecture described in the submission, highlighting a nested transformers-like structure with a state-space model (SSM) layer. A link to an arXiv paper was shared for additional reference.
4. Within this conversation, "sal9000" contributed by noting the hierarchical blocks within the Mamba Hierarchy model are considered SSM Mamba, further elaborating on the SSM concept within the Mamba architecture. This demonstrates an engagement with the technical aspects of the architecture introduced by StripedHyena.

### JPEG XL and the Pareto Front

#### [Submission URL](https://cloudinary.com/blog/jpeg-xl-and-the-pareto-front) | 481 points | by [botanical](https://news.ycombinator.com/user?id=botanical) | [291 comments](https://news.ycombinator.com/item?id=39559281)

Version 0.10 of libjxl has just been released, bringing significant improvements in memory usage and speed for JPEG XL encoding. This release includes the implementation of a "streaming encoding" API, allowing large images to be processed in chunks, resulting in more memory-friendly encoding and faster speeds. For example, compressing a large NASA image now requires significantly less RAM and time compared to the previous version. The update showcases how different effort settings affect memory usage, compression time, and file size, highlighting the trade-offs in compression techniques. Additionally, the concept of Pareto optimality in compression methods is discussed, emphasizing the balance between compression density and encode speed. The new libjxl version achieves Pareto-optimal results, outperforming previous versions and other compression formats like PNG and lossless AVIF. Overall, this update marks a substantial advancement in JPEG XL encoding, offering enhanced performance for various use cases.

The discussion revolves around the release of Version 0.10 of libjxl, particularly focusing on improvements in JPEG XL encoding and the comparison with other formats like WebP, PNG, AVIF, and lossless JPEG2000. There are debates about the benefits of lossless WebP versus other formats like MozJPEG, optiPNG, and AVIF, as well as discussions about the limitations and advantages of different compression techniques. The conversation also delves into topics such as HDR imaging, compatibility issues, color space limitations, and the performance of various image formats on different platforms. Moreover, there are technical explanations about compression methods, color space transformations, and comparisons of compression densities among different image formats. The dialogue highlights the complexity and nuances of image compression techniques and the ongoing development in the field.

### Elon Musk sues OpenAI and Sam Altman over alliance with Microsoft

#### [Submission URL](https://www.ft.com/content/6a4cfcd6-b39d-46bb-b40a-2ace23682996) | 14 points | by [dkjaudyeqooe](https://news.ycombinator.com/user?id=dkjaudyeqooe) | [9 comments](https://news.ycombinator.com/item?id=39560528)

In a surprising turn of events, Elon Musk has filed a lawsuit against OpenAI and Sam Altman, alleging breach of contract. The lawsuit has stirred curiosity and raised eyebrows in the tech community. Musk, known for his involvement in various innovative projects, is now taking legal action against his former partners. This story is creating ripples in the tech world and leaving many wondering about the details and implications of this legal battle. Stay tuned for updates on this developing story.

The discussion on this submission covers various viewpoints regarding Elon Musk's lawsuit against OpenAI and Sam Altman. 
1. User "drlly" seems to be enjoying the drama and speculates about the potential motives behind Musk's actions, suggesting a personal vendetta and competition in the AI sector.
2. User "lcng" questions the validity of the lawsuit, pointing out the need to look at the facts in the legal filing rather than resorting to personal attacks and character judgments.
3. User "jstnclft" provides a link for further information on the topic.
4. User "more_corn" delves into the idea of wealthy individuals like Musk scrutinizing non-profit organizations for seeking profit, drawing a comparison between OpenAI and Tesla's differing profit motives.
5. User "fnrdpglt" argues that OpenAI's mission of serving humanity may conflict with profit-seeking motives, while also contrasting the organization's goals with those of Tesla.
6. Users "dprctv" and "tctsrc" touch on the importance of distinguishing between seeking profit and benefiting society, with one viewpoint suggesting that seeking profit can actually help humanity at a larger scale.
7. User "lcng" calls for supportive arguments rather than derogatory remarks. 

Overall, the discussion highlights a mix of opinions on the motivations behind Musk's lawsuit and the contrasting goals of OpenAI as a non-profit organization in the tech industry.

---

## AI Submissions for Thu Feb 29 2024 {{ 'date': '2024-02-29T17:11:26.750Z' }}

### NVK is now ready for prime time

#### [Submission URL](https://www.collabora.com/news-and-blog/news-and-events/nvk-is-now-ready-for-prime-time.html) | 83 points | by [jalict](https://news.ycombinator.com/user?id=jalict) | [19 comments](https://news.ycombinator.com/item?id=39547971)

The open-source Vulkan driver for NVIDIA hardware in Mesa, NVK, is now ready for prime time! With the recent merge request eliminating non-conformant implementation warnings and updating the Meson configuration option to just "nouveau," NVK is set to be included in Mesa 24.1. The driver is now a conformant Vulkan 1.3 implementation on various NVIDIA GPUs, offering improved performance and stability. Additionally, efforts are underway to ensure DXVK runs smoothly on upstream Mesa, with plans for D3D12 emulation via VKD3D-Proton in the works. OpenGL support via Zink + NVK is also being developed to provide OpenGL 4.6 on top of NVK. Exciting times ahead for NVIDIA users on Linux distros!

The discussion surrounding the submission about the open-source Vulkan driver for NVIDIA hardware in Mesa, NVK, on Hacker News delves into various technical aspects and implications of this development:

1. Users discuss the importance of achieving performance and stability with Vulkan 1.3 compliance, highlighting the significance of conformant implementation and technical aspects such as different extensions' functionality across hardware vendors like AMD and Nvidia.
2. There is a comparison between Vulkan and OpenGL, noting the potential improvements and differences between the two APIs, with further exploration into the hardware features fundamental for ray tracing and the limitations on older hardware.
3. Participants delve into the Vulkan roadmap and the need for compatibility and feature expansion to address a wide range of cases, touching upon the comparison with DirectX 12 Ultimate features and potential API enhancements in Vulkan 2.0.
4. The conversation navigates towards the challenges of making Vulkan a single API covering both low-end mobile and high-end desktop targets, considering the practicality of separating these targets and the implications for hardware capabilities.
5. Additionally, hindsight reflections are shared on the separation of OpenGL and GLES, improving configuration profiles for Vulkan, and the ongoing efforts to enhance Nvidia support on Linux desktops.

Overall, the discussion explores the technical nuances, strategic considerations, and future possibilities stemming from the advancements in the NVK driver and its impact on the Linux distro ecosystem.

### Self-pay gas station pumps break across NZ as software can't handle Leap Day

#### [Submission URL](https://arstechnica.com/gadgets/2024/02/leap-year-glitch-broke-self-pay-pumps-across-new-zealand-for-over-10-hours/) | 59 points | by [ooboe](https://news.ycombinator.com/user?id=ooboe) | [42 comments](https://news.ycombinator.com/item?id=39553755)

On this Leap Day in New Zealand, the calendar quirk led to a significant disruption at gas stations as payment systems failed to process card transactions. The software glitch caused self-serve pumps to be out of order for over 10 hours, impacting major suppliers like Allied Petroleum, BP, Gull, Waitomo, and Z Energy. The issue, traced back to a "leap year glitch," left some customers unable to refuel unless they paid in-store. The incident serves as a reminder of the growing reliance on technology and how even a small hiccup like a bonus day can have widespread effects. Efforts are underway to investigate the cause and prevent similar events in the future, with some businesses offering apologies and discounts to affected customers. As the situation gets resolved, maybe gas station operators will remember to update their calendars in four years' time.

The discussion on the submission covers a range of perspectives and experiences related to the Leap Day glitch at gas stations in New Zealand. Some users pointed out differences in fueling practices between countries like the US and Europe, where full prepayment is common in some regions. Others shared their experiences with prepayment systems, such as in the UK where paying with cash was still prevalent. The topic of contactless payments also surfaced, with users discussing the varying acceptance and functionality of such methods in different countries. Additionally, there were mentions of payment practices in countries like France, India, and Latin America.

The conversation also touched on security measures at gas stations, with references to prepayment requirements in certain locations like France and Pakistan. The issue of manual authorization when using cards at gas stations was mentioned, contrasting practices from different countries like Denmark where cards are mandatory, and Finland where terminals don't dispense fuel until payment is made.

Furthermore, some users shared humorous anecdotes about their interactions at gas stations, such as mistakenly not noticing the option to pay at self-serve pumps in Europe or encountering delays due to unfamiliar fueling processes.

The discussion also veered towards technical aspects, like the potential scripting and logic flaws that could lead to calendar-related glitches. One user flagged a comment that pointed out the possibility of data manipulation issues involving dates and recommended approaches to avoid such errors.

Finally, the thread also referenced a related Hacker News post about leap year bugs and the challenges they pose in software development.

### The Claro Programming Language

#### [Submission URL](https://docs.clarolang.com/) | 152 points | by [signa11](https://news.ycombinator.com/user?id=signa11) | [66 comments](https://news.ycombinator.com/item?id=39545501)

The Claro programming language is making waves as a statically typed JVM language offering a clear path to building highly concurrent and scalable applications. With modern build tooling in mind, Claro allows swapping dependencies effortlessly and eliminates the need for runtime "Dependency Injection" frameworks. Developed by a Xoogler, Claro boasts a structured concurrency model ensuring programs are non-blocking, data-race free, and optimally scheduled. It emphasizes data-oriented programming with strict separation between data and functionality, offering a robust standard library and build time metaprogramming. While still in development, Claro welcomes contributions and aims to scale seamlessly for projects of any size. Join Claro's journey with its single maintainer, Jason Steving, and be a part of this exciting development!

The discussion on the submission about the Claro programming language on Hacker News covered various aspects of the language's design and implementation:

- **User "3PS"** expressed interest in the language's declarative concurrency approach but was disappointed by the decision to use UTF-16 strings, suggesting that UTF-8 could have been a more efficient choice for multilingual applications.
- **User "kaba0"** raised concerns about deadlocks in concurrency, stating that sharing nothing communication via messages can help avoid deadlocks.
- **User "jsnstvng"** thanked the community for showing interest in Claro's Graph Procedures feature, highlighting its powerful concurrency abstraction that simplifies complexities related to concurrency.
- **User "thsz"** mentioned SQL SERIALIZABLE solutions and shared a link related to Claro's guaranteed non-blocking data-race free and deadlock-free programming.
- **User "glnn"** wondered about constrained code to satisfy properties for a better user experience, to which **"jsnstvng"** responded by explaining Claro's approach to handle constraints and properties in code to avoid deadlocks and allow for explicit blocking and completion.
- **User "trvsgrggs"** shared thoughts on various programming languages, expressing nostalgia for older languages and touching upon the evolution of languages like Scheme, Forth, C++, Pascal, and more.
- **User "klyrs"** discussed the potential game-changing aspects of innovations in programming languages, including Claro's focus on graph-based multithreaded work scheduling and its implications for program organization and syntax.
- **User "llndr"** and **"bmtc"** engaged in a discussion about REPLs in Rust and the importance of language advancements from a software-writing perspective, touching upon REPL-related development practices and the evolution of programming languages.

Overall, the comments reflected a mix of appreciation for Claro's concurrency abstractions, concerns regarding specific language design choices, and broader discussions on the evolution and impact of programming languages on software development practices.

### Essential Math for AI

#### [Submission URL](https://www.oreilly.com/library/view/essential-math-for/9781098107628/) | 38 points | by [teleforce](https://news.ycombinator.com/user?id=teleforce) | [9 comments](https://news.ycombinator.com/item?id=39545820)

Top Story: "Essential Math for AI" by Hala Nelson is a newly released guide that focuses on the fundamental mathematical concepts essential for success in the field of artificial intelligence. The book explores a wide range of mathematical topics critical for AI, such as regression, neural networks, optimization, backpropagation, convolution, Markov chains, and more, with a focus on real-world applications rather than dense academic theory. Whether you are just starting your career or have years of experience, this book aims to provide you with the necessary foundation to delve deeper into the AI realm.

The book covers various topics including data analysis, fitting functions to data, optimization for neural networks, convolutional neural networks, and computer vision. It also includes Jupyter notebooks with Python code and visualizations to aid understanding. With AI becoming increasingly integral to businesses, having a strong grasp of the underlying math is crucial for building successful AI solutions. "Essential Math for AI" aims to equip engineers, data scientists, and students with the mathematical fluency needed to interpret and explain the decisions made by AI systems.

For those looking to enhance their mathematical understanding in the realm of AI, this book provides a comprehensive guide to the core mathematical principles that power AI systems.

The discussion on the submission revolves around various viewpoints regarding the "Essential Math for AI" book:

- jnlsncm expresses skepticism about the theory that the brain works similarly to backpropagation in artificial neural networks, highlighting the differences between the complexity of neurons in brains and artificial networks. They argue that neural networks fundamentally work as approximators of functions efficiently due to hardware advancements such as GPUs and TPUs.
- ysc points out the similarities and differences between neurological systems in the brain and artificial neural networks, emphasizing the need for a clearer mathematical model of neurons in artificial networks to capture the fundamental building blocks accurately.
- Buttons840 discusses the significance of mathematics in neural networks, mentioning the evolution from basic neural networks in the 1960s to more sophisticated architectures like Transformers in 2017, attributing the success of neural networks to mathematical sophistication.
- dr_kiszonka shares a link to the GitHub repository for "Essential Math for AI" for further reference.
- ndrlgc expresses curiosity about the content of the book and references a PDF for more information.
- mistrial9 humorously notes that large portions of major AI systems are not currently understood, highlighting the gap in understanding between developers and these complex systems.
- fncyfrdbt mentions a section of the book regarding the mysterious success of neural networks and their lack of strong theoretical foundations, asserting the importance of mathematical understanding in AI topics covered by the book.

Overall, the discussion touches upon the application of mathematical concepts in understanding artificial intelligence, the comparison between biological and artificial neural networks, and the need for a stronger theoretical foundation in AI.

### Google brings Stack Overflow's knowledge base to Gemini for Google cloud

#### [Submission URL](https://techcrunch.com/2024/02/29/google-brings-stack-overflows-knowledge-base-to-gemini/) | 53 points | by [onatm](https://news.ycombinator.com/user?id=onatm) | [36 comments](https://news.ycombinator.com/item?id=39552701)

In a groundbreaking move, TechCrunch and the developer Q&A giant, Stack Overflow, are teaming up to provide AI companies access to a valuable knowledge base through a newly launched API called OverflowAPI. The collaboration's initial partner is Google, which plans to leverage Stack Overflow's vast data trove to enhance Gemini for Google Cloud by offering validated answers directly within the Google Cloud console. This initiative builds on the previous launch of OverflowAI, showcasing the ongoing efforts to infuse AI capabilities into Stack Overflow's platform. 

While financial details remain undisclosed, this partnership signals a broader trend in content-driven services seeking compensation for their data utilization by large language models like AI chatbots. Stack Overflow CEO Prashanth Chandrasekar emphasized the program's openness to all partners and highlighted the importance of providing trustworthy, expert-validated answers for developers, even as AI tools reshape the developer workflow.

Google's VP of developer experience for Google Cloud, Gabe Monroy, outlined a seamless integration of Stack Overflow's vast knowledge base within the Google Cloud console to provide a comprehensive solution for developers. The joint effort aims to merge AI tools with the human expertise that has defined Stack Overflow's high-quality user base and content over the years. By balancing AI-driven features with the platform's commitment to accuracy and quality, both Google and Stack Overflow seek to offer developers a unified and reliable source of information and support.

Looking ahead, the partnership may also lead to advancements in Google's code completion model, Codey, underscoring the potential for continued collaboration and innovation in the AI and developer community.

The discussion on the submission about the collaboration between TechCrunch, Stack Overflow, and Google through the OverflowAPI revolves around various aspects:

1. **Data Utilization by AI Providers**: There is concern about how AI companies leverage the data from platforms like Stack Overflow and Reddit. Users discuss issues with the degradation of quality over time due to AI processing and the potential challenges of ensuring accurate responses from language models (LLMs).
2. **Licensing and Ownership of Knowledge**: Some users highlight the importance of licensing and sharing knowledge transparently, while others point out instances where AI models have misunderstood or misinterpreted information. The debate touches upon the reliability of AI-generated responses in comparison to human contributions.
3. **Google's Integration and AI Development**: Discussions focus on the benefits of integrating Stack Overflow's knowledge base into Google Cloud console and the potential advancements in Google's code completion model, Codey, resulting from this partnership.
4. **Concerns about AI and Knowledge Management**: Some users express concerns about the implications of advanced AI technologies on knowledge management and community interactions, questioning the trustworthiness and accuracy of AI-generated content.
5. **AI's Impact on Search Engines and Interaction**: There is a conversation about AI's role in search engine results, user interactions, and the challenges posed by AI-driven search experiences, along with reflections on browsing behaviors and user trust in AI-generated content.

The conversation encompasses a wide array of perspectives on AI utilization, data ownership, knowledge sharing, and the implications of AI advancements on community-driven platforms like Stack Overflow and Reddit.

### Wikipedia No Longer Considers CNET "Generally Reliable" Source After AI Scandal

#### [Submission URL](https://futurism.com/wikipedia-cnet-unreliable-ai) | 27 points | by [nabla9](https://news.ycombinator.com/user?id=nabla9) | [4 comments](https://news.ycombinator.com/item?id=39556073)

In a shocking turn of events, Wikipedia has revoked the "generally reliable" status of CNET as a source following an AI scandal. The controversy erupted when it was revealed that AI-generated articles published by CNET were riddled with errors and plagiarism, sparking heated debates among Wikipedia editors.
The fallout from the scandal extended beyond CNET, with other Red Ventures-owned sites like Bankrate and CreditCards.com also under scrutiny for similar practices. Allegations surfaced that AI-generated content was being distributed across Red Ventures' portfolio, raising concerns about transparency and credibility.

The saga unfolded as CNET's reputation took a hit, with accusations of editorial misconduct adding fuel to the fire. Despite efforts to address the issue, including pausing AI initiatives and issuing corrections, the damage was irreversible in the eyes of Wikipedia editors.
As a result, Wikipedia's guidelines now classify CNET as unreliable for the period it utilized AI and highlight a deterioration in editorial standards since its acquisition by Red Ventures in 2020. The once-trusted source has now been relegated to a cautionary tale about the pitfalls of embracing AI-driven content without proper oversight.

The comments on Hacker News in response to the submission about Wikipedia revoking CNET's status as a reliable source due to an AI scandal included discussions about AI-generated content and the importance of individual writers over automated processes. 
User "ChrisArchitect" pointed out that the discussion was a duplicate of a previous thread, directing readers to the relevant conversation. 
User "Spivak" expressed happiness that content farms generating AI articles are being shunned, as they believe it allows regular people to follow individual writers and editors rather than faceless brands. 
In response to Spivak's comment, user "pmlttc" mentioned that there are plenty of individual writers to follow, providing links to websites showcasing generated faces of non-existent individuals. Another user, "pyl," raised concerns about the distorted backgrounds of the generated images, suggesting that they are horribly disfigured. 
Overall, the discussion touched on the implications of relying on AI-generated content, the value of individual writers, and the challenges faced by content farms in maintaining transparency and credibility.

---

## AI Submissions for Wed Feb 28 2024 {{ 'date': '2024-02-28T17:11:20.659Z' }}

### The Era of 1-bit LLMs: ternary parameters for cost-effective computing

#### [Submission URL](https://arxiv.org/abs/2402.17764) | 933 points | by [fgfm](https://news.ycombinator.com/user?id=fgfm) | [411 comments](https://news.ycombinator.com/item?id=39535800)

The Era of 1-bit LLMs: A paper on Hacker News discusses the rise of 1-bit Large Language Models, paving the way for more efficient and cost-effective models in the field of Machine Learning. The authors introduce BitNet b1.58, a variant where every parameter is ternary {-1, 0, 1}, matching the performance of full-precision models while being more resource-friendly. This advancement in LLMs has the potential to redefine how models are trained and opens possibilities for specialized hardware design. Click the link to delve into the details of this groundbreaking research!

The discussion on Hacker News revolves around a paper discussing the era of 1-bit Large Language Models (LLMs) and their potential impact on Machine Learning. Some users express surprise at the switching of existing LLMs from floating-point values to ternary values (-1, 0, 1) and the efficiency gains this brings. References to past works like BinaryConnect and the significance of Straight Estimators are made. There are debates on the efficacy of trinary weights in improving performance and memory efficiency. The conversation delves into technical details of matrix multiplications, the significance of trinary values in networks, and the impact on the training of large models. Users discuss the implications for hardware design, the complexity of model training, and the insights gained from comparing different models. Some users question the reported perplexity results of the 70B model and the trade-offs between memory usage and performance. Additionally, the discussion touches on the challenges and potential benefits of quantization in model training.

### Calendar meeting links used to spread Mac malware

#### [Submission URL](https://krebsonsecurity.com/2024/02/calendar-meeting-links-used-to-spread-mac-malware/) | 83 points | by [todsacerdoti](https://news.ycombinator.com/user?id=todsacerdoti) | [18 comments](https://news.ycombinator.com/item?id=39540793)

Malicious hackers are using a clever tactic that starts with adding a link to the victim's calendar on Calendly, a popular scheduling application. The attackers pose as well-known cryptocurrency investors, scheduling video calls that lead to the installation of malware on macOS systems. One startup seeking investment for a blockchain platform fell victim to this scam, as detailed by KrebsOnSecurity.

The scam begins with an impersonator on Telegram, using the name and profile of a legitimate investor, to schedule a video call through Calendly. When the victim clicks the meeting link, a script is run quietly installing malware. In this case, the scam artists pretended to be from Signum Capital, a reputable investment firm based in Singapore. The victim, known as "Doug," eventually realized the ruse after encountering technical difficulties during the video call setup, prompting a suspicious response.

This incident sheds light on a sophisticated phishing campaign linked to North Korean state-sponsored hackers who use social engineering tactics. The malicious script, disguised as a legitimate meeting link, leads users to unwittingly download and execute malware, granting hackers control over the victim's computer. The attackers, associated with the group BlueNoroff, a subgroup of the notorious Lazarus hacking group, target various industries to steal funds and cryptocurrency.

While macOS systems are not immune to such threats, the incident emphasizes the need for vigilance and cybersecurity measures, especially as cybercriminals evolve their tactics to bypass existing defenses. It serves as a critical reminder for individuals and organizations in the cryptocurrency space to stay wary of social engineering attacks and prioritize security protocols to safeguard sensitive information.

The discussion on the Hacker News submission revolved around the malicious scheme targeting individuals in the cryptocurrency space through a sophisticated phishing campaign. Users pointed out flaws in Calendly's security and discussed how attackers exploit calendar links to deliver malware. Some users mentioned the prevalence of such tactics across various platforms, such as Microsoft Teams and Outlook, highlighting the importance of cybersecurity measures. The conversation also touched on the vulnerability of macOS systems to script-based attacks and the need for security improvements in default applications. Additionally, there was talk about the ongoing conflicts in Ukraine and the importance of organizations strengthening their cybersecurity protocols to protect against social engineering attacks like the one detailed in the article.

### Building unorthodox deep learning GPU machines

#### [Submission URL](https://www.kyleboddy.com/2024/01/28/building-deep-learning-machines-unorthodox-gpus/) | 91 points | by [simonpure](https://news.ycombinator.com/user?id=simonpure) | [21 comments](https://news.ycombinator.com/item?id=39532892)

At Driveline Baseball, they've built a powerful GPU cluster machine named cogito, utilizing last-gen rackmounted server hardware procured from eBay. The setup includes unique machines from Cirrascale with impressive specs like dual E5-2667 CPUs, 128GB DDR3 RAM, and custom PLX 8780-based cards for added PCIe slots. Despite being unconventional and requiring tweaks for cooling and setup, these machines offer great value for deep learning tasks.

The author, Kyle Boddy, shares the adventure of integrating RTX 3090 GPUs sourced from Facebook Marketplace and deprecated crypto mining rigs into the cluster. By combining components and experimenting with configurations, a system with 8x RTX 3090s running on custom PSUs emerges, showcasing a blend of resourcefulness and technical know-how.

Future posts are promised to delve into benchmarks, NVLink setups, and more technical insights, with updates shared on Kyle's Twitter account @drivelinekyle. The journey of repurposing overlooked server hardware into a high-performance machine is a testament to creativity and DIY spirit in the realm of deep learning infrastructure.

- There is a discussion about Nvidia's EULAs not allowing RTX 3090 for data-center compute purposes, with some users expressing concerns about driver license issues.
- A user shared a link to an archived article discussing the potential luck-driven success of unorthodox machines.
- Users discuss the recognition of the DIY spirit in building similar infrastructure setups, comparing Intel's and AMD's server-based rigs as viable alternatives due to incremental price increases.
- There is a conversation about utilizing Intel Optane drives for swapping drives, as well as the historical context of gaming GPUs entering the world of machine learning.
- Users debate the competitiveness of consumer-grade GPUs like the RTX 3090 compared to potential upcoming models like the 4080 and 4090, with varying opinions on their pricing and performance.
- The discussion touches on the perception of AMD's and Nvidia's consumer-grade cards in the context of Deep Learning Super Sampling (DLSS) and FidelityFX Super Resolution (FSR).
- A user shares thoughts on building unorthodox web server setups and the challenges of managing traffic and resources effectively.
- Some users report encountering website errors while others comment on Hacker News going down.

### How The Pentagon learned to use targeted ads to find its targets

#### [Submission URL](https://www.wired.com/story/how-pentagon-learned-targeted-ads-to-find-targets-and-vladimir-putin/) | 234 points | by [nova22033](https://news.ycombinator.com/user?id=nova22033) | [119 comments](https://news.ycombinator.com/item?id=39540738)

In 2019, Mike Yeagley, a government contractor and technologist, raised concerns in Washington, DC about the national security risks associated with popular dating app Grindr. Yeagley demonstrated how Grindr's geolocation data, accessible through online advertising exchanges, could be exploited, potentially compromising the security of government employees. By drawing geofences around sensitive government buildings, Yeagley identified Grindr users working at these locations and tracked their movements, highlighting the inadvertent exposure of personal information. Despite past discrimination against LGBTQ individuals in intelligence agencies, Yeagley's goal was not punitive but to underscore the need for data privacy. He emphasized that advertising data, available for purchase, could pose a significant threat if misused, while also serving as a valuable tool when used ethically. Yeagley's insights shed light on the complex intersection of technology, privacy, and national security.

The discussion surrounding the submission covers various topics. Here are some highlights:

1. Some users drew parallels between Grindr's geolocation data concerns and Facebook's targeted advertising practices.
2. There was a debate on the tracking of undocumented migrants and the use of technology to monitor them.
3. Users discussed the collection of location data by companies like Apple and the potential misuse of political phrasings for tracking purposes.
4. There was a detailed conversation about the implications of Grindr's geofencing activities around government buildings and the potential risks of tracking personal information.
5. Users expressed concerns about national security and privacy in relation to the use of cell phones and electronic devices in sensitive areas.
6. The discussion touched on the importance of enforcing labor laws for undocumented workers and the political dynamics surrounding immigration policies.

Overall, the conversation delved into the intersection of technology, privacy, national security, and immigration issues in light of the Grindr geolocation data concerns.

### Generating Expressive Portrait Videos with Audio2Video

#### [Submission URL](https://humanaigc.github.io/emote-portrait-alive/) | 73 points | by [hackerlight](https://news.ycombinator.com/user?id=hackerlight) | [23 comments](https://news.ycombinator.com/item?id=39533326)

A new project, EMO, is making waves in the digital world by creating expressive portrait videos using a unique audio-to-video diffusion model. The technology, developed by researchers at the Institute for Intelligent Computing at Alibaba Group, allows users to input a single image and vocal audio to generate animated videos with varied head poses and facial expressions. The framework consists of two stages: Frames Encoding and Diffusion Process, which work together to create lifelike avatars that sync with the audio input.

The versatility of EMO is demonstrated through examples such as creating vocal avatar videos of characters like Audrey Hepburn and AI personas with audio sources ranging from Ed Sheeran to Eminem. The technology can handle singing, talking, and even rap performances, ensuring that the animated characters stay in sync with the audio content.

Moreover, EMO supports multiple languages and portrait styles, giving users the freedom to bring diverse characters to life in a dynamic and expressive manner. The innovation opens up possibilities for creating multilingual and multicultural content, showcasing the potential for portraying characters from different eras and mediums through realistic animations.

With EMO, the boundaries of character animation are pushed further, allowing for a seamless blend of audio and visual elements to create engaging and immersive portrait videos.

The discussion on the EMO project covers various aspects such as the technology used, comparisons with other similar projects, concerns about authenticity and security, implications for journalism, and the technical challenges involved. 

One user expresses skepticism about the project, suggesting a different approach involving cryptographic signing of original sources to ensure privacy and authenticity. Another user points out the difficulty in differentiating computer-generated 3D models from real ones and highlights the limitations of current technology in this area.

There is also a discussion about the potential ethical implications of such technology, especially in the context of deepfake videos and the need for stringent measures to prevent misuse. The conversation touches upon the role of journals in verifying content and the importance of establishing trust in online sources.

Furthermore, users delve into the technical aspects of generating realistic animations, including the use of LIDAR data, gyroscope data, and GPS data for improving accuracy. There are concerns raised about the limitations of current hardware and software services, especially in terms of security and potential vulnerabilities.

Overall, the discussion reflects a mix of skepticism, curiosity, and awareness about the challenges and opportunities presented by advancements in avatar technology.

### Pingora: build fast, reliable and programmable networked systems

#### [Submission URL](https://github.com/cloudflare/pingora) | 224 points | by [KajMagnus](https://news.ycombinator.com/user?id=KajMagnus) | [39 comments](https://news.ycombinator.com/item?id=39535969)

Today on Hacker News, a library called Pingora, created by Cloudflare, is making waves. Pingora is a Rust framework designed to build speedy, dependable, and adaptable network services. With over 40 million internet requests served per second for several years, Pingora is battle-tested and proven to be reliable.

Some key features of Pingora include its support for Async Rust, fast and secure HTTP 1/2 proxying, TLS over OpenSSL or BoringSSL, gRPC and websocket proxying, customizable load balancing and failover strategies, and compatibility with various observability tools.

If security is your top priority, Pingora offers a memory-safe alternative to C/C++ services. For performance-sensitive applications, Pingora is fast and efficient. And for services that require extensive customization, Pingora provides highly programmable APIs.

To get started with Pingora, there's a quick starting guide available, along with a user guide covering configuration, server setup, and building custom HTTP server and proxy logic. API documentation for all the crates is also provided.

Notable crates in the Pingora workspace include Pingora-core, Pingora-proxy, Pingora-error, Pingora-http, Pingora-openssl, Pingora-boringssl, and more. Pingora supports Linux as its tier 1 environment, with focus on Unix environments like macOS. Both x86_64 and aarch64 architectures are supported.

Pingora follows a rolling MSRV policy with the current MSRV at 1.72. Contributions to Pingora are welcome, and the project is licensed under the Apache License, Version 2.0.

For developers looking to build fast, reliable, and evolvable network services, Pingora seems like a promising library to explore.

- **Mcphrrnm** mentions another Cloudflare blog post announcing the open-sourcing of Pingora, and **dkptms** provides a link to the current announcement.
- **Lewisl9029** expresses excitement about Cloudflare's move to replace NGINX with Pingora, mentioning the benefits of utilizing Rust for performance-sensitive applications. They also touch upon the potential advantages of memory caching.
- **Dng** adds related information about Cloudflare's transition from Nginx to the Rust-written Pingora and provides links to previous discussions on Hacker News.
- **Nps** shares a quick glance at NGINX's binary downloads, installation, configuration process, and suggests that setting up large businesses on Cloudflare might not be ideal due to certain limitations.
- **Pmchrrnm** discusses River, a reverse proxy product mentioned briefly in Cloudflare's blog post, and compares it to Pingora, highlighting scripting capabilities.
- **Ptcyk** notes an inconsistency in the TLS support level mentioned in the submission, and **Mcphrrnm** comments on the Rustls project's future focus on dropping TLS 1.2 support.
- **Throwaway63467** points out the lack of mentioning support for HTTP3 in Pingora, with **Prnl** clarifying that Cloudflare supports QUIC on the CDN side.
- **Jmsr** makes a comment about Cloudflare's work culture and staff.
- **Drrh** mentions a missed opportunity to supplement the functionality of Caddy with the highly programmable APIs provided by Pingora.
- **Sytten** congratulates the authors of the submission after waiting for a couple of years.
- **KajMagnus** relates Nginx vulnerabilities to the potential benefits of using Rust for security.
- **Snxyn** recalls the incident of Cloudbleed and mentions that transitioning to Rust could be a wise decision for Cloudflare, avoiding similar security issues faced with C++.

The discussion on Hacker News covers various aspects of Cloudflare's Pingora, such as its features, comparison to NGINX, TLS support, scripting capabilities, HTTP3 support, Cloudflare's technical decisions, and potential security benefits of using Rust. Participants express enthusiasm for Pingora's performance and customization features, along with discussing Cloudflare's shift towards Rust for increased security.

### Don't mock machine learning models in unit tests

#### [Submission URL](https://eugeneyan.com/writing/unit-testing-ml/) | 74 points | by [7d7n](https://news.ycombinator.com/user?id=7d7n) | [73 comments](https://news.ycombinator.com/item?id=39534856)

Eugeneyan shared a thought-provoking post on Hacker News about the challenges of unit testing machine learning code. Unlike traditional software unit tests that validate small pieces of logic, unit testing in machine learning involves testing the models themselves. The post discusses how ML code differs from regular software, the need to rethink unit testing strategies for ML code, and provides guidelines for testing ML code and models effectively.

Key points covered include: the difference in writing code that contains logic versus code that learns logic in ML, the importance of testing against the actual model in some scenarios, challenges with large ML models, and guidelines for unit testing ML code and models. The post also includes code examples for initializing models with random or empty weights and writing critical tests against the actual model.

The author invites feedback and best practices for unit testing machine learning code and models, making this post a valuable resource for anyone working in the field.

The discussion on Hacker News revolved around the topic of unit testing in machine learning (ML) code and models. Some users discussed the challenges and nuances of unit testing ML models, with one user expressing surprise at the critique of ML models in the initial post. Another user mentioned feeling disappointed by the content of the article.

There was a debate about the importance of distinguishing between integration tests and unit tests, with one user emphasizing the significance of fast tests for detecting issues quickly. Another user highlighted the need to properly define and structure tests based on the specific functionality being tested. Google's approach to unit testing in software development was also mentioned in the thread.

Additionally, there were comments on the historical context of terminology related to unit testing and the changing meanings of words within the programming community. Some users shared experiences of slow tests hindering development progress, emphasizing the importance of efficient testing practices.

Lastly, there was a discussion on the decision-making process when selecting mocking frameworks for testing ML code, with a user emphasizing the importance of determining the purpose of the test and the suitability of the chosen mocking framework.

### US Military pulls the trigger, uses AI to target air strikes

#### [Submission URL](https://www.theregister.com/2024/02/27/us_military_maven_ai_used/) | 59 points | by [cannibalXxx](https://news.ycombinator.com/user?id=cannibalXxx) | [57 comments](https://news.ycombinator.com/item?id=39535450)

The US military is harnessing the power of AI to pinpoint targets in air strikes conducted in Iraq and Syria. Although humans still make the final decisions, machine learning algorithms have assisted in over 85 airstrikes this year. This initiative, spearheaded by the US Department of Defense, aims to enhance operational efficiency and intelligence gathering. The use of AI in warfare is a controversial topic, with Google previously withdrawing from a similar project due to ethical concerns raised by its employees. Despite some challenges faced with AI recommendations for weapon selection and attack planning, the military emphasizes human oversight at every stage. As the US endeavors to stay ahead of its adversaries, the integration of AI in combat decisions is becoming increasingly crucial, with an emphasis on responsible adoption and addressing security risks. The deployment of AI technology in military operations signals a new era in modern warfare, blending human expertise with cutting-edge artificial intelligence capabilities.

The discussion on the submission revolving around the US military's use of AI in pinpointing targets during airstrikes covers a variety of perspectives and insights. Some users express concerns about the reliability of AI in target selection, highlighting the importance of human oversight. Others discuss specific incidents, such as the bombing of a hospital by the United States and the casualty rates in conflicts like Iraq and Ukraine. The conversation delves into ethical considerations, accountability for AI-driven decisions, and comparisons to historical events. Additionally, there are debates on the trustworthiness of certain sources like Wikipedia and the Lancet journal, as well as the potential risks and advantages of military applications of AI technology. The discussion also touches on technical details, the intersection of AI and big data in military operations, and references to popular culture like Skynet from the Terminator franchise.

### Europe probes Microsoft's €15M stake in AI upstart Mistral

#### [Submission URL](https://www.theregister.com/2024/02/28/eu_microsoft_mistral/) | 126 points | by [neverrroot](https://news.ycombinator.com/user?id=neverrroot) | [128 comments](https://news.ycombinator.com/item?id=39536824)

The European Commission is scrutinizing Microsoft's €15 million investment in French AI startup Mistral, following the release of a new large language model. Mistral, founded by ex-Google DeepMind and Meta researchers, has received significant funding and is valued at around €1.8 billion. The startup recently unveiled its latest models and partnered with Microsoft to bring them to Azure. While such deals are common, EU regulators are concerned about consolidation of power in the AI industry.

EU regulators are also focusing on undersea internet cable resilience, TikTok probe deepening, compliance with AI laws in the UK, and the legality of backdoored encryption. The European Commission is implementing the AI Act to enforce regulations on AI systems, particularly high-impact models like Mistral Large. These models will require disclosure of training data sources and safeguards against generating harmful content.

Stay tuned for updates from Microsoft and Mistral on this developing story.

The discussion on Hacker News about the EU's scrutiny of Microsoft's investment in Mistral, an AI startup, touched upon various aspects. Some key points include concerns about Microsoft's involvement potentially affecting Mistral's independence, comparisons with other AI startups, the importance of national security in EU regulations, restrictions on foreign investments in AI companies, and the complexities of international relations in the tech industry.

There were arguments about the implications for European startups if a Chinese company were to invest in them, the role of different governments in regulating AI investments, and discussions around the impact of previous trade wars and their potential influence on current situations.

Moreover, there were mentions of historical events, such as NATO alliances, US-presidential policies, and their relationships with Europe and Russia, highlighting the intricate interplay between politics, security, and investments in the tech space. The discussion also featured diverse perspectives on national defense, spending, and the balance of power in international affairs.

Overall, the conversation presented a nuanced view of the complexities surrounding AI investments, national security concerns, and geopolitical influences on the tech industry.

### Two Chinese Supercomputers (With Mystery Hardware Components) Go Online

#### [Submission URL](https://www.oodaloop.com/archive/2024/02/27/two-chinese-supercomputers-with-mystery-hardware-components-go-online/) | 24 points | by [koqoo](https://news.ycombinator.com/user?id=koqoo) | [3 comments](https://news.ycombinator.com/item?id=39539143)

In the latest from the world of supercomputing, the Top500 list highlights the U.S.'s Frontier system at the top spot, reaching exascale performance with a billion billion calculations. However, rumors suggest that China may have surpassed this but did not participate in the rankings. The new Aurora system by Intel in the U.S. enters at No. 2, with a promise to exceed Frontier when fully operational. Additionally, Chinese supercomputing capabilities are shrouded in secrecy, with reports of their new Tianhe Xingyi system boasting doubled performance using homegrown chips. The race for supercomputing supremacy continues to intrigue and provoke speculation in the tech world.

The discussion on the submission highlights different perspectives on China's strong position in supercomputing. User "Havoc" mentions how sanctions can impact technological developments in China, while user "mcphg" points out that China's use of homegrown chips could strengthen their independence in the industry, referencing the Tianhe-2 system and Intel Xeon chips. User "c_o_n_v_e_x" offers insights on Nvidia's financial results and the challenges around Singapore's control tower location, suggesting a shortage of rack space and high demand in Singapore leading to GPUs being shipped to China. This discussion emphasizes the complexity and competitiveness in the global supercomputing landscape.

### Google pays publishers to test AI tool that scrapes sites to craft new content

#### [Submission URL](https://www.adweek.com/media/google-paying-publishers-unreleased-gen-ai/) | 69 points | by [vincent_s](https://news.ycombinator.com/user?id=vincent_s) | [69 comments](https://news.ycombinator.com/item?id=39536645)

Google is making waves in the news world by offering select publishers access to a cutting-edge generative AI platform in exchange for feedback. The program, part of the Google News Initiative, aims to assist smaller publishers in creating quality content efficiently. However, concerns have been raised about the potential impact on original sources and the role of journalists in reporting the news. This bold move by Google is seen as a way to support publishers while also navigating ongoing scrutiny over its influence in the media landscape.

The discussion on the Hacker News submission revolves around various perspectives on Google's new AI platform for publishers. Some users express concerns about the quality of AI-generated content, emphasizing the importance of human touch in writing. Others highlight potential issues such as SEO optimization, commercial drive for low-quality content, and the impact on journalism. Additionally, there are discussions on strategies to protect online content from web crawlers and the evolving role of AI in content creation. Users also debate the value of AI-generated horoscopes, the legality of AI-generated news, and the balance between AI assistance and preserving human creativity in writing. Overall, the conversation touches upon the implications of AI in the media landscape and the evolving dynamics between technology and human creativity.