import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Wed Dec 24 2025 {{ 'date': '2025-12-24T17:09:47.097Z' }}

### Asterisk AI Voice Agent

#### [Submission URL](https://github.com/hkjarral/Asterisk-AI-Voice-Agent) | 159 points | by [akrulino](https://news.ycombinator.com/user?id=akrulino) | [83 comments](https://news.ycombinator.com/item?id=46380399)

Asterisk AI Voice Agent: open-source, realtime AI voice for Asterisk/FreePBX

What it is
- An MIT-licensed AI voice agent that plugs into Asterisk/FreePBX via RTP (ExternalMedia) and AudioSocket.
- Modular pipeline lets you mix and match STT, LLM, and TTS providers, or run privacy-first local pipelines.
- Ships with “golden baseline” configs validated for production, plus an Admin UI and CLI for setup and debugging.

Why it matters
- Brings modern barge-in, turn-taking, analytics, and tool integrations to existing PBX/call-center stacks without vendor lock-in (Docker, configurable providers, on-prem friendly).
- Supports both pipeline mode and “full agent” providers (e.g., Google, Deepgram, OpenAI, ElevenLabs) for native VAD/turn-taking.

What’s new in v4.5.3
- Call history and analytics: full transcripts, tool executions, errors; search/filter; export as CSV/JSON.
- Barge-in upgrades: instant interruption, platform flush, parity across RTP/AudioSocket.
- More models: Faster Whisper (GPU-accelerated STT), MeloTTS; hot-swap models from the dashboard.
- MCP tool integration: connect agents to external services via Model Context Protocol.
- RTP security hardening: endpoint pinning, allowlists, SSRC-based cross-talk prevention.
- Pipeline-first default: local_hybrid enabled by default; readiness probes reflect component health.

Getting started
- git clone, run preflight (creates .env and JWT_SECRET), docker compose up admin-ui, then ai-engine.
- Access Admin UI at http://localhost:3003 (default admin/admin), run the setup wizard.
- Add the generated dialplan to FreePBX (Stasis(asterisk-ai-voice-agent)) and verify health at http://localhost:15000/health.

Notes
- Works with both ExternalMedia RTP and AudioSocket; see the transport compatibility matrix in docs.
- Security: change the default password and restrict port 3003 in production.

Repo: https://github.com/hkjarral/Asterisk-AI-Voice-Agent

**Asterisk AI Voice Agent: open-source, realtime AI voice for Asterisk/FreePBX**
A new MIT-licensed AI voice agent brings modern features like barge-in (interruption handling), turn-taking, and analytics to existing Asterisk and FreePBX stacks. It supports a modular pipeline, allowing administrators to mix and match providers for Speech-to-Text (STT), LLMs, and Text-to-Speech (TTS), or run privacy-focused local pipelines using Docker. Version 4.5.3 introduces call history analytics, GPU-accelerated local models (Faster Whisper), and tooling integrations via the Model Context Protocol.

**Summary of Discussion on Hacker News:**

The discussion focused heavily on the user experience of AI phone systems, debating the trade-offs between efficiency, latency, and "human-like" interactions.

*   **Customer Service vs. Spam:** Opinions were split on whether this technology improves or degrades support. One user highlighted a dealership effectively using AI for appointment scheduling, which was preferable to sitting on hold. Others argued that these tools often ultimately serve to block access to human agents, citing frustrating loops with current support bots (like Amazon’s) and the potential for the technology to arm scammers with better automated tools.
*   **Latency Challenges:** A significant portion of the thread examined the "awkward silence" problem. While some users noted 2–3 second delays are still common, others argued that state-of-the-art systems (like OpenAI’s realtime API or Deepgram) are pushing latency below 500ms. User **numpad0** detailed technical strategies to mitigate this, such as pre-generating filler audio ("uh-huh"), streaming buffers, and using faster, specialized TTS models.
*   **The "Uncanny Valley" and Deception:** Several commenters emphasized that AI agents should not pretend to be human. Users expressed that while natural language processing is useful, the system should clearly identify itself as a machine. If an agent feigns humanity but fails at basic empathy or semantic understanding, it feels like a scam.
*   **Input Preferences:** There is still a strong preference among technical users for deterministic inputs. Many argued that "Pressing 1" or using a web form is superior to voice interactions, which can be difficult in noisy environments or frustrating when the AI hallucinates intent.
*   **Integration Complexity:** A few commenters touched on the difficulty of the backend work, noting that correlating Call Detail Records (CDRs) and recordings in legacy systems like Asterisk is surprisingly difficult, making a "bundled" dashboard highly valuable.

### Show HN: Vibium – Browser automation for AI and humans, by Selenium's creator

#### [Submission URL](https://github.com/VibiumDev/vibium) | 366 points | by [hugs](https://news.ycombinator.com/user?id=hugs) | [105 comments](https://news.ycombinator.com/item?id=46377597)

Vibium: a one-binary, zero-setup way to let AI agents drive a real browser

What it is
- An open-source browser automation stack built for AI agents and humans. A single Go binary (“Clicker,” ~10MB) manages Chrome’s lifecycle, proxies WebDriver BiDi over WebSocket, and exposes an MCP server so tools like Claude Code can control the browser with no manual setup. Apache-2.0 licensed.

Why it’s interesting
- Agent-first design: Native MCP integration means you can add full browser control to Claude Code with one command: claude mcp add vibium -- npx -y vibium.
- Zero drama setup: npm install vibium fetches the Clicker binary and automatically downloads Chrome for Testing to a user cache. No driver juggling.
- Modern protocol: Uses WebDriver BiDi rather than legacy CDP plumbing, with a built-in proxy on :9515.

What you get
- Clicker binary: Chrome detection/launch, BiDi proxy, MCP server over stdio, auto-wait for elements, PNG screenshots.
- JS/TS client: Simple sync and async APIs (go, find, click, type, screenshot, quit). Works via require, dynamic import, or ESM/TS.
- MCP tools out of the box: browser_launch, browser_navigate, browser_find, browser_click, browser_type, browser_screenshot, browser_quit.
- Platform support: Linux x64, macOS (Intel and Apple Silicon), Windows x64.
- Caching and control: Downloads live under a per-OS cache; set VIBIUM_SKIP_BROWSER_DOWNLOAD=1 if you manage browsers yourself.

How it compares
- Compared to Playwright/Puppeteer: similar end goal (drive a browser), but Vibium targets LLM agents and MCP workflows from the start, bundles the runtime into one binary, and speaks BiDi by default. Today it’s JS-first; Python/Java clients are on the roadmap.

Roadmap and status
- V1 focuses on core control via MCP and the JS client. Planned: Python/Java clients, a memory/navigation layer (“Cortex”), a recording extension (“Retina”), video recording, and AI-powered element locators.
- Recent updates: MCP server landed (Day 10), polish/error handling (Day 11), published to npm (Day 12).
- Repo traction: ~1.2k stars, 52 forks.

The takeaway
If you’ve struggled to glue agents to a real browser, Vibium’s “single binary + npm install” approach and native MCP tooling make it unusually frictionless to spin up reliable, BiDi-based automation for both agents and traditional testing.

**Summary of the Discussion**

The discussion on Hacker News was headlined by the project creator, Jason Huggins (`hgs`—creator of Selenium and Appium), engaging with a community heavily invested in Playwright.

**The Playwright Comparison**
The dominant theme was the comparison to Playwright. Many users expressed reluctance to switch, citing Playwright’s reliability, speed, and ability to eliminate the "flakiness" associated with older tools like Selenium.
*   **The Creator’s Take:** `hgs` acknowledged Playwright as the current "defacto standard" for developers. He positioned Vibium not as a Playwright killer, but as a bridge for the massive legacy Selenium userbase to enter the AI agent era.
*   **Agent-Native vs. Dev-Native:** While Playwright is "batteries included" for testing pipelines, Vibium aims to be "batteries included" for *agents* (bundling the browser, runtime, and MCP server in one binary).

**The "Sense-Think-Act" Vision**
When pressed by users on why Vibium is necessary when one could just wrap Playwright in MCP, `hgs` outlined a broader three-part vision:
*   **Act (V1):** The current release ("Clicker"), which handles execution.
*   **Sense (V2 - "Retina"):** A layer to record durable interaction signals and observe the world.
*   **Think (V2 - "Cortex"):** A navigation memory layer that builds a model of the workflow, so the LLM acts on a plan rather than reasoning about raw HTML from scratch.
He argued that while Playwright solves the "Act" portion perfectly, Vibium aims to build the missing "Sense" and "Think" layers required for robust robotic process automation.

**Technical Limitations & Features**
*   **Network Interception:** Users noted that Playwright excels at modifying network requests and mocking backends (crucial for testing). `hgs` confirmed Vibium currently lacks deep network interception/DOM injection capabilities but plans to extend in that direction.
*   **Simplicity:** Several users appreciated the ease of installation (`npm install` vs. complex driver setups), seeing value for quick agentic tasks where setting up a full E2E test suite environment is overkill.
*   **Competition:** Users mentioned other emerging tools in this space, such as Stagehand (Director AI) and DeepWalker (for mobile).

### AI Image Generators Default to the Same 12 Photo Styles, Study Finds

#### [Submission URL](https://gizmodo.com/ai-image-generators-default-to-the-same-12-photo-styles-study-finds-2000702012) | 14 points | by [donatzsky](https://news.ycombinator.com/user?id=donatzsky) | [3 comments](https://news.ycombinator.com/item?id=46380644)

AI image generators collapse into 12 “hotel art” styles, study finds

- What they did: Researchers (Hintze et al., in the journal Patterns) ran a “visual telephone” loop: Stable Diffusion XL generated an image from a short prompt; LLaVA described it; that description became the next prompt for SDXL. They repeated this 100 times, across 1,000 runs. They also tried swapping in other models.

- What happened: The image sequences almost always converged on one of just 12 generic motifs—think maritime lighthouses, formal interiors, urban nightscapes, rustic architecture. The original concept vanished quickly, and by ~turn 100 the style had coalesced. Extending to 1,000 turns produced variations, but still within those same motifs.

- Why it matters: It suggests strong “attractor” states and homogenization in generative pipelines—an echo of mode collapse—driven by model priors and dataset biases toward stock-like imagery. Even changing models didn’t break the trend. The authors dub the result “visual elevator music,” highlighting how easy copying style is compared to producing taste or originality.

- Takeaway for practitioners: Don’t expect open-ended creativity from iterative, model-to-model loops. To avoid sameness, you may need explicit style constraints, diversity objectives, strong negative prompts, or human-in-the-loop curation—otherwise the system drifts toward the same few safe, generic looks.

**Discussion Summary:**

Commenters split their focus between the study's methodology and the cultural implications of "visual elevator music."

*   **Critique of the "Loop":** Users argued the headline is somewhat misleading. They noted that the "mode collapse" results from the specific experimental design—feeding the output back into the input hundreds or thousands of times—rather than a flaw in a single generative prompt. One commenter wryly observed that this outcome is just a demonstration of standard "attractor dynamics."
*   **The "Sugar" Analogy:** Expanding on the paper's "elevator music" metaphor, discussion ventured into the philosophical. One user compared this hyper-optimized, generic imagery to refined sugar or a "crystalline substance"—concentrated and "shiny" enough to stimulate the senses, but ultimately devoid of nutritional substance or survival value in reality.

---

## AI Submissions for Tue Dec 23 2025 {{ 'date': '2025-12-23T17:09:23.040Z' }}

### Local AI is driving the biggest change in laptops in decades

#### [Submission URL](https://spectrum.ieee.org/ai-models-locally) | 238 points | by [barqawiz](https://news.ycombinator.com/user?id=barqawiz) | [235 comments](https://news.ycombinator.com/item?id=46360856)

IEEE Spectrum: Your Laptop Isn’t Ready for LLMs. That’s About to Change

Main idea: Cloud LLMs work, but latency, outages, and privacy push demand for on‑device AI. Today’s laptops mostly can’t handle it; the next wave of “AI PCs” is about fixing that.

Key points:
- Why current laptops struggle: Typical machines have 4–8 CPU cores, no dedicated GPU/NPU, and 16 GB RAM. Large models need massive memory; even small local models often drop features or quality. Image/video generation has been desktop‑tower territory.
- NPUs enter: Neural Processing Units are built for matrix multiplies, delivering far better performance per watt than GPUs for inference—crucial on battery. Expect NPUs to ship alongside CPUs as standard.
- The real bottleneck is memory: Capacity and bandwidth matter more than raw TOPS. Big context windows, multimodality, and advanced prompting/routing explode memory/IO needs. Quantization helps but trades off accuracy.
- Software has to catch up: Local runtimes must schedule work across CPU/GPU/NPU efficiently and support features like personalization and RAG without cloud round‑trips.
- This forces a laptop redesign: More/faster unified memory, better thermals, high‑bandwidth storage, and deeper AI acceleration on‑die—shedding legacy constraints from the pre‑AI PC era.

What to watch as a buyer: NPU perf (real INT8/FP16, not just marketing TOPS), 32–64 GB RAM, fast LPDDR, SSD bandwidth, and healthy local‑AI runtime support. Manage expectations: great for 3–7B models and on‑device assistants; trillion‑parameter giants remain a data‑center job—for now.

Why it matters: Privacy, offline reliability, and latency gains could make local AI the default for everyday tasks.

**Apple’s Absence and the Unified Memory Advantage**
A significant portion of the discussion criticized the article for overlooking Apple Silicon, which many users argue is currently the only viable platform for running LLMs on laptops.
*   **Unified Memory is King:** Commenters pointed out that Apple's Unified Memory Architecture allows laptops to run large models that discrete GPU laptops cannot handle without massive VRAM.
*   **Price Comparison:** While some users lamented the "Apple Tax" for high RAM configurations (e.g., $4500 for 128GB), others provided data showing that comparable PC workstations (HP ZBooks, ASUS ROG Flow) with similar RAM specs are often priced similarly or higher.

**Local Utility vs. Cloud Superiority**
Users debated whether running models locally is effectively useful or just a novelty.
*   **Use Cases:** Proponents cited coding assistants (reducing lag/privacy concerns), spam filtering (high accuracy on local data), and specific tasks like TTS/ASR as valid use cases.
*   **Limitations:** Skeptics noted that while 7B–30B parameter models run well on M-series chips, they still lack the reasoning capabilities of massive cloud models (Claude Opus, GPT-4). For complex business logic, cloud APIs are still preferred.
*   **Hardware Baseline:** There is a consensus that 8GB–16GB RAM is insufficient; 32GB–64GB is the "sweet spot" for usable local AI, with M-series Max/Ultra chips providing the best performance per watt.

**The Economic Case: Rent vs. Buy**
A philosophical debate emerged regarding the long-term economics of AI hardware.
*   **The VC Subsidy:** Some users argued that buying expensive hardware is unnecessary because cloud inference is cheap.
*   **The "Rug Pull" Theory:** Counter-arguments suggested that current cloud prices are artificially subsidized by venture capital. Users warned that once VC funding dries up, cloud providers will likely introduce ads, privacy invasions, or significant price hikes, making on-device hardware a verified hedge against "enshittification."

### Codex is a Slytherin, Claude is a Hufflepuff

#### [Submission URL](https://bits.logic.inc/p/codex-is-a-slytherin-claude-is-a) | 17 points | by [sgk284](https://news.ycombinator.com/user?id=sgk284) | [7 comments](https://news.ycombinator.com/item?id=46368858)

Logic’s engineers ran part one of the first 12 Advent of Code problems through four agents—Codex, Gemini, Claude, and Mistral—under minimal instructions, no assistance, no retries. All produced runnable solutions for all 12 in under 20 minutes, but none achieved a perfect score.

What stood out
- Codex vs Gemini: Nearly identical on speed, complexity, and lines of code. Key difference: Codex wrote zero comments and hit 11/12 accuracy; Gemini left 168 comments, often thinking out loud and debating edge cases mid-function.
- Claude: Slowest overall due to getting stuck on Day 12; drop that and its average complexity falls from 16.5 to 13.9. Style: clean headers, careful types and boundary checks—robust over rushed.
- Mistral: Classes everywhere—used OOP in every solution—often overengineering relative to the tasks.

Qualitative archetypes (per-solution classifier)
- Dominant vibe for most: Pragmatist.
- Codex: Wizard (clever, dense, minimal ceremony).
- Gemini: Professor (explanatory, stream-of-consciousness).
- Claude: Some Over-Engineer tendencies (linked to Day 12).
- Mistral: Over-Engineer through and through.

Hogwarts sorting (light-hearted, but telling)
- Codex → Slytherin: terse, goal-driven, efficient.
- Claude → Hufflepuff: patient, thorough, sturdy code.
- Gemini → Gryffindor: bold, talks it out, commits and moves.
- Mistral → Ravenclaw: theory-heavy, systems-first, sometimes at the expense of solving the task.

Does “personality” come from the model or the tooling?
- Re-running via Factory.ai’s Droid (swapping scaffolding) left Codex still firmly Slytherin—and even improved—suggesting the model drives most of the behavior, not just the wrapper.

It’s a cheeky, unscientific bake-off, but it surfaces real UX differences: Codex for terse accuracy, Claude for robust correctness, Gemini for transparent reasoning, and Mistral for architecture-minded code.

Discussion focused on the utility of qualitative "vibe-based" evaluations over standard benchmarks, with users suggesting that determining "character traits" is now more helpful than analyzing marginal percentage differences in performance.

*   **Claude:** Users praised the model as "pleasant" to communicate with, noting that it produces simple, understandable code and architectures that integrate well with existing frameworks like Django, often requiring less rewriting than competitors.
*   **Gemini:** Commenters validated the article’s observation regarding Gemini's specific quirks; one user expressed frustration with its "annoying habit" of including its reasoning process and self-corrections directly inside code comments. Others noted it tends to write disconnected scripts that ignore existing libraries.
*   **Codex/GPT:** Described as fast, conversational, and "clever," though some users felt it can be stubborn, occasionally ignoring specific instructions in favor of the practices embedded in its training data.
*   **The Analogy:** While some found the qualitative comparison refreshing, others dismissed the Harry Potter references, noting that model updates happen too frequently for these specific archetypes to hold long-term.

---

## AI Submissions for Sun Dec 21 2025 {{ 'date': '2025-12-21T17:10:20.277Z' }}

### A guide to local coding models

#### [Submission URL](https://www.aiforswes.com/p/you-dont-need-to-spend-100mo-on-claude) | 540 points | by [mpweiher](https://news.ycombinator.com/user?id=mpweiher) | [301 comments](https://news.ycombinator.com/item?id=46348329)

You don’t need $100/mo for Claude Code? Author tests, then retracts. Logan Thorneloe bought a 128 GB RAM MacBook Pro to see if beefy local models could replace paid coding copilots. After weeks of tinkering, he revises his claim: local LLMs are surprisingly strong (even some 7B models) and can cover ~90% of routine dev work, but the last 10%—reliability, edge cases, and production-critical tasks—still favors frontier cloud tools. Practical hurdles bit hardest: tool-use is flaky, thinking traces get stuck, dev workloads (Docker, etc.) eat RAM forcing smaller, weaker models, and latency/throughput trade-offs matter. His new stance: local models are excellent supplements that can lower costs or handle side projects, but they’re not a full replacement for professional coding assistants; free/cheap options like Google’s Gemini CLI further blunt the ROI. The post also includes a step-by-step local setup guide, memory-sizing advice, quantization tips, and model recommendations—plus a candid correction noting the original hypothesis overstated the case.

The discussion broadens the submission's hardware analysis into a debate on the economics, privacy, and workflows of cloud versus local AI assistants.

**Key themes include:**

*   **TCO and Latency vs. Cost:** Commenters argue that the "savings" of local models are often illusory. One user calculates the Total Cost of Ownership for a 4090 rig (depreciation + electricity) at roughly $733/year, noting that you only break even if you were spending over $61/month on cloud APIs. Furthermore, local models suffer from slower turnaround and smaller context windows (32k local vs. 200k+ cloud), making "whole codebase" reasoning difficult compared to the instant responses of models like Claude.
*   **Subscription Tier Strategies ($20 vs. $200):** Users debate the necessity of high-tier plans.
    *   **The "Pro" Regret:** Some users who purchased $100-$200/mo enterprise/team tiers reported using only ~10% of their quota, suggesting it is more cost-effective to "stack" multiple cheaper subscriptions (e.g., Gemini Advanced + ChatGPT Plus) rather than relying on a single expensive tier.
    *   **The "Vibecoding" Trap:** Conversely, others argue that $20/mo plans are insufficient for intense "vibecoding" sessions (iterative, whole-app generation), claiming they hit rate limits within 10–20 minutes when handling complex prompts or large contexts.
*   **Privacy and Google:** While Google's $20/mo Gemini plan is praised for its high token limits and value, users caution that its privacy policy regarding training on user data is less clear or protective compared to OpenAI and Anthropic's paid API/Team tiers.
*   **Micro-Optimizations & CLI Tools:** A technical sub-thread focuses on low-cost workflows, specifically piping `man` pages or documentation into simpler LLMs (using tools like `llm`, `pandoc`, or the OP’s tool `Pal`) to solve specific command-line problems without burning expensive "reasoning" tokens or dealing with cloud latency.

### How I protect my Forgejo instance from AI web crawlers

#### [Submission URL](https://her.esy.fun/posts/0031-how-i-protect-my-forgejo-instance-from-ai-web-crawlers/index.html) | 103 points | by [todsacerdoti](https://news.ycombinator.com/user?id=todsacerdoti) | [56 comments](https://news.ycombinator.com/item?id=46345205)

A tiny Nginx “cookie gate” to fend off crawlers killing a Forgejo instance

- The author’s self-hosted Forgejo was repeatedly taken down by bots hammering the web UI and API (ignoring robots.txt). Heavy solutions like Anubis felt overkill, so they built a minimalist Nginx gate.
- How it works: on first visit, if the request isn’t from git/git‑lfs and a specific cookie isn’t present, Nginx returns a 418 page with inline JS that sets a cookie and reloads. After that, traffic passes normally. Git clones over HTTP/HTTPS work via a user‑agent allowlist.
- Why it’s effective: most unsophisticated crawlers don’t execute JS, so they never get the cookie and move on. Real users barely notice a single reload. A 302 with Set-Cookie didn’t help; JS did.
- Trade-offs: requires JavaScript; trivial to bypass once targeted; not a security control. But it’s cheap, fast to deploy, and good enough to shed abusive bot load for now. The author suggests renaming the cookie for uniqueness and acknowledges that widespread use would prompt crawler adaptation.
- Backstory/refs: inspired by a Caddy-based “You don’t need Anubis” approach; this is the Nginx variant tailored for Forgejo. Potential future escalation paths include Anubis or iocaine if bots adapt.

Why it matters: Self-hosters need lightweight, user-friendly defenses. This pattern offers a pragmatic middle ground between robots.txt (ignored) and heavyweight bot walls—especially for sites where git clients must remain unaffected.

Here is a summary of the discussion:

**The "Infinite Graph" Problem**
Commenters pointed out that Git forges are uniquely vulnerable to crawlers not just due to volume, but due to structure. Users like *marginalia_nu* and *pdrzg* explained that naive crawlers often get trapped in the "infinite graph" of git history (commits, diffs, blame views, and archives). A simple breadth-first search by a bot can explode into millions of unique URLs, effectively DDoSing the server by forcing it to reconstruct historical file views.

**Alternative Defense Strategies**
While the Nginx cookie gate was praised for simplicity, users discussed several other approaches to bot management:
*   **ASN Blocking:** *nclrdg* shared a strategy of tracking request rates by subnet and blocking entire ASNs (specifically targeting hosting providers and budget VPSs rather than consumer ISPs) when thresholds are crossed.
*   **Honeypots & Port Blocking:** *BLKNSLVR* suggested using "UninvitedActivity," a method that bans IPs attempting to connect to closed ports or non-Cloudflare whitelisted IPs attempting to access port 443.
*   **Poisoning:** Discussion drifted toward hostile responses, with *jkwl* mentioning "Iocaine," a project designed to serve garbage data to poison AI training sets, and others joking about serving Markov chains to scrapers.

**The Nature of the Traffic**
There was debate regarding the source of the abuse. Some users reported aggressive traffic from specific regions (e.g., Singapore) or specific bots (Amazonbot ignoring `robots.txt`). Others, like *m0llusk*, suggested that the "hammering" might not be sophisticated AI companies, but rather students and "script kiddies" running basic scrapers with no rate limiting as they learn to code.

**Technological Trade-offs**
*srbntr* noted the obvious downside: this strictly requires JavaScript, breaking access for privacy-focused users or non-standard browsers. *flxgn*, the author of the original "You don't need Anubis" article that inspired the submission, appeared in the thread to express support for the Nginx logic implementation.

### Waymo halts service during S.F. blackout after causing traffic jams

#### [Submission URL](https://missionlocal.org/2025/12/sf-waymo-halts-service-blackout/) | 278 points | by [rwoll](https://news.ycombinator.com/user?id=rwoll) | [382 comments](https://news.ycombinator.com/item?id=46342412)

Waymo pauses SF robotaxi service after blackout strands cars at dark intersections; resumes Sunday

- A citywide power outage in San Francisco on Dec. 20 knocked out traffic lights and reportedly spotty cell service, leaving clusters of Waymo autonomous vehicles stalled at intersections and blocking lanes across multiple neighborhoods. Videos showed lines of AVs blinking hazard lights while human drivers crept around them.
- PG&E’s rolling blackouts affected roughly a third of the city (about 125,000 customers). In response, Waymo suspended ride-hailing Saturday evening, saying it was focused on rider safety and keeping routes clear for emergency crews.
- Observers speculated that dark signals confused the cars’ decision-making and that flaky connectivity may have hampered remote assistance.
- The incident underscores a key AV challenge: resilience during infrastructure failures (power, comms) and how robotaxis handle all-way-stop scenarios at scale.
- Update: Waymo said Sunday afternoon its fleet was back on the roads.

Here is a summary of the discussion on Hacker News:

**Intersection Behavior and Local Driving "Culture"**
A significant portion of the discussion analyzed why the vehicles failed, with an eyewitness (`cjsplt`) noting that while a dead traffic light legally becomes a 4-way stop, the practical reality in San Francisco is very different.
*   **The Problem:** While human drivers in SF handle these scenarios with "slow rolls" and aggressive "interleaving" (zipper merging), Waymo vehicles appeared to wait for massive, clear slots to move.
*   **The Deadlock:** As the AVs hesitated, human drivers eventually navigated around them to fill the gaps. This behavior seemingly confused the AVs further or triggered a failsafe mode, causing them to freeze entirely and lose their "turn" in the rotation.
*   **Laws vs. Norms:** Users debated the strict interpretation of traffic laws versus local customs. While the law requires a full stop and yield-to-right, users noted that in high-traffic urban environments, strict adherence effectively brings traffic to a halt, whereas the flexible human approach maintains flow.

**Resilience and "The Montreal Test"**
The incident sparked skepticism regarding the readiness of AVs for conditions outside of fair-weather California bubbles.
*   User `b112` argued that if the fleet cannot handle a simple blackout, they are nowhere near ready for winter climates like Montreal, where snow obscures lane markings, ice changes breaking distances, and slush covers sensors.
*   The critique highlights that current AV tech relies on "perfect conditions" and struggles when visual indications are obscured or when "rules" become ambiguous.

**Emergency Response and "The Big One"**
The failure raised alarm regarding disaster scenarios, specifically a major earthquake.
*   Users worried that if a simple power outage causes a mass "bricking" of AV fleets, a major disaster could see these vehicles blocking evacuation routes and impeding emergency services.
*   Some questioned why the "failsafe" seems to be freezing in place rather than pulling to the curb, noting that in a true emergency, this behavior could be catastrophic.

**General Sentiment**
*   **Post-Mortem:** Commenters speculated on the internal turmoil at Waymo, guessing that the company will have to tweak software to handle "self-induced congestion" better.
*   **Reputation:** While some viewed this as a necessary learning step for the AI, others felt it was a significant reputation hit, dispelling the illusion that the cars are currently "smarter" than human drivers in complex, unscripted scenarios.

### Measuring AI Ability to Complete Long Tasks

#### [Submission URL](https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/) | 235 points | by [spicypete](https://news.ycombinator.com/user?id=spicypete) | [186 comments](https://news.ycombinator.com/item?id=46342166)

METR: Measuring AI ability to finish long tasks is doubling every ~7 months

- What’s new: METR proposes a simple, comparable metric for agentic LLMs—the task-completion time horizon: the human time length of tasks a model can complete with a given success rate (e.g., 50% or 80%). Code and raw data are released; full paper linked in the post.

- Key results:
  - Over the past 6 years, the 50% success horizon has grown exponentially with a ~7‑month doubling time (roughly 1–4 doublings/year).
  - Today’s best generalist agents nearly always solve tasks humans do in <4 minutes, but succeed <10% on tasks taking humans >~4 hours.
  - Example: Claude 3.7 Sonnet’s 50% horizon is about one hour.
  - Extrapolation: If the trend holds for 2–4 more years, general agents could autonomously handle many week‑long software tasks; a 10× calibration error would shift that arrival by only ~2 years.

- Method in brief: Use diverse multi-step software and reasoning tasks; measure completion time by qualified humans; fit a logistic curve for model success vs human time; read off horizons at fixed success rates. Uncertainty is shown via hierarchical bootstrap confidence intervals.

- Why it matters: This helps reconcile superhuman benchmark scores with models’ inconsistent usefulness on real projects—current limitations stem less from knowledge gaps and more from long-horizon reliability and sequencing of actions.

- Caveats: Results depend on task mix and human baselines; trends persist across subsets but get noisier. “Autonomous” capability here is defined by the evaluated agent setup; details are in the paper.

**Discussion Summary:**

The discussion focuses on the trade-off between productivity and pedagogical depth when using AI agents for development.

*   **The "Struggle" vs. Efficiency:** The thread began with user *sbdvs* noting they implemented vector search in 15 minutes using AI (down from 4+ hours) but felt they "learned essentially nothing." This sparked a debate on the role of friction in learning. *throwaway613745* and *bhy* argued that "grunt work," debugging, and failed assumptions are necessary to build implicit "working knowledge"—comparing AI usage to "paint by numbers" versus learning to draw.
*   **High-Value vs. Low-Value Friction:** *smnw* (Simon Willison) countered that not all struggle is educational. He argued that AI removes "useless struggle" (like fighting syntax errors or YAML configurations) which often causes beginners to quit, allowing learners to focus on higher-level concepts. He cited the **Worked-example effect** (Wikipedia), suggesting that studying completed solutions can be more effective than solving problems from scratch.
*   **Compression of Experience:** *mms* described LLMs as "compression algorithms for experience," prompting a discussion on whether using agents prevents developers from gaining the intuition required to maintain systems.
*   **Technical Debt and "Vibe Coding":** *vsrg* and *Applejinx* warned that agentic coding leads to "vibe coding" (running code until it looks right) without understanding the underlying logic. They argued this generates massive, soul-crushing technical debt that future maintainers—lacking the "struggle" history of the code's creation—will be unable to fix.
*   **Skill Rot:** Several users voiced concerns that if entry-level "grunt work" is automated, junior practitioners will never acquire the deep expertise necessary to become senior architects (*dns_snek*).

### Get an AI code review in 10 seconds

#### [Submission URL](https://oldmanrahul.com/2025/12/19/ai-code-review-trick/) | 135 points | by [oldmanrahul](https://news.ycombinator.com/user?id=oldmanrahul) | [61 comments](https://news.ycombinator.com/item?id=46346391)

A simple trick is making the rounds: append .diff to any GitHub pull request URL, copy the raw diff, and paste it into an LLM (Claude, ChatGPT, etc.) with a short prompt like “please review.” No Copilot Enterprise, extensions, or special tooling required.

Why it matters:
- Fast first pass: catches obvious bugs, missing tests, and edge cases before involving teammates.
- Shorter cycle times: you show up to human review with a cleaner PR and fewer nits.
- Works anywhere: it’s just the diff text, so it’s tool-agnostic.

How to try:
- Take your PR URL, e.g., https://github.com/USER/REPO/pull/123
- Add .diff: https://github.com/USER/REPO/pull/123.diff
- Copy the raw diff, paste into your LLM, and ask for a review.

Caveats:
- Not a replacement for peer review.
- Be mindful of org policy and IP—avoid pasting private code into external LLMs unless approved or use an enterprise/on-prem model.
- Very large diffs may need chunking or summarization.

Here is a summary of the discussion:

While the submission proposes a lightweight hack for AI code reviews using GitHub's `.diff` endpoint, the discussion reveals that most developers find simple diffs insufficient for high-quality feedback. The conversation evolved into a debate on context management, the role of AI in the review lifecycle, and preventing notification fatigue.

**Context and Workflow**
*   **The context bottleneck:** Several users pointed out that raw diffs often lack the context required for an LLM to provide meaningful insights. **Smaug123** and **infl8ed** argued that checking out the full branch or concatenating touched files yields significantly better results than just the diff. **fwmr** suggested using `-U99999` to provide full-file context, as LLMs often struggle to differentiate between unchanged context and actual modifications in standard diffs.
*   **Tools vs. Tricks:** While the `.diff` trick is free, many users prefer integrated tooling like Cursor or bespoke scripts that fetch repository context. **dnlstr** noted that workflows utilizing tools like Claude inside Cursor can spot mismatched coding styles, missing docs, and bugs effectively, serving as a "pre-pass" before human eyes.
*   **Model selection:** Opinions varied on model necessity. **Smaug123** argued that high-end models (like "GPT-5/High") are competent reviewers worth the cost, while **vrdvrm** advocated for faster, cheaper models like Gemini-1.5-Flash for routine tasks.

**Philosophy: Augmentation vs. Replacement**
*   **Low-hanging fruit:** There is strong consensus that AI excels at catching "dumb mistakes," missing edge cases, and enforcing lint-like rules, allowing humans to focus on the big picture. **smnw** and **bllq** highlighted AI's ability to spot logical errors and unused variables that humans miss due to fatigue.
*   **Knowledge transfer:** **mvnbk** strongly opposed using AI as a *replacement*, arguing that code review is the primary mechanism for knowledge sharing and architectural alignment within a team—something an LLM cannot replicate.
*   **The "Linter" Approach:** **mvnbk** and others suggested that AI feedback should be treated as a local pre-commit check or a strict linter rather than a conversational PR participant.

**Friction and Fatigue**
*   **Notification spam:** **mrldd** complained about automated AI reviewers being too "chatty," flooding teams with notifications for nitpicky or incorrect suggestions (1 useful comment per 20).
*   **The "Lazy Reviewer" Anti-pattern:** **fssl** shared a horror story of a CTO who copy-pasted raw AI outputs into PRs, creating massive amounts of useless noise and eventually getting AI reviews banned at the company. However, they noted AI is still useful for helping juniors *draft* reviews to learn the tone and structure of good feedback.

### Clair Obscur having its Indie Game Game Of The Year award stripped due to AI use

#### [Submission URL](https://www.thegamer.com/clair-obscur-expedition-33-indie-game-awards-goty-stripped-ai-use/) | 176 points | by [anigbrowl](https://news.ycombinator.com/user?id=anigbrowl) | [395 comments](https://news.ycombinator.com/item?id=46342902)

Indie Game Awards yank Clair Obscur’s GOTY over gen‑AI use; Blue Prince promoted

- What happened: After airing a pre-recorded show naming Clair Obscur: Expedition 33 Game of the Year and Debut Game, the Indie Game Awards rescinded both wins hours later, citing the game’s confirmed use of generative AI art during production.
- Why the reversal: Sandfall Interactive had attested no gen‑AI was used when submitting. On the day of the IGA 2025 premiere, the studio confirmed gen‑AI art had been used (since patched out), which violates the IGAs’ “hard stance” against gen‑AI in nominations and ceremony.
- New winners: Blue Prince is now Game of the Year; Sorry We’re Closed takes Debut Game. Their acceptance speeches will be recorded and published after the holiday break, likely early 2026.
- Context: A wider industry debate is brewing over acceptable AI tooling. Larian’s Swen Vincke recently said no generative AI content for its next Divinity, but endorsed AI-assisted pipelines (e.g., mocap cleanup). A resurfaced July 2025 interview had Sandfall’s COO acknowledging “some AI” in production.
- Why it matters: This is a rare post-broadcast disqualification that sets a strict precedent on gen‑AI for at least one awards body. It also spotlights a gray zone between banned “gen‑AI art” and permitted non-generative production tools—and raises questions about verification and transparency in awards submissions.

Based on the discussion, the reaction is divided between those blaming the studio for dishonesty and those criticizing the awards body for technological zealotry.

**Key themes include:**

*   **Dishonesty vs. Technicality:** Several users argue the disqualification is justified not just because of the art, but because Sandfall representatives explicitly answered "No" to questions regarding AI use during development. These commenters feel that using AI for placeholders technically constitutes "development," treating the check-box attestation as a lie.
*   **The "Digital Amish" Critique:** A significant contingent finds the "blanket ban" on AI unreasonable, labeling the awards body as "Digital Amish." They argue that using AI for internal placeholders (like brick textures) or tedious tasks (like lip-syncing) is standard efficiency ("work-saving") and distinct from replacing creative human connection.
*   **The "Speed Limit" Analogy:** A sub-thread uses speeding tickets as a metaphor. Users debate whether the "leftover" AI texture was a punishable offense (strict liability) or a negligible margin of error (like driving 10% over the limit) that should be tolerated in a massive project.
*   **Defining the Gray Area:** Commenters highlight the difficulty in drawing a moral line between "Generative AI" and other algorithmic tools. Users point out that tools for smoothing lines, noise reduction, or procedural animation effectively "generate" data, asking why those are acceptable while placeholder textures are not.
*   **"Witch Hunt" Accusations:** Some users describe the investigation as a "witch hunt" targeting a legitimate game over a minor oversight, though others push back, arguing that "witch hunt" is a loaded term used to discredit valid enforcement of stated rules.