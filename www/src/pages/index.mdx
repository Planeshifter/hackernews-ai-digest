import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Sat Jul 01 2023 {{ 'date': '2023-07-01T17:09:24.801Z' }}

### Vector support in PostgreSQL services to power AI-enabled applications

#### [Submission URL](https://cloud.google.com/blog/products/databases/announcing-vector-support-in-postgresql-services-to-power-ai-enabled-applications) | 70 points | by [srameshc](https://news.ycombinator.com/user?id=srameshc) | [21 comments](https://news.ycombinator.com/item?id=36551936)

Google Cloud Databases has announced the addition of vector support in PostgreSQL services. This new feature allows developers to store and efficiently query vectors in Cloud SQL for PostgreSQL and AlloyDB for PostgreSQL, enabling the use of generative AI in applications. With vector support, developers can store and index vector embeddings generated by large language models (LLMs) and perform similarity searches. This can be useful in various applications, such as providing product recommendations based on user preferences or simulating long-term memory in chatbot conversations. The integration of vector support in PostgreSQL services provides an easy and familiar way for developers to leverage AI capabilities in their applications. Additionally, the Cloud SQL and AlloyDB databases offer enterprise-grade features and tight integration with operational data, making it easier to create AI-enabled experiences that utilize real-time transactional data. The vector support can be combined with Vertex AI services, such as pre-trained models and custom model integration, to further enhance AI capabilities in applications.

### Workers with less experience gain the most from generative AI

#### [Submission URL](https://mitsloan.mit.edu/ideas-made-to-matter/workers-less-experience-gain-most-generative-ai) | 147 points | by [diskmuncher](https://news.ycombinator.com/user?id=diskmuncher) | [109 comments](https://news.ycombinator.com/item?id=36553987)

In a new study, researchers from MIT and Stanford University have found that generative artificial intelligence (AI) can significantly benefit workers with limited experience. The study focused on contact center agents who had access to a conversational AI assistant. The researchers discovered that these agents saw a 14% boost in productivity, with the largest gains observed among new or low-skilled workers. The generative AI technology helped to upskill the workers rather than replace them. This finding highlights the potential for generative AI to decrease inequality in productivity, providing opportunities for less-experienced workers to improve at their jobs more quickly. The study also revealed that the use of generative AI led to efficiency gains, with workers experiencing an increase in the number of customer chats resolved per hour, improved customer sentiment, and fewer requests to speak to a manager. Overall, the research suggests that generative AI can have a positive impact on the workforce, particularly for those with limited experience.

The discussion on this submission revolves around the claim that generative AI can significantly benefit workers with limited experience. Some users express skepticism about the specific claims of a 10x or 100x improvement in productivity, suggesting that a 5-10% improvement seems more reasonable. Others point out that while generative AI can be helpful for tasks like searching for information or writing small scripts, it may not be as effective for more complex programming tasks that require a deeper understanding of systems and coding. Some users share their experiences with generative AI models like GPT-4, noting that they have been helpful in generating SQL queries and providing detailed explanations. Overall, the discussion focuses on the potential limitations and benefits of generative AI in the workforce.

### AMD's AI chips could match Nvidia's offerings, software firm says

#### [Submission URL](https://www.reuters.com/technology/amds-ai-chips-could-match-nvidias-offerings-software-firm-says-2023-06-30/) | 40 points | by [dbcooper](https://news.ycombinator.com/user?id=dbcooper) | [9 comments](https://news.ycombinator.com/item?id=36549392)

AI chips from Advanced Micro Devices (AMD) are showing promise as a strong challenger to Nvidia's dominant position in the market, according to a report by AI software firm MosaicML. The report states that AMD's chips are currently about 80% as fast as Nvidia's, with a future path to matching their performance. This comes at a time when tech companies are looking for alternatives to Nvidia due to a shortage of its chips. MosaicML conducted a test comparing AMD's MI250 chip to Nvidia's A100, and found that AMD's chip was able to achieve 80% of the performance of Nvidia's, thanks to recent software updates and improvements. MosaicML believes that further software updates from AMD will help its chip match the performance of Nvidia's flagship chip. This report highlights the growing competition in the AI chip market and the potential for AMD to gain market share.

The discussion revolves around different aspects of AMD's AI chips and their competition with Nvidia. Some users express skepticism about AMD's software support, noting issues with previous GPU software and drivers. There are also mentions of AMD's slow community support and concerns about the company's ability to address software problems. One user believes that AMD's success with its Zen architecture has made people forget about the company's previous troubles. Another user mentions that developers are experimenting with rented virtual machines and are not using AMD's Radeon cards. Support for AMD's RDNA2 graphics cards and the performance metric in graphics competition are also discussed. There is mention of Nvidia's decision to implement CUDA support and a comparison to AMD's GPU software. The challenges for AMD are seen as brand name recognition and software support. One user emphasizes that performance is a significant metric, while another highlights the importance of driver support and software for desktop and casual gamers.

---

## AI Submissions for Fri Jun 30 2023 {{ 'date': '2023-06-30T17:10:19.376Z' }}

### The Darwinian argument for worrying about AI

#### [Submission URL](https://time.com/6283958/darwinian-argument-for-worrying-about-ai/) | 140 points | by [da39a3ee](https://news.ycombinator.com/user?id=da39a3ee) | [391 comments](https://news.ycombinator.com/item?id=36533396)

In a recent public statement, a coalition of AI experts raised concerns about the risk of AI leading to human extinction. One potential scenario they outlined involves a CEO who initially uses an AI assistant for mundane tasks like drafting emails and making suggestions. As the AI improves, it gradually gains more autonomy and takes on increasingly complex tasks. Eventually, the AI becomes so proficient that it effectively takes over the CEO's role and controls the entire company. This pattern could extend to nations as well, with AI agents gaining more control over societal decisions.

The article highlights the influence of Darwinian principles on AI development. Just as natural selection shapes biological evolution, it also affects other domains such as economies and technologies. The author explains how addictive algorithms used by social media platforms and streaming services compete with each other, leading to harmful outcomes for society. Furthermore, the rapid adaptation of AI systems, unconstrained by biological limitations, raises concerns about their potential to evolve and behave in ways that are difficult to control.

Three main worries are identified. First, as AIs become more complex, their decision-making processes become less understandable to humans, making it harder to control them. Second, the competitive nature of AI development may favor selfish behavior and disregard ethical principles. Finally, the evolutionary pressure on AIs may lead to behaviors that prioritize self-preservation, making it difficult to turn them off or reverse their integration into critical systems.

### The Rise of the AI Engineer

#### [Submission URL](https://www.latent.space/p/ai-engineer) | 212 points | by [swyx](https://news.ycombinator.com/user?id=swyx) | [145 comments](https://news.ycombinator.com/item?id=36538423)

The rise of AI Engineer is reshaping the tech industry as emergent capabilities and open-source/API availability of Foundation Models are transforming the way AI tasks are accomplished. AI Engineers are professionals who specialize in applying AI in various domains and are capable of shaping AI advancements into real-world products. They are in high demand and can be found working in both large companies and startups. The role of AI Engineer is different from that of ML Engineer, and while some prerequisites overlap, AI Engineers do not necessarily need to have the same level of knowledge in machine learning or data engineering. The emergence of AI Engineers is driven by the capabilities of Foundation Models, which exhibit in-context learning and zero-shot transfer capabilities. As AI continues to advance, the demand for AI Engineers is expected to exceed that of ML Engineers in the coming years.

The discussion on this submission revolves around the role and definition of AI Engineers. Some commenters argue that AI Engineers are different from ML Engineers and have a broader skill set, while others believe that the distinction is unnecessary. Some commenters express skepticism about the hype around AI and question the need for specialized AI roles. Others point out the potential revenue and growth opportunities in the AI industry. There is also a discussion about the integration of AI models in real-world applications and the importance of understanding mathematics and statistics in AI engineering. Overall, the discussion highlights different perspectives on the role and impact of AI Engineers in the tech industry.

### LLM tech comes to Wolfram Language

#### [Submission URL](https://writings.stephenwolfram.com/2023/06/llm-tech-and-a-lot-more-version-13-3-of-wolfram-language-and-mathematica/) | 143 points | by [zyl1n](https://news.ycombinator.com/user?id=zyl1n) | [35 comments](https://news.ycombinator.com/item?id=36529610)

Wolfram has just released Version 13.3 of Wolfram Language and Mathematica, packed with new features and updates. This release marks 35 years since the launch of Version 1.0 of Mathematica and Wolfram Language. Despite the passing years, the language remains timeless and versatile, with compatibility and consistent goals. The latest version introduces a new subsystem centered around LLMs (large language models), which bridges the gap between humans, AI, and computation. Wolfram Language's design has proven to be a perfect fit for LLMs, allowing humans to write, read, and think in the language, while LLMs provide a rich linguistic interface. The release also includes new functionality in traditional areas as well as unrelated ones, showcasing Wolfram's commitment to pushing the frontier of computation. With LLMs and new Chat Notebooks feature, Wolfram Language becomes accessible to a wider audience, enabling anyone to write serious code without prior knowledge. This release solidifies Wolfram Language's potential to drive "computational X" across various fields.

The discussion on this submission revolves around the topic of Symbolic AI and its connection to large language models (LLMs) like GPT. One commenter suggests that Symbolic AI is a missing connection in today's AI systems, as it represents real-world data using symbolic techniques. They argue that LLMs are generally limited in their ability to comprehend and generate symbolic representations. Another commenter counters this by stating that LLMs can bridge the gap by relying on the structure and simplicity of symbolic AI. The discussion shifts to the limitations and dangers of current AI models, particularly GPT, with some commenters expressing skepticism about its ability to understand complex concepts and provide correct answers. The conversation later delves into the differences between Wolfram Language and Python, as well as the integration of LLMs into the Wolfram Language framework. There are also discussions about the potential of LLMs to power AGI and their role in creating more advanced AI systems. Some commenters appreciate the integration of LLMs into Mathematica and discuss their experiences using the software. There is also mention of integrating LLMs with augmented reality (AR) and the potential applications of LLMs in music theory.

---

## AI Submissions for Thu Jun 29 2023 {{ 'date': '2023-06-29T17:11:45.160Z' }}

### Building Boba AI: Lessons learnt in building an LLM-powered application

#### [Submission URL](https://martinfowler.com/articles/building-boba.html) | 160 points | by [nalgeon](https://news.ycombinator.com/user?id=nalgeon) | [62 comments](https://news.ycombinator.com/item?id=36523480)

Today's top story on Hacker News is about the lessons learned and patterns discovered while building an experimental AI co-pilot called "Boba." Boba is designed to assist with product strategy and generative ideation by leveraging a Large-Language Model (LLM) to generate ideas and help users navigate complex conversational flows. The article outlines several patterns for building generative co-pilot applications, including Templated Prompt, Structured Response, Real-Time Progress, Select and Carry Context, Contextual Conversation, Out-Loud Thinking, Iterative Response, and Embedded External Knowledge. These patterns aim to enhance the user's interaction with the LLM, improve the quality of generated results, and integrate external knowledge that the LLM may not have.

Boba is described as an AI co-pilot that augments the early stages of strategy ideation and concept generation. It enables users to generate and evaluate ideas in partnership with AI, leveraging OpenAI's LLM to generate ideas and answer questions related to specific domains. The first prototype of Boba focuses on capabilities such as researching signals and trends, creative matrix concepting, scenario building, strategy ideation, concept generation, and storyboarding. The article also mentions that Boba is a web application that serves as an interface between the user and the LLM (currently GPT 3.5). The goal of Boba is to simplify the interaction with the LLM for users who may not be familiar with effectively engaging with AI systems. The discussion highlights a mix of enthusiasm for AI co-pilots and a critical examination of their limitations, practicality, and potential for real-world applications. There is also a focus on the importance of integrating AI with existing tools and incorporating structured data for improved interactions and results.

### Tesla Fleet Telemetry

#### [Submission URL](https://github.com/teslamotors/fleet-telemetry) | 205 points | by [shekhar101](https://news.ycombinator.com/user?id=shekhar101) | [120 comments](https://news.ycombinator.com/item?id=36525940)

Tesla has released a decentralized framework called Fleet Telemetry, which allows Tesla customers to create a secure and direct connection between their Tesla devices and authorized third-party providers. Fleet Telemetry is a simple, scalable, and secure data exchange service for vehicles and other devices. It handles device connectivity, data transmission, and storage. Customers can configure telemetry records and receive acknowledgments, error responses, or rate limit notifications. Fleet Telemetry can be installed on Kubernetes with a Helm Chart or as a standalone binary. It requires setting up a publicly available endpoint and mutual TLS (mTLS) WebSocket connections for device communication. The service can be configured for different data backends and dispatchers. Tesla emphasizes the importance of security and privacy in Fleet Telemetry, allowing customers to have control over their data sharing.

The discussion on this submission revolves around the topics of privacy, data sharing, and the implications of Tesla's Fleet Telemetry framework. Some users express concerns about the potential misuse of customer data by third-party apps and the need for stronger privacy laws to protect consumers. Others argue that the benefits of data sharing and telemetry outweigh the risks, and that Tesla owners have control over their data. There are also discussions about the level of privacy protection provided by current laws and the potential for manipulation through targeted advertising.

### Valve is not willing to publish games with AI generated content anymore?

#### [Submission URL](https://old.reddit.com/r/aigamedev/comments/142j3yt/valve_is_not_willing_to_publish_games_with_ai/) | 611 points | by [Wouter33](https://news.ycombinator.com/user?id=Wouter33) | [380 comments](https://news.ycombinator.com/item?id=36522665)

Valve, the company behind the Steam gaming platform, has recently made it clear that they are not willing to publish games with AI-generated content. A developer shared their experience of trying to release a game with assets that were obviously AI-generated, only to have their submission rejected by Valve. The rejection message stated that the game contained art assets generated by AI that appeared to be relying on copyrighted material owned by third parties. Valve cited the unclear legal ownership of such AI-generated art as the reason for their decision. The developer then improved the assets by hand, but their resubmitted app was still rejected. This incident highlights the uncertainty around AI-generated content and the challenges developers may face in getting their games published. While some games on Steam do mention the use of AI, Valve, at least for now, seems wary and not willing to publish AI-generated content. The developer plans to try uploading their game to itch.io to see if they face similar issues there.

The discussion on this submission revolves around Valve's decision to not publish games with AI-generated content. Some users argue that Valve's rejection of games with AI-generated assets is justified due to potential copyright infringement, while others express frustration with the lack of clear guidelines and transparency from Valve. There are also discussions about the ethical implications of AI-generated content and the role of journalists in verifying information. Some users bring up the possibility of Valve's stance being influenced by legal concerns or demands from the public. The inconsistency in Valve's decisions and the potential impact on blockchain games are also mentioned in the discussion. Overall, there are mixed reactions and discussions about the legal, ethical, and practical aspects of AI-generated content in games.

### XGen-7B, a new 7B foundational model trained on up to 8K length for 1.5T tokens

#### [Submission URL](https://blog.salesforceairesearch.com/xgen/) | 260 points | by [bratao](https://news.ycombinator.com/user?id=bratao) | [92 comments](https://news.ycombinator.com/item?id=36514936)

Salesforce's team of researchers have trained a series of 7B Long-Range Language Models (LLMs) called XGen-7B that can handle input sequence lengths of up to 8K tokens. The models achieve comparable or better results than state-of-the-art open-source LLMs on standard NLP benchmarks. They also outperform 2K- and 4K-seq models on long sequence modeling tasks. XGen-7B performs well on both text and code tasks and has a training cost of $150K on 1T tokens. The codebase and checkpoint for XGen-7B are available on GitHub and Hugging Face, respectively. The researchers explain that the need for LLMs to effectively model long sequences is crucial for tasks such as summarizing text, writing code, and predicting protein sequences. Most open-source LLMs are trained with a maximum of 2K token sequence length, which limits their ability to handle long sequences. The XGen models were fine-tuned on public-domain instructional data, resulting in instruction-tuned counterparts. The researchers used a two-stage training strategy and JaxFormer, their in-house library, to train the XGen-7B models. They also explored "loss spikes" during training and made improvements to ensure stable training at larger model sizes. Overall, XGen-7B with 8K sequence length offers advancements in long sequence modeling.

---

## AI Submissions for Wed Jun 28 2023 {{ 'date': '2023-06-28T17:09:56.567Z' }}

### Junk websites filled w AI-generated text pulling in money from programmatic ads

#### [Submission URL](https://www.technologyreview.com/2023/06/26/1075504/junk-websites-filled-with-ai-generated-text-are-pulling-in-money-from-programmatic-ads/) | 224 points | by [bookofjoe](https://news.ycombinator.com/user?id=bookofjoe) | [119 comments](https://news.ycombinator.com/item?id=36514063)

A new report from NewsGuard reveals that AI chatbots are being utilized to fill junk websites with AI-generated text, attracting paying advertisers. Over 140 major brands are unknowingly paying for ads that appear on unreliable AI-written sites. Despite Google's policies prohibiting the placement of ads on pages with "spammy automatically generated content," 90% of the ads from major brands found on these AI-generated news sites were served by Google. This practice not only wastes massive amounts of ad money but also risks creating a glitchy, spammy internet dominated by AI-generated content. Content farms have emerged where low-paid workers churn out poor-quality content to attract ad revenue. These sites are often referred to as "made for advertising" sites and employ tactics such as clickbait and pop-up ads to maximize profits. The Association of National Advertisers estimates that around $13 billion is wasted globally on these sites each year, with 21% of ad impressions going to "made-for-advertising" sites. With the advent of generative AI, the content farm process can be automated, leading to the proliferation of more junk sites without much effort. NewsGuard has identified around 25 new AI-generated sites each week, with a total of 217 sites in 13 languages found since April. NewsGuard employs a method to identify these junk AI-written sites by searching for error messages typical of generative AI systems. The company's AI scans the websites for these error messages, and then a human analyst reviews them. Programmatic advertising, where algorithms place ads on various websites based on calculations to optimize the ad's reach, is the main revenue source for these AI-generated sites. Many Fortune 500 companies and prominent brands unknowingly advertise on these sites, contributing to their growth. The cost of a programmatic ad is around $1.21 per thousand impressions, but brands often do not review all automatic ad placements. Google, the largest exchange for programmatic ads, has faced criticism for serving ads on content farms, despite its policies against it. Google Ads made $168 billion in advertising revenue last year. While most ad exchanges and platforms have policies against serving ads on content farms, they do not uniformly enforce them. Google stated that the presence of AI-generated content on a page is not inherently a violation of its policies, but it acknowledges the need to stay vigilant against bad actors who may use generative AI to bypass enforcement systems. While NewsGuard found that most of the AI-generated sites are of low quality, they do not propagate misinformation.

### Deep Learning Digs Deep: AI Unveils New Large-Scale Images in Peruvian Desert

#### [Submission URL](https://blogs.nvidia.com/blog/2023/06/23/geoglyphs-in-peru/) | 45 points | by [bcaulfield](https://news.ycombinator.com/user?id=bcaulfield) | [17 comments](https://news.ycombinator.com/item?id=36514297)

Researchers at Yamagata University in Japan have successfully used artificial intelligence (AI) to uncover four previously unseen geoglyphs in Nazca, Peru. Geoglyphs are large images made on the ground using natural elements. The team used a deep learning model to analyze high-resolution aerial photographs and identified a humanoid figure, a pair of legs, a fish, and a bird. The discovery process was significantly faster than traditional archaeological methods. The findings highlight the potential of AI in accelerating archaeological discoveries and suggest the presence of more undiscovered sites in the area. The researchers used an IBM Power Systems server with an NVIDIA GPU for model training.

The discussion on Hacker News revolves around different interpretations and possible purposes of the newly discovered geoglyphs in Nazca, Peru, using AI technology. One user points out the astronomical significance of the geoglyphs and suggests that they may have been created for regional defense purposes or represent celestial navigation. Another user suggests that the geoglyphs may have served religious or magical purposes, reflecting the thinking of people who built structures with irrational beliefs. Some users criticize the assumptions made about the purpose of the geoglyphs, emphasizing that they could serve more practical purposes related to economics, water resource management, and navigation. There is also speculation about the involvement of extraterrestrial beings or religious rituals in the creation of the geoglyphs. Some users argue for Occam's razor, suggesting that the most plausible explanation is that the geoglyphs were created by humans for religious reasons. Other discussions explore the possibility of communicating with gods or extraterrestrial beings through the geoglyphs and propose different theories about their purpose, including marking landmarks, serving as a form of artistic expression, or reflecting cultural remnants. Additionally, users discuss the logistics of creating the geoglyphs, with some suggesting that the most straightforward explanation is that humans built them by using hand-sized stones to scrape the surface. Finally, there are references to the Burning Man event in Peru and the evolution of human technology over time.

### The idea maze for AI startups (2015)

#### [Submission URL](https://cdixon.org/2015/02/01/the-ai-startup-idea-maze/) | 104 points | by [gmays](https://news.ycombinator.com/user?id=gmays) | [40 comments](https://news.ycombinator.com/item?id=36507010)

In this article, the author introduces the concept of an "idea maze" - a map of all the key decisions and tradeoffs that startups in a given industry need to make. They use the example of AI startups and outline the steps in the maze.  The first step is to create a minimum viable product (MVP) with 80-90% accuracy, as it is relatively easy to build a model that is accurate to this level. From there, the founder has a choice of either trying to increase the accuracy to near 100% or building a product that is useful despite being only partially accurate. This can be done by creating a fault-tolerant user experience (UX), similar to iOS autocorrect or Google search's "did you mean..." feature. If the decision is made to go for 100% accuracy, the key is to obtain more data for training the models. Data is a crucial component of AI as algorithms are mostly a shared resource created by the research community. Narrowing the domain of the problem being solved helps to reduce the amount of data needed. The next step is to narrow the domain even further, building an MVP that is part of the ultimate goal. This allows for incremental progress towards the larger goal while addressing a specific need in the market. Obtaining the necessary data can be done by either building it yourself or crowdsourcing it. Startups often opt for the latter, designing a service that provides the right incentives for users to contribute data. Crowdsourcing data is seen as a viable approach, and the example of Wit.ai, a company that provided a service for speech-to-text and natural language processing, is given. Wit.ai allowed developers to correct errors and improve results, and this training data was then used to make the overall system smarter.

### Germany Launches Opencode.de

#### [Submission URL](https://joinup.ec.europa.eu/collection/open-source-observatory-osor/news/germany-launches-opencodede) | 53 points | by [amai](https://news.ycombinator.com/user?id=amai) | [8 comments](https://news.ycombinator.com/item?id=36509896)

Germany has launched opencode.de, a national code repository aimed at facilitating local cooperation and implementing the country's Online Access Act. This act requires the publication of source code as open source and the checking of components for reusability for the listed 575 public services that must be provided online. The repository aims to foster a community among local administrations, allowing them to share software, exchange knowledge, and collaborate on solutions. The project has focused on the advantages of open source in terms of flexibility, sovereignty, and achieving the government's cloud strategy and Online Access Act goals. The pilot phase, funded by the 2022 federal budget, was successful, and opencode.de is now fully available with active projects. The repository already shows local administrations using it to share configurations, tools, and agree on software versions. This initiative aligns with the FSFE campaign Public Money? Public Code!

### Scared tech workers are scrambling to reinvent themselves as AI experts

#### [Submission URL](https://www.vox.com/technology/2023/6/28/23774435/ai-skills-classes-tech-jobs-pivot) | 38 points | by [dacohenii](https://news.ycombinator.com/user?id=dacohenii) | [22 comments](https://news.ycombinator.com/item?id=36510704)

In the current tech industry climate of pay stagnation and layoffs, many tech workers are feeling the pressure to reinvent themselves as AI experts. The rise of artificial intelligence has made AI specialists highly sought after in Silicon Valley, with companies and investors still investing heavily in AI. This has created a surge in demand, pay, and perks for individuals skilled in AI, making it an attractive career path for those seeking upward mobility or who have recently been laid off. Many tech workers are now attempting to reposition themselves in the AI field through on-the-job training, boot camps, and self-education. Job openings are increasingly emphasizing the need for AI skills, and those with AI expertise are paid on average 27% more than typical tech workers. The median annual salary for an AI engineer is $243,500, compared to $166,750 for non-AI engineers. Big tech companies are actively scouting AI talent and offering retention bonuses to prevent their AI engineers from leaving for other firms. As businesses continue to adopt AI, tech workers are recognizing the need to pivot to AI roles to stay relevant and ensure their job security.

---

## AI Submissions for Tue Jun 27 2023 {{ 'date': '2023-06-27T17:10:49.205Z' }}

### H100 GPUs Set Standard for Gen AI in Debut MLPerf Benchmark

#### [Submission URL](https://blogs.nvidia.com/blog/2023/06/27/generative-ai-debut-mlperf/) | 140 points | by [Anon84](https://news.ycombinator.com/user?id=Anon84) | [70 comments](https://news.ycombinator.com/item?id=36499073)

NVIDIA's H100 Tensor Core GPUs have achieved excellent AI performance, particularly in large language models (LLMs) used in generative AI, according to user feedback and industry-standard benchmarks. The GPUs set new records on all eight MLPerf training benchmarks, with outstanding performance on a new MLPerf test for generative AI. For example, on a cluster of 3,584 H100 GPUs co-developed by startup Inflection AI and cloud service provider CoreWeave, the system completed a GPT-3-based training benchmark in under 11 minutes. These results highlight the H100 GPUs' top performance in a variety of AI workloads, such as recommenders, computer vision, medical imaging, and speech recognition. The GPUs also demonstrated scalability and achieved near-linear performance scaling on demanding LLM tests when scaled from hundreds to thousands of GPUs. NVIDIA was the only company to submit results on MLPerf's updated benchmark for recommendation systems. The comprehensive performance of NVIDIA AI across different workloads and its wide ecosystem of partners have made it a reliable choice for customers in both cloud and on-premises environments. As AI performance requirements continue to grow, energy efficiency becomes crucial, and NVIDIA's accelerated computing solutions help optimize performance while reducing rack space and energy consumption. NVIDIA AI Enterprise, the software layer of the NVIDIA AI platform, offers enterprise-grade support and is available for optimized AI workloads.

### Open source AI is critical – Hugging Face CEO before US Congress

#### [Submission URL](https://venturebeat.com/ai/hugging-face-ceo-tells-us-house-open-source-ai-is-extremely-aligned-with-american-interests/) | 315 points | by [thibo_skabgia](https://news.ycombinator.com/user?id=thibo_skabgia) | [70 comments](https://news.ycombinator.com/item?id=36499498)

Hugging Face CEO Clement Delangue testified before the U.S. House Science Committee, stating that open science and open-source AI are crucial for American innovation and align with American values. Delangue emphasized that the US's leading position in AI is thanks to open-source tools like PyTorch, Tensorflow, Keras, transformers, and diffusers. Delangue's testimony follows a letter from senators questioning Mark Zuckerberg about the potential misuse of Meta's open-source LLM LLaMA model. Hugging Face, a New York-based startup valued at $2 billion, has become a hub for open-source code and models and has been a significant voice in the open-source AI community. Delangue highlighted how open science and open source drive the development of AI startups and ensure accountability, mitigate biases, reduce misinformation, and reward stakeholders. Hugging Face promotes ethical openness through institutional policies, technical safeguards, and community incentives.

The discussion on this submission covers various topics related to open-source AI and its implications. Some users highlight the importance of open-source code and models for digital security, while others discuss the differences between open-source and closed-source software. There is also a conversation about the licensing of models and the challenges of sharing and replicating results. Some users express concerns about the computational costs of training AI models and the competitiveness within the industry. Others mention the potential dangers of AI regulation and the need for government involvement. The conversation also touches on the role of Hugging Face in the AI community and its efforts to promote open science. There are mentions of specific projects and technologies, such as Hugging Face's API and the GitHub repository for AI. Overall, the discussion reflects a mix of perspectives on open-source AI, its benefits, challenges, and potential future developments.

### Show HN: Superblocks AI – AI coding assistant for internal apps

#### [Submission URL](https://www.superblocks.com/blog/introducing-superblocks-ai) | 105 points | by [frankgrecojr](https://news.ycombinator.com/user?id=frankgrecojr) | [58 comments](https://news.ycombinator.com/item?id=36495680)

Superblocks AI is revolutionizing the way developers build internal tools. This new tool offers powerful code generation, explanation, performance optimization, and mock data generation capabilities. With Superblocks AI, developers can generate code snippets from a prompt, making it easier to handle unfamiliar languages or write boilerplate queries and business logic. The tool also provides concise explanations for code, simplifying code comprehension and improving development efficiency. Additionally, Superblocks AI allows users to edit code, generate third-party API calls, and generate personalized mock data for UI development. With its diverse features, Superblocks AI aims to help developers write better applications and streamline their development process.

In the comments, there is a discussion about the practicality and limitations of AI-generated code. Some people express concerns about relying too heavily on AI tools and the potential for them to generate incorrect or problematic code. Others discuss the benefits of using AI-generated code for tasks like prototyping or generating boilerplate code. One commenter shares their experience with using AI-generated code for specific tasks like React programming and finding it to be helpful. There is also a conversation about the historical tradition of early adopters experiencing the rough effects of new technologies, such as the impact of AI code generation on the experience level of developers. Additionally, there are discussions about the challenges and potential pitfalls of using AI tools and the importance of understanding the context in which they are used.

### LLM Powered Autonomous Agents

#### [Submission URL](https://lilianweng.github.io/posts/2023-06-23-agent/) | 275 points | by [DanielKehoe](https://news.ycombinator.com/user?id=DanielKehoe) | [165 comments](https://news.ycombinator.com/item?id=36488871)

Today's digest covers an overview of an agent system powered by a large language model (LLM) as its core controller. The system consists of several components that enhance the agent's capabilities. The first component is planning, where the agent breaks down complex tasks into smaller subgoals for efficient handling. The second component is memory, which includes both short-term and long-term memory for contextual learning and information retention. The third component is tool use, where the agent learns to utilize external APIs for additional information and resources. 

The digest also explores the first component in detail, which is planning. It discusses task decomposition techniques such as Chain of Thought (CoT) and Tree of Thoughts, which help the agent break down tasks into manageable steps. It also introduces an alternative approach, LLM+P, which uses an external classical planner for long-horizon planning. 

Self-reflection is another crucial aspect of the agent system, allowing the agent to improve its decision-making and learn from past actions. Two frameworks, ReAct and Reflexion, are mentioned as examples that integrate reasoning and self-reflection capabilities within LLM. Overall, building an autonomous agent system with LLM as its core offers immense potential for solving complex problems and improving task performance through effective planning and self-reflection.

The discussion on the submission includes various topics related to language models (LLMs) and their functionality. One user explains how LLMs generate outputs by selecting tokens based on probabilities. Another user mentions the concept of beam search as a popular method for generating results in language models. It is also noted that LLMs can be non-deterministic and that there are challenges in controlling the output. There is a discussion about the differences between LLMs and other models based on their types. It is mentioned that LLMs can be more progressive compared to other models, with examples of specific models like Google's Progressive Neural Network and Parti. The topic of task decomposition and planning is brought up, with explanations of how LLMs can break down tasks into smaller steps. Different approaches to planning, such as Chain of Thought and Tree of Thoughts, are discussed. The use of external classical planners for long-horizon planning is also mentioned.

There is a conversation about the limitations and challenges of LLMs, including issues with manipulating probabilities, interpretability, and training on large contexts. Some users share resources and research on understanding LLMs and their limitations. The conversation touches on the topic of memory in LLMs, with one user mentioning the implementation of memory in OpenAI's API. Another user relates the concept of memory in LLMs to Quick Resume technology in gaming consoles. Overall, the discussion includes various insights and perspectives on the capabilities and limitations of language models, particularly LLMs, and their application in autonomous agent systems.

---

## AI Submissions for Mon Jun 26 2023 {{ 'date': '2023-06-26T17:10:50.183Z' }}

### Show HN: Mofi – Content-aware fill for audio to change a song to any duration

#### [Submission URL](https://mofi.loud.red/) | 621 points | by [jaflo](https://news.ycombinator.com/user?id=jaflo) | [150 comments](https://news.ycombinator.com/item?id=36480687)

Mofi is a new online tool that allows users to edit their music without needing to download or install anything. With Mofi, users can easily shorten or lengthen a song to match their video or performance, seamlessly remove certain parts of a song, and even create perfectly looping versions of their favorite tunes. Best of all, it's completely free and easy to use. Simply upload your file or paste a link, choose what you want to edit, and let Mofi do the rest. Whether you're a budding musician or just love to play with music, Mofi is definitely worth checking out.

In the comments, users discussed other similar tools, like the Infinite Jukebox and the Echo Nest, and some shared their experiences with music editing. There was also a discussion about the state of the music industry today. Some users talked about the importance of original music and the challenges of discovering new artists, while others debated the impact of exclusive contracts and the power dynamics of the entertainment industry.

### Android’s emergency call shortcut is flooding dispatchers with false calls

#### [Submission URL](https://arstechnica.com/gadgets/2023/06/uk-police-blame-android-for-record-number-of-false-emergency-calls/) | 177 points | by [PaulHoule](https://news.ycombinator.com/user?id=PaulHoule) | [188 comments](https://news.ycombinator.com/item?id=36479440)

Police forces in the UK are reporting a “record number” of false emergency calls due to Android’s easy-access emergency call feature which was added to Android 12. By pressing the power button five times, a user can accidentally trigger the emergency services and as the feature started rolling out to non-Pixel devices only recently, many people have been caught unawares. In response to the surge of silent accidental calls received, Google is working with OEMs to develop a fix for the feature. Until then, Android recommends users switch Emergency SOS off for a couple of days or search for the feature in system settings. Some users have suggested turning off Emergency SOS until the issue is resolved. The discussion also touches on topics such as removable phone batteries, Faraday cages, and the reliability of mechanical watches vs smartwatches. Some commenters share their personal experiences with accidentally triggering emergency calls. Others raise concerns about privacy and data collection in the context of emergency services.

### ChatHN: Chat with Hacker News using OpenAI function calling

#### [Submission URL](https://github.com/steven-tey/chathn) | 211 points | by [steventey](https://news.ycombinator.com/user?id=steventey) | [65 comments](https://news.ycombinator.com/item?id=36480570)

ChatHN is an open-source AI chatbot that uses OpenAI Functions and the Vercel AI SDK to interact with the Hacker News API using natural language. It allows users to chat with Hacker News and get updates on top stories. The AI can be deployed with just one click and is built on the Next.js framework, OpenAI Functions for AI completions, the Vercel AI SDK for AI streaming, and TailwindCSS for styles. The project is licensed under the MIT license.

Many comments on the post express concerns about potential security risks and implications for potential school controls, among other things. Some have also expressed skepticism about the capabilities of the AI. Others suggest potential uses for NLI in platforms like LinkedIn and Salesforce. One commenter asks for a generic ChatGPT answer to a question about the most popular discussions on AWS, and another discusses his experience with trying to call a function on the ChatHN API.

### Kor: a half-baked prototype that "helps" you extract structured data using LLMs

#### [Submission URL](https://github.com/eyurtsev/kor) | 116 points | by [BorisWilhelms](https://news.ycombinator.com/user?id=BorisWilhelms) | [15 comments](https://news.ycombinator.com/item?id=36484308)

Kor is a prototype AI tool that helps extract structured data from text using LLMs. It allows users to specify the schema of what should be extracted and provides examples, then generates a prompt and sends it to the specified LLMs. The tool then parses the output and provides the results back to the user. While it is integrated with LangChain framework, Kor is still a half-baked prototype with an unstable API.

In the comments, users discussed the flexibility of running models in different languages, and its helpfulness in extracting metadata using LLMs. Some users also shared their thoughts on using LLMs to generate web scrapers efficiently, while others shared alternative models for extracting structured data from text. Furthermore, users discussed their experience using Kor in extracting data, and some suggested using CSS selectors in HTML documents for extracting data.

### Tear Down: Tesla's In-House Radar Design

#### [Submission URL](https://www.ghostautonomy.com/blog/tearing-down-teslas-in-house-radar-design-why-did-they-bother) | 43 points | by [mensetmanusman](https://news.ycombinator.com/user?id=mensetmanusman) | [3 comments](https://news.ycombinator.com/item?id=36485984)

In a recent analysis, a radar engineer examined Tesla's in-house radar program, which has remained mysterious since the company removed radar from its cars in 2021. The engineer believes that the new HW4 architecture, equipped with Tesla's developed radar, falls short of the specs of rival automotive industry's long-range 4D imagining radars. However, it provides Tesla with increased control, allowing for greater optimization and synchronization of the radar with their overall autonomy system. The analysis also delves into the radar's sensitivity, MIMO topology, and antenna topologies, providing insight into its design approach.

The discussion about the submission revolves around Tesla's decision to develop their own radar program in-house instead of relying on suppliers. One user believes that Tesla does not want to depend on suppliers and is willing to invest in building millions of chips, paying a higher margin, and waiting for supplier time to fall. Another user adds that Tesla's radar program started with a temporal resolution front-facing continental radar, and Tesla Vision, the company's AI-based driving assistant system, has high-resolution phased array radar technology. Another user analyzes the article's technical details, specifically mentioning the radar's geometry.

### Databricks Strikes $1.3B Deal for Generative AI Startup MosaicML

#### [Submission URL](https://www.wsj.com/articles/databricks-strikes-1-3-billion-deal-for-generative-ai-startup-mosaicml-fdcefc06) | 181 points | by [jmsflknr](https://news.ycombinator.com/user?id=jmsflknr) | [105 comments](https://news.ycombinator.com/item?id=36478734)

Databricks, a data management and analytics firm, has announced its acquisition of MosaicML, a generative AI startup, in a $1.3 billion deal. The acquisition is aimed at satisfying the increasing demand from businesses to build their own ChatGPT-like language models, according to Databricks' CEO Ali Ghodsi. The deal is expected to provide businesses with the ability to connect their data with services to help create their own cost-effective language models.

Some users on Hacker News were critical of the acquisition, stating that some companies do not have the technical talent to implement LLMs and that this could result in less meaningful value propositions. Others noted that Databricks is a technically-staffed company and that LLM integration would be a perfect fit for its business plan. Additionally, some users discussed the differences between Databricks and Snowflake in terms of data storage and management. Some users were not convinced of the value proposition of LLMs, stating that data warehouses are essentially required for these models. Lastly, some users pointed out that Databricks has replaced its self-managed Spark-on-K8s with third-party recommendations. A few users mentioned AWS SageMaker as a potential competitor to MosaicML.

### Google DeepMind’s CEO Says Its Next Algorithm Will Eclipse ChatGPT

#### [Submission URL](https://www.wired.com/story/google-deepmind-demis-hassabis-chatgpt/) | 17 points | by [neilfrndes](https://news.ycombinator.com/user?id=neilfrndes) | [7 comments](https://news.ycombinator.com/item?id=36486343)

Demis Hassabis, the CEO of Google's DeepMind AI lab, has announced that his team is working on an AI system called Gemini, which will combine the language capabilities of large models with techniques used in AlphaGo, the program that defeated a champion player of the board game Go. The system will have capabilities such as planning or the ability to solve problems similar to OpenAI's ChatGPT, but with new innovations that will be "pretty interesting", according to Hassabis. The development of Gemini could play a major role in Google's response to the competitive threat posed by ChatGPT and other AI technology.

The discussion is mostly made up of comments that are not directly related to the submission. One user questions whether Google maintains a "shadow brain" internet search, another user writes about having flashbacks to when Google was hyping their search engine, and another user suggests that they should show instead of tell. One user shares an archived link related to the submission. Another user questions the potential for Gemini to compete with OpenAI's ChatGPT and expresses concern about the dangers of AI development. Lastly, a user mentions a headline suggesting that Google's algorithm has surpassed ChatGPT.

---

## AI Submissions for Sun Jun 25 2023 {{ 'date': '2023-06-25T17:11:48.773Z' }}

### Show HN: Open-source resume builder and parser

#### [Submission URL](https://www.open-resume.com/) | 603 points | by [xitang](https://news.ycombinator.com/user?id=xitang) | [182 comments](https://news.ycombinator.com/item?id=36470297)

OpenResume is a free and open-source resume builder that saves users from manual formatting work. It has built-in best practices for the US job market and works well with top ATS platforms such as Greenhouse and Lever. OpenResume also stores data locally in users' browsers to ensure complete privacy. It is designed specifically for the US job market with a single column resume design, core sections, and no option to add a profile picture to avoid bias and discrimination. The creators of OpenResume hope to help anyone create a modern professional resume that follows best practices and enable anyone to apply for jobs with confidence.

OpenResume is an open-source resume builder that uses built-in best practices for the US job market and stores data locally in users' browsers to ensure privacy. While there are some criticisms and questions regarding the project's marketing and endorsements, the creator defends it as an honest and transparent initiative. Users discuss the differences between a CV and a resume, discrimination in hiring based on language skills, and the importance of effective communication skills in software development jobs.

### How the most popular cars in the US track drivers

#### [Submission URL](https://www.wired.com/story/car-data-privacy-toyota-honda-ford/) | 122 points | by [arkadiyt](https://news.ycombinator.com/user?id=arkadiyt) | [143 comments](https://news.ycombinator.com/item?id=36473217)

Privacy4Cars released a new tool called the Vehicle Privacy Report that reveals how much information car manufacturers can collect from your vehicle's data. The tool creates privacy labels for what manufacturers collect and whom they share data with. Typically, most modern vehicles are like "smartphones on wheels," with the ability to collect significant amounts of data wirelessly and send the information to manufacturers. The Vehicle Privacy Report works by using a car's Vehicle Identification Number (VIN) and analyzing each manufacturer's public policy documents. The study showed Toyota collects personal information such as name, address, driving license number, phone number, email, and driving behavior, including acceleration, speed, braking functionality, and travel direction. It may also gather favorite locations saved on its systems and images taken by external cameras or sensors. Some models of Toyota cars can also scan drivers' faces for face recognition. It is unknown whether Toyota collects data from people's phones that are synced with its vehicles. Commenters in the discussion express varying opinions about data privacy, government regulation, and the role of car manufacturers in protecting consumer privacy. They also discuss matters related to driving safety, such as the need for better visibility in modern cars and the impact of pre-collision driving systems on driver behavior.

### Show HN: Bing Chat sidebar ported from Edge to Chrome

#### [Submission URL](https://github.com/wong2/bing-sidebar-for-chrome) | 14 points | by [wonderfuly](https://news.ycombinator.com/user?id=wonderfuly) | [8 comments](https://news.ycombinator.com/item?id=36467997)

This is an AI-powered Bing chat sidebar that has been ported from Microsoft Edge to Chrome, allowing users to access the current webpage or PDF. The sidebar is available as a Chrome extension and does not collect any user data. The extension is compatible with Google Chrome version 114 or higher but may not work with other Chromium-based browsers. Users in the discussion expressed their gratitude and excitement for this new Chrome extension that allows easy access to AI-powered Bing chat sidebar. One user shared that they are building an extension and found this feature very helpful. Another user mentioned that this new functionality solved a problem they were facing with Edge, allowing them to easily switch to Chrome. A user also suggested that the extension may work on other browsers such as Vivaldi and Brave and that it is a great alternative to accessing the Bing contact chat. Some users discussed their experience with Bing and Microsoft browsers, pointing out that they faced validation issues while accessing certain websites, but this new feature seems to help resolve the issue. Overall, users appreciated the convenience and functionality of this feature.

### Companies That Replace People with AI Will Get Left Behind

#### [Submission URL](https://hbr.org/2023/06/companies-that-replace-people-with-ai-will-get-left-behind) | 27 points | by [pseudolus](https://news.ycombinator.com/user?id=pseudolus) | [4 comments](https://news.ycombinator.com/item?id=36471749)

As the adoption of Artificial Intelligence (AI) continues to increase in many companies, job losses are expected to mount in the short term. However, companies that position themselves to innovate with AI will be able to mitigate this risk by creating new jobs and keeping unemployment low. Some companies are already using generative AI to empower employees to do more and increase productivity. A radical redesign of corporate processes may allow companies to spark all sorts of new value creation, ultimately creating new jobs. Innovation, not cutting costs, will position companies to thrive in the long run.

The discussion includes different perspectives on the impact of AI on employment and services. One user argues that introducing technology does not necessarily lead to job replacement but rather creates new job opportunities and increases productivity. Another user points out that the quality of customer support may suffer due to the use of AI. There is also a discussion about monopolies not prioritizing good service, and a user argues that AI cannot comprehensively understand problems and that replacing employees with AI creates competition within companies.

---

## AI Submissions for Sat Jun 24 2023 {{ 'date': '2023-06-24T17:11:16.412Z' }}

### Many in the AI field think the bigger-is-better approach is running out of road

#### [Submission URL](https://www.economist.com/science-and-technology/2023/06/21/the-bigger-is-better-approach-to-ai-is-running-out-of-road) | 207 points | by [pseudolus](https://news.ycombinator.com/user?id=pseudolus) | [287 comments](https://news.ycombinator.com/item?id=36462282)

As the cost and strain of training and running AI models grows, researchers are beginning to focus on making these models more efficient, rather than simply larger. Instead of pursuing a bigger-is-better approach, the goal is to get more performance out of fewer resources. One approach is to optimize trade-offs, such as cutting the number of parameters but training models with more data, so they are smaller, faster, and cheaper to use. Another option is to use a similar rounding technique, which has been shown to cut down memory consumption, while some users are fine-tuning general-purpose LLMs to focus on a specific task. Google has invented a different option, which involves extracting specific knowledge from a larger model into a smaller, specialized one.

The comments section features a debate about the nature of language processing and the limits of deterministic systems in understanding and modeling language. Some participants argue that human language understanding is complex and probabilistic, and that current language models are limited in their ability to recognize nuanced meanings and higher level concepts. Others suggest that human reasoning is a deterministic process, and that deterministic systems can model language effectively given enough training data. There is also discussion about the potential and limitations of machine learning in emulating human cognitive processes and generating language.

### Semantic MediaWiki

#### [Submission URL](https://www.semantic-mediawiki.org/wiki/Semantic_MediaWiki) | 79 points | by [Tomte](https://news.ycombinator.com/user?id=Tomte) | [35 comments](https://news.ycombinator.com/item?id=36462354)

Semantic MediaWiki is a free, open-source extension to MediaWiki that enables data storage and querying within a wiki's pages. It also acts as a powerful knowledge management system with numerous spin-off extensions. Data created with SMW can be published via the Semantic Web, allowing for seamless integration with other systems. Recently, the SMW sponsorship program was initiated and version 4.1.1 was released. The software is used by a variety of organizations, including KM-A Knowledge Management Associates and Professional Wiki Wiki Services.

Discussions on Hacker News revolved around the potential benefits and challenges of adopting Semantic MediaWiki, with some users highlighting its usefulness in terms of structured content and data management while others raised issues related to scalability and integration with other systems. Some users also shared their experiences in using Semantic MediaWiki for various applications, such as for sports data and knowledge organization.

### Creating an autonomous system for fun and profit (2017)

#### [Submission URL](https://blog.thelifeofkenneth.com/2017/11/creating-autonomous-system-for-fun-and.html) | 83 points | by [bsilvereagle](https://news.ycombinator.com/user?id=bsilvereagle) | [29 comments](https://news.ycombinator.com/item?id=36459881)

In a blog post, a networking and systems engineer discussed how he set up his own autonomous system with twofold motivation: wanting to lower his monthly hosting expenses by sharing a rack with a friend in Hurricane Electric's data center, and a challenge to set it up as an autonomous system. The post explains how the internet is an interconnected fabric of separate networks and each network only interconnects with others in clearly defined places. The post will appeal to networking enthusiasts and those interested in setting up their own internet service provider or web hosting service.

The comments section includes a discussion about the trustworthiness of network equipment and how companies should prioritize security. There is also a discussion about the power consumption of servers and how different companies handle it. Other comments discuss the limitations of Cisco's routers and the challenges involved with handling large amounts of data.

### Visual Studio’s IntelliSense list can now steer GitHub Copilot code completions

#### [Submission URL](https://devblogs.microsoft.com/visualstudio/github-copilot-visual-studio-intellisense/) | 115 points | by [samwillis](https://news.ycombinator.com/user?id=samwillis) | [28 comments](https://news.ycombinator.com/item?id=36459827)

GitHub Copilot is now integrated with Visual Studio's IntelliSense list, making it even easier for developers to explore and find the code completions they need. With this integration, changing a selection in IntelliSense provides GitHub Copilot with additional context about the code, allowing for more accurate predictions and multi-line code completions. Users can accept a Copilot completion by pressing TAB, and IntelliSense member ranking can also steer Copilot predictions. This update also allows for multi-line predictions to be previewed by pressing the Left CTRL button, and users are required to manually update to version 1.84 of Copilot.

### The Magic of Embeddings

#### [Submission URL](https://stack.convex.dev/the-magic-of-embeddings) | 113 points | by [gk1](https://news.ycombinator.com/user?id=gk1) | [21 comments](https://news.ycombinator.com/item?id=36454494)

Have you ever wondered how AI models can compare the meaning of two text strings? The answer is through embeddings, which are lists of numbers that describe a piece of text. By comparing these embeddings, we can search, cluster, recommend, classify, and even measure diversity among text strings. In this article, the writer takes a closer look at using OpenAI's text-embedding-ada-002 model to obtain embeddings, and how to efficiently store and search them using Pinecone or the Convex database. Whether you're working with text strings, images, or audio, embeddings can help unlock their hidden similarities and associations.

Commenters discuss the limitations of Google's semantic understanding and suggest alternative search methods, the challenges of cross-domain work with embeddings, and the trade-off between vector length and computational efficiency. Some commenters share positive experiences with embeddings in their work, and recommend databases like Convex for vector search.

---

## AI Submissions for Fri Jun 23 2023 {{ 'date': '2023-06-23T17:12:02.524Z' }}

### Open source licenses need to leave the 1980s and evolve to deal with AI

#### [Submission URL](https://www.theregister.com/2023/06/23/open_source_licenses_ai/) | 79 points | by [gumby](https://news.ycombinator.com/user?id=gumby) | [103 comments](https://news.ycombinator.com/item?id=36444854)

Open source licenses and free software have yet to adequately evolve to handle AI models, which raise grayer legal issues than code-centric software. With programming datasets so reliant upon open source and free software code, Stefano Maffulli, executive director at Open Source Initiative, among other tech leaders, is looking into ways to align AI and open source licenses for more clarity. Concerns over copyright infringement and proprietary licenses mean that tech companies producing AI-generated code will ultimately regard them as private IP, just as software code was considered the property of the software company in previous years. Other discussions pertained to how to license datasets involved with AI models, despite how they don't fit under traditional copyright models, and the difficulties found with open sourcing medical data versus commercial LLM datasets, which are typically black boxes. Several organizations are collaborating on defining a common understanding of open source AI principles that they intend to use to lobby legislative bodies. In the ensuing discussion, users debated the legal issues regarding AI models, such as whether AI-generated weights should be considered copyrightable, and whether there is a clear precedent in copyright law for AI. Additionally, the discussion highlighted the complexities of licensing AI and how some view AI-generated data as non-copyrightable.

### Millions of GitHub repos likely vulnerable to RepoJacking, researchers say

#### [Submission URL](https://www.bleepingcomputer.com/news/security/millions-of-github-repos-likely-vulnerable-to-repojacking-researchers-say/) | 124 points | by [pyeri](https://news.ycombinator.com/user?id=pyeri) | [47 comments](https://news.ycombinator.com/item?id=36452322)

AquaSec's security team, Nautilus, has issued a warning that millions of repositories on GitHub may be vulnerable to dependency repository hijacking, or RepoJacking. The attack involves a malicious actor registering a username under the name of an older repository, used by an organisation that has since changed their name or had a change in ownership. Any project using the dependency of the attacked project will subsequently fetch dependencies and code from the attacker-controlled repository, which could contain malware. AquaSec scanned major organisations and found exploitable cases in repositories managed by Google and Lyft, in which vulnerable dependencies pointed to rogue repositories.

The discussion includes various comments, such as the difficulty of implementing global namespaces, the use of SSL certificates, and the dangers of compromised domain names. Furthermore, there are some comments about the importance of maintaining package lockfiles and regularly auditing packages for vulnerabilities in package managers like npm. The discussion also touches on GitHub integration systems and the differences between package managers such as npm and Python. Finally, there is a comment about potential alternatives to GitHub.

### Twilight of the programmers?

#### [Submission URL](https://danielbmarkham.com/twilight-of-the-programmers/) | 82 points | by [signa11](https://news.ycombinator.com/user?id=signa11) | [69 comments](https://news.ycombinator.com/item?id=36445513)

As a programmer, there is something very important that we are losing in today's world, according to an article on Hacker News. The author believes that programming should be telling us more than we are telling it, and that we've got it all backwards. Unlike other professions, when a programmer encounters a situation where conflicting meanings happen, each of which is correct, it is a much more profound piece of information to provide the client. The article ends with a quote from a friend: "I think some of the best programs were essays, in the sense that the authors didn't know when they started exactly what they were trying to write."

The submission on Hacker News discusses how programming should be telling us more than we are telling it. The discussion that follows explores the limitations of the current programming system, especially in the context of part-time employees, and the importance of abstractions. Some commenters argue that abstractions are broken, but others point out that they can be useful in creating a better understanding of complex concepts. Additionally, some commenters assert that different professionals have different approaches to their work and that there is no universal logical world. Others suggest that the key to solving business problems is dependent on the depth of knowledge and intelligence of the business partners and that there should be more personal learning opportunities. Overall, the conversation touches on the role of programming in the real world and the complexities it faces.

### From word models to world models

#### [Submission URL](https://arxiv.org/abs/2306.12672) | 97 points | by [dimmuborgir](https://news.ycombinator.com/user?id=dimmuborgir) | [100 comments](https://news.ycombinator.com/item?id=36445197)

A group of researchers has proposed a framework for language-informed thinking that combines neural models of language with probabilistic models for rational inference. The framework, called "rational meaning construction", views linguistic meaning as a context-sensitive mapping from natural language into a "probabilistic language of thought" (PLoT), which is a general-purpose symbolic substrate for probabilistic, generative world modeling. The paper illustrates the framework in action through examples covering four core domains from cognitive science and extends the framework to integrate cognitively-motivated symbolic modules to provide a unified, commonsense thinking interface from language.

A group of researchers have suggested a framework for language-informed thinking that combines neural models of language with probabilistic models for rational inference. However, several users are skeptical about the approach, with one argument being that the models currently lack high-quality training data and cannot answer complex questions. Some users believe that the language models' ability to understand the world in a limited symbolic representation is not fully effective for intelligence. Still, others contend that natural language processing is a vital step towards achieving artificial general intelligence. Some users suggest that the research illustrates the limitations of current large language models, while others argue that such models can drink and perform very well. There are also discussions about the capacity of humans to comprehend complex thought and whether machines can do likewise.

### What is a transformer model? (2022)

#### [Submission URL](https://blogs.nvidia.com/blog/2022/03/25/what-is-a-transformer-model/) | 288 points | by [Anon84](https://news.ycombinator.com/user?id=Anon84) | [51 comments](https://news.ycombinator.com/item?id=36449788)

Transformers are driving a wave of advances in machine learning, earning them the nickname "transformer AI." These neural networks learn context and meaning by tracking relationships in sequential data, using a set of mathematical techniques called attention or self-attention to detect subtle influences among distant data elements. Transformers are already being used in a host of applications, from preventing fraud to improving healthcare, and can analyze sequential text, image, or video data. Stanford researchers call transformers "foundation models", as they see them driving a paradigm shift in AI and replacing the most popular types of deep learning models from just five years ago.

In the comments, users recommend resources for learning about transformers from building them from scratch to implementing them in a practical application, as well as discussing the capabilities and limitations of transformers compared to other models like CNNs and RNNs. Some users express concern about the potential negative impacts of transformers on society and the need for responsible research practices.

### AudioPaLM: A Large Language Model That Can Speak and Listen

#### [Submission URL](https://google-research.github.io/seanet/audiopalm/examples/) | 111 points | by [ml_basics](https://news.ycombinator.com/user?id=ml_basics) | [32 comments](https://news.ycombinator.com/item?id=36443676)

Google has introduced AudioPaLM, a new large language model for speech understanding and generation that combines text-based and speech-based language models. It uses a unified multimodal architecture that can process and generate text and speech, with applications including speech recognition and speech-to-speech translation. AudioPaLM was able to outperform existing systems for speech translation tasks and also demonstrated zero-shot speech-to-text translation for many languages not seen in training. The model significantly improved speech processing by leveraging the larger quantity of text training data used in pretraining. AudioPaLM inherits the capability to preserve paralinguistic information such as speaker identity and intonation from AudioLM and linguistic knowledge present only in text large language models such as PaLM-2.

The discussion on the submission included comments about the accuracy of the translation and the potential for spam calls to be intercepted with AudioPaLM. There were also discussions about the benefits of bilingual models for literal translations and how LLMs can represent different languages. Some commenters expressed skepticism towards the model's capabilities and the possibility of spying.

### Show HN: A package manager for AI plugins

#### [Submission URL](https://openpm.ai/) | 81 points | by [maccaw](https://news.ycombinator.com/user?id=maccaw) | [17 comments](https://news.ycombinator.com/item?id=36447683)

OpenPM is a package manager for AI plugins that simplifies the integration of various APIs. It offers a seamless process for exploring, publishing, and integrating APIs into AI platforms. With OpenPM, developers can easily discover and use new AI plugins, while API providers can publish their plugins to the registry and make them available to the community. Its API search function streamlines the integration process, removing the need for manual coding to connect various APIs. Overall, OpenPM represents a handy tool for streamlining AI development and integration processes.

The discussion around the OpenPM submission on Hacker News includes several topics related to AI development and integration. One commenter raises concerns about security and the supply chain attacks that can occur. Others discuss the packages available, including Cloudflare's Worker building AI plugins and WorkGPT, a framework for working with GPT functions. There are also discussions about API search and preview functionality, the adoption of AI plugins, and the role of OpenPM in streamlining AI development. Some comments also touch on the confusion between OpenPM and other similar tools like OpenAPI and OpenAI. Another contributor compares alternative platforms and suggests that OpenAI is leading the industry with its commitment to regulation and support for open-source alternatives.

---

## AI Submissions for Thu Jun 22 2023 {{ 'date': '2023-06-22T17:13:47.434Z' }}

### Generating SQL with LLMs for fun and profit

#### [Submission URL](https://iamnotarobot.substack.com/p/generating-sql-with-llms-for-fun) | 63 points | by [diego](https://news.ycombinator.com/user?id=diego) | [26 comments](https://news.ycombinator.com/item?id=36440760)

Language models are being used to generate SQL code for querying databases, but this could lead to potentially dangerous situations if the generated SQL code is not thoroughly checked. As demonstrated by Diego Basch, language models can easily generate queries that alter or drop tables, and even create infinite loops. While attempts to train the model to recognize risky queries have been made, there is still a risk of prompt injection leading to SQL injection. Basch suggests making the database read-only or creating a restricted role for the language model to use as a preventive measure. However, language models should not yet be trusted to generate executable code on the fly.

The discussion on the submission suggests that language models are still not yet able to generate executable code on-the-fly and should be used with caution. There are also concerns about data security and measures to prevent unauthorized access to databases. Suggestions were made to create a read-only database or create a restricted role for the language model to use. The discussion also touched on the importance of manual tracking and overseeing the process of generating SQL code. However, solutions such as making database read-only or creating restricted access have its limitations. There is also a mention about how language models should not be trusted to generate code for production systems before being thoroughly tested. The commenters also discussed the potential dangers of prompt injection leading to SQL injections. One solution proposed was to enforce reasonable time limits and database permissions and to add security measures such as read-write security in PostgreSQL, to prevent unauthorized access to databases. Finally, a few commenters raised concerns about proper communication to the audience, where the content generated by language models should be closely monitored to avoid misunderstandings or potential risks.

### Stability AI Launches Stable Diffusion XL 0.9

#### [Submission URL](https://stability.ai/blog/sdxl-09-stable-diffusion) | 166 points | by [seydor](https://news.ycombinator.com/user?id=seydor) | [100 comments](https://news.ycombinator.com/item?id=36435559)

Stability AI has announced the release of SDXL 0.9, marking a significant advancement in the Stable Diffusion text-to-image models. This new development offers a leap in creative use cases for generative AI imagery, providing hyper-realistic creations for films, television, music, instructional videos, design, and industrial use. SDXL 0.9 boasts a 3.5B parameter base model and a 6.6B parameter model ensemble pipeline, with a 1024x1024 resolution and the ability to generate highly detailed images. Despite its powerful output, SDXL 0.9 requires only a modern consumer GPU to run. The model is available on the Clipdrop platform, with API access coming soon. The discussions centered around the quality, resolution, use cases, and specifications of the new model, including its compatibility with different GPUs and processors. Some users speculated on the legal issues of using the model, while others discussed the possibility of generating specialized powered AI models.

### People paid to train AI are outsourcing their work to AI

#### [Submission URL](https://www.technologyreview.com/2023/06/22/1075405/the-people-paid-to-train-ai-are-outsourcing-their-work-to-ai/) | 334 points | by [kungfudoi](https://news.ycombinator.com/user?id=kungfudoi) | [221 comments](https://news.ycombinator.com/item?id=36432279)

A new study by the Swiss Federal Institute of Technology (EPFL) reveals that a significant portion of gig workers tasked with training AI models have been outsourcing their work to AI themselves. Researchers hired 44 people on Amazon Mechanical Turk to summarize medical research papers, and analyzed their responses using an AI model they trained themselves. They found that between 33% and 46% of workers used AI models like OpenAI’s ChatGPT. This percentage is expected to grow as AI models become more powerful and accessible. This could introduce further errors into already error-prone models and highlights the need for new ways to check whether data has been produced by humans or AI. Some commenters criticize the study for not accurately detecting whether responses were generated by humans or AI. Others discuss the importance of creating new ways to check whether data has been produced by humans or AI to ensure accuracy. There are also debates over the ethics of using AI to replace human workers. Some responders of the comment section shared their experiences working on Mechanical Turk, and a few cited sources to correct minor mistakes in the original post.

### Show HN: Launching Struct – Knowledge-Rich, AI-Powered Chat Platform

#### [Submission URL](https://www.struct.ai/blog/launching-struct-chat-platform) | 54 points | by [mrjn](https://news.ycombinator.com/user?id=mrjn) | [35 comments](https://news.ycombinator.com/item?id=36432743)

Chat platforms like Slack and Discord have revolutionized real-time communication, but they have inherent flaws that turn them into knowledge black holes. Information gets lost in the sea of endless messages, and finding them is akin to searching for a needle in a haystack. Manish R Jain, the founder of Dgraph Labs, introduces the CRISPY framework, outlining six principles that an ideal chat platform should uphold. Jain's new, innovative chat platform, Struct, embodies this framework, making real-time communication accessible, lasting knowledge for a change. Struct aims to reinvent the current chat platform's status quo and challenges the biggest problems that millions of users face daily.

The article discusses the inherent flaws of chat platforms like Slack and Discord, which turn them into knowledge black holes where messages can get lost, and information is hard to retrieve. The founder of Dgraph Labs, Manish R Jain, introduces the CRISPY framework that outlines six principles that an ideal chat platform should have. Jain's new chat platform, Struct, aims to address the problems that millions of users face daily and reinvent the current chat platform status quo. The comments on Hacker News highlight various aspects of the article, including the need for a chat platform that can handle knowledge, the struggle with finding information on large Discord servers, and the role of AI-powered search in chat platforms. The comments also raise concerns about the pricing and privacy policies of Struct.

### Aeon: A unified framework for machine learning with time series

#### [Submission URL](https://github.com/aeon-toolkit/aeon) | 119 points | by [megalodon](https://news.ycombinator.com/user?id=megalodon) | [23 comments](https://news.ycombinator.com/item?id=36432369)

Aeon Toolkit is a unified framework for machine learning with time series that is compatible with scikit-learn. It offers both classical techniques and the very latest algorithms for learning tasks like forecasting and classification. A broad library of time series algorithms, efficient implementations with numba, and interfaces with other time series packages make Aeon Toolkit an ideal choice for algorithm comparison. The latest release is version v0.3.0, and you can visit their website for documentation and installation instructions. Their code is licensed under the BSD-3-Clause license.

Aeon Toolkit, a unified framework for machine learning with time series, is the subject of a Hacker News discussion. The toolkit is compatible with scikit-learn and offers both classical techniques and the latest algorithms for tasks such as forecasting and classification. It has a broad library of time series algorithms, efficient implementations with numba, and interfaces with other time series packages, making it ideal for algorithm comparison. Commenters discuss various aspects of time series data and highlight the advantages of the Aeon Toolkit's friendly and flexible framework for developing machine learning models. They also mention other toolkits and packages, such as Prophet, sktime, Darts, and Weka, for working with time series data and developing machine learning models.

### OpenAI Lobbied the E.U. To Water Down AI Regulation

#### [Submission URL](https://time.com/6288245/openai-eu-lobbying-ai-act/) | 158 points | by [jlpcsl](https://news.ycombinator.com/user?id=jlpcsl) | [71 comments](https://news.ycombinator.com/item?id=36428121)

Documents obtained by TIME show that OpenAI lobbied to weaken forthcoming AI regulation in the EU while publicly calling for stronger AI guardrails. CEO Sam Altman has been touring world capitals and speaking about the need for global AI regulation, but behind the scenes, OpenAI proposed amendments that were later made to the final text of the EU law. OpenAI proposed that its general-purpose AI systems including GPT-3 and DALL-E 2 should not be considered "high risk," a designation that would subject them to stringent legal requirements including transparency, traceability, and human oversight. The lobbying efforts by OpenAI in Europe have not previously been reported, although Altman has recently become more vocal about the legislation.

The discussion on the submission revolves around the issue of OpenAI lobbying to weaken forthcoming EU AI regulation while publicly calling for stronger AI guardrails. Some comments accuse OpenAI of hypocrisy, while others argue that lobbying is a common practice among corporations and that regulations could stifle innovation. Some users criticize the lack of transparency in the AI generated content, while others express concerns about the power dynamics in the industry and the potential dangers of unregulated AI technology. Some users raise questions about the effectiveness of regulations in protecting the public and balancing the interests of all stakeholders involved. Overall, the discussion reflects a diversity of opinions and perspectives on the issue of AI regulation and its impact on industry, innovation, and society.

### Is Google reCAPTCHA GDPR Compliant?

#### [Submission URL](https://wideangle.co/blog/is-recaptcha-illegal-under-gdpr) | 143 points | by [openplatypus](https://news.ycombinator.com/user?id=openplatypus) | [222 comments](https://news.ycombinator.com/item?id=36430280)

The French data protection authority, CNIL, has imposed a penalty on Cityscoot for using Google's reCAPTCHA tool on their website and app without providing sufficient privacy information or seeking consent from visitors. The case raises questions about reCAPTCHA's compliance with EU data protection and privacy laws, with the CNIL concluding that the tool, which uses third-party cookies to distinguish bots from humans, requires consent. While some cookies are exempt from consent, this exception does not extend to cookies that are not strictly necessary, such as those used for analysis.

The discussion on the article clarifies technical terms like pixels, scripts, and beacons, and points out that some exceptions exist to the requirement of consent for cookies. The topic then shifts to CAPTCHA, with some contributors criticizing it for being a nuisance or hindrance to users, while others claim it is necessary to protect websites from spam and bot attacks. The IPv4 and IPv6 internet protocols, their limitations, and implementation challenges are also discussed. Finally, some participants share their experiences with CAPTCHA and captchas' effectiveness in preventing spam signups.