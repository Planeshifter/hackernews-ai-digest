import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Mon Feb 12 2024 {{ 'date': '2024-02-12T17:11:13.897Z' }}

### Home Assistant: Three years later

#### [Submission URL](https://eamonnsullivan.co.uk/posts-output/home-automation-three-years/2024-02-11-home-assistant-three-years-later/) | 234 points | by [eamonnsullivan](https://news.ycombinator.com/user?id=eamonnsullivan) | [151 comments](https://news.ycombinator.com/item?id=39345122)

In this blog post, the author reflects on their experience with Home Assistant, a popular home automation software, after using it for almost three years. They discuss what hasn't changed, such as the ability to integrate devices from different manufacturers and the use of Node-RED for automations. However, they note that their approach to automation has evolved, focusing more on subtle and seamless actions rather than flashy effects. They also emphasize the importance of local control, opting for devices that work without relying on cloud services. The author mentions their reliance on the Home Assistant Cloud service for secure remote access and expresses cautious optimism about the new local-focused standard for home automation called Matter. They also discuss the growing importance of Home Assistant in their daily life and mention their considerations for replacing their hardware setup in the future. Overall, the author highlights their journey with Home Assistant and the valuable lessons they've learned along the way.

The discussion on the submission revolves around personal experiences with Home Assistant and home automation in general. Some individuals express frustration with cloud services and the reliance on internet connectivity for functionality. Others discuss their DIY projects and the challenges they face in integrating different devices. The topic of local control and the rejection of cloud services is also brought up, with some commenters expressing concerns about privacy and potential issues with companies discontinuing support. The discussion also touches on the compatibility of different smart home devices and the importance of standardization. Some users share their preferred hardware options, such as Z-Wave switches, while others mention their experiences with mechanical and digital solutions for light switches. The conversation concludes with skepticism about the future adoption of the Matter standard and doubts about its current functionality.

### The Rise and Fall of GOFAI

#### [Submission URL](https://billwadge.com/2024/02/12/the-rise-and-fall-of-gofai/) | 47 points | by [herodotus](https://news.ycombinator.com/user?id=herodotus) | [36 comments](https://news.ycombinator.com/item?id=39344934)

Bill Wadge, an AI expert, recently wrote a blog post discussing the rise and fall of Good Old Fashioned AI (GOFAI). GOFAI, which grew out of the 1956 Dartmouth AI meeting, is based on symbolic reasoning and has had several triumphs throughout history. Wadge highlights the invention of numbers and numerals as the first triumph, as they allowed for reliable symbolic reasoning about quantities. The invention of the abacus and similar mechanisms also automated symbolic computation with numerals. Aristotle's classification of valid syllogisms and the development of calculus and mathematical notation further extended the domain of symbolic reasoning. However, GOFAI encountered significant challenges as well. Russell's Paradox dealt a blow to Frege's axiomatization of set theory, and Gödel's incompleteness theorems showed that any formal system powerful enough to formalize arithmetic is incomplete. Despite these setbacks, GOFAI continued to make progress with the introduction of the λ calculus and Turing machines. However, it became clear by the 1956 Dartmouth conference that GOFAI had its limitations.

The discussion about the submission revolves around various aspects of Good Old Fashioned AI (GOFAI). Some commenters argue that GOFAI is not relevant and that modern AI techniques, particularly those based on machine learning, have far surpassed its capabilities. They mention examples like Deep Blue, which defeated Garry Kasparov in chess using brute-force search, and suggest that GOFAI approaches like symbolic reasoning are no longer practical.
Others point out that GOFAI has been successful in certain domains, such as natural language processing, planning, and scheduling. They argue that GOFAI techniques, like those implemented in Lisp, have their own advantages and should not be dismissed entirely.
There is also a discussion about the fundamental problems of GOFAI, including Gödel's incompleteness theorems and the undecidability problem. Some commenters argue that machine learning, while based on mathematical foundations, also has its limitations and is not a complete solution to AI.
The debate continues with discussions on the definition of GOFAI, its relationship with other fields like cybernetics, and the role of logic in AI. Some commenters highlight the successes of GOFAI in areas like automated provers and expert systems, while others express skepticism and believe that GOFAI is limited in its scalability.
Overall, the discussion reflects different perspectives on the strengths and weaknesses of GOFAI compared to modern AI approaches, and the ongoing debate about the future direction of AI research and development.

### AMD funded a drop-in CUDA implementation built on ROCm: It's now open-source

#### [Submission URL](https://www.phoronix.com/review/radeon-cuda-zluda) | 1001 points | by [mfiguiere](https://news.ycombinator.com/user?id=mfiguiere) | [390 comments](https://news.ycombinator.com/item?id=39344815)

In a surprising move, AMD has quietly funded a project to bring binary compatibility between NVIDIA CUDA applications and the AMD ROCm stack. This means that many CUDA applications can now run on AMD Radeon GPUs without the need for developers to adapt the source code. The project, known as ZLUDA, was initially developed to enable CUDA support on Intel graphics, but was later adapted for use on AMD GPUs. Although it's not 100% fail-safe and some features are not supported, the implementation has been successful in running CUDA-enabled software on ROCm. The open-source code is dual-licensed under Apache 2.0 or MIT, and it leverages the Rust programming language for the Radeon implementation. This development opens up new possibilities for end-users who want to run CUDA applications on AMD GPUs without any additional effort.

The discussion on Hacker News regarding the submission about AMD funding a project for binary compatibility between NVIDIA CUDA applications and the AMD ROCm stack covers a range of topics.
One commenter points out the dominance of NVIDIA in the machine learning field due to the popularity of CUDA. They argue that if AMD can provide seamless compatibility for CUDA machine learning tasks, it could potentially eat into NVIDIA's market share.
Another commenter raises concerns about the ever-changing landscape of machine learning frameworks and the difficulty of keeping up with them. They suggest that instead of investing in low-level optimizations for AMD GPUs, it might be strategically better for AMD to invest in compilers and software tools that allow high-level languages to write efficient kernels for AMD hardware.
There is also a discussion about the business decision behind AMD's funding of the project. Some commenters express frustration at AMD's lack of support for the ROCm stack, while others speculate about AMD's long-term strategy in the machine learning market.
One commenter brings up the success of the RADV project, which is an open-source Radeon Vulkan driver developed by Red Hat and Valve. They argue that if AMD can fund and support projects like ZLUDA, it can be beneficial for the community as a whole.
Additionally, there are discussions about the control NVIDIA has over CUDA software and hardware, the misconception that deep learning frameworks are built solely on CUDA, and the potential limitations of ZLUDA compared to the official CUDNN library.

Overall, the discussion reflects a mixture of excitement, skepticism, and curiosity about AMD's move to fund the ZLUDA project and the potential impact it could have on the machine learning community.

### GeneGPT, a tool-augmented LLM for bioinformatics

#### [Submission URL](https://github.com/ncbi/GeneGPT) | 106 points | by [brianzelip](https://news.ycombinator.com/user?id=brianzelip) | [18 comments](https://news.ycombinator.com/item?id=39348902)

GeneGPT is a tool-augmented large language model (LLM) designed to address the challenges faced by LLMs in handling specialized biomedical knowledge. With the goal of improving information retrieval in this domain, GeneGPT uses NCBI Web APIs to answer questions related to biomedical information. This approach leverages in-context learning and a unique decoding algorithm to execute API calls. Experimental results show that GeneGPT outperforms previous state-of-the-art models on eight GeneTuring tasks, with an average score of 0.83. The model surpasses BioGPT and ChatGPT, achieving a significant improvement in accuracy. In addition, GeneGPT demonstrates the potential of integrating domain-specific tools with LLMs to improve access and accuracy in specialized knowledge areas. The code and data for GeneGPT are available on GitHub.

The discussion on this submission revolves around various aspects of the GeneGPT project and related topics in the field of biomedical information retrieval. 
One user points out that the initial prompt for the language model project doesn't specify whether it's using the OpenAI API or some other method. Another user provides clarification, saying that the project is using NCBI Web APIs wrapped in command-line interfaces (CLIs) to perform tasks and suggests that the GPT OpenAI Marketplace could be a suitable platform for this.
Another user mentions that they found a similar project called LLaVA-Med, which also focuses on using domain-specific tools with language models.
There is a brief comment suggesting that the name "NCBI-APIs-GPT" would be more appropriate for this project.
The discussion then shifts to the topic of genetic information. One user jokingly suggests that they will make the GPT process their 23andMe DNA data to confirm special abilities. Another user mentions a tool called LLaVA-Med, which they believe is the best modern classifier they have found.
The conversation also touches on the evaluation of language models and the importance of measuring factors like question-answering accuracy and exact match performance.
A user brings up the possibility of using language models to predict phenotypes from genotype data, highlighting the potential of genetic engineering and its impact on phenotypic traits.
There's a discussion about transfer learning and the potential for transformer models to map genotypes to phenotypes accurately. Some users mention the challenges of gene regulation and the complexity of genetic data.
One user raises concerns about data security and the risks of handling sensitive genetic information. They suggest that genetic datasets contain subtle patterns that can be exploited.

Finally, there are a couple of comments unrelated to the main topic, with one user expressing disappointment that a language model is based on Gene Rayburn from Match Game and another user confirming the nature of the post.

### A celebrated cryptography-breaking algorithm just got an upgrade

#### [Submission URL](https://www.quantamagazine.org/celebrated-cryptography-algorithm-gets-an-upgrade-20231214/) | 46 points | by [thunderbong](https://news.ycombinator.com/user?id=thunderbong) | [3 comments](https://news.ycombinator.com/item?id=39341180)

Researchers have developed a new algorithm that improves the efficiency of lattice basis reduction, a technique widely used in cryptography and mathematics. The algorithm, known as LLL-style, allows for the reduction of the bases of lattices with thousands of dimensions, significantly expanding the range of scenarios in which LLL-like approaches can be used. The technique combines multiple strategies, including a recursive structure and precise number management, to achieve its efficiency. The algorithm has the potential to enhance the security and performance of cryptographic systems.

The discussion surrounding the submission is rather limited. One user, "dfrst," shares a link to the full paper of 63 pages, providing more in-depth information on the algorithm. Another user, "dng," shares a link to another article about the algorithm upgrade, which has attracted five comments. There is a third user, "ChrisArchitect," who simply mentions the year "2023," potentially indicating that the algorithm is expected to be significant in that year. Unfortunately, without further details, it is difficult to discern the specific content of the comments or the significance of the mentioned year.
- Biological AI and the evolution of plant intelligence were mentioned, raising the idea of millions of years of closed-loop plant systems evolving into AI.

Overall, the discussion covered a wide range of topics related to AI, energy consumption, economics, and the environment.

---

## AI Submissions for Sun Feb 11 2024 {{ 'date': '2024-02-11T17:09:40.809Z' }}

### Disney's newest robot demonstrates collaborative cuteness

#### [Submission URL](https://spectrum.ieee.org/disney-robot-2666681104) | 39 points | by [kungfudoi](https://news.ycombinator.com/user?id=kungfudoi) | [15 comments](https://news.ycombinator.com/item?id=39339264)

Disney's newest robot, Duke Weaselton, is demonstrating the power of collaborative cuteness. At Walt Disney Imagineering, researchers have been exploring the idea of robots collaborating with each other, and Duke Weaselton is the result of one such collaboration. Duke, a furry character, interacts with a purple kiosk onstage, with both characters showcasing different kinds of motion. By working together as one system, they are able to achieve dynamic and expressive movements. This opens up exciting opportunities for entertainment robotics and demonstrates that two robots can be much more capable than one.

The discussion on this submission revolves around a few different topics. 
- One user mentions that Disney's YouTube channel regularly posts interesting developments in animation, human tracking, and motion. They provide a link to the channel and encourage checking it out.
- Another user asks about the high pixelated gif they see on a website, to which someone responds that it is likely a WebM file, an open media format designed for the web. There is then a discussion about whether iOS supports WebM, with one user mentioning that Safari on iOS does not support it.
- Someone else comments that the robot in question appears to be collaborating and wonders what it would be like if it spoke to technical experts at IEEE, the professional organization for technical advancements. There is a response mentioning that spectrum proceedings (presumably an IEEE publication) would be interested in this kind of development.
- A user points out that the discussion has deviated from the topic and that the robot is indeed a robot. This leads to a brief conversation about the definition of a robot.

Overall, the discussion touches on topics like Disney's other developments, media formats, technical organizations, and the nature of robots.

### Show HN: LLMWare – Small Specialized Function Calling 1B LLMs for Multi-Step RAG

#### [Submission URL](https://github.com/llmware-ai/llmware) | 46 points | by [doberst0103](https://news.ycombinator.com/user?id=doberst0103) | [5 comments](https://news.ycombinator.com/item?id=39335263)

llmware is a unified framework for developing LLM (Language and Learning Models) based application patterns, including Retrieval Augmented Generation (RAG). It provides a set of tools that make it easy to build knowledge-based enterprise applications using open source models and secure integration of enterprise knowledge. With llmware, developers can access and use a wide range of models through the Model Catalog, ingest, organize, and index a collection of knowledge with the Library feature, and query libraries with various filters using the Query feature. The framework is designed to be beginner-friendly while also catering to the needs of experienced AI developers.

The discussion around the submission mainly consists of comments from the project creator, blzngbnn. They provide some insights into the development of LLMWare, including the ability to run models locally on standard consumer CPUs and the provision of cheap and efficient model processing. They mention using SLIM models for powerful timing and internal business process enhancements using LLMs.
Another user, doberst0103, thanks blzngbnn for their feedback. Then trcrblltx reminds blzngbnn to give credit to llmcpp for running models and mentions the project's platform for Retrieval Augmented Generation (RAG) function calling.
In response, doberst0103 acknowledges the credit that should be given to Georgi Gerganov and mentions their amazing advancements demonstrated in YouTube videos.
Finally, blzngbnn agrees that llmcpp, the open-source community, and getting AI models directly into the hands of people are all excellent things. They express excitement for the power of local LLMs and mention eagerly waiting for open-source platforms to fulfill their potential by 2024.

### Show HN: Loz – Automate Git Commit Messages with LLM

#### [Submission URL](https://github.com/joone/loz) | 18 points | by [cubix4u](https://news.ycombinator.com/user?id=cubix4u) | [4 comments](https://news.ycombinator.com/item?id=39331538)

LOZ is a powerful command-line tool that harnesses the capabilities of Large Language Models (LLMs) like code-lamma and GPT-3.5 to automatically generate GIT commit messages based on code changes. With 201 stars and 11 forks, this project is gaining traction in the developer community. The tool is written in TypeScript, JavaScript, Shell, and Dockerfile. The repository has 3 contributors, including the creator, Joone Hur. If you're looking for a way to streamline your commit workflow and make it more efficient, LOZ might be the tool you need.

The discussion on the LOZ tool revolves around the use of Large Language Models (LLMs) for generating commit messages and the effectiveness of such messages. One commenter points out that the AI doesn't understand the business context of the code changes and cannot explain the higher-level reasoning behind them. They argue that the generated messages lack usefulness in providing helpful information for debugging or modifying code.

Another commenter argues that commit messages should describe the context and reasoning behind the code changes, making the generated messages essentially pointless.
Someone shares their experience with using a Python script to generate commit messages using ChatGPT 1 (powered by GPT) and suggests a similar approach to LOZ.
Lastly, a user mentions the effectiveness of GitHub Copilot in summarizing pull requests (PRs), working seamlessly within the GitHub workflow. They suggest experimenting with multiple models and comparing comments to determine what works best.

---

## AI Submissions for Sat Feb 10 2024 {{ 'date': '2024-02-10T17:09:34.692Z' }}

### Grand-Master Level Chess Without Search: Modeling Choices and Their Implications

#### [Submission URL](https://gist.github.com/yoavg/8b98bbd70eb187cf1852b3485b8cda4f) | 93 points | by [georgehill](https://news.ycombinator.com/user?id=georgehill) | [37 comments](https://news.ycombinator.com/item?id=39328684)

Researchers at Google DeepMind have released a paper detailing a learned system that can play blitz-chess at a grandmaster level without using search. This is significant because previous computer-chess systems at this level have relied on search components. The paper has generated a range of reactions, with some impressed by the implications for AI learning abilities and others pointing out that it only applies to blitz-chess. However, upon closer examination, the paper does not claim certain assertions being made about the AI's abilities. The paper is well-written and does not over-claim, but there are some misconceptions regarding the model's achievements. The paper utilized a learning setup with 10 million human-human chess games, training a transformer-based model to predict the winning probability of moves. The model was then used to play games without search, achieving grandmaster-level performance. Overall, the results are impressive, but it is important not to overstate the capabilities of the AI system.

The discussion on the submission revolves around different interpretations of the paper and the capabilities of the AI system. Some commenters point out that the paper does not claim certain assertions being made about the AI's abilities. They explain that while the AI can play blitz-chess at a grandmaster level without using search, it does not mean it can perform the same way in other contexts. 
There is also a debate about the role of search in chess playing. Some argue that grandmasters rely on search and that the AI system in the paper simply flattens the search paths in high-dimensional space. Others contend that strong chess players understand the structure of positions and do not rely heavily on search.
Some comments highlight the limitations of the paper's training method and question the validity of its claims. They argue that the learning method used by the AI is not capable of learning arbitrary rules and that the model simply predicts move probabilities based on the provided training data.
There are also discussions about the importance of prior chess knowledge and the ability of the AI to recognize specific patterns in the game. Some commenters mention Chess960 and traditional chess as different domains where the AI's performance may vary.

There is criticism of DeepMind's research and claims, with some commenters accusing them of exaggerating their results and generating hype. Others argue that these criticisms are common in the research field and that they mostly come from uninformed individuals. Overall, the discussion covers a range of perspectives on the paper and the implications of the AI's abilities.

### Walmart, Delta, and Starbucks are using AI to monitor employee messages

#### [Submission URL](https://www.cnbc.com/2024/02/09/ai-might-be-reading-your-slack-teams-messages-using-tech-from-aware.html) | 151 points | by [cebert](https://news.ycombinator.com/user?id=cebert) | [87 comments](https://news.ycombinator.com/item?id=39326201)

Artificial intelligence (AI) firm Aware is helping major companies like Walmart, Delta, T-Mobile, Chevron, and Starbucks analyze employee messages for various purposes, according to the company. Aware's AI technology allows companies to understand employee sentiment in real-time, monitor compliance, and identify behaviors such as bullying, harassment, and discrimination. The analytics tool does not flag individual employee names but can do so in extreme cases where there are predetermined risks. While this employee surveillance AI market is rapidly expanding, there are concerns about privacy and treating employees like "inventory." Aware's revenue has grown by 150% on average over the past five years, and its competitors include Qualtrics, Relativity, Proofpoint, Smarsh, and Netskope.

The discussion surrounding the submission revolves around the concerns regarding privacy and the ethical implications of employee surveillance. Some users argue that the expectation of privacy in workplace communications is low and that companies have the right to monitor and analyze internal communications. Others express concerns about the potential abuse of such technology and the erosion of privacy. The discussion also touches on broader topics such as the normalization of surveillance and the role of governments and corporations in data collection. Some users argue that the surveillance and monitoring of internal corporate communications should be subject to legal regulations, while others emphasize the importance of individual accountability and self-regulation. Overall, the discussion highlights the complex ethical and societal implications of using AI technology for employee monitoring.

### Will Artificial Intelligence Lead to War?

#### [Submission URL](https://nationalinterest.org/feature/will-artificial-intelligence-lead-war-208958) | 16 points | by [rntn](https://news.ycombinator.com/user?id=rntn) | [17 comments](https://news.ycombinator.com/item?id=39329478)

Artificial Intelligence (AI) has the potential to reshape the dynamics of international conflicts and pose risks to global stability. Large language models and generative AI can impact strategic thinking and alter perceptions of capabilities and intentions. The use of AI by adversarial powers like China and North Korea could challenge the effectiveness of deterrence strategies by predicting and discounting actions taken by the United States. The reliance on AI in strategic decision-making may lead to misperceptions, false confidence, and even an increased risk of conflict. China aims to use AI as part of a "smart deterrence" policy, but it could inadvertently act as a catalyst for war. The People's Liberation Army (PLA) of China is integrating predictive AI to outwit opposing militaries and achieve strategic effects. Similarly, North Korea is quietly developing AI and machine learning capabilities for military purposes. The regime prioritizes military power and may apply AI to strategic planning. These developments highlight the need for policymakers to scrutinize and invest in three areas: reinforcing deterrence strategies, enhancing military-to-military dialogue, and investing in AI capabilities for defensive purposes. Overall, the use of AI in military contexts introduces uncertainties and potential destabilization, increasing the risk of conflict.

The discussion on this submission covers various aspects related to the use of AI in international conflicts. 
One user points out that China has a history of modernizing its military and may be planning cyber and psychological warfare based on advanced technologies like AI. They express concerns about the potential misinformation and misperceptions that AI can create, as well as the risk of a war as a result.
In response, another user mentions that the American public may have a distorted perception of the military capabilities of China and the effectiveness of American deterrence strategies. They also discuss the overwhelming support for nuclear response among the American population.
Another user brings up the issue of corruption within the Chinese military, comparing it to Russia's corruption problems. They argue that China's military corruption may be a result of stealing technology and resources, and it partially explains their rapid advancement.
The discussion then shifts to the impact of AI on military capabilities. One user suggests that the Malthusian model of population growth applies to China, as it heavily relies on imported resources to sustain its expanding population.
A separate discussion focuses on concerns about TikTok, a Chinese app, and the potential influence it may have on American children. This leads to a debate about the responsibility of parents and the infiltration of Chinese propaganda.
Another user mentions the importance of China's role as a supplier of cheap products to American companies and raises the point that AI, particularly GPTs (large language models), can have productive and potentially entertaining applications in mining cryptocurrencies.
There are also brief comments about leadership and the unwillingness of nations to deescalate tension, the impact of natural stupidity, the influence of AI on wars, and the potential for AI to aid in drug development.

In summary, the discussion covers concerns about China's use of AI in military planning, the potential for misinformation and misperception in international conflicts, the impact of corruption on military advancements, the role of imported resources in sustaining population growth, the influence of Chinese apps on American children, the productivity of AI in cryptocurrency mining, and other miscellaneous topics.

### Who makes money when AI reads the internet for us?

#### [Submission URL](https://www.engadget.com/who-makes-money-when-ai-reads-the-internet-for-us-200246690.html) | 28 points | by [kripy](https://news.ycombinator.com/user?id=kripy) | [12 comments](https://news.ycombinator.com/item?id=39324689)

"The Browser Company's new iPhone app, Arc Search, has stirred up controversy with its 'Browse for Me' feature that reads web pages and summarizes them into a single, custom-built web page using large language models from OpenAI and others. While the feature has been praised for its convenience, critics argue that it diminishes the need for users to visit actual websites, depriving creators of compensation for their work. The CEO of The Browser Company, Josh Miller, acknowledges the ethical concerns but believes that the way web creators monetize their content needs to evolve. However, the lack of a clear compensation plan for creators has raised further questions. The future of web browsing and AI's role in the dissemination of information remains a topic of debate."

The discussion regarding the submission centers around various perspectives on the "Browse for Me" feature of The Browser Company's new iPhone app, Arc Search. Some users express concerns over the potential negative impact on web creators and their compensation. They argue that the app's summarization feature may discourage users from visiting actual websites and diminish the value of creators' work. The lack of a clear compensation plan for creators raises further questions and ethical concerns.

Other users suggest alternative monetization models, such as allowing microtransactions for viewing individual articles or implementing subscription-based systems. Some propose using cryptocurrencies like Bitcoin, while others mention the potential benefits of AI-powered web readers for normal company websites and publishers. 

One user humorously compares Arc Search to a hypothetical scenario of rolling up newspapers to keep people busy while they read the news. Another user points out the prevalence of AI-generated content and the issues it creates, such as clickbait, low-quality articles, and manipulation of search engine rankings. They also mention the need for a transparent filtration system to ensure a clean and reliable online environment.

There is a discussion about the blocking of Arc's crawler, with some users sharing their opinions on the potential consequences. One user believes that blocking Arc may not have a significant impact on its AI-enhanced browsing and search capabilities and compares it to similar technologies from Google. They express concerns about the future of content consumption and suggest exploring options like paywalls and redistributive payment arrangements.

There is also a comment highlighting the cyclical nature of publishers complaining about AI-generated content, suggesting that publishers should focus on producing quality and unique content rather than blaming AI advancements.