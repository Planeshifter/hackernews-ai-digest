import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Wed Sep 20 2023 {{ 'date': '2023-09-20T17:10:49.002Z' }}

### Show HN: SeaGOAT – local, “AI-based” grep for semantic code search

#### [Submission URL](https://github.com/kantord/SeaGOAT) | 232 points | by [kantord](https://news.ycombinator.com/user?id=kantord) | [37 comments](https://news.ycombinator.com/item?id=37583219)

SeaGOAT is a powerful code search engine that uses vector embeddings to enable semantic code searching in your codebase. Developed by GitHub user kantord, SeaGOAT is designed to help developers quickly find specific pieces of code by understanding the meaning behind the code, rather than relying on traditional keyword-based searches. The tool is built to be fast and efficient, making it suitable for both small and large codebases.

The main advantage of SeaGOAT is its local-first approach, meaning that it runs entirely on your local machine without sending your code or queries to any external servers. This ensures that your code remains secure and private. SeaGOAT supports multiple programming languages and can be easily integrated into your development workflow.

To use SeaGOAT, you need to install Python 3.11 or newer, as well as the dependencies ripgrep and bat (optional, but recommended). Once installed, you can start the SeaGOAT server and use the "gt" or "seagoat" command to query your code repository. You can search for code snippets based on their semantic meaning or use regular expressions for more specific searches.

SeaGOAT is actively developed and maintained, and the GitHub repository includes detailed documentation on how to get started, install dependencies, run tests, and contribute to the project. It's worth noting that the developer behind SeaGOAT, kantord, is actively seeking new job opportunities as a Senior Full Stack Developer and has over 10 years of professional software development experience.

If you're looking for a powerful code search engine that combines the benefits of semantic searching with privacy and control, SeaGOAT might be the solution you've been searching for. Check out the GitHub repository for installation instructions and more information on how to use SeaGOAT in your development workflow.

The discussion on the SeaGOAT submission covers various topics related to the code search engine:

1. Compatibility and performance: A user mentions that they are running a large project and are interested in CUDA acceleration. Another user responds that SeaGOAT currently does not support it but suggests using ChromaDB for complex queries. The original user notes that they are also interested in complex queries and asks about additional query parameters. The developer of SeaGOAT explains that it currently supports complex queries and provides links to the API documentation.

2. Support for different programming languages: A user asks about the supported programming languages in SeaGOAT. The developer clarifies that SeaGOAT supports various programming languages such as Python, C++, TypeScript, and more, which can be found in the project documentation.

3. Limitations and improvements: Users discuss the limitations of SeaGOAT, including its limited file extensions support and the difficulty of adding new features. The developer acknowledges the limitations and welcomes pull requests for improvements. They also mention that the hard-coded limitations are mostly for performance reasons.

4. Comparison with other code search tools: A user compares SeaGOAT with another code search tool they have tested. They note that the licensing of the other tool is restrictive and that it doesn't allow specifying the path for returning tests alongside code. There is no further discussion on this topic.

5. Semantic code embeddings and related projects: Users discuss the use of embeddings for semantic code searching and mention other projects and techniques related to this area. They discuss issues with sentence embeddings and suggest solutions such as embedding whitening and training with chunked codebases. The conversation also touches on the difficulties of incorporating comments and the indexing of code with language models like GPT-3.

6. Use cases and applications: Users mention various use cases for code search, including finding relevant code snippets in specific repositories, extracting function and variable names from vector embeddings, and using speech recognition for navigating code.

Overall, the discussion focuses on the capabilities, limitations, and potential improvements of SeaGOAT, while also exploring related topics in code search and semantic code analysis.

### Q-Transformer: Scalable Reinforcement Learning via Autoregressive Q-Functions

#### [Submission URL](https://q-transformer.github.io/) | 92 points | by [GaggiX](https://news.ycombinator.com/user?id=GaggiX) | [15 comments](https://news.ycombinator.com/item?id=37580224)

The Q-Transformer is a scalable reinforcement learning method that can train multi-task policies using both human demonstrations and autonomously collected data. It utilizes a Transformer to represent Q-functions and applies effective sequence modeling techniques for Q-learning by discretizing and autoregressing the action space. This approach outperforms previous offline RL algorithms and imitation learning techniques on a diverse real-world robotic manipulation task suite. Additionally, Q-Transformer can estimate affordance values, making it suitable for planning and execution systems. The method is shown to provide high-quality affordance values and outperforms previous combinations of QT-Opt and RT-1.

The discussion around the submission includes various comments and suggestions.

- One user mentions the advantage of using the RNN inference component in the Q-Transformer method. They also highlight the low computational requirements and mobile device-friendly performance of the method.

- Another user requests clarification on the memory efficiency of RWKV.

- There is a mention of discussions and suggestions happening on Discord, with a link provided for further engagement.

- One user finds the Q-Transformer approach interesting but raises questions regarding its applicability to multi-agent tasks.

- There are suggestions for learning reinforcement learning from scratch and understanding different approaches through reading textbooks, blogs, and tutorials.

- A user recommends a tutorial they wrote on deep reinforcement learning, emphasizing the need to grasp the theory and math behind RL.

- Another user suggests the book Grokking Deep Reinforcement Learning for a beginner-friendly introduction to RL concepts. They also recommend the Gymnasium library for practical implementations.

- A course and a YouTube video are mentioned as useful resources for learning RL.

- A user shares a GitHub repository with a simple reinforcement learning simulation, which replicates the Deep Q-Network algorithm for game playing.

- There is a comment expressing interest in taking courses but finding them expensive.

### Neurons in Large Language Models: Dead, N-Gram, Positional

#### [Submission URL](https://arxiv.org/abs/2309.04827) | 104 points | by [PaulHoule](https://news.ycombinator.com/user?id=PaulHoule) | [52 comments](https://news.ycombinator.com/item?id=37583136)

A recent paper titled "Neurons in Large Language Models: Dead, N-gram, Positional" analyzes the behavior of neurons in large language models. The researchers focus on the OPT family of models, ranging from 125m to 66b parameters, and study whether an FFN neuron is activated or not. They find that the early part of the network is sparse and represents discrete features, with a significant percentage of neurons being "dead," meaning they never activate on diverse data. The researchers also observe that alive neurons in these models serve as token and n-gram detectors and that the corresponding FFN updates remove information about triggering tokens. Interestingly, this is the first example of mechanisms specialized in removing information from the residual stream. As models scale, they become even more sparse, with a higher number of dead neurons and token detectors. Finally, the researchers identify positional neurons, which are activated or not depending largely on position rather than textual data.

The discussion on this submission covers various topics related to artificial neural networks and their behavior. 

- Some commenters discuss the concept of dead neurons and their association with ReLU networks. There is a debate about whether dead neurons are actually beneficial or not in network regularization. Anecdotal evidence and alternative activation functions such as Leaky ReLUs and Mish are also mentioned.
- The understanding of artificial neural networks and how they process input data is questioned. It is noted that the researchers may not fully understand the concept of artificial neurons and their role in categorizing and decoding data. The discussion touches on common arguments and comments regarding the reduction of models' size and the removal of dead neurons.
- Commenters discuss the potential methods for reducing model size by tracking neuron activation frequency during training or through weight pruning techniques. Examples of K-Means algorithms and clustering centers are mentioned as ways to regularize dead neurons.
- The topic of pruning in neural networks is brought up. Structured pruning is mentioned as a method to remove weights with minimal impact on quality, and there is a suggestion that randomization or column removal during training can also be effective.
- The complexity and challenges of pruning networks are discussed, with some mention of methods like activation pruning and weight pruning. The trade-off between accuracy and efficiency is acknowledged, and the penalties for sparsity in neural networks are mentioned.
- There is a discussion of the limitations and capabilities of artificial neural networks compared to human brain function. Commenters express surprise at the ability of neural networks to approximate human-level functioning in tasks like visual recognition and pattern matching. The importance of reinforcement learning and alignment in human intelligence is also mentioned.
- The topic of simulating real neurons and their complexities is brought up. Commenters note that accurately simulating the behavior of neurons and their connectivity in the brain is difficult and that mapping neural connectivity is a challenging task.
- The concept of qualia, consciousness, and the philosophical implications of artificial intelligence are briefly discussed, with some mention of the complexity of understanding and replicating certain aspects of human cognition.
- Finally, there are discussions about the logic capabilities of individual neurons and the complexity of XOR logic in neural networks. Some commenters mention that XOR logic can be achieved by single-layer perceptrons, while others propose that XOR logic is not a breakthrough and that the problem of NP-hardness still exists.

### Algorithm-assisted discovery of an intrinsic order among mathematical constants

#### [Submission URL](https://fermatslibrary.com/s/algorithm-assisted-discovery-of-an-intrinsic-order-among-mathematical-constants) | 84 points | by [BerislavLopac](https://news.ycombinator.com/user?id=BerislavLopac) | [11 comments](https://news.ycombinator.com/item?id=37581889)

A team of mathematicians from the Technion - Israel Institute of Technology have developed a computer algorithm that has discovered an unprecedented number of continued fraction formulas for fundamental mathematical constants. The algorithm, which utilizes thousands of personal computers worldwide, has revealed a novel mathematical structure called the conservative matrix field. This field unifies thousands of existing formulas, generates infinitely many new formulas, and unveils unexpected relations between different mathematical constants. The algorithm's discoveries also enable new mathematical proofs of irrationality and can be used to generalize proofs for the irrationality of specific constants. This research highlights the power of experimental mathematics and demonstrates the prospects of large-scale computational approaches to solving longstanding open problems and discovering connections across diverse fields of science.

The discussion on Hacker News about the submission includes various comments. 

- One user comments on the original arXiv link shared in the submission and notes that currently, the PDF viewer does not allow zooming or adjusting the UI to read the word comments properly.
- Another user expresses a desire for a proof of irrationality and wishes the research team good luck in their endeavors.
- One user praises the mathematicians for their work, while another points out that until proven, irrationality results personally do not get much attention.
- A user thanks for the fascinating rabbit hole regardless of any merit to the claims, and discusses the rapidly advancing field of experimental mathematics. They express interest in machine learning and AI's interesting results and understanding.
- Another user responds to the original submission's summary, stating that continued fraction formulas for mathematical constants in the form c = a0 + a1/(b1 + a2/(b2 + ...)) show polynomial-like behavior, but if we stop at a certain point, the continued fraction p_n/q_n grows extremely rapidly. They mention that limits of mathematical constants cannot be easily determined as well as identifying polynomials that correspond to mathematical constant combinations.
- A user brings up meaningful contexts and examples when discussing mathematical constants and mentions how certain natural numbers can be impressive by contradicting the concept of interesting numbers.
- Another user comments on the nature of mathematical constants, stating that they can be both silly rational and transcendental numbers consistently in different contexts, and that they are concepts not commonly encountered in mathematics.
- A user shares a YouTube video exploring the topic.

### 75% of Americans Believe AI Will Reduce Jobs

#### [Submission URL](https://news.gallup.com/opinion/gallup/510635/three-four-americans-believe-reduce-jobs.aspx) | 18 points | by [paulpauper](https://news.ycombinator.com/user?id=paulpauper) | [23 comments](https://news.ycombinator.com/item?id=37591481)

According to a new study conducted by Gallup, three out of four Americans believe that artificial intelligence (AI) will reduce the number of jobs in the U.S. over the next decade. Only 19% of respondents think that AI will have no impact on job numbers, while 6% believe it will actually increase job opportunities. The study also found that Americans generally have a negative view of AI's potential harm, with 40% thinking that it does more harm than good. However, they also recognize the benefits of AI in certain tasks, such as customizing online content, recommending products or services, and assisting students with coursework. The study also revealed that most Americans have low trust in businesses using AI responsibly. This highlights the need for businesses to demonstrate their commitment to using AI for positive purposes and to address the knowledge deficit surrounding AI among the general public.

The discussion on this submission revolves around the impact of AI on jobs and the potential benefits and drawbacks of AI advancement. One commenter suggests that AI will replace certain jobs that involve repetitive tasks, while another argues that jobs requiring creative thinking and human interaction will not be easily replaced. They specifically mention the threat that machine translation poses to the literary translation field, as well as the potential for AI to replace other creative professions and reduce the human touch in various industries. There is also a discussion about the socioeconomic impact of AI, with one commenter highlighting the potential shift in jobs and the need for workers to adapt to new roles. Another commenter argues that AI could lead to significant changes in the workforce, with certain jobs being destroyed and new ones being created. The conversation also touches on the potential limitations and challenges of AI, such as the complexity and fragility of AI systems. One commenter emphasizes that AI systems can malfunction or break, which could lead to negative consequences in various domains. They bring up issues related to the expertise required to fix AI malfunctions and the potential ethical concerns surrounding AI design. The discussion also delves into the philosophical and societal implications of AI. One commenter contemplates the possibilities of AI advancements replacing certain human activities, such as writing, painting, and exploring intellectual disciplines. They argue that with AI's ability to replicate knowledge work, certain professions may become obsolete, leading to new challenges and opportunities. Overall, the discussion on this submission reflects a range of perspectives on the impact of AI on jobs and society, highlighting the potential benefits and drawbacks.

### DALL·E 3

#### [Submission URL](https://openai.com/dall-e-3) | 668 points | by [davidbarker](https://news.ycombinator.com/user?id=davidbarker) | [480 comments](https://news.ycombinator.com/item?id=37586900)

OpenAI has announced DALL·E 3D, the latest version of its text-to-image model. DALL·E 3D is a significant improvement over its predecessor, DALL·E 2, as it better understands nuance and detail, allowing users to accurately generate images based on their textual descriptions. The new model is built on ChatGPT, enabling users to collaborate with it to refine their prompts and bring their ideas to life. DALL·E 3D will be available to ChatGPT Plus and Enterprise customers in October. OpenAI has also taken steps to prioritize safety, implementing measures to prevent the generation of violent, adult, or hateful content. The company is researching ways to help users identify AI-generated images and is developing a provenance classifier for this purpose. Furthermore, DALL·E 3D is designed to decline requests that imitate the style of living artists, and creators can choose to opt their images out from training future image generation models.

The discussion on this submission includes several different topics and perspectives. Here are some key points:

- Some users express their excitement about the announcement, while others have reservations and questions about the new DALL·E 3D model.
- There is a discussion about the potential copyright issues related to the generated images and the inclusion of famous artist names in the prompts.
- Users discuss the technical aspects of the model and suggest improvements, such as better control over image generation and the ability to download full prompts and images.
- The integration of DALL·E 3D with ChatGPT is highlighted as a significant development, with some users expressing their preference for the ChatGPT interface.
- The stability and reliability of the AI-generated images are discussed, along with the potential for further advancements in the field of AI and art generation.

Overall, there is a mix of excitement, curiosity, and suggestions for improvements in the discussion around DALL·E 3D.

---

## AI Submissions for Tue Sep 19 2023 {{ 'date': '2023-09-19T17:10:35.468Z' }}

### Graph Neural Networks use graphs when they shouldn't

#### [Submission URL](https://arxiv.org/abs/2309.04332) | 126 points | by [Pseudomanifold](https://news.ycombinator.com/user?id=Pseudomanifold) | [20 comments](https://news.ycombinator.com/item?id=37571535)

Graph Neural Networks (GNNs) have become a popular approach for learning on graph data in various domains. However, a new paper titled "Graph Neural Networks Use Graphs When They Shouldn't" by Maya Bechler-Speicher and her colleagues challenges the assumption that GNNs always make accurate predictions based on graph structure. The researchers show that GNNs tend to overfit the graph structure, even when it is non-informative for the predictive task. They provide a theoretical explanation for this phenomenon and propose a graph-editing method to mitigate the overfitting. The paper concludes with empirical evidence that this method improves the accuracy of GNNs across multiple benchmarks. This research has implications for the use of GNNs in fields such as social networks, molecular biology, and medicine.

The discussion on the submission revolves around various aspects of Graph Neural Networks (GNNs) and their use in learning on graph data. Here are some key points raised by the commenters:

- One commenter mentions that GNNs tend to overfit the graph structure, even when it is non-informative for the predictive task at hand. They provide links to the research paper challenging this assumption.
- Another commenter suggests that attention layers and nested graph convolution layers can help GNNs learn graph structures effectively.
- There is a discussion on the use of graph editing and graph representation in mitigating overfitting in GNNs.
- Some commenters share their experiences with working on GNNs and highlight the importance of studying the behavior and dynamics of GNNs.
- The limitations and challenges of using GNNs in practical applications are also mentioned, such as computational complexity and the need for regularization techniques.
- It is pointed out that GNNs can have a problem of overfitting due to imbalanced class distribution and dependence on specific graph interactions.
- Several papers and research works related to GNNs are shared, covering topics like message passing, algorithmic reasoning, diffusion, sparsity, training tricks, expressive power, and over-squashing.

Overall, the discussion highlights the potential issues and solutions related to the use of GNNs in various domains.

### The physical process that powers a new type of generative AI

#### [Submission URL](https://www.quantamagazine.org/new-physics-inspired-generative-ai-exceeds-expectations-20230919/) | 96 points | by [digital55](https://news.ycombinator.com/user?id=digital55) | [14 comments](https://news.ycombinator.com/item?id=37570743)

Physicists at MIT have introduced a new method of generative AI called the Poisson flow generative model (PFGM). Rather than using black box algorithms like traditional neural networks, PFGM is based on the principles of diffusion and the distribution of charged particles. PFGM represents data with charged particles that create an electric field, and the model learns to estimate that electric field through the training process. This allows PFGM to generate high-quality images, similar to diffusion-based models, but at a much faster speed. The use of physical processes in AI models could open the door to harnessing other physical phenomena to improve neural networks.

The discussion on Hacker News surrounding the submission about the Poisson flow generative model (PFGM) involves various perspectives. 

One commenter points out that the concept of Boltzmann Machines is nothing new, and the use of black box algorithms in neural networks has been replaced by diffusion-based models. Another commenter adds that implementing Poisson flow generative models could be challenging due to memory constraints, but a breakthrough in GPU RAM manufacturing could potentially solve this issue. The discussion then diverges into a debate about the reliance on AI SaaS subscription services and the associated costs.

Another thread of the discussion touches on the idea that physical processes can be effectively modeled in neural networks, opening up possibilities for incorporating other physical phenomena. However, a commenter from the World Economic Forum mentions the challenges in accurately predicting service waiting times.

Some comments express interest in the differences between Poisson flow generative models and traditional diffusion models, while others discuss the potential benefits of utilizing physical processes in network modeling.

One commenter wonders why the article does not provide direct comparisons between Poisson flow generative models and other diffusion models, while another commenter provides a link to relevant research papers.

There is also appreciation for the elegance of incorporating principles from physics into AI models. However, someone points out that counting on quantum computing to solve such problems might be wishful thinking.

Lastly, there is a discussion about how swapping decoding techniques in NLP frequently leads to generating novelty in text generation tasks, but traditional methods still have their merits. The debate focuses on the trade-off between control and novelty in generating text.

### 64-bit bank balances ‘ought to be enough for anybody’?

#### [Submission URL](https://tigerbeetle.com/blog/2023-09-19-64-bit-bank-balances-ought-to-be-enough-for-anybody/) | 237 points | by [todsacerdoti](https://news.ycombinator.com/user?id=todsacerdoti) | [347 comments](https://news.ycombinator.com/item?id=37568856)

TigerBeetle, a systems programming company, has decided to use 128-bit integers to store financial amounts and balances, moving beyond the previous use of 64-bit integers. While some may argue that 64 bits is sufficient, TigerBeetle realized they needed to go beyond this limit to store all kinds of transactions adequately. By using binary encoding, computers can represent numbers, and larger numbers require more bits. Fractions and decimal numbers pose challenges for computers, as they cannot accurately express decimal numbers using binary floating point. TigerBeetle solves this problem by representing money as whole numbers, using a minimal integer factor defined by the user. They also avoid using negative numbers and instead keep separate positive integer amounts for debits and credits. 128-bit integers are necessary to represent values smaller than a cent and meet the precision and scale requirements of various applications. TigerBeetle's database, called TigerBeetle, can count not only money but anything that can be modeled using double-entry accounting, such as inventory items or API calls. The company also considers the future-proof aspect of their system, as long-running systems can accumulate high transaction volumes over time. Unexpected events like hyperinflation can also push a currency towards the upper limits of a 64-bit integer, making 128-bit integers a necessary choice. Overall, TigerBeetle's decision to switch to 128-bit integers ensures more robust and flexible financial storage capabilities.

The discussion on this submission revolves around the use of decimal points and rounding in financial software. Some commenters discuss the problems that arise when handling fractions and decimals in computer systems and emphasize the importance of accurately calculating taxes and sales transactions. Others mention the different regulations and rules in different jurisdictions regarding rounding and decimal precision. Some commenters express surprise at the number of people who overlook decimal precision and make mistakes in billing and financial calculations. There is also discussion about the use of integer arithmetic and the limitations of binary representation in computers. Overall, commenters emphasize the need for precise and accurate financial calculations and highlight the challenges and potential pitfalls in implementing billing and accounting software.

### Google DeepMind's AI successor predicts how 71M mutations cause disease

#### [Submission URL](https://endpts.com/google-deepminds-alphafold-successor-predicts-how-71m-mutations-cause-disease/) | 52 points | by [birriel](https://news.ycombinator.com/user?id=birriel) | [7 comments](https://news.ycombinator.com/item?id=37578616)

Google DeepMind has announced the development of a new AI system called AlphaMissense. This technology is the successor to AlphaFold, which was known for its ability to predict the structures of proteins. AlphaMissense, on the other hand, focuses on predicting the likelihood that genetic mutations, specifically 71 million missense mutations, will cause disease. Each missense mutation refers to a single-letter change in an amino acid that makes up a protein sequence. The announcement of AlphaMissense comes alongside the publication of a paper in the journal Science.

The discussion surrounding the announcement of Google DeepMind's AlphaMissense AI system on Hacker News includes several comments:

1. User "blvl" wrote a quick script that checked 23andme data and found various percentages of mutated genes associated with different conditions.
2. User "kgtsmn" thanked "blvl" for the information and added percentages of mutated genes classified as benign, pathogenic, and likely benign across different classifications.
   a. "kgtsmn" also mentioned the MTHFR C677T mutation and its association with reduced enzyme activity and elevated homocysteine levels in individuals with decreased activity in the AA genotype.
3. "blvl" responded, stating that the rabbit hole goes deep and mentioned the SIRT1 mutation's association with longevity traits.
4. User "pknmd" expressed their understanding of 23andme data and questioned the need for medical professionals due to the self-reported nature of the data. They also found it interesting to compare the percentages of pathogenic and non-pathogenic mutated genes.
5. User "pfd1986" shared a link to the published paper in the journal Science regarding AlphaMissense and an additional link to a non-paywalled article about it.
6. User "PBnFlash" speculated about the potential impact of powerful AI systems like AlphaMissense on healthcare and medical research.
7. User "7e" made a general comment about experts not making decisive guesses.

Overall, the discussion involved users sharing their findings, questioning the need for medical professionals in analyzing genetic data, and discussing the potential implications of AI systems like AlphaMissense in the field of healthcare and genetics.

### The Princeton researchers calling out ‘AI snake oil’

#### [Submission URL](https://www.semafor.com/article/09/15/2023/the-princeton-researchers-calling-out-ai-snake-oil) | 32 points | by [irtefa](https://news.ycombinator.com/user?id=irtefa) | [7 comments](https://news.ycombinator.com/item?id=37576259)

The Princeton researchers, Arvind Narayanan and Sayash Kapoor, behind the popular newsletter and upcoming book "AI Snake Oil" are on a mission to dispel hype and clarify the limits of AI. They focus on distinguishing between predictive AI and generative AI, with most of the snake oil concentrated in predictive AI. They highlight the lack of statistical validity in certain predictive AI applications, such as AI hiring tools. They also express concerns about the potential flood of disinformation from generative AI, but argue that addressing other AI-related harms, like non-consensual deepfakes, should take precedence. The researchers propose that AI companies publish regular transparency reports to shed light on potential harms and usage patterns. They also discuss the need for better controls on the open access archive arXiv.org to prevent misinterpretation of AI research studies.

The discussion on this submission seems to cover a range of topics related to AI and its limitations:

1. Some commenters discuss the distinction between predictive AI and generative AI, with the consensus that most of the "snake oil" is concentrated in predictive AI.
2. A link is shared regarding the challenges of replacing scientific reproducibility with machine learning approaches.
3. The potential risks of AI are debated, with one comment suggesting that the greatest risk comes from humans controlling the technology.
4. There is a discussion about the potential shortcomings of GPT-4 when it comes to professional benchmarks and generating the correct answers to wrong questions.
5. The capabilities of ChatGPT as a "bullshit generator" are mentioned, with some being impressed by its ability to generate seemingly plausible responses.
6. A suggestion is made to focus on addressing non-consensual deepfakes and the spread of misinformation as priorities rather than the harms of generative AI.
7. The idea of companies publishing transparency reports to shed light on potential harms and usage patterns of AI is proposed.
8. Concerns are raised about the need for better controls on the open access archive arXiv.org to prevent misinterpretation of AI research studies.
9. A link to an archive discussing the potential dystopian aspects of AI is shared.
10. A commenter expresses their amusement with the ongoing discussion and suggests not taking it too seriously.

The conversation also includes some meta-discussion, with one commenter requesting others not to engage in shallow dismissals and to provide constructive criticism. Another commenter flags a comment as snarky.

### Tackling the curse of dimensionality with physics-informed neural networks

#### [Submission URL](https://arxiv.org/abs/2307.12306) | 75 points | by [jhoho](https://news.ycombinator.com/user?id=jhoho) | [17 comments](https://news.ycombinator.com/item?id=37565140)

In a recent paper submitted to arXiv, researchers Zheyuan Hu, Khemraj Shukla, George Em Karniadakis, and Kenji Kawaguchi introduce a new method for tackling the curse of dimensionality with Physics-Informed Neural Networks (PINNs). The curse of dimensionality refers to the heavy computational burden that exponentially increases as the dimensionality of a problem increases. The authors propose a method called Stochastic Dimension Gradient Descent (SDGD) that decomposes the gradient of Partial Differential Equations (PDEs) into pieces corresponding to different dimensions and randomly samples a subset of these dimensional pieces in each iteration of training PINNs. The proposed method has been experimentally demonstrated to solve notoriously hard high-dimensional PDEs, such as the Hamilton-Jacobi-Bellman and Schrödinger equations, in thousands of dimensions very quickly on a single GPU. In fact, the researchers were able to solve nontrivial nonlinear PDEs in 100,000 dimensions in just 6 hours on a single GPU using SDGD with PINNs. This new method has the potential to scale up the solving of arbitrary high-dimensional PDEs using PINNs.

The discussion on this submission covers various topics related to the dimensions and complexity of problems, as well as the potential advantages of quantum computers.

One user notes that in machine learning, vectors are typically considered to have numerical properties, while in physics, vectors can represent multiple dimensions. Another comment clarifies that the confusion arises from how different disciplines define and describe dimensions in their specific contexts.

Another user mentions that solving the Schrödinger equation in thousands of dimensions is possible for non-quantum mechanical systems, but it becomes more challenging for quantum-hard problems. A response to this comment suggests trying to solve the quantum harmonic oscillator potential, which is analytically solvable. However, another user points out that the Schrödinger equation is a separable differential equation that implies a specific network structure, which may not apply in general cases.

The discussion then moves to the advantages of quantum computers in solving complex problems. One user mentions that classical computers can calculate mean field energies for thousands of interacting electrons, but quantum computers have an advantage when it comes to calculating exchange correlation energies for interacting electrons. Another user adds that the evaluation of transition probabilities and energy differences can also be advantageous in quantum computers.

Ultimately, the comments touch on various aspects related to the dimensions and complexity of problems, highlighting differences in approaches between machine learning and physics, and discussing the potential advantages of quantum computers in solving complex equations.

---

## AI Submissions for Mon Sep 18 2023 {{ 'date': '2023-09-18T17:10:15.039Z' }}

### Self-supervised learning: The dark matter of intelligence (2021)

#### [Submission URL](https://ai.meta.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/) | 160 points | by [reqo](https://news.ycombinator.com/user?id=reqo) | [18 comments](https://news.ycombinator.com/item?id=37558813)

The AI field has made great strides in developing AI systems that can learn from labeled data, but there is a limit to how far supervised learning can take us. Supervised learning is not sufficient for building more intelligent generalist models that can perform multiple tasks and acquire new skills without massive amounts of labeled data. To address this limitation, researchers believe that self-supervised learning (SSL) may hold the key to unlocking the "dark matter" of intelligence in AI systems.

SSL enables AI systems to learn from vast amounts of unlabeled data, allowing them to recognize and understand more subtle and less common representations of the world. SSL has already shown great success in natural language processing (NLP), with models like BERT and RoBERTa achieving higher performance than those solely trained in a supervised manner. Recent research projects, such as SEER, have demonstrated that SSL can also excel in computer vision tasks.

Self-supervised learning works by obtaining supervisory signals from the data itself, leveraging the underlying structure in the data. For example, in NLP, a system can hide part of a sentence and predict the hidden words from the remaining words. In computer vision, it can predict future frames in a video from the current ones. By using the structure of the data, self-supervised learning can make use of a variety of supervisory signals without relying on labeled data.

Self-supervised learning has had a significant impact on NLP, enabling models to be pretrained on large unlabeled text datasets and then fine-tuned for specific tasks. However, applying SSL to computer vision tasks is a relatively new frontier. Researchers are exploring energy-based models, joint embedding methods, and latent-variable architectures to further advance self-supervised learning and reasoning in AI systems.

By combining supervised learning with SSL, AI systems can develop a deeper, more nuanced understanding of the world. This can bring us closer to achieving human-level intelligence and enable AI systems to learn new skills without requiring massive amounts of labeled data for each task. Self-supervised learning holds great promise in the quest to unlock the dark matter of intelligence in AI.

The discussion on this submission revolves around various aspects of self-supervised learning (SSL) and its potential in advancing artificial intelligence (AI) systems. Some key points from the comments include:

- The success of SSL in natural language processing (NLP) is noted, with models like BERT and RoBERTa achieving high performance by leveraging large unlabeled text datasets.
- There is a mention of different techniques in SSL, such as SimCLR, BYOL, and masking-based models, and their application in NLP and computer vision tasks.
- The use of SSL in computer vision is considered a relatively new area of exploration.
- The importance of SSL in addressing the limitations of supervised learning and achieving a deeper understanding of the world is emphasized.
- LeCun's contrastive learning course materials are recommended as a resource for understanding SSL.
- There is a discussion on the concept of "dark knowledge" and how AI systems can benefit from accessing subtle and implicit information present in unlabeled data.
- The role of humans in solving arbitrary problems and the capabilities of AI systems in comparison are debated.
- A study exploring the philosophical aspects of dark matter intelligence is suggested as reading material.
- One commenter mentions feeling the presence of dark matter intelligence in the industry and its potential in resolving complex issues.

Additionally, one comment redirects readers to a related Twitter thread for more information.

### Data accidentally exposed by Microsoft AI researchers

#### [Submission URL](https://www.wiz.io/blog/38-terabytes-of-private-data-accidentally-exposed-by-microsoft-ai-researchers) | 699 points | by [deepersprout](https://news.ycombinator.com/user?id=deepersprout) | [218 comments](https://news.ycombinator.com/item?id=37556605)

In a recent mishap, Microsoft's AI research team accidentally exposed 38 terabytes of private data on GitHub. The exposed data includes a backup of two employees' workstations, containing secrets, private keys, passwords, and over 30,000 internal Microsoft Teams messages. The researchers shared their files using an Azure feature called SAS tokens, which allows for data sharing from Azure Storage accounts. However, in this case, the access level was not properly limited, resulting in the unintended exposure. This incident highlights the importance of proper management and monitoring of SAS tokens to avoid potential security risks.

The discussion on this submission covers a variety of topics related to the accidental exposure of Microsoft's private data. One user suggests that AI models should be serialized in a secure format to prevent malicious injection, while another user raises concerns about targeted attacks and the potential manipulation of training data. There is also discussion about the risks of dynamically typed languages and the importance of proper security measures. The conversation touches on topics such as log4j vulnerability, password security, encryption, and the use of programming languages. Some users advocate for stricter language typing, while others argue that programming language choice is not the main issue. There is also a brief discussion about non-encrypted PDFs and the comparison between Microsoft Office and LibreOffice. Overall, the discussion highlights the complexities and challenges of securely managing and protecting data in the context of AI research.

### GPT 3.5 vs. Llama 2 fine-tuning: A Comprehensive Comparison

#### [Submission URL](https://ragntune.com/blog/gpt3.5-vs-llama2-finetuning) | 46 points | by [samlhuillier](https://news.ycombinator.com/user?id=samlhuillier) | [12 comments](https://news.ycombinator.com/item?id=37560125)

In a recent post, the author shares their experiments comparing the fine-tuning performance of GPT 3.5 and Llama 2 in an SQL task and a functional representation task. They found that while GPT 3.5 performed slightly better on both datasets, the cost of training and deploying it was significantly higher. The author provides code and data for both tasks and explains that they wanted to explore the possibility of achieving comparable performance with manual fine-tuning at a lower cost. They used subsets of the Spider dataset and the Viggo functional representation dataset, which are known for teaching structured outputs rather than facts. The author also details the setup of their experiments, including the decision to use Code Llama 34B and Lora fine-tuning. They conclude that while fine-tuning GPT 3.5 may be suitable for initial validation or MVP work, models like Llama 2 might be more cost-effective for advanced tasks.

The discussion among Hacker News users on this post covers several topics related to the comparison between GPT 3.5 and Llama 2, as well as the considerations for cost and lifetime memberships with OpenAI. Here are the key points:

1. Some users express concerns about the cost of using Llama, particularly in comparison to GPT 3.5, suggesting that the lifetime memberships offered by OpenAI do not make sense considering the high ongoing costs of using the models.

2. Others comment on the practicality of relying on cloud computing and the theory behind it. They argue that it may not be the best approach for long-term projects, highlighting the importance of considering cost and scalability.

3. One user mentions that the terminology "functional representation dataset" is not well-defined, but they acknowledge the potential benefits of using structured propositional knowledge, citing examples like Viggo.

4. Another user expresses their struggles in finding good datasets for fine-tuning and asks for tips on creating sufficient datasets for specific use cases.

5. One user expresses interest in a similar comparison involving the RAG model and tasks related to it.

6. A user mentions that the notebook shared in the post demonstrates a reproducible evaluation process that correlates with general value and control evaluation.

Overall, the discussion revolves around the trade-offs between cost, performance, and the practicality of using different models for fine-tuning tasks. Some users express interest in alternative approaches and datasets for structured outputs and long-term projects.

### Those trying to pick AI winners should remember the dotcom days

#### [Submission URL](https://www.ft.com/content/82168156-006f-4d75-a4e9-0b6bdccef3b2) | 30 points | by [ent101](https://news.ycombinator.com/user?id=ent101) | [7 comments](https://news.ycombinator.com/item?id=37559105)

As AI continues to dominate conversations in the tech industry, it's crucial to remember the lessons learned from the dotcom era. The dotcom bubble burst in the early 2000s, leaving many startups and investors in shambles. This article highlights the importance of being cautious and realistic when evaluating AI winners, as history has shown that not every promising technology lives up to the hype.

he author emphasizes the need to exercise caution when assessing the potential winners in the AI space. Just like during the dotcom era, where everyone believed that the internet would revolutionize the world, there is now an overwhelming optimism around AI. However, it is crucial to separate the hype from reality and carefully evaluate each AI technology's actual capabilities and applications.

The article raises important questions for investors and entrepreneurs in the AI space. It reminds them to consider the scalability, practicality, and long-term sustainability of AI solutions before making any commitments. While AI holds immense potential, it's important not to get carried away by lofty promises and to remain realistic about the challenges and limitations that AI technologies face.

Overall, this article serves as a valuable reminder to analyze AI winners through a critical lens and to approach the AI landscape with the lessons learned from the dotcom bubble in mind. It encourages readers to seek a balance between optimism and caution and to make informed decisions when navigating the AI ecosystem.

### Stephen Fry says his voice stolen from Harry Potter audio books,replicated by AI

#### [Submission URL](https://fortune.com/2023/09/15/hollywood-strikes-stephen-fry-voice-copied-harry-potter-audiobooks-ai-deepfakes-sag-aftra-simon-pegg-brian-cox-matthew-mcconaughey/) | 26 points | by [thunderbong](https://news.ycombinator.com/user?id=thunderbong) | [23 comments](https://news.ycombinator.com/item?id=37554055)

Actor Stephen Fry has spoken out about the potential harm of AI in Hollywood, specifically regarding the use of AI to replicate actors' voices without their permission. Fry, who is a member of the actors' union SAG-AFTRA, mentioned his personal experience of having his identity digitally cloned and played a clip of an AI system mimicking his voice at the CogX Festival in London. He warned that AI technology is advancing rapidly and could soon produce deepfake videos that are just as convincing. Other actors, including Brian Cox and Simon Pegg, have also expressed concerns about AI in the film industry.

The discussion on Hacker News revolves around various aspects of AI replication of actors' voices and the potential implications. Some users express skepticism, comparing AI voice replication to long-standing celebrity impersonators and suggesting that legal theories might be able to cover this issue. Others discuss the technical aspects of AI voice cloning and mention Brian Blessed's distinctive voice. 

One user points out that AI recordings of coworkers in web meetings have been created, implying that the theft of voices is not a new issue. Another mentions the history of AI and its impact on various industries. 

The conversation also touches on the debate of whether AI can replace human creativity and whether AI-generated content can be considered art. One user references Walter Benjamin's 1935 philosophy and the implications of AI replication on artistic expression. 

There is a discussion about the commercial applications of AI voice cloning and how it could be used in large-scale projects. The post raises the question of whether actors' consent should be required to replace their voices in certain circumstances.

Some users argue that the focus should be on other more pressing global issues, such as climate change and humanitarian crises, rather than worrying about AI voice cloning. One user suggests that the discussion is radical and the focus should be shifted.