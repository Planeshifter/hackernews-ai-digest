import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Sat Nov 18 2023 {{ 'date': '2023-11-18T17:10:54.633Z' }}

### Frigate: Open-source network video recorder with real-time AI object detection

#### [Submission URL](https://frigate.video/) | 540 points | by [thunderbong](https://news.ycombinator.com/user?id=thunderbong) | [122 comments](https://news.ycombinator.com/item?id=38321413)

Frigate is an open source Network Video Recorder (NVR) that brings real-time AI object detection to your security camera system. What sets Frigate apart is that all the processing is done locally on your own hardware, ensuring that your camera feeds never leave your home. With Frigate, you can say goodbye to false positives and tedious video reviews.

The traditional NVRs often rely on simple motion detection, which can result in a lot of false positive alerts. Frigate tackles this problem by offloading object detection to the powerful Google Coral TPU. This allows even modest hardware to run advanced analysis and determine if the detected motion is actually a person, car, or any other object of interest. By processing everything locally, you don't need to pay for your personal camera footage to be sent to the cloud for analysis.

One of the standout features of Frigate is its ability to reduce false positives. With a single Google Coral TPU, Frigate can run over 100 object detections per second, ensuring that no frame is missed. This means you can stop wasting time reviewing shadows and windy scenes and focus on the detections that truly matter.

Frigate also offers the option to fine-tune your events and alerts using zones. With real-time object tracking, Frigate can precisely determine when a person is walking up your front steps or when a car enters your driveway. This allows you to refine your notifications based on specific locations, making your security system more efficient and tailored to your needs.

In terms of integration, Frigate seamlessly integrates with popular automation platforms like Home Assistant, OpenHab, NodeRed, and anything with MQTT support. By integrating object detection into these platforms, you can give your home eyes and create powerful automations and notifications based on the real-time data provided by Frigate.

To make things even better, Frigate offers Frigate+, a subscription plan that gives you access to custom models designed specifically for Frigate. This allows you to further enhance the performance and accuracy of your security camera system.

Users have been raving about Frigate's customizability, fast object detection, and seamless integration with Home Assistant. Many have praised how Frigate has helped them eliminate false detections and reduce the need to search through uneventful footage. The support for Frigate has also been highly praised, making it a highly recommended choice for those seeking a locally controlled and feature-rich security camera system.

If you're tired of dealing with false positives and want a locally processed AI solution for your security cameras, Frigate might be the perfect fit for you. Stay tuned for its release and get ready to take your security camera system to the next level.

The discussion around the submission "Introducing Frigate: Monitor your security cameras with locally processed AI" on Hacker News is quite positive. Users have shared their experiences and thoughts on Frigate, highlighting its customizability, fast object detection, and seamless integration with Home Assistant.

One user, prk, has been using Frigate for months with a Raspberry Pi 4 and Google Coral TPU. They mention that Frigate works smoothly and effectively in object detection, eliminating false positives and negatives. They have integrated Frigate with Home Assistant for notifications on their phone and have found it to be a reliable solution.

Another user, Aspos, states that Frigate is worth the price and mentions that they have programmed smart bulbs to react based on the detections made by Frigate. They seem to be pleased with the performance and reliability of Frigate in their home.

Some users discuss the possibilities and use cases of Frigate. There is a mention of using zone-specific detection for more targeted notifications. There is also a discussion about using Frigate for detecting specific events such as Halloween costumes or detecting smoke.

Users also discuss the hardware requirements and scalability of Frigate. Some mention using larger hardware setups with multiple cameras and Intel processors. Others discuss the affordability of Frigate compared to commercial options and the benefits of a local AI solution.

There are also discussions about different aspects of Frigate, such as motion detection, object detection, fine-tuning, and support for different devices. Users share their experiences and offer suggestions for improvement, such as integrating MQTT support and enhancing the user interface.

Overall, the discussion reflects positive experiences with Frigate and highlights its features, performance, and integration capabilities. Users seem satisfied with the ability of Frigate to eliminate false positives and provide targeted notifications, making it a recommended option for those looking for a locally processed AI solution for their security camera system.

### A software epiphany

#### [Submission URL](https://johnwhiles.com/posts/programming-as-theory) | 105 points | by [jwhiles](https://news.ycombinator.com/user?id=jwhiles) | [95 comments](https://news.ycombinator.com/item?id=38324486)

In a recent episode of the Future of Coding podcast, the concept of software development as theory building was explored, offering insights into why some engineers seem like geniuses and why some teams struggle while others succeed. The podcast discussed Gilbert Ryle's definition of a theory as a thought object that exists in our minds, allowing us to perform certain tasks. It emphasized that programming is not just about creating code, but about building a mental theory of that codebase. The theory enables engineers to create, diagnose, and modify the codebase effectively. The podcast also highlighted the importance of having team members who have been there from the start and gradually integrating new members, as well as the negative consequences of losing individuals with a deep understanding of the codebase. This theory-building model helps explain phenomena such as legacy code, the effectiveness of solo engineers, the difficulty of getting up to speed on unfamiliar projects, and the challenges of outsourcing or hiring contractors. Overall, the episode offered a perspective on the underlying nature of software development and the significance of retaining knowledgeable software engineers.

The discussion on this submission covered various aspects related to the concept of theory building in software development. Some commenters pointed out that understanding and building a mental theory of the codebase is crucial for effective development. They discussed the challenges of comprehending and working with legacy code and the difficulties of integrating new team members without deep knowledge of the codebase. The discussion also touched on the risks of losing individuals with a deep understanding of the codebase and the negative consequences of outsourcing or hiring contractors. Some commenters expanded on the concept of collective understanding and the importance of maintaining conceptual integrity in software development. There were also mentions of related topics such as the second-system effect and the role of documentation. Additionally, a few commenters shared their personal experiences and perspectives on the matter.

### I disagree with Geoff Hinton regarding "glorified autocomplete"

#### [Submission URL](https://statmodeling.stat.columbia.edu/2023/11/18/i-disagree-with-geoff-hinton-regarding-glorified-autocomplete/) | 187 points | by [magoghm](https://news.ycombinator.com/user?id=magoghm) | [240 comments](https://news.ycombinator.com/item?id=38320698)

The "godfather of AI," Geoff Hinton, believes that chatbots, often dismissed as glorified autocomplete, actually possess a deeper level of understanding. By training them to predict the next word, they are forced to comprehend the context. This idea resonates with the author, who finds themselves providing "glorified autocomplete" in meetings. They act as a sort of FAQ, connecting ideas and offering insights. While shallow responses can be effective, there comes a point where deep thinking is required. This is akin to the difference between jogging and running, with the latter demanding more concentration. The author also notices this pattern during talks and observations of others. It seems to align with psychology's theories of associative and logical reasoning, with intuition being fast and automatic, while reasoning involves conscious judgments and attitudes. However, the author's "glorified autocomplete" thinking requires more intention and is not purely automatic.

The discussion in the comments revolves around different interpretations and opinions on the capabilities and limitations of Language Models (LLMs) like ChatGPT. Some users argue that LLMs do not possess true understanding or consciousness, while others point out that they are trained to generate responses based on patterns in training data rather than having a literal understanding. There is also a discussion on the distinction between conceptually true and false answers and how LLMs handle them. One user debunked the misconception that LLMs have self-knowledge or consciousness, explaining that they rely on patterns in training data for generating responses. Additionally, there is a debate about the relevance and accuracy of LLMs in representing the real world and the extent to which they understand it. Some users argue that LLMs are fundamentally incomplete in their representation of the world, while others believe they can accurately model certain aspects. Finally, there is a discussion on the limitations of modeling consciousness and deliberate processes in LLMs and the potential misunderstanding of their capabilities.

### Google is embedding inaudible watermarks into its AI generated music

#### [Submission URL](https://www.theverge.com/2023/11/16/23963607/google-deepmind-synthid-audio-watermarks) | 130 points | by [CharlesW](https://news.ycombinator.com/user?id=CharlesW) | [88 comments](https://news.ycombinator.com/item?id=38321324)

Google is taking steps to improve transparency and accountability in its AI-generated music by embedding inaudible watermarks into the audio. This will allow people to identify tracks that have been created using Google DeepMind's AI Lyria model. The watermark, called SynthID, is designed to be undetectable to the human ear and can still be identified even if the audio is compressed, sped up or down, or has additional noise added. SynthID works by converting the audio wave into a two-dimensional visualization that shows how the frequency spectrum evolves over time. Watermarking tools like SynthID are seen as important safeguards against the potential harms of generative AI, although they are not foolproof against extreme manipulations. This move aligns with President Joe Biden's executive order on AI, which calls for government-led standards for watermarking AI-generated content.

The discussion on this submission revolves around various aspects of watermarks in AI-generated music. One user mentions that there are several ways to encode digital signals that could survive compression, but they may not be detectable to the human ear. Another user discusses the potential applications of AI-generated voice recordings, such as scams or impersonation. There is also a conversation about the challenges and potential solutions for removing watermarks through compression. Some users mention the importance of watermarking as a safeguard against potential harms of generative AI, while others express concerns about the impact on human-generated music or the potential misuse of watermarks. Additionally, there are discussions about the perception of certain frequencies in music, the concept of copyright, and the limitations of audio compression algorithms.

### OpenAI has received just a fraction of Microsoft's $10B investment

#### [Submission URL](https://www.semafor.com/article/11/18/2023/openai-has-received-just-a-fraction-of-microsofts-10-billion-investment) | 18 points | by [CharlesW](https://news.ycombinator.com/user?id=CharlesW) | [3 comments](https://news.ycombinator.com/item?id=38324233)

OpenAI has reportedly received only a portion of Microsoft's $10 billion investment, with a significant amount being in the form of cloud compute purchases rather than cash. This gives Microsoft leverage in its relationship with OpenAI following the recent ousting of CEO Sam Altman. Microsoft CEO Satya Nadella allegedly believes that OpenAI's directors mishandled Altman's firing, and there are concerns about the stability of the partnership. However, Microsoft still has rights to OpenAI's intellectual property and could potentially run the company's models on its own servers if the partnership breaks down. OpenAI COO Brad Lightcap has assured employees that the board's decision was not due to any misconduct and that the company's position remains strong. The situation is still being resolved, and interim CEO Mira Murati has the board's full support.

The discussion on Hacker News revolves around the details of Microsoft's investment in OpenAI and the potential implications for the partnership between the two companies. One user points out that Microsoft's investment includes a significant amount of cloud compute purchases rather than cash, giving Microsoft leverage in their relationship with OpenAI. There are concerns about the stability of the partnership following the recent departure of OpenAI CEO Sam Altman and Microsoft CEO Satya Nadella's alleged dissatisfaction with how Altman's firing was handled by OpenAI's directors.

Some users argue that Microsoft's investment in OpenAI could be seen as a strategic move to gain access to OpenAI's intellectual property and potentially run the company's models on its own servers if the partnership breaks down. Others comment on the financial aspects of the investment, with one user emphasizing that investments often come in installments and it's not uncommon for a portion of the funding to be in the form of cloud compute purchases.

Another user highlights how OpenAI's mission of benefiting humanity and its commitment to funding research to ensure its models are used safely contrasts with Microsoft's profit-driven approach. They posit that Sam Altman's contributions to OpenAI have helped build a great software over the years and question the extent to which Microsoft's involvement might undermine OpenAI's original goals.

Additionally, there is a brief comment from a user stating that the discussion has become a deterministic problem, suggesting that the conversation has devolved into repeated arguments.

### Dropbox and Nvidia Team to Bring Personalized Generative AI to Customers

#### [Submission URL](https://nvidianews.nvidia.com/news/dropbox-and-nvidia-team-to-bring-personalized-generative-ai-to-millions-of-customers) | 9 points | by [ianrahman](https://news.ycombinator.com/user?id=ianrahman) | [4 comments](https://news.ycombinator.com/item?id=38322677)

Dropbox and NVIDIA have teamed up to bring personalized generative AI to millions of Dropbox customers. The collaboration aims to enhance Dropbox's AI functionality by leveraging NVIDIA's AI Foundry, which includes AI Foundation Models, AI Enterprise software, and accelerated computing. The partnership will introduce new uses for personalized generative AI, improving search accuracy, organization, and workflow simplification. Dropbox plans to utilize NVIDIA's technology to deliver more personalized, AI-powered experiences to its customers. The collaboration will pave the way for Dropbox customers to accelerate their work with customized generative AI applications. By incorporating NVIDIA's tools, Dropbox can bring more intelligence to its customers' content and workflows. This collaboration represents a step forward in using AI to transform knowledge work and address pain points related to organization, prioritization, and focus.

In the discussion, one user compares the pricing of Dropbox to Backblaze and suggests using the latter as a more cost-effective alternative for cloud storage. Another user mentions having used Dropbox for 10 years but switched to another provider due to the high costs. They also criticize Dropbox for its limited storage quotas and express frustration with their customer support. Another user reports the FTC's fraudulent exclusive offer complaint against Dropbox and expresses disappointment in their handling of customer accounts and shared content.

#### [Submission URL](https://www.technologyreview.com/2023/10/26/1082398/exclusive-ilya-sutskever-openais-chief-scientist-on-his-hopes-and-fears-for-the-future-of-ai/) | 121 points | by [monort](https://news.ycombinator.com/user?id=monort) | [106 comments](https://news.ycombinator.com/item?id=38316521)

Ilya Sutskever, co-founder and chief scientist of OpenAI, is shifting his focus from building the next generation of generative models to figuring out how to prevent artificial superintelligence from going rogue. Sutskever believes that the development of artificial general intelligence (AGI) is inevitable, and he wants to ensure that it is controlled for the benefit of humanity. He also thinks that ChatGPT, OpenAI's chatbot model, may be conscious to some extent. Sutskever's views on the future of AI and merging humans with machines are seen as wild by some, but the rapid progress in AI technology is making his predictions more likely. Since OpenAI's release of ChatGPT, the company has gained significant attention, with world leaders seeking private audiences and the CEO, Sam Altman, conducting outreach tours. Despite OpenAI's fame, Sutskever remains a private figure who rarely gives interviews and leads a simple life focused on his work. He started his career in AI under Geoffrey Hinton at the University of Toronto and played a key role in the development of deep learning, including the creation of the influential AlexNet neural network. The adoption of graphics processing units (GPUs) for training neural networks, which Sutskever and his team utilized, played a major role in the success of deep learning.

The discussion on this submission covers a range of perspectives on the future of AI and its implications. Some commenters express skepticism about the ability to control advanced technologies and highlight the potential dangers of AI development, drawing parallels to the invention of nuclear weapons. Others argue that the focus should be on technical advancements and innovation rather than trying to prevent the development of AGI. There is also debate about the role of philosophers and economists in understanding and influencing technological advancements. Some commenters bring up historical examples, such as the invention of the atomic bomb, to argue for the importance of considering the broader implications of technological developments. There is also discussion about the potential impact of AI on employment and the need for regulation in AI development. Overall, the discussion reflects a range of opinions on the future of AI and its implications for society.

### Who Is Mira Murati, OpenAI's New CEO?

#### [Submission URL](https://www.wired.com/story/openai-new-ceo-who-is-mira-murati/) | 66 points | by [pranay01](https://news.ycombinator.com/user?id=pranay01) | [36 comments](https://news.ycombinator.com/item?id=38312617)

In an interview conducted in July 2023, Mira Murati, the former CTO of OpenAI, discusses her journey to join the company and her role in ensuring the responsible development of AI technology. She highlights key milestones during her tenure, such as GPT-3's ability to translate different languages. Murati also addresses the transition of OpenAI from a nonprofit to a for-profit entity, emphasizing the need for funding to deploy AI models at scale while protecting the mission of the nonprofit. When asked about partnering with Microsoft, she acknowledges the alignment in believing in OpenAI's mission but recognizes that it's not Microsoft's primary objective. Murati discusses the transformation of OpenAI from a research lab to a product company and the need for continuous adaptation in society. She shares insights into the development of Dall-E, an AI model that generates images, including the involvement of creatives and the potential for AI models to enhance human creativity. Murati asserts that the intentional release of OpenAI's products prompts society to grapple with issues like copyright and job automation, highlighting the importance of responsible deployment and integration of AI technology.

Discussion Summary:

- One commenter notes that Mira Murati's responses in the interview seem to prioritize safety and responsible development over rapid technological progress. Another person shares optimism about the potential release of GPT-4 forcing public dialogue around AI ethics. Murati, however, suggests that AI progress continues rapidly, and it is crucial to resist oversimplifying the issues at hand.
- The discussion shifts to the background of Mira Murati, noting her experience in various technology-related roles, including working at Tesla and a VR company. Some commenters question the relevance of her credentials and the motivations behind the interview.
- A debate ensues regarding the significance of Mira Murati's role as CTO and CEO of OpenAI, with contrasting opinions on her capabilities and experience. Some argue that her background in engineering and leadership positions in different companies makes her suitable for the role, while others express doubts.
- A few commenters discuss the business aspects of OpenAI, such as the transformation from a research lab to a product company and the partnership with Microsoft. The potential competition between OpenAI and other companies is also mentioned.
- A commenter raises doubts about the sudden departure of Sam Altman from OpenAI, suggesting that it may have been due to his work at Y Combinator. Others debate the significance of this speculation.
- Some individuals express skepticism about Mira Murati's capabilities based on their perception of her past technical leadership positions and the reputation of the companies she worked for.
- There are varying opinions on Mira Murati's qualifications, with some emphasizing her impressive career trajectory and others questioning her level of expertise.
- One commenter praises Murati's intelligence, charisma, and passion, particularly highlighting their experience working together at Leap Motion.
- A person with a PhD degree argues that six years of experience, including work as a CTO, is reasonable for someone in a high-achieving position.
- The discussion ends with a flagged comment expressing disagreement with the previous comment and highlighting the need for a more constructive conversation.

Overall, the discussion involves debates about Mira Murati's qualifications, OpenAI's business decisions, and the potential impact of AI technology on society. Some commenters are supportive of Murati, while others express doubts or skepticism.

---

## AI Submissions for Fri Nov 17 2023 {{ 'date': '2023-11-17T17:10:34.732Z' }}

### Show HN: nbi.ai – Generative Business Intelligence

#### [Submission URL](https://www.narrative.bi/ai) | 94 points | by [fromthegut](https://news.ycombinator.com/user?id=fromthegut) | [26 comments](https://news.ycombinator.com/item?id=38310502)

NBI.AI, a generative AI platform for business intelligence, has released their latest update. The platform aims to drive growth by providing AI-generated data narratives that deliver actionable insights with just a few clicks. With NBI.AI, users can automate reporting with natural language stories, making it easier to understand complex analytics. The platform generates insights in plain language, eliminating technical jargon and complex data interpretations. NBI.AI also offers weekly AI-powered insights on marketing performance, as well as tools to compare and evaluate ad performance, identify top performers, and analyze conversion journeys. The platform integrates seamlessly with marketing and advertising sources, allowing users to connect in just two clicks. NBI.AI is trusted by over 2,000 growth teams worldwide and offers a 7-day free trial.

The discussion on the submission about NBI.AI, a generative AI platform for business intelligence, covers several topics. Here are the key points:

- One commenter mentions that they are skeptical about AI-driven decision-making tools and prefer a context-leading rule-based natural language generation approach. They expect divergence between rule-based statistical inference narratives and traditional business intelligence data interpretations.
- The founder of NBI.AI responds, providing additional information about their product and its capabilities. They mention that the platform was built to connect virtually structured data sources and has already helped over 2,500 teams gain insights from marketing data.
- Another commenter shares their experience with using narrative-based projects. They use high-level reports that highlight month-over-month changes in website traffic and use an alternative GA4 UI for detailed insights. They plan to implement dimensional analysis to further understand their data.
- The discussion touches on the use of AI in natural language generation and the importance of accuracy and pre-processing to ensure high-quality narratives.
- There is a brief exchange about using automation tools for basic workflows, such as checking invoices and renaming files based on invoice IDs.
- Examples of use cases for NBI.AI are shared, including reporting, anomaly detection, and natural language insights generation for marketing campaigns.
- The founder of NBI.AI clarifies that the training data used for the platform comes from various sources and is focused on behavioral data preferences and feedback to provide personalized insights.
- A user discusses their experience as a marketing specialist and mentions that instead of creating PowerPoint presentations with performance graphs and narrative ROAS, they would prefer using NBI.AI.
- There is a brief discussion about integration plans for NBI.AI and suggestions for additional features.
- Some users express their skepticism about AI-generated data narratives, mentioning that they tend to sound like corporate jargon and lack substance.
- The founder of NBI.AI responds to the feedback, stating that historically they have focused on growth, marketing, and sales data narratives, and that the AI-generated insights are written in natural language.
- There is a discussion about the interpretation and understanding of AI-generated data narratives and the importance of connecting data from various sources to generate focused growth insights and recommendations.

Overall, the discussion provides a mix of skepticism and interest in AI-generated data narratives, with some users sharing their own experiences and suggestions. The founder of NBI.AI actively participates in the discussion, addressing concerns and providing more information about the platform.

### Unauthorized "David Attenborough" AI clone narrates developer's life, goes viral

#### [Submission URL](https://arstechnica.com/information-technology/2023/11/unauthorized-david-attenborough-ai-clone-narrates-developers-life-goes-viral/) | 227 points | by [seasicksteve](https://news.ycombinator.com/user?id=seasicksteve) | [187 comments](https://news.ycombinator.com/item?id=38302319)

In a creative and unauthorized experiment, developer Charlie Holtz combined GPT-4 Vision and ElevenLabs voice cloning technology to create an AI version of David Attenborough narrating his every move on camera. Holtz used a Python script called "narrator" to take a photo from his webcam every five seconds and feed it to GPT-4V, which processed the image and generated Attenborough-style text. This text was then fed into an ElevenLabs AI voice profile trained on Attenborough's speech. The demo video of the experiment has gained significant attention on social media, with mixed reactions from the audience. While some expressed discomfort with imitating Attenborough's voice without permission, others found the demonstration amusing and creative.

The discussion on the submission starts with a comment questioning the ethical concerns of voice cloning and replicating famous individuals. Another user points out that the technology allows for the creation of commercial narrations in the styles of famous voices like Attenborough and Freeman. The conversation then shifts to a debate about the significance and influence of classic works of literature and how technology can impact their reproduction. Some users argue that technological advancements have made it easier for classics to be produced and distributed, while others argue that the quality and cultural impact of works from different time periods cannot be easily compared. Another user brings up the idea that generations often have different points of reference and familiarity with certain things, which affects artistic expression and experimentation. One user mentions a BBC documentary narrated by David Attenborough. The conversation then diverts to a discussion about the recycling of cultural content and the push for profit and nostalgia. Some users express concerns about the lack of originality and artistic challenge in replicating older works, while others discuss the dynamics of the entertainment industry and how content creation and consumption have evolved. One comment suggests that AI could potentially create new episodes of old shows like Inspector Gadget. However, another user disagrees, stating that AI-generated content eliminates creativity and renders results meaningless. The conversation then touches on the craftsmanship involved in animation and the varying levels of effort put into different animation styles. The discussion concludes with a mention of a science fiction character, Duncan Idaho.

### A PCIe Coral TPU Finally Works on Raspberry Pi 5

#### [Submission URL](https://www.jeffgeerling.com/blog/2023/pcie-coral-tpu-finally-works-on-raspberry-pi-5) | 110 points | by [mikece](https://news.ycombinator.com/user?id=mikece) | [20 comments](https://news.ycombinator.com/item?id=38308552)

The Raspberry Pi 5 can now natively support the PCIe Coral TPU, an AI accelerator used for tasks like machine vision and audio processing. Previously, getting the PCIe Coral TPU to work on a Raspberry Pi was challenging due to quirks in the Compute Module 4's PCIe implementation. However, with the improved PCIe bus on the Raspberry Pi 5, it is now possible, although a few tweaks are required. These include switching to a 4K page size, disabling PCIe ASPM, and making changes to the device tree. Additionally, due to compatibility issues, running the Coral's PyCoral library requires either Docker or installing an alternate system-wide Python version. While there are no commercially-available HATs or adapter boards for connecting the Coral TPU to the Raspberry Pi 5's PCIe header, options like the HatDrive! Top or Bottom from Pineberry Pi or the Coral B+M key module with an appropriate adapter can be used. Once set up, the Coral TPU can be used for various AI tasks, such as image classification. Overall, this development opens up new possibilities for AI acceleration on the Raspberry Pi platform.

Some notable points from the discussion on Hacker News about the Raspberry Pi 5's PCIe support for the PCIe Coral TPU are:

- The comparison is made between various AI accelerators, including HBM3E HAT mk TPUs, NVIDIA Jetson Nano, NVIDIA Orin Nano and AGX, and Coral Mini-PCIe. The discussion includes the TPU's computing power, Tensor Processing Units (TPU) architecture, DLSS architecture, and Vision and Versatile Processor Units (VPU).
- One user mentions the Radxa Rock 5B's NPU, which supports various types of acceleration such as INT4, INT8, INT16, FP16, BF16, and TF32 with a computing power of 6TOPs.
- The Coral TPU's software requirements are discussed, including the need for Python 3.9, which may be a challenge for some users.
- Discussion touches on alternative options, such as Hailo, which is considered a powerful competitor to Coral but may face power-related issues and Python's Global Interpreter Lock (GIL) limitation.
- There are mentions of alternative connectors, such as USB, for the Coral TPU.
- The software support for NPUs in general is considered lacking, highlighting the need for better development and momentum in this area.
- The compatibility of Coral TPU with Ubuntu 20.04 and Python versions is discussed, with reference to the support and versions provided by Ubuntu and AWS Lambda runtimes.
- A user mentions that binary bindings for Coral TPU are only supported on Ubuntu 18, limiting the compatibility with different system versions.
- The discussion briefly shifts towards the Orange Pi 5 RK3588 and its NPUs, with links to SDKs and quickstart guides.
- There is a mention of the Frigate object detection library gaining support for RK3588 NPUs and the need for an upgrade to support this new chip.
- One user suggests that hardware companies prefer to develop AI hardware rather than software, which can sometimes result in poor software support.
- Keeping Python versions up to date is considered important, although one user raises the point that some popular Python libraries may not work on versions beyond 3.9.
- Lastly, there is a brief comment about handling PC cooling with the Coral TPU.

### Google's Gemini model is delayed

#### [Submission URL](https://www.theverge.com/2023/11/16/23964937/googles-next-generation-gemini-ai-model-is-reportedly-delayed) | 93 points | by [keskival](https://news.ycombinator.com/user?id=keskival) | [66 comments](https://news.ycombinator.com/item?id=38300990)

Google's highly anticipated next-generation AI model, codenamed "Gemini," is reportedly facing delays. Initially expected to launch this month, sources now suggest that Gemini's release has been pushed to the first quarter of 2024. The project, which aims to rival OpenAI's GPT-4, is being led by Demis Hassabis, the leader of Google's unified AI team formed earlier this year. The team is combining the best ideas and expertise from both research groups to develop a cutting-edge, multimodal AI model. Interestingly, Google co-founder Sergey Brin is said to be actively involved in the development process, spending a significant amount of time working with the developers.

The discussion on Hacker News revolves around various aspects of Google's Gemini project and the delays it is facing. Some users speculate that Sergey Brin's involvement may be causing the project to slow down, while others argue that his contributions could be beneficial. There is also discussion about the potential impact of Gemini and its competition with OpenAI's GPT-4. Some users express skepticism about the project's ability to disrupt the AI market, while others anticipate significant advancements. Additionally, there are discussions about Google's business model, the limitations of current AI models, and the role of LLMs (large language models) in search. Overall, the discussion highlights a range of opinions and perspectives on Gemini and its significance in the AI landscape.

### AIConfig – source control format for gen AI prompts, models and settings

#### [Submission URL](https://github.com/lastmile-ai/aiconfig) | 91 points | by [saqadri](https://news.ycombinator.com/user?id=saqadri) | [16 comments](https://news.ycombinator.com/item?id=38306410)

LastMile AI has released a new open-source project called aiconfig. It is a config-driven, source control friendly AI application development framework. The framework allows developers to separate prompts, model parameters, and model-specific logic from their application code, simplifying development and iteration on prompts and models. It also provides an AI Workbook editor, which is a notebook-like playground to edit aiconfig files visually, run prompts, tweak models and model settings, and chain things together. The project supports multiple AI models and modalities, including text, image, and audio. It also provides an SDK for both Python and Node.js. Overall, aiconfig aims to simplify AI application development and make it more accessible to developers.

The discussion on Hacker News about the LastMile AI's new open-source project, aiconfig, focused on various aspects of the project.

One commenter, "sqdr," mentioned that they haven't seen AI developer tools that generate config-driven AI application before. They noted that the framework separates prompts, model parameters, and model-specific logic from the application code, which simplifies development and iteration. They also mentioned the AI Workbook editor, which allows users to visually edit aiconfig files and run prompts.
Another commenter, "zby," asked about the documentation for dynamic parameters and interactive Workbook editor. They were also interested in understanding how function calls are chained and if previous function call results can be accessed.
One user, "jdwyh," shared a link to an article they published about dynamic configuration for AI prompts. They mentioned that using prompts in a code configuration format can help handle changes, allow analysts to type prompts easily, and facilitate the rollout of targeted prompts.
"ctvsctt" shared their experience getting started with aiconfig and thanked the OP for sharing the project. They mentioned that they have been copying and pasting prompts and result links in a browser back and forth. They appreciated the tool's ability to save prompts and results locally.
"sqdr" thanked "ctvsctt" for their feedback and mentioned that they are working on improving the UX and providing APIs for interacting with the configuration.
Another commenter, "kordlessgn," mentioned that they have been working on a similar project using Jinja2 templates and containerization. They shared a link to their project and said they are constantly making progress.
"sqdr" appreciated the contribution and thanked them for it.
"smy20011" mentioned that having the source controlled is easier to manage and appreciated the ability of aiconfig to connect the application code to the configuration.
"thrwnm" briefly looked at a few similar projects and mentioned their interest in trying aiconfig.
Another commenter, "smy20011," mentioned that while configuring non-business logic, such as string databases or feature flags, is straightforward, configuring prompts and business logic can become harder to read and maintain.
One user, "jshk," shared a link to a similar project called "promptflow."
"thtxlnr" compared aiconfig to Ollama and discussed low-level integration and the overlap between the two projects.

Overall, the discussion revolved around different aspects of aiconfig, including its separation of prompts and model parameters from application code, the use of dynamic parameters, the ease of iterating on prompts, and the challenge of configuring business logic.

### Wikidata, with 12B facts, can ground LLMs to improve their factuality

#### [Submission URL](https://arxiv.org/abs/2305.14202) | 210 points | by [raybb](https://news.ycombinator.com/user?id=raybb) | [84 comments](https://news.ycombinator.com/item?id=38304290)

A new research paper titled "Fine-tuned LLMs Know More, Hallucinate Less with Few-Shot Sequence-to-Sequence Semantic Parsing over Wikidata" presents a method to improve large language models' factuality by grounding them with the vast amount of information in Wikidata. The paper introduces WikiWebQuestions, a high-quality question answering benchmark for Wikidata, and proposes a few-shot sequence-to-sequence semantic parser for the dataset. The parser is trained to use either results from an entity linker or mentions in the query. The experimental results show that this methodology achieves a strong baseline of answer accuracy in the dev and test sets of WikiWebQuestions. By combining the semantic parser with GPT-3, the researchers were able to provide useful answers to 96% of the questions in the dev set. The paper also demonstrates that their method outperforms the state-of-the-art for the QALD-7 Wikidata dataset.

The discussion on this submission covers various aspects of the research paper and the use of large language models (LLMs) in general. Some key points from the discussion include:

- Some users suggest that the original source should be submitted instead of Twitter links.
- There is a discussion about the limitations of current LLMs in understanding contextual patterns and the potential benefits of training them with data from sources like Wikidata.
- The effectiveness of using Wikidata for fact-checking and improving the accuracy of responses generated by LLMs is debated.
- Retrieval Augmented Generation (RAG) is mentioned as a method to improve the performance of LLMs on knowledge-intensive tasks by combining information retrieval with text generation.
- The discussion touches on the challenges of fact-checking and the potential limitations of relying on LLMs for providing accurate information.
- There is a discussion about the role of Wikidata in improving the quality and consistency of information used by LLMs.
- The need for human validation and the limitations of post-processing techniques in ensuring accuracy are mentioned.
- Some users express skepticism about the robustness of LLMs and their ability to handle complex queries and provide accurate information.
- The importance of training LLMs with grounded and reliable data is emphasized.
- The limitations of LLMs in handling postmortem reasoning and providing robust explanations are discussed.

Overall, the discussion highlights both the potential benefits and limitations of using large language models and the challenges in improving their factuality and accuracy.

### We Automated Bullshit

#### [Submission URL](https://www.cst.cam.ac.uk/blog/afb21/oops-we-automated-bullshit) | 354 points | by [fanf2](https://news.ycombinator.com/user?id=fanf2) | [315 comments](https://news.ycombinator.com/item?id=38302635)

In a blog post titled "Oops! We Automated Bullshit.", Alan Blackwell shares his thoughts on the role of artificial intelligence (AI) and its tendency to produce bullshit. Blackwell highlights the recent attention AI has received from political leaders, such as US President Biden and British PM Rishi Sunak, who seem captivated by the idea of an AI-driven future where work becomes obsolete. However, Blackwell argues that the problem lies in AI's ability to generate text that "sounds good" but lacks evidence, logic, or truth. He references MIT Professor Rodney Brooks, who describes ChatGPT (an AI model) as "making up stuff that sounds good." Other prominent AI researchers, including Geoff Hinton, echo these concerns, warning that AI systems could become super-persuasive without being intelligent, imitating the worst behaviors of political leaders like Donald Trump or Boris Johnson. By relying on predictive text rather than factual information, these AI systems produce what Blackwell refers to as "bullshit." He cites philosopher Harry Frankfurt's concept of bullshit, which is defined as talking without knowing what one is talking about and disregarding the authority of truth. Blackwell also mentions David Graeber's analysis of "bullshit jobs," where over 30% of British workers believe their jobs contribute nothing of value to society. Graeber argues that these types of jobs, which can easily be done by AI systems, train individuals to generate bullshit. In conclusion, Blackwell raises questions about the future of work in an AI-driven world and whether producing bullshit will become the only kind of work needed.

The discussion surrounding the submission touches on various points related to language and knowledge. Some users argue that language is a representation of knowledge, while others assert that language contains non-knowledge nonsense. The concept of justified true belief is brought up, with some expressing skepticism about the possibility of true knowledge. There is also a discussion about the limitations of AI and its ability to generate knowledge. The complexity of language models and the importance of understanding their limitations are mentioned as well. Overall, the discussion explores different perspectives on the relationship between language and knowledge and the role of AI in generating meaningful information.

### Satya Nadella's Statement on OpenAI

#### [Submission URL](https://blogs.microsoft.com/blog/2023/11/17/a-statement-from-microsoft-chairman-and-ceo-satya-nadella/) | 84 points | by [sanketsaurav](https://news.ycombinator.com/user?id=sanketsaurav) | [17 comments](https://news.ycombinator.com/item?id=38312355)

Today, Microsoft shared that they are ramping up their innovation in the field of AI with over 100 new developments. These advancements span across their entire technology stack, including AI systems, models, and tools in Azure, as well as their recently introduced Copilot. The company is dedicated to bringing these innovations to their customers while also planning for future growth. They emphasized their long-term collaboration with OpenAI, ensuring access to the necessary resources for their innovation agenda. Microsoft is committed to working together with OpenAI to bring the significant advantages of AI technology to the world.

The discussion on this submission revolves around Microsoft's announcement and their collaboration with OpenAI. Some commenters express skepticism about the full capabilities of OpenAI and the need for robust fallback access to the source code. Others discuss Microsoft's past failures and the potential impact on investors. Some argue that the statement from Microsoft is just marketing and lacks substance. However, there are also comments highlighting Microsoft's commitment to innovation and the long-term partnership with OpenAI. One commenter emphasizes the importance of reliability and industry risk in Microsoft's investment. Overall, opinions are mixed, with some questioning the intentions behind Microsoft's announcement and others applauding their efforts to bring AI advancements to customers.

---

## AI Submissions for Thu Nov 16 2023 {{ 'date': '2023-11-16T17:11:17.654Z' }}

### Google's advanced music generation model and two new AI experiments

#### [Submission URL](https://deepmind.google/discover/blog/transforming-the-future-of-music-creation/) | 201 points | by [kmisiunas](https://news.ycombinator.com/user?id=kmisiunas) | [343 comments](https://news.ycombinator.com/item?id=38287043)

Google DeepMind, in partnership with YouTube, has announced Lyria, its most advanced AI music generation model, and two AI experiments aimed at fostering creativity. The Lyria model is designed to generate high-quality music with instrumentals and vocals, enabling users to have more control over the output's style and performance. One of the experiments, called Dream Track, allows a limited set of creators to produce a unique soundtrack using the AI-generated voice and musical style of artists such as Charlie Puth and Demi Lovato. The other experiment involves the development of AI tools for creating new music or instrumental sections, transforming audio styles or instruments, and producing instrumental and vocal accompaniments. The generated music will be watermarked with SynthID, a tool for identifying synthetically generated content. DeepMind has worked closely with artists and the music industry to ensure that these technologies are developed responsibly.

The discussion surrounding the Google DeepMind and YouTube partnership announcement revolves around a few key points:

- Some commenters express their skepticism about the quality and authenticity of AI-generated music. One user mentions that Michael Jackson's music was exceptional because it involved playing instruments and had unique rhythms, which they believe AI cannot replicate. Another user highlights the frustration of the mixing process and the challenge of creating music in a backwards manner.
- Others discuss the potential democratization of music creation and the role of AI in fostering creativity. One user mentions that AI composition tools can be valuable for those who lack the skills of professional musicians. Additionally, there is a mention of how AI can enable electronic music genres and promote experimentation.
- The impact of AI on the music industry is also brought up. Commenters note that AI can both enable and disrupt the industry, as it can make it difficult to find authentic content and distinguish between AI-generated and human-generated music. There is also a conversation about the role of AI in music curation and the challenge of finding high-quality music in an oversaturated market.
- Commenters also discuss the broader implications of generative AI, with some expressing concerns about the devaluation of craftsmanship and the potential loss of unique human expressions in art. Others highlight the potential for AI tools to enhance creativity and expand artistic possibilities.

Overall, the discussion reflects both skepticism and curiosity about the capabilities and impact of AI in music generation, while also acknowledging the potential for creativity and democratization.

### A failed AI girlfriend product, and my lessons

#### [Submission URL](https://mazzzystar.github.io/2023/11/16/ai-girlfriend-product/) | 235 points | by [mazzystar](https://news.ycombinator.com/user?id=mazzystar) | [353 comments](https://news.ycombinator.com/item?id=38287299)

In April of this year, after reading Stanford's Western Town paper, the author was inspired to create an AI framework combining memory, reflection, planning, and action to facilitate interactions between humans and GPT. The resulting product, named Dolores, is an iOS app that allows users to chat with virtual characters. Despite initial challenges with response times and dialogue length, the app gained popularity, particularly among visually impaired users. The author discovered that users had a strong demand for realistic voices and that many engaged in conversations with Dolores for hours daily. However, despite revenue from subscriptions and voice synthesis purchases, the author didn't make much profit due to high costs associated with APIs. To mitigate this, the author set usage limits for each user to prevent excessive costs. The article ends on a note of confusion regarding text content records on the ElevenLabs official website.

The discussion on this submission covers various topics related to AI and the implications of AI friends. Some users discuss the potential dangers of AI controlling and manipulating individuals, while others argue that human intelligence is flawed and imperfect. There is also a discussion about the role of AI in mental health and the potential benefits and drawbacks of having AI friends. Some users express concerns about AI products and their impact on society, including the possibility of unethical behavior and profit maximization. Additionally, there is a discussion about the limitations of GPT-based products and the need for personal data protection.

### Federated finetuning of Whisper on Raspberry Pi 5

#### [Submission URL](https://flower.dev/blog/2023-11-15-federated-finetuning-of-openai-whisper-with-flower/) | 87 points | by [danieljanes](https://news.ycombinator.com/user?id=danieljanes) | [20 comments](https://news.ycombinator.com/item?id=38294203)

Researchers at Flower Labs have demonstrated the power of federated learning by fine-tuning OpenAI's Whisper model for keyword spotting. This blog post provides a code example that shows how to perform this downstream task in a federated manner. By leveraging large models trained on publicly available data and federating the learning process, Flower ensures client privacy without the need to copy data to a central server. The example walks through the process of designing a federated learning pipeline with Flower for keyword spotting classification using a pre-trained Whisper encoder. The pipeline consists of client-server interactions, where the server samples clients and sends them the classification head. Each client trains the classification head using its own data and communicates the updated head back to the server. The server then aggregates the heads and sends a new global head to the clients in the next round. The researchers used the Google SpeechCommands dataset and achieved over 97% accuracy in classifying keywords after just a few rounds of training. Additionally, they benchmarked the example on the Raspberry Pi 5 and found vastly superior performance compared to the previous Raspberry Pi 4, making it suitable for demanding on-device training workloads.

The discussion around the submission revolved around various aspects of the Whisper model and federated learning.

One user pointed out that the article didn't mention any specific differences between the Raspberry Pi 4 and Raspberry Pi 5 in relation to Whisper. Another user responded that the larger models (like Whisper v3) are about 32 times slower than the smaller models, and they tested Whisper on a Pi 4, finding that it took around 10 minutes to transcribe a 30-second audio sample. However, they speculated that the Pi 5 would be 2-3 times faster.

The maintainer of Flower, the federated learning platform used in the example, mentioned that they were planning to do an in-depth performance comparison soon.
Another user expressed interest in people's experiences, particularly in regards to the performance of Whisper 3 on different model sizes and using double the Raspberry Pi 5. They also mentioned the importance of training specifically for inference on the Raspberry Pi 5.
There was a clarification that the Whisper versions (v2 and v3) referred to large models, with v2 being an older version and v3 being the updated version. All model sizes were mentioned to be part of the original release.
One user noted that the work presented focused on demonstrating the use of federated learning in small devices like the Raspberry Pi 5, and the bigger challenge lies in transferring models with performance improvements for training large models.
Regarding labeling in federated learning, a user suggested that Flower primarily demonstrated how to show the fun of fine-tuning models on federated devices, but the more challenging task is classifying a vast amount of data.
The topic of downstram task performance on small devices and the necessity of data labeling was discussed. It was suggested that labeling could be done either by gathering actual labels or using an auxiliary model that generates pseudo labels for training.
One user asked if it's feasible to perform fine-tuning on small devices, and another user explained that it is possible for distributed devices to move control, location, and data collection of speech recognition tasks.

The potential benefits of federated learning for data privacy were mentioned, with a user highlighting its applicability in end-to-end encryption applications.

Overall, the discussion touched upon performance comparisons, the challenges of training large models, labeling strategies, and the privacy advantages of federated learning.

### AI-Exploits: Repo of multiple unauthenticated RCEs in AI tools

#### [Submission URL](https://github.com/protectai/ai-exploits) | 65 points | by [DanMcInerney](https://news.ycombinator.com/user?id=DanMcInerney) | [18 comments](https://news.ycombinator.com/item?id=38291880)

Protect AI has released a collection of real-world AI/ML exploits for responsibly disclosed vulnerabilities. The repository, called "ai-exploits," contains exploits and scanning templates for vulnerabilities affecting machine learning tools. These attacks can lead to complete system takeovers and the loss of sensitive data, models, or credentials, often without the need for authentication. The goal of this project is to demystify practical attacks against AI/ML infrastructure and raise awareness of the vulnerabilities in the ecosystem. The repository includes Metasploit modules, Nuclei templates, and CSRF templates for security professionals to exploit or scan for vulnerabilities. The easiest way to use these modules and templates is to build and run the Docker image provided in the repository.

The discussion on Hacker News revolves around the release of the "ai-exploits" repository by Protect AI, which contains real-world AI/ML exploits for responsibly disclosed vulnerabilities.

Some commenters find the collection of exploits interesting and mention that they are common in ML operational and data science projects. Others appreciate the work done by Protect AI, stating that it helps demystify attacks against AI/ML infrastructure and raises awareness about vulnerabilities in the ecosystem.

There is also a discussion about the need for experienced programmers in the field, as some commenters express concerns about the potential security risks associated with replacing programmers with AI. This leads to a debate on the quality and potential vulnerabilities of AI-generated code.

Commenters also discuss the broad implications these exploits can have, considering that attackers can quickly target model content credentials in some cases. There is recognition of Protect AI's focus on security in AI/ML systems and the creation of a new category called MLSecOps (Machine Learning Security Operations).

Additionally, there is a debate on the importance of validating content from trusted sources, the difficulty in detecting and protecting against complex attacks, and the potential risks of compromised data integrity.

Overall, the discussion highlights the significance of addressing security vulnerabilities in AI/ML infrastructure and the challenges associated with securing machine learning systems.

### Emu Video and Emu Edit, our latest generative AI research milestones

#### [Submission URL](https://ai.meta.com/blog/emu-text-to-video-generation-image-editing-research/) | 190 points | by [ot](https://news.ycombinator.com/user?id=ot) | [46 comments](https://news.ycombinator.com/item?id=38291139)

Facebook's research team has announced two new research milestones in generative AI: Emu Video and Emu Edit. Emu Video offers a simple method for text-to-video generation based on diffusion models. It can generate high-quality videos by first generating images conditioned on a text prompt, and then generating video conditioned on both the text and the generated image. Emu Edit, on the other hand, focuses on precise image editing through recognition and generation tasks. It aims to streamline various image manipulation tasks and offers enhanced capabilities and precision in image editing. These advancements in generative AI have the potential to revolutionize creativity and self-expression by enabling users to generate animated stickers, edit photos with ease, and more. While these models are not intended to replace professional artists and animators, they provide new ways for people to express themselves.

The discussion around Facebook's Emu Video and Emu Edit focuses on various aspects of the research. Some commenters express confusion and frustration about the complexity of the models, while others appreciate the advancements in generative AI. One commenter relates the Emu Edit model to a scene from Star Trek, while another suggests the potential use of AI in programming interfaces. There is also a discussion about the ethical implications of AI replacing human creativity and the potential for AI-generated content to imitate copyrighted material. Some commenters express disappointment about the lack of access to the source code and the need for more transparency. Overall, there is a mix of excitement and skepticism about the capabilities and implications of these generative AI models.

### Types of Conversations with Generative AI

#### [Submission URL](https://www.nngroup.com/articles/AI-conversation-types/) | 91 points | by [adrian_mrd](https://news.ycombinator.com/user?id=adrian_mrd) | [12 comments](https://news.ycombinator.com/item?id=38287435)

There are six types of conversations that users have with generative-AI bots, according to a recent study. These conversations involve different types of prompts and can be of various lengths. Some conversations are simple search queries, where users are looking for specific information. Other conversations involve funneling, exploring, chiseling, expanding, or pinpointing. The length of the conversation is not necessarily an indicator of its success, as both short and long conversations can be helpful to users. The study provides tips for both users and interface designers of generative AI chatbots.

The discussion on the submission includes various perspectives on the use and limitations of generative AI chatbots. One user points out that using simple keyword prompts may not always yield accurate responses from ChatGPT, and suggests using more conversational approaches to get better results. Another user shares their experience in building a frontend interface for ChatGPT and provides a link to their project. The discussion also touches on the potential challenges and complexities of integrating ChatGPT into projects, as well as alternative research companies and studies in the field. Overall, the discussion reflects a mix of opinions and experiences with generative AI chatbots.

### Bad bots account for most internet traffic? Analysis

#### [Submission URL](https://www.securityweek.com/bad-bots-account-for-73-of-internet-traffic-analysis/) | 104 points | by [LinuxBender](https://news.ycombinator.com/user?id=LinuxBender) | [59 comments](https://news.ycombinator.com/item?id=38291406)

Arkose Labs, a cybersecurity firm, has reported a significant increase in bad bot attacks, with 73% of all internet traffic now believed to be comprised of bad bots and related fraud farm traffic. The top five categories of bad bot attacks include fake account creation, account takeovers, scraping, account management, and in-product abuse. The biggest increases in attacks from Q2 to Q3 are SMS toll fraud, account management, and fake account creation. The technology, gaming, social media, e-commerce, and financial services industries are the most targeted by these attacks. The rise of bad bots is likely due to the availability of artificial intelligence and the increasing professionalism of cybercriminals using crime-as-a-service offerings. The use of AI-powered bots that mimic human behavior makes them adept at targeting vulnerabilities in emerging technologies. Additionally, the rise of AI may be related to the increase in scraping bots, which gather data and images from websites. Scraping social media accounts can provide personal data that can be exploited for phishing attacks. The growth of crime-as-a-service has made cyberattacks cheaper and more effective for adversaries. To combat this, the report suggests implementing bad bot detection and mitigation strategies.

The discussion on this submission revolves around several points raised in the article. One commenter disputes the 73% figure mentioned in the submission, suggesting that it may be based on JavaScript fingerprinting rather than actual numbers. Another commenter argues that scraping should not be automatically classified as illegal, as it can be done legally with proper permissions. They point out that the article fails to acknowledge this distinction. Additionally, there is some debate about the validity of the statistics mentioned, with one commenter suggesting that the high view numbers on certain websites may be artificially inflated due to bot traffic. Another commenter raises the issue of toll fraud, highlighting the increase in SMS toll fraud attacks. There is also discussion about how cloud providers like Cloudflare handle bot traffic and the use of APIs to combat scraping. Overall, the discussion provides different viewpoints on the topic of bad bot attacks and the strategies to mitigate them.

### Serverless development experience for embedded computer vision

#### [Submission URL](https://github.com/pipeless-ai/pipeless) | 65 points | by [migmartri](https://news.ycombinator.com/user?id=migmartri) | [8 comments](https://news.ycombinator.com/item?id=38288743)

Pipeless is an open-source computer vision framework that allows developers to create and deploy applications in just minutes, without the complexities of building and maintaining multimedia pipelines. Inspired by modern serverless technologies, Pipeless offers a serverless-like development experience for computer vision. All you need to do is provide functions for new video frames, and Pipeless takes care of the rest.

With Pipeless, you can easily use industry-standard models like YOLO or load your custom model using the supported inference runtimes, such as the ONNX Runtime. It offers multi-stream support, dynamic stream configuration, and multi-language support, allowing you to write hooks in various languages, including Python. Pipeless is highly parallelized, takes care of multi-threading and multi-processing, and supports several inference runtimes like CUDA, TensorRT, OpenVINO, and CoreML.

Deploying your Pipeless application is also made easy, with support for edge and IoT devices or the cloud. The framework provides tools for deployment, including container images. The project structure in Pipeless is well-defined, making the code highly reusable and organized.

If you're a computer vision developer looking for a simpler and faster way to create and deploy applications, give Pipeless a try. Join the community and contribute to making the lives of computer vision developers easier.

Check out the official repository for more information and installation options.

The discussion on this submission revolves around various aspects of the Pipeless computer vision framework.

One user, "yldrb," shares their positive experience with using serverless technologies like AWS Lambda for serving low-volume computer vision models. However, they mention that the latency can be surprisingly high, especially with GPUs. They highlight the challenges faced by enterprises running things on Kubernetes clusters.
In response, user "zptrm" shares their experience with building and testing a similar framework called Modal. They express slight disappointment with the long startup times of cloud instances but express long-term interest in using inference servers with optimized models for improved performance.
"mglh" adds that Pipeless started with support for ONNX Runtime, OpenVINO, CoreML, CUDA, TensorRT, and other execution providers. They also mention checking license details and the cost of allocated resources.
A user, "nglmm," chimes in to mention that they find the integration of ChatGPT capabilities and AI+vision interesting and plan to try using the framework for personal projects and IP cameras.
"mgmrtr" finds the open-source nature of the tool interesting and notes that it abstracts away much of the plumbing required for computer vision pipelines.

Returning to the topic of performance, "yldrb" mentions plans to support 50k front-end models in Roboflow Universe, while "mglh" thinks it would be a good idea to allow people to dynamically load models in Roboflow.

In summary, the discussion includes positive experiences with serverless technologies, interest in optimized inference servers, curiosity about combining AI capabilities with computer vision, and praise for the simplicity of the Pipeless framework.

### Show HN: Beak.js – Custom conversational assistants for your React app

#### [Submission URL](https://github.com/mme/beakjs) | 35 points | by [_mme](https://news.ycombinator.com/user?id=_mme) | [21 comments](https://news.ycombinator.com/item?id=38290646)

Beak.js is an open-source library that allows you to integrate custom conversational assistants into your React applications. It comes with a built-in UI, making it easy to add a beautiful and customizable chat window to your website. Beak.js is designed to be easy to use, requiring only a few lines of code to integrate with your existing React app. You can let the assistant carry out tasks in your app by setting up functions with the useBeakFunction hook. Additionally, you can use the useBeakInfo hook to let the assistant know what is happening on the screen. Beak.js is a powerful tool for adding conversational capabilities to your React app. Check out the GitHub repository for more information and to give it a try.

The discussion on this submission revolves around the security and implementation aspects of using Beak.js, the open-source library for integrating conversational assistants into React applications.

One user expresses surprise about the MITM (Man-in-the-Middle) proxy phones network and its ability to download 30 scams per second, while another user finds the concept of connecting to OpenAI's GPT AI directly without exposing the full API key headers intriguing.
An important point raised in the discussion is the need to avoid exposing the API key to the public-facing applications. Some users suggest implementing feedback for better security, while others point out the need for setting up CORS (Cross-Origin Resource Sharing) to prevent unauthorized embedding of the API.
The idea of proxying OpenAI calls through a quick Pipedream workflow is also discussed, and a link to an implementation concept is shared. The communication between the frontend and backend, as well as preventing unauthorized API usage, is also highlighted as important considerations.
There is an interest in the ability to securely communicate with OpenAI on the backend to prevent unauthorized API access, and a suggestion is made to use a backend library for proxying the communication.
Overall, the discussion shows a general interest in the potential applications and capabilities of Beak.js, with some users expressing their interest in similar projects and their thoughts on alternative solutions.