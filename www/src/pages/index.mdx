import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Wed Sep 06 2023 {{ 'date': '2023-09-06T17:10:24.501Z' }}

### AOL pretends to be the internet

#### [Submission URL](https://thehistoryoftheweb.com/postscript/aol-pretends-to-be-the-internet/) | 148 points | by [janvdberg](https://news.ycombinator.com/user?id=janvdberg) | [243 comments](https://news.ycombinator.com/item?id=37404918)

AOL, formerly known as Quantum Computer Services, started out as a closed-loop network for computer users to connect with one another. However, in the mid-90s, AOL rebranded as America Online and aimed to bring networked computing to the masses. Their marketing strategy included an aggressive direct mailing campaign, sending out floppy disks and later CD-ROMs to households. The disks provided a few hours of free service, and once people started using AOL, they became hooked. AOL positioned itself as a content provider and brokered deals with major publishers and media companies. They wanted to own the experience of digital networks for their consumers and even considered producing original content exclusive to AOL. However, the rise of the Internet posed a threat to AOL's closed system. In 1994, AOL expanded support to certain Internet protocols, but notably, they did not have a web browser until acquiring BookLink in 1995 and adding Microsoft's Internet Explorer in 1996. AOL tried to position themselves as part of the Internet, but their identity eventually became subsumed by the wider, community-driven web and internet.

The discussion revolves around various aspects of URLs, QR codes, and the use of technology in daily life.

- Some users mention the use of AOL keywords and how they were an alternative to typing in full URLs. There is a reminiscence of using AOL and how it was a recognizable domain name that people would type.
- Others share their experiences with using URLs and how some websites reject URLs with a "www" prefix. There is a debate about whether it is necessary to include "www" before a website address.
- The importance of user experience and customer support is discussed, with some users emphasizing the need for intuitive interfaces and responsive support.
- The topic of email addresses is brought up, with users discussing the format and difficulty in understanding email addresses. Some users share their experiences with using different domain names and encountering confusion.
- The use of QR codes in various scenarios is debated, with some users finding them useful while others have had difficulty scanning them. The idea of QR codes in restaurants and the perceived assumptions about people's technological proficiency are discussed.
- The practicality and security of scanning QR codes are also mentioned. Some users express concerns about trust, unknown URLs, and relying on third-party services.
- The discussion also touches on the use of QR codes in different countries and how their popularity varies.

Overall, the discussion provides a range of perspectives on the use of technology in daily life, including URLs, email addresses, and QR codes. There are debates about convenience, security, and usability in different technological contexts.

### Proofs based on diagonalization help reveal the limits of algorithms

#### [Submission URL](https://www.quantamagazine.org/alan-turing-and-the-power-of-negative-thinking-20230905/) | 81 points | by [nsoonhui](https://news.ycombinator.com/user?id=nsoonhui) | [46 comments](https://news.ycombinator.com/item?id=37401228)

In a recent article on Quanta Magazine, Ben Brubaker explores the power of negative thinking in the work of legendary computer scientist Alan Turing. Turing pioneered the concept of "uncomputable" problems, which are problems that cannot be solved algorithmically. He proved the existence of such problems using a counterintuitive strategy based on a mathematical technique called diagonalization. Diagonalization involves building up a missing string bit by bit to ensure that it differs from every string on a given list. This approach plays well with infinity and has been used by mathematicians and computer scientists, including Turing, to prove various results. In Turing's case, he utilized diagonalization to construct an obstinate problem that would thwart every algorithm on an infinite list, demonstrating the existence of unsolvable problems. This contrarian approach to problem-solving has had a profound impact on the field of computer science and continues to inspire researchers today.

The discussion on this submission revolves around the concept of diagonalization and its application to proving the existence of uncomputable problems. Some commenters argue that the diagonalization argument used by Turing is not sufficient to prove the existence of uncomputable real numbers, while others provide counterarguments to support Turing's approach. There is also a discussion about constructive vs non-constructive mathematics and the limitations of diagonalization in a constructive framework. Overall, the discussion explores different perspectives on the power of negative thinking and its impact on computer science and mathematics.

### Can LLMs learn from a single example?

#### [Submission URL](https://www.fast.ai/posts/2023-09-04-learning-jumps/) | 426 points | by [jdkee](https://news.ycombinator.com/user?id=jdkee) | [133 comments](https://news.ycombinator.com/item?id=37399873)

A recent study on large language models (LLMs) has discovered a surprising anomaly in their training process. It was found that these models are capable of rapidly memorizing examples from the dataset after just a single exposure, which goes against the conventional wisdom about neural network sample efficiency. This unexpected phenomenon prompted further investigation and a series of experiments to validate and understand this behavior. The results indicate that the models indeed exhibit rapid memorization capabilities, questioning the current approach to training and utilizing LLMs. This finding opens up new possibilities for improving the efficiency and effectiveness of neural networks and challenges the long-standing assumption of their slow learning process.

The discussion surrounding the submission begins with a user named "jph00" expressing surprise at the rapid memorization capabilities of large language models (LLMs) and how it challenges existing assumptions about neural network sample efficiency. Another user, "og_kalu," suggests trying to freeze the training of LLMs after initial exposure to improve efficiency. "jph00" shares that the experiments conducted didn't show catastrophic forgetting, but rather the models becoming overly confident. They see this as an opportunity rather than a problem.

The conversation then shifts to other topics related to training and understanding neural networks. Users discuss training natural intelligence models, the potential for catastrophic forgetting, and the importance of freezing lower layers in LLMs to prevent forgetting fundamental knowledge. There is also mention of the Internal Family Systems model and its similarities to neural network training.

Some users express their appreciation for the investigation and its potential implications, while others request further explanation and clarification. There is a brief discussion about catastrophic forgetting and its impact on model performance. Another user asks for an explanation of the phenomenon in simpler terms (eli5). The concept of catastrophic forgetting is further explored, with some users suggesting modifications to the sampling model.

Overall, the discussion touches upon various aspects of neural network training, the mechanisms underlying LLMs, and the potential implications of the anomaly discovered in their training process.

### Artificial Consciousness Remains Impossible (Part 2)

#### [Submission URL](https://mindmatters.ai/2023/09/artificial-consciousness-remains-impossible-part-2/) | 20 points | by [momirlan](https://news.ycombinator.com/user?id=momirlan) | [82 comments](https://news.ycombinator.com/item?id=37412555)

In this article, the author presents responses to counterarguments against their thesis from part one. They address objections claiming that consciousness and intentionality are subjective and cannot be proven. The author argues that without intentionality, we would not understand anything, as words would not refer to anything. They also refute the idea that consciousness is something that is "done" and argue that an AGI could perform tasks without being conscious. 

The author then discusses objections from functionalism, which suggest that replicating the functions of a brain would lead to artificial consciousness. They argue that functionalist arguments fail because duplicating a function requires complete visibility and measurability of all functions and dependencies, which is not possible due to underdetermination. 

Finally, the author addresses behaviorist objections, which claim that reproducing conscious behaviors equates to producing consciousness. They disagree with the idea that observable behaviors alone indicate consciousness and reference the Chinese Room argument. 

Overall, the author provides counterarguments against various objections to their thesis, emphasizing the importance of intentionality and highlighting the limitations of functionalism and behaviorism in explaining consciousness.

The discussion on this submission covers various perspectives on consciousness and artificial intelligence. One commenter argues that artificial neural networks today do not have the same complexity and topology of states as real neural networks, while another points out that replicating the topology is not sufficient for generating consciousness. There is also a debate about whether consciousness is dependent on specific states of neurons or if it is a separate non-physical property. 

The conversation then shifts to the limitations of current AI models and the importance of memory in consciousness. Some commenters express their experiences of consciousness and memory, while others argue that consciousness cannot exist in artificial systems. 

The discussion also touches on functionalism, with one commenter arguing that replicating brain functions would not lead to artificial consciousness. Another commenter brings up the example of AlphaZero, a computer program that plays creative games of chess, to demonstrate that computers can exhibit creativity without conscious intention. 

There is also a disagreement about the significance of considering individuals' consciousness and the possibility of artificial consciousness. Some commenters argue that humans place too much importance on themselves, while others posit that empathy and understanding others' experiences are essential. 

The discussion concludes with a debate about the nature of intentional and qualitative experiences and their role in determining the existence of consciousness.

### Run ChatGPT-like LLMs on your laptop in 3 lines of code

#### [Submission URL](https://github.com/amaiya/onprem) | 141 points | by [amaiya](https://news.ycombinator.com/user?id=amaiya) | [33 comments](https://news.ycombinator.com/item?id=37412793)

OnPrem.LLM is a Python package that simplifies running large language models (LLMs) on-premises using non-public or sensitive data, even behind corporate firewalls. It was inspired by the privateGPT GitHub repo and Simon Willison's LLM command-line utility, and is designed to seamlessly integrate local LLMs into practical applications.

To get started, simply install OnPrem.LLM after installing PyTorch. You can then import the LLM class and instantiate it. By default, a 7B-parameter model is used, but you can specify a 13B-parameter model or provide the URL to an LLM of your choice.

Once set up, you can send prompts to the LLM to solve problems using few-shot prompting. You provide an example of what you want the LLM to do, and it generates the desired output. You can also talk to your documents by ingesting them into a vector database and asking questions about them. The LLM will generate answers based on the content of your documents.

Ingesting the documents involves specifying the directory containing the documents and creating embeddings for them. This process may take some time, but once complete, you can query your documents using the LLM's `ask` method to get answers to specific questions.

OnPrem.LLM is a powerful tool for running large language models on-premises and working with non-public data. With its user-friendly interface and seamless integration, it enables practical applications using LLMs behind corporate firewalls or with sensitive data. Give it a try and see how it can enhance your workflow!

The discussion on this submission revolves around various aspects of running large language models (LLMs) locally and the features and limitations of the OnPrem.LLM tool.

- One user shares a link to instructions on how to run the LLM locally on a Macbook with M1/M2 and 32GB RAM using the OnPrem.LLM tool. They mention that following these instructions allows them to run 34B models using both the CPU and GPU.

- Another user mentions that GGUF (Great Generic Unifying Framework) format was introduced in August 2023 as a replacement for GGML (Great Generic Modeling Language), and GGUF is supported by the OnPrem.LLM tool.

- A conversation ensues regarding the compatibility of OnPrem.LLM tool with different formats, and it is clarified that while it currently supports GGML format, there are plans to support GGUF format in future versions.

- A user praises the simplicity and usefulness of the OnPrem.LLM tool, mentioning that they have tried it and found it to be a great way to reduce the complexity of working with LLMs locally.

- Another user mentions that they learned about Simon Willison's LLM from the discussion and finds it interesting.

- A user shares a related project they have written, which handles streaming requests to local LLMs, and they mention that the OnPrem.LLM library offers a similar functionality.

- There is a brief discussion about the size of the LLM models, with one user explaining that a 7B-parameter model takes around 7GB of space.

- The effectiveness and efficiency of local LLM models are discussed, with one user mentioning that it depends on the specific use case and the optimization of the models.

- Some users express interest in trying out the OnPrem.LLM tool and commend the OP (original poster) for their work.

Overall, the discussion highlights the functionality and potential use cases of the OnPrem.LLM tool, as well as some technical details and comparisons with other similar tools and libraries.

### AI-generated child sex imagery has every US Attorney General calling for action

#### [Submission URL](https://arstechnica.com/information-technology/2023/09/ai-generated-child-sex-imagery-has-every-us-attorney-general-calling-for-action/) | 17 points | by [CharlesW](https://news.ycombinator.com/user?id=CharlesW) | [24 comments](https://news.ycombinator.com/item?id=37412525)

Attorneys general from all 50 states in the US, along with officials from four territories, have called on Congress to establish an expert commission to study how generative artificial intelligence (AI) could be used to exploit children through the creation of child sexual abuse material (CSAM). The attorneys general are concerned that AI is creating a new frontier for abuse that makes prosecution more difficult, particularly as open-source image synthesis technologies make it easy to create AI-generated pornography. They also want existing laws against CSAM to be expanded to explicitly cover AI-generated materials.

The discussion on this submission covers a range of perspectives on the issue of AI-generated child sexual abuse material (CSAM). Some users argue that AI-generated CSAM should not be legally considered CSAM, as it is fundamentally different from traditional material and may not involve the exploitation of actual children. Others express concerns about the potential harm caused by AI-generated CSAM and suggest that existing laws against CSAM should be expanded to explicitly cover AI-generated materials. There is also debate about the role of AI in identifying and preventing CSAM, with some users suggesting that AI could be used for better recognition and verification of CSAM, while others argue that AI could also be used to create more convincing and harmful content. Overall, the discussion explores different perspectives on the legal and ethical implications of AI-generated CSAM.

---

## AI Submissions for Tue Sep 05 2023 {{ 'date': '2023-09-05T17:09:32.081Z' }}

### Gizmodo fires Spanish staff amid switch to AI translator

#### [Submission URL](https://arstechnica.com/information-technology/2023/09/ai-took-my-job-literally-gizmodo-fires-spanish-staff-amid-switch-to-ai-translator/) | 55 points | by [stalfosknight](https://news.ycombinator.com/user?id=stalfosknight) | [26 comments](https://news.ycombinator.com/item?id=37399336)

Last week, Gizmodo en Español, the Spanish-language site of parent company G/O Media, fired its staff and replaced them with AI translations of English-language articles. The move has been met with criticism, as some of the AI-translated articles contain glitches and sudden switches from Spanish to English. This trend of media companies using AI tools to maximize content output while minimizing costs is controversial within the journalism community. While AI translation technology has improved, it still cannot fully replace human translators and can result in subtle errors and lack of cultural knowledge. Despite the potential negative implications, many media companies continue to experiment with AI-written content.

The discussion revolves around the quality of GPT-4 translations and the limitations of machine translation systems. Some users argue that GPT-4 translations, especially for languages like Korean and Japanese, are not accurate and struggle with context and cultural nuances. They note that the lack of certain grammatical elements and the difficulty in translating stylized or metaphorical texts are major challenges for machine translation. However, others point out that GPT-4 has made significant improvements in translation, and some even claim that it surpasses other translation services like DeepL and Google Translate. It is also highlighted that machine translation systems like GPT-4 can handle tasks that human translators wouldn't expect, such as jokes and technical writing. Overall, the debate emphasizes the need for human translation and the limitations of machine translation tools, particularly in capturing cultural and contextual nuances. Some users express concern over the impact on translation industry professionals who may see their work affected or diminished by the rise of AI translation.

### Android 14 blocks all modification of system certificates, even as root?

#### [Submission URL](https://httptoolkit.com/blog/android-14-breaks-system-certificate-installation/) | 312 points | by [pimterry](https://news.ycombinator.com/user?id=pimterry) | [234 comments](https://news.ycombinator.com/item?id=37391521)

In the latest release of Android, Android 14 (API v34), modifications to system certificates have been further restricted, even on fully rooted devices. This move marks a departure from Android's initial promise of openness and user control. Previously, users were able to modify the set of trusted certificates on their devices, but with Android 14, this becomes much more challenging. The set of certificate authorities (CAs) on a device determines the security of encrypted network traffic, and restricting user control over this can have significant implications for privacy and security research, app debugging, and enterprise network configurations. While rooting Android devices has been a workaround to these limitations, it is not officially endorsed by Google. In the past, rooted devices allowed users to manually add trusted certificates to the system store through file modifications. However, this process can be complex, as the /system directory is typically read-only. Workarounds include making the /system directory writable or mounting a temporary read-write file system. These methods have been effective in providing interception setups on rooted Android devices and emulators, enabling developers and researchers to analyze app communications. However, with the tighter restrictions in Android 14, these methods may become less effective. Overall, this shift towards limited user control highlights the ongoing tension between openness and vendor control in the Android ecosystem.

The discussion on this submission primarily revolves around the limitations and implications of the tighter restrictions on modifying system certificates in Android 14. Some users express concerns about the potential impact on privacy and security research, app debugging, and enterprise network configurations. There are discussions about workarounds on rooted devices, including making the /system directory writable or mounting a temporary read-write file system. Some users highlight the tension between openness and vendor control in the Android ecosystem. 

Other discussions touch on topics such as modifying system certificates on vanilla Android, the risks and dangers of unauthorized modifications to the OS, and the security and privacy implications of such modifications. There are also discussions about the benefits and drawbacks of using custom ROMs and the challenges faced by developers and users in finding overlaps between custom ROMs and necessary apps, such as banking apps.

### Stadiums Are Already Using Facial Recognition at Games

#### [Submission URL](https://gizmodo.com/9-stadiums-using-facial-recognition-games-rose-bowl-met-1850798207) | 23 points | by [ourmandave](https://news.ycombinator.com/user?id=ourmandave) | [8 comments](https://news.ycombinator.com/item?id=37394736)

Facial recognition technology is being increasingly used in sports stadiums across the US, but privacy advocates are raising concerns. The Philadelphia Phillies baseball team recently introduced a facial recognition authentication method called "Go-Ahead Entry" to decrease wait times at entry gates, but the system encountered glitches, causing security to create a "buffer zone" for the cameras. Other sports stadiums, including the New York Mets and the Cleveland Guardians, have also implemented facial recognition systems to speed up entry. However, critics argue that the technology does not work well for women or non-white individuals. A coalition of artists, including Rage Against the Machine, has pledged to boycott performing at stadiums that use facial recognition technology.

The discussion around the submission revolves around the use of facial recognition technology in sports stadiums. One commenter highlights the benefits of the technology, mentioning that it greatly reduces the time taken to find missing children, which is a frequent occurrence. Another commenter adds that the technology can also log where and when tickets are scanned. However, there are concerns raised about privacy and the technology's effectiveness, particularly for women and non-white individuals. One artist coalition has even pledged to boycott stadiums that use facial recognition technology. 

In another thread, a user criticizes those who complain about facial recognition, stating that the technology improves over time. They also argue that the concerns at Madison Square Garden (MSG) are exaggerated, as the headlines just represent the opinions of a few individuals. However, another commenter points out that the use of facial recognition in MSG has drawn criticism for its impact on privacy and its potential to disproportionately affect women and non-white individuals.

Another user shares their personal experience, discussing how their company's software for controlling video feeds in a stadium recognized faces, objects like backpacks and guns, and detected tampering. They mention that the system had specific steps triggered for different events, including spotting blacklisted individuals. They add that the company had hired regional architects and an Israeli project manager to develop the software.

In a different comment, a person shares their anecdote about the San Francisco Giants' game day, mentioning their confusion with metal detectors and cameras when attempting to enter using their StubHub tickets. They express concerns about the potential dangers of the technology.

Another user highlights the dystopian and conservative nature of facial recognition being implemented in public places such as California. They mention encountering systems in Japan and Houston but express anger and frustration over the rapid adoption of the technology in the Bay Area, where they have experienced it firsthand, including managing a traced engineering team in India.

In a final comment, a user criticizes the lack of discussion around facial recognition technology in general terms. They argue that people only seem to care when it directly affects them and their freedom and democracy, drawing a parallel to the lack of discussion about voting booth surveillance.

---

## AI Submissions for Mon Sep 04 2023 {{ 'date': '2023-09-04T17:09:43.400Z' }}

### Show HN: finetune LLMs via the Finetuning Hub

#### [Submission URL](https://github.com/georgian-io/LLM-Finetuning-Hub) | 75 points | by [rsaha7](https://news.ycombinator.com/user?id=rsaha7) | [7 comments](https://news.ycombinator.com/item?id=37381296)

LLM-Finetuning-Hub is a repository that contains code and insights for fine-tuning various large language models (LLMs) for different use cases. The repository provides an Evaluation Framework that includes four pillars: performance, time to train, cost to train, and inferencing. It aims to assist users in leveraging LLMs for their business needs, deciding which LLM suits their requirements, and boosting reproducibility efforts. The repository offers ready-to-use scripts for fine-tuning LLMs and performing hyperparameter optimization. The setup process is straightforward, involving the creation of a conda environment, installation of relevant packages, and running the fine-tuning scripts for the desired LLM. The repository also provides instructions for zero-shot and few-shot learning using the fine-tuned LLMs. Currently, the experiments have been conducted on an AWS EC2 instance with one 24GB Nvidia GPU.

The discussion in the comments about the submission revolves around different aspects of fine-tuning large language models (LLMs). 

One user expresses confusion about the purpose of fine-tuning LLMs and mentions the difficulty they face in finding proper data and understanding the process. Another user responds, explaining that fine-tuning LLMs is often done without a specific dataset, and it can be helpful for tasks such as question-answering by converting text articles or tweets into questions. They suggest that the lack of proper documentation might be the reason for the confusion.

Another user argues that fine-tuning LLMs can lead to nonsensical results because most of the training data is from non-FAANG (Facebook, Apple, Amazon, Netflix, Google) sources, which may not accurately represent real-world scenarios. They suggest alternative approaches, such as using code to guide the model's responses or manually searching related documentation.

There is also a discussion about the process of fine-tuning LLMs and the different components involved. One user suggests an approach that involves manually searching related documentation using BM25F and BERT models, and then using the retrieved snippets to help answer questions. They also mention the importance of latency requirements in the process.

A user raises the distinction between fine-tuning LLMs and training templates, stating that they are working on projects using GPU-backed instances on Google Cloud. Finally, one user appreciates the project and mentions its similarities to another project they are working on.

### TinyLlama project aims to pretrain a 1.1B Llama model on 3T tokens

#### [Submission URL](https://github.com/jzhang38/TinyLlama) | 198 points | by [cmitsakis](https://news.ycombinator.com/user?id=cmitsakis) | [60 comments](https://news.ycombinator.com/item?id=37379984)

The TinyLlama project aims to pretrain a 1.1B Llama model on 3 trillion tokens within just 90 days using 16 A100-40G GPUs. It adopts the same architecture and tokenizer as Llama 2, making it compatible with many open-source projects built upon Llama. Despite its compactness with only 1.1B parameters, TinyLlama can cater to various applications with restricted computational and memory requirements.

The project has released a schedule for rolling out intermediate checkpoints, allowing developers to track its progress. TinyLlama has already made significant progress and continues to do so. The project also provides potential use cases for tiny, yet powerful language models, such as assisting speculative decoding of larger models and enabling real-time dialogue generation in video games.

The training details of TinyLlama are also shared, including the training setup and hardware used. The codebase supports features like multi-gpu and multi-node distributed training, flash attention, fused layernorm, and more. These optimizations enable a high throughput of 24k tokens per second per A100-40G GPU, allowing for efficient training and reduced memory footprint.

Overall, TinyLlama offers a powerful and compact language model solution for various applications, and its progress in pretraining the 1.1B model is worth keeping an eye on.

The discussion on Hacker News revolves around the pretraining of the 1.1B Llama model on 3 trillion tokens by the TinyLlama project. One user wonders if the metatraining process will have less curvature and convergence due to the scheduled checkpoints, to which another user responds that the gradual decrease in learning rate may not necessarily help. A discussion also arises about the scaling laws and the performance of different models. Some users express skepticism about the cost and resources required for training these models, while others discuss the relevance of Chinchilla scaling laws and the practicality of deploying large-scale language models. Another user highlights the potential bottlenecks and cost efficiency issues when it comes to inference latency and the availability of GPUs in production environments. The scalability and economic implications of running large models in various settings are also debated. Overall, the discussion touches upon various aspects of training and deploying large language models, including the technical and practical considerations involved.

### Is macOS’s new XProtect behavioural security preparing to go live?

#### [Submission URL](https://eclecticlight.co/2023/09/04/is-macoss-new-xprotect-behavioural-security-preparing-to-go-live/) | 91 points | by [GavinAnderegg](https://news.ycombinator.com/user?id=GavinAnderegg) | [100 comments](https://news.ycombinator.com/item?id=37380104)

Apple's macOS security feature, XProtect, has received significant updates in recent months. XProtect consists of several components, including XProtect Remediator, which detects and removes known malware, and XProtect Behaviour Service (XBS), which observes potentially malicious behavior. The latest updates to XProtect Remediator have expanded its capabilities to detect 19 different types of malware on macOS 10.15 and later. Meanwhile, XBS has been recording potentially malicious behavior but has not yet taken any action to block it. The recent discovery of Bastion rules suggests that XBS may soon become more active in protecting macOS users. However, it remains unclear how updates to these rules are recognized by syspolicyd, the system policy daemon, and implemented without restarting macOS. These recent updates indicate Apple's dedication to improving macOS security, and it is anticipated that XBS will be ready to intervene in security threats in the near future.

The discussion around the submission mainly revolves around the effectiveness of Apple's security feature, XProtect, and the potential improvements it could bring to macOS security. Some users express skepticism about third-party security software and argue that Apple's built-in protection should be sufficient. Others raise concerns about Apple's documentation and control over third-party software that interacts with XProtect. There are also discussions about the challenges faced by developers when building applications for macOS and the limitations of the SwiftUI framework. Overall, the discussion highlights the importance of robust security measures and the need for continuous improvement in protecting macOS users.

### Robots pouring drinks in Vegas: As AI grows, the city's workers brace for change

#### [Submission URL](https://www.npr.org/2023/09/04/1197138244/vegas-ai-workers-brace-for-change) | 20 points | by [CharlesW](https://news.ycombinator.com/user?id=CharlesW) | [21 comments](https://news.ycombinator.com/item?id=37386280)

Las Vegas, a city known for its tourism and hospitality industry, is bracing for the impact of artificial intelligence (AI) and automation on its workforce. With robots already serving drinks and check-in kiosks replacing hotel front desk staff, the city's economy is at an inflection point as companies seek to reduce labor costs through technology. Studies show that between 38% to 65% of jobs in Las Vegas could be automated by 2035. To adapt to this change, the city will need to diversify its economy and focus on occupations that are less easily replaced by AI. The Culinary Union, which represents thousands of service and hospitality workers, is closely monitoring these developments and is prepared to strike over AI if necessary. While some workers believe that AI can never fully replace the human touch and experience, others are concerned about the potential loss of jobs. Overall, Las Vegas is undergoing a transformation as it prepares for the impact of AI on its service-heavy economy.

The discussion on this submission revolves around the use of AI and automation in Las Vegas, particularly in the hospitality industry. Some commenters highlight the potential benefits of AI, such as an AI bartender recognizing customers and handling return orders, as well as the potential for improved mental health by reducing pressure on human bartenders. Others express concerns about the loss of quality and consistency that may come with using robots instead of human bartenders. One commenter mentions that robotic bartenders could lead to wastage of individual drinks and potential theft, while another shares a story of an incident in a kitchen where a robot malfunctioned. Some commenters argue that the transition to AI and automation will result in a reduction in the number of workers supporting non-workers, while others suggest that this could be a positive shift. There are also discussions about the complexity of vending machine technology, the value of human touch in service, and the cost-effectiveness of using robots.

---

## AI Submissions for Sun Sep 03 2023 {{ 'date': '2023-09-03T17:09:36.877Z' }}

### Vector Search with OpenAI Embeddings: Lucene Is All You Need

#### [Submission URL](https://arxiv.org/abs/2308.14963) | 86 points | by [kwindla](https://news.ycombinator.com/user?id=kwindla) | [61 comments](https://news.ycombinator.com/item?id=37373635)

Researchers Jimmy Lin, Ronak Pradeep, Tommaso Teofili, and Jasper Xian have released a paper titled "Vector Search with OpenAI Embeddings: Lucene Is All You Need," challenging the notion that a dedicated vector store is necessary for leveraging deep neural network advancements in search. The researchers demonstrate that Lucene, leveraging hierarchical navigable small-world network (HNSW) indexes, is sufficient for vector search in a standard bi-encoder architecture. Their findings suggest that there is no compelling reason to introduce a dedicated vector store into modern AI stacks for search, given the substantial investments made in existing infrastructure.

The discussion on the submission mainly focused on the use of Lucene and PostgreSQL with pgvector for vector search. Many comments expressed that using Lucene for vector search is simpler and more straightforward for development and production purposes. Some users mentioned that using managed databases like RDS on AWS or Azure is a good option, while others pointed out the importance of security backups regardless of the technology being used. 

There was also a discussion about the practicality and performance of Lucene compared to other vector search options like Elasticsearch or Solr. Some users mentioned that Elasticsearch and Solr are used for production systems that require higher performance and features, while others emphasized the simplicity and usability of Lucene. 

One user raised the issue of naming and the importance of respecting breakthrough papers that have fundamentally changed the machine learning landscape, while another expressed frustration with complaints about the paper and urged for more constructive discussions. 

There were also mentions of other tools and libraries for vector search, such as Faiss, Hnswlib, and Annoy, as well as discussions about the performance metrics and limitations of different approaches in handling large datasets and scaling. 

Overall, the discussion revolved around the effectiveness of using Lucene for vector search and the various options and considerations in implementing vector search in different scenarios.

### The Eleuther AI Mafia

#### [Submission URL](https://www.latent.space/p/rwkv#%C2%A7the-eleuther-mafia) | 89 points | by [swyx](https://news.ycombinator.com/user?id=swyx) | [60 comments](https://news.ycombinator.com/item?id=37368264)

In a recent episode of the Latent Space: The AI Engineer Podcast, host swyx discusses the reinvention of Recurrent Neural Networks (RNNs) for the Transformer era with Eugene Cheah, CTO of UIlicious. The conversation dives into the concept of Receptance Weighted Key Value (RWKV) models, which aim to scale better than traditional Transformers while remaining competitive on reasoning benchmarks. RWKV models have garnered attention for their ability to handle large context sizes and support multiple languages. The discussion also touches on the international, uncredentialed community behind the RWKV project and its similarities to the early days of Eleuther AI. Overall, this episode provides insights into the advancements and challenges of LLM (Language Model) research beyond Transformers.

The discussion around the submission focuses on different aspects of the Receptance Weighted Key Value (RWKV) models and their potential impact on language models (LLMs). 
Some commenters express interest in the RWKV models and how they compare to traditional Transformers and RNNs. They note that RWKV models have shown promising results in terms of speed and performance for large context sizes. There is also discussion about the importance of quantization and stability in supporting RWKV models.
Others comment on the need for independent verification of the claims made about RWKV models and whether they can truly outperform existing models. Some suggest that the current landscape of language models needs more evidence and standardized benchmarks to properly evaluate their performance.
There is also discussion about the similarities between the emergence of RWKV models and the early days of Eleuther AI, emphasizing the role of the open-source and uncredentialed community in driving innovation in the field of LLM research. Overall, the discussion highlights the ongoing advancements and challenges in LLM research beyond Transformers, while also acknowledging the need for further validation and benchmarking of new models like RWKV.

### Vigil, the eternal morally vigilant programming language

#### [Submission URL](https://github.com/munificent/vigil) | 117 points | by [alex_marchant](https://news.ycombinator.com/user?id=alex_marchant) | [25 comments](https://news.ycombinator.com/item?id=37366678)

Vigil, the Eternal Morally Vigilant Programming Language: Vigil is a programming language that takes testing, contracts, and safety to a whole new level. It ensures that code that fails to meet certain specifications is not allowed to run. Vigil is similar to Python in terms of syntax and semantics but adds a layer of supreme moral vigilance. It uses the implore statement to require certain properties for parameters and the swear statement to state what it promises to return. If a function fails to meet its obligations or throws unhandled exceptions, Vigil will delete it from the source code. The goal is to ensure that the program meets its requirements by forbidding code that fails to do so automatically. This unique approach to programming safety sets Vigil apart from other languages.

The discussion on Hacker News surrounding the submission about Vigil, the morally vigilant programming language, covers a variety of opinions. Some users express skepticism about the effectiveness of Vigil, arguing that deleting code that fails to meet requirements may be too extreme of a punishment. Others joke about the consequences of making programming mistakes in Vigil, with one user suggesting that the language may lead programmers to redeem themselves by deleting their own code. Another user points out that while Vigil may punish code for unhandled exceptions, Python's handling of exceptions can also be problematic. The discussion also includes some off-topic comments, such as references to competitive programming and Monty Python sketches. Overall, the opinions on Vigil seem divided, with some finding it interesting and others expressing doubts about its practicality.

### Call of Duty enlists AI to eavesdrop on voice chat and help ban toxic players

#### [Submission URL](https://www.pcgamer.com/call-of-duty-enlists-ai-to-eavesdrop-on-voice-chat-and-help-ban-toxic-players-starting-today/) | 20 points | by [herbertl](https://news.ycombinator.com/user?id=herbertl) | [22 comments](https://news.ycombinator.com/item?id=37370934)

Activision Blizzard has announced a partnership with AI company Modulate to integrate its voice moderation tool, ToxMod, into the Call of Duty games. ToxMod is designed to identify and enforce against toxic speech in real-time, including hate speech, discriminatory language, and harassment. The AI tool analyzes tone and intent in speech, going beyond just keywords to determine what is and isn't toxic. It can also detect terms and phrases related to white supremacist groups and violent radicalization. ToxMod will roll out in Call of Duty at the launch of Modern Warfare 3 on November 10.

The discussion on the announcement of Activision Blizzard's partnership with Modulate to integrate the ToxMod voice moderation tool into Call of Duty games sparked a range of opinions. Some comments expressed concerns over the potential slippery slope of censorship and surveillance, questioning the need for monitoring and moderation in games. Others argued that the AI moderation tool's proficiency in identifying and banning toxic players greatly outweighed privacy concerns. There was further debate about the effectiveness of moderation in games like Overwatch and the impact it had on creating a more enjoyable gaming environment. Some commenters also discussed the nature of video games and their depiction of violence, while others highlighted experiences of encountering racist and toxic comments while gaming. The conversation shifted to issues of racism and sexism in gaming communities, with differing views on whether games themselves perpetuate these behaviors or simply reflect society. One commenter humorously suggested that fun is impossible in a world without racism and sexism, leading to further exchanges about the role of moderation and player-driven servers. The discussion ultimately expanded into larger societal issues surrounding racism and prejudice.

---

## AI Submissions for Wed Aug 30 2023 {{ 'date': '2023-08-30T17:10:19.632Z' }}

### Show HN: I automated half of my typing

#### [Submission URL](https://github.com/eschluntz/compress) | 738 points | by [eschluntz](https://news.ycombinator.com/user?id=eschluntz) | [298 comments](https://news.ycombinator.com/item?id=37326870)

"Compress" is a tool that automates the process of creating keyboard shortcuts from a corpus of your own writing. It analyzes the text to find common phrases that can be replaced with shorter abbreviations, and then generates configuration files for Autokey, a linux program that implements keyboard shortcuts. The tool also includes an optional feature to parse a Slack Data Export of your messages and create a corpus from it. The suggested abbreviations are ranked by the amount of characters saved multiplied by the frequency of the phrase. The tool aims to generate memorable abbreviations using heuristics and preferences. It provides instructions on how to install and use the tool, and offers customization options for adding or editing shortcuts. Overall, "Compress" simplifies typing by reducing the need to type out frequently used phrases.

The discussion around the submission "Compress" includes various comments and suggestions from the Hacker News community. One user mentions that there is already a built-in Chinese language typing system called Shuangpin, which allows users to type English characters that correspond to whole words in Chinese. The user also suggests that there are other typing systems and shorthand methods available for different languages. Another user raises concerns about the tool's longevity and suggests that a universal version could be more useful. They propose that the tool should be able to process books, emails, text messages, and other sources of diverse backgrounds and contexts to benefit a wider range of people.

A discussion thread also explores the limitations of shorthand systems and abbreviations. One user points out that common shorthand systems, like Evans Basic English Code or Phillips Code, are often insufficient in practice. Another user mentions that certain abbreviations, like dictionary abbreviations or abbreviations for frequently used words, can be considered. There are also comments discussing the practicality of using the tool on Android phones, with some users mentioning the difficulties they face while typing and suggesting alternative approaches to improve typing speed. Some users share their experiences with typing speed and discuss various factors that can impact typing efficiency. The discussion touches on factors such as keyboard ergonomics, typing techniques, and the benefits of specialized keyboards or typing systems. The topic of stenography is also brought up, with users recommending the Open Steno Project as a resource for those interested in shorthand and high-speed typing.

Overall, the discussion revolves around the potential benefits and challenges of the "Compress" tool, as well as broader topics related to typing efficiency and alternative typing methods.

### High-Speed AI Drone Overtakes World-Champion Drone Racers

#### [Submission URL](https://www.news.uzh.ch/en/articles/media/2023/Drone-race.html) | 275 points | by [geox](https://news.ycombinator.com/user?id=geox) | [167 comments](https://news.ycombinator.com/item?id=37323834)

Researchers from the University of Zurich and Intel have achieved a major milestone in artificial intelligence by creating an autonomous system capable of beating human champions at a physical sport: drone racing. The AI system, called Swift, won multiple races against three world-class drone racing champions. Swift was trained using reinforcement learning in a simulated environment, where it taught itself to fly by trial and error. The drone flies autonomously in real time, reacting to data from an onboard camera, just like human racers. Swift achieved the fastest lap overall, half a second ahead of the best lap by a human pilot. However, human pilots proved to be more adaptable to changing conditions. The researchers believe that pushing the envelope in autonomous flight is important not only for drone racing, but also for applications such as forest monitoring, space exploration, and rescue missions.

The discussion on this submission covers various aspects of the achievement of the AI system Swift beating human champions in drone racing. Some commenters discuss the technical aspects of training the AI using reinforcement learning and the complexity of modeling the dynamics of drones. Others highlight the challenges of simulating real-world physics accurately and the importance of learning from real-world data. There is also a mention of past achievements in robotics and AI in drone racing, as well as some humor about AI developing winning AI pilots. The limitations of the AI system are also mentioned, such as localization and external system mapping. Overall, the discussion shows excitement about the progress made in autonomous drone racing.

### ChatLZMA – text generation from data compression

#### [Submission URL](http://pepijndevos.nl/2023/07/15/chatlmza.html) | 117 points | by [bschne](https://news.ycombinator.com/user?id=bschne) | [18 comments](https://news.ycombinator.com/item?id=37318810)

A programmer on Hacker News shared an interesting idea on building ChatGPT using data compression techniques. They proposed compressing a large corpus of text to create an encoding table, compressing a prompt along with some random data, and then decompressing the random data to generate a response. While the approach may seem unconventional, the initial results were surprisingly amusing, producing coherent-ish words in mere seconds of "training." The programmer shared some code snippets in Python showcasing their progress so far. Although they are unsure whether the generated words are accidental prompts or just the result of flushing the buffer, the output is intriguing nonetheless. This experiment highlights the potential for leveraging compression algorithms in natural language processing tasks.

The discussion on the Hacker News submission revolves around the idea of using data compression techniques to build ChatGPT, a conversational AI model. Some comments express skepticism about compressing text and the effectiveness of the approach. Others mention similar compression-based approaches in different domains such as gaming FAQs and network data.  One commenter suggests building a context-token probability table using SQLite and suggests the use of prediction and learning algorithms to improve efficiency. Another commenter introduces the concept of Weighted Finite State Transducers for speech recognition. There is also a mention of GPT-40, a model that uses base64 compression as a potential starting point for implementing compression in ChatGPT. Some commenters share related resources, including a paper on Parameter-Free Classification Method Compressors and a YouTube video demonstrating simple implementations. Others express interest in exploring the idea further, with one commenter mentioning the possibility of applying the approach to Gulliver's Travels or Finnegans Wake. Overall, the discussion explores the potential benefits and challenges of incorporating data compression techniques into ChatGPT, with varying levels of enthusiasm and skepticism.

### Designing deep networks to process other deep networks

#### [Submission URL](https://developer.nvidia.com/blog/designing-deep-networks-to-process-other-deep-networks/) | 69 points | by [weird_science](https://news.ycombinator.com/user?id=weird_science) | [9 comments](https://news.ycombinator.com/item?id=37328609)

Researchers have explored the idea of designing deep neural networks (DNNs) that can process the weights of other neural networks, enabling them to perform operations on pretrained models. This concept has practical applications, such as editing 3D objects represented using Implicit Neural Representations or adapting image classifiers to different domains. Previous work has attempted to process deep weight spaces by vectorizing the parameters and applying a fully connected network. However, this approach overlooks the structural properties of neural networks and hurts generalization. To address this, researchers propose using deep architectures that are insensitive to transformations of model weights, similar to how convolutional neural networks are insensitive to image shifts. The study draws upon the field of Geometric Deep Learning, which focuses on learning objects while being invariant to transformations, to develop equivariant architectures. These architectures enable neural networks to effectively process the weights of other neural networks, creating opportunities for more efficient and flexible model operations.

The discussion on this submission begins with a comment by "Culonavirus" stating that the idea of designing deep neural networks that can process the weights of other neural networks is interesting. They mention that Nvidia's recent projects are pushing the boundaries of reconstructing 3D objects using deep learning.

In response, "gbrsr" expresses their surprise at Nvidia's hardware and software collaboration, suggesting that Nvidia is leveraging its strong chips for efficient computation, which they find impressive.

Another user, "rvz," comments that while the post explains the lack of technical questions in the responses, it takes experiments and reimplementation of the findings in papers to fully understand the research. They note that posting non-code explanations generates little interest.

"Pzz" finds the perspective interesting and connects it to a method called "Dynamic Dispatch Type Overloading" in programming. They provide an example to illustrate their point and mention their contribution in implementing a library for optimizing overall distortion in approximation.

"Lbbt" suggests that Nvidia's management is doubling down on AI chips, leading to exponentially increasing demand.

A user with the handle "wht-n-tsts" references the novel "Neuromancer" and its antagonist "Wintermute" in their comment.

"Blammar" responds to "wht-n-tsts" by suggesting that controlling chips that can self-assemble, like those described in the book, has been a long-standing desire for humans. They mention the challenges of self-manufacturing in vacuum tube systems and their resilience in space environments.

Overall, the discussion touches on Nvidia's projects, the implementation challenges, and references to sci-fi novels.

### ThirdAI Uses Ray for Parallel Training of Billion-Parameter NN on Commodity CPUs

#### [Submission URL](https://www.anyscale.com/blog/how-thirdai-uses-ray-for-parallel-training-of-billion-parameter-neural-networks-on-commodity-cpus) | 77 points | by [thirdailab](https://news.ycombinator.com/user?id=thirdailab) | [15 comments](https://news.ycombinator.com/item?id=37325173)

In a blog post by ThirdAI, they share how they utilize the Ray framework for distributed training of deep learning and foundational models using only CPUs. ThirdAI is an early-stage startup dedicated to democratizing AI models by training and deploying large neural networks on commodity CPU hardware. Their proprietary BOLT engine, a new deep learning framework, allows their sparse deep learning models to outperform dense architectures on GPUs in both training time and inference latency. To scale their models to terabyte-scale datasets and billion-parameter models, they have built a distributed data parallel engine powered by Ray. They discuss how Ray enables them to build an industry-grade distributed training solution with features such as fault-tolerance, multiple modes of communication, and seamless scalability. They also highlight their recent migration from Ray Core to Ray Trainer, which provides a simplified developer experience and enhanced fault tolerance and automatic scaling. With Ray, ThirdAI is able to achieve near-linear scaling for distributed training on a popular terabyte-scale benchmark dataset.