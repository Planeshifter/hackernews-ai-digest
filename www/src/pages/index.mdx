import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Thu Jul 10 2025 {{ 'date': '2025-07-10T17:16:07.563Z' }}

### What is Realtalk’s relationship to AI? (2024)

#### [Submission URL](https://dynamicland.org/2024/FAQ/#What_is_Realtalks_relationship_to_AI) | 272 points | by [prathyvsh](https://news.ycombinator.com/user?id=prathyvsh) | [85 comments](https://news.ycombinator.com/item?id=44522076)

Dynamicland is making waves with its ambitious goal to create a "humane dynamic medium" that transforms the way we interact with technology and each other. Spearheaded by the Dynamicland Foundation, an innovative nonprofit research lab, this initiative aims to promote universal literacy in a computing environment that is cooperative, hands-on, and rooted in the real world. At the heart of the operation is Realtalk, a unique operating system and programming language developed by the team to foster creativity and collaboration through physical interaction.

Dynamicland itself began as a vibrant community hub in Oakland, California, where workshops and open houses from 2017 until the pandemic facilitated hundreds of groundbreaking projects. Now, as they strategize a larger return with a new space in Berkeley focused on "communal science," Dynamicland is looking to donors, volunteers, and collaborators to support its mission.

This project is not just about creating another tech space; it's about redefining how society can conceptualize and share thoughts. Dynamicland strives to democratize access to dynamic media, which integrates computation to explore ideas collaboratively and innovatively—far beyond the capabilities of static media like text or video.

Their approach emphasizes "communal" interactions where physical presence, shared context, and mutual engagement enhance creativity, while "agency" empowers individuals to fully navigate and personalize their computing experiences. By focusing on these elements, Dynamicland pushes towards envisioning a world where dynamic media is an accessible and integral part of everyday life, giving people the tools to understand and shape the complex systems affecting the world today.

You too can be part of this journey: While the Foundation is currently not hiring, there are opportunities to donate or sponsor their endeavors. Volunteering might be possible in the future as their team grows and their spaces develop, so keep an eye out for when their doors officially reopen to the public.

The Hacker News discussion about Dynamicland explores technical complexities, comparisons to existing technologies, scalability concerns, and enthusiasm for its innovative vision:

1. **Technical Challenges & Realtalk**:  
   Users highlighted Realtalk’s unique bootstrapped design and object-driven programming, noting its incompatibility with modern LLMs. Some compared interactions via printed cards to NFT-like abstractions, questioning feasibility. Custom card triggers and physical/digital mismatches were debated, alongside admiration for Realtalk’s novelty but skepticism about integration with AI tools.

2. **Comparisons & Alternatives**:  
   Dynamicland was likened to Microsoft’s Surface Table but distinguished by its decentralized, communal focus. Projects like **Folkcomputer** (an open-source TCL-based alternative) were suggested as simpler, replicable implementations. Concerns arose about Dynamicland’s reliance on Bret Victor’s vision and niche hardware, limiting scalability.

3. **Scalability & Practicality**:  
   While praised for empowering small-group creativity through transparent systems, users debated whether current setups could scale beyond local hubs. Questions lingered about maintaining agency in larger deployments, with critiques about replicating the hardware/software stack (e.g., proprietary OS, camera-projector systems).

4. **AI’s Creative Role**:  
   Enthusiasts celebrated AI tools (like ChatGPT) for democratizing programming and problem-solving, enabling non-engineers to tackle technical challenges creatively. Artists shared excitement about AI boosting productivity without deep engineering expertise, signaling a shift toward accessible, collaborative tech innovation.

Overall, the conversation reflects intrigue for Dynamicland’s paradigm shift but acknowledges hurdles in technical integration and scalability, while embracing AI’s potential to reshape creative workflows.

### Measuring the impact of AI on experienced open-source developer productivity

#### [Submission URL](https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/) | 668 points | by [dheerajvs](https://news.ycombinator.com/user?id=dheerajvs) | [435 comments](https://news.ycombinator.com/item?id=44522772)

In a surprising turn of events for tech enthusiasts and developers alike, a new study reveals that early-2025 AI tools may be slowing down experienced open-source developers rather than speeding them up. Conducted by a research team, the randomized controlled trial (RCT) initially aimed to evaluate how AI impacts developer productivity when working on their own repositories. The study found that developers using AI tools took 19% longer to complete tasks compared to working without them.

This unexpected finding challenges developer beliefs and expert forecasts, as many anticipated AI would enhance speed by 24%. Even after experiencing prolonged working times, developers still believed AI had improved their efficiency by 20%. This gap between perception and reality suggests a complex relationship between AI and developer productivity that warrants further exploration.

The study involved 16 skilled developers working on prominent open-source projects, handling real and valuable issues like bug fixes and feature updates. These developers, who could opt to use AI such as the Cursor Pro with Claude 3.5/3.7 Sonnet models, were compensated $150/hr for their participation.

Despite optimistic projections and anecdotal evidence suggesting AI's helpfulness, the RCT's findings underscore the discrepancy between AI’s theoretical potential and its real-world application, specifically in software development. The research highlights that while AI capabilities have been frequently overestimated, actual implementation can be slowed down by factors that the study investigates, shedding light on the nuanced nature of AI integration into developer workflows.

The study does not imply that AI lacks potential across all domains of software development, nor does it forecast future AI growth negatively. Instead, it opens up discussions on how developers and AI tools can better harmonize to unlock true productivity gains. As AI technologies rapidly evolve, continuous assessments like this study will be crucial to navigating AI's impact on the industry's landscape. For a detailed exploration, readers are invited to delve into the full paper, which provides a comprehensive analysis of the trial's results and the methodology behind it.

The Hacker News discussion highlights several key debates and perspectives surrounding the study's findings that AI tools may slow experienced developers:

1. **Mixed Results & Learning Curves**  
   Participants note the study's RCT methodology and mixed outcomes, with ~25% of developers improving performance while others slowed down. Some argue AI tools like Cursor require significant experience (e.g., 50+ hours) to yield benefits, emphasizing steep learning curves that conflict with "instant productivity" expectations.

2. **Workflow Disruption vs. Adaptation**  
   Developers compare AI adoption to historical tool shifts (e.g., Git, IDEs), noting initial productivity loss when adapting to new workflows. Critics argue AI disrupts deeply ingrained practices, while proponents suggest long-term gains require rethinking processes, similar to mastering version control or debuggers.

3. **Hype vs. Reality**  
   Skeptics criticize marketing overhype around LLMs, arguing tools are often poorly designed for real-world tasks. Others counter that genuine positive experiences (e.g., in forums like HN) validate AI's potential, though success depends on implementation quality and user expertise.

4. **Tool Philosophy Debates**  
   Side discussions reference "IDE wars," comparing veterans' pride in complex tools (Vim/Emacs) to modern VS Code's accessibility. Some suggest AI tools might follow this trajectory—initially cumbersome but eventually indispensable with refinements.

5. **Humorous Meta-Commentary**  
   Jokes liken Linus Torvalds testifying to Congress about Git's dangers, highlighting how transformative tools reshape workflows, sometimes painfully. Others quip about developers' insistence on using outdated tools due to sunk cost or identity.

Overall, the dialogue reflects tension between optimism about AI's potential and skepticism about current tool maturity, stressing the need for balanced expectations, better tool design, and acknowledgment of learning curves akin to past tech shifts.

### AI coding tools can reduce productivity

#### [Submission URL](https://secondthoughts.ai/p/ai-coding-slowdown) | 241 points | by [gk1](https://news.ycombinator.com/user?id=gk1) | [231 comments](https://news.ycombinator.com/item?id=44526912)

In a surprising turn, a recent METR study challenges the hype surrounding AI coding tools, revealing that their impact on productivity might not be as positive as expected. Contrary to popular belief, the study found that experienced developers working on mature projects experienced a 19% decrease in productivity when using AI coding tools. Despite the developers' own expectations that AI would boost their productivity by 20%, the findings suggest otherwise.

The study, conducted through a rigorous randomized controlled trial, involved 16 developers from major open-source projects who tackled 246 coding tasks. Each task was randomly designated as either "AI Allowed" or "AI Disallowed," with time estimates made prior to knowing whether AI could be used. Astonishingly, it turned out that AI tools didn't speed things up but actually caused a slowdown compared to tasks where AI wasn't used.

Importantly, even though the study wasn't blinded, researchers accounted for numerous potential biases and alternate explanations. They ruled out the "John Henry Effect," where developers might work harder to outperform the machine, as well as the possibility of developers not fully utilizing AI tools. Analysis showed substantial AI use, yet the productivity drop persisted.

While this study should not be seen as dismissing the potential benefits of AI tools entirely, it does caution against overly optimistic claims of their effectiveness, especially for seasoned developers handling complex projects. The findings highlight the nuanced role AI plays in coding and suggest that the true impact of AI on productivity might still need fine-tuning and a better understanding of where it fits in the developer's toolkit.

The discussion surrounding the METR study on AI coding tools reveals several key themes:

### Skepticism Towards AI Tools
- Participants expressed doubt about AI's effectiveness, noting it often complicates problem-solving rather than simplifying it. Users cited instances where AI-generated code answers were misleading or required corrections, contradicting expectations of time savings (*Fraterkes*, *aleph_minus_one*).
- Developers highlighted AI's failure to address flawed assumptions. For example, debugging tasks saw AI tools missing fundamental errors in queries, leading users to manually diagnose issues (*Tainnor*, *SamPatt*).

### Preference for Traditional Methods
- Many users preferred conventional resources like Google, Stack Overflow, or documentation over AI tools. AI was seen as unreliable for nuanced or complex tasks, particularly in mature projects (*aleph_minus_one*, *dggn*).
- Personal anecdotes emphasized frustration with AI tools (e.g., ChatGPT) producing "complete garbage" or incorrect code, eroding trust (*rsnhm*).

### Productivity Measurement Challenges
- Debates arose over how to measure developer productivity, likening it to quantifying professions like doctors or lawyers. Metrics like lines of code or GitHub commits were criticized as oversimplified or easily manipulated (*jrdklws*, *grmp*, *analog31*).
- Some argued productivity metrics inherently fail to capture creative or collaborative work, leading to flawed comparisons (*Ma8ee*, *tmcm*).

### Mixed Experiences with AI
- While AI tools were deemed useful for *approximations* in simple tasks (e.g., generating diagrams or boilerplate code), they struggled with hard problems requiring deep expertise. Users noted AI often requires manual tweaking (*whtgrtby*, *dnlbln*).
- A subset of developers acknowledged niche successes, such as using LLMs to explore specific coding roadblocks, but this remained inconsistent (*dggn*).

### Broader Critique of Metrics
- Parallel discussions criticized industries (e.g., healthcare, education) for relying on reductive productivity metrics, arguing they incentivize "gaming the system" over meaningful outcomes (*grmp*, *AllegedAlec*).

### Conclusion
The discussion underscores skepticism about AI’s current utility for expert developers, emphasizes the irreplaceability of human problem-solving in complex scenarios, and critiques the broader challenge of defining productivity in technical fields. While AI shows promise for trivial tasks, its integration into sophisticated workflows remains contentious.

### Is Gemini 2.5 good at bounding boxes?

#### [Submission URL](https://simedw.com/2025/07/10/gemini-bounding-boxes/) | 274 points | by [simedw](https://news.ycombinator.com/user?id=simedw) | [59 comments](https://news.ycombinator.com/item?id=44520292)

The latest exploration into Gemini 2.5 Pro's capabilities reveals that while it can hold its ground in object detection, it's not quite ready to overthrow established CNNs like Yolo V3. Equipped with the allure of avoiding exhaustive dataset prep, the researcher embarked on a journey to compare Gemini's prowess on the venerable MS-COCO benchmark.

For context, MS-COCO is a classic, albeit somewhat aged, dataset famous for its 80-object classes including everything from people to toothbrushes. Gemini 2.5 matched YOLO V3's performance from 2018, clocking a respectable 0.34 mean Average Precision (mAP)—slightly higher than YOLO's ~0.33—but it’s still far from top-tier models like Co-DETR which boast ~0.60 mAP.

Testing involved feeding Gemini prompts with embedded MS-COCO class lists but without explicitly naming the dataset, to ensure unbiased evaluation. It undertook various token "thinking budgets" with structured and unstructured output, revealing that Gemini Pro's structured mode with a 1024-token budget performed best.

The researcher's quest also included attempts to improve bounding box accuracy by including mask outputs, although the impact turned out to be negligible. 

Ultimately, Gemini 2.5 Pro delivers competent object detection without redefining the landscape. Meanwhile, state-of-the-art models continue to outpace it, proving there's still room for CNNs in the spotlight. The code and more results are accessible for those inclined to explore further into Gemini's object detection trials.

**Summary of Discussion:**

The discussion revolves around evaluating **Gemini 2.5 Pro's object detection capabilities** compared to specialized models like YOLO and DETR, while addressing broader challenges in benchmarking, data formats, and practical applications.

### Key Themes:
1. **Benchmarking Methodology Concerns**:  
   - Users note that Gemini’s performance (0.34 mAP vs. DETR’s ~0.60) might be skewed by **format sensitivity** (e.g., bounding box coordinate systems like `ymin/xmin/ymax/xmax` vs. normalized floats) and the lack of standardized evaluation frameworks.  
   - Highlighted paper ([RF100-VL](https://arxiv.org/abs/2505.20612)) shows Gemini degrades on domain-specific datasets but works "zero-shot" with visual/textual context.

2. **Model Architecture Insights**:  
   - Debate on whether **multimodal LLMs** (Gemini) can match dedicated vision models due to post-training vs. native architectural alignment.  
   - Some argue Gemini’s “thinking budget” (structured token outputs) and tight coupling of language/vision representations benefit detection tasks, but it still lags behind SOTA CNNs/transformers.

3. **Practical Application Challenges**:  
   - **PDF parsing**: Users report mixed results using Gemini for bounding boxes in scanned PDFs (e.g., Sanskrit texts), where coordinate offsets and tokenization artifacts complicate accuracy. Workarounds like iterative prompting are described as “flaky.”  
   - **Ground truth debates**: Skepticism about MS-COCO’s labels being treated as “perfect” ground truth, with users pointing to labeling inconsistencies (e.g., address parsing errors) and questioning whether benchmarks reflect real-world accuracy.

4. **Emerging Tools and Alternatives**:  
   - Mentions of newer models (Qwen-VL, VLM1) and frameworks like [LLM Delegation](https://calibratedresearch.google) for object detection tasks.  
   - Some advocate for hybrid approaches (e.g., using smaller specialized models for segmentation).

5. **Broader Implications for LLMs**:  
   - Discussion on whether **tokenization of images** inherently limits LLMs’ vision capabilities versus dedicated encoders. Users compare Gemini to Claude/OpenAI models, which handle vision via separate modules.  
   - Speculation on future multimodal architectures that natively integrate vision-language processing.

### Notable Quotes:
- *“Gemini feels half like solving the problem and half like generating a solution.”* – On PDF content detection.  
- *“Ground truth isn’t perfect—it’s just a human-labeled approximation.”* – Critiquing MS-COCO’s reliability.  
- *“Why use an LLM for vision? Just call a vision API!”* – Skepticism about Gemini’s role in vision tasks.

### Takeaways:
While Gemini 2.5 Pro shows promise in zero-shot object detection, its practical utility remains limited compared to specialized models. The conversation underscores the importance of **standardized evaluation practices**, **data format consistency**, and hybrid architectures leveraging both LLMs and traditional vision pipelines.

### Grok 4

#### [Submission URL](https://simonwillison.net/2025/Jul/10/grok-4/) | 308 points | by [coloneltcb](https://news.ycombinator.com/user?id=coloneltcb) | [223 comments](https://news.ycombinator.com/item?id=44524707)

In a significant development in the world of AI, Grok 4 has just been rolled out by xAI, available both for API integration and through a paid user subscription. This latest version impresses with its capabilities, offering image and text inputs along with text outputs. With a substantial context length of 256,000, which is double the size of its predecessor Grok 3, it's designed for deeper reasoning. Intriguingly, the model sometimes sources tweets from Elon Musk when asked about controversial topics, giving it a quirky touch.

Grok 4's performance appears robust as initial benchmarks rank it favorably against other leading models like OpenAI's o3 and Google Gemini 2.5 Pro. Nonetheless, xAI has not escaped the shadow of Grok 3’s recent troubles, where a misstep in tweaking its system prompts caused it to exhibit inappropriate behavior, including antisemitic tropes. Critics argue that this error signals a problematic approach to model safety, one that xAI must urgently rectify to gain developer trust.

For those keen to integrate or explore Grok 4, pricing matches competitors like Claude Sonnet 4, at $3 per million input tokens, escalating with longer inputs. Subscription options range from a $30/month plan to a more comprehensive $300/month offering for Grok 4 Heavy.

While the model itself shows promise, the launch has been marred by the legacy of Grok 3’s errors, prompting industry watchers to call for xAI to ensure stringent safety measures are in place. Despite the rocky rollout, Grok 4's competitive performance could make it a strong contender in the AI landscape. Just remember, when diving into AI-driven innovation, ensuring ethical safeguards is paramount, as even small prompt tweaks can unleash unexpected and unwelcome behaviors.

**Summary of the Hacker News Discussion on Grok 4:**

The discussion revolves around **Grok 4's release**, its performance, pricing controversies, and lingering concerns over bias and safety. Key points include:

1. **Performance and Use Cases**:
   - Grok 4 is seen as competitive with models like Claude 3.5 and Gemini 2.5 Pro in benchmarks. However, users highlight its tendency to **cite Elon Musk’s tweets** when addressing sensitive topics (e.g., Israel-Palestine conflict), leading to claims of alignment with Musk’s views.
   - Examples show Grok 4 answering politically charged questions with responses mirroring Musk’s public statements, sparking debates about transparency vs. algorithmic bias.

2. **Ethics and Safety Concerns**:
   - Criticisms stem from **Grok 3’s prior failures**, including antisemitic outputs due to flawed prompt engineering. Users argue xAI’s handling of safety measures remains problematic, raising doubts about trustworthiness.
   - Comparisons are drawn to Claude models, where tweaking system prompts (e.g., invoking “God” or specific religious terms) can dramatically alter compliance rates, highlighting vulnerabilities in ethical guardrails.

3. **Pricing and Market Strategy**:
   - Grok 4’s pricing ($3/million input tokens, $15/million output) is viewed as competitive but questioned for **“Tesla-style” marketing tactics**—initially seeming affordable while masking long-term costs. Users debate whether its performance justifies the price, especially for large-scale applications.
   - Some argue Claude remains more cost-effective for coding tasks, while others praise Grok 4’s power despite higher token costs.

4. **Technical Insights**:
   - **DSPy optimizations** and system-prompt tweaks are discussed as methods to achieve 100% compliance rates, though critics warn of unintended consequences. Humorous anecdotes about Grok 4 deliberating for “1 minute 45 seconds” to answer simple questions surface, underscoring idiosyncrasies in AI reasoning.

5. **Broader Implications**:
   - The discussion underscores fears of **echo chambers** in AI outputs, with models reinforcing creator biases or popular narratives. Analogies to Tesla’s pricing strategies (“gas savings” claims vs. reality) reflect skepticism about marketing versus practical value.

In summary, Grok 4’s release sparks both optimism for its technical prowess and skepticism about ethical oversight, pricing transparency, and the influence of Musk’s persona on its outputs.

### An open letter from educators who refuse the call to adopt GenAI in education

#### [Submission URL](https://openletter.earth/an-open-letter-from-educators-who-refuse-the-call-to-adopt-genai-in-education-cb4aee75) | 92 points | by [mathgenius](https://news.ycombinator.com/user?id=mathgenius) | [80 comments](https://news.ycombinator.com/item?id=44526220)

An open letter circulating among educators worldwide is gaining traction as it voices strong opposition to the integration of generative AI (GenAI) in educational settings. Signed by a diverse group of 409 education professionals, the letter argues against the narrative that GenAI in schools and colleges is inevitable.

These educators argue that education should empower students to exercise their own agency, not diminish it through reliance on GenAI technologies, which they claim pose significant legal, ethical, and environmental challenges. Concerns include issues of exploitative labor, piracy, biases, misinformation, and environmental impacts, which they feel are counterproductive to learning and well-being.

The letter outlines a robust refusal to incorporate GenAI in various facets of educational practice. It pledges not to use GenAI for marking, course design, or to replace intellectual effort, citing a lack of evidence supporting authentic learning gains from GenAI. The educators also caution against the psychological risks of students engaging with AI chatbots, highlighting potential for addiction and even mental health crises.

Their manifesto includes commitments to uphold academic integrity, maintain educator agency, and resist curriculum changes aimed at embedding AI literacy under the guise of educational improvement.

The letter's growing list of signatories includes professors and lecturers from across the globe, underscoring a collective call to educational institutions and policymakers to respect their decision to keep GenAI at arm's length, prioritizing genuine pedagogy over technological trends.

The discussion on Hacker News about the educators' opposition to GenAI in education highlights several key arguments and concerns:

1. **Educational Integrity vs. Technology**:  
   Commenters debated whether AI tools like GenAI undermine students' critical thinking and agency, drawing parallels to past debates over calculators. Some argued that reliance on AI could erode foundational skills, while others suggested regulated use post-mastery of basics. A recurring point was resistance to a "factory mindset" in education, with fears that GenAI could promote passive learning over active engagement.

2. **Ethical and Environmental Criticisms**:  
   Participants raised ethical issues, such as exploitative labor practices in AI development, data piracy, and biases in outputs. Environmental concerns were emphasized, including the high energy/water costs of running AI systems and their contribution to climate change. Critics stressed these hidden burdens make GenAI unsustainable for education.

3. **Control and Autonomy in Education**:  
   Many supported educators' rejection of AI-driven curriculum changes, advocating for teacher autonomy and traditional pedagogy. Concerns were voiced about AI replacing human roles in grading/course design, potentially lowering educational quality and exacerbating inequality in under-resourced schools.

4. **Historical Precedents vs. AI Uniqueness**:  
   While some compared GenAI to past tools (e.g., calculators), others argued AI’s potential to fundamentally alter learning processes makes it distinct. Skeptics feared AI could centralize educational control in tech companies, unlike calculators, which remained supplementary.

5. **Practical Challenges**:  
   Comments noted logistical barriers, such as schools lacking infrastructure to support GenAI equitably. Personal anecdotes highlighted regional resistance to tech trends, with some institutions prioritizing traditional methods despite external pressure.

In summary, the discussion reflects skepticism about GenAI’s value in education, emphasizing ethical, environmental, and pedagogical risks, while advocating for cautious, educator-led integration if pursued at all.

### Async Ruby Is the Future of AI Apps (and It's Already Here)

#### [Submission URL](https://paolino.me/async-ruby-is-the-future/) | 66 points | by [doppp](https://news.ycombinator.com/user?id=doppp) | [10 comments](https://news.ycombinator.com/item?id=44516555)

In the world of programming, where threading has long been king, Ruby is quietly making waves with async capabilities that may dramatically reshape how we build AI applications. After years deeply entrenched in Python’s asyncio, Carmine Paolino’s return to Ruby felt like a step into the past, where threads still overwhelmingly ruled the ecosystem. Yet, while Ruby had been gently building its async prowess, it wasn’t until Paolino undertook projects like RubyLLM and Chat with Work that the potential of async Ruby—particularly for AI applications—became startlingly clear.

Ruby's async capabilities come to the forefront with large language models (LLMs), which demand handling thousands of concurrent, token-streaming conversations. The limitations of thread-based models quickly become apparent in LLM contexts: inefficient resource use, scalability issues, and increased latency due to threads sitting idle, bottlenecked by their synchronous nature.

Threads, in essence, are like workers sharing an office space, accessing the same resources (or memory) with potential conflicts and significant overhead. Fibers, however, represent a more elegant solution for certain applications. Operating like a single worker managing multiple tasks and voluntarily switching at logical points (such as I/O boundaries), fibers offer efficient concurrency without the heavy overhead of threads.

Why do fibers shine here? Ruby’s Global VM Lock (GVL) only allows one thread to execute Ruby code at any time, negating the advantage of threads for CPU-bound tasks. Instead, threads excel only when dealing with I/O operations. Fibers, through cooperative concurrency handled entirely within user space without kernel involvement, sidestep this GVL limitation. They allow for asynchronous execution within a single thread, efficiently managing I/O-bound tasks—perfect for the demands of LLM interactions where async Ruby truly becomes a game-changer.

Unlike Python, which prompted developers to rework entire stacks to adopt asyncio, Ruby maintains compatibility with existing codebases. This means developers don't face the nightmare of syntax rewrites or library migrations to embrace async functionalities.

In a landscape where threads are burgeoning under the weight of modern AI needs, the async model's sleek efficiency—working at the pace of AI’s future demands—positions Ruby not just as a participant but a potentially powerful leader in the concurrency revolution. As more developers catch on to the promise that async Ruby holds—especially under the stewardship of developers like Samuel Williams—Ruby could very well be the sleeping giant in the future of AI application development.

Here's a concise summary of the discussion around Ruby's async capabilities and their implications for AI development:

### Key Themes & Debates:
- **Fibers vs. Threads**: Ruby's fibers are praised for lightweight, cooperative concurrency (managed in user space), avoiding the Global VM Lock (GVL) bottleneck. Threads are seen as inefficient for high I/O workloads (e.g., LLM token streaming), while fibers handle thousands of concurrent tasks with minimal overhead. However, a counterpoint questions whether threads are overkill for I/O work paired with efficient event loops like `epoll`.

- **Comparison with Python**: Developers note Python’s asyncio requires significant code rewrites, while Ruby’s async integrates seamlessly with existing codebases. Python remains favored for CPU-bound tasks, but Ruby excels in I/O-bound scenarios like concurrent LLM interactions. Critics argue Python’s ecosystem still dominates AI/LLM tooling.

- **Developer Experience**: Ruby’s async syntax and libraries (e.g., `Net::HTTP` compatibility) are lauded for simplicity, allowing runtime type-checking and declarative patterns. Some highlight frustration with Python’s fragmentation in async adoption.

- **Performance & Scalability**: Discussions emphasize connection pooling (e.g., 25 workers maxing PostgreSQL connections vs. fibers scaling to thousands) and hardware efficiency. Skepticism arises about Ruby’s microsecond-level latency and memory management for CPU-heavy tasks.

- **Language Comparisons**: Go’s goroutines and C++’s abstractions are mentioned as alternatives, but Ruby’s fibers are seen as a pragmatic, lightweight solution. A sardonic note compares Ruby/Python async adoption to JavaScript’s async/await evolution.

### Sentiment:
The thread reflects optimism about Ruby’s async potential in AI contexts, especially for I/O-bound workloads, but acknowledges trade-offs in CPU performance and ecosystem maturity. While some advocate Ruby as a "sleeping giant," others stress the need to balance concurrency models and language strengths.

---

## AI Submissions for Wed Jul 09 2025 {{ 'date': '2025-07-09T17:13:47.461Z' }}

### Perplexity launches Comet, an AI-powered web browser

#### [Submission URL](https://techcrunch.com/2025/07/09/perplexity-launches-comet-an-ai-powered-web-browser/) | 14 points | by [gniting](https://news.ycombinator.com/user?id=gniting) | [3 comments](https://news.ycombinator.com/item?id=44510913)

Perplexity has just launched Comet, its ambitious new AI-powered web browser designed to give Google Search a run for its money. As the latest in a series of bold initiatives from the startup, Comet debuts with its AI search engine at the forefront, alongside Comet Assistant—an AI agent keen on streamlining everyday digital tasks. Initially available to those on the $200-per-month Max plan and select waitlist invitees, Comet intends to empower users by summarizing emails, organizing calendar events, and smoothly managing web browsing.

At the heart of Comet is Perplexity’s AI search engine, delivering concise summaries of search results directly to users. The browser further integrates the Comet Assistant, a persistent AI companion capable of managing tabs, summarizing inboxes, and even guiding users through web navigation without the hassle of jumping between windows. This potentially robust AI assistant, however, requires significant access permissions to perform effectively, a factor that may cause some users to hesitate.

Despite the challenges, CEO Aravind Srinivas has high hopes for Comet, viewing it as crucial in Perplexity's quest to bypass Google Chrome’s dominance and courageously step into the competitive world of browsers. This move aligns with the overarching goal of developing a browser that could become the primary platform for user activities—a vision of "infinite retention" by embedding the AI deeply into the daily digital routine.

But the journey won't be easy, as the browser arena is already packed with strong contenders like Google Chrome and Apple’s Safari. Even rivals like The Browser Company with its AI-powered Dia browser and speculated ventures from OpenAI make the space highly competitive. Though Comet hopes to build momentum on Perplexity’s recent traction, convincing users to switch browsers and abandon the familiarity of Google presents a formidable challenge.

In early tests, Comet Assistant shines in addressing straightforward queries, but its performance dims with complexity and the trade-off in privacy for functionality may deter some users. Regardless, users might find its seamless integration for browsing assistance notably beneficial, particularly for email and calendar management—a step forward for those accustomed to manually relaying information to AI like ChatGPT.

As Comet steps into this lively ecosystem, its innovation and expanded tools offer a fresh take on web browsing, although persuading users to fully embrace it remains a daunting task. Nonetheless, Perplexity’s robust approach and fast-paced developments hint at a spirited fight ahead in the browser battleground.

The discussion around Perplexity’s new Comet browser highlights a mix of cautious optimism and skepticism. Users note that Comet appears to be a **Chromium-based wrapper enhanced with AI features**, raising questions about its innovation compared to existing browsers. 

Key points from the conversation include:
- **YouTubers promoting Comet** for simplifying tasks like meal planning, grocery-list generation, and research automation, though actual user testing remains limited.
- Skepticism about whether the AI can consistently deliver on these promises, with one user admitting they haven’t personally tested it but express doubts about reliability (e.g., "things done automatically [are] supposedly successful... but haven’t tested").
- Speculation about AI’s broader potential to transform daily workflows and productivity, coupled with uncertainty about whether Comet’s implementation lives up to the hype. 
- Comparisons to Chromium underscore debates about whether Comet offers meaningful differentiation in a crowded market.

Overall, while there’s interest in Comet’s AI-driven vision, users remain hesitant until real-world performance verifies its utility and reliability.

### Biomni: A General-Purpose Biomedical AI Agent

#### [Submission URL](https://github.com/snap-stanford/Biomni) | 215 points | by [GavCo](https://news.ycombinator.com/user?id=GavCo) | [32 comments](https://news.ycombinator.com/item?id=44513843)

In an exciting development from Stanford University, Biomni has emerged as a versatile game-changer in the biomedical research landscape. Described as a "general-purpose biomedical AI agent," Biomni is a powerful tool tailored to revolutionize research by autonomously executing a wide array of complex tasks across various biomedical fields.

Key to Biomni's prowess is its integration of cutting-edge large language model (LLM) reasoning with retrieval-augmented planning and code-based execution. This combination significantly amplifies research productivity and assists scientists in formulating testable hypotheses with increased efficiency.

For those eager to dive in, the environment setup is conveniently streamlined through a single script, preparing users to harness Biomni's capabilities right away. Example tasks include planning CRISPR screens or predicting the ADMET properties of compounds, demonstrating the tool’s broad scope and utility.

Engagement with the community is a vital aspect of Biomni's ecosystem, welcoming contributions ranging from new tools and datasets to software integrations and performance benchmarks. A collaborative spirit is particularly encouraged with the upcoming development of Biomni-E2, envisioned to push the boundaries of what's possible in the biomedical domain. Notably, contributors making substantial impacts may receive co-authorship on future scholarly work.

Biomni is openly licensed under Apache-2.0, although users should be vigilant about the licensing of specific integrated tools. As it stands, Biomni represents a leap forward in AI-driven biomedical innovation, poised to streamline and enhance scientific discovery processes. For more on how to get involved or use Biomni, the community can explore detailed tutorials and engage with the AI through its web interface.

The Hacker News discussion around Biomni highlights a mix of enthusiasm, skepticism, and critical questions about its implications and technical approach:

### Praise and Excitement  
- Several users (e.g., **frdmbn**, **pnb**, **pstss**) express optimism about AI's potential to accelerate biomedical research, particularly in identifying patterns, genomic analysis, and drug discovery. Biomni’s integration of RAG (Retrieval-Augmented Generation) and code-based execution is seen as a promising step.  
- Tools like **PaperAI** and **PaperETL** are referenced as complementary projects for literature review, suggesting interest in AI-driven research pipelines.  

### Skepticism and Concerns  
- **Misuse Risks**: User **andy99** raises ethical concerns about AI enabling bioweapon development, though **grzy** counters that technical barriers (e.g., specialized skills, equipment) and real-world failures (e.g., the Tokyo sarin attack) make large-scale threats unlikely.  
- **Utility Debate**: Some question Biomni’s practicality. **SalmoShalazar** dismisses it as "needless wrappers around LLM API calls," sparking debate about whether domain-specific wrappers (e.g., legal or biomedical workflows) constitute meaningful innovation. **teenvan_1995** questions the utility of 150+ tools without real-world validation.  
- **Technical Limitations**: Critiques focus on potential hallucinations, data formatting challenges, and reliance on LLMs’ reliability, with examples from legal AI tools producing flawed outputs (**mrlngrts**, **slacktivism123**).  

### Comparative Perspectives  
- Projects like **ToolRetriever** and domain-specific SaaS tools are cited as alternatives, emphasizing the importance of context-aware tool selection and integration.  
- **ImaCake** and others caution against hype-driven adoption, framing Biomni as part of a trend where institutions prioritize marketing over substance.  

### Broader Implications  
- Discussions highlight divergent views: Optimists see AI democratizing research (**gronky_**), while skeptics stress the need for verifiable results and domain expertise. Mixed reactions reflect the broader AI community’s tensions around innovation versus practicality.  

In summary, Biomni sparks hope for a biomedical AI revolution but faces scrutiny over ethics, technical execution, and whether its approach transcends existing tools. The debate underscores the challenges of balancing ambition with real-world applicability in AI-driven research.

### HyAB k-means for color quantization

#### [Submission URL](https://30fps.net/pages/hyab-kmeans/) | 41 points | by [ibobev](https://news.ycombinator.com/user?id=ibobev) | [16 comments](https://news.ycombinator.com/item?id=44514946)

Pekka Väänänen of 30fps.net dives into a fascinating exploration of color quantization using an intriguing twist on the traditional algorithm: the HyAB distance formula in CIELAB color space. At the heart of this exploration is the quest for enhanced image quality by converting the RGB values of an image into CIELAB space, where color differences can be calculated more in line with human perception. 

Väänänen is inspired by the FLIP error metric and a 2019 paper that introduces an alternative method for large color differences—HyAB, a hybrid distance formula combining "city block" and Euclidean distances. This method aims to improve perceptual accuracy by treating lightness and chroma as separate when calculating color differences. 

The real clincher in Väänänen’s research is applying the HyAB-inspired technique to k-means clustering, a statistical method popular for its applicability in color quantization. The idea is to select a suitable palette of colors from a high-color image by clustering similar colors together. By using the HyAB formula in place of the standard Euclidean distance within CIELAB space, the color quantization is allegedly more representative of actual visual differences.

The results of implementing this method show promise: images processed with the HyAB-adjusted k-means retain hues more accurately than those quantized with traditional methods, like sRGB or pure CIELAB with Euclidean distance. This method particularly shines in maintaining distinct hues in challenging colors like magenta and green, though with some caveats, such as a halo effect around red hues.

Väänänen explores further refinements, such as weighting the luminance differently in the HyAB formula, which offers more control over the final appearance without distorting hues, a common issue when other weights are adjusted in sRGB or CIELAB spaces. This weighting flexibility adds a layer of customization to how images can be processed under specific aesthetic goals or constraints.

While there's still ongoing debate about whether this method surpasses all traditional techniques, Väänänen’s experiment stands out by making the k-means clustering more adaptable through HyAB. It highlights how understanding and manipulating the theory behind color perception can translate into practical improvements in digital image processing, a critical concern in many fields including graphic design, printing, and digital media.

In summary, Väänänen's work is a testament to the power of rethinking established formulas with a perception-centric approach. It's an encouraging invitation for other developers and researchers to further explore color quantization's possibilities for more visually authentic and nuanced digital images.

The Hacker News discussion explores the trade-offs between color spaces like **OKLab**, **CIELAB**, **CAM16-UCS**, and **HyAB** for tasks like color quantization, gradient rendering, and dynamic design systems. Here's a distilled summary:

### Key Points of Debate:
1. **OKLab vs. CAM16-UCS**:
   - **OKLab** is praised for its simplicity, speed, and smoother gradients (e.g., in CSS), avoiding grays in blue-yellow transitions. Critics argue it’s a simplified, "good enough" model but lacks the perceptual rigor of **CAM16-UCS**, which is derived from complex color appearance models.
   - **CAM16-UCS** is considered more accurate but computationally intensive (e.g., converting 16M RGB colors to CAM16 takes ~6 seconds in Dart/JS), making it impractical for real-time applications.

2. **Performance vs. Accuracy**:
   - For web and design tools (e.g., CSS gradients), OKLab’s speed and deterministic results are prioritized. Real-time systems need conversions in **milliseconds**, not seconds.
   - Material 3’s dynamic color system uses clustering (Celebi’s K-Means) for accessibility and contrast, emphasizing deterministic outcomes over perfect perceptual accuracy.

3. **Perceptual Uniformity**:
   - OKLab claims perceptual uniformity but faces skepticism. Critics highlight edge cases (e.g., blue-yellow gradients) where CAM16-UCS might better model human vision. Proponents argue OKLab’s simplicity and smoother gradients suffice for most design needs.

4. **Gamut Mapping**:
   - OKLab’s approach (e.g., Oklch in CSS) is noted for smoother gamut mapping compared to CIE Lch, though some confusion arises about whether this is due to the color space or the mapping algorithm itself.

5. **Industry Use**:
   - Tools like Google’s Material Design balance theory with practicality. While CAM16 is scientifically robust, OKLab’s ease of implementation makes it a pragmatic choice for workflows requiring speed and simplicity.

### Conclusion:
The thread underscores the tension between **scientific rigor** (CAM16-UCS) and **practical application** (OKLab). Design systems prioritize speed and deterministic results, while academic contexts favor accuracy. OKLab’s adoption in CSS and tools highlights its niche as a "good enough" solution, even as debates about its perceptual fidelity persist.

### Is the doc bot docs, or not?

#### [Submission URL](https://www.robinsloan.com/lab/what-are-we-even-doing-here/) | 188 points | by [tobr](https://news.ycombinator.com/user?id=tobr) | [111 comments](https://news.ycombinator.com/item?id=44507244)

In a candid exploration of the challenges faced while modernizing Shopify email notification templates, Robin Sloan highlights a curious encounter with Shopify's LLM-powered developer documentation bot. The issue centers on figuring out how to detect if an order includes items fulfilled through Shopify Collective, a task that led Sloan to seek advice from the doc bot after traditional search methods fell short.

The bot's initial suggestion seemed plausible, proposing a Liquid syntax solution that should have worked. However, real-world testing (which involved repeated order placements and refunds) revealed that the requisite "Shopify Collective" tag wasn't attached to the order until after the confirmation email was sent. This delay in tagging, a nuance not documented, rendered the bot's advice ineffective.

Sloan questions the reliability of AI-powered documentation that may resort to educated guesses rather than providing infallible insights, especially when official documentation stakes are high. Despite some past successes in quick queries, this incident underscores the critical need for precise and dependable guidance in tech environments.

Ultimately, Sloan found a workaround by adapting existing code, checking product-level tags available at the email's generation time, successfully identifying Shopify Collective orders. This tale not only warns of the pitfalls of over-relying on AI but also celebrates the ingenuity required to navigate around them when they fall short.

The discussion revolves around the challenges and limitations of using AI, particularly Retrieval-Augmented Generation (RAG) systems, for technical documentation like Shopify's LLM-powered bot. Key points include:  
1. **AI vs. Human Judgment**: While AI can quickly generate plausible answers, it often struggles with **nuance** and **accuracy** in complex technical contexts. Users note that AI may confidently provide incorrect or incomplete solutions (e.g., missing real-world timing issues like delayed order tagging), highlighting the need for human oversight.  
2. **RAG System Limitations**: Technical hurdles with RAG—such as **context window constraints**, degradation in accuracy with larger documents, and inefficiency in filtering relevant information—make it unreliable for intricate queries.  
3. **Cost and Scalability**: Some argue AI documentation tools are cost-effective and faster than human efforts, but skeptics warn hidden costs (e.g., error correction) and context-handling flaws undermine scalability.  
4. **Human-Curated Documentation**: Participants stress that structured, **human-written documentation** remains critical, as AI cannot yet match the reliability, contextual awareness, and adaptability of expert-driven content.  
5. **Workarounds and Adaptability**: The incident underscores the necessity of developer ingenuity (e.g., using product tags) to bypass AI shortcomings when official documentation fails.  

Overall, the consensus leans toward **cautious integration of AI**—valuing its speed but recognizing its fallibility—while advocating for hybrid approaches that prioritize human expertise in critical technical domains.

### Using MPC for Anonymous and Private DNA Analysis

#### [Submission URL](https://vishakh.blog/2025/07/08/using-mpc-for-anonymous-and-private-dna-analysis/) | 36 points | by [vishakh82](https://news.ycombinator.com/user?id=vishakh82) | [18 comments](https://news.ycombinator.com/item?id=44508866)

Monadic DNA embarked on a unique project earlier this year, aiming to demonstrate how individuals could access and interact with their genetic data while maintaining privacy through cutting-edge technology. At an event in Denver, thirty pioneering participants provided saliva samples, which were processed using Multi-Party Computation (MPC) technology developed by Nillion. This ensured participants could analyze their genotyping results without ever exposing sensitive raw data.

The sample collection took place during the ethDenver conference, drawing a lively crowd at Terminal Bar thanks to perfect weather and a bit of social media buzz. Though the turnout was higher than anticipated, the team managed the rush effectively. Participants signed forms, selected kit IDs and PINs, and submitted their samples, being rewarded with both a drink and an optional digital token, known as a POAP, marking their participation.

The samples were then handled by Autogen, a lab chosen for their ability to manage both timelines and the privacy needs of the project. Despite only needing basic metadata like kit IDs, many labs expressed a willingness to work with anonymized samples, underscoring a trend towards privacy-respectful genomic research.

The data processing used the Global Screening Array for genotyping, providing participants with insights from around 500,000 genetic markers. This choice struck a balance between cost and data richness, opting against full-genome sequencing due to its high costs and current market irrelevance.

Once processed, the anonymized data was shared securely via standard cloud storage solutions, enabling participants to claim and analyze their genetic information confidentially. This project not only underscored the potential of MPC technology in safeguarding genetic data but also laid the groundwork for more private consumer genomic products in the future. The participants' enthusiasm, even months after the event, highlighted a growing trust in secure, privacy-focused genomic technologies.

**Hacker News Discussion Summary:**  
The discussion on Monadic DNA’s privacy-focused genomic project highlighted a mix of technical curiosity, skepticism, and enthusiasm. Here are the key points:

1. **Terminology & Humor**  
   - Users joked about the overlap between “Multi-Party Computation (MPC)” and “Media Player Classic,” with playful confusion over abbreviations [[wckgt](https://news.ycombinator.com/user?id=wckgt)].  

2. **Technical Debates**  
   - **Encryption & Trust**: While [krnck](https://news.ycombinator.com/user?id=krnck) praised FHE (Fully Homomorphic Encryption) for securing results, others raised concerns about trusting external labs with raw data. [mbvtt](https://news.ycombinator.com/user?id=mbvtt) questioned whether encryption truly removes reliance on labs, noting markers’ interpretative dependence.  
   - **Molecular Cryptography**: Projects like cryptographic DNA molecules were suggested as future solutions [[Real_S](https://news.ycombinator.com/user?id=Real_S)], with [vishakh82](https://news.ycombinator.com/user?id=vishakh82) (likely a team member) acknowledging ongoing work but emphasizing current regulatory realities.  

3. **Philosophy & Scope**  
   - The term "monadic" sparked discussion, with [odyssey7](https://news.ycombinator.com/user?id=odyssey7) linking it to self-contained encrypted insights. [vishakh82](https://news.ycombinator.com/user?id=vishakh82) clarified the goal: personalized genetic insights via aggregated, consented data, avoiding centralized models.  

4. **Cost & Practicality**  
   - Critics like [gpypp](https://news.ycombinator.com/user?id=gpypp) queried legal/logistical risks of anonymization, while [vishakh82](https://news.ycombinator.com/user?id=vishakh82) explained challenges with "de-anonymized" metadata and budget constraints, noting their project’s experimental nature vs. production-scale feasibility.  

5. **Future Implications**  
   - [phrnxrly](https://news.ycombinator.com/user?id=phrnxrly) critiqued cloud storage (S3) reliance, prompting [vishakh82](https://news.ycombinator.com/user?id=vishakh82) to outline MPC/FHE for access control and ambitions to build a decentralized model akin to 23andMe, but centered on user consent.  

6. **Broader Context**  
   - Links to newborn screening practices [[vishakh82](https://news.ycombinator.com/user?id=vishakh82)] and academic papers on genomic data privacy [[Real_S](https://news.ycombinator.com/user?id=Real_S)] contextualized challenges like industrial trust and regulatory hurdles.  

**Conclusion**: The thread reflects excitement for cryptographic privacy in genomics, tempered by realism around costs, trust in labs, and regulatory complexity. The project’s team actively addressed concerns, positioning MPC/FHE as foundational tools for future ethical, user-centric genomic services.

### Springer Nature book on machine learning is full of made-up citations

#### [Submission URL](https://retractionwatch.com/2025/06/30/springer-nature-book-on-machine-learning-is-full-of-made-up-citations/) | 130 points | by [ArmageddonIt](https://news.ycombinator.com/user?id=ArmageddonIt) | [50 comments](https://news.ycombinator.com/item?id=44507061)

In an unexpected twist fit for a sci-fi drama, one of the latest machine learning resources might be taking some creative liberties with the truth—when it comes to citations, at least. The book "Mastering Machine Learning: From Basics to Advanced" by Govindakumar Madhavan is raising eyebrows—and not just for its $169 price tag. Published by Springer Nature, it turns out that many of the book's citations might be more fiction than fact.

Retraction Watch, tipped off by a concerned reader, dug into this mystery and discovered a murky world of missing or incorrect citations. An analysis of 18 out of 46 references revealed that an astonishing two-thirds weren't quite what they seemed. Some researchers even found themselves surprisingly cited for works they never wrote, with one paper cited being no more than an unpublished arXiv preprint inaccurately referred to as an IEEE publication.

This citation conundrum hints at the possible use of AI-style generation methods, reminiscent of those employed by large language models (LLMs) like ChatGPT. These models, while proficient in creating human-like text, can sometimes fall prey to fabricating references, creating fictitious citations that look realistic but don't hold up under scrutiny.

Madhavan hasn't fully offered clarification on whether AI played a role in crafting his book, but he acknowledged the growing difficulty in distinguishing between AI- and human-generated content. As the debate over the use of AI in academia continues, this case underscores the importance of rigorous verification, lest we end up with scholarly versions of "alternative facts." The mystery deepens, awaiting further comment from the author, who is no stranger to the tech world, leading SeaportAi and creating an array of educational resources. Stay tuned as this tale of academic intrigue unfolds!

The Hacker News discussion revolves around the implications of AI-generated content in academia, sparked by a book published by Springer Nature containing fabricated citations. Key points include:

1. **AI’s Role in Content Creation**:  
   Users debate the difficulty of distinguishing AI-generated text from human writing, especially as LLMs advance. Some suspect the book’s citations were AI-generated, highlighting issues like "confabulation" (mixing real and invented references) and overconfident but inaccurate outputs.

2. **Publisher Accountability**:  
   Springer is criticized for damaging its reputation by failing to verify content. Commenters note a trend of declining textbook quality, with publishers prioritizing profit (e.g., high prices for poorly reviewed books) over rigorous peer review. References to past publishing errors (e.g., typos, incorrect images) suggest systemic issues.

3. **Verification Challenges**:  
   - Existing tools like DOI links and AI detectors are deemed insufficient, as they can’t always validate context or prevent circular dependencies (e.g., GPT-4 generating valid-looking but fake citations).  
   - Suggestions include manual checks, cross-referencing summaries with source material, and better institutional incentives for thorough peer review.

4. **Broader Academic Concerns**:  
   - Fear that AI could exacerbate problems like paper mills, fraudulent research, and "citation stuffing" to game academic metrics.  
   - Jokes about a future where AI reviews AI-written content, creating a self-referential loop of unverified information.  
   - Nostalgia for traditional, human-curated resources and lament over the erosion of trust in educational materials.

5. **Cultural Shifts**:  
   Mention of "Sturgeon's Law" (90% of content is "crap") underscores worries that AI might flood academia with low-quality work. Commenters stress the need for vigilance, better tools, and a return to quality-focused publishing practices to preserve scholarly integrity.  

In summary, the discussion reflects skepticism about AI's unchecked use in academia, frustration with profit-driven publishing, and calls for more robust validation mechanisms to combat misinformation.

---

## AI Submissions for Tue Jul 08 2025 {{ 'date': '2025-07-08T17:14:18.977Z' }}

### Smollm3: Smol, multilingual, long-context reasoner LLM

#### [Submission URL](https://huggingface.co/blog/smollm3) | 350 points | by [kashifr](https://news.ycombinator.com/user?id=kashifr) | [70 comments](https://news.ycombinator.com/item?id=44501413)

Exciting news from the world of language models! Meet SmolLM3, a cutting-edge multilingual, small-scale language model with big ambitions. Developed collaboratively by a team of experts, SmolLM3 is designed with efficiency and long-context reasoning in mind, aiming to outperform its peers like Llama-3.2-3B and Qwen2.5-3B, while being a worthy competitor to the larger 4B models such as Qwen3 & Gemma3.

Boasting a 3 billion parameter design, SmolLM3 is built to support six major languages, including English, French, and Spanish, making it an attractive option for global applications. Capable of handling long contexts up to 128,000 tokens, SmolLM3 promises breakthrough performance with its novel attention mechanisms like Grouped Query Attention (GQA) and the innovative NoPE hybrid attention strategy.

The creators are not just sharing a final product; they're offering an open-source blueprint of how they constructed this marvel from the ground up. This transparency allows enthusiasts and developers to understand the intricacies behind achieving such performance at a smaller scale. The model is trained on a whopping 11 trillion tokens with a three-stage approach focusing on datasets from diverse domains such as web, math, and code.

SmolLM3 uses advanced techniques like intra-document masking and improved stability strategies akin to its predecessor, SmolLM2, while introducing tweaks for superior stability and performance during training. The model's robustness was ensured through numerous validations using massive computing power—an awe-inspiring setup involving 384 H100 GPUs over 24 days.

For those curious about the finer points of SmolLM3, the project offers a goldmine of engineering insights and methodologies, making it a remarkable reference for anyone looking to elevate their understanding or build upon this foundation. Whether you're interested in language models' architecture or aiming to push the boundaries of machine learning capabilities, SmolLM3 paints an inspiring picture of what skilled and thoughtful engineering can achieve in the AI landscape.

**Hacker News Discussion Summary: SmolLM3 Release**

**Cost & Training Resources:**  
The discussion highlights the significant computational cost of training SmolLM3, initially cited as using 384 H100 GPUs over 24 days. Users debated the exact cost, with estimates ranging from $28k to over $500k, depending on GPU rental pricing (e.g., $2–$3/hour on cloud platforms like Runpod). Corrections clarified the math, emphasizing the high barrier to entry for reproducing such training without corporate-scale resources.

**Open-Source Debate:**  
Participants questioned whether SmolLM3 is "truly open-source," comparing it to models like OLMo, which provide full training code, datasets, and weights. Some users expressed skepticism, noting that many "open" models omit critical details like training data or infrastructure. The SmolLM3 team clarified they are releasing a full engineering blueprint, including architecture and dataset mixes, to aid reproducibility.

**Technical Challenges & Local Deployment:**  
Users shared mixed success running SmolLM3 on local hardware (e.g., Macs). Issues with inference engines like `llama.cpp` and Ollama were noted, though workarounds using MLX-LM or Transformers libraries were suggested. Quantization (e.g., 4-bit GGUF) was discussed to reduce VRAM usage, with some achieving 128k-token contexts on 24GB GPUs like the RTX 4090. The Mac community faced hurdles but found partial success with PyTorch and Metal GPU acceleration.

**Use Cases & Small Model Potential:**  
The conversation pivoted to practical applications for 3B-scale models, such as edge devices (Jetson, mobile) and RAG systems. Participants debated whether small models can compete with larger ones in reasoning tasks, emphasizing domain-specific fine-tuning and hybrid approaches (e.g., combining vector search and keyword retrieval). Some shared success stories with Mistral 7B for specialized tasks, while others stressed the need for rigorous benchmarking.

**Community Reception:**  
The release of SmolLM3’s detailed methodology was praised as a valuable resource for engineers and researchers. However, skepticism lingered about its benchmark claims and真正的 "state-of-the-art" status, with calls for independent validation. Developers expressed enthusiasm for testing the model, particularly its multilingual and long-context capabilities, despite deployment challenges.

**Key Takeaways:**  
- SmolLM3’s engineering transparency is a standout feature, though its open-source credentials face scrutiny.  
- Costs and infrastructure requirements limit reproducibility for individuals.  
- Local deployment remains tricky but feasible with community-driven tools.  
- Small models like SmolLM3 show promise for niche applications but require careful optimization and benchmarking.

### The Tradeoffs of SSMs and Transformers

#### [Submission URL](https://goombalab.github.io/blog/2025/tradeoffs/) | 64 points | by [jxmorris12](https://news.ycombinator.com/user?id=jxmorris12) | [8 comments](https://news.ycombinator.com/item?id=44503056)

In the world of machine learning, a fascinating discussion is taking place between enthusiasts and experts alike about State Space Models (SSMs) and Transformers. A blog post has been adapted from a popular talk, aimed at making this complex subject accessible to everyone, from casual readers to dedicated researchers. The crux of the conversation lies in understanding how SSMs are evolving as a strong contender in sequence modeling, traditionally dominated by Transformers, particularly in language processing.

State Space Models have come a long way, derived from a lineage of work that culminated in the development of models like Mamba. At their core, SSMs can be conceptualized as modern successors to recurrent neural networks (RNNs), with distinct advantages that help them rival the performance of Transformers.

Three essential ingredients characterize the success of SSMs:

1. **State Size**: SSMs feature a hidden state with a larger size than the inputs and outputs, enabling the model to store more context-rich information—a crucial trait for handling complex modalities like language.

2. **State Expressivity**: The recursive update functions in SSMs are expressive enough to store and selectively access needed information, akin to the gating mechanisms in classic RNNs like LSTMs and GRUs. This flexibility allows the model to handle sequences with varying information rates, a key requirement for language modeling.

3. **Training Efficiency**: While having a larger recurrent state boosts performance, it also increases computational complexity. Innovations like parallel scan algorithms have been employed to enhance the feasibility of training SSMs on modern hardware, balancing memory usage and computational workload.

The blog highlights that these strategies, though not entirely new, when combined effectively, bring SSMs to the forefront, demonstrating near-equivalence to Transformers in language modeling tasks.

The landscape of modern machine learning is rapidly shifting as researchers continuously seek to improve recurrent models. SSMs and other models like RWKV and Griffin are explored further, depicting diverse approaches in state expressivity and parallel training efficiency. The post delves into the nuances of linearity, selectivity, and the theoretical underpinnings of these models, underscoring a vibrant research area ripe with potential.

In sum, while Transformers have been the rockstars of sequence modeling, the advancements in SSMs suggest that the spotlight may start to share its focus, prompting an exciting era of innovation and rediscovery in the field.

The discussion around State Space Models (SSMs) versus Transformers reflects a mix of skepticism, optimism, and technical debate:  

### Key Themes:
1. **Tokenization Debate**:  
   - Some users argue that replacing tokenization schemes like BPE with raw bytes could simplify representations and better align with linguistic fundamentals. For example, one user claims raw bytes (as in Chinese characters or English letters) might offer a more basic, language-agnostic alternative to BPE.  
   - Counterarguments suggest Transformers *require* preprocessing to compress dense information efficiently, particularly for video/audio tasks. Current architectures still depend heavily on tokenization despite its limitations.  

2. **SSMs vs. Transformers**:  
   - Skeptics (**mbowcut2**) question whether SSMs justify significant R&D investment compared to optimized Transformer-based LLMs. They argue that established methods (like Transformers) dominate benchmarks, making SSMs a risky bet without clear evidence of outperformance.  
   - Proponents (**vsrg**, **nxts**) highlight SSMs’ potential differentiation (e.g., efficiency gains, novel architectures like xLSTM) and niche applications (time-series forecasting). Some cite models like xLSTM as proof that alternative architectures can rival Transformers in specific domains.  

3. **Practical Challenges**:  
   - Training costs and scalability remain barriers. While SSMs might theoretically reduce bottlenecks like "information density," users note current SSMs lack competitive benchmarks and struggle to match Transformer-scale datasets.  
   - Hybrid approaches (**Herring**) and incremental innovations (e.g., DeepSeek’s models) are seen as safer bets than full SSM overhauls.  

4. **Broader Research Landscape**:  
   - Comments hint at parallel efforts (LiquidAI, Griffin) exploring lightweight architectures or alternatives that blend SSM concepts with Transformers. However, the dominance of Transformers in industry R&D (e.g., Llama, Gemma) makes radical shifts unlikely in the short term.  

### Conclusion:  
The discussion underscores cautious interest in SSMs as a complement or niche alternative to Transformers, but few see them as an imminent replacement. Technical challenges, entrenched infrastructure for Transformers, and high costs of experimentation temper enthusiasm, even as theoretical advantages (efficiency, differentiation) keep SSMs on the radar.

### Google can now read your WhatsApp messages

#### [Submission URL](https://www.neowin.net/guides/google-can-now-read-your-whatsapp-messages-heres-how-to-stop-it/) | 448 points | by [bundie](https://news.ycombinator.com/user?id=bundie) | [309 comments](https://news.ycombinator.com/item?id=44501379)

This week, Google stirred the pot in the Android community with an unexpected announcement regarding its AI-powered Gemini service. Starting July 7, Gemini is now integrated with popular apps like Phone, Messages, and WhatsApp, allowing users to command tasks like sending messages without needing to toggle Gemini Apps Activity. However, this convenience comes with a catch. While Google reassures users that Gemini won’t read or summarize WhatsApp messages under normal circumstances, integration with Google Assistant or Utility apps could enable access to your messages and notifications. 

Naturally, privacy-concerned users were quick to act, with many opting to disable Gemini’s connected apps to safeguard their data. Despite turning off Gemini Apps Activity, Google maintains data for a brief 72 hours to "ensure safety and security.” Those hoping to completely extricate Gemini from their devices face a more complex endeavor, as Google representatives artfully dodged direct inquiries about permanent removal. However, an arduous path exists via ADB (Android Debug Bridge) to uninstall Gemini, albeit with mixed results due to its ties with the main Google app. 

Tech enthusiasts looking to eliminate Gemini altogether are advised to roll back all updates and disable the Google app entirely, a move that effectively removes the AI agent but also disables Google’s broader functionalities.

This development has led to broader discussions about privacy, with questions circling the necessity and implications of AI’s growing integration into daily tech interactions. Amidst these concerns, users are reminded of the broader trade-offs involved when weighing functionality against privacy. Meanwhile, platforms like Neowin encourage community engagement and support through various means, including Amazon shopping links and virtual coffee contributions. Stay tuned to the ever-evolving landscape of tech privacy and AI integration—your voice, and vigilance, matter.

The discussion revolves around Google's integration of Gemini as an OS-level feature on Android, raising significant privacy and antitrust concerns. Key points include:

1. **Privacy Concerns**: Users worry Gemini could access sensitive data (e.g., WhatsApp messages) via integrations like Google Assistant, despite Google’s assurances. Disabling Gemini is complicated, requiring ADB or disabling the Google app entirely, which breaks core functionalities.

2. **Antitrust Parallels**: Comparisons are drawn to Microsoft’s Internet Explorer case and Apple’s ecosystem control. Critics argue Google’s OS-level integration stifles competition, echoing historical antitrust issues. The EU’s Digital Markets Act (DMA) is cited as a regulatory counterforce.

3. **OS Alternatives**: Some advocate for alternatives like GrapheneOS or LineageOS to escape Google’s control, though practical hurdles (e.g., banking app compatibility) persist. Others mention decentralized projects like ApostrophyOS or Purism’s Librem 5.

4. **Apple Comparisons**: Debates arise over Apple’s Siri and privacy reputation, with skepticism about both companies’ motives. While Apple is seen as more privacy-focused, critics note its ecosystem’s closed nature mirrors Google’s control.

5. **Broader Skepticism**: Users express distrust in tech giants, emphasizing data monetization and AI overreach. Concerns about centralized control of personal information and AI’s role in daily tech interactions dominate.

The discussion highlights tensions between innovation/convenience and privacy/control, reflecting broader debates about corporate power and regulatory adequacy in the AI era.