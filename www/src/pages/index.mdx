import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Sun Nov 23 2025 {{ 'date': '2025-11-23T04:30:02.051Z' }}

### An Economy of AI Agents

#### [Submission URL](https://arxiv.org/abs/2509.01063) | 100 points | by [nerder92](https://news.ycombinator.com/user?id=nerder92) | [64 comments](https://news.ycombinator.com/item?id=46020096)

An Economy of AI Agents (arXiv). Survey/primer by Gillian K. Hadfield and Andrew Koh on what happens when long-horizon, largely autonomous AI agents start transacting across the economy.

Why it matters
- Treats AI agents as economic actors that plan, bargain, buy/sell, and coordinate—with minimal human oversight—forcing a rethink of market design, firm boundaries, and regulation.

What the authors survey and the open questions they raise
- Markets: How agent-agent and human-agent interactions affect price discovery, liquidity, and competition; risks of tacit algorithmic collusion and rapid cascades.
- Organizations: How firms restructure when “swarms” of agents do procurement, logistics, R&D, and negotiations; new principal–agent problems and monitoring.
- Institutions: What’s needed for well-functioning markets—identity and licensing for agents, liability and insurance, auditability and logging, reputation/credit systems, standards and protocols, and dispute resolution.
- Policy and design: Mechanism design for mixed human/agent markets, antitrust in an algorithmic world, data and privacy constraints, security and authentication, and measurement of productivity and labor impacts.

Bottom line
- It’s a map of the economics research and policy agenda for an agentic future: don’t just build smarter agents—build the market rules and infrastructure that make their interactions safe and efficient.

**Discussion Summary**

The discussion focuses heavily on the practical and theoretical frictions of integrating autonomous agents into existing economic and technical structures:

*   **Determinism vs. Probabilistic Autonomy:** A lengthy technical debate dominates the thread regarding whether AI agents can replace traditional workflow engines. User `zkmn` argues that businesses depend on deterministic, rule-based logic for accountability and requirements, asserting that probabilistic, "on-the-fly" decision-making is too risky for core business processes. In contrast, `mewp2` argues that rigid rules fail against unstructured real-world data (like customer service queries), suggesting agents are superior for categorizing inputs, routing tasks, and even generating runtime code to solve specific problems (e.g., scraping travel data) that hardcoded logic cannot handle.
*   **Sci-Fi Parallels:** Commenters drew immediate comparisons to science fiction, specifically Charles Stross’s *Accelerando*—which depicts an economics of AI entities operating at speeds/complexities incomprehensible to humans—and Robin Hanson’s *The Age of Em*.
*   **Organizational Structure:** The conversation touched on the revival of Decentralized Autonomous Organizations (DAOs). While some users dismissed DAOs as merely "open source co-ops" or failed blockchain experiments, others argued that the introduction of truly agentic AI makes the DAO concept viable in ways previous iterations were not.
*   **Labor Implications:** A cynical undercurrent noted that these efficiencies likely signal a future where companies simply stop hiring people, shifting the economy toward capital efficiency over labor participation.

### Claude Status – Elevated error rates on the API

#### [Submission URL](https://status.claude.com/incidents/538r2y9cjmhk) | 53 points | by [throwpoaster](https://news.ycombinator.com/user?id=throwpoaster) | [63 comments](https://news.ycombinator.com/item?id=46023364)

Anthropic resolves brief Claude API hiccup

- What happened: Anthropic reported elevated failure rates on the Claude API (api.anthropic.com) on Nov 23, 2025.
- Timeline (UTC):
  - 13:05 Investigating reports of higher-than-normal failures
  - 13:06 Continued investigation
  - 13:53 Fix implemented; monitoring
  - 14:11 Incident marked resolved
- Duration: Roughly 1 hour from first report to resolution.
- Scope: Claude API only; no root cause disclosed yet.
- Dev takeaway: If you saw spikes in errors around that window, they should be cleared now. Worth checking logs and ensuring robust retry/backoff logic. You can subscribe to Anthropic’s status updates for future incidents.

**Dependency vs. Skill:** The outage sparked a heated "old guard vs. new guard" debate. Some users scoffed at "VB coders" and "wrapper wrappers" who were paralyzed by the downtime, implying that "real" programmers shouldn't be reliant on AI. Others pushed back, arguing that for greenfield projects and boilerplate (like writing RBAC systems or stitching together client-mandated libraries), maintaining high velocity without AI is inefficient.

**Infrastructure & Cause:** Speculation led many to believe the release of "Claude Code" caused the instability. This segued into a technical discussion about the fragility of LLM infrastructure compared to standard web services, noting that the challenges of loading massive models (MoE/expert loading) and managing hardware dependencies (GPUs/TPUs) make "five nines" reliability much harder to achieve.

**Identity Crisis:** A philosophical sub-thread explored why developers feel so threatened by these tools. Commenters discussed how AI challenges the "mystique" and high-status nature of the profession, suggesting that the gatekeeping seen in the comments stems from a fear of coding becoming a commodity.

**Tooling & Fallbacks:** Users discussed redundancy strategies, such as routing traffic to Gemini, OpenAI, or ZAI when Anthropic wobbles. There was also a specific critique of the new "Claude Code" tool, with one user noting that while good for scaffolding, the standard Sonnet web interface was significantly better at deep refactoring and code reduction.

**Humor:** Amidst the frustration, jokes emerged about human-written software becoming a premium artisanal product, dubbed "Meat Code."

### Meet the AI workers who tell their friends and family to stay away from AI

#### [Submission URL](https://www.theguardian.com/technology/2025/nov/22/ai-workers-tell-family-stay-away) | 48 points | by [breve](https://news.ycombinator.com/user?id=breve) | [14 comments](https://news.ycombinator.com/item?id=46027290)

Meet the AI workers who tell their friends and family to stay away from AI (The Guardian): The piece profiles the contract raters and Mechanical Turk annotators who help make chatbots seem reliable—many of whom now avoid using generative AI themselves and warn loved ones off it. One rater nearly misclassified a racist slur, sparking fears about unseen errors at scale; a Google contractor says she was asked to judge AI Overviews’ health answers without medical training and has banned her 10-year-old from chatbots. Workers describe pressure for speed over rigor and say their feedback often feels ignored—an incentive mismatch experts say favors shipping over validation, a bad sign as more people rely on LLMs for information. Amazon says MTurk workers choose tasks and requesters set terms; Google says ratings are just one signal and don’t directly shape models, and that it has safeguards. The broader takeaway: the people paid to make AI safer increasingly don’t trust the outputs—or the companies racing them to market.

**Selection Bias & Methodology**
A significant portion of the discussion focused on the article's methodology. Commenters argued that interviewing a half-dozen dissatisfied workers out of an estimated global workforce of millions lacks statistical rigor. Critics felt this approach was "cherry-picking" to reinforce pre-existing biases against AI, comparing it to finding physicists who believe the earth is flat or doctors opposed to vaccines—possible, but not representative.

**Evaluating "Moon Cricket"**
The article’s mention of a rater nearly misclassifying the slur "moon cricket" sparked a sub-thread on the term’s obscurity. Several users, including those living in the American South near active hate groups, noted they had never heard the term firsthand. Others identified it as extremely dated slang (circa 1930s), suggesting that failing to flag such an archaic term might be an understandable lapse rather than a sign of systemic failure.

**Client Hallucinations & Medical Advice**
The conversation shifted to personal anecdotes regarding AI reliability.
*   **Consulting Headaches:** One user described a client who believed a ChatGPT hallucination about a "special Amazon business account" over their consultant's advice; another noted that while fixing these misconceptions creates billable hours, it feels wasteful.
*   **Medical Risks:** Users expressed concern over people using LLMs for medical diagnosis, noting discrepancies between AI answers and cited sources. However, a counterpoint was raised that Internet users have long scared themselves with medical misinformation via sites like WebMD ("It's always cancer").

**The "Drug Dealer" Analogy**
Echoing the sentiment that tech workers ban their own families from using their products, the top comment invoked the "drug dealer doesn't consume their own product" trope, noting that social media elites often restrict their children’s access to technology in favor of older, disconnected hardware (e.g., ThinkPads running Debian).

### 'Is AI creating a new code review bottleneck for senior engineers?'

#### [Submission URL](https://thenewstack.io/is-ai-creating-a-new-code-review-bottleneck-for-senior-engineers/) | 16 points | by [MilnerRoute](https://news.ycombinator.com/user?id=MilnerRoute) | [3 comments](https://news.ycombinator.com/item?id=46024884)

The New Stack’s “subscribe” wall is doing a lot more than asking for an email. Visitors are met with a full-page sign-up flow that blocks content and requires numerous fields: first/last name, company, country, ZIP, job level and role, org size, organization type, and industry. If you previously unsubscribed, it forces a separate re-subscribe step before you can proceed. While TNS says it doesn’t sell data and requires agreement to its Terms and Privacy Policy, the experience reads more like a B2B lead-gen form than a simple newsletter opt-in. It’s a striking example of the growing shift from paywalls to data walls—trading access for detailed personal and professional info, with clear UX friction for readers.

User **vrdvrm** describes a frustrating experience encountering "garbage" code in a Pull Request; after reviewing the comments and commits, they realized the account involved had dozens of rejected PRs across various projects. **Gphph** suggests handling such situations with a "polished professional response" copy-pasted directly, while **vrdvrm** adds that the associated project appears suspicious.

### Show HN: Dank-AI – Ship production AI agents 10x faster

#### [Submission URL](https://www.dank-ai.xyz/) | 6 points | by [deltadarkly](https://news.ycombinator.com/user?id=deltadarkly) | [5 comments](https://news.ycombinator.com/item?id=46021135)

What it is
- An open-source (MIT) toolkit to build, containerize, and run multiple AI agents as microservices using plain JavaScript/Node—no Python stack required.
- CLI-driven workflow: npm install -g dank-ai → dank init → dank run. It auto-builds optimized Docker images, can push to registries, and offers real-time monitoring out of the box.

How it works
- Define agents in JavaScript (not YAML): pick an LLM (OpenAI, Anthropic, Cohere, Ollama, or custom), set CPU/memory/timeouts, write prompts, and attach event handlers (request/response hooks, etc.).
- Multi-agent orchestration: each agent runs as a containerized service. Designed for CI/CD—dank build and dank build --push integrate with GitHub Actions/GitLab CI.
- Infra-agnostic deployment: run on Kubernetes, Docker Swarm, AWS ECS, Azure ACI, or your own servers. Each agent can get a dedicated hostname with TLS.
- Management features: per-agent resource quotas, dynamic scaling, live dashboards, historical metrics, endpoint routing and rate limiting, API key management, RBAC, secrets and env vars.

Why it matters
- Targets the huge JS/Node ecosystem with a production-friendly, Docker-native path to ship “agents as microservices,” reducing glue code between LLM logic and ops.
- Compared to many Python-first agent frameworks, Dank centers on containerization and deployment from day one, which may simplify moving from prototype to prod.

Notable commands
- Install: npm install -g dank-ai
- Init: dank init my-agent-project
- Run: dank run
- Build/push: dank build and dank build --push registry.com/my-agent:v1.0

Caveats and questions
- Docker is a hard dependency; cold starts and container overhead still apply.
- GPU/accelerator scheduling isn’t documented; unclear for heavy or on-prem LLMs.
- Monitoring/alerting depth and scaling policies may need a closer look for larger fleets.

Bottom line: If you want to ship JS-based AI agents like any other microservice—with Docker images, registries, TLS, RBAC, and infra portability—Dank aims to be the batteries-included path.

**Discussion Summary**

The conversation focused on the architectural viability of containerized agent workflows versus current single-runtime frameworks. While one user raised concerns about coordination conflicts in event-driven swarm architectures, the maintainer argued that the current pattern of running agents locally doesn't scale, positioning Dank’s microservice approach as a necessary evolution for connecting agents to "real work" (with a dedicated "Workflows" feature coming soon).

Other comments highlighted the practical appeal of the tool:
*   **Market Fit:** Users noted the utility of targeting the massive JavaScript developer pool (referencing a "98%" statistic).
*   **UX:** One tester praised the quick setup, comparing the CLI experience favorably to Vercel.
*   **Skepticism:** A detractor dismissed the project as simply another "wrapper for LLM wrappers."

---

## AI Submissions for Sat Nov 22 2025 {{ 'date': '2025-11-22T01:58:38.160Z' }}

### Show HN: I built a wizard to turn ideas into AI coding agent-ready specs

#### [Submission URL](https://vibescaffold.dev/) | 24 points | by [straydusk](https://news.ycombinator.com/user?id=straydusk) | [12 comments](https://news.ycombinator.com/item?id=46018229)

AI-Powered Spec Generator is a landing-page pitch for a tool that turns rough product ideas into full technical specs via a single structured chat. It promises to replace back-and-forth prompt tinkering with a guided flow that produces production-ready documentation and plans.

What it generates:
- ONE_PAGER.md: product definition, MVP scope, user stories
- DEV_SPEC.md: schemas, API routes, security protocols, architecture diagrams
- PROMPT_PLAN.md: stepwise, LLM-testable prompt chains
- AGENTS.md: system prompts/directives for autonomous coding agents

Who it’s for: founders, PMs, and tech leads who want faster idea-to-MVP handoff, plus teams experimenting with agent-based development.

Why it matters: centralizes product, architecture, and prompt-engineering into a consistent spec bundle, aiming to cut planning time and reduce ambiguity between stakeholders and AI agents.

Caveats to keep in mind: like any LLM-driven planning tool, outputs will need human review for feasibility, security, and scope creep; “architecture diagrams” and protocols are only as solid as the inputs and model.

**Discussion Summary:**

The discussion focuses on the tool's user experience, specific output bugs, and its role in the "AI coding agent" ecosystem.

*   **UX and Copy Critique:** One user (`nvdr`) praised the "slick" styling but encountered a bug where the tool returned placeholder text ("Turn messy ideas...") instead of a generated spec. This sparked a debate about the homepage copy—users suggested calling ideas "messy" has negative connotations, though the creator (`strydsk`) noted it was intended to highlight the tool's clarity.
*   **Technical Glitches:** The creator attributed some of the erratic behavior (specifically "jumping steps" in the wizard) to a regression or API-level usage limits while using `gpt-4o-mini`.
*   **Role in Workflow:** Users sought clarity on how this links to actual coding. The consensus—confirmed by the creator—is that this tool creates the *roadmap* (specs and plans) which are then fed into separate autonomous coding agents, rather than the tool being an agent that indexes codebases itself.
*   **Feature Requests:** There was a strong suggestion for a distinct "Plan Mode" to help users evaluate the strategy before generation; the creator agreed this was a key differentiator and provided a sample "Prompt Plan" output in response.

### New Apple Study Shows LLMs Can Tell What You're Doing from Audio and Motion Data

#### [Submission URL](https://9to5mac.com/2025/11/21/apple-research-llm-study-audio-motion-activity/) | 68 points | by [andrewrn](https://news.ycombinator.com/user?id=andrewrn) | [29 comments](https://news.ycombinator.com/item?id=46015578)

Apple says LLMs can infer your activity from sensor summaries—without training on your data

- What’s new: An Apple research paper explores “late multimodal fusion” using LLMs to classify activities from short text summaries of audio and IMU motion data (accelerometer/gyroscope), not the raw signals.
- How it works: Smaller models first convert audio and motion streams into captions and per-modality predictions. An LLM (tested: Gemini 2.5 Pro and Qwen-32B) then fuses those textual hints to decide what you’re doing.
- Data: Curated 20-second clips from the Ego4D dataset across 12 everyday activities (e.g., cooking, vacuuming, laundry, weights, reading, watching TV, using a computer, sports, pets, dishes, eating).
- Results: Zero- and one-shot classification F1 scores were “significantly above chance” with no task-specific training; one-shot examples improved accuracy. Tested in both closed-set (12 known options) and open-ended settings.
- Why it matters: LLM-based late fusion can boost activity recognition when aligned multimodal training data is scarce, avoiding bespoke multimodal models and extra memory/compute for each app.
- Privacy angle: The LLM sees only short text descriptions, not raw audio or continuous motion traces—potentially less sensitive and lighter-weight to process.
- Reproducibility: Apple published supplemental materials (segment IDs, timestamps, prompts, and one-shot examples) to help others replicate the study.
- Big picture: Expect more on-device orchestration where compact sensor models summarize streams and a general-purpose LLM does the reasoning—useful for health, fitness, and context-aware features without deep per-task retraining.

Here is a summary of the discussion on Hacker News:

**Privacy and Surveillance Concerns**
A significant portion of the discussion focused on the privacy implications of "always-on" sensing. Users drew parallels to *1984* and "telescreens," with some arguing that modern smartphones already surpass those dystopian surveillance tools in capability. Commenters expressed concern that even if data is encrypted or anonymized now, companies may hoard it to decrypt later when technology advances (e.g., quantum computing). Others noted that this granular activity tracking poses specific dangers to high-risk individuals like activists and journalists, regardless of how benign the consumer feature appears.

**Apple Watch Performance & UX**
The conversation shifted to current Apple Watch capabilities, with users debating the reliability of existing activity detection. Some complained that newer models feel slower to detect workouts (like running) compared to older generations or competitors. Others defended this as a design choice, suggesting the system now requires a larger data window to ensure "confidence" and prevent false positives, though they noted Apple communicates this mechanism poorly to users.

**Technical Implementation and Ethics**
Technically-minded commenters clarified the papers distinction: the LLM does not process raw data but relies on smaller, intermediate models to generate text captions first. Some questioned the efficiency of this, suggesting standard analytics might be sufficient without adding an LLM layer. While some acknowledged the positive potential—such as distinguishing a senior citizen falling from a parent playing with their kids—others argued that beneficial use cases (like nuclear power) do not automatically justify the existence of the underlying dangerous capabilities (like nuclear weapons).

### Show HN: PolyGPT – ChatGPT, Claude, Gemini, Perplexity responses side-by-side

#### [Submission URL](https://polygpt.app) | 17 points | by [ncvgl](https://news.ycombinator.com/user?id=ncvgl) | [12 comments](https://news.ycombinator.com/item?id=46013984)

A new open‑source desktop app promises to end tab‑hopping between AI chats by letting you type once and query multiple models—like ChatGPT, Gemini, and Claude—simultaneously. It mirrors your prompt to all connected model interfaces and shows responses side‑by‑side in real time, making it handy for prompt crafting, QA, and quick model comparisons.

Highlights:
- Cross‑platform downloads: Mac, Windows, Linux; code available on GitHub
- Supports “4+” models (including ChatGPT, Gemini, Claude)
- One prompt, mirrored to all interfaces; live, side‑by‑side outputs
- Free, open source, and positioned as privacy‑focused

Good fit for teams and tinkerers who routinely compare models or iterate on prompts. Practical caveats remain (provider logins/API keys, rate limits, usage costs, and provider ToS), but the friction reduction and real‑time comparison view are the draw.

The discussion focuses on the delivery method, technical implementation, and potential features for evaluating the AI outputs:

*   **Web vs. Native:** Several users requested a web-based version, expressing strong reluctance to install native applications (specifically Electron wrappers) from unknown sources. They cited security concerns and a preference for the control and accessibility tools available in their own customized browsers.
*   **Alternatives:** One commenter pointed out that **Open WebUI** already has this functionality built‑in.
*   **Implementation details:** There was a brief debate on the underlying mechanics, specifically comparing the use of API keys versus embedding web apps, and how those choices affect context handling.
*   **"AI Judge" Feature:** A significant portion of the thread explored adding a feature where a "Judge" model compares the parallel responses to determine the best output. Ideas included using a "blind jury" of models or a democratic voting system among the agents, though one user noted past experiments where agent democracy led to AI models "conspiring" against the rules.

### Google tells employees it must double capacity every 6 months to meet AI demand

#### [Submission URL](https://arstechnica.com/ai/2025/11/google-tells-employees-it-must-double-capacity-every-6-months-to-meet-ai-demand/) | 46 points | by [cheshire_cat](https://news.ycombinator.com/user?id=cheshire_cat) | [28 comments](https://news.ycombinator.com/item?id=46013463)

Google says it must double AI serving capacity every six months—targeting a 1000x scale-up in 4–5 years—while holding cost and, increasingly, power flat. In an internal all-hands seen by CNBC, Google Cloud VP Amin Vahdat framed the crunch as an infrastructure race that can’t be won by spending alone: the company needs more reliable, performant, and scalable systems amid GPU shortages and soaring demand.

Key points:
- Bottlenecks: Nvidia’s AI chips are “sold out,” with data center revenue up $10B in a quarter. Compute scarcity has throttled product rollouts—Sundar Pichai said Google couldn’t expand access to its Veo video model due to constraints.
- Google’s plan: build more physical data centers, push model efficiency, and lean on custom silicon. Its new TPU v7 “Ironwood” is claimed to be nearly 30x more power efficient than Google’s first Cloud TPU (2018).
- Competitive backdrop: OpenAI is pursuing a massive US buildout (reported six data centers, ~$400B over three years) to reach ~7 GW, serving 800M weekly ChatGPT users who still hit usage caps.
- The bet: Despite widespread “AI bubble” chatter (Pichai acknowledges it), Google views underinvesting as riskier than overcapacity. Pichai warned 2026 will be “intense” as AI and cloud demand collide.

Why it matters: A six‑month doubling cadence implies a Moore’s Law–style race, but with power and cost ceilings that force co-design across chips, models, and data centers. If demand holds, winners will be those who align compute, energy, and reliability; if it doesn’t, capex-heavy bets could sting.

Here is the summary of the discussion:

**Demand: Organic vs. Manufactured**
A major point of contention was the source of the "demand" driving this infrastructure buildup. While some users pointed to ChatGPT’s high global ranking (5th most popular website) as evidence of genuine consumer interest, others argued Google’s specific demand metrics are inflated. Skeptics noted that "shimming" AI into existing products—like Gmail, Docs, and Search—creates massive internal query volume without necessarily reflecting user intent or willingness to pay. Several commenters likened these forced features to a modern "Clippy," expressing annoyance at poor-quality AI summaries in Search.

**Feasibility and Physics**
Commenters expressed deep skepticism regarding the technical feasibility of Google’s roadmap. Users argued that doubling capacity every six months is "simply infeasible" given that semiconductor density and power efficiency gains are slowing (the end of Moore’s Law), not accelerating. Critics noted that optimizations like custom silicon and co-design can't fully overcome the physical constraints of raw materials, construction timelines, and energy availability needed to sustain 1000x growth in five years.

**The "Bubble" and Post-Crash Assets**
The discussion frequently drifted toward the "AI bubble" narrative. Users speculated on the consequences of a market correction, comparing it to the housing crash.
*   **Hardware Fallout:** Many hoped a crash would result in cheap hardware for consumers, specifically discounted GPUs for gamers, inexpensive RAM, and rock-bottom inference costs that could make "AI wrapper" business models viable.
*   **Infrastructure:** There was debate over what happens to specialized data centers if the tenants fail; while some suggested conversion to logistics centers (e.g., Amazon warehouses), others noted that the electrical and HVAC infrastructure in AI data centers is too over-engineered to be cost-effective for standard storage.

**Comparison to Past Tech Shifts**
Users debated whether AI is a frantic infrastructure race or a true paradigm shift. Some questioned if AI has reached "mass consumer" status comparable to the PC or smartphone, citing older generations who still don't use it. Conversely, others argued that student adoption of LLMs indicates a permanent shift in how the future workforce will operate, justifying the massive investment.

### Google begins showing ads in AI Mode (AI answers)

#### [Submission URL](https://www.bleepingcomputer.com/news/artificial-intelligence/google-begins-showing-ads-in-ai-mode-ai-answers/) | 19 points | by [nreece](https://news.ycombinator.com/user?id=nreece) | [7 comments](https://news.ycombinator.com/item?id=46012525)

- What’s new: Google is rolling out “Sponsored” ads directly within AI Mode (its answer-engine experience). Until now, AI answers were ad-free.
- How it appears: Ads are labeled “Sponsored” and currently show at the bottom of the AI-generated answer. Source citations mostly remain in a right-hand sidebar.
- Who sees it: AI Mode is free for everyone; Google One subscribers can switch between advanced models like Gemini 3 Pro, which can generate interactive UIs for queries.
- Why it matters: This is a clear monetization step for Google’s AI answers and a test of whether users will click ads in conversational results as much as in classic search. Placement at the bottom suggests Google is probing for higher CTR without disrupting the main answer.
- The backdrop: Google has been nudging users into AI Mode over the past year. Keeping it ad-free likely helped adoption; adding ads tests the business model—and could reshape SEO, publisher traffic, and ad budgets if performance holds.
- What to watch:
  - Do AI answer ads cannibalize or complement traditional search ads?
  - Changes in ad load/placement over time.
  - Regulatory scrutiny around disclosures and ranking in AI experiences.
  - Publisher referral impacts as AI answers absorb more user intent.

Discussion prompt: Will users click “Sponsored” links in AI answers at rates comparable to top-of-page search ads—or does the chat-style format depress ad engagement?

**Google starts showing ads inside AI Mode answers**
Google is rolling out "Sponsored" ads directly within its AI Mode answer engine, a feature that was previously ad-free. These ads appear at the bottom of AI-generated responses, while citations remain in the sidebar. This move represents a significant test of monetization for conversational search, potentially reshaping SEO and publisher traffic as Google probes for higher click-through rates without disrupting the primary user experience.

**Hacker News Discussion Summary:**

The introduction of ads into Google's AI Mode sparked a discussion regarding user interface comparisons, the potential for "extortionary" business models, and the future of ad blocking.

*   **Perplexity vs. Google:** Users compared the new layout to Perplexity. While some find Perplexity superior for semantic understanding and source checking, others analyzed Google’s specific UI choices (blocks of links vs. scrolling), with one user describing the integration of irrelevant or cluttered link blocks as "embarrassing" compared to organic layouts.
*   **Monetization Concerns:** Several comments expressed cynicism regarding the intent behind these ads.
    *   One user theorized that AI might eventually refuse to answer "DIY" questions (e.g., plumbing instructions) to force users toward paid local service ads, comparing the model to "mugshot publishing" extortion.
    *   Others noted that Google already forces brands to bid on their own trademarks (like Nike or Adidas) to secure top slots; embedding ads in AI is seen as a way to maintain this gatekeeper status and potentially bypass current ad-blocking technologies.
*   **Ad Blocking:** The conversation inevitably touched on countermeasures, with users predicting the immediate rise of "AI ad blockers" designed specifically to scrub sponsored content from generated answers.

---

## AI Submissions for Tue Nov 11 2025 {{ 'date': '2025-11-11T17:16:35.247Z' }}

### We ran over 600 image generations to compare AI image models

#### [Submission URL](https://latenitesoft.com/blog/evaluating-frontier-ai-image-generation-models/) | 190 points | by [kalleboo](https://news.ycombinator.com/user?id=kalleboo) | [99 comments](https://news.ycombinator.com/item?id=45890186)

LateNiteSoft (makers of Camera+, Photon, REC) ran 600+ AI image edits to see which models work best for everyday photo tweaks—and to decide what to support in their new MorphAI app. Because they refuse “unlimited” AI pricing with fair‑use gotchas, they built CreditProxy, a pay‑per‑generation billing layer (and are inviting trial users).

How they tested
- Realistic use cases: pets, kids, landscapes, cars, product shots
- Naive prompts (what typical users actually type), not prompt‑engineered
- Tracked speed and behavior across models

Latency (avg during tests)
- OpenAI gpt-image-1: High ~80s (Medium ~36s)
- Gemini: ~11s
- Seedream: ~9s
- Times were stable across prompts

Findings
- Classic/photoreal filters: Gemini best preserves original detail and resists hallucinations, but often weakens or refuses edits—especially on people. OpenAI applies stronger looks but introduces “AI slop,” notably on faces. Seedream had some odd shortcuts.
- Long exposure: OpenAI did best when the effect made sense (landscapes, cars) but failed on cats/product and got trippy on portraits. Gemini often did nothing. Seedream leaned on generic light streaks.
- Heat map: None showed real “heat” understanding; Seedream mostly assumed humans emit heat.
- Creative effects (vintage, kaleidoscope, etc.): Gemini is conservative; OpenAI more creative but less faithful.

Why it matters
- Model choice should be task‑driven: Gemini for faithful edits, OpenAI for bold stylization (with risk), Seedream for speed and low cost but less grounding.
- For consumer photo apps, predictable costs, latency, and “do no harm” edits often beat raw creativity.

There’s a big, flip‑throughable comparison gallery on the post (with keyboard shortcuts).

**Summary of Hacker News Discussion:**

1. **Model Comparisons & Quirks:**  
   - **Gemini** is praised for preserving details and refusing risky edits (e.g., faces), but often returns unchanged images or weakens edits.  
   - **OpenAI** (GPT-image-1) is seen as more creative but introduces "AI slop," altering faces/objects and applying a yellow tint. Users debate whether this tint is intentional (e.g., vintage styling) or a technical flaw.  
   - **Seedream** excels in speed and cost but sacrifices detail, using shortcuts like generic light streaks.  

2. **Technical Insights:**  
   - OpenAI’s pipeline regenerates images semantically rather than editing pixels directly, leading to unintended changes (e.g., altered faces). This is attributed to tokenization and latent space architecture.  
   - Gemini’s conservatism, especially with people, may stem from safety filters.  

3. **Practical Challenges:**  
   - Users report frustration with models ignoring prompts (e.g., Gemini refusing edits) or altering unintended areas, necessitating manual checks.  
   - Cost and latency matter: Seedream’s speed appeals to small creators, while OpenAI’s pricing and reliability raise concerns.  

4. **Community Reactions:**  
   - Skepticism about "AI slop" as hype vs. substance, with critiques of stock photo industry impacts.  
   - Debate over whether OpenAI’s yellow tint is a feature (stylistic choice) or a bug.  
   - Interest in hybrid workflows (e.g., SDXL, LoRAs) for better control, highlighting a gap in commercial SaaS offerings.  

5. **Notable Quotes:**  
   - *"Models sometimes alter objects they weren’t supposed to touch… a complete failure."*  
   - *"Peak quality in realistic rendering might already be behind us."* (referring to DALL-E 3’s trade-offs).  

**Key Takeaway:**  
The discussion underscores the need for task-specific model selection, transparency in AI editing behavior, and tools that balance creativity with fidelity. Community sentiment leans toward cautious adoption, emphasizing manual oversight and hybrid approaches for professional use.

### Scaling HNSWs

#### [Submission URL](https://antirez.com/news/156) | 206 points | by [cyndunlop](https://news.ycombinator.com/user?id=cyndunlop) | [42 comments](https://news.ycombinator.com/item?id=45887466)

Scaling HNSWs: antirez’s hard-won lessons from bringing HNSW to Redis

- Not just another intro: After a year building an HNSW-based “Redis experience,” antirez shares advanced, practical insights—what it takes to make HNSW low-latency and production-ready, not just paper-correct.
- HNSW isn’t the final form: The original paper is excellent but incomplete for real systems. He added true deletions (beyond tombstones), and questions how necessary the hierarchy (“H”) really is—early results suggest a flat, single-layer variant can work but with higher seek time. The sweet spot may be modified level selection rather than all-or-nothing.
- Memory is the real enemy: HNSW is pointer-heavy and multi-level; vectors are big. Extra layers cost ~1.3x space on average (with p=0.25), so hierarchy isn’t the main bloat—vector storage is.
- Biggest win: 8‑bit quantization by default. Per-vector max-abs scaling delivers roughly 4x faster search and ~4x smaller vectors with near-identical recall in practice. Pointers still dominate some footprint, but this is the low-hanging fruit that moves the needle in Redis.
- Why this quantization: Using a single max-abs per vector keeps cosine similarity fast—compute a simple scale factor and do the heavy lifting in the integer domain with unrolled loops and multiple accumulators for modern CPUs. It’s faster than min/max quantization while preserving accuracy.
- Tradeoffs he didn’t take (yet): Pointer compression could save memory (upper bytes often identical on 64-bit) but may cost latency; he hasn’t adopted it given Redis’s performance bar.
- Direction of travel: Don’t assume “evolution” just means on-disk HNSW. There’s room for fresh data-structure ideas around hierarchy, level selection, deletions, and quantization that can beat conventional wisdom.

Why it matters: If you’re building vector search in latency-sensitive systems, quantization and careful algorithmic choices can deliver big wins without killing recall—and some revered parts of HNSW may be optional with the right tweaks. Redis Vector Sets ships with 8-bit quantization on by default for exactly these reasons.

**Summary of Discussion:**

The discussion around antirez's insights on scaling HNSW for Redis highlights several technical challenges, trade-offs, and alternative approaches in vector search systems:

### 1. **Filtered Searches & Performance**  
   - Applying metadata filters (e.g., regional constraints) during HNSW traversal can degrade performance, as it requires checking each candidate vector against filter criteria. Solutions like **Turbopuffer** (200ms latency for 100B vectors) and **Vespa’s hybrid search** were cited as addressing this, though antirez notes Redis prioritizes low-latency by limiting graph traversal depth early if filters are restrictive.  
   - **Lucene/Elasticsearch** shortcuts filtering by pre-determining eligible nodes, but worst-case scenarios still involve brute-force distance comparisons.  

### 2. **Quantization & Efficiency**  
   - Redis’s **8-bit per-vector quantization** (using max-abs scaling) was praised for reducing memory usage by ~4x and speeding up searches while preserving recall. Critics noted that **DiskANN** and other systems achieve similar gains via int8/binary quantization but require trade-offs in recall.  
   - antirez clarified that Redis’s approach prioritizes CPU-friendly integer operations and avoids complex schemes like product quantization (PQ), balancing practicality with near-identical recall for most use cases.  

### 3. **Hierarchy in HNSW**  
   - Debate arose over whether HNSW’s hierarchical layers ("H") are essential. antirez’s experiments suggest a **flat, single-layer variant** could suffice with higher seek times, proposing modified level selection as a middle ground. Academic references (e.g., "Hubs in HNSW") were shared, underscoring ongoing research into hierarchical efficiency.  

### 4. **Implementation Challenges**  
   - **Memory vs. Latency**: Pointer compression was discussed but deemed risky for Redis’s strict latency goals.  
   - **Single-Threaded Design**: Redis’s single-threaded model influenced HNSW implementation choices, favoring simplicity and deterministic performance over parallelism.  

### 5. **Alternative Approaches**  
   - **Vespa** and **SPFresh** were highlighted for hybrid search optimizations.  
   - Broader themes emerged on **system design philosophy**: Simplicity and "good enough" solutions (e.g., 60th vs. 72nd recall percentile) often trump theoretical perfection, especially in latency-sensitive applications like RAG.  

### Key Takeaway:  
The discussion underscores that real-world vector search systems require pragmatic trade-offs—quantization, filtered search shortcuts, and hierarchy adjustments—to balance speed, memory, and recall. Redis’s choices reflect a focus on practical, low-latency solutions over algorithmic purity.

### Adk-go: code-first Go toolkit for building, evaluating, and deploying AI agents

#### [Submission URL](https://github.com/google/adk-go) | 80 points | by [maxloh](https://news.ycombinator.com/user?id=maxloh) | [23 comments](https://news.ycombinator.com/item?id=45891968)

Google open-sources ADK for Go: a code-first toolkit for building and deploying AI agents

What it is: ADK (Agent Development Kit) for Go is a modular, model-agnostic framework focused on building, evaluating, and orchestrating AI agents using idiomatic Go. It’s optimized for Gemini but works with other models and frameworks.

Why it matters: Go is a natural fit for cloud-native, concurrent systems. ADK brings a strongly typed, testable, versionable approach to agent development—aimed at production-grade workloads and multi-agent orchestration—without locking you into a specific model or deployment target.

Highlights
- Code-first and idiomatic Go: define agent logic, tools, and orchestration in code for flexibility and testability.
- Rich tool ecosystem: use prebuilt tools or wire in custom functions to extend agent capabilities.
- Multi-agent systems: compose specialized agents into larger workflows.
- Deploy anywhere: easy containerization; strong fit for Cloud Run and cloud-native environments.
- Model-agnostic, Gemini-optimized: integrates with Gemini while staying portable.

Quick start: go get google.golang.org/adk

Details: Apache-2.0 licensed, ~2.8k GitHub stars, with companion ADKs for Python and Java plus docs and samples at google.github.io/adk-docs/.

**Summary of Hacker News Discussion:**

### **Key Themes**
1. **Go’s Strengths for AI Agents**:
   - **Concurrency & Performance**: Users highlight Go’s native concurrency (goroutines/channels) as ideal for AI agents handling parallel tasks (e.g., HTTP requests, database operations) without serialization bottlenecks. Its compiled binaries and efficiency suit cloud/serverless deployments (e.g., Cloud Run).
   - **Type Safety & Testability**: Go’s strong typing and idiomatic design enable reliable, maintainable agent code. Some contrast this with Python’s flexibility, which can lead to runtime errors in complex systems.

2. **Comparison with Python/Java**:
   - **Python ADK**: Praised for simplicity (e.g., defining agents as objects with tools) and built-in features (debugging, session management). However, Go is seen as better for production-scale systems requiring strict concurrency and type safety.
   - **Java**: Noted for enterprise-grade performance but seen as less agile for rapid agent development. Go strikes a balance between performance and developer ergonomics.

3. **Use Cases & Skepticism**:
   - **Production Readiness**: Users see ADK-Go as promising for multi-agent orchestration in cloud-native environments, especially with Gemini optimizations. Some question if inference latency (often model-dependent) negates Go’s runtime advantages.
   - **Model Agnosticism**: While Gemini-optimized, the framework’s portability across models (e.g., OpenAI, Claude) is appreciated, though integration efforts vary.

4. **Tooling & Ecosystem**:
   - **Prebuilt Tools**: The ADK’s tool ecosystem (e.g., HTTP/SQLite connectors) simplifies agent development. Custom tool integration via Go functions is seen as a plus.
   - **Debugging/Orchestration**: Features like session management and callbacks for data anonymization are highlighted as valuable for complex workflows.

### **Notable Opinions**
- **Rust vs. Go**: A user notes Rust’s popularity but argues Go’s concurrency model is more approachable for agent development.
- **Python’s Dominance**: Some acknowledge Python’s hold on AI prototyping but see Go as better for scaling “script-like” agents into robust applications.
- **Deployment Flexibility**: Go’s compiled binaries are praised for serverless/edge deployments, with one user sharing success in production serverless functions.

### **Criticisms & Questions**
- **Learning Curve**: A few users express surprise at Go’s type-driven agent definitions (similar to TypeScript) but find it manageable.
- **Gemini Lock-In?**: Clarified that ADK is model-agnostic, though Gemini optimizations are a focus.

### **Miscellaneous**
- **Community Excitement**: Several users express enthusiasm for Go’s role in advancing multi-agent systems and cloud-native AI.
- **References**: Links to prior HN posts about agents and Claude’s Python implementation are shared for comparison.

**Overall Sentiment**: Positive, with developers seeing ADK-Go as a compelling option for building scalable, type-safe AI agents in production, particularly where concurrency and cloud-native deployment matter. Python remains favored for prototyping, but Go’s strengths in reliability and performance are seen as filling a critical niche.

### Xortran - A PDP-11 Neural Network With Backpropagation in Fortran IV

#### [Submission URL](https://github.com/dbrll/Xortran) | 46 points | by [rahen](https://news.ycombinator.com/user?id=rahen) | [10 comments](https://news.ycombinator.com/item?id=45892174)

XOR Neural Network in FORTRAN IV (RT-11, PDP-11/34A) — A delightful retrocomputing crossover: a tiny multilayer perceptron that learns XOR, written in 1970s-era FORTRAN IV and run under RT-11 on a PDP‑11/34A (via the SIMH emulator). It’s a legit backprop network: 1 hidden layer (4 neurons, leaky ReLU), MSE loss, tanh output, “He-like” Gaussian init via a Box–Muller variant, and learning-rate annealing. The whole thing trains 17 parameters and converges in minutes on real hardware (or at a realistic 500K throttle in SIMH), printing loss every 100 epochs and nailing the XOR targets. It compiles with the original DEC FORTRAN IV compiler and needs just 32 KB plus an FP11 floating-point unit. Includes an RT‑11 disk image, so you can attach it in SIMH and run, or build with .FORTRAN and .LINK. A neat proof that backprop doesn’t require modern frameworks—just patience, floating point, and a 1970s minicomputer.

The discussion highlights a mix of nostalgia, technical insights, and historical context around retrocomputing and early neural networks:  

- **Retro Hardware & Neural Networks**: Users reminisce about professors implementing neural networks on PDP-11s in the 1980s, noting limitations like the PDP-11/34A’s modest power (roughly comparable to an IBM XT) but praising its ability to handle sustained workloads with its FPU. References are made to historical models like the Neocognitron (1980s) and the role of VAX systems in later backpropagation research.  

- **FORTRAN IV Nuances**: Debate arises around FORTRAN IV’s features, including its use of FORTRAN 66 extensions, lack of modern constructs (e.g., structured `If/Then/Else`), and reliance on hardware FPUs or software emulation. The project’s compatibility with the original DEC compiler and constraints (32 KB memory, FP11 support) sparks appreciation for its efficiency.  

- **Humor & Corrections**: A lighthearted thread corrects Fortran version timelines (Fortran IV in 1966 vs. Fortran 77 in 1977), jokingly referencing Charles Babbage’s Analytical Engine. Another user points out the ironic overlap between PDP-11 hardware and the “Parallel Distributed Processing” (PDP) connection in neural network literature.  

- **Appreciation for Simplicity**: Commentators laud the project for demonstrating core concepts without modern frameworks, emphasizing the value of understanding fundamentals over today’s complexity.  

Overall, the exchange blends technical admiration for early computing with wry humor about its historical quirks.

### AI documentation you can talk to, for every repo

#### [Submission URL](https://deepwiki.com/) | 161 points | by [jicea](https://news.ycombinator.com/user?id=jicea) | [118 comments](https://news.ycombinator.com/item?id=45884169)

Devin DeepWiki: turn any repo into an AI‑generated code wiki
A new tool called Devin DeepWiki promises “index your code with Devin,” letting you add a GitHub repo and get a browsable, wiki‑style view of the codebase with AI summaries and search. The demo shows a catalog of popular projects (VS Code, Transformers, Express, SQLite, React, Kubernetes, etc.) you can pick to “understand,” suggesting it pre‑indexes large OSS repos for instant exploration. The pitch is faster onboarding and code comprehension: instead of hopping across files, you get cross‑linked context and natural‑language explanations.

Why it’s interesting
- Speaks to the growing demand for AI‑first code navigation and docs, competing with tools like Sourcegraph/Cody, CodeSee, and auto‑docs generators.
- Could be useful for due diligence, learning popular frameworks, or ramping onto large legacy codebases.

What to watch
- Accuracy and hallucinations in summaries; keeping the wiki in sync with fast‑moving repos.
- Privacy/security for private code and indexing scope.
- How it handles truly large monorepos and language/tooling diversity.

The discussion around Devin DeepWiki highlights skepticism and critical feedback, focusing on **accuracy, documentation integrity, and practical usability**:

1. **Accuracy Concerns**:  
   - Users criticize AI-generated summaries and diagrams for being **outdated, incorrect, or misleading**. For example, the tool inaccurately claims a VS Code extension exists, but the linked repository shows it’s experimental/unreleased.  
   - Debate arises over whether AI can reliably handle subjective or nuanced topics (e.g., React vs. functional frameworks, OOP vs. FP), with concerns that LLMs might **reinforce biases or misinterpretations** instead of clarifying them.

2. **Documentation Frustrations**:  
   - The project’s own documentation is flagged as **confusing or incomplete**, such as installation instructions for an unreleased VS Code extension. Users note that incomplete or incorrect docs waste time and erode trust, especially for contributors trying to build/use the tool.  
   - A meta-point emerges: If AI-generated docs (like DeepWiki’s) are error-prone, they risk creating a **“hallucination spiral”** where future AI models train on flawed data, worsening accuracy over time.

3. **Project Transparency**:  
   - Critics argue the demo’s pre-indexed OSS repos (e.g., VS Code, React) mask the tool’s limitations. The maintainer admits parts are experimental but defends the approach as a calculated risk.  
   - Some users question the ethics of promoting unfinished tools, suggesting it prioritizes hype over practicality, especially for private codebases.

4. **Mixed Reactions to AI’s Role**:  
   - While some acknowledge AI’s potential to surface high-level insights, others stress that **human-curated documentation** remains irreplaceable for precision.  
   - A recurring theme: AI-generated docs might complement but not replace manual efforts, particularly in filling gaps for legacy/unmaintained projects.

**Key Takeaway**:  
The discussion reflects cautious interest in AI-powered code navigation tools but emphasizes the need for accuracy, transparency, and human oversight. DeepWiki’s current implementation raises red flags, but its concept sparks debate about balancing automation with reliability in developer tools.

### How to Train an LLM: Part 1

#### [Submission URL](https://omkaark.com/posts/llm-1b-1.html) | 15 points | by [parthsareen](https://news.ycombinator.com/user?id=parthsareen) | [3 comments](https://news.ycombinator.com/item?id=45891801)

What it is
- A hands-on series documenting the author’s attempt to build a domain-specific LLM from scratch. Part 1 sets a clean, “boring” Llama 3–style baseline and maps out the training math, memory, and token budgeting before getting fancy.

Model and data
- Architecture: ~1.24B params, Llama 3–ish
  - 16 layers, hidden size 2048, SwiGLU (×4), 32 heads with 8 KV heads (GQA), RoPE theta 500k, vocab 2^17, tied embeddings, no attention/MLP bias, norm_eps 1e-5.
- Context: targeting 4096 at the end, but trains mostly at 2048 (common practice: short context for 80–90% of steps, extend near the end).
- Data: Karpathy’s fine-web-edu-shuffled.
- No cross-document masking (for now).

Compute plan
- Hardware: 8×H100 80GB.
- Token budget: Chinchilla-style 1:20 params:tokens → ~20B tokens for a 1B model.
- Global batch target: 1M tokens (GPT-3 XL–style).
  - With FP32 ballpark estimates and a 5GB “misc” reserve per GPU, each H100 fits ~7×[2048] sequences per step.
  - Across 8 GPUs: micro-batch ≈ [56, 2048] = 114,688 tokens/step.
  - Gradient accumulation: ceil(1,048,576 / 114,688) = 10 micro-steps per global batch.
  - Steps: 20B / 1M = 20,000 optimizer updates; with accumulation, ≈200,000 forward/backward micro-steps.

Memory insights (intuition, FP32, unfused)
- Rough peaks by phase:
  - Forward: Weights + Activations.
  - Backward: Weights + Activations + Gradients (often the peak).
  - Optimizer step: Weights + Gradients + Optimizer states (~4× params in bytes).
- Activation memory dominates at realistic batch sizes due to unfused ops saving intermediates.
- Empirical activation cost scales linearly with batch size; ~7.95GB per [1,2048] sequence in this setup.

Immediate optimizations planned
- torch.compile and FlashAttention to fuse ops and slash activations.
- Gradient accumulation (already used).
- More to come (mixed precision, custom kernels, infra upgrades).

Why it matters
- Clear, number-first walkthrough of how far 8×H100 can push a 1B Llama-style pretrain without exotic tricks.
- Sets a reproducible baseline before exploring “BLASphemous” optimizations, longer context, inference-friendly tweaks, and a custom “token farm.”

What’s next
- Improving training infra, scaling token throughput, extending context efficiently, and architectural changes aligned with the final task. The domain target is still under wraps.

The discussion touches on contrasting perspectives about LLM deployment and hardware requirements:

1. **Mobile vs. Server Debate**: One user argues LLMs should prioritize optimization for mobile/portable devices (cheaper, easier maintenance) rather than expensive server infrastructure. They suggest deploying LLMs directly on phones or edge devices.

2. **Counterexample with Laptops**: A reply highlights running a 70B-parameter LLM on a $300 laptop with 96GB RAM using tools like `llm.cpp`, implying powerful models can already operate on consumer-grade hardware. The user mentions purchasing the laptop for non-AI reasons, suggesting incidental compatibility with AI workloads.

3. **Unclear Contribution**: A third comment ("cdcntnt nwbrrwd") appears fragmented or mistyped, offering no clear insight.

**Key Takeaway**: The exchange reflects ongoing tensions in the AI community between centralized (server-based) and decentralized (edge/mobile) LLM deployment strategies, with practical examples demonstrating feasibility on modest hardware.