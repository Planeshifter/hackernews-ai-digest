import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Wed Jan 31 2024 {{ 'date': '2024-01-31T17:11:41.321Z' }}

### MobileDiffusion: Rapid text-to-image generation on-device

#### [Submission URL](https://blog.research.google/2024/01/mobilediffusion-rapid-text-to-image.html) | 241 points | by [jasondavies](https://news.ycombinator.com/user?id=jasondavies) | [57 comments](https://news.ycombinator.com/item?id=39210458)

On Hacker News, a blog post titled "MobileDiffusion: Rapid text-to-image generation on-device" caught the attention of readers. The post discusses the challenges of generating high-quality images from text prompts on mobile devices and introduces a new approach called MobileDiffusion. This approach aims to achieve rapid text-to-image generation on mobile devices by optimizing the model architecture and reducing the computational complexity.

The post explains that traditional text-to-image diffusion models are inefficient on mobile devices due to the iterative denoising process and the complexity of the network architecture. However, recent advancements in inference solutions on Android and iOS have made it possible to run these models on mobile devices at a fraction of the time.

MobileDiffusion is described as an efficient latent diffusion model specifically designed for mobile devices. It incorporates DiffusionGAN, which enables one-step sampling during inference and fine-tunes a pre-trained diffusion model using a Generative Adversarial Network (GAN) to model the denoising step.

The authors tested MobileDiffusion on iOS and Android premium devices and found that it can generate a 512x512 high-quality image in half a second. The model size is also relatively small, with just 520M parameters, making it well-suited for mobile deployment.

The blog post goes on to provide a detailed analysis of the architectural efficiency of text-to-image diffusion models, focusing on the UNet architecture used in MobileDiffusion. The authors explore the impact of different components and operations within the architecture, such as transformer blocks and convolution blocks, and propose strategies for optimizing efficiency.

Overall, the introduction of MobileDiffusion offers an exciting development in the field of text-to-image generation on mobile devices. The ability to generate high-quality images rapidly and efficiently opens up new possibilities for enhancing user experience and addressing privacy concerns.

The discussion on the Hacker News submission revolves around various aspects of the MobileDiffusion model and its implications.

One user interprets the MobileDiffusion model as a potential solution for mobile deployment, highlighting its efficiency and suitability for rapid image generation. Another user raises concerns about AI watermarking and the potential harm it could cause in terms of traceability and content control.

A discussion ensues regarding the difference between harmful deepfakes and harmless fakes, with one user emphasizing the importance of distinguishing between them. Another user expresses discomfort with restricting AI-generated content and argues for a balance between technical solutions and legislative regulations.

The conversation shifts to the enforcement of existing laws and the effectiveness of banning specific technologies in dealing with harassment and offensive content. The potential power of certain words in causing distress and inciting violence is also discussed, with some users pointing out the importance of clarifying the context and intent behind such words.

The conversation then veers towards the legal power Google has in combating deepfakes and the responsibility of platforms in addressing issues like harassment and pornography. One user shares an article about the connection between online harassment and suicides.

A user mentions the anticipation of powering features in the next-generation Pixel devices and the progress made in terms of on-device processing. This leads to a discussion about the technical aspects of the MobileDiffusion model, including its network architecture and training process.

The conversation takes a tangent to discuss Google's marketing tactics and the perception of its services and devices. Other topics touched upon include the embarrassment of Google Research, the potential wasted by Google over the years, and the need for aligning research with product development.

One user argues that Google's research efforts have resulted in wasted potential and suggests that applied product development is more valuable. Another user points out that research is necessary, but determining successful products is a separate challenge.

The discussion concludes with a user expressing their disappointment in Google's recent actions.

### Testing how hard it is to cheat with ChatGPT in interviews

#### [Submission URL](https://interviewing.io/blog/how-hard-is-it-to-cheat-with-chatgpt-in-technical-interviews) | 237 points | by [michael_mroczka](https://news.ycombinator.com/user?id=michael_mroczka) | [427 comments](https://news.ycombinator.com/item?id=39206731)

A recent experiment conducted by interviewing.io explored the potential for cheating in technical interviews using ChatGPT. The experiment involved a group of interviewers asking different types of questions to a pool of interviewees who were instructed to use ChatGPT during the interview. The three question types included verbatim LeetCode questions, modified LeetCode questions, and custom questions.
The results of the experiment revealed that interviewees were able to successfully cheat using ChatGPT in various ways. In the verbatim LeetCode questions, 45% of interviewees were able to pass with the help of ChatGPT, while in the modified LeetCode questions, 56% were successful. For the custom questions, the success rate was even higher at 77%.
These findings suggest that companies may need to reconsider the types of questions they ask in technical interviews to prevent cheating. While the experiment acknowledges that the lack of video in the interviews reduces realism, it highlights the potential for cheating even in real interviews.
Overall, the experiment raises important questions about the impact of AI technology like ChatGPT on the integrity of technical interviews and calls for changes in the interview process to adapt to these new challenges.

The discussion on Hacker News revolves around various aspects of the experiment conducted by interviewing.io, exploring the potential for cheating in technical interviews using ChatGPT.
Some users point out that the use of AI in interviews allows candidates to cheat easily, as AI can instantly solve coding problems. Others mention that the lack of video in the interviews reduces realism and that companies should consider a more realistic environment to prevent cheating.
The discussion also touches on the importance of honesty and transparency in interviews, with some users highlighting the need for a strong ethical component in the interview process. There is also debate about the effectiveness of current interview procedures and the types of questions used.
Some users argue that cheating can be delayed or detected through certain measures, such as making small changes to questions or using collaborative coding sessions with assistants like ChatGPT. However, others emphasize the need for basic coding questions and the importance of assessing fundamental skills rather than relying heavily on AI tools.
The discussion also touches on broader topics such as the state of education and the need for critical thinking skills in the software engineering field. There is a suggestion that a more comprehensive approach to teaching programming is necessary.
Some users express concerns about the security implications of using AI tools like ChatGPT in interviews, while others argue that security concerns depend on the specific use case and the level of trust given to candidates.

Overall, the discussion raises questions about the practicality and effectiveness of using AI tools in technical interviews, and the need for continuous improvements and adaptations in the interview process to maintain integrity and accurately assess candidates' abilities.

### LLaVA-1.6: Improved reasoning, OCR, and world knowledge

#### [Submission URL](https://llava-vl.github.io/blog/2024-01-30-llava-1-6/) | 188 points | by [tosh](https://news.ycombinator.com/user?id=tosh) | [39 comments](https://news.ycombinator.com/item?id=39206375)

Meta, formerly known as Facebook, has shared an update on their AI efforts, including the training of their next-gen model called Llama-3 and the construction of a massive compute infrastructure. The CEO, Mark Zuckerberg, mentioned in a social media post that their long-term vision is to build general intelligence and make it widely available to benefit everyone. The post also highlighted the integration of their AI research efforts, FAIR and GenAI, and their investment in NVIDIA's H100 GPUs. They aim to have 35,000 H100s by the end of the year, totaling almost 600,000 H100 equivalents of compute when other GPUs are included. Additionally, the post mentioned Meta's progress in developing AI-centric computing devices such as the Ray Ban Meta smart glasses.

The discussion on this submission revolves around various aspects of Meta's AI efforts, specifically regarding the training of the Llama-3 model and the development of a massive compute infrastructure. Some comments express excitement about the progress made in AI models and the potential improvements in AI capabilities. Others discuss the challenges and trade-offs associated with training large-scale models, such as the resource optimization needed and the need to address privacy concerns. There is also a discussion about AI applications outside of large-scale models, such as robotics and computer vision. Some users mention their personal projects and interests related to AI, including OCR (optical character recognition) models and the ability to generate captions for images. 

Overall, the comments showcase both enthusiasm for the advancements in AI technology and a critical examination of its implications and challenges.

### DeepSeek Coder: Let the Code Write Itself

#### [Submission URL](https://deepseekcoder.github.io/) | 198 points | by [fintechie](https://news.ycombinator.com/user?id=fintechie) | [54 comments](https://news.ycombinator.com/item?id=39209814)

DeepSeek AI has developed DeepSeek Coder, a series of code language models that can generate code by itself. These models are trained on a combination of code and natural language data in both English and Chinese. With sizes ranging from 1B to 33B versions, the models are trained on a large code corpus and further fine-tuned with instruction data. DeepSeek Coder achieves state-of-the-art performance among open code models and is free for research and commercial use. It outperforms existing models such as CodeLLama-34B and GPT-3.5-turbo on various coding benchmarks. To try DeepSeek Coder, visit their website or find more information on their Github and HuggingFace pages. For any inquiries or issues, you can contact them at agi_code@deepseek.com.

The discussion on this submission revolves around different aspects of using AI for generating code and the challenges associated with it. One user shares their experience with using AI locally, while another suggests trying Azure's Codex as a solution. There is also a mention of OpenAI's GPT-4 as a potential option. Another user raises concerns about the correctness of AI-generated code and suggests recording inputs and outputs to verify the functionality. The discussion also touches on the difficulties of writing code, including the challenges of writing tests and documentation. Some users recommend trying existing solutions like Copilot and Retrieval-Augmented Generation (RAG). Concrete advice is given on using RAG and considering existing code generation models, but also highlighting the limitations and potential issues with scaling. One user shares their experience with trying LLM but facing difficulties with its integration. Additionally, the announcement of DeepSeek Coder 33B and its reduced context length is mentioned. The discussion concludes with a user questioning the high memory requirement of the DeepSeek Coder and its impact on laptops.

### Building an early warning system for LLM-aided biological threat creation

#### [Submission URL](https://openai.com/research/building-an-early-warning-system-for-llm-aided-biological-threat-creation) | 109 points | by [minimaxir](https://news.ycombinator.com/user?id=minimaxir) | [168 comments](https://news.ycombinator.com/item?id=39207291)

OpenAI is developing a blueprint for evaluating the risk that large language models (LLMs) could assist in creating biological threats. In their evaluation, they found that GPT-4 only provides a mild improvement in accuracy for biological threat creation. The study involved biology experts and students who were randomly assigned to either a control group with internet access only or a treatment group with access to GPT-4 in addition to the internet. While the uplift in accuracy and completeness was not statistically significant, it serves as a starting point for further research. OpenAI is seeking community feedback and input on their work.

The discussion on this submission includes various perspectives and opinions. One user criticizes the study, suggesting that the researchers are pretending to be scientists and comparing their work to software engineering. Another user comments on the difficulty of controlling biological threats, mentioning the minimal control advocated by a lobby group in the UK for checking DNA, RNA, and protein sequences. The discussion continues with debates about the feasibility of manipulating RNA and DNA sequences in creating viral strains, the lack of strict controls in the undergraduate biology field, and the potential risks of AI technology in bioengineering. Some users also express skepticism about the effectiveness of GPT-4 in providing reasonable and consistent reasoning compared to human reasoning. The discussion touches on a wide range of topics related to biological threats, AI capabilities, and the complexity of biological research.

### XFaaS: Hyperscale and Low Cost Serverless Functions at Meta

#### [Submission URL](https://www.micahlerner.com/2024/01/23/xfaas-hyperscale-and-low-cost-serverless-functions-at-meta.html) | 188 points | by [greghn](https://news.ycombinator.com/user?id=greghn) | [75 comments](https://news.ycombinator.com/item?id=39200239)

XFaaS, a paper presented at the Symposium on Operating Systems Principles, describes Meta's internal system for serverless functions. XFaaS runs trillions of function calls per day on over 100,000 servers and addresses several challenges in running a large-scale serverless system. These challenges include handling load spikes, ensuring fast function startup and execution, global load balancing, resource utilization, and preventing overload of downstream services. 

The architecture of XFaaS consists of five main components: Submitter, load balancers, DurableQ, Scheduler, and Worker Pool. Clients schedule function execution through the Submitter, which interfaces with downstream components of the system. Load balancers ensure effective utilization of distributed system resources, and the Scheduler determines the order of function calls based on their criticality, execution deadline, and capacity quota. The Scheduler also uses a traffic matrix to decide if functions should be sourced from different regions for load balancing purposes. Worker Pool handles the execution of functions, and Locality Groups limit a function's execution to a subset of the pool to improve worker utilization.

XFaaS implements performance optimizations such as time-shifted computing, which allows flexibility in when a function executes, and cooperative JIT compilation. It also prevents overload of downstream services by implementing backpressure, a concept borrowed from TCP and other distributed systems.

Overall, XFaaS provides insights into Meta's serverless system and the challenges they faced in achieving hyperscale and low-cost serverless functions.

The discussion on this submission covers several aspects of serverless functions and their implementation. One commenter mentions that larger organizations often struggle with adopting new practices and mandates, while others discuss how Firebase is a good case study for successful serverless function implementation. The simplicity and fast deployment of serverless functions are highlighted, although there are concerns about the difficulty of maintaining codebases over time. Some commenters also discuss the potential benefits of using FaaS in contrast to traditional infrastructure providers. The discussion also touches on topics such as event-driven architecture, the importance of observability, and the trade-offs between serverless functions and other technologies like Kubernetes. There are varying opinions on the suitability of FaaS for different types of workloads and the role of infrastructure providers in managing hardware resources. The discussion also includes comments about the historical context of FaaS and its relation to disruptive technologies. Overall, the discussion provides additional insights and perspectives on the topic of serverless functions.

### Give AI curiosity, and it will watch TV forever (2018)

#### [Submission URL](https://qz.com/1366484/give-ai-curiosity-and-it-will-watch-tv-forever) | 74 points | by [yamrzou](https://news.ycombinator.com/user?id=yamrzou) | [95 comments](https://news.ycombinator.com/item?id=39208029)

Researchers from OpenAI, a non-profit AI lab, in collaboration with UC Berkeley and the University of Edinburgh, have developed an AI algorithm that can explore and even beat video games without human guidance. The algorithm was given a simple definition of curiosity, which involved predicting what the environment would look like one frame into the future and being rewarded for how wrong the prediction was when the next frame occurred. The researchers found that the AI agents were able to explore more than 50 video games using this curiosity-based approach. However, the AI agents also exhibited curious behaviors like deliberately dying to see the Game Over screen and becoming engrossed with a fake TV, flipping through channels to find something new. The ability to exhibit curiosity allows AI algorithms to learn and interpret the world autonomously, improving their problem-solving capabilities.

The discussion revolves around the impact of technology, particularly YouTube and iPads, on children's learning and development. Some users argue that excessive screen time on iPads and the content consumed on platforms like YouTube can be detrimental to children's intellectual growth. They suggest that these platforms prioritize engagement and maximizing growth potential rather than promoting educational content. Others discuss the potential benefits of platforms like TikTok, which offer short-form educational content. However, concerns are raised about the harmful effects of social media and the need for responsible content creation. There is also a debate about neuroplasticity and its impact on learning, with some arguing that the brain's ability to change is limited in adults compared to children. The discussion also touches upon the role of algorithms and parental control on platforms like YouTube. Overall, the discussion reflects a range of perspectives on the influence of technology on children's learning outcomes.

### Words Make It Obvious That Your Text Is Written by AI

#### [Submission URL](https://medium.com/practice-in-public/these-words-make-it-obvious-that-your-text-is-written-by-ai-9b04f399d88c) | 14 points | by [doener](https://news.ycombinator.com/user?id=doener) | [6 comments](https://news.ycombinator.com/item?id=39210841)

Writer James Presbitero Jr. shares his insights on how to identify AI-generated content by pointing out seven common words and phrases that AI tends to use. He emphasizes the importance of maintaining a human touch in writing and offers tips on how to edit out these AI giveaways. Presbitero also mentions the significance of sentence length in creating more engaging and human-sounding content. While AI can be a valuable writing tool, he cautions against relying on it too heavily without proper editing.

The discussion on this submission focuses on various aspects of AI-generated writing. 
One commenter, vndrb, humorously suggests that writing AI-generated content is as simple as following a set of instructions, similar to a 5th-grade student writing a structured MLA 5-paragraph essay. They go on to mention the importance of including variant phrases and a concluding paragraph to make the AI-generated content less obvious.
Another commenter, vba616, raises an interesting point by suggesting that training data for AI-generated content often consists of large travel essays or generic format mandated by teachers. They argue that this approach can limit creativity and force students to produce mechanical and formulaic work that lacks context and individuality.
Ryndtzl adds to the conversation by recommending that AI-generated content avoid including specific words and phrases that are commonly associated with AI. They suggest that removing these giveaways can make the writing seem more human-like.
In response to vba616's comment, kdrbym suggests that instead of focusing on specific points, the AI-generated content should provide more descriptive alternatives and concrete examples. Hmmyhvc replies simply with "sounds LLM," indicating agreement with kdrbym's point.

Overall, the discussion highlights the challenges of creating AI-generated content that sounds authentic and human-like.

---

## AI Submissions for Tue Jan 30 2024 {{ 'date': '2024-01-30T17:20:02.100Z' }}

### .ai website registrations are a windfall for tiny Anguilla

#### [Submission URL](https://spectrum.ieee.org/ai-domains) | 291 points | by [headalgorithm](https://news.ycombinator.com/user?id=headalgorithm) | [198 comments](https://news.ycombinator.com/item?id=39194477)

Artificial intelligence (AI) has had a significant impact on the tiny Caribbean island of Anguilla, thanks to its unique domain name extension, .ai. In the late 1980s, Anguilla was assigned the .ai domain, which has now become in high demand for AI companies. In an interview with IEEE Spectrum, Vince Cate, who manages domain registrations for the Anguillan government, discussed how the AI boom has affected .ai. Cate explained that he became the manager of the .ai domain after reaching out to Jon Postel, who was in charge of top-level domains. Since then, the surge in AI interest has led to a significant increase in .ai domain registrations. Cate mentioned that after the release of ChatGPT in November 2022, their sales went up by almost four times in the subsequent five months. This boom in registrations has proven to be a windfall for Anguilla, contributing significantly to the government's budget. Unlike other countries that have opened up their top-level domains to foreign companies for extended periods, Anguilla has kept control of its domain locally, ensuring that the government receives most of the revenue generated from .ai registrations.

The discussion on this submission revolves around various aspects related to domain names and their control, as well as the implications of the .ai domain extension being in high demand for AI companies. Here are some key points from the discussion:

- Some users discuss the financial impact of the .ai domain registrations for Anguilla, with one user pointing out that the revenue from these registrations contributes significantly to the government's budget.
- There is a debate about the future demand for .ai domains and whether the current boom is sustainable. One user suggests that the trend might die down after the initial excitement around AI projects.
- The discussion also touches upon the registration and renewal fees for .ai domains. Some users express surprise at the high cost of renewals, while others argue that it is reasonable given the demand for these domains.
- The comparison between the .ai domain and other country-code top-level domains (ccTLDs) is brought up, with one user mentioning that unlike some other countries, Anguilla has kept control of its domain locally to ensure that the government receives most of the revenue.
- The conversation expands to discuss other ccTLDs and their control. Some users point out that the control of ccTLDs lies with the respective countries, and organizations like ICANN or ISO do not have jurisdiction over them.
- There is a mention of Tuvalu, another small island nation that has leveraged its unique domain extension, .tv, for economic gain. Users discuss the economic implications of the domain registrations for Tuvalu and its membership in the United Nations.
- The influence of political changes on domain names is also mentioned, with one user raising the example of Yugoslavia and the fate of its TLD after its breakup.

Overall, the discussion explores different angles related to domain name extensions, their control, and their impact on countries and economies.

### AI Companies and Advocates Are Becoming More Cult-Like

#### [Submission URL](https://www.rollingstone.com/culture/culture-features/ai-companies-advocates-cult-1234954528/) | 43 points | by [legrande](https://news.ycombinator.com/user?id=legrande) | [37 comments](https://news.ycombinator.com/item?id=39194435)

The Rabbit R1, an AI gadget showcased at the Consumer Electronics Show, has sparked concerns about the dangers of relying too heavily on AI personal assistants. The Rabbit R1 claims to be able to create a "digital twin" of the user that can directly use all of their apps. While similar voice-activated products like Amazon Alexa already exist, the Rabbit's ability to access and utilize personal apps raises questions about data security and privacy. Despite these concerns, the Rabbit R1 sold out its first 10,000 preorder units at CES. This growing trend of relying on AI assistants has drawn comparisons to cult dynamics, as users willingly surrender their agency and decision-making power to these devices. It remains to be seen how this reliance on AI will continue to evolve and impact society.

The discussion on this submission covers a range of topics related to AI and its implications. Some users express skepticism about the capabilities of AI, suggesting that AI systems like chatbots are not capable of understanding complex human behavior and should not be seen as remarkable. Others discuss the potential dangers of AI and the need for regulation in the field. There is also a discussion about whether AGI (Artificial General Intelligence) technology is capable of exponential improvement or if it has inherent limits. Some users argue that AI has the potential to save lives and address important societal issues, while others caution against the risks and potential ethical concerns. Overall, the discussion reflects differing opinions on the impact and future of AI technology.

### The Vision Pro

#### [Submission URL](https://daringfireball.net/2024/01/the_vision_pro) | 26 points | by [coloneltcb](https://news.ycombinator.com/user?id=coloneltcb) | [8 comments](https://news.ycombinator.com/item?id=39195112)

Apple has been testing a new product called Apple Vision Pro, which combines a VR/AR headset, a spatial computing productivity platform, and a breakthrough personal entertainment device. The Vision Pro comes in a large retail box and includes the headset, a battery, charger, cables, and accessories. To turn it on, you connect the external battery pack's power cable and rotate it to lock it in place. There are small LED indicators and a pleasant welcoming sound when it's ready to use. The headset is worn with a Solo Knit Band, which can be tightened or loosened using a dial behind the right ear. The software guides users through the calibration process for eye tracking, and the onboarding process is smooth, allowing users to transfer Apple ID credentials and Wi-Fi passwords from their iPhone or iPad. However, there are some challenges with the Vision Pro hardware. First, it requires an external battery pack connected via a power cable, which is heavy and adds bulk. Second, the headset itself is heavy and can cause fatigue. Additionally, it's quite large and noticeable when worn. Despite these issues, the headset's fit and comfort can be adjusted, and users can choose between different bands.

The discussion on Hacker News revolves around the Apple Vision Pro, a new product that combines a VR/AR headset with a spatial computing productivity platform and an entertainment device. One user, gnchls, mentions that the Vision Pro requires an external battery pack, which adds bulk and is inconvenient. They also note that the headset itself is heavy and can cause fatigue. Another user, Kluggy, adds that Spigen announced a $99 carrying case for the Vision Pro. 

Gnchls further expresses skepticism about the Vision Pro, comparing it to Apple's previous product generations and suggesting that the device may not be worth the high price. Judge2020 responds by highlighting Apple's tremendous market value and revenue, indicating that the Vision Pro could potentially be a successful product. Smnbrnzz adds that Apple's GDP revenue is nearly 5% of the global GDP, emphasizing the company's significance.

MichaelZuo notes that while the Vision Pro could be a perfect personal entertainment device, it may compromise comfort due to its weight and size. They suggest that Apple might resolve these issues in future iterations. Finally, rdx flags a comment by dknflsk, stating that the summary removed positive aspects and only included negatives.

Overall, the discussion covers various aspects of the Vision Pro, including its design, potential success, and room for improvement.

---

## AI Submissions for Mon Jan 29 2024 {{ 'date': '2024-01-29T17:10:36.490Z' }}

### A Tinkertoy computer that plays tic-tac-toe (1989)

#### [Submission URL](https://web.archive.org/web/20070110215459/http://www.rci.rutgers.edu:80/~cfs/472_html/Intro/TinkertoyComputer/TinkerToy.html) | 34 points | by [anschwa](https://news.ycombinator.com/user?id=anschwa) | [10 comments](https://news.ycombinator.com/item?id=39176705)

A group of MIT students have built a computer entirely out of Tinkertoys that can play tic-tac-toe. The computer uses a read head to scan through 48 rows of Tinkertoy "memory spindles" to determine its next move. Each spindle represents a combination of X's and O's that could arise during the game. The computer is operated by a human who cranks the read head and adjusts the core piece inside it to register the opponent's moves. When the computer finds a memory that matches the current state of the game, it indicates its move. The students who built this Tinkertoy computer have since graduated, with one of them, Daniel Hillis, going on to found Thinking Machines, Inc., which produces the Connection Machine. The universality of computation in Tinkertoys is a fascinating concept, as Marvin Minsky noted in the preface to LogoWorks.

The discussion surrounding the submission includes several comments that provide additional information and context related to the concept of building computers out of Tinkertoys. 

- One user mentions that in 1982, a French magazine called Science Vie published a cardboard computer simulator named Ordinapoche, designed by Joel de Rosnay. They provide a link to the French version of the article.
- Another user notes that traditional toy computers made out of cardboard existed prior to the Tinkertoy computer, such as the Little Man Computer in 1965 and the CARDIAC (CARDboard Illustrative Aid to Computation) in 1968. They provide links to more information about these toy computers.
- A user shares a link to a collection by Computer History Museum, featuring Tinker Toy Logic circuits made out of TTL (Transistor-Transistor Logic), indicating that more advanced versions of Tinkertoy computers were built using different materials.
- One user mentions that an article titled "Tinker Toy Computer" by AK Dewdney was featured in the "Computer Recreations" column, which was followed by Martin Gardner's "Mathematical Recreations" column in the same magazine. The user suggests that Dewdney's columns were later compiled into books.
- A user provides a link to an article about MENACE (Matchbox Educable Noughts and Crosses Engine) from the 1960s, which was another example of a toy computer that played noughts and crosses. Another user adds that MENACE was able to learn how to play tic-tac-toe after being given precomputed positions.

Overall, the discussion expands on the concept of building toy computers and provides additional resources and examples related to Tinkertoy computers and other similar projects.

### Ingenuity had more computing power than all NASA deep space missions combined

#### [Submission URL](https://arstechnica.com/space/2024/01/now-that-weve-flown-on-mars-what-comes-next-in-aerial-planetary-exploration/) | 102 points | by [rntn](https://news.ycombinator.com/user?id=rntn) | [59 comments](https://news.ycombinator.com/item?id=39175423)

NASA's Ingenuity helicopter has made history with its groundbreaking flights on Mars, marking the first time powered flight has been achieved on another planet. The success of Ingenuity's flights demonstrates that aerial mobility is possible on celestial bodies beyond Earth, which will revolutionize exploration efforts in the future. In addition to the monumental feat of powered flight, Ingenuity utilized off-the-shelf commercial parts due to the demanding conditions on Mars. By doing so, the mission has shown that using readily available components can yield astonishing results and may influence the design and implementation of future NASA missions. The impact of Ingenuity's achievements is expected to be similar to the transformative effect aviation had on human endeavors and exploration on Earth. NASA scientists envision a future where helicopters or similar aerial vehicles explore previously inaccessible areas on Mars, such as the canyons of Valles Marineris, opening up boundless possibilities for scientific discovery.

The discussion on Hacker News surrounding NASA's Ingenuity helicopter's historic flights on Mars covers various aspects of the mission. Some commenters discuss the use of off-the-shelf components and how readily available parts can yield impressive results in space exploration. Others mention the importance of programming skills in space missions, highlighting the need for a balance between scientific and programming expertise. There are also discussions about the reliability of commercial components in space environments and the challenges of radiation hardening. Furthermore, there are comments about the efficiency of modern programming and the waste of resources in certain applications. Lastly, some commenters discuss the redundancy and reliability of spacecraft control systems, comparing them to the impressive calculations made by previous space missions.

### Show HN: WhisperFusion – Low-latency conversations with an AI chatbot

#### [Submission URL](https://github.com/collabora/WhisperFusion) | 264 points | by [mfilion](https://news.ycombinator.com/user?id=mfilion) | [101 comments](https://news.ycombinator.com/item?id=39176570)

Collabora has released WhisperFusion, a new tool that enhances the capabilities of WhisperLive and WhisperSpeech to enable seamless conversations with an AI. With 625 stars and 27 forks on GitHub, WhisperFusion aims to provide a smooth and immersive experience for interacting with artificial intelligence. The tool builds upon the existing Whisper technologies and offers improved conversational functionality. Collabora invites developers to explore and contribute to the WhisperFusion project on GitHub.

The discussion on this submission covers various aspects of the WhisperFusion project. Some users discuss the issues related to interruptions during conversations and how existing AI speech recognition services handle this problem. Others mention the limitations of current implementations and their experiences with different AI assistants. There is also a discussion about the challenges and trade-offs in implementing low-latency high-quality voice chat. Some users discuss the benefits and drawbacks of pre-programmed phrases in conversation systems. The conversation also touches upon topics like the importance of context in conversation systems, the relationship between latency and word error rate, and different approaches to streaming speech recognition. The discussion concludes with users expressing interest in the project and discussing packaging and distribution issues with Python applications and TensorRT.

### Streetview scraper v1: cheap, arbitrary sized streetview images

#### [Submission URL](https://loichovon.com/posts/streetview-scraper.html) | 66 points | by [efishnc](https://news.ycombinator.com/user?id=efishnc) | [8 comments](https://news.ycombinator.com/item?id=39175670)

Loic Hovon, a developer, has shared a project on Hacker News that allows for scraping larger and cheaper Street View images from the Google Maps JavaScript API. Despite acknowledging that this project goes against the Maps Platform's Terms of Service and may result in a ban or retribution, Hovon believes it is fair game for academic research purposes. The project provides the ability to gather multiple angles and different time periods for a location, as well as obtain arbitrarily sized images without watermarks. Hovon also includes the code and explains the process of using Selenium to load the JavaScript API and scrape the images. However, it's worth noting that this method may be slower than using the Static API and requires waiting between screenshots. Overall, the project offers a more cost-effective solution for collecting a streetview image dataset.

The discussion on Hacker News regarding Loic Hovon's project to scrape larger and cheaper Street View images from the Google Maps JavaScript API had a few different points of view. 
One user, "gnvl," expressed concern about the potential consequences of using unauthorized methods and violating Google's Terms of Service. They mentioned that Google can be very strict when it comes to banning users and imposing penalties.
Another user, "xnx," was surprised that scraping Street View images could be done through the API. They mentioned that in the past, people had to resort to using browser automation and taking screenshots to collect street view images.
In response, "KolmogorovComp" pointed out that using Selenium and injecting additional JavaScript code into the page could achieve the same result as the API. They provided a link to a blog post explaining version two of the scraper, which involves injecting additional JavaScript code.
"Xnx" admitted that they did not understand the technique described and asked for further clarification. Another user, "fshnc," chimed in and shared the fact that clicking through street views in a browser and taking screenshots had been done before in tools like Streetview Hyperlapse back in 2013.
A user named "dvt" highlighted the issue of cost when scraping Street View images from the Google Maps API. They mentioned that constantly scraping the API for a long period of time could result in being throttled or needing multiple IP addresses to bypass limitations.
In response, "mplws" mentioned that there are traditional scrapers specifically designed for the Panorama API, which can quickly scrape Google's property while also dealing with reCAPTCHAs.

Overall, the discussion included concerns about violating terms of service, surprise at the scraping methods, and considerations regarding cost and potential obstacles when using these techniques.