import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Sat Jan 03 2026 {{ 'date': '2026-01-03T17:08:16.502Z' }}

### Scaling Latent Reasoning via Looped Language Models

#### [Submission URL](https://arxiv.org/abs/2510.25741) | 78 points | by [remexre](https://news.ycombinator.com/user?id=remexre) | [13 comments](https://news.ycombinator.com/item?id=46481849)

TL;DR: A new open-source family of “Looped Language Models” (LoopLM), called Ouro, bakes multi-step reasoning into pretraining by letting the model iterate in latent space with a learned, dynamic depth. Small models (1.4B/2.6B) reportedly match the reasoning performance of state-of-the-art models up to 12B params, trained on 7.7T tokens.

What’s new
- Pretraining for reasoning, not just post-training prompts: Instead of relying on chain-of-thought at inference, Ouro trains models to perform iterative computation internally during pretraining.
- Latent loops + learned depth: The model “thinks” via internal loops in its hidden states, with an entropy-regularized objective that encourages it to allocate just enough steps per input.
- Scales well at small sizes: Ouro 1.4B and 2.6B are claimed to match much larger 12B models across diverse benchmarks.

Why it matters
- Smaller, smarter models: If latent looping reliably boosts reasoning, you can get big-model reasoning on smaller footprints—promising for cost, latency, and edge deployments.
- Beyond verbose CoT: Internal loops could reduce dependence on long chain-of-thought outputs (fewer tokens, less leakage), while keeping or improving reasoning quality.
- Manipulation > memory: Authors argue gains come from better “knowledge manipulation” rather than just more parameters or data memorization.

How it works (at a glance)
- Iterative hidden-state updates: The network applies multiple internal reasoning steps before emitting tokens.
- Dynamic depth via entropy regularization: A training objective that nudges the model to adaptively decide how many internal steps to take.
- Massive pretraining: Trained on 7.7T tokens to make the looped computation robust and general.

Notable claims
- 1.4B/2.6B Ouro models match up to 12B SOTA LLMs on a wide range of reasoning benchmarks.
- Reasoning traces are more aligned with final answers than typical chain-of-thought outputs.
- Controlled experiments suggest improvements come from how the model uses knowledge, not just how much it stores.

Caveats and open questions
- Inference cost/latency: Latent loops don’t emit tokens, but they still add compute—what’s the real-world speed/cost trade-off?
- Generality and robustness: How widely do the gains hold across domains and languages not in the benchmark set?
- Practical integration: Tool use, retrieval, and guardrails with looped inference remain to be validated at scale.

Availability
- The authors say models are open-sourced and provide a project page; details and weights are reportedly available. Authors include Yoshua Bengio and collaborators.

**Hacker News Discussion Summary**

The discussion focused on the architectural mechanics of Ouro and the safety implications of its opaque reasoning process.

*   **comparisons to ODEs and Universal Transformers:** Commenters drew strong parallels between Ouro and "Universal Transformers" or Neural ODEs (Ordinary Differential Equations), effectively describing the model as a solver that iterates in latent space. There was a technical debate regarding "flow-matching" in language models; users clarified that while language inputs and outputs are discrete tokens, the internal operations (and thus the looping) occur in a continuous multi-dimensional vector space, allowing for smooth interpolation.
*   **The "Black Box" Safety Concern:** A significant portion of the thread debated the interpretability of "latent loops." Unlike standard Chain-of-Thought (CoT), which produces human-readable reasoning steps, Ouro's internal steps are abstract vector manipulations not mapped to the vocabulary. Users compared this to "Coconut" models (Continuous Chain of Thought), noting that while this method is computationally efficient, it poses a safety risk because the "thought process" is illegible to humans and harder to monitor or guardrail.
*   **Visualizing the Architecture:** Participants used pseudo-code to illustrate the difference between Ouro and standard LLMs. While traditional models pass data through a fixed stack of distinct layers (Layer 1 $\to$ Layer 2 $\to$ ...), Ouro was described as looping through the *same* layer structure iteratively. It was noted that this depth is dynamic: the model runs more loops for difficult tokens and fewer for easy ones before outputting a result.

### Recursive Language Models

#### [Submission URL](https://arxiv.org/abs/2512.24601) | 147 points | by [schmuhblaster](https://news.ycombinator.com/user?id=schmuhblaster) | [23 comments](https://news.ycombinator.com/item?id=46475395)

Recursive Language Models: pushing LLMs past context limits by letting them call themselves

- What’s new: Alex L. Zhang, Tim Kraska, and Omar Khattab propose “Recursive Language Models” (RLMs), an inference-time strategy where the LLM treats a long prompt as an external environment, programmatically scans/decomposes it, and recursively calls itself on relevant snippets.
- Why it matters: This aims to break fixed context windows without retraining. The authors report handling inputs up to two orders of magnitude longer than the model’s context and, even on shorter prompts, outperforming base LLMs and common long‑context scaffolds across four diverse tasks—at comparable or lower per‑query cost.
- How it works (high level): The model acts as a controller that decides what to read next, how to chunk, and when to recurse—an instance of “inference‑time scaling” where more compute and structure at inference improve quality.
- For builders: If validated, this could offer a simpler alternative to bespoke long‑context pipelines, with potential gains in quality and cost. Open questions include latency/compute trade‑offs, robustness of the controller loop, and failure modes on messy real‑world corpora.

Paper: “Recursive Language Models” (9 pages + 24pp appendix)
arXiv: 2512.24601 (cs.AI, cs.CL) — authors’ claims based on four long‑context tasks
PDF: https://arxiv.org/pdf/2512.24601

Here is a summary of the discussion:

**Is this just Agents/RAG by another name?**
A significant portion of the discussion focused on terminology and classification. Several users argued that "Recursive Language Models" is mostly a rebrand of existing "subagent" architectures or "agentic scaffolds" (like BabyAGI or workflows used in Cursor and Claude Code).
*   **Recursion vs. Depth:** Commenters noted that if the system only goes one level deep (Main -> Subagent), as some suggested the paper implies, calling it "recursive" is a stretch; it is effectively just a subagent workflow.
*   **Task vs. Context Decomposition:** User *wsbdnr* offered a nuanced distinction: while standard agentic workflows usually view multiple calls as *task* decomposition, this paper frames it as *context* decomposition—treating the text as an environment to be navigated.

**RAG vs. RLM**
Users debated how this differs from standard Retrieval-Augmented Generation (RAG).
*   **The "Auto-RAG" Shift:** *bob1029* and *NitpickLawyer* identified the key innovation: in standard RAG, a human developer hard-codes the retrieval logic (chunking, embedding, searching). In RLM, the LLM itself acts as the controller, dynamically deciding what to read, search, or "page in" from the text-as-environment.
*   **Environment:** The consensus was that RLM treats long prompts as an external environment for the model to interact with symbolically, rather than just a stream of tokens to digest.

**Implementation & Training**
*   **Inference, not Weights:** Users clarified for those misled by the title that this is purely an inference-time strategy (scaffolding/prompting) and does not involve training new model weights or differentiable architectures.
*   **Tooling Wishlist:** The discussion touched on the desire for major providers (OpenAI, Anthropic) to expose these types of "computation hooks" in their APIs, allowing developers to inspect or swap the context-management logic (like that used in Claude Code) rather than interacting with opaque black boxes.

---

## AI Submissions for Fri Jan 02 2026 {{ 'date': '2026-01-02T17:08:25.564Z' }}

### TinyTinyTPU: 2×2 systolic-array TPU-style matrix-multiply unit deployed on FPGA

#### [Submission URL](https://github.com/Alanma23/tinytinyTPU-co) | 122 points | by [Xenograph](https://news.ycombinator.com/user?id=Xenograph) | [50 comments](https://news.ycombinator.com/item?id=46468237)

TinyTinyTPU: a bite-size, working TPU you can simulate and run on a Basys3 FPGA. It implements a full TPU-style pipeline around a 2×2 systolic array, making TPU internals tangible for learning and experimentation.

Highlights
- End-to-end design: 2×2 systolic array (4 PEs) plus accumulator, activation (ReLU), normalization, and quantization stages
- Works today on a low-cost Basys3 (Artix-7) board via a simple UART host interface and Python driver
- Multi-layer MLP inference with double-buffered activations; includes demos (e.g., a mouse-gesture classifier)
- Thorough test suite with cocotb + Verilator and optional waveforms; module and top-level coverage
- Open-source flow supported (Yosys + nextpnr) in addition to Xilinx Vivado

Architecture notes
- Systolic dataflow: activations move horizontally; partial sums vertically
- Diagonal wavefront weight loading to align systolic timing
- Weight FIFO → MMU → Accumulator → Activation → Normalization → Quantization pipeline
- UART protocol for commands/results; 115200 8N1

Resource footprint on Basys3 (XC7A35T)
- ~1k LUTs (≈5%), ~1k FFs (≈3%), 8 DSP48E1 slices, 10–15 BRAMs, ~25k gates
- 100 MHz clock; reset on BTNC; RX/TX on B18/A18

Developer experience
- Sim: Verilator 5.x, cocotb, GTKWave/Surfer; make targets for unit and integration tests with waveforms
- FPGA: Vivado or Yosys/nextpnr build; Python host scripts for loading weights/activations and reading results
- Clear, modular repo with DEBUGGING_GUIDE and per-module tests (PE, MMU, accumulator, activation pipeline, UART, full system)

Why it’s interesting
- A minimal yet complete TPU you can read, simulate, and tinker with—ideal for understanding systolic arrays, post-MAC pipelines, and hardware-software co-design
- Demonstrates how a TPU scales: this 2×2 version is educational; the same concepts underpin larger arrays like TPU v1’s 256×256

Try it
- Run all sims from sim/ with make test (WAVES=1 for traces)
- Flash to Basys3 and use the provided Python driver to push weights/activations and execute inference
- Optional gesture demo trains a 2-layer MLP and performs real-time classification on the FPGA

While the submission focuses on an educational TPU implementation, the discussion broadens into a debate on the future of AI hardware, specifically comparing FPGAs, GPUs, and ASICs in the context of large-scale inference.

**The Evolution of AI Hardware**
*   **The Crypto Analogy:** User **mrntrwb** likens the trajectory of AI inference to Bitcoin mining: moving from CPUs to GPUs, briefly to FPGAs, and finally to ASICs. They predict that GPU-based inference will soon become obsolete due to inefficiency compared to purpose-built chips (like Google's TPU or Groq).
*   **The Counter-Argument:** Others, including **fblstr** and **ssvrk**, argue that modern Data Center GPUs are already effectively ASICs given the amount of die area dedicated to fixed-function matrix multiplication (Tensor Cores) rather than graphics. **NitpickLawyer** notes that high-end accelerators are much closer to ASICs than traditional video cards.

**FPGAs vs. GPUs for Inference**
*   **Performance Claims:** A heated debate emerged regarding whether FPGAs can compete with top-tier GPUs (H200/B200). User **dntcs** claims to have worked on FPGA systems that outperform H200s on Llama3-class models, largely by bypassing memory bottlenecks.
*   **Skepticism:** **fblstr** challenges this, noting that while memory bandwidth is the bottleneck, the sheer compute density (PetaOPS) of chips like the Blackwell B200 is difficult for general-purpose FPGA fabric to match.
*   **Bandwidth is King:** Multiple users (**tcnk**, **bee_rider**) agree that the real constraint for inference is memory fabric and bandwidth. **tcnk** highlights modern platforms like the Alveo V80 with PCIe 5.0 and 200G NICs as the current state-of-the-art for programmable in-network compute.

**Market Dynamics**
*   **Hyperscaler Custom Silicon:** The discussion notes that major tech companies (Google, Amazon, Meta, Microsoft) effectively already use custom silicon (TPUs, Inferentia, Maia) for their internal workloads, reducing reliance on Nvidia for inference.
*   **Edge Hardware:** **Narew** and **mffklst** briefly discuss older "stick" format TPUs (Google Coral, Intel compute sticks), noting they are now dated and struggle to compete with low-power GPU/SOC options like Jetson.

**Other Technical Notes**
*   **0-_-0** and **hnkly** drew parallels between neural networks and CPU branch predictors, discussing the potential for AI to handle heuristic tasks (like speculative execution) to skip expensive deterministic computations.
*   **zhm** clarified that while TPUs are often associated with Transformers, architectures like the TPUv5 (Ironwood) were designed specifically for efficient LLM training, whereas other chips (like Etched's Sohu) are true Transformer-specific ASICs.

### AB316: No AI Scapegoating Allowed

#### [Submission URL](https://shub.club/writings/2026/january/ab316/) | 36 points | by [forthwall](https://news.ycombinator.com/user?id=forthwall) | [19 comments](https://news.ycombinator.com/item?id=46461347)

California’s AB316, as described by the poster, adds Civil Code 1714.46 and bars “the AI did it” as a liability defense: if an AI system causes harm, developers or users can’t claim autonomy as a shield. The law broadly defines AI as systems that infer from inputs to generate outputs affecting physical or virtual environments.

The author (not a lawyer) thinks this is reasonable but vague, and raises thorny questions about who’s on the hook when things go wrong:
- Where does liability sit between model makers (e.g., OpenAI), app builders, and deployers?
- How does this play with open-source models used in critical contexts (e.g., an aircraft system)?
- Will claims hinge on marketing representations or integration choices?

Expected knock-on effects: more investment in guardrails and safety layers, tighter operational controls, stronger contracts and indemnities, and a budding market for AI liability insurance. The takeaway: unpredictability won’t excuse harm; if your system can cause damage—like a chatbot giving dangerous advice—you’re responsible for preventing it.

**Discussion Summary:**

Commenters grappled with the boundaries of liability, using analogies ranging from food safety to science fiction to explore whether unpredictability should absolve developers of blame.

*   **The "Eggshell Skull" Doctrine:** The discussion opened with a grim hypothetical: if a chatbot encourages a user to commit suicide, is the developer liable? While some users felt a bot shouldn't be held to the same standard as a human, others cited the "eggshell skull" legal rule. This doctrine suggests a defendant is liable for the resulting harm even if the victim had a pre-existing vulnerability (like suicidal ideation), implying developers cannot use a user's mental state to shield themselves from the consequences of a bot's "persuasive" errors.
*   **The Zoo Analogy:** One user reframed the AB316 logic using a zoo comparison. The law essentially states that "the AI is a wild animal" is not a valid defense. Just as a zoo is responsible for containment regardless of a tiger's natural instincts, an AI deployer is responsible for the system's output, regardless of its inherent unpredictability.
*   **Product Liability Parallels:** Participants drew comparisons to the Jack in the Box E. coli outbreaks and faulty car parts. The consensus leaned toward treating AI as a commercial product: if a company sells "sausages made from unsanitary sources" (or a model trained on toxic data), they face strict liability for the outcomes.
*   **Redundancy vs. Clarity:** A debate emerged over whether this law is redundant, given that product liability laws already exist. However, proponents argued the legislation is necessary to specifically close the "autonomy loophole," preventing defendants from claiming a system's "black box" nature puts its actions outside their legal control.
*   **The "Catbox" Sophistry:** In a philosophical turn, a user cited the "Schrödinger's Catbox" from the novel *Endymion*—a device where a death is triggered by random radioactive decay, purportedly absolving the user of murder. The commenter argued that corporate reliance on AI stochasticity is a similar moral sophistry, attempting to use randomness to dilute ethical responsibility.

### Everyone's Watching Stocks. The Real Bubble Is AI Debt

#### [Submission URL](https://www.bloomberg.com/news/newsletters/2025-12-31/everyone-s-watching-stocks-the-real-bubble-is-ai-debt) | 48 points | by [zerosizedweasle](https://news.ycombinator.com/user?id=zerosizedweasle) | [27 comments](https://news.ycombinator.com/item?id=46468579)

Howard Marks flags rising leverage behind the AI boom as a late‑cycle warning sign

- The Oaktree co-founder says the AI trade has shifted from being funded by Big Tech’s cash piles to being increasingly financed with debt, a change he finds worrisome.
- He argues the AI rally looks further along than earlier in the year, with growing leverage a classic sign of maturing (and potentially bubbly) markets.
- Why it matters: Debt magnifies both gains and losses. If AI-driven revenues don’t arrive fast enough to cover swelling capex and financing costs, the pain could spread from equity to credit markets.
- What to watch: Hyperscalers’ capex and borrowing trends, off-balance-sheet commitments (long-term purchase and leasing deals), and credit spreads tied to the AI supply chain and data-center buildout.
- Context: Marks’ latest memo (“Is It a Bubble?”) doesn’t call a top outright but underscores that the risk profile of the AI trade has changed as leverage enters the picture.

**Daily Digest: Hacker News Discussion**

**Investment Strategy Amidst "Doom" Signals**
The thread opened with users questioning where to allocate capital given the economic warnings (bubbles, debt, and inflation). Responses ranged from adhering to standard long-term strategies (such as Vanguard's 80/20 split) to fleeing to safety. While some advocated for holding cash to avoid potential market crashes of 15%+, others argued that cash is a poor hedge during inflationary periods driven by potential government "money printing." There was also a brief, contentious suggestion to pivot toward specific foreign indices (like Spain) or gold as safety plays.

**Historical Parallels and Timing**
Highlighting the difficulty of acting on macro warnings, one commenter pointed to the Dot-com era: Alan Greenspan famously warned of "irrational exuberance" in 1996, yet the bubble did not burst for several more years. The consensus suggested that while valuations may be unsupported, timing the exact top remains notorious difficult.

**Validating the Leverage Shift**
Validating the article's core thesis, a user shared their own analysis of Big Tech balance sheets (specifically Meta, Microsoft, and Amazon). They noted a distinct shift starting around the release of ChatGPT in late 2022: these previously cash-rich "fangs" have significantly increased their debt loads to finance the AI buildout, a fundamental change in risk profile that led the user to exit a 10-year position in the sector.

---

## AI Submissions for Thu Jan 01 2026 {{ 'date': '2026-01-01T17:09:10.570Z' }}

### Build a Deep Learning Library

#### [Submission URL](https://zekcrates.quarto.pub/deep-learning-library/) | 120 points | by [butanyways](https://news.ycombinator.com/user?id=butanyways) | [15 comments](https://news.ycombinator.com/item?id=46454587)

Build a Deep Learning Library from Scratch (free online book)

- Learn by building: start with a blank file and NumPy, implement your own autograd engine and a small suite of layer modules.
- By the end, you’ll train models on MNIST, plus a simple CNN and a simple ResNet—gaining a clear, under‑the‑hood understanding of modern DL stacks.
- Free to read online; pay‑what‑you‑want support via Gumroad. Questions/feedback: zekcrates@proton.me.

**Discussion Summary:**

*   **Community Implementations:** The comment section became a showcase for "learning by doing," with multiple users sharing their own custom ML libraries built from scratch in both Python and C++. One notable mention included a user who successfully replicated GPT-2 using their own NumPy-based library.
*   **Comparison to Karpathy:** When asked how this compares to Andrej Karpathy’s "Zero to Hero" series, the author explained that while Karpathy’s *Micrograd* operates on scalars, this book focuses on tensors. However, the author still recommended Karpathy’s videos as a complementary resource.
*   **Depth of Abstraction:** Some technical discussion arose regarding the use of NumPy. Critics argued that relying on NumPy hides the implementation details of tensors themselves; the author agreed, suggesting a future C++ backend might address this, while others expressed interest in a "build NumPy from scratch" guide to cover that gap.

### Building an internal agent: Code-driven vs. LLM-driven workflows

#### [Submission URL](https://lethain.com/agents-coordinators/) | 67 points | by [pavel_lishin](https://news.ycombinator.com/user?id=pavel_lishin) | [31 comments](https://news.ycombinator.com/item?id=46456682)

TL;DR: LLMs plus tools can automate complex internal workflows, but tiny error rates can break trust. Imprint now treats LLM orchestration as the fast default and “graduates” important workflows to deterministic, code-driven coordinators.

The story:
- Problem: An LLM agent auto-tagged Slack messages with :merged: by parsing PR links and checking GitHub. It worked great—until it occasionally marked unmerged PRs as merged, causing real risk (people stopped looking).
- Lesson: Determinism matters. A 1% error on internal ops can erase 99% of the value.

What they built:
- Two coordinators in the same framework:
  - coordinator: llm (default) — LLM selects and sequences tools; handler enforces limits and termination.
  - coordinator: script — a checked-in Python script gets the same tools, triggers, and “virtual files” (Slack/Jira attachments). It can optionally invoke a subagent LLM for specific steps, but control is explicit and reviewable.
- Engineers can one-shot convert working prompts into code (via Claude Code), preserving behavior while gaining reliability, speed, and code review.

Why it matters:
- Hybrid pattern: start with LLMs for exploration and simple cases; promote to code when you need guarantees on correctness, latency, or cost.
- Even as models improve, use LLMs narrowly for truly intelligent decisions; handle iterative, stateful orchestration with deterministic software.
- Framing: “progressive enhancement” for agents—LLM when sufficient, code when necessary.

**Determinism, Evals, and the "Script vs. Agent" Debate**

The discussion around Will Larson's post focused heavily on the engineering trade-offs between probabilistic LLMs and deterministic code for internal tooling.

*   **Skepticism on LLM Necessity:** Many commenters questioned the premise of using an LLM for tasks with established APIs (like checking GitHub PR status). The consensus among skeptics was that if a deterministic API exists, wrapping it in an LLM introduces unnecessary cost, latency, and "judgment" errors into what should be a binary operation.
*   **The Struggle with Non-Determinism:** A significant portion of the thread debated why LLMs cannot be perfectly deterministic even with `temperature=0`. Technical explanations cited floating-point non-determinism in GPUs and batching variances. This unpredictability frustrates iterative development, where small changes to a "spec" (prompt) can result in a completely rewritten, broken solution rather than a slight modification.
*   **"Evals" are the New Unit Tests:** To mitigate hallucinations, commenters suggested treating LLM outputs as untrusted candidates that must pass rigid, deterministic tests (Evals). For example, rather than trusting the LLM to check a PR, the LLM should write the code to check the PR, which is then verified against a mock environment.
*   **Defining the Boundary:** Users argued that the "Code vs. LLM" framing is slightly misleading. The emerging best practice is using LLMs for *intent understanding* and handling unexpected states (e.g., messy HTML, vague user requests), while handing off the *mechanical execution* to standard code.
*   **Security Risks:** A smaller sub-thread highlighted the danger of "prompt injection" in internal agents, noting that an agent with broad permissions (reading Slack, checking GitHub) is a high-value target for malicious input manipulation.

### Hierarchical Navigable Small World (HNSW) in PHP

#### [Submission URL](https://centamori.com/index.php?slug=hierarchical-navigable-small-world-hnsw-php&lang=en) | 91 points | by [centamiv](https://news.ycombinator.com/user?id=centamiv) | [15 comments](https://news.ycombinator.com/item?id=46454968)

HNSW in PHP: fast vector search without scanning everything

- The problem: Doing cosine similarity against every vector is linear-time and crawls at scale (think scanning 10M docs one by one).
- The idea: Implement HNSW (Hierarchical Navigable Small World) in PHP—a layered graph where upper levels act like “highways” and lower levels like “side streets,” letting you zoom toward the target quickly.
- How it works:
  - Greedy descent from the highest layer to Level 1: at each layer, hop to neighbors that improve similarity until no improvement, then drop a layer.
  - Precision pass at Level 0: run a best-first search with a priority queue. The ef parameter controls the candidate set size (bigger ef = higher recall, slower). M controls max connections per node (bigger M = more memory, better recall).
- Implementation notes: Uses cosine similarity, SplPriorityQueue, and a winners list to track top results; part of an open-source PHP project called Vektor for native vector search.
- Why it matters: Brings ANN-style speedups to pure PHP—no external services—while exposing clear speed/accuracy/memory trade-offs via ef and M.

Takeaway: If you’re doing semantic search in PHP, HNSW gives you near-instant queries with tunable precision instead of O(N) scans.

Here is a summary of the discussion:

The author, **cntmv**, joined the comments to explain their motivation: they wanted to deeply understand HNSW mechanics without determining them via external libraries, noting that modern PHP (8.x/JIT) is surprisingly capable of handling this workload. They positioned the library as a "drop-in solution" for PHP monoliths to add semantic search without managing external services like Qdrant or Pinecone.

Key discussion points included:

*   **Practical Use Cases:** User **hu3** asked about the feasibility of indexing 1,000–10,000 Markdown or PHP files for an LLM agent. The author confirmed the library handles 1,000 documents with millisecond search times, though they advised careful chunking when parsing code snippets.
*   **Dependency Confusion:** User **Random09** pointed out that the examples seemed to require OpenAI. The author clarified that the library is completely model-agnostic (compatible with Ollama, HuggingFace, etc.) but agreed to update the README, as the current "Hello World" example relies on OpenAI for convenience.
*   **Educational Value:** Several users praised the blog post's clarity, highlighting the use of "fantasy-based examples" (comparing programming to magic incantations) and the implementation itself, which **fthsx** described as creating "executable pseudocode" that makes complex algorithms easier to understand than lower-level implementations.

### Cycling Game (Mini Neural Net Demo)

#### [Submission URL](https://www.doc.ic.ac.uk/~ajd/Cycling/) | 21 points | by [ungreased0675](https://news.ycombinator.com/user?id=ungreased0675) | [3 comments](https://news.ycombinator.com/item?id=46458029)

Cycling Neuroevolution is a slick, interactive demo where a pack of identical cyclists are controlled by tiny neural nets that evolve race-by-race to get faster over a 2 km course with randomized terrain. You can watch tactics emerge—push on climbs, recover on descents, drafting trains, late sprinters—as selection and small weight mutations hone their behavior.

Highlights
- How it works: Each rider’s neural net takes in speed, current power, battery level (W′), short- and long-horizon gradient, gap to the rider ahead, and race progress; it outputs a per-timestep change in power (scaled by a Power Multiplier). After each race, the top 5 riders are kept; the next generation mixes exact copies with mutants (σ=1.0, 20 weights mutated).
- Physics/physiology: 87 kg rider+bike, CwA 0.32, Cr 0.004; drafting cuts drag by up to ~40%. Aerobic threshold 250 W, W′ 15 kJ that drains above threshold and recovers below; max sprint 750 W, tapering with battery state.
- What you can do: Click a rider to inspect their controller; red/blue inputs show real-time positive/negative contributions. Press space to force evolution mid-race, ‘r’ to reset the top-5 view, reload for fresh terrain and new populations.

Why it’s neat
- It’s an approachable, visual primer on neuroevolution with interpretable signals and believable cycling dynamics, showing how simple controllers plus selection pressure can yield lifelike race strategy.

By Andrew Davison (Imperial College London, 2025) — @ajddavison for ideas/suggestions.

**Discussion Summary:**
Commenters scrutinized the neural network's inputs, appreciating how separate moving averages for gradients (100m vs. 1000m) allow agents to distinguish between short rollers and sustained climbs. There was also a technical debate regarding sprint behaviors: while one user suggested a dedicated "distance to finish" input to trigger end-game bursts, another argued that the existing race progress metric should sufficiently handle late-race strategy.

### Show HN: A local-first financial auditor using IBM Granite, MCP, and SQLite

#### [Submission URL](https://github.com/simplynd/expense-ai) | 19 points | by [simplynd](https://news.ycombinator.com/user?id=simplynd) | [3 comments](https://news.ycombinator.com/item?id=46450489)

expense-ai is an open-source, local-first “Senior Auditor” for personal finance that turns raw bank statements into verified insights using the Model Context Protocol. Running entirely on your machine via Ollama, it pairs two Granite models (8B for reasoning, 2B for vendor normalization) with a FastAPI backend, an MCP server exposing SQLite tools, and a React dashboard. The LLM orchestrates SQL-backed queries to guarantee mathematically correct totals, filters out internal transfers/credit card settlements, and cleans messy merchant strings into readable vendor names—all without sending data to the cloud.

Highlights:
- Privacy by default: all processing is local (Ollama + SQLite)
- Agentic architecture: LLM chooses deterministic MCP tools; SQL handles all math
- Smart hygiene: vendor normalization and internal transfer filtering
- Practical workflow: upload text-based PDFs (no OCR yet), review/categorize, add manual fixed/cash expenses, visualize trends, then ask the “Senior Auditor” for verified analyses
- Stack: React UI, FastAPI app API, FastMCP server, Granite 8B/2B via Ollama; uses Astral’s uv for Python deps

Getting started: pull granite3.3:8b and :2b in Ollama, run the MCP server and FastAPI via uv, then npm run dev for the UI. Limitations: no OCR, auto-categorization is future work. Repo: github.com/simplynd/expense-ai (early-stage; ~23 stars).

Here is a summary of the discussion:

The creator, **smplynd**, provided a technical breakdown of the architecture, explaining that they achieved "100% mathematical accuracy" by strictly using the LLM to generate SQL queries via the Model Context Protocol (MCP) rather than letting the model perform calculations directly. They highlighted that the Granite 8B and 2B models offered the best balance of speed and consistency for local hardware. The author also reflected on the development process, noting that while AI is a great co-pilot, it requires developer oversight for "plumbing" issues like strict API types and CORS configurations.

Other comments focused on trust and security:
*   **IntelliAvatar** inquired about the execution-time security of the MCP implementation, specifically how the tool handles validation and side-effects when accessing the filesystem or network.
*   **da_grift_shift** pushed back on the reliability of the system, suggesting that the "100% accuracy" claim—and perhaps the author's own generated comment text—demonstrates that AI outputs still require a "quick manual review" before being submitted.