import { SparkleIcon } from '@/components/SparkleIcon'
import { generateRssFeed } from '@/lib/generateRssFeed'

export async function getStaticProps() {
  if (process.env.NODE_ENV === 'production') {
    await generateRssFeed()
  }
  return { props: {} }
}

---

## AI Submissions for Mon Sep 18 2023 {{ 'date': '2023-09-18T17:10:15.039Z' }}

### Self-supervised learning: The dark matter of intelligence (2021)

#### [Submission URL](https://ai.meta.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/) | 160 points | by [reqo](https://news.ycombinator.com/user?id=reqo) | [18 comments](https://news.ycombinator.com/item?id=37558813)

The AI field has made great strides in developing AI systems that can learn from labeled data, but there is a limit to how far supervised learning can take us. Supervised learning is not sufficient for building more intelligent generalist models that can perform multiple tasks and acquire new skills without massive amounts of labeled data. To address this limitation, researchers believe that self-supervised learning (SSL) may hold the key to unlocking the "dark matter" of intelligence in AI systems.

SSL enables AI systems to learn from vast amounts of unlabeled data, allowing them to recognize and understand more subtle and less common representations of the world. SSL has already shown great success in natural language processing (NLP), with models like BERT and RoBERTa achieving higher performance than those solely trained in a supervised manner. Recent research projects, such as SEER, have demonstrated that SSL can also excel in computer vision tasks.

Self-supervised learning works by obtaining supervisory signals from the data itself, leveraging the underlying structure in the data. For example, in NLP, a system can hide part of a sentence and predict the hidden words from the remaining words. In computer vision, it can predict future frames in a video from the current ones. By using the structure of the data, self-supervised learning can make use of a variety of supervisory signals without relying on labeled data.

Self-supervised learning has had a significant impact on NLP, enabling models to be pretrained on large unlabeled text datasets and then fine-tuned for specific tasks. However, applying SSL to computer vision tasks is a relatively new frontier. Researchers are exploring energy-based models, joint embedding methods, and latent-variable architectures to further advance self-supervised learning and reasoning in AI systems.

By combining supervised learning with SSL, AI systems can develop a deeper, more nuanced understanding of the world. This can bring us closer to achieving human-level intelligence and enable AI systems to learn new skills without requiring massive amounts of labeled data for each task. Self-supervised learning holds great promise in the quest to unlock the dark matter of intelligence in AI.

The discussion on this submission revolves around various aspects of self-supervised learning (SSL) and its potential in advancing artificial intelligence (AI) systems. Some key points from the comments include:

- The success of SSL in natural language processing (NLP) is noted, with models like BERT and RoBERTa achieving high performance by leveraging large unlabeled text datasets.
- There is a mention of different techniques in SSL, such as SimCLR, BYOL, and masking-based models, and their application in NLP and computer vision tasks.
- The use of SSL in computer vision is considered a relatively new area of exploration.
- The importance of SSL in addressing the limitations of supervised learning and achieving a deeper understanding of the world is emphasized.
- LeCun's contrastive learning course materials are recommended as a resource for understanding SSL.
- There is a discussion on the concept of "dark knowledge" and how AI systems can benefit from accessing subtle and implicit information present in unlabeled data.
- The role of humans in solving arbitrary problems and the capabilities of AI systems in comparison are debated.
- A study exploring the philosophical aspects of dark matter intelligence is suggested as reading material.
- One commenter mentions feeling the presence of dark matter intelligence in the industry and its potential in resolving complex issues.

Additionally, one comment redirects readers to a related Twitter thread for more information.

### Data accidentally exposed by Microsoft AI researchers

#### [Submission URL](https://www.wiz.io/blog/38-terabytes-of-private-data-accidentally-exposed-by-microsoft-ai-researchers) | 699 points | by [deepersprout](https://news.ycombinator.com/user?id=deepersprout) | [218 comments](https://news.ycombinator.com/item?id=37556605)

In a recent mishap, Microsoft's AI research team accidentally exposed 38 terabytes of private data on GitHub. The exposed data includes a backup of two employees' workstations, containing secrets, private keys, passwords, and over 30,000 internal Microsoft Teams messages. The researchers shared their files using an Azure feature called SAS tokens, which allows for data sharing from Azure Storage accounts. However, in this case, the access level was not properly limited, resulting in the unintended exposure. This incident highlights the importance of proper management and monitoring of SAS tokens to avoid potential security risks.

The discussion on this submission covers a variety of topics related to the accidental exposure of Microsoft's private data. One user suggests that AI models should be serialized in a secure format to prevent malicious injection, while another user raises concerns about targeted attacks and the potential manipulation of training data. There is also discussion about the risks of dynamically typed languages and the importance of proper security measures. The conversation touches on topics such as log4j vulnerability, password security, encryption, and the use of programming languages. Some users advocate for stricter language typing, while others argue that programming language choice is not the main issue. There is also a brief discussion about non-encrypted PDFs and the comparison between Microsoft Office and LibreOffice. Overall, the discussion highlights the complexities and challenges of securely managing and protecting data in the context of AI research.

### GPT 3.5 vs. Llama 2 fine-tuning: A Comprehensive Comparison

#### [Submission URL](https://ragntune.com/blog/gpt3.5-vs-llama2-finetuning) | 46 points | by [samlhuillier](https://news.ycombinator.com/user?id=samlhuillier) | [12 comments](https://news.ycombinator.com/item?id=37560125)

In a recent post, the author shares their experiments comparing the fine-tuning performance of GPT 3.5 and Llama 2 in an SQL task and a functional representation task. They found that while GPT 3.5 performed slightly better on both datasets, the cost of training and deploying it was significantly higher. The author provides code and data for both tasks and explains that they wanted to explore the possibility of achieving comparable performance with manual fine-tuning at a lower cost. They used subsets of the Spider dataset and the Viggo functional representation dataset, which are known for teaching structured outputs rather than facts. The author also details the setup of their experiments, including the decision to use Code Llama 34B and Lora fine-tuning. They conclude that while fine-tuning GPT 3.5 may be suitable for initial validation or MVP work, models like Llama 2 might be more cost-effective for advanced tasks.

The discussion among Hacker News users on this post covers several topics related to the comparison between GPT 3.5 and Llama 2, as well as the considerations for cost and lifetime memberships with OpenAI. Here are the key points:

1. Some users express concerns about the cost of using Llama, particularly in comparison to GPT 3.5, suggesting that the lifetime memberships offered by OpenAI do not make sense considering the high ongoing costs of using the models.

2. Others comment on the practicality of relying on cloud computing and the theory behind it. They argue that it may not be the best approach for long-term projects, highlighting the importance of considering cost and scalability.

3. One user mentions that the terminology "functional representation dataset" is not well-defined, but they acknowledge the potential benefits of using structured propositional knowledge, citing examples like Viggo.

4. Another user expresses their struggles in finding good datasets for fine-tuning and asks for tips on creating sufficient datasets for specific use cases.

5. One user expresses interest in a similar comparison involving the RAG model and tasks related to it.

6. A user mentions that the notebook shared in the post demonstrates a reproducible evaluation process that correlates with general value and control evaluation.

Overall, the discussion revolves around the trade-offs between cost, performance, and the practicality of using different models for fine-tuning tasks. Some users express interest in alternative approaches and datasets for structured outputs and long-term projects.

### Those trying to pick AI winners should remember the dotcom days

#### [Submission URL](https://www.ft.com/content/82168156-006f-4d75-a4e9-0b6bdccef3b2) | 30 points | by [ent101](https://news.ycombinator.com/user?id=ent101) | [7 comments](https://news.ycombinator.com/item?id=37559105)

As AI continues to dominate conversations in the tech industry, it's crucial to remember the lessons learned from the dotcom era. The dotcom bubble burst in the early 2000s, leaving many startups and investors in shambles. This article highlights the importance of being cautious and realistic when evaluating AI winners, as history has shown that not every promising technology lives up to the hype.

he author emphasizes the need to exercise caution when assessing the potential winners in the AI space. Just like during the dotcom era, where everyone believed that the internet would revolutionize the world, there is now an overwhelming optimism around AI. However, it is crucial to separate the hype from reality and carefully evaluate each AI technology's actual capabilities and applications.

The article raises important questions for investors and entrepreneurs in the AI space. It reminds them to consider the scalability, practicality, and long-term sustainability of AI solutions before making any commitments. While AI holds immense potential, it's important not to get carried away by lofty promises and to remain realistic about the challenges and limitations that AI technologies face.

Overall, this article serves as a valuable reminder to analyze AI winners through a critical lens and to approach the AI landscape with the lessons learned from the dotcom bubble in mind. It encourages readers to seek a balance between optimism and caution and to make informed decisions when navigating the AI ecosystem.

### Stephen Fry says his voice stolen from Harry Potter audio books,replicated by AI

#### [Submission URL](https://fortune.com/2023/09/15/hollywood-strikes-stephen-fry-voice-copied-harry-potter-audiobooks-ai-deepfakes-sag-aftra-simon-pegg-brian-cox-matthew-mcconaughey/) | 26 points | by [thunderbong](https://news.ycombinator.com/user?id=thunderbong) | [23 comments](https://news.ycombinator.com/item?id=37554055)

Actor Stephen Fry has spoken out about the potential harm of AI in Hollywood, specifically regarding the use of AI to replicate actors' voices without their permission. Fry, who is a member of the actors' union SAG-AFTRA, mentioned his personal experience of having his identity digitally cloned and played a clip of an AI system mimicking his voice at the CogX Festival in London. He warned that AI technology is advancing rapidly and could soon produce deepfake videos that are just as convincing. Other actors, including Brian Cox and Simon Pegg, have also expressed concerns about AI in the film industry.

The discussion on Hacker News revolves around various aspects of AI replication of actors' voices and the potential implications. Some users express skepticism, comparing AI voice replication to long-standing celebrity impersonators and suggesting that legal theories might be able to cover this issue. Others discuss the technical aspects of AI voice cloning and mention Brian Blessed's distinctive voice. 

One user points out that AI recordings of coworkers in web meetings have been created, implying that the theft of voices is not a new issue. Another mentions the history of AI and its impact on various industries. 

The conversation also touches on the debate of whether AI can replace human creativity and whether AI-generated content can be considered art. One user references Walter Benjamin's 1935 philosophy and the implications of AI replication on artistic expression. 

There is a discussion about the commercial applications of AI voice cloning and how it could be used in large-scale projects. The post raises the question of whether actors' consent should be required to replace their voices in certain circumstances.

Some users argue that the focus should be on other more pressing global issues, such as climate change and humanitarian crises, rather than worrying about AI voice cloning. One user suggests that the discussion is radical and the focus should be shifted.

---

## AI Submissions for Sun Sep 17 2023 {{ 'date': '2023-09-17T17:10:53.971Z' }}

### Large Language Models for Compiler Optimization

#### [Submission URL](https://arxiv.org/abs/2309.07062) | 202 points | by [og_kalu](https://news.ycombinator.com/user?id=og_kalu) | [100 comments](https://news.ycombinator.com/item?id=37549216)

A recent paper on arXiv titled "Large Language Models for Compiler Optimization" explores the use of large language models for code optimization. The authors present a 7B-parameter transformer model trained to optimize LLVM assembly for code size. The model takes unoptimized assembly as input and outputs a list of compiler options to best optimize the program. During training, the model predicts instruction counts before and after optimization, as well as the optimized code itself, which significantly improves its performance. The model outperforms two state-of-the-art baselines that require thousands of compilations, achieving a 3.0% improvement in reducing instruction counts over the compiler. Additionally, the model demonstrates strong code reasoning abilities, generating compilable code 91% of the time and perfectly emulating the output of the compiler 70% of the time. This work showcases the potential of large language models in compiler optimization.

The discussion revolves around various aspects of the paper on large language models (LLMs) for compiler optimization. One comment notes that the paper does not discuss the importance of generated code semantics and suggests trying the approach on larger benchmarks. Another commenter highlights the misconception that LLMs directly model instructions, explaining that they instead generate passes for the compiler optimization. Another thread focuses on the challenges of achieving correctness in LLMs and the difficulty of measuring correctness. There is also a discussion about the potential of LLMs in addressing compilation optimization problems, but some express skepticism and suggest alternative approaches. The importance of both correctness and performance in compiler optimization is emphasized, as well as the need for further research in this area.

### Apple’s new Transformer-powered predictive text model

#### [Submission URL](https://jackcook.com/2023/09/08/predictive-text.html) | 495 points | by [nojito](https://news.ycombinator.com/user?id=nojito) | [241 comments](https://news.ycombinator.com/item?id=37541093)

Apple's upcoming iOS and macOS versions will feature a predictive text feature powered by a "Transformer language model." Despite Apple's focus on polish and perfection, this may be one of the first Transformer-based models they will ship. However, many details about the feature remain unclear. The feature suggests completions for individual words, occasionally suggesting multiple words when they are obvious. A Python script was used to snoop on AppleSpell activity and stream the most probable suggestions from the predictive text model. The model was located in a bundle file, which contains Espresso model files used during typing. Although the model couldn't be reverse-engineered, it is believed that the predictive text model is kept in this location. The vocabulary set for the model consists of 15,000 tokens, including special tokens, contractions, emojis, and a list of normal-looking tokens. The model's architecture appears to be GPT-2-based.

The discussion on this submission revolves around various aspects of Apple's predictive text feature powered by a "Transformer language model." Some users express surprise and disappointment that Apple's model is generating seemingly irrelevant and grammatically incorrect suggestions. Others speculate on the capabilities and limitations of the model, comparing it to GPT-2 and discussing the quality of its predictions. There is also discussion about the potential for Apple to improve the feature by incorporating higher-quality data or using GPT-3. Several users highlight the challenges of text prediction and autocorrection, including issues with slang and abbreviations. Some users share their experiences with Apple's spell checker and suggest using other tools like Google's spell check for better accuracy. In addition, there are comments about the nature of AI and the potential for AI technologies to be oversold or misused. Finally, there is a brief discussion about the practical limitations and privacy concerns of hosting large AI models on servers.

### A.I. and the Next Generation of Drone Warfare

#### [Submission URL](https://www.newyorker.com/news/news-desk/ai-and-the-next-generation-of-drone-warfare) | 73 points | by [fortran77](https://news.ycombinator.com/user?id=fortran77) | [97 comments](https://news.ycombinator.com/item?id=37549529)

The Deputy Secretary of Defense, Kathleen Hicks, has announced the Replicator initiative, an effort to modernize the American arsenal by adding fleets of artificially intelligent, unmanned, and relatively cheap weapons and equipment. These "attritable" machines can suffer attrition without compromising a mission. The initiative aims to field attritable autonomous systems at scale within the next eighteen to twenty-four months. Instead of concentrating resources on expensive and complicated equipment, Replicator aims to deploy equipment with a shorter shelf life, allowing for constant reinvention of technologies. The use of inexpensive aerial vehicles in concert with one another, known as drone swarms, is a key aspect of Replicator. This approach is based on "iPhone economics," where inexpensive physical devices with expensive software are deployed, so if the enemy destroys them, expensive software is not lost. The war in Ukraine provided proof of concept for drone swarms, as Ukraine used cheaper unmanned aerial vehicles to counter Russia's costly missile systems.

The discussion surrounding the announcement of the Replicator initiative has touched on a variety of topics. Some users have expressed concerns about the potential dangers of developing AI weapons and the implications for democracy. Others have referenced movies like Terminator and Slaughterbots, highlighting the ethical dilemmas associated with deploying such technology. The use of drones in the war in Ukraine has been cited as proof of concept for the effectiveness of drone swarms. There is also discussion about the challenges of balancing cost-effective defense with the need for human-designed weapons, as well as the difficulty of countering cheap and rapidly produced drone technology. The potential for AI-powered killbots and the threat they pose to humans is another topic of concern. Overall, there is a mix of skepticism, caution, and ethical considerations in the discussion.

### The Home Assistant Green is here

#### [Submission URL](https://www.theverge.com/23875557/home-assistant-green-announcement-price-specs-ten-year-anniversary) | 80 points | by [Tomte](https://news.ycombinator.com/user?id=Tomte) | [22 comments](https://news.ycombinator.com/item?id=37548884)

The Home Assistant Green is a new product introduced by the creators of Home Assistant, a software commonly used by privacy-focused individuals who want the benefits of smart home technology without compromising security. The Home Assistant Green aims to make the software more accessible to a wider range of users by providing an all-in-one box with a palatable price. Priced at $99, the Home Assistant Green features powerful hardware, including a RK3566 quad-core CPU, 32GB eMMC storage preloaded with Home Assistant's platform, 4GB of LDDR4x RAM, USB 2.0 slots, HDMI out, and a microSD slot for expansion. The device is designed to run solely on the Home Assistant Operating System and simplifies the onboarding process for users. To get started, users simply plug in the device, connect it to their router via ethernet, and go through the setup process using their phone or computer. The system will automatically detect compatible devices on the user's network. The Home Assistant Green is a convenient and affordable option for those who want to try out Home Assistant without the hassle of setting up hardware.

The discussion on this submission covers a variety of topics related to the Home Assistant Green and the Home Assistant software. Here are some key points:

- One commenter mentions the differences between the Home Assistant Green and the Home Assistant Yellow. They note that the Yellow version supports Raspberry Pi CM4-based boards, including CPU, RAM, networking, ZigBee, and Thread capability, while the Green version does not have these features. They speculate that the Green version might be a way to target a lower price point and simplify the installation process for new users.
- Another user mentions that they recently discovered the Home Assistant Supervisor, a well-designed and actively maintained open-source Python application. They appreciate the high-quality Python libraries and frameworks used in the project, but note a shortage of such libraries for studying open-source applications.
- Some users comment on the price and availability of the Home Assistant Green, noting that it is priced at $99 and that 1,000 units are available today, with 14,000 units available in October.
- The discussion also touches on the compatibility of Home Assistant with various smart home protocols. One user mentions their interest in Zigbee and Thread protocols, while another expresses disappointment with the current state of smart home integration in Home Assistant.
- There is some discussion about voice assistants and the integration of Home Assistant with Google Assistant. One user asks if Home Assistant supports voice commands, and another mentions that they are working on making a voice assistant for Home Assistant.
- Some users discuss the integration of Home Assistant with Apple HomeKit, noting that it provides HomeKit integration that works well.
- The conversation also touches on the use of Home Assistant for non-technical people and the discovery of smart home devices for touch support.
- Finally, there are some comments about other technologies and services that users have integrated with Home Assistant, such as YoLink, AdGuard, and Tailscale. Users share their experiences and discuss the ease of setting up these integrations.

### Spellburst: LLM–Powered Interactive Canvas

#### [Submission URL](https://arxiv.org/abs/2308.03921) | 95 points | by [araes](https://news.ycombinator.com/user?id=araes) | [13 comments](https://news.ycombinator.com/item?id=37540109)

Researchers Tyler Angert, Miroslav Ivan Suzara, Jenny Han, Christopher Lawrence Pondoc, and Hariharan Subramonyam have introduced an exciting new creative-coding environment called Spellburst. The interface is built on a large language model (LLM) and aims to enhance the process of exploratory creative coding. In the field of digital artwork, artists often start with a high-level semantic concept, like a "stained glass filter," and then programmatically implement it by tweaking code parameters such as shape, color, lines, and opacity. However, translating these semantic constructs into program syntax can be challenging, and existing programming tools do not lend themselves well to rapid creative exploration.

Spellburst addresses these challenges by providing a node-based interface that allows artists to create generative art and explore variations through branching and merging operations. The platform also incorporates expressive prompt-based interactions, enabling artists to engage in semantic programming. Moreover, Spellburst offers dynamic prompt-driven interfaces and direct code editing, allowing users to seamlessly switch between semantic and syntactic exploration.

The researchers evaluated Spellburst with artists and found that it has the potential to enhance creative coding practices. This innovative tool not only facilitates the translation of artistic ideas into code but also bridges the gap between semantic and syntactic spaces. The findings from this study could inform the design of future computational creativity tools.

Spellburst's novel approach to creative coding has the potential to revolutionize the way artists bring their ideas to life through code. With its user-friendly interface and powerful features, Spellburst opens up new possibilities for exploratory creative coding.

The discussion on Hacker News mainly centers around the novelty and potential applications of Spellburst, the new creative-coding environment introduced by the researchers.  One user shares a link to a video about the system on YouTube, highlighting its extensive design and development process. Another user mentions that Large Language Models (LLMs) have been gaining popularity and provides a link to a User Interface conference paper discussing their working structures and applications. Some users express interest in trying out Spellburst but have trouble finding where they can access it. One user mentions that there have been previews and tweets about its release but cannot find any public release. Another user comments that they are excited to try it and are willing to participate in testing. The discussion also touches on the use of metaphorical scratch paper in creative coding and the potential benefits it can offer. Some users mention the challenges of version control in creative coding tasks and express enthusiasm about the innovative features that Spellburst offers. One user comments on the post itself, providing an introduction and expressing their interest in the application of Large Language Models in creative endeavors.

---

## AI Submissions for Sat Sep 16 2023 {{ 'date': '2023-09-16T17:10:10.057Z' }}

### Generative Image Dynamics

#### [Submission URL](https://generative-dynamics.github.io/) | 310 points | by [hughes](https://news.ycombinator.com/user?id=hughes) | [26 comments](https://news.ycombinator.com/item?id=37536016)

Google Research has released a paper and demo showcasing their latest project called Generative Image Dynamics. This approach models an image-space prior on scene dynamics, allowing for the transformation of a single image into a seamless looping video or an interactive dynamic scene. The model learns from motion trajectories in real video sequences, such as trees swaying in the wind or clothes billowing. Using a frequency-coordinated diffusion sampling process, the model predicts per-pixel long-term motion representations in the Fourier domain, which are called neural stochastic motion textures. These textures can then be converted into dense motion trajectories that span an entire video. The project also includes an image-based rendering module, which can be used to turn still images into dynamic videos, or to allow users to interact with objects in photos. A demo is available where users can click and drag a point on an image to see how the scene moves. The project enables the simulation of object dynamics in response to user excitation and the generation of slow-motion videos by interpolating predicted motion textures.

The discussion on this submission revolves around various aspects of the project. Some users are discussing the potential of using generative images and cinemagraphs for marketing purposes, noting that they can have a bigger impact on viewers than regular still photos. Others are sharing examples of subtle movement in cinemagraphs and suggesting ways to qualify and describe these types of images. There is also a discussion about the feasibility of implementing the technology in video games, with some users noting that realistic physics and dynamic movements are already being handled in games like Red Dead Redemption 2. One user shares examples of games that have impressive grass and physics simulations. Another user mentions the limitations of the gaming industry in adopting deep learning and AI models due to performance and complexity issues. They also discuss the potential negative effects of randomly breaking immersion in games by generating non-continuous movements for characters.

The discussion also touches on related topics, such as the stability of video game physics and interacting with floors in games, the potential of combining photogrammetry and physics for realistic effects, and the use of low-vector movement requirements in EbSynth. One user appreciates Google's research efforts and shares excitement about the possibilities of combining machine learning and gaming. Another user expresses anticipation for stable diffusion GPT models in video games and notes the challenges in implementing them without extensive computational resources. Overall, the discussion showcases different perspectives on the applications, limitations, and potential of generative image dynamics in various domains, including marketing and gaming.

### Adobe will charge “credits” for generative AI

#### [Submission URL](https://helpx.adobe.com/firefly/using/generative-credits-faq.html) | 130 points | by [tambourine_man](https://news.ycombinator.com/user?id=tambourine_man) | [145 comments](https://news.ycombinator.com/item?id=37538878)

Generative credits are a new feature introduced by Adobe that provide priority processing of generative AI content. These credits are used when performing actions such as generating text effects, loading more images in Text to Image, using generative recolor in Adobe Illustrator, using text effects in Adobe Express, and using generative fill in Adobe Photoshop. The consumption of generative credits depends on the computational cost of the generated output and the value of the generative AI feature used. However, certain actions, such as using generative AI features defined as "0" in the rate table or trying a prompt in the Firefly gallery without refreshing, do not consume generative credits. 

The number of generative credits you have depends on your plan, and they reset each month. Different plans offer different numbers of generative credits, with higher-tier plans including more credits. For example, the Creative Cloud All Apps plan includes 1,000 generative credits per month, while lower-tier plans may include 250 or 100 generative credits. Adobe Stock paid subscriptions also include 500 generative credits per month. 

It's important to note that until November 1, 2023, subscribers of Creative Cloud, Adobe Firefly, Adobe Express, and Adobe Stock won't be subject to generative credit limits. However, starting from November 1, 2023, credit limits will apply. Adobe plans to expand generative AI features to include higher-resolution images, animation, video, and 3D in the future, and the number of generative credits consumed for those features may be greater. Overall, generative credits aim to enhance creative possibilities and empower users to create extraordinary content using AI technology.

The discussion on this submission covers various topics related to the use of generative AI, Adobe's plans, hardware requirements, and the implications for users. Some users express concerns about charging for AI-powered features and the need for compliance with regulations. Others discuss the possibility of Adobe moving towards local deployment of AI models and the trustworthiness of Adobe products. There are also discussions about the capabilities of consumer GPUs, the cost of hardware, and the potential energy consumption of running AI models. Users analyze the performance of different GPUs and compare them to Adobe's offerings. The discussion also touches on the future of AI models, the limitations of hardware, and the impact of energy constraints. Some users mention the availability of open-source alternatives and the flexibility of locally-run models. There are also discussions about licensing and the commercial use of AI-generated content.

### Unity's Self-Combustion Engine

#### [Submission URL](https://www.gamesindustry.biz/unitys-self-combustion-engine-this-week-in-business) | 148 points | by [erickhill](https://news.ycombinator.com/user?id=erickhill) | [144 comments](https://news.ycombinator.com/item?id=37535910)

Unity, the popular game development platform, faced backlash after introducing a new Runtime Fee. The fee applies to Unity developers of a certain size and requires them to pay a fee every time their game is installed on a new device after January 1, 2024. The fee is based on game installs, not sales, which has created confusion among developers. Unity initially stated that demos would count as installs, but later clarified that demos, trials, game bundles, and giveaways would not be included. However, subscription services like Game Pass would count as an install. In response to the fee, a collective of studios pulled Unity and IronSource ads from their titles and called upon others to do the same. Developers of popular games like Among Us and Slay the Spire have expressed their inclination to switch engines if the changes go through, citing trust as a crucial factor for developers using a commercial game engine. Unity, having recognized the importance of supporting developers in the long term, now faces the challenge of addressing concerns and restoring trust within its community.

The discussion on this topic revolves around several key points. Some users express skepticism about Unity's decision and suggest that the company is trying to gain a market advantage. Others raise concerns about the impact of the fee on developers and question Unity's handling of the situation. There is also a discussion about the similarities and differences between Unity and other game development engines like Unreal and Cryengine. In addition, some users discuss the broader implications of market dominance and the impact on industries such as taxis and ride-sharing. The discussion also touches on issues such as the safety of ride-sharing services and the impact of technology on traditional businesses. Some users also discuss the challenges faced by new generations of entrepreneurs and the changing dynamics of the business world. Finally, there is a debate about the pricing and accessibility of software in general, with some users arguing that the current market dynamics favor larger businesses and hinder smaller ones.

### Show HN: Superflows – open-source AI Copilot for SaaS products

#### [Submission URL](https://github.com/Superflows-AI/superflows) | 24 points | by [henry_pulver](https://news.ycombinator.com/user?id=henry_pulver) | [5 comments](https://news.ycombinator.com/item?id=37533503)

Superflows is an open-source toolkit that allows you to build an AI assistant for SaaS products. This AI assistant can understand natural language queries and make calls to the software's API to provide answers. For example, a CRM user could ask about the status of a deal or ask for recommendations on how to get deals back on track. The toolkit also provides a developer dashboard to configure and test the assistant, as well as pre-built UI components for easy integration into your product. You can try out the cloud version for free or self-host it. Superflows aims to make it easier for users to interact with software products and get the information they need.

The discussion on the submission started with a comment from user "SaarasM" who mentioned that they had recently tried to build a similar tool for their SaaS product but were not satisfied with its reliability. They were interested in trying out Superflows and asked if the tool had good reliability. User "henry_pulver" responded that they have had issues with reliability in similar tools, with only 80% of the tasks working smoothly and the remaining 20% being a challenge. However, they mentioned that they would like to try Superflows and provided their contact information for further discussion. User "RobertVDB" chimed in to mention that they have seen support for open-source models and that it is possible to self-host the models. In response to this, "henry_pulver" mentioned that they are currently working on a project called Base Llama 2, which aims to improve reliability by prompting users to provide feedback on the model's performance. They mentioned that they are also working on fine-tuning the model's prompts for better accuracy and will release it soon for others to self-host. Lastly, user "jmrmblw" expressed their appreciation for the developer dashboard and mentioned that open-source tools like Superflows are helpful for debugging purposes. Overall, the discussion mainly revolved around the reliability of similar tools, the possibility of self-hosting models, and positive feedback on Superflow's developer dashboard and open-source nature.

### GPT-4 is not getting worse

#### [Submission URL](https://coagulopath.com/gpt-4-is-not-getting-worse/) | 141 points | by [COAGULOPATH](https://news.ycombinator.com/user?id=COAGULOPATH) | [164 comments](https://news.ycombinator.com/item?id=37532522)

In a recent post on Hacker News, the author reflects on their initial criticism of GPT-4, OpenAI's state-of-the-art AI text generation model. They admit that their previous tests were flawed and biased, leading to an inaccurate assessment of the model's performance. The author acknowledges their personal dislike for GPT-4's tone and the hype surrounding AI, which may have influenced their desire for the model to fail. However, they have since reconsidered their stance and find themselves defending AI against unfounded criticisms. The author highlights a study that shows a decline in GPT-4's performance in identifying prime numbers but argues that mistakes are a part of learning and progress. Overall, the author's perspective has shifted, and they now recognize the need for a more balanced and objective approach when evaluating AI models.

The discussion on this Hacker News submission revolves around various aspects of GPT-4 and OpenAI's AI text generation models. Here are some key points from the conversation:

- One user mentioned encountering a bug in OpenAI's API that causes the response to stop streaming after 5 minutes of debugging prompt lines. They also mentioned that skipping sections in the generated output seems to be a common issue in information extraction tasks.
- Another user expressed annoyance at how frequently things change in AI models, making it challenging to keep up with updates and causing issues in their coding work.
- Some users discussed the limitations of GPT-4, such as its difficulty in reliably multiplying large numbers. However, others argued that mistakes in AI models are to be expected and should be seen as an opportunity for learning and improvement.
- There were discussions about the limitations of OpenAI's API in terms of response times and resource allocation. Some users pointed out that the 5-minute time limit for generating responses is insufficient and that OpenAI should provide better support.
- The topic of unintended behavior in AI models was raised, with users suggesting that people should not expect perfect results and should be aware of the limitations and potential issues.
- There were also discussions about server configuration, network connectivity, and potential streaming problems related to the OpenAI API.

Overall, the conversation highlighted the challenges and limitations of AI models like GPT-4 and the need for better documentation, support, and understanding of AI technologies.

### Mesa-optimization algorithms in Transformers

#### [Submission URL](https://arxiv.org/abs/2309.05858) | 23 points | by [kelseyfrog](https://news.ycombinator.com/user?id=kelseyfrog) | [5 comments](https://news.ycombinator.com/item?id=37531815)

Researchers from various institutions have released a paper titled "Uncovering mesa-optimization algorithms in Transformers," aiming to understand the superior performance of Transformers in deep learning. The study suggests that Transformers possess an architectural bias towards mesa-optimization, a learned process within the forward pass of a model. The research team reverse-engineered autoregressive Transformers trained on simple sequence modeling tasks to uncover underlying gradient-based mesa-optimization algorithms. They also demonstrated that the learned forward-pass optimization algorithm could be applied to solve supervised few-shot tasks. The researchers propose a novel self-attention layer called the mesa-layer, which can efficiently solve optimization problems specified in context and potentially improve the performance of Transformers. Overall, this work sheds light on the presence of mesa-optimization as a crucial but hidden operation within trained Transformers.

The discussion on this submission revolves around the significance and implications of the research paper.  One commenter finds the research fascinating and mentions that it explores the optimization process in Transformers, which can potentially lead to significant improvements in performance. Another commenter appreciates the in-depth analysis and difficulty of the paper, stating that it tackles complex concepts and demonstrates sophisticated methodologies. They also note that the paper is internationally significant. Another commenter points out that the paper primarily focuses on natural language processing tasks and how Transformers can be applied to different digital domains. They mention that the paper offers valuable applications, but they do not provide much feedback beyond that.

Further discussion delves into a detailed breakdown of the paper's content. It includes sections on the hypothesis of mesa-optimization in Transformers, reverse-engineering Transformers to uncover the internal optimization process, the few-shot learning capabilities of Transformers, the introduction of the mesa-layer, and the generalization of previous work. The commenters analyze the theoretical connections, such as linear self-attention gradient descent, and the two-stage mesa-optimizer. They also discuss the empirical analysis, where the paper concludes that Transformers implicitly perform optimization steps and propose the mesa-layer to enhance model performance. Overall, the discussion appreciates the importance of understanding the optimization process in Transformers and the potential impact of the proposed mesa-layer. Commenters delve into the technical aspects of the research and provide insights into its significance within the field of deep learning.